# Detected language: en (p=1.00)

[0.00s -> 8.42s]  So, hopefully everyone's having a good time with assignment one.
[8.42s -> 9.94s]  It's due tonight.
[9.94s -> 11.86s]  Let us know if you need an extension.
[11.86s -> 14.96s]  Assignment two is coming out soon.
[14.96s -> 19.64s]  We're putting on the finishing touches onto some of the Triton stuff.
[19.64s -> 20.64s]  Hopefully you'll enjoy it.
[20.64s -> 25.10s]  You'll get to implement Flash Attention 2, or parts of Flash Attention 2, which I think
[25.10s -> 27.60s]  will be nice.
[27.60s -> 29.64s]  So today we're going to talk about GPUs.
[29.64s -> 34.44s]  GPUs are the thing that makes our language models go.
[34.44s -> 37.86s]  So they're pretty critical to get, right?
[37.86s -> 43.70s]  And if you haven't really studied the hardware that makes your models run, they can seem
[43.70s -> 44.88s]  pretty mysterious.
[44.88s -> 51.24s]  So my goal today is to try to make CUDA and GPUs less magic.
[51.24s -> 54.88s]  And one of the things that I want to demystify, you don't have to understand the plot, there's
[54.88s -> 57.80s]  a lot on the slide I know.
[57.80s -> 60.18s]  Why do GPUs get slow?
[60.18s -> 62.60s]  And they get slow in very mysterious ways.
[62.60s -> 67.08s]  I will try to talk through this plot towards the end of lecture.
[67.08s -> 72.12s]  As you increase the size of your matrix multiplies, you might expect it either gets
[72.12s -> 78.52s]  slower or faster or whatever, you get these very unpredictable looking wave-like patterns.
[78.52s -> 82.70s]  And you're like, why is my GPU fast at certain multiples of certain numbers and
[82.70s -> 84.04s]  slow at others, right?
[84.04s -> 85.04s]  It's very mysterious.
[85.04s -> 87.68s]  We'll try to understand that.
[87.68s -> 91.60s]  The other thing is we would like to understand how to make fast algorithms.
[91.60s -> 94.64s]  I think almost all of you have heard of flash attention.
[94.64s -> 100.32s]  It's the thing that makes much longer contexts possible by very cleverly computing the attention
[100.32s -> 103.04s]  operation inside a transformer.
[103.04s -> 107.84s]  And so maybe you would like to come up with new algorithms or new implementations like
[107.84s -> 109.12s]  flash attention, right?
[109.12s -> 112.90s]  Like what primitives and what components do we need to understand in order to be able
[112.90s -> 113.90s]  to do that, right?
[113.90s -> 116.06s]  So those are kind of the two learning goals of today.
[116.06s -> 119.08s]  The first one is, you know, by the end of the lecture, you should feel kind of comfortable
[119.08s -> 120.08s]  with GPUs.
[120.08s -> 121.82s]  You should kind of understand how they work.
[121.82s -> 125.42s]  And the second one is you should feel comfortable accelerating certain parts of your
[125.42s -> 126.42s]  algorithms.
[126.42s -> 127.42s]  You make a new architecture.
[127.42s -> 133.14s]  You should hopefully feel like you can try to accelerate that with CUDA.
[133.14s -> 137.62s]  And because hardware is not necessarily the domain in which I work, you know, there's
[137.62s -> 140.94s]  special resources that I have to give a lot of credit to, especially Horace Hays'
[140.94s -> 144.54s]  blog where he's got a lot of fun GPU facts that you can learn about.
[144.54s -> 148.70s]  For example, why are matrix multiplies that are filled with zeros faster than ones that
[148.70s -> 149.82s]  are not filled with zeros?
[149.82s -> 152.30s]  You can learn by going to his blog.
[152.30s -> 155.94s]  There's also other resources that I've drawn from, like the CUDA mode group and the nice
[155.94s -> 157.86s]  TPU book from Google.
[157.86s -> 161.02s]  If this topic interests you, you know, I'd encourage you to go and look at those resources
[161.02s -> 162.02s]  to learn more.
[162.02s -> 166.06s]  Because this is in some ways like a shallow, but hopefully, you know, complete coverage
[166.06s -> 168.52s]  of the hardware.
[168.52s -> 173.78s]  So today we're only going to focus on, you know, non-parallel parts of the hardware stack.
[173.78s -> 177.94s]  So we're going to study the GPU, like a single accelerator in depth, how they work and some
[177.94s -> 179.08s]  important parts.
[179.08s -> 183.12s]  I'm also going to talk very, very briefly about TPUs, because in some ways, they're
[183.12s -> 185.16s]  very similar conceptually to a GPU.
[185.16s -> 187.96s]  And so my discussion here is going to carry over.
[187.96s -> 192.64s]  And then once we understand kind of the hardware and execution model of the GPU, then we're
[192.64s -> 196.96s]  going to try to understand what makes GPUs go fast on certain workloads, what makes
[196.96s -> 197.96s]  them slow.
[197.96s -> 199.56s]  And then we're going to try to understand the performance.
[199.56s -> 203.52s]  And then the last part, this is kind of going to be almost like a hands-on piece.
[203.52s -> 205.88s]  I'm going to try to walk through flash attention, right?
[205.88s -> 209.36s]  I'm going to take all of the lessons that we've learned and try to walk you through
[209.36s -> 212.38s]  flash attention, saying, see, here's how it all comes together, right?
[212.38s -> 217.08s]  So that's the last part of today's lecture.
[217.08s -> 221.24s]  So you know, many of you have, you know, taken an NLP course.
[221.24s -> 224.76s]  And these days in an NLP course, I think you teach some amount of scaling laws.
[224.76s -> 227.08s]  And so you've probably seen this, right?
[227.08s -> 229.84s]  And so this is just setting the context.
[229.84s -> 234.32s]  We know that having more compute is helpful for training large language models.
[234.32s -> 237.20s]  This is a pre-training scaling chart, but you could replace this with an inference
[237.20s -> 239.48s]  scaling chart if you would like.
[239.48s -> 242.94s]  It's generally agreed upon that the more compute you have, the more processing you
[242.94s -> 243.94s]  can do on your data.
[243.94s -> 244.94s]  You can ingest more data.
[244.94s -> 246.36s]  You can train larger models.
[246.36s -> 248.36s]  All those lead to improved performance, right?
[248.36s -> 252.16s]  So you might think of, of course, you know, deep learning is really important, but what's
[252.16s -> 256.88s]  really driven performance is, you know, faster hardware, better utilization, improved parallelization,
[256.88s -> 257.88s]  right?
[257.88s -> 263.44s]  So that's kind of setting the stage of why hardware is important to understand.
[263.44s -> 266.22s]  And of course, you know, once you think about compute scaling, you ask, okay, how
[266.22s -> 267.52s]  do we get compute scaling?
[267.52s -> 271.04s]  How do we get our models to train faster?
[271.04s -> 276.08s]  So kind of in the early days, you know, of semiconductor scaling, if you were thinking
[276.08s -> 280.20s]  about, okay, CPUs, how do they get faster?
[280.20s -> 283.72s]  They, you know, would scale under something called Dennard scaling, right?
[283.72s -> 287.48s]  With Moore's law, you would sort of double the amount of transistors on a chip every
[287.48s -> 288.48s]  year.
[288.48s -> 293.44s]  And if you have this doubling, what you end up is Dennard scaling, where smaller
[293.44s -> 297.00s]  and smaller transistors can be driven at faster and faster clock speeds with lower
[297.00s -> 300.12s]  and lower power, which in turn give you more performance, right?
[300.12s -> 303.72s]  And then in the 1980s to 2000s, this sort of tapped out.
[303.72s -> 307.40s]  You can kind of see in this chart here by Hennessy and Patterson that single-thread
[307.40s -> 311.04s]  performance, that's the blue dots here, that basically started to taper out.
[311.04s -> 314.72s]  Of course, the number of transistors didn't really, you know, start falling off.
[314.72s -> 317.92s]  You did have, you know, chips with higher and higher transistor densities, but that
[317.92s -> 318.92s]  wasn't helpful.
[318.92s -> 322.88s]  It wasn't giving you higher throughput on single threads.
[322.88s -> 327.72s]  And so this means that we can't just do computation faster in absolute terms.
[327.72s -> 332.60s]  You know, what we have to make up for it with is parallel scaling, right?
[332.60s -> 338.20s]  So the story of scaling for deep learning and neural networks is going from single-thread
[338.20s -> 342.96s]  scaling, which is just doing your computation faster in absolute terms, to parallel scaling,
[342.96s -> 346.84s]  where you have a lot of workloads that are all computed at once.
[346.84s -> 351.42s]  And this is one of my favorite, you know, sort of compute scaling charts by Bill Dowley
[351.42s -> 356.16s]  in his keynote, where, you know, he's showing the super-exponential increase in the number
[356.16s -> 364.64s]  of integer operations per second going from, you know, the earliest K20s to the H100, right?
[364.64s -> 370.42s]  And it's kind of like this really remarkable exponential or super-exponential curve.
[370.42s -> 375.56s]  And so, you know, we have to really understand how to take advantage of this curve in order
[375.56s -> 378.68s]  to really get the most out of our language model, right?
[378.68s -> 382.92s]  So that's kind of going to be our goal.
[382.92s -> 387.24s]  Since I've already hinted at this kind of important difference, right, CPU is something
[387.24s -> 391.16s]  that I think everyone's familiar with once you start doing programming, right?
[391.16s -> 392.16s]  It's this execution model.
[392.16s -> 396.16s]  If you have a program, it goes through, and in a single thread, it executes step-by-step
[396.16s -> 397.16s]  what's happening.
[397.16s -> 400.84s]  And in order to support that kind of an execution model, what do you need?
[400.84s -> 402.68s]  Well, you need big control units.
[402.68s -> 407.06s]  You just need to generally run these things very quickly because you have a lot of branching
[407.06s -> 409.64s]  and you have a lot of conditional control logic, right?
[409.64s -> 415.56s]  So a CPU, this is an abstracted diagram, is going to dedicate, you know, a lot of its
[415.56s -> 420.24s]  chip towards, you know, large control branch prediction, and it's going to run these, you
[420.24s -> 422.56s]  know, very quickly because it doesn't have that many threads.
[422.56s -> 426.36s]  You know, there are CPUs with lots and lots of cores now, but compared to a GPU,
[426.36s -> 428.16s]  it's almost nothing.
[428.16s -> 435.00s]  And so, in contrast, the GPU has really tons and tons of compute units, ALUs, right?
[435.00s -> 436.88s]  So those are the little green boxes.
[436.88s -> 441.00s]  And there's much smaller amounts of the chip dedicated to control, so there's a little
[441.00s -> 446.00s]  bit of control logic sort of orchestrating tons and tons of compute units, you know,
[446.00s -> 448.56s]  operating in parallel.
[448.56s -> 452.44s]  And I think mentally, so this is kind of the picture of what is being emphasized
[452.44s -> 454.62s]  in a CPU versus a GPU.
[454.62s -> 458.56s]  But if you kind of look at what the design goals are, they are designed for very different
[458.56s -> 460.36s]  sort of goals.
[460.36s -> 463.20s]  So you can think about CPUs as optimizing for latency.
[463.20s -> 465.48s]  I want to finish my tasks as quickly as possible.
[465.64s -> 472.28s]  So if I have tasks T1 through T4 here on the right side, you know, in a CPU, I'm going to try
[472.28s -> 474.60s]  to finish each task as quickly as possible.
[474.60s -> 478.80s]  And so if you want any one of these tasks to be finished quickly, T1 is going to complete
[478.80s -> 480.00s]  really quickly.
[480.00s -> 482.76s]  In GPU, you're optimizing for high throughput.
[482.76s -> 484.04s]  Like, I don't care about latency.
[484.04s -> 488.48s]  I just want all of my tasks that I have in aggregate to complete as quickly as possible.
[488.48s -> 491.76s]  And to support that, you know, maybe you have lots of threads, and these threads can go
[491.76s -> 493.68s]  to sleep and wake up very quickly.
[494.16s -> 499.64s]  And in the end, you know, you finish all of your workload, T1 through T4, you know, before the CPU
[499.64s -> 503.36s]  one does, even though individually, all of these have sort of higher latency, right?
[503.36s -> 507.12s]  So they have different sort of design principles and design goals.
[508.84s -> 510.34s]  Okay.
[510.34s -> 515.24s]  And so a GPU has a pretty different anatomy, and I don't know if, you know, you all have
[515.24s -> 518.92s]  ever looked at what a GPU sort of layout diagram looks like.
[518.92s -> 523.44s]  I'll actually show you the chip figures in a moment here.
[523.44s -> 529.92s]  But the core idea, and this is important conceptual concepts behind a GPU, is that a GPU
[529.92s -> 534.88s]  executes, you know, many, many SM, streaming multiprocessors.
[534.88s -> 538.76s]  And a streaming multiprocessor, you can kind of think of as an atomic unit.
[538.76s -> 543.76s]  When you're programming in something like Triton, they're going to operate at the level of SM.
[543.76s -> 549.48s]  And within each SM, it contains many SPs, streaming processors.
[549.48s -> 555.20s]  And a streaming processor is going to execute a whole bunch of threads in parallel.
[555.20s -> 558.52s]  So one way to think about it is SM has a bunch of control logic.
[558.52s -> 559.84s]  It can decide what to execute.
[559.84s -> 561.60s]  It can do, for example, branching.
[561.60s -> 568.48s]  SPs are going to take the same instruction and apply it to many different pieces of data, right?
[568.48s -> 572.80s]  And so you can do tons and tons of parallel computation under this model.
[572.80s -> 575.40s]  An SM is sort of each granular unit of control.
[575.40s -> 578.48s]  SP can do a lot of computation individually.
[578.48s -> 584.48s]  And if you look at an A100, which is the previous generation GPU at this point, you've got 128 SMs.
[584.48s -> 587.56s]  You know, that's a lot more than the most cores for CPUs.
[587.56s -> 594.80s]  And each of these SMs is going to have a very large number of SPs and specialized sort of matrix multiply units inside them.
[599.20s -> 600.96s]  And so that's kind of the compute model.
[600.96s -> 601.88s]  Was there a question? Sorry.
[601.88s -> 613.68s]  So the question was, is this GPU the same as that GPU?
[613.68s -> 617.40s]  Yes, like this is a kind of cartoon version of this.
[617.40s -> 619.76s]  You could kind of think of each row as being an SM.
[619.76s -> 621.16s]  It's got its own control units.
[621.16s -> 628.04s]  Each green block might be sort of one of these green blocks here, like an SP32 sort of processing unit inside of it.
[628.20s -> 634.68s]  And each SM can sort of, you know, operate various pieces that it owns, like the tensor cores, to do computation.
[637.24s -> 637.44s]  Cool.
[639.68s -> 639.92s]  Okay.
[640.40s -> 643.44s]  And there's going to be two important things.
[643.44s -> 645.28s]  You think of GPU as, you know, computers.
[645.28s -> 646.20s]  They compute.
[646.20s -> 650.28s]  But actually, computation is only one of the two important things we have to keep track of, right?
[650.28s -> 658.00s]  Memory is arguably more important at this point, and it will continue to be more important in terms of the performance profiles of how we run our
[658.00s -> 659.84s]  programs on the GPU.
[659.84s -> 664.84s]  And so to understand memory, you kind of have to understand the physical layout of the GPU and the chip.
[664.84s -> 672.16s]  Because in some sense, you know, when you're operating at such fast speeds, the physical proximity of the memory starts to matter quite a bit.
[672.16s -> 680.00s]  And so I will show you kind of the physical proximity of how things are laid out and how that relates to how you should think about memory access and performance.
[680.00s -> 685.52s]  So the closer a piece of memory is to each SM, the faster it's going to be.
[685.52s -> 694.00s]  So there's going to be certain very, very, very fast kinds of memory, like L1 and shared memory, and that's going to live inside of the SM, right?
[694.00s -> 695.44s]  And that's going to be really fast, right?
[695.44s -> 702.04s]  Things like registers, things like things you're reading and writing very frequently, you're going to want to put into the L1 and shared memory.
[702.04s -> 708.28s]  L2 cache, as you can kind of see, there's these green areas, which are SMs, and then there's these blue areas.
[708.28s -> 710.12s]  This is on the GPU chip, right?
[710.12s -> 714.20s]  These are L2 memory that's sort of right next to the SMs, right?
[714.20s -> 718.28s]  So they're not inside the SMs, but they're physically still quite close, right?
[718.28s -> 720.96s]  And these are still pretty fast.
[720.96s -> 726.44s]  You know, they're still a factor of 10 slower, but they're still reasonably fast.
[726.44s -> 734.24s]  And then outside of the chip itself, this is sort of a, you know, I think this is like a 3090 card or something like this, or maybe a PCIe A100.
[734.24s -> 736.28s]  Oh, this is a PCIe A100.
[736.28s -> 741.88s]  You know, you've got your GPU here, and you've got actually DRAM sort of living next to the chip, right?
[741.88s -> 745.52s]  So it has to actually go physically outside of the chip and connect.
[745.52s -> 750.12s]  And you can kind of see on this chip diagram here, these yellow connectors at the edges.
[750.12s -> 752.04s]  These are HBM connectors.
[752.04s -> 758.44s]  These are connecting to the DRAM chips that are outside of the actual GPU.
[758.44s -> 762.48s]  And you can kind of see the speed that it takes to access these, right?
[762.48s -> 771.68s]  The on-SM memory is much, much faster, like 20 clock cycles to access something from there, whereas it's going to take something like 200 or 300 clock cycles to access
[771.68s -> 774.56s]  something from the L2 cache or global memory, right?
[774.56s -> 777.00s]  And this factor of 10 is going to hurt you real bad, right?
[777.00s -> 785.96s]  So if you have a piece of computation that requires you to access global memory, right, it might mean that you actually run out of work to do on your SM.
[785.96s -> 787.20s]  You've multiplied all the matrices.
[787.20s -> 788.00s]  You've run out.
[788.00s -> 789.36s]  Now you just have to idle, right?
[789.36s -> 790.48s]  So utilization won't be good.
[790.48s -> 798.28s]  And this will be a really key theme, thinking about memories in some sense, the key to thinking about how GPUs work.
[798.28s -> 804.20s]  And in assignment two, you're going to actually be writing high-performance code for a GPU.
[804.20s -> 810.28s]  So you have to actually think about the execution model of how a GPU actually executes things.
[810.28s -> 815.36s]  And this is somewhat complicated, but not insanely so.
[815.36s -> 818.88s]  There's sort of three granularities of things that you need to think about.
[818.88s -> 822.32s]  There's blocks, there's warps, and there's threads.
[822.32s -> 825.72s]  And that's the order in which kind of the granularity narrows down, right?
[825.72s -> 829.04s]  Blocks are kind of these big groups of threads.
[829.04s -> 832.16s]  And each block is going to be assigned to an SM.
[832.16s -> 834.64s]  So think about this as each SM is kind of a worker.
[834.64s -> 836.48s]  It's its own autonomous unit.
[836.48s -> 840.00s]  And a block is going to be assigned to an SM to process, right?
[840.00s -> 841.88s]  So this is each granular unit.
[841.88s -> 845.28s]  Now, then within these blocks are a whole bunch of threads.
[845.28s -> 848.88s]  Each thread is a sort of a piece of task that needs to be done.
[848.88s -> 852.20s]  And when these threads execute, they're going to execute in groups.
[852.20s -> 853.96s]  And this is a thing called a warp, right?
[854.00s -> 856.96s]  So you take a block, which is a collection of threads,
[856.96s -> 859.72s]  and you're going to take threads from that block,
[859.72s -> 863.96s]  and they're going to execute in groups of 32 consecutively numbered threads each time.
[863.96s -> 865.84s]  And that's sort of called warps.
[865.84s -> 868.56s]  And so you can kind of see at this diagram here what's happening.
[868.56s -> 869.96s]  You've got a bunch of blocks.
[869.96s -> 872.16s]  Each block is assigned to a different SM.
[872.16s -> 875.08s]  And within each block, there's going to be many different warps.
[875.08s -> 878.76s]  And each warp is going to consist of a whole bunch of threads,
[878.76s -> 883.92s]  and all of these threads are going to execute the same instruction on different data, right?
[883.92s -> 885.76s]  And so this is kind of the execution model.
[885.76s -> 890.40s]  Right now, it seems probably mysterious what these blocks and warps and threads are.
[890.40s -> 893.68s]  They will have important implications for our performance
[893.68s -> 897.00s]  in how we design things like CUDA kernels later.
[897.00s -> 898.48s]  So hopefully, you can kind of remember this.
[898.48s -> 902.12s]  I'll refresh your memory kind of as we go.
[902.12s -> 904.72s]  Hopefully, that's clear.
[904.72s -> 908.28s]  So that was the kind of logical execution model of a GPU.
[908.28s -> 912.60s]  And if you understand that, you kind of understand how GPUs execute things.
[912.60s -> 915.44s]  There's also a logical sort of memory model of a GPU.
[915.44s -> 917.48s]  So now I'm not showing you the physical hardware.
[917.48s -> 921.48s]  This is just kind of how you think about the programming of a GPU.
[921.48s -> 922.92s]  And so there's registers.
[922.92s -> 926.52s]  So these are really fast, you know, storing single numbers type storage.
[926.52s -> 930.68s]  You've got local memory, you've got shared memory, and you've got global memory, right?
[930.68s -> 934.64s]  And that increases in sort of the memory hierarchy.
[934.64s -> 937.16s]  It gets slower and slower and slower.
[937.16s -> 939.44s]  And your code can sort of write to global memory.
[939.44s -> 944.52s]  But you can also write to constant memory, which is not something that's used too often.
[944.52s -> 948.64s]  And so each thread can access its own register and shared memory.
[948.64s -> 952.12s]  But information that goes across blocks need to be written to global memory.
[952.12s -> 954.18s]  This is actually quite important, right?
[954.18s -> 959.00s]  So now it means that whenever you write a thread that executes something, ideally it's
[959.00s -> 962.16s]  operating on sort of the same small amount of data.
[962.16s -> 964.68s]  So you load that small amount of data into shared memory.
[964.68s -> 967.32s]  All the threads are very happy accessing that shared memory.
[967.32s -> 968.32s]  It terminates.
[968.32s -> 969.32s]  It's done.
[969.32s -> 970.32s]  It's done.
[970.32s -> 971.32s]  It's a great execution model.
[971.32s -> 974.16s]  Instead, if you have a thread that needs to access data all over the place, you know,
[974.16s -> 976.00s]  that's going to have to access global memory.
[976.00s -> 977.00s]  That's very, very slow.
[977.00s -> 981.12s]  This thing will come back, you know, as we talk about different ways of operating
[981.12s -> 983.32s]  on a GPU.
[983.32s -> 984.74s]  Hopefully that's clear.
[984.74s -> 989.12s]  That's kind of the very, you know, high-level four-slide overview of a GPU.
[989.12s -> 995.88s]  If you have kind of questions about how any of that works, feel free to ask me as I go on.
[995.88s -> 996.88s]  Okay.
[996.88s -> 998.88s]  So here's a side thread.
[998.88s -> 1003.44s]  Last year, I didn't cover this because I think resources on TPUs was a little thin, but the
[1003.44s -> 1007.64s]  nice TPU book or internet website that I mentioned at the start of the lecture came
[1007.64s -> 1012.12s]  out, and that has actually a lot of nice details, and I talked to a few Google people
[1012.12s -> 1013.12s]  about the TPU.
[1013.12s -> 1016.88s]  And at a high level, it's very, very similar to a GPU.
[1016.88s -> 1019.84s]  And so I want to just talk for a moment about TPUs.
[1019.84s -> 1024.12s]  You may never, you know, operate on a TPU, but I think it's important to understand
[1024.12s -> 1029.36s]  that these alternative accelerators operate in many ways very similarly.
[1029.36s -> 1032.68s]  So here's a diagram of what a TPU looks like.
[1032.68s -> 1037.08s]  So there's something called a tensor core, and mentally, you can think about a tensor
[1037.08s -> 1039.86s]  core as being similar to an SM or streaming multiprocessor.
[1039.86s -> 1043.56s]  Each of these are kind of its own atomic units that can operate on data.
[1043.56s -> 1048.52s]  There's a scalar unit, which is basically a control unit, and it can also do CPU-like
[1048.52s -> 1050.08s]  arbitrary things.
[1050.08s -> 1052.40s]  You've got a vector unit that can operate on vectors.
[1052.40s -> 1055.52s]  So if you've got a vector and you want to operate entry-wise on it, that's a good place
[1055.52s -> 1056.52s]  to do it.
[1056.52s -> 1060.56s]  And then it's got a very big specialized, you know, part of the chip dedicated to just
[1060.56s -> 1063.50s]  doing matrix multiplies called the MXU.
[1063.50s -> 1066.92s]  And then it's got very fast memory for vector memory and SM.
[1066.92s -> 1070.96s]  Both of these are very fast on-chip, or like on tensor core memory.
[1070.96s -> 1073.92s]  And then there's high bandwidth memory that lives outside of the chip, right?
[1073.92s -> 1076.52s]  So hopefully you see the similarities to an SM, right?
[1076.52s -> 1080.76s]  There's slow memory outside, very fast memory inside, and there's specialized hardware
[1080.76s -> 1083.04s]  to do matrix multiplication.
[1083.04s -> 1085.32s]  Core structure is very much the same.
[1085.32s -> 1089.04s]  The difference is, I'll talk about this in the parallelism lecture next week, you know,
[1089.04s -> 1092.48s]  how the accelerators are networked together is a little bit different.
[1092.48s -> 1096.78s]  And then also, you know, notice I didn't talk about warps, I didn't talk about any
[1096.78s -> 1098.52s]  of that other stuff.
[1098.52s -> 1102.76s]  Tensor cores are in some ways very simple because they're optimized to just do matrix
[1102.76s -> 1103.76s]  multiplies, right?
[1103.76s -> 1107.08s]  Like, the tensor core, unlike the GPU, doesn't attempt to do anything but that.
[1107.08s -> 1109.48s]  And so that's in some ways very, very simple.
[1109.52s -> 1112.96s]  Much simpler in architecture, but conceptually doing the same thing.
[1115.32s -> 1116.32s]  Yes?
[1116.32s -> 1123.60s]  Is it called tensor because it's also in some ways optimized to take on general tensors,
[1123.60s -> 1127.20s]  or maybe this is just enough to work on some new ones?
[1127.20s -> 1130.44s]  Yeah, so the question was, you know, is it called tensor because it can operate
[1130.44s -> 1132.48s]  on arbitrary tensors?
[1132.48s -> 1136.48s]  So it can operate on arbitrary tensors, like in do the indexing.
[1136.48s -> 1139.36s]  The operations that MXC performs is a matrix multiply.
[1139.40s -> 1143.04s]  And so it would always be like a batch matrix multiply operating on a tensor.
[1143.04s -> 1146.20s]  So it's kind of both a yes and a no answer, if that makes sense.
[1146.20s -> 1149.88s]  So they operate on tensors, but the operations they always perform are matrix multiplies,
[1149.88s -> 1154.40s]  not more complicated tensor operations that you can do.
[1154.40s -> 1155.92s]  Cool.
[1155.92s -> 1161.36s]  The reason why the GPU has been so successful is that, you know, it scales up really easily.
[1161.36s -> 1164.80s]  If you want more processing power, just add more SMs, right?
[1164.80s -> 1168.64s]  You don't have to worry about driving the clock faster and getting more heat dissipation
[1168.64s -> 1169.68s]  problems.
[1169.68s -> 1174.88s]  Programming-wise, CUDA is intimidating, but it's actually, you know, not as horrendous
[1174.88s -> 1177.20s]  to program because of its programming model.
[1177.20s -> 1181.64s]  Like, the way it works is, within each SM, right, you have a thread, and it executes
[1181.64s -> 1184.52s]  the same instruction on a bunch of different pieces of data, right?
[1184.52s -> 1186.48s]  That's conceptually sort of easy to reason about.
[1186.48s -> 1188.08s]  You can think through what that means.
[1188.08s -> 1191.12s]  And especially it's nice if you're operating over a matrix and you're doing sort of very
[1191.12s -> 1191.96s]  simple operations.
[1191.96s -> 1194.88s]  It's exactly this kind of SIMP model.
[1194.88s -> 1198.44s]  Finally, each of these threads are very lightweight, and they can be kind of stopped
[1198.44s -> 1199.80s]  and started at any time.
[1199.80s -> 1203.92s]  And so if you need to wait for another thread, or if you need to sort of, like, evict something
[1203.92s -> 1207.32s]  and, like, start another process, all these threads are very lightweight.
[1207.32s -> 1210.56s]  So this just kind of means that there's not much state associated with the threads,
[1210.56s -> 1215.08s]  and they can kind of be stopped and started, which allows GPUs to get high utilization
[1215.08s -> 1219.80s]  within sort of each SM.
[1219.80s -> 1225.52s]  So GPUs, you know, obviously graphics processing units, and for much of its life, you know,
[1225.52s -> 1231.56s]  in the early days, it was not used to do scientific computing, but, you know, people,
[1231.56s -> 1235.60s]  because it was programmable, researchers figured out how to use, you know,
[1235.60s -> 1238.88s]  early NVIDIA GPUs to do fast matrix multiplies.
[1238.88s -> 1242.96s]  This is one of the early papers on, you know, doing fast matrix multiplies with
[1242.96s -> 1246.32s]  graphics hardware, and it shows, you know, how you can, you know, hack kind of
[1246.32s -> 1250.36s]  things like the texture buffer and so on to get it to do matrix multiplies, right?
[1250.36s -> 1254.12s]  And so, you know, even without specific support for map moles, you know,
[1254.16s -> 1256.68s]  researchers figured out how to do it.
[1256.68s -> 1260.84s]  But I think now, you know, especially in this day and age, NVIDIA and others have
[1260.84s -> 1262.72s]  realized matrix multiplies are special.
[1262.72s -> 1266.12s]  Like, if you're doing deep learning, right, most of your workload is matrix
[1266.12s -> 1270.60s]  multiplies, and so matrix multiplies are, in some sense, blessed operations.
[1270.60s -> 1275.72s]  So this is a chart showing the number of teraflops per second by different
[1275.72s -> 1280.04s]  generations of NVIDIA GPUs, and the orange line is your map mole flops, right?
[1280.04s -> 1282.56s]  Like, your performance you can get if you're doing map moles.
[1282.56s -> 1285.36s]  The blue line is your non-map mole flops, right?
[1285.36s -> 1290.28s]  And you see kind of this big, big gap at V100s when they started putting in sort
[1290.28s -> 1295.08s]  of tensor cores that were specialized hardware to do matrix multiplies,
[1295.08s -> 1299.12s]  and you see this gigantic gap in the matrix multiply performance relative
[1299.12s -> 1301.96s]  to the non-map mole performance, right?
[1301.96s -> 1304.80s]  And so, if you're going to design any sort of a neural architecture,
[1304.80s -> 1307.44s]  I was saying this, you know, in the architecture part as well,
[1307.44s -> 1311.44s]  you have to have most of your workload be matrix multiplies because that's the
[1311.44s -> 1314.44s]  first thing that's, you know, orders of magnitude faster than any other
[1314.44s -> 1317.44s]  operation that you're going to be able to do on a GPU, right?
[1317.44s -> 1320.56s]  So if you make, like, a non-map mole-based neural network,
[1320.56s -> 1323.64s]  you're going to be in big, big trouble.
[1323.64s -> 1327.16s]  And then kind of the last thing that I want you to kind of understand as just
[1327.16s -> 1331.28s]  general facts, you know, map moles is fast is one thing,
[1331.28s -> 1334.40s]  but the other thing that's important to remember is kind of the relative
[1334.40s -> 1337.32s]  scaling of the different components of the GPU.
[1337.32s -> 1341.48s]  So this is a very nice chart that shows, you know, how quickly different
[1341.48s -> 1345.72s]  components of the GPU or different components of the, let's call it,
[1345.72s -> 1348.40s]  like, LM training stack are scaling.
[1348.40s -> 1354.44s]  So the blue line is the connectivity from the GPU to the host, right?
[1354.44s -> 1356.76s]  Like, the server that it's attached to, right?
[1356.76s -> 1360.76s]  So you can use PCIe, you can use NVLink, you can use all these fancy
[1360.76s -> 1361.68s]  interconnects.
[1361.68s -> 1364.04s]  They are growing, but they're growing somewhat slowly, right?
[1364.20s -> 1368.84s]  So this chart is like normalized scaling, you know, bandwidth relative to when,
[1368.84s -> 1372.60s]  you know, the first generation of interconnects.
[1372.60s -> 1376.36s]  The green line, this is the global memory speed, right?
[1376.36s -> 1379.88s]  So you go from GDDR to HBMQE, and that's much, much faster, right?
[1379.88s -> 1380.56s]  This is log scale.
[1380.56s -> 1385.00s]  It's 100x faster, but this is still kind of slow scaling, right?
[1385.00s -> 1387.92s]  And the gray line here, right, this is compute scaling.
[1387.92s -> 1390.52s]  This is the number of floating point operations if you're, you know,
[1390.52s -> 1392.24s]  considering the map mole flops.
[1392.24s -> 1395.12s]  This is how fast the compute has been scaling.
[1395.12s -> 1396.72s]  And this is astoundingly fast.
[1396.72s -> 1400.68s]  It's like one to 100,000 times faster.
[1400.68s -> 1404.60s]  And so, kind of in the early days of the scaling, maybe your problems were
[1404.60s -> 1405.80s]  flops-based, right?
[1405.80s -> 1409.16s]  Like, you just didn't have enough flops to do your matrix multiplications.
[1409.16s -> 1412.16s]  But now, you know, all the way to the right with the H100s, you know,
[1412.16s -> 1414.68s]  these are astoundingly fast GPUs.
[1414.68s -> 1417.60s]  Your bottlenecks are probably going to end up being memory, right?
[1417.60s -> 1419.40s]  Because memory is not growing as fast, right?
[1419.40s -> 1422.64s]  And as we go into the future, you know, this is not really going to change.
[1422.64s -> 1424.28s]  DRAM is very hard to scale.
[1424.28s -> 1426.24s]  You're going to keep getting this bigger and bigger gap, right?
[1426.24s -> 1429.20s]  So, if you're ever designing, you know, hardware-efficient algorithms,
[1429.20s -> 1431.24s]  you're going to have to think more and more about memory, right?
[1431.24s -> 1433.20s]  And so, we're going to keep a lookout on that.
[1433.20s -> 1434.72s]  I'm going to keep emphasizing this.
[1434.72s -> 1439.20s]  It's one of the important themes in GPUs, okay?
[1439.20s -> 1442.80s]  So, you know, I've been kind of throwing lots of GPU facts at you,
[1442.80s -> 1445.84s]  especially if you haven't, you know, seen this recently and maybe,
[1445.84s -> 1447.16s]  I mean, kind of new.
[1447.16s -> 1448.60s]  So, just to recap, right?
[1448.60s -> 1451.88s]  GPUs are these massively parallel processing systems.
[1451.88s -> 1455.72s]  They have same instructions applied across many different threads,
[1455.72s -> 1458.92s]  and they have these things called SMs, which are kind of like cores that,
[1458.92s -> 1461.84s]  you know, there's many, many of them in the GPUs.
[1461.84s -> 1464.88s]  Compute and matrix multiplies have scaled really fast,
[1464.88s -> 1466.80s]  and they have scaled faster than memory.
[1466.80s -> 1471.32s]  And that is an important part of the characteristics that you think about GPUs.
[1471.32s -> 1473.08s]  But there is some fast memory, right?
[1473.08s -> 1475.76s]  It's not like everything is slow, so there's nothing we can do.
[1475.76s -> 1477.40s]  There is the memory hierarchy, right?
[1477.40s -> 1479.60s]  So, some kinds of memory are very, very fast.
[1479.60s -> 1481.28s]  Other kinds of memories are slow.
[1481.28s -> 1484.48s]  And so, if we exploit this hierarchy, maybe we can get things that are really,
[1484.48s -> 1485.48s]  really fast, right?
[1485.48s -> 1488.40s]  So, that's kind of things to remember about the GPU.
[1488.40s -> 1491.60s]  And if you remember these facts, you know, you're going to be able to think
[1491.60s -> 1495.28s]  pretty cleanly about the performance components that I'm going to talk about next.
[1495.28s -> 1500.08s]  Any questions before I move on to the next part?
[1500.08s -> 1503.80s]  Okay, cool.
[1503.80s -> 1508.00s]  So, now you all are GPU experts, and what we would like to do is we would like
[1508.00s -> 1512.84s]  to make machine learning workloads go very fast on a GPU.
[1512.84s -> 1516.04s]  And so, I'm going to start with this chart, and one of our goals will be
[1516.04s -> 1518.16s]  to understand what this chart exactly is.
[1518.16s -> 1520.88s]  I think it'll be a good puzzle to get us motivated.
[1520.88s -> 1526.36s]  And so, here, what we are doing is we are multiplying square matrices together, right?
[1526.36s -> 1530.28s]  So, the x-axis is the size of my square matrix multiplies.
[1530.28s -> 1535.52s]  And, you know, the y-axis here, this is the number of operations per second that I'm doing.
[1535.52s -> 1540.56s]  So, you can kind of think of this as hardware utilization on the y-axis, right?
[1540.56s -> 1544.08s]  And so, as I get bigger and bigger matrices, I'm going to get better and better hardware
[1544.08s -> 1548.48s]  utilization because, you know, I have more work to do, so I don't, you know, that
[1548.48s -> 1553.44s]  overwhelms the overhead of sort of, you know, launching jobs and things like this.
[1553.44s -> 1555.72s]  But there's all these weird things that are happening, right?
[1555.72s -> 1560.20s]  You see one, two, three different, four different lines, right?
[1560.20s -> 1564.72s]  And each of these lines are kind of wavy in a way that's kind of, you know, looks very
[1564.72s -> 1565.72s]  unpredictable, right?
[1565.72s -> 1570.32s]  And so, we would like to kind of understand what exactly is going on with these lines.
[1570.32s -> 1574.64s]  And by the end of this section, my promise is that you will kind of understand exactly
[1574.64s -> 1575.64s]  each one of these phenomenon.
[1575.64s -> 1578.24s]  You'll be able to say, yeah, that plot looks totally normal.
[1578.24s -> 1581.28s]  That is a natural thing for a GPU to do.
[1581.28s -> 1586.56s]  Okay, so the very first part, right, is if you look at that plot, you will notice
[1586.56s -> 1589.04s]  that it looks a little bit like this, right?
[1589.04s -> 1592.52s]  And if you've taken a systems hardware course, you know, you should remember this as kind
[1592.52s -> 1594.60s]  of the roof line model.
[1594.60s -> 1599.80s]  The roof line model basically says if we're looking at, you know, throughput or utilization,
[1599.80s -> 1603.04s]  you know, what we're going to find is, you know, there's two regimes.
[1603.04s -> 1607.04s]  There's going to be a regime that is sort of memory limited, right, that is on the
[1607.04s -> 1609.96s]  left side of this curve on the green over here.
[1609.96s -> 1612.80s]  And then there's a part that is throughput limited on the right side.
[1612.80s -> 1616.68s]  In some sense, you can kind of think of it as on the right side, we have, we are fully
[1616.68s -> 1618.36s]  utilizing our compute units.
[1618.40s -> 1621.28s]  All the matrix multiply units are multiplying all the time.
[1621.28s -> 1626.08s]  And on the diagonal here, we just have some sort of memory bottleneck.
[1626.08s -> 1632.00s]  And so, our ability to do computation is limited by kind of the amount of sort of
[1632.00s -> 1635.00s]  intensity that we have, the amount of flops per byte that we have, right?
[1635.00s -> 1639.08s]  So, we want to avoid being in this left side region where we're memory bound.
[1639.08s -> 1641.56s]  And we would like to be on this right side where we're getting, in some sense,
[1641.56s -> 1644.64s]  full utilization of all of our compute units.
[1644.64s -> 1646.64s]  So, that's, in some sense, the goal.
[1646.64s -> 1649.92s]  And hopefully, this roofline model looks something like this, right?
[1649.92s -> 1652.92s]  Like, we've got sort of this diagonal part, and then we've got this flat part
[1652.92s -> 1653.92s]  all the way at the top here.
[1653.92s -> 1656.48s]  So, that's one part of the mystery.
[1658.92s -> 1663.24s]  And so, this is, this turns out to be kind of complex, right?
[1663.24s -> 1668.84s]  The simple way to say this is let's make sure that we're not accessing memory unnecessarily, right?
[1668.84s -> 1672.80s]  We have as few memory accesses to slow global memory as possible.
[1672.80s -> 1677.36s]  But it turns out that in order to do that, we need a large array of tricks.
[1677.36s -> 1680.76s]  There's a lot of different things that you could do that would mess you up,
[1680.76s -> 1682.76s]  that would make you very slow.
[1682.76s -> 1684.32s]  And the first one's not a memory bottleneck.
[1684.32s -> 1685.52s]  I'll just mention it.
[1685.52s -> 1686.64s]  It doesn't come up too often.
[1686.64s -> 1690.24s]  We'll get it out of the way, and then we'll talk about the remaining five items
[1690.24s -> 1694.00s]  that, in some sense, are really core to thinking about GPU performance.
[1696.92s -> 1701.84s]  Okay, so the first thing that I want to talk about is conditionals.
[1701.88s -> 1706.00s]  So, as I said before, GPUs, their execution model is something called SIMP, right?
[1706.00s -> 1708.48s]  Single instruction multi-thread.
[1708.48s -> 1713.20s]  And so, every thread in a warp is going to execute the same instruction,
[1713.20s -> 1715.32s]  and it's going to do so on different data.
[1715.32s -> 1718.88s]  And so, what happens if I write a piece of code that looks like this?
[1718.88s -> 1723.24s]  I have an if statement, and if, you know, the thread index is less than four, do something.
[1723.24s -> 1726.16s]  If the thread index is greater than or equal to four, then do something else, right?
[1726.16s -> 1728.80s]  I have this very simple conditional model.
[1728.80s -> 1736.68s]  If I run this on the GPU, what's going to happen is that I'm going to run the A instruction
[1736.68s -> 1738.12s]  on four of my threads.
[1738.12s -> 1743.20s]  I will actually pause my other four threads, which are supposed to be executing the else part,
[1743.20s -> 1746.24s]  and then these other four threads will come alive, and they will execute X,
[1746.24s -> 1749.20s]  and my original four threads will go to sleep.
[1749.20s -> 1752.24s]  And I will just alternate executing each of these instructions.
[1752.24s -> 1753.24s]  Why is that?
[1753.24s -> 1758.08s]  I can't execute A and X at the same time on these different threads, right?
[1758.08s -> 1761.44s]  As I said, again, every thread has to execute the same instruction.
[1761.44s -> 1766.36s]  So, conditional statements within a single warp can be really, really damaging,
[1766.36s -> 1770.88s]  because they will force you to pause any of the threads that are not doing exactly
[1770.88s -> 1776.48s]  the main sort of control flow execution.
[1776.48s -> 1779.72s]  Okay, so that was the only non-memory thing that I wanted to mention.
[1779.72s -> 1783.64s]  And it should be kind of obvious that you should probably not be putting conditionals
[1783.64s -> 1787.24s]  into sort of your massively parallel compute unit.
[1787.24s -> 1790.32s]  But once we've gotten that out of the way, sort of the other tricks that we need
[1790.32s -> 1793.52s]  to consider are all kind of memory-based.
[1793.52s -> 1796.84s]  The first thing I want to sort of mention is lower precision.
[1796.84s -> 1798.00s]  And this is a big trick.
[1798.00s -> 1799.04s]  This is an important trick.
[1799.04s -> 1801.12s]  You should do it all the time.
[1801.12s -> 1807.00s]  There's kind of a...going back to this plot of Bill Dally, there's a sleight of hand here.
[1807.00s -> 1811.04s]  This looks really good, because the numbers are going up and up and up.
[1811.04s -> 1816.72s]  But if you look at, you know, what's driving GPU progress over all these years,
[1816.72s -> 1819.64s]  you actually kind of see that it's number representations.
[1819.64s -> 1826.04s]  You go from FP32 to FP16 to INT8 to so on, you get many orders of magnitude gains
[1826.04s -> 1830.08s]  from just having lower and lower precision in your GPU operations.
[1830.08s -> 1833.88s]  And let me sort of clarify why that's so important, right?
[1833.88s -> 1837.96s]  If you have fewer bits in all the things that you're computing and your weights and so on,
[1837.96s -> 1839.72s]  you have much fewer bits to move, right?
[1839.72s -> 1843.64s]  So even if you're accessing these bits from global memory, they become much,
[1843.64s -> 1845.68s]  much less of a concern.
[1845.68s -> 1849.52s]  So let's just give a simple example, and let's just think about kind of arithmetic
[1849.52s -> 1852.32s]  intensity of a simple element-wise operation, right?
[1852.32s -> 1855.96s]  So I'm going to do it in ReLU, so that's X equals max 0 and X.
[1855.96s -> 1858.32s]  And I'm going to do that on a vector of size N.
[1858.32s -> 1861.12s]  Let's say naively, I'm going to do this on float 32, right?
[1861.12s -> 1863.72s]  So how many memory accesses do I have?
[1863.72s -> 1865.24s]  I have to read my X.
[1865.24s -> 1870.24s]  I have to write the result of if X less than 0, and that's all in float 32.
[1870.24s -> 1872.44s]  So that's kind of 8 bytes, right?
[1872.44s -> 1873.68s]  And how many operations do I do?
[1873.72s -> 1877.32s]  Well, I have to do X less than 0, so that's one comparison operation.
[1877.32s -> 1878.52s]  And I do one flop, right?
[1878.52s -> 1882.84s]  So I do 8 bytes per single floating point operation.
[1882.84s -> 1887.20s]  If I do this in float 16 now, well, you know, I haven't changed the flops intensity here,
[1887.20s -> 1891.32s]  but I have the memory access, and so now I have 4 bytes per flop, right?
[1891.32s -> 1894.60s]  In some sense, I've like gotten double the memory bandwidth for free,
[1894.60s -> 1898.40s]  assuming that I can get away with flop 16.
[1898.40s -> 1901.40s]  And this is a key part of how a lot of things are designed.
[1901.40s -> 1906.76s]  Part of the assignment is going to be you're going to try and play with various like mixed
[1906.76s -> 1911.44s]  precision or low precision training and other kinds of things.
[1911.44s -> 1916.00s]  And a key part here is that not all the parts of your network and your training
[1916.00s -> 1918.04s]  algorithm should be put into low precision, right?
[1918.04s -> 1921.92s]  So let me give you an example of matrix multiplies.
[1921.92s -> 1926.04s]  So in matrix multiplies that are mixed precision, what you would do is you would have,
[1926.04s -> 1930.08s]  you know, your inputs be 16 bits, so these are low precision.
[1930.08s -> 1934.36s]  And then you're going to do your multiplication in full 32 bit, right?
[1934.36s -> 1937.72s]  And that's useful because the intermediate computations, as you're like accumulating
[1937.72s -> 1940.56s]  partial sums, you would like that to be in high precision.
[1940.56s -> 1945.76s]  And so you're accumulating this with the FP32 accumulator.
[1945.76s -> 1950.80s]  And then, you know, your tensor core will return an FP32 result, which you can,
[1950.80s -> 1955.00s]  you know, downcast, if you would like, back into 16 bit, right?
[1955.00s -> 1959.44s]  And so we have our inputs in 16 bit, but things like the accumulation we might want
[1959.48s -> 1961.88s]  to do in 32, right?
[1961.88s -> 1963.44s]  So there's lots of different things.
[1963.44s -> 1966.08s]  There's operations that can use 16 bit storage.
[1966.08s -> 1968.08s]  There's operations that might need more precision.
[1968.08s -> 1972.52s]  So you want to keep it in like either FP32 or FP16.
[1972.52s -> 1976.08s]  You might want to have operations that need more range, like X functions.
[1976.08s -> 1980.16s]  If you don't have sort of the dynamic range, they might blow up or zero out.
[1980.16s -> 1982.24s]  And so you might want to put those in VF16.
[1982.24s -> 1987.00s]  There's a lot of sort of careful engineering that has to happen in order to make sure that,
[1987.00s -> 1991.84s]  you know, these models are actually stable when they're being trained with lower precision.
[1991.84s -> 1995.72s]  But if you can do it, that's really great because you've basically doubled the throughput
[1995.72s -> 2004.16s]  of your bottleneck going from 32 to 16 bit, right, if your memory is your bottleneck.
[2004.16s -> 2005.44s]  Okay.
[2005.44s -> 2009.24s]  The other one, and I think this is kind of what a lot of people think of when they say,
[2009.24s -> 2011.96s]  like, I'm going to write a CUDA kernel or something.
[2012.00s -> 2018.28s]  Operator fusion is kind of both very intuitive and both like a fun, natural one to think about.
[2018.28s -> 2023.96s]  So one memory, or sorry, one mental model of how a GPU works and how memory works is this kind
[2023.96s -> 2027.48s]  of fun diagram of a factory from Horace Heat, right?
[2027.48s -> 2031.36s]  So imagine you have a factory, and your factory is your compute part, right?
[2031.36s -> 2037.12s]  And so, you know, it takes in little box widgets and then outputs little triangle widgets.
[2037.64s -> 2042.40s]  And if you grow your compute, but your bell conveyor, you know, that takes memory to compute
[2042.40s -> 2046.48s]  is, you know, finite bandwidth, you know, you're not going to be able to use your second factory, right?
[2046.48s -> 2051.52s]  Like, you're still capped by the speed at which you can transfer things from memory to compute.
[2051.52s -> 2053.96s]  And so, you've got this bottleneck.
[2053.96s -> 2055.40s]  Now, of course, you already knew that, right?
[2055.40s -> 2058.60s]  I've been sort of hammering in the memory bottleneck thing.
[2058.60s -> 2063.04s]  But I think one insidious way in which you can incur a ton of overhead without really
[2063.04s -> 2066.80s]  realizing it is kind of this left-hand side computation pattern, right?
[2066.80s -> 2071.28s]  So, you know, imagine the left side of this plot is where the memory is.
[2071.28s -> 2073.24s]  The right side is your compute unit.
[2073.24s -> 2078.48s]  And so, to do computation, I start with a square, and I move my squares from my memory to my compute.
[2078.48s -> 2079.24s]  I do some operation.
[2079.24s -> 2080.80s]  I turn them into triangles, right?
[2080.80s -> 2082.92s]  Now, I ship my triangles back to memory.
[2082.92s -> 2085.32s]  And then, you know, okay, I realize I need the triangles again.
[2085.32s -> 2087.00s]  So, I ship them back into the compute unit.
[2087.00s -> 2089.72s]  Now, the triangles become circles, and then so on and so forth, right?
[2089.72s -> 2093.72s]  I send my compute sort of back and forth and back and forth back to memory.
[2093.72s -> 2096.36s]  And you might call this kind of a very naive approach.
[2096.36s -> 2100.92s]  And if you were just doing operations naively on the GPU and just shipping the results straight
[2100.92s -> 2103.76s]  back to global memory, this is what you'd end up with, right?
[2103.76s -> 2107.68s]  And if you count the number of times a piece of data went back and forth, this is pretty terrible.
[2107.68s -> 2110.04s]  You've incurred tons of memory overhead.
[2110.04s -> 2114.56s]  Now, you should be able to realize that if you look at the right side, well, this compute,
[2114.56s -> 2115.84s]  well, there's no dependencies.
[2115.84s -> 2121.48s]  I should be able to go square to triangle to circle to rectangle and ship the rectangle back, right?
[2121.48s -> 2124.44s]  I can just keep everything in the compute unit the whole time, right?
[2124.44s -> 2126.12s]  And that's the right-hand side diagram.
[2126.12s -> 2128.36s]  And this is the mental model of a fused kernel, right?
[2128.36s -> 2132.96s]  You have a bunch of operations that are going to happen on a piece of data in sequence.
[2132.96s -> 2136.52s]  Instead of writing it back into storage, what I'm going to do is I'm going to do all
[2136.52s -> 2140.40s]  the computation as much as I can in one place, and then only when I have to,
[2140.40s -> 2142.08s]  ship it back to memory, right?
[2142.08s -> 2147.56s]  So, that's this idea of kernel fusion.
[2147.56s -> 2154.36s]  Okay, there's some very simple examples of how if you write some naive code, you might,
[2154.36s -> 2156.36s]  you know, get sort of a naive set of launches.
[2156.36s -> 2157.64s]  So, here's an example.
[2157.64s -> 2161.04s]  I wrote a little, let's say, neural network module.
[2161.04s -> 2165.36s]  You know, let's say I write a neural network module that takes in x,
[2165.36s -> 2168.84s]  and it produces sine squared x and cosine squared x, right?
[2168.84s -> 2169.76s]  Simple code.
[2169.76s -> 2173.84s]  Now, if I run this, you know, the computation graph in PyTorch is going
[2173.84s -> 2176.16s]  to look something like this, and it's going to, you know,
[2176.16s -> 2177.84s]  launch a whole bunch of CUDA kernels.
[2177.84s -> 2182.88s]  It's going to launch, take in the x, and it'll launch a CUDA kernel to compute sine x.
[2182.88s -> 2185.52s]  It'll launch one to compute cosine x, then sine squared of x,
[2185.52s -> 2188.76s]  then cosine squared of x, and sine squared x plus cosine squared of x, right?
[2188.76s -> 2192.56s]  So, there's a bunch of back and forth that has to happen in order to do this computation.
[2192.56s -> 2195.52s]  It's exactly the left-hand side figure that I showed you before.
[2195.52s -> 2202.24s]  But, if you were a little smarter, right, and you either wrote your own CUDA kernel
[2202.24s -> 2206.48s]  or you used something like Torch Compile, well, you can easily realize
[2206.48s -> 2210.16s]  that those five operations don't really depend on very much.
[2210.16s -> 2215.16s]  Like, they use only a little bit of memory, and so you can fuse them into a single operation
[2215.16s -> 2220.40s]  that does everything on GPU on a single thread without sending things back to global memory, right?
[2220.40s -> 2225.08s]  So, really easy fusion operations like this can be done automatically by compilers.
[2225.08s -> 2227.52s]  I just mentioned Torch Compile.
[2227.52s -> 2230.56s]  If you aren't already doing this, you know, you should consider strongly thinking
[2230.56s -> 2232.72s]  about using Torch Compile everywhere.
[2232.72s -> 2236.12s]  We'll show you in the assignment Torch Compile as well.
[2236.12s -> 2238.84s]  It's pretty nice.
[2238.88s -> 2240.72s]  Okay.
[2240.72s -> 2244.72s]  So, I've gone through precision and fusion.
[2244.72s -> 2249.28s]  If anyone has questions, let me know before I move on to recomputation
[2249.28s -> 2253.68s]  and other kinds of tricks that we can do on the GPU.
[2253.68s -> 2255.72s]  Okay, good.
[2255.72s -> 2259.36s]  So, another thing that we can do is called recomputation.
[2259.36s -> 2268.00s]  And recomputation is this idea of sort of spending more compute to avoid having to do memory access, right?
[2268.00s -> 2271.76s]  So, remember that your, you know, original back propagation lecture.
[2271.76s -> 2274.16s]  This one's actually from CS221.
[2274.16s -> 2274.72s]  What do we do?
[2274.72s -> 2276.92s]  Well, we take our inputs at the very bottom.
[2276.92s -> 2278.24s]  These are the yellow ones.
[2278.24s -> 2280.52s]  And then we propagate activations upwards.
[2280.52s -> 2283.40s]  Those are also the yellow values on the tree.
[2283.40s -> 2285.44s]  And then we compute the Jacobians backwards.
[2285.44s -> 2287.84s]  Those are the green values on the edges.
[2287.84s -> 2290.12s]  And then to compute my gradients, I'm going to propagate.
[2290.12s -> 2291.44s]  You multiply.
[2291.44s -> 2297.04s]  So, the Jacobian and the activations, I'm going to propagate the gradients backward, right?
[2297.04s -> 2302.60s]  Well, if you think about it, those yellow values after the forward pass have to be stored, right?
[2302.60s -> 2306.52s]  And then they're stored and then they have to be taken from global memory where I stored them
[2306.52s -> 2308.00s]  and put them into the compute unit, right?
[2308.00s -> 2310.36s]  Mechanically, that's how it has to happen.
[2310.36s -> 2316.16s]  But that might actually be a ton of sort of memory inputs and outputs happening.
[2316.16s -> 2319.28s]  Instead, you might actually be able to avoid this.
[2319.28s -> 2323.76s]  So, let me give you an example of how recomputation can speed things up.
[2323.76s -> 2327.28s]  Here's another sort of silly function that I might write.
[2327.28s -> 2330.24s]  I'm just going to stack three sigmoids on top of each other, right?
[2330.24s -> 2331.00s]  You can look at the left.
[2331.00s -> 2332.28s]  That's the forward graph.
[2332.28s -> 2337.40s]  That should be exactly, you know, your mental model of three sigmoids on top of each other.
[2337.40s -> 2340.92s]  Now, you know, the computation and graph for this, I'm going to compute the sigmoids,
[2340.92s -> 2344.40s]  and I'm going to store S1 and S2, which are the activations of the sigmoids.
[2344.40s -> 2346.16s]  And I have my outputs.
[2346.16s -> 2349.72s]  And then, you know, that's my sort of forward pass.
[2349.72s -> 2352.16s]  Now, the backward pass in this is kind of terrible.
[2352.16s -> 2357.40s]  When I do my backward graph, I need to go and take S1 and S2, and I need to take, you know,
[2357.40s -> 2362.32s]  the gradients coming sort of backwards into this out box, and then push it into this,
[2362.32s -> 2365.68s]  you know, backwards computation, and I'll get the gradient of X, right?
[2365.68s -> 2371.12s]  So, I need to have three memory reads, one memory write in order to compute the backwards pass.
[2371.12s -> 2374.88s]  And then, for the forward pass, I need to do one memory read of X, and I need to do three
[2374.88s -> 2377.56s]  memory writes for S1, S2, and out, right?
[2377.56s -> 2378.84s]  So, hopefully, that's clear.
[2378.84s -> 2381.84s]  This is, you know, a decent amount of memory reads and writes.
[2381.84s -> 2385.44s]  I have to do eight of them, and I have very low arithmetic intensity because I have no
[2385.44s -> 2389.12s]  matrix multiplies at all.
[2389.12s -> 2393.52s]  So the idea of recomputation is to say, I don't want to store those activations
[2393.52s -> 2394.52s]  at all, right?
[2394.52s -> 2395.88s]  Like, I'm not going to put them into memory.
[2395.88s -> 2399.64s]  I'm just going to recompute them on the fly in my backward pass, right?
[2399.64s -> 2403.08s]  So now, in my new forward pass, I don't store S1 and S2.
[2403.08s -> 2406.92s]  I take X as input, I compute my sigmoids, and I get my output, right?
[2406.92s -> 2410.80s]  So now, that's one memory read for X, one memory write for out, right?
[2411.40s -> 2414.72s]  Now, in my backward pass, right, I don't have activations anymore.
[2414.72s -> 2418.80s]  So what I'm going to do is I'm going to get both D out, which is, you know, the backward
[2418.80s -> 2422.36s]  signal coming in from above, and then X, which is my input, right?
[2422.36s -> 2427.36s]  So I'm going to take two of those, which is two memory reads, and then sort of on the fly,
[2427.36s -> 2431.92s]  in my SM, in my local memory, I'm going to compute each of these sigmoids, and I'm going
[2431.92s -> 2433.48s]  to put them into the backward graph, right?
[2433.48s -> 2439.56s]  I'm going to recompute S1, S2, and out on the fly inside sort of my local memory, and because I do
[2439.56s -> 2444.68s]  that, there's no global memory reads happening here, and then I have one memory write, which
[2444.68s -> 2445.52s]  is DX, right?
[2445.52s -> 2450.60s]  So now, if you compare the two, I have 5 eighths of the memory access for the exact
[2450.60s -> 2451.68s]  same computation, right?
[2451.68s -> 2455.92s]  The price that we paid is that I'm going to have to recompute these three sigmoids,
[2455.92s -> 2460.68s]  but if you were running sort of idle anyway because you were memory capped, this is a
[2460.68s -> 2461.60s]  great tradeoff, right?
[2461.60s -> 2464.40s]  Like, you would be very happy with this because now you've traded compute, which you
[2464.40s -> 2468.24s]  have too much of, for memory bandwidth, which you had too little of, right?
[2468.24s -> 2473.92s]  So this is one great way of trading one thing you need for another thing that you have.
[2473.92s -> 2477.56s]  And of course, this is different.
[2477.56s -> 2482.36s]  It's the same trick as sort of gradient checkpointing and re-computing activations
[2482.36s -> 2485.64s]  for memory savings, but this is being done for a different reason.
[2485.64s -> 2490.04s]  This is for sort of execution speed, not just because you're running out of memory, right?
[2490.04s -> 2495.24s]  So it's the same technique, but for different goals.
[2495.28s -> 2501.48s]  Okay, and then this one, I think, is actually kind of a really interesting one and not one that I knew
[2501.48s -> 2507.64s]  until I started sort of really looking into how the hardware model of a GPU and DRAM works.
[2507.64s -> 2515.88s]  So the slow memory, the global memory called DRAM in a GPU, that's actually very, very slow.
[2515.88s -> 2520.48s]  And in order to make it faster, there's certain optimizations that are being done at the
[2520.48s -> 2521.80s]  hardware level.
[2521.80s -> 2526.36s]  And one of the optimizations that's done at a hardware level for DRAM is that when you go
[2526.36s -> 2530.40s]  and read a piece of memory, you don't actually get just that value back.
[2530.40s -> 2534.80s]  You actually get a whole chunk of the memory back, and this is called burst mode.
[2534.80s -> 2541.00s]  So let's say I went on and tried to read the very first value of this big memory block, right?
[2541.00s -> 2545.96s]  Instead of just the memory giving me back zero, it would actually give me back zero, one, two, three, right?
[2545.96s -> 2547.56s]  It would give me back four values at once.
[2547.56s -> 2548.56s]  It'll be like, here you go.
[2548.76s -> 2552.16s]  I'm sure you'll need the one, two, and three, too in the future.
[2552.16s -> 2556.56s]  And so each address space is cut up into what's called burst sections, and then you're given the
[2556.56s -> 2560.64s]  entire burst section rather than just what you looked for.
[2560.64s -> 2565.28s]  And this might seem very mystifying, like why would the memory give you three extra,
[2565.28s -> 2569.16s]  you know, bytes for free when you're just asking for one?
[2569.16s -> 2573.04s]  There's sort of like a very interesting hardware reason, which is that when you're addressing
[2573.04s -> 2576.72s]  into the memory, you know, in order to send the signal out from the memory,
[2576.72s -> 2579.20s]  that those bytes have to be moved to an amplifier.
[2579.20s -> 2583.24s]  That's the slow step, and once you've done that, you can get many, many bytes for free.
[2583.24s -> 2585.72s]  And so that's why sort of this burst section thing exists.
[2585.72s -> 2590.92s]  It's kind of masking this more expensive step of actually moving where the data is stored
[2590.92s -> 2594.12s]  to this amplifier.
[2594.12s -> 2599.56s]  But kind of regardless, this kind of means that we might be able to significantly accelerate sort
[2599.56s -> 2603.64s]  of our memory access if the pattern of memory access is good, right?
[2603.72s -> 2609.08s]  So if I want to read this entire, you know, block over here, if I access it in random
[2609.08s -> 2614.12s]  order, right, then I'm going to have to, you know, basically query a number of times
[2614.12s -> 2618.32s]  equal roughly to the length of my query, right?
[2618.32s -> 2622.72s]  But if I sort of go and I check the very first value, then I'm going to get all this
[2622.72s -> 2624.36s]  entire burst section at once.
[2624.36s -> 2627.32s]  And then if I go and check number four, I'll get this burst section, the second burst
[2627.32s -> 2628.64s]  section at once.
[2628.64s -> 2633.80s]  And so I can, you know, basically get four times the throughput if I'm really clever
[2633.80s -> 2640.08s]  about my memory accesses and only access just the bits I need from each burst section.
[2640.08s -> 2642.24s]  So this is called memory coalescing.
[2642.24s -> 2648.88s]  So if all the threads in a warp fall within the same burst, then basically the sort
[2648.88s -> 2652.52s]  of smart hardware and programming model will basically group those queries.
[2652.52s -> 2656.00s]  Instead of querying zero, one, two, three, it will group them and say, just give me
[2656.04s -> 2660.32s]  zero, and then I will be able to read out all the zero, one, two, three at once from
[2660.32s -> 2662.96s]  this kind of burst mode DRAM, right?
[2662.96s -> 2667.28s]  So remember that, you know, a warp is 32 sort of numbered threads, and so memory
[2667.28s -> 2669.36s]  accesses from a warp happen together.
[2669.36s -> 2672.96s]  And so when these warps are reading in to these kind of burst sections, there's
[2672.96s -> 2677.36s]  optimizations that can be done so that you're getting all four bytes at once rather
[2677.36s -> 2679.12s]  than getting one of them at a time individually.
[2679.12s -> 2683.68s]  And so that will 4x the throughput that you have on your memory, right?
[2683.68s -> 2686.72s]  So these are kind of very simple things, but they're actually very important.
[2686.72s -> 2690.32s]  Like, imagine I'm going to do matrix multiplications, right?
[2690.32s -> 2693.76s]  This is a core thing that you're going to have to do a ton if you were to sort
[2693.76s -> 2698.28s]  of implement, let's say, neural network really from scratch in CUDA.
[2698.28s -> 2703.28s]  In this case, imagine I'm going to read my matrices in one of two ways.
[2703.28s -> 2705.84s]  I can read it by traversing the rows, right?
[2705.84s -> 2710.28s]  So each thread is going to traverse the row, or I can sort of read it in sort
[2710.28s -> 2711.20s]  of column order.
[2711.24s -> 2714.80s]  So each thread is going to go down a column, right?
[2714.80s -> 2718.08s]  Turns out that this left one, where you're sort of going across different
[2718.08s -> 2722.40s]  rows, so each thread is accessing a different, sorry, each thread is going
[2722.40s -> 2727.84s]  through columns, this left model is going to be quite slow because the memory
[2727.84s -> 2730.28s]  reads are not going to be coalesced.
[2730.28s -> 2733.60s]  Whereas if you're going to this right side where each of the threads are going
[2733.60s -> 2739.32s]  down, so they're incrementing in rows, then these memory reads will be coalesced.
[2739.32s -> 2742.56s]  And so, you know, you can think about it for a moment why this is true.
[2742.56s -> 2744.92s]  When I first looked at this diagram, I was like, isn't it reversed?
[2744.92s -> 2745.72s]  It's actually not.
[2745.72s -> 2747.32s]  This is the correct one.
[2747.32s -> 2753.52s]  And the way to think about this, right, is let's say on this right-hand side
[2753.52s -> 2757.52s]  diagram over here, I'm going to have a thread that's trying to, a series of
[2757.52s -> 2760.20s]  threads that's trying to access, you know, left to right.
[2760.20s -> 2763.72s]  So each thread is going to try to load, you know, the very first element,
[2763.72s -> 2768.56s]  and then in the next time step, I'm going to load the element from this
[2768.56s -> 2772.28s]  column, the second column, and then the third column and the fourth column and so on.
[2772.28s -> 2775.60s]  So if that happens, what happens at time step one, right?
[2775.60s -> 2779.32s]  At time step one, my first thread loads this point, and then the second
[2779.32s -> 2782.00s]  thread loads this point, and then this point and that point, right?
[2782.00s -> 2783.56s]  So those can't be coalesced at all.
[2783.56s -> 2785.64s]  They're reading different burst sections.
[2785.64s -> 2790.24s]  And so that means that I have to read this entire chunk of memory in order to
[2790.24s -> 2792.20s]  perform any sort of an operation.
[2792.20s -> 2796.28s]  Instead, if I was sort of going in the column direction, all the threads will be
[2796.32s -> 2800.40s]  reading within the single burst section, and then so only one memory read operation
[2800.40s -> 2803.16s]  needs to be performed, and you get all the memory at once, right?
[2803.16s -> 2806.32s]  This is a very low-level optimization, but this is very important, right?
[2806.32s -> 2810.40s]  If your memory traversal order is all wrong, you will actually get much slower
[2810.40s -> 2813.48s]  memory accesses than you really want.
[2813.48s -> 2815.84s]  Okay.
[2815.84s -> 2820.76s]  So then that brings us to kind of the very last and kind of big one,
[2820.76s -> 2823.92s]  and this is the idea of piling.
[2824.16s -> 2829.48s]  And piling is this idea that you would like to group together memory accesses in order
[2829.48s -> 2833.68s]  to minimize the amount of global memory access that we have to do.
[2833.68s -> 2838.28s]  And so to explain this one, I'm going to try to go through this example of a matrix
[2838.28s -> 2843.72s]  multiply, and hopefully I'll be able to sort of explain to you why sort of a naive
[2843.72s -> 2847.64s]  algorithm for doing matrix multiply is going to be very problematic.
[2847.64s -> 2851.28s]  And then afterwards, I'm going to give you a tiled version of the same idea,
[2851.32s -> 2854.52s]  and hopefully you'll be able to see why that's going to reduce the number of global
[2854.52s -> 2857.44s]  memory reads that you have to do.
[2857.44s -> 2861.88s]  So let's start with this very simple matrix multiply algorithm.
[2861.88s -> 2866.24s]  So, you know, I've got a matrix, you know, I got this M matrix on the left side,
[2866.24s -> 2870.80s]  I've got my N matrix on the top, and in order to compute, you know,
[2870.80s -> 2874.72s]  the matrix-matrix product, right, I'm going to have to traverse over the
[2874.72s -> 2878.96s]  rows of M and the columns of N and then take the inner product and store that
[2878.96s -> 2882.12s]  into this P matrix, right, the corresponding rows.
[2882.12s -> 2887.16s]  And I've written out here each of the threads, the thread 00, 01, 1011,
[2887.16s -> 2891.12s]  corresponding to where they're sort of storing their outputs and sort of the
[2891.12s -> 2895.04s]  access order in which they access each of the individual elements.
[2895.04s -> 2899.04s]  Now, notice here that, you know, what's going to happen is that the memory
[2899.04s -> 2903.96s]  access here is not coalesced, like the row matrices here,
[2903.96s -> 2906.92s]  these are going to be accessed in a non-coalesced order.
[2906.92s -> 2912.32s]  And I have repeated memory accesses, right, so I've got M00 being accessed
[2912.32s -> 2917.28s]  in the first thread, M00 accessed here, N10 being accessed in two
[2917.28s -> 2921.16s]  different threads, you know, so these values are being kind of read
[2921.16s -> 2925.32s]  over and over from global memory into many different threads.
[2925.32s -> 2928.76s]  So this is going to be potentially very slow.
[2928.76s -> 2933.44s]  So there's a question of, can we avoid having too many global memory
[2933.44s -> 2934.16s]  reads and writes?
[2934.16s -> 2937.28s]  So what I would ideally like to do, right, so let me explain kind of the
[2937.28s -> 2940.40s]  ideal outcome first, and then I'll explain the algorithm.
[2940.40s -> 2944.56s]  The ideal outcome is that I would like to spend one sort of, you know,
[2944.56s -> 2948.96s]  chunk of time loading pieces from global memory to shared memory,
[2948.96s -> 2952.52s]  where things are fast, I want to do a ton of computation in shared memory,
[2952.52s -> 2955.48s]  and then I want to kind of be done with that piece of data, right?
[2955.48s -> 2959.12s]  That's the ideal outcome, I've minimized my global memory accesses.
[2959.12s -> 2963.20s]  So now, how can I do this in this matrix multiply world?
[2963.20s -> 2965.84s]  So now what I'm going to do is I'm going to take my matrices,
[2965.84s -> 2968.60s]  both the M matrix and the N matrix, and I'm going to cut them up, right,
[2968.60s -> 2969.68s]  into tiles.
[2969.68s -> 2972.04s]  So here, I've cut this up into two by two tiles.
[2972.04s -> 2975.48s]  So I've got a two by two M tile and a two by two N tile, right?
[2975.48s -> 2980.08s]  So I've got basically smaller submatrices within each of the matrix.
[2980.08s -> 2983.00s]  And now imagine that my shared memory is big enough to be able to fit these
[2983.00s -> 2986.64s]  submatrices, right, within each of these SMs.
[2986.64s -> 2990.32s]  So now this gives a very, very simple algorithm with which we can
[2990.32s -> 2991.60s]  do computation.
[2991.60s -> 2995.44s]  So what I'm going to do is I'm going to first load, you know,
[2995.44s -> 2999.56s]  let's say this M00 tile on the top left over here, and I'm going to also
[2999.56s -> 3004.64s]  load my N00 tile into shared memory here, right?
[3004.64s -> 3007.84s]  So now I have these partial sums that I can compute.
[3007.84s -> 3014.32s]  I can take, you know, the row product of M00M01 with N00N10,
[3014.32s -> 3016.32s]  and I can increment that into P00.
[3016.32s -> 3019.92s]  I can do the same with all the different submatrices that I can fill
[3019.92s -> 3021.56s]  out over here, right?
[3021.56s -> 3025.72s]  Now, then once I'm completely done sort of processing these two tiles,
[3025.72s -> 3029.04s]  then I can load a new tile over here, and then I can repeat that
[3029.04s -> 3033.64s]  computation with my M tile and my N2.0 tile loaded into shared memory.
[3033.64s -> 3036.56s]  And then I can sort of increment my partial sums in P, right?
[3036.56s -> 3040.68s]  So now I've really sort of consolidated and reduced the amount of global memory
[3040.68s -> 3042.08s]  access I have to do, right?
[3042.08s -> 3045.80s]  I load as much memory as I can at once into shared memory.
[3045.80s -> 3050.16s]  I do all of my sort of submatrix computations on that tile that I can,
[3050.16s -> 3052.72s]  and then I move on to the next one, right?
[3052.72s -> 3057.28s]  And, of course, the other nice thing is that because I'm loading an entire tile,
[3057.28s -> 3060.92s]  you know, I can traverse these submatrices in whatever order I want,
[3060.92s -> 3064.20s]  like column measure or row measure, and so I can coalesce all the memory
[3064.20s -> 3067.76s]  accesses whenever I'm loading a tile from global to shared memory, right?
[3067.76s -> 3074.08s]  So there's kind of wins all around here when we tile our accesses.
[3074.08s -> 3077.68s]  So we can do a little bit of tiling math.
[3077.68s -> 3081.68s]  So we've got, let's say, a matrix A, a matrix B, and a matrix C.
[3081.68s -> 3085.40s]  So let's say the full matrix Cs, these are square matrices, are of size N.
[3085.40s -> 3088.32s]  And let's say I have a pile of size T, right?
[3088.32s -> 3089.60s]  Oh, yes, question.
[3089.60s -> 3098.64s]  In this slide of load out to zero and sub-3, we're loading N00 again.
[3098.64s -> 3101.12s]  Oh, so in that case, I just wrote it for completeness.
[3101.12s -> 3104.40s]  But N00, let's say, is just, you know, stored in shared memory.
[3104.40s -> 3105.72s]  Let's just keep it cached.
[3105.76s -> 3107.68s]  I won't load it again.
[3107.68s -> 3110.52s]  That's definitely just there for completeness, not that you would actually
[3110.52s -> 3113.60s]  like discard and reload the matrix again.
[3113.60s -> 3116.00s]  That would be kind of insane.
[3116.00s -> 3117.24s]  Cool.
[3117.24s -> 3121.76s]  Okay, and so we can kind of do very simple tiling math to think about,
[3121.76s -> 3122.80s]  you know, what's happening.
[3122.80s -> 3126.80s]  So let's say I'm going to do a N by N matrix multiply, right?
[3126.80s -> 3130.20s]  So if I do a non-tiled matrix multiply, if I'm just going over rows and
[3130.20s -> 3134.60s]  columns, then every input, every time I process it, has to come from global memory.
[3134.60s -> 3137.88s]  So each input is read sort of N times from global memory, right?
[3137.88s -> 3141.36s]  So each of these is read sort of N times.
[3141.36s -> 3146.52s]  If I do a tiled matrix multiply, well, you know, the global reads are operating
[3146.52s -> 3151.08s]  over tiles, so I'm reading each input N over T times from global memory,
[3151.08s -> 3153.36s]  and I'm reading T times within each tile, right?
[3153.36s -> 3156.96s]  Of course, I'm doing matrix-matrix multiplies, so I can't reduce the total
[3156.96s -> 3157.76s]  number of reads.
[3157.76s -> 3163.00s]  I have to read all the matrix elements, but I can shift the reads into basically
[3163.00s -> 3164.24s]  fast shared memory, right?
[3164.24s -> 3168.88s]  So I do T times memory reads into shared memory and N over T times
[3168.88s -> 3170.00s]  from global memory.
[3170.00s -> 3173.12s]  And that's great because if we have a big shared memory that can store big
[3173.12s -> 3177.12s]  tiles, that's a factor of T reduction in the total amount of data that has to
[3177.12s -> 3178.76s]  come from global memory, right?
[3178.76s -> 3182.68s]  So tiling can be really, really powerful of an idea when you're operating
[3182.68s -> 3187.80s]  over matrices and you can move things into shared memory.
[3187.80s -> 3190.24s]  Tiling is quite complex.
[3190.28s -> 3195.96s]  This is the source of many, many sort of confusing things about GPU and matrix
[3195.96s -> 3197.84s]  multiply performance.
[3197.84s -> 3200.92s]  One thing that can happen, right, once we start tiling things,
[3200.92s -> 3204.72s]  you start asking things about discretization, right?
[3204.72s -> 3207.00s]  So imagine I have a tile size of 128.
[3207.00s -> 3210.84s]  That seems like a nice, good, round tile size.
[3210.84s -> 3215.12s]  But then, you know, when I have a full matrix of 256 size, that's great.
[3215.12s -> 3216.60s]  That's a two-by-two tile.
[3216.60s -> 3218.40s]  Things load nicely.
[3218.40s -> 3223.20s]  Now, let's say I have a 257 size tile on the column side.
[3223.20s -> 3227.84s]  Now this is a bad time because I need to have six tiles in order to cover this
[3227.84s -> 3231.88s]  matrix and the two tiles on the right are very, very sparse.
[3231.88s -> 3234.08s]  There's just not much stuff in there, right?
[3234.08s -> 3239.36s]  And the problem with this is that each tile is going to be assigned to SM, right?
[3239.36s -> 3243.68s]  So each of these tiles is going to be a block and each thread is going to be
[3243.68s -> 3245.24s]  operating within each tile.
[3245.24s -> 3249.04s]  So those two tiles on the right, they're not going to be doing very much at all, right?
[3249.04s -> 3251.36s]  Those SMs are going to be basically be sitting idle.
[3251.36s -> 3254.28s]  And if you were kind of compute cap, you would have wanted to more evenly
[3254.28s -> 3256.88s]  distribute the load between SMs, right?
[3256.88s -> 3262.44s]  So you have to basically optimize your tile sizes to try to avoid these kinds of scenarios.
[3262.44s -> 3268.16s]  But in reality, right, there's a lot of complex things that go into setting the tile size, right?
[3268.16s -> 3270.16s]  Remember, you have to coalesce your memory accesses.
[3270.16s -> 3271.56s]  You have to think carefully about that.
[3271.56s -> 3276.20s]  You have to not exceed your shared memory size, right?
[3276.20s -> 3278.04s]  So the tiles can't be too big.
[3278.04s -> 3283.00s]  And you have to divide the matrix dimension hopefully evenly or as close to evenly as possible so you
[3283.00s -> 3289.88s]  don't end up with this situation of sort of an underutilized SM at the very end here.
[3289.88s -> 3290.32s]  Yes?
[3290.64s -> 3301.92s]  So if you have, say, smaller sizes, like, would GPUs do something like revetting, where they can, like, fetch the next tile beforehand?
[3301.92s -> 3307.36s]  And if so, like, would that happen at some level?
[3307.36s -> 3313.84s]  Yeah, so you're asking about whether or not you can, like, overlap memory reads and computation.
[3313.84s -> 3317.08s]  And yeah, that's naturally done in GPUs.
[3317.08s -> 3320.24s]  Like, they're always, like, trying to use the available bandwidth.
[3320.24s -> 3323.84s]  Like, as long as shared memory is available, they can go and put things into it.
[3323.84s -> 3330.64s]  The issue is that whenever you're, you know, effectively utilizing your SMs, you're basically maxed out on your shared memory, right?
[3330.64s -> 3333.40s]  That's, like, the bottlenecked resource.
[3333.40s -> 3338.28s]  And so there is no place to prefetch in some sense.
[3338.28s -> 3338.80s]  Cool.
[3338.80s -> 3341.72s]  Okay.
[3341.72s -> 3353.64s]  And the other thing that is very, very, you know, we're getting into the weeds here, complex, is the interaction between tiling and sort of burst sections.
[3353.64s -> 3364.60s]  So imagine I have a matrix layout that's kind of like this, where, you know, I have my nice burst sections, and each burst section lines up nicely with a tile.
[3364.60s -> 3372.16s]  So to read this tile, all I have to do is to, you know, get four different burst sections, and I've gotten this entire tile.
[3372.16s -> 3383.64s]  Now, imagine what happens if I add sort of one element extra, and the way the matrix is laid out, you know, my sort of tile starts, sort of my burst sections flow over.
[3383.64s -> 3389.20s]  So now what's happening is when I load my tile, I'm going to load this first part, and that's really great.
[3389.20s -> 3391.88s]  I get the entire first row as a burst section.
[3391.88s -> 3400.20s]  Now, in the second row, this actually belongs to two different burst sections, and so I have to do two reads in order to get this second row, and so on and so forth.
[3400.20s -> 3413.00s]  So I've essentially doubled the number of memory accesses because I've added a single extra element at the very end there that's kind of bumped up the alignment of my burst section and my aligned layout.
[3413.00s -> 3426.76s]  And so basically, if tiles or if your matrix sizes aren't multiples of your burst section, you can easily end up with situations like this where the rows don't line up with the burst section, and you've doubled the amount of memory access that you have to do.
[3426.76s -> 3436.56s]  And the way to get around this is you have to do padding to be able to kind of get nice, round matrix sizes so that your burst sections line up with the size of your tiles.
[3436.56s -> 3451.84s]  So this is getting very into the weeds here, but if you really want to squeeze out all the performance from your matrix multiplies, these are the kinds of things you have to think about, and you will get bitten by this if you're not thinking about it.
[3451.84s -> 3460.16s]  And of course, I guess things like Torch Compile and all the CUDA optimizations for matrix multiplies, they're doing exactly the kinds of stuff that I just talked about.
[3460.16s -> 3463.84s]  That's the way you get better performance.
[3463.84s -> 3486.84s]  And so, you know, all this matrix complexity, you know, ends up in situations like this, where, you know, the, I'm reading Andrei's tweet here, but you know the most dramatic optimization to nano GPT is to increase the vocab size from 50257 to 5304, which is the nearest multiple 64, which gives you much, much higher occupancy.
[3486.84s -> 3501.84s]  Careful with your powers of two right so that's a 25% speed up from adding, how many it's like 5057 or 47 dimensions to your vocab like that's kind of like, you know, how does that happen, right.
[3501.84s -> 3515.84s]  And so, that kind of brings us back to the mystery, like, you know, I was dragging you through all the GPU details on in the hopes that you know you'll have a full understanding of all the performance characteristics, but in some sense the payoff is, you know, I now get to explain
[3515.84s -> 3524.84s]  to you how this chart comes to be, and at the end you won't find matrix multiply performance to be so mysterious or scary at the end here, right.
[3524.84s -> 3537.84s]  So, the very first part is very very simple like we understand compute intensity right this is exactly the roofline that I pointed out at the very beginning right so so up until here, which is about 1536 right.
[3537.84s -> 3551.84s]  You don't have enough matrix multiply work to do right that just loading the matrix and doing very basic IO right that you have to do is becoming a bottleneck below this point right so throughput is going to fall through to the ground.
[3551.84s -> 3557.84s]  Past this point, you just don't have enough memory bandwidth to support your compute units.
[3557.84s -> 3571.84s]  So on the right side here, in theory, right if you if I draw the upper envelope, this is the kind of maximum achievable performance so it's possible up here to saturate all my compute units and get really great performance, but if you kind of mess up your matrix
[3571.84s -> 3585.84s]  sizing, you can end up in these kind of really weird places, and within each one of these you can kind of end up in a weird trough. And so we're going to kind of think a little bit about, you know, why do you have all these different places you can end up.
[3585.84s -> 3587.84s]  So the first thing.
[3587.84s -> 3589.84s]  This first line here.
[3589.84s -> 3601.84s]  This is a piling alignment issue. So if you look at kind of the multiples here so I've now colored, each of these lines, based on kind of the visibility of the matrix size.
[3601.84s -> 3611.84s]  And this is the size by which it's divisible. So if it's divisible by 32, then you're in good shape you're in these purple dots up here, if you're divisible by 16.
[3611.84s -> 3625.84s]  If you're actually still up here there's two colors. And then if you're green your cake was a you're up here. If you're orange or cake was to, and if you're a cake was one, you're all the way down here if you're not divisible by any number, don't pick prime dimensions,
[3625.84s -> 3629.84s]  you're not going to get very good throughput on your matrix multiplies.
[3629.84s -> 3646.84s]  And a big part of this is going to be, you know, once you get to kind of K equals two and K equals one, you are basically forcing the situation where you can no longer read tiles in the sort of nicely aligned way with your burst reads, and that's going to lead to some serious issues.
[3646.84s -> 3649.84s]  So, so that's kind of a problem.
[3649.84s -> 3663.84s]  But then okay so that's one part of the mystery, but I think another part of the mystery remains like so within this orange line, you know, I think, if you zoom into here you see this giant drop right from from this point, all the way down to this point where you're just kind of wondering
[3663.84s -> 3670.84s]  what happened here, how could I lose so much performance, increasing my dimension by two.
[3670.84s -> 3676.84s]  And so let's just look at these numbers, and it's just I think this is a fun puzzle so I'm just going to walk you through the puzzle.
[3676.84s -> 3681.84s]  This is going to happen when you transition from 1792 to 1790.
[3681.84s -> 3687.84s]  I guess three or four size, let's say four here, just so that it's a factor of two still.
[3687.84s -> 3695.84s]  Well, why does that happen. Okay, well, let's say that we're using a tile size of 256 by 128 that's a pretty natural size.
[3695.84s -> 3707.84s]  As a fun, fun fact, you know the matrix multiply units in the GPUs there, they're naturally operating on matrices of roughly size 128 so 256 by 128 is a very nice tile size right.
[3707.84s -> 3718.84s]  So that means how many tiles are there well there's seven times 14 tiles right because we're dividing the dimension of the matrix by the size of our tiles. That's a total of 98 different tiles.
[3718.84s -> 3726.84s]  And if we increase this by one. Well, you know, we're going to have to round up each one of our coordinates, and so we're going to have a lot more tiles 120 of them. Right.
[3726.84s -> 3729.84s]  So we've increased the number of tiles by quite a bit.
[3729.84s -> 3743.84s]  Well, you know, what's going to happen is, not only did we significantly increase the tiles and some of them have lower utilization, which is bad, but actually even worse, an A100 has 108 SMs, right.
[3743.84s -> 3756.84s]  And if I if you go all the way back to the kind of the GPU execution model right SMs can can execute in parallel, and they're kind of the execution units. And so when you have 98 SMs, they all go and run right again you can dispatch them all all the SMs are running,
[3756.84s -> 3769.84s]  you know you've got great utilization. Once you go to 120 tiles. Now you've got more tiles than SMs, so 108 of those will execute, and then you will go back and you'll say all right I got some more SMs have very very low utilization you're going to execute
[3769.84s -> 3778.84s]  the remaining 12 and wait for those to complete right and that's going to be really bad so if you look at your utilization, a good utilization for a while you'll drop off a cliff and then you'll sort of finish up your job.
[3778.84s -> 3781.84s]  So, this is something called wave quantization.
[3781.84s -> 3793.84s]  And so ideally your tile sizes are either much bigger than the number of SMs, or, you know, they're, they're not like this where you're just like barely over the SM and you've caused this quantization sort of error.
[3793.84s -> 3796.84s]  Additionally, cool.
[3796.84s -> 3809.84s]  All right. I know this is this is low level details but in many ways, you know, I've been saying through through many classes that language models and deep learning is attention to detail, and these kinds of attention to detail is the things that allow people
[3809.84s -> 3817.84s]  to scale up elements to really really large sizes and get great performance. So it's worth knowing, even if you're not a person that's going to do systems engineering.
[3817.84s -> 3831.84s]  So, what were the tricks right on key ideas here. First one is, you got to reduce the amount of memory accesses right so there's lots of ways to do it, you can do coalescing right so that you're not, you can sort of reuse leads that you're getting
[3831.84s -> 3832.84s]  for free.
[3832.84s -> 3840.84s]  You can do fusion so that you can fuse multiple operations together and avoid unnecessary reason rights.
[3841.84s -> 3848.84s]  So, you know, even if you're going to do reads they're going to be from much faster memory. And that's going to be sort of piling tricks that you can do.
[3848.84s -> 3860.84s]  And then finally, you can kind of trade memory for other resources that you do have right so you can trade it for compute which is going to be recomputation, or you can trade it for just numerical precision or stability which is going to be quantization
[3860.84s -> 3868.84s]  right. So there's lots of bags of tricks that you have in order to get sort of performance out right so there's lots of things you can do.
[3868.84s -> 3878.84s]  You just have to be really mindful of kind of the role that memory plays in the performance of a GPU. That's kind of the key thing to get the most out.
[3878.84s -> 3879.84s]  Cool.
[3879.84s -> 3885.84s]  Any questions on that before I sort of move to the final part with flash attention.
[3885.84s -> 3886.84s]  Okay, good.
[3886.84s -> 3899.84s]  All right, so now I'm going to put it all together right like I'm going to try to make it so that all the the tricks that I taught you aren't these like random disconnected facts about GPUs, they're kind of part of the standard performance optimization
[3899.84s -> 3909.84s]  toolkit and flash attention and flash attention to will hopefully teach you how that all comes together to build one of the, you know, the foundations I guess of modern high performance transformers.
[3909.84s -> 3918.84s]  So, flash attention, you know, we know that it dramatically accelerates attention. And most of you probably know that that's done through some CUDA kernel magic.
[3918.84s -> 3931.84s]  But maybe you don't know all the details right so you know what the the paper says is okay so there's one part that's happening which is you know you do attention on a unoptimized, you know, pie towards transformer implementation if you fuse the
[3931.84s -> 3936.84s]  kernel and you do some things you can get significant significant speed ups.
[3936.84s -> 3948.84s]  And from the paper you know they say we apply to establish techniques tiling and re computation to overcome the technical challenge of computing exact attention is subcratic quadratic HBM accesses right so so it's not sub quadratic you know computation
[3948.84s -> 3960.84s]  because you can't do they have to compute, you know, attention, in general, but they're going to get sub quadratic accesses to the high bandwidth or global memory right and so that's really the key if your memory is the bottleneck, you know, you want to make
[3960.84s -> 3968.84s]  that not quadratic, so that at least you can pay for quadratic cost with your compute, rather than with your memory.
[3968.84s -> 3975.84s]  So, just for a really quick recap, you know at this point you've implemented attention many many times in many classes.
[3975.84s -> 3981.84s]  Right, so it's going to be three different matrix multiplies you've got a KQ and V, with a softmax in between.
[3981.84s -> 3988.84s]  So, the matrix multiplies are pretty simple that can be you know done with tiling, I showed you examples like that.
[3988.84s -> 4001.84s]  What's different about attention, well there's a softmax thing that's going to be the real tricky bit. And then once we can deal with the softmax on all the sort of matrix multiply things I was talking about will just come into play.
[4001.84s -> 4016.84s]  So, the matrix multiply as I said before, is exactly what I taught you so if you look at the figure one from the flash attention paper. This is really just a simple tiled matrix multiply right you see, you know, the, the K matrix the Q matrix, you see it cut up into small
[4016.84s -> 4030.84s]  blocks, you know small blocks of it are being copied to SRAM, they're being multiplied, and then they're being, you know, accumulate or descent to the HBM, where you do softmax is, and then you multiply with a V, right.
[4030.84s -> 4040.84s]  This is all just really simple. In terms of the KQV matrix multiply, but now we have to think about the softmax right like what's going on with the softmax.
[4040.84s -> 4054.84s]  So the key thing here is the softmax sorry I'm going to roll back one step. So the issue with the softmax what's the problem with the softmax. It's a global operation, right, the softmax in an attention operates row by row, you have to sum the entire row right to compute
[4054.84s -> 4069.84s]  so the sum normalizing term of the softmax. And that's very problematic if I have tiles. Right. Ideally, I want to do everything within the tiles right I don't ever want to have to write back to the big matrix and so I need a softmax that can be computed
[4069.84s -> 4074.84s]  within each tile right I want to do as much computation within each tile as possible.
[4074.84s -> 4080.84s]  So, the key thing here is to use what's called the online softmax.
[4080.84s -> 4092.84s]  And so what is that if you have a stream of values, right, normally, the batch version of the softmax you take all of your x one through x events, and you would exponentiate them some them and you would divide them right that's why you would do in your normal
[4092.84s -> 4104.84s]  softmax, and then you would you know maybe compute the maximum value and you subtract that in order to be able to make this numerically stable right so this is the standard numerically stable softmax on the left side.
[4104.84s -> 4109.84s]  So the online softmax I've taken this from from Michael often in gimelstein in 2018.
[4109.84s -> 4117.84s]  Well, you can sort of realize that you can pull out, be a sort of like a telescope being some kind of an argument.
[4117.84s -> 4132.84s]  So this is basically the current running, sort of normalizer term, and the current sort of top term of each of the x i minus max of x k right so what you're going to do is you're going to maintain your current max that you've seen over x one through x of j,
[4132.84s -> 4146.84s]  which is my current iteration. And then I'm also going to maintain sort of this correction term if my max updated this is going to basically correct my max, and then I'm going to add my sort of new term over here right so it's this d of j is going
[4146.84s -> 4164.84s]  to track online, the top term of this equation two over here. And then, you know, at the end, I can also then compute the normalizer, and then sort of get the normalized y of i that I want right this d of v is itself, sort of the normalization term that I need.
[4164.84s -> 4174.84s]  So, the key thing here is that this can be done online, I don't need the x one through x event up front. All I need is sort of the stream of x one through x.
[4174.84s -> 4187.84s]  And that's really key because I can now compute the softmax tile by tile right within each tile, I can run this algorithm, and that will let me compute kind of a partial softmax for that tile, and then I can sort of right back if I need to all the components
[4187.84s -> 4200.84s]  that I sort of, I'm keeping track of, and that's all that I kind of need in order to do this computation right so I never have to materialize the full and n squared matrix, in order to compute the softmax.
[4200.84s -> 4214.84s]  And so that's basically it. But once you have that, you know, you've put it all together and you can get the forward pass of flash attention and if you go and look at the flash attention to paper, which is going to be a thing that we're going to ask you to implement
[4214.84s -> 4223.84s]  so you're going to be following through kind of these, these steps here, you're going to see exactly this idea so first you're going to have your, your KQ matrix multiply.
[4223.84s -> 4236.84s]  And this is going to be piled so these are little tile chunks, and they're going to be multiplied, and how am I going to compete the softmax well I'm going to maintain sort of a running value of the sort of exponentiated sums, and then I'm going to keep
[4236.84s -> 4251.84s]  incrementally updating it and correcting for the maximum terms, and by doing that I can compute all the necessary quantities kind of tile by tile, sort of going from one tile to another, and then just multiply once again with tiles with V in the end, and that will give
[4251.84s -> 4255.84s]  me sort of my full softmax output. Right.
[4255.84s -> 4267.84s]  Yes.
[4267.84s -> 4276.84s]  So the question was you can't compute this until you, you are done with all the tiles and see if the double back on all the tiles.
[4276.84s -> 4281.84s]  So,
[4281.84s -> 4287.84s]  that's right so you will have to before you can output your softmax you will have to go through all the tiles. This is correct.
[4287.84s -> 4301.84s]  But by, let's say I do all the tiles once right like I do all n squared tiles. At that point I have all the components that I need in order to directly output the softmax at that point, I don't have to do the computation, because I have the normalizer
[4301.84s -> 4317.84s]  right by going through each of these kind of tiles at the end of going through all these tiles, I've built up, you know, l three, or ll of n which is the sum of all of the exponentiated terms, so I already have that in my sort of my shared memory for this last tile,
[4317.84s -> 4325.84s]  and then that allows me to exponentiate and divide, and then return all the components.
[4325.84s -> 4327.84s]  Okay.
[4327.84s -> 4339.84s]  So, the backward pass, I'm not going to cover on, you can do re computation tile by tile which will allow you to avoid storing the softmax right remember, you know, I always want to avoid storing anything that's of size and square.
[4339.84s -> 4351.84s]  And so here I've been sort of clever with the tiles so that I don't have to store any of the end squared components when I'm computing for example the softmax, but in the backwards pass if I store the activations, that's already something that's
[4352.84s -> 4366.84s]  So I don't want to store my end squared activations, I'm going to have to recompute it on the fly tile by tile. When I do the backwards pass right so that's a really key other trick that they do in order to make the backwards pass possible, but otherwise, it's fairly standard
[4366.84s -> 4375.84s]  to do the same thing as computing the gradients just tile by tile, and doing that computation.
[4375.84s -> 4388.84s]  Okay, that brings us to the end here. Hopefully you've kind of seen how all of the pieces I talked about about piling and coalescing and re computation, come together to give you a flash attention and all these really cool things that make your transformers
[4388.84s -> 4391.84s]  go much faster.
[4391.84s -> 4399.84s]  So, to, you know, recap for the whole lecture, right, hardware is kind of the thing that has really powered all the language models that we have today.
[4399.84s -> 4409.84s]  So if you really want to leverage your hardware, you have to understand the low level details I think all the systems advances really engaged with a lot of the concepts that I taught today.
[4409.84s -> 4421.84s]  And the current GPU sort of scaling, you know that plot is really the one you should remember, really, really incentivizing encourages you to think about memory movement right like the memory movement is the bottleneck in all this.
[4421.84s -> 4428.84s]  And so you don't want to just think about oh how do I reduce the number of flops, that's important to really you really have to think about okay how do I make my memory movements.
[4428.84s -> 4431.84s]  How do I make my memory movements more efficient.
[4431.84s -> 4447.84s]  And then finally, if you, you know, have to do a certain amount of computation. Well to optimize things, the way to do it is to optimize your data movement right to be able to avoid as much movement from the high bandwidth memory, or the global memory as possible you want to reduce that
[4447.84s -> 4454.84s]  and have everything in the very very fast shared memory, and that leads to good performance on things like flash attention.
[4454.84s -> 4455.84s]  Thanks everyone.
[4458.84s -> 4460.84s]  Thank you.
