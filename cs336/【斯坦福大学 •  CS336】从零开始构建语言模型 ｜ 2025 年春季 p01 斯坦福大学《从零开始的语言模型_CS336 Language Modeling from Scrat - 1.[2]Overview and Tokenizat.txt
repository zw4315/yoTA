# Detected language: en (p=1.00)

[0.00s -> 15.32s]  Welcome everyone, this is CS336 Language Models from Scratch, and this is the core staff,
[15.32s -> 18.16s]  so I'm Percy, one of your instructors.
[18.16s -> 21.96s]  I'm really excited about this class because it really allows you to see the whole language
[21.96s -> 26.24s]  modeling building pipeline end-to-end, including data systems and modeling.
[26.56s -> 30.80s]  Tatsu, I'll be co-teaching with him, so I'll let everyone introduce themselves.
[30.80s -> 37.64s]  Hi everyone, I'm Tatsu, I'm one of the co-instructors, I'll be giving lecture in a week or two, probably
[37.64s -> 38.64s]  a few weeks.
[38.64s -> 43.84s]  I'm really excited about this class, Percy and I spent a while being a little disgruntled,
[43.84s -> 47.84s]  thinking like, what's the really deep technical stuff that we can teach our students today?
[47.84s -> 51.64s]  And I think one of the things is really, you gotta build it from scratch to understand
[51.64s -> 55.44s]  it, so I'm hoping that that's sort of the ethos that came away from our class.
[55.44s -> 60.24s]  As Tatsu mentioned, this is the second time we're teaching the class, we've grown the class
[60.24s -> 68.28s]  by around 50%, one big thing is we're making all the lectures on YouTube so that the world
[68.28s -> 71.72s]  can learn how to build language models from scratch.
[71.72s -> 79.48s]  Okay, so why do we decide to make this course and endure all the pain?
[79.48s -> 84.56s]  So let's ask GPT-4, so if you ask it, why teach a course on building language models
[84.60s -> 92.32s]  from scratch, the reply is, teaching a course provides foundational understanding of techniques,
[92.32s -> 97.24s]  fosters innovation, kind of the typical kind of generic bladders.
[97.24s -> 99.64s]  Okay, so here's the real reason.
[99.64s -> 103.92s]  So we're in a bit of a crisis, I would say.
[103.92s -> 108.72s]  Researchers are becoming more and more disconnected from the underlying technology.
[108.76s -> 115.04s]  Eight years ago, researchers would implement and train their own models in AI, even six
[115.04s -> 121.72s]  years ago, you at least take the models like BERT and download them and fine-tune them,
[121.72s -> 128.04s]  and now many people can just get away with prompting a proprietary model.
[128.04s -> 133.66s]  So this is not necessarily bad, right, because as you enter these layers of abstraction,
[133.66s -> 139.82s]  you can all do more, and a lot of research has been unlocked by the simplicity of being
[139.82s -> 145.62s]  able to prompt a language model, and I do my share of prompting, so there's nothing
[145.62s -> 150.90s]  wrong with that, but it's also remembered that these abstractions are leaky.
[150.90s -> 155.58s]  So in contrast to programming languages or operating systems, you don't really understand
[155.58s -> 161.50s]  what the abstraction is, it's a string in and a string out, I guess, and I would
[161.54s -> 166.30s]  say that there's still a lot of fundamental research to be done that required tearing up
[166.30s -> 171.30s]  the stack and co-designing different aspects of the data and the systems and the model,
[171.30s -> 178.06s]  and I think really that full understanding of this technology is necessary for fundamental research.
[178.06s -> 180.42s]  So that's why this class exists.
[180.42s -> 186.98s]  We want to enable the fundamental research to continue, and our philosophy is to understand it,
[186.98s -> 189.26s]  you have to build it.
[189.26s -> 195.66s]  So there's one small problem here, and this is because of the industrialization
[195.66s -> 197.46s]  of language models.
[197.46s -> 206.42s]  So GPD-4 has rumored to be 1.8 trillion parameters, cost $100 million to train.
[206.42s -> 213.66s]  You have XAI building the clusters with 200,000 H100s, if you can imagine that.
[213.66s -> 220.10s]  There's an investment of over 500 billion, supposedly, over four years.
[220.10s -> 223.18s]  So these are pretty large numbers, right?
[223.18s -> 228.06s]  And furthermore, there's no public details on how these models are being built.
[228.06s -> 234.42s]  Here from GPD-4, this is even two years ago, they very honestly say that due to the
[234.42s -> 240.46s]  competitive landscape and safety limitations, we're going to disclose no details.
[240.54s -> 245.02s]  So this is the state of the world right now.
[245.02s -> 248.86s]  And so, in some sense, frontier models are out of reach for us.
[248.86s -> 256.74s]  So if you came into this class thinking you're each going to train your own GPD-4, sorry.
[256.74s -> 262.42s]  So we're going to build small language models, but the problem is that these might
[262.42s -> 264.18s]  not be representative.
[264.18s -> 268.26s]  And here's some of two examples to illustrate why.
[268.26s -> 271.54s]  So here's kind of a simple one.
[271.54s -> 277.14s]  If you look at the fraction of flops spent in the attention layers of a transformer
[277.14s -> 280.82s]  versus a MLP, this changes quite a bit.
[280.82s -> 288.94s]  So this is a tweet from Steven Frola from quite a few years ago, but this is still true.
[288.94s -> 292.66s]  If you look at small models, it looks like the number of flops in the attention
[292.66s -> 296.90s]  versus the MLP layers are roughly comparable.
[296.90s -> 302.70s]  But if you go up to 175 billion, then the MLPs really dominate.
[302.70s -> 304.94s]  Right, so why does this matter?
[304.94s -> 308.86s]  Well, if you spend a lot of time at small scale and you're optimizing the
[308.86s -> 315.82s]  attention, you might be optimizing the wrong thing because at larger scale,
[315.82s -> 319.02s]  it gets washed out.
[319.02s -> 322.66s]  This is kind of a simple example because you can literally make this plot without
[322.66s -> 324.02s]  actually any compute.
[324.06s -> 327.78s]  You just, like, do, it's napkin math.
[327.78s -> 330.42s]  Here's something that's a little harder to grapple with.
[330.42s -> 331.82s]  It's just emergent behavior.
[331.82s -> 335.90s]  So this is a paper from Jason Wei from 2022.
[335.90s -> 343.86s]  And this plot shows that as you increase the amount of training flops and you look
[343.86s -> 348.22s]  at accuracy on a bunch of tasks, you'll see that for a while,
[348.22s -> 351.34s]  it looks like the accuracy, nothing is happening.
[351.34s -> 356.14s]  And all of a sudden, you get these kind of, you know, emergent of various
[356.14s -> 358.14s]  phenomena like in-context learning.
[358.14s -> 361.90s]  So if you were hanging around at this scale, you would be concluding that,
[361.90s -> 365.58s]  well, these language models really don't work when, in fact,
[365.58s -> 369.14s]  you had to scale up to get that behavior.
[369.14s -> 371.78s]  So don't despair.
[371.78s -> 375.10s]  We can still learn something in this class.
[375.10s -> 378.54s]  But we have to be very precise about what we're learning.
[378.54s -> 380.62s]  So there's three types of knowledge.
[380.62s -> 383.82s]  There's the mechanics of how things work.
[383.82s -> 385.10s]  This we can teach you.
[385.10s -> 386.66s]  We can teach you what a transformer is.
[386.66s -> 388.38s]  You'll implement a transformer.
[388.38s -> 392.38s]  We can teach you how model parallelism leverages GPUs efficiently.
[392.38s -> 397.22s]  These are just like kind of the raw ingredients, the mechanics.
[397.22s -> 399.02s]  So that's fine.
[399.02s -> 401.66s]  We can also teach you mindset.
[401.66s -> 404.78s]  So this is something a bit more subtle and seems like a little bit,
[404.78s -> 406.62s]  you know, fuzzy.
[406.62s -> 412.18s]  But this is actually, in some ways, more important, I would say,
[412.18s -> 417.46s]  because the mindset that we're going to take is that we want to squeeze as
[417.46s -> 421.94s]  most out of the hardware as possible and take scaling seriously, right?
[421.94s -> 425.26s]  Because in some sense, the mechanics, all of those, we'll see later,
[425.26s -> 429.14s]  that all of these ingredients have been around for a while, but it was really,
[429.14s -> 434.14s]  I think, the scaling mindset that OpenAI pioneered that led to this next
[434.14s -> 437.22s]  generation of AI models.
[437.22s -> 441.22s]  So mindset, I think, hopefully, we can, you know, bang it into you that to think
[441.22s -> 442.98s]  in a certain way.
[442.98s -> 445.82s]  And then thirdly is intuitions.
[445.82s -> 451.74s]  And this is about which data and modeling decisions lead to good models.
[451.74s -> 455.42s]  This, unfortunately, we can only partially teach you.
[455.42s -> 461.26s]  And this is because what architectures and what data sets work at most scales might
[461.26s -> 464.78s]  not be the same ones that work at large scales.
[464.78s -> 469.30s]  And, but, you know, that's just, but hopefully you got two and a half out
[469.30s -> 473.70s]  of three, so that's pretty good bang for your buck.
[473.70s -> 478.78s]  Okay, speaking of intuitions, there's a sort of, I guess, sad reality of things
[478.78s -> 483.90s]  that, you know, you can tell a lot of stories about why certain things in the
[483.90s -> 487.98s]  transformer, the way they are, but sometimes it's just, you know,
[488.38s -> 491.86s]  you do the experiments and the experiments speak.
[491.86s -> 495.06s]  So for example, there's this Noam Shazia paper that introduced the SWIGLU,
[495.06s -> 498.78s]  which is something that we'll see a bit more in this class,
[498.78s -> 501.98s]  which is a type of non-linearity.
[501.98s -> 506.26s]  And in the conclusion, you know, the results are quite good,
[506.26s -> 507.58s]  and this got adopted.
[507.58s -> 510.14s]  But in the conclusion, there is this honest statement that we offer
[510.14s -> 514.58s]  no explanation except for this is divine benevolence.
[514.58s -> 516.22s]  So there you go.
[516.30s -> 522.30s]  This is the extent of our understanding.
[522.30s -> 526.70s]  Okay, so now let's talk about this bitter lesson that I'm sure people have,
[526.70s -> 527.66s]  you know, heard about.
[527.66s -> 532.50s]  I think there's a sort of a misconception that a bitter lesson means that scale is
[532.50s -> 537.34s]  all that matters, algorithms don't matter, all you do is pump more capital into
[537.34s -> 539.62s]  building the model, and you're good to go.
[539.62s -> 542.10s]  I think this couldn't be further from the truth.
[542.10s -> 547.30s]  I think the right interpretation is that algorithms at scale is what matters.
[547.30s -> 551.54s]  And because at the end of the day, your accuracy of your model is really a
[551.54s -> 557.26s]  product of your efficiency and the number of resources you put in.
[557.26s -> 562.42s]  And actually, efficiency, if you think about it, is way more important at larger
[562.42s -> 566.26s]  scale because if you're spending, you know, hundreds of millions of dollars,
[566.26s -> 571.62s]  you cannot afford to be wasteful in the same way that if you're looking at
[571.62s -> 577.26s]  running a job on your local cluster, you might run it again, you fail,
[577.26s -> 582.90s]  you debug it, and if you look at actually the utilization and the use,
[582.90s -> 587.90s]  I'm sure OpenAI is way more efficient than any of us right now.
[587.90s -> 591.54s]  So efficiency really is important.
[591.54s -> 596.62s]  And furthermore, I think this point is maybe not as well appreciated in the sort
[596.62s -> 602.38s]  of scaling rhetoric, so to speak, which is that if you look at efficiency,
[602.38s -> 605.74s]  which is a combination of hardware and algorithms, but if you just look at the
[605.74s -> 612.54s]  algorithm efficiency, there's this nice OpenAI paper from 2020 that showed
[612.54s -> 620.02s]  over the period of 2012 to 2019, there was a 44x algorithmic efficiency
[620.02s -> 624.58s]  improvement in the time that it took to train ImageNet to a certain level
[624.58s -> 626.30s]  of accuracy.
[626.50s -> 631.34s]  So this is huge, and I think if you, I don't know if you could see the abstract
[631.34s -> 634.74s]  here, this is faster than Moore's Law.
[634.74s -> 636.34s]  So algorithms do matter.
[636.34s -> 642.70s]  If you didn't have this efficiency, you would be paying 44 times more cost.
[642.70s -> 648.06s]  This is for image models, but there's some results for language as well.
[648.06s -> 653.06s]  Okay, so with all that, I think the right framing or mindset to have is what is
[653.06s -> 657.74s]  the best model one can build, given a certain compute and data budget?
[657.74s -> 658.30s]  Okay?
[658.30s -> 661.46s]  And this question makes sense no matter what scale you're at,
[661.46s -> 666.74s]  because you're sort of like, it's accuracy per resources.
[666.74s -> 669.82s]  And of course, if you can raise the capital and get more resources,
[669.82s -> 671.02s]  you'll get better models.
[671.02s -> 676.94s]  But as researchers, our goal is to improve the efficiency of the algorithms.
[676.94s -> 679.38s]  Okay, so maximize efficiency.
[679.38s -> 682.42s]  We're going to hear a lot of that.
[682.42s -> 689.18s]  Okay, so now let me talk a little bit about the current landscape and a little
[689.18s -> 693.58s]  bit of, I guess, you know, obligatory history.
[693.58s -> 697.54s]  So language models have been around for a while now.
[697.54s -> 701.74s]  You know, going back to Shannon, you know, who looked at language models as a way
[701.74s -> 709.06s]  to estimate the entropy of English, I think in AI, they really were prominent
[709.06s -> 713.58s]  in NLP, where they were a component of larger systems like machine translation,
[713.58s -> 714.90s]  speech recognition.
[714.90s -> 719.02s]  And one thing that's maybe not as appreciated these days is that if you
[719.02s -> 725.30s]  look back in 2007, Google was training 30 large n-gram models,
[725.30s -> 728.66s]  so 5-gram models, over 2 trillion tokens, which is a lot more tokens
[728.66s -> 734.86s]  than GPT-3, and it was only, I guess, in the last two years that we've
[734.86s -> 737.70s]  gotten to that token count.
[738.10s -> 741.94s]  But they were n-gram models, so they didn't really exhibit any of the interesting
[741.94s -> 745.22s]  phenomena that we know of language models today.
[745.22s -> 749.06s]  Okay, so in the 2010s, I think a lot of the, you can think about this,
[749.06s -> 753.50s]  a lot of the deep learning revolution happened, and a lot of the ingredients sort
[753.50s -> 756.02s]  of kind of falling into place, right?
[756.02s -> 759.66s]  So there was the first neural language model from Yachter Benjold's group
[759.66s -> 762.10s]  in back in 2003.
[762.10s -> 764.54s]  There was seek-to-seek models.
[764.58s -> 769.30s]  This, I think, was a big deal for, you know, how do you basically model
[769.30s -> 775.50s]  sequences from Ilya and Google folks.
[775.50s -> 780.30s]  There's an atom optimizer, which still is used by the majority of people
[780.30s -> 782.38s]  dating over a decade ago.
[782.38s -> 787.30s]  There's a tension mechanism, which was developed in the context
[787.30s -> 792.42s]  of machine translation, which then led up to the famous attention
[792.46s -> 797.02s]  technology you need, or the AKA the transformer paper in 2017.
[797.02s -> 800.02s]  People were looking at how to scale a mixture of experts.
[800.02s -> 806.98s]  There was a lot of work around late 2010s on how to essentially do model
[806.98s -> 808.10s]  parallelism.
[808.10s -> 812.14s]  And they were actually figuring out how you could train, you know, 100 billion
[812.14s -> 812.78s]  prime-dome models.
[812.78s -> 815.94s]  They didn't train it for very long, because these were like more
[815.94s -> 817.06s]  system work.
[817.06s -> 827.74s]  But all the ingredients were kind of in place by the time 2020 came around.
[827.74s -> 834.54s]  So I think one other trend which was starting NLP was the idea of,
[834.54s -> 838.58s]  you know, these foundation models that could be trained on a lot of text and
[838.58s -> 841.38s]  adapted to a wide range of downstream tasks.
[841.38s -> 845.22s]  So Elmo, BERT, you know, T5.
[845.22s -> 851.38s]  These were models that were, for their time, very exciting.
[851.38s -> 854.70s]  We kind of maybe forget how excited people were about, you know,
[854.70s -> 858.14s]  things like BERT, but it was a big deal.
[858.14s -> 861.34s]  And then I think, I mean, this is abbreviated history,
[861.34s -> 867.06s]  but I think one critical piece of the puzzle is, you know,
[867.06s -> 873.06s]  OpenAI just taking these ingredients, you know, and applying very nice
[873.06s -> 878.54s]  engineering and really kind of pushing on the kind of the scaling laws,
[878.54s -> 882.30s]  embracing it as, you know, this is the kind of the mindset piece.
[882.30s -> 885.34s]  And that led to GPT-2 and GPT-3.
[885.34s -> 890.42s]  Google, you know, obviously was in the game and trying to,
[890.42s -> 893.42s]  you know, compete as well.
[893.42s -> 898.86s]  But that sort of paved the way, I think, to another kind of line of work,
[898.86s -> 902.34s]  which is these were all closed models.
[902.42s -> 905.90s]  So models that weren't released and you can only access via API,
[905.90s -> 909.38s]  but they were all open models starting with, you know,
[909.38s -> 913.78s]  early work by, you know, Eleuther, right after GPT-3 came out,
[913.78s -> 920.82s]  Meta's early attempt, which didn't work maybe as quite as well, Bloom.
[920.82s -> 925.02s]  And then Meta, Alibaba, DeepSeek, AI2, and there's a few others,
[925.02s -> 930.18s]  which I haven't listed, have been creating these open models,
[930.18s -> 933.54s]  where the weights are released.
[934.54s -> 938.62s]  One other piece of, I think, tidbit about openness, I think, is important,
[938.62s -> 940.86s]  is that there's many levels of openness.
[940.86s -> 943.30s]  There's closed models, like GPT-4.
[943.30s -> 947.46s]  There's open weight models, where the weights are available,
[947.46s -> 949.42s]  and there's actually a paper, a very nice paper,
[949.42s -> 954.66s]  with lots of architectural details, but no details about the data set.
[954.66s -> 956.58s]  And then there's open source models,
[956.58s -> 959.74s]  where all the weights and data are available,
[959.78s -> 962.70s]  and the paper, where they're honestly trying to explain
[962.70s -> 966.74s]  as much as they can, you know, but of course,
[966.74s -> 969.82s]  you can't really capture everything, you know, in a paper,
[969.82s -> 973.34s]  and there's no substitute for learning how to build it,
[973.34s -> 975.38s]  except for kind of doing it yourself.
[976.38s -> 978.62s]  Okay, so that leads to kind of the present day,
[978.62s -> 982.94s]  where there's a whole host of, you know,
[982.94s -> 986.86s]  frontier models from OpenAI, Anthropic, XAI, Google,
[986.86s -> 989.14s]  Meta, DeepSeek, Alibaba, Tencent,
[989.14s -> 991.62s]  and probably a few others, that,
[991.62s -> 994.82s]  or sort of dominate the current, you know, landscape.
[995.82s -> 997.94s]  So we're kind of in just this interesting time,
[997.94s -> 1001.42s]  where, you know, just to kind of reflect,
[1001.42s -> 1004.26s]  a lot of the ingredients, like I said, were developed,
[1004.26s -> 1007.10s]  which is good, because I think we're gonna revisit
[1007.10s -> 1010.30s]  some of those ingredients, and trace
[1010.30s -> 1014.30s]  how these techniques work, and then we're going to try
[1014.30s -> 1017.46s]  to move as close as we can to best practices
[1017.50s -> 1022.30s]  on frontier models, but, you know, using information
[1022.30s -> 1025.46s]  from essentially the open, you know, community.
[1026.98s -> 1029.70s]  And reading between the lines from what we know
[1029.70s -> 1031.48s]  about the closed models.
[1032.90s -> 1035.94s]  Okay, so just as an interlude,
[1037.46s -> 1040.10s]  so what are you looking at here?
[1040.10s -> 1044.62s]  So this is a executable lecture,
[1044.62s -> 1047.54s]  so it's a program where I'm stepping through,
[1047.54s -> 1049.10s]  and it delivers the content of lecture.
[1049.10s -> 1052.06s]  So one thing that I think is interesting here
[1052.06s -> 1056.38s]  is that you can embed code, so if you,
[1057.34s -> 1058.62s]  you can just step through code,
[1058.62s -> 1061.18s]  and I think this is a smaller screen than I'm used to,
[1061.18s -> 1063.94s]  but you can look at the environment variables
[1063.94s -> 1065.62s]  as you're stepping through code,
[1065.62s -> 1069.66s]  so that's useful later when we start actually
[1069.66s -> 1072.16s]  trying to drill down and giving code examples.
[1072.16s -> 1073.98s]  You can see the hierarchical structure of the lecture,
[1074.18s -> 1075.82s]  like we're in this module, and you can see
[1075.82s -> 1078.02s]  where it was called from main,
[1078.02s -> 1080.18s]  and you can jump to definitions,
[1081.38s -> 1084.62s]  like supervised fine-tuning, which we'll talk about later.
[1085.74s -> 1089.38s]  Okay, and if you think this looks like a Python program,
[1090.38s -> 1093.18s]  well, it is a Python program,
[1093.18s -> 1096.74s]  but I've made it, you know, processed it,
[1096.74s -> 1099.06s]  so for your viewing pleasure.
[1099.06s -> 1104.06s]  Okay, so let's move on to the course logistics now.
[1109.28s -> 1111.38s]  Actually, maybe I'll pause for questions.
[1112.90s -> 1117.90s]  Any questions about what we're learning in this class?
[1120.42s -> 1121.26s]  Yeah.
[1122.46s -> 1125.22s]  Do you expect a graduate from this class
[1125.22s -> 1128.10s]  to be able to lead a team to build a frontier model,
[1128.14s -> 1130.06s]  or other skills for this?
[1130.06s -> 1133.58s]  Okay, so the question is, would I expect a graduate
[1133.58s -> 1135.54s]  from this class to be able to lead a team
[1135.54s -> 1136.66s]  and build a frontier model?
[1136.66s -> 1139.98s]  Of course, with like a billion dollars of capital,
[1139.98s -> 1140.82s]  yeah, of course.
[1141.78s -> 1145.02s]  I would say that it's a good step,
[1145.02s -> 1149.52s]  but there's definitely many pieces that are missing,
[1149.52s -> 1151.58s]  and I think, you know, we thought about,
[1151.58s -> 1154.46s]  we should really teach like a series of classes
[1154.46s -> 1157.82s]  that eventually leads up to as close as we can get,
[1158.42s -> 1162.54s]  but I think this is maybe the first step of the puzzle,
[1162.54s -> 1163.62s]  but there are a lot of things,
[1163.62s -> 1165.42s]  and I'm happy to talk offline about that,
[1165.42s -> 1167.10s]  but I like the ambition, yeah.
[1168.18s -> 1170.14s]  That's what you should be doing, taking the class
[1170.14s -> 1173.30s]  so you can go lead teams and build frontier models.
[1173.30s -> 1174.14s]  Okay.
[1177.10s -> 1180.26s]  Okay, let's talk a little bit about the course.
[1180.26s -> 1182.70s]  So here's a website, everything's online.
[1182.70s -> 1185.36s]  This is a five-unit class,
[1186.36s -> 1191.36s]  but I think that maybe doesn't express the level here,
[1191.80s -> 1193.60s]  as well as this quote that I pulled out
[1193.60s -> 1195.68s]  from a course evaluation.
[1195.68s -> 1197.60s]  The entire assignment was approximately the same amount
[1197.60s -> 1201.16s]  of work as all five assignments from the CS224N
[1201.16s -> 1202.12s]  plus the final project,
[1202.12s -> 1203.80s]  and that's the first homework assignment.
[1203.80s -> 1206.04s]  So not to all scare you off,
[1206.04s -> 1209.52s]  but just giving some data here.
[1210.84s -> 1214.16s]  So why should you endure that?
[1214.16s -> 1215.32s]  Why should you do it?
[1216.20s -> 1218.76s]  This class is really for people who have
[1218.76s -> 1222.00s]  sort of this obsessive need to understand how things work,
[1222.00s -> 1225.78s]  all the way down to the atom, so to speak.
[1225.78s -> 1229.28s]  And I think when you get through this class,
[1229.28s -> 1230.94s]  I think you will have really leveled up
[1230.94s -> 1233.00s]  in terms of your research engineering.
[1233.00s -> 1235.16s]  The level of comfort that you'll have
[1235.16s -> 1237.08s]  in building ML systems at scale
[1237.08s -> 1240.04s]  will just be, I think, something.
[1240.88s -> 1242.54s]  There's also a bunch of reasons
[1242.54s -> 1244.66s]  that you shouldn't take the class.
[1244.66s -> 1247.66s]  For example, if you want to get any research done
[1247.66s -> 1250.72s]  this quarter, maybe this class isn't for you.
[1250.72s -> 1252.74s]  If you're interested in learning
[1252.74s -> 1254.90s]  just about the hottest new techniques,
[1255.74s -> 1257.08s]  there are many other classes
[1257.08s -> 1260.38s]  that can probably deliver on that better
[1260.38s -> 1262.62s]  than, for example, you spending a lot of time
[1262.62s -> 1263.96s]  debugging BPE.
[1265.46s -> 1269.06s]  And this is really, I think, a class about
[1269.06s -> 1271.82s]  the primitives and learning things bottom up
[1271.82s -> 1275.62s]  as opposed to kind of the latest.
[1276.50s -> 1280.14s]  And also, if you're interested in building language models
[1280.14s -> 1284.58s]  or, you know, for X, this is probably not
[1284.58s -> 1287.88s]  the first class you would take.
[1287.88s -> 1291.02s]  I think, practically speaking,
[1291.02s -> 1293.26s]  as much as I kind of made fun of prompting,
[1293.26s -> 1295.26s]  prompting is great, fine-tuning is great.
[1295.26s -> 1297.46s]  If you can do that and it works,
[1297.46s -> 1299.46s]  then I think that is something
[1299.46s -> 1301.14s]  you should absolutely start with.
[1301.94s -> 1304.10s]  I don't want people taking this class
[1304.10s -> 1306.38s]  and thinking that any problem,
[1306.38s -> 1309.06s]  the first step is to train a language model from scratch.
[1309.06s -> 1312.32s]  That is not the right way of thinking about it.
[1314.02s -> 1317.28s]  Okay, and I know that many of you,
[1318.90s -> 1321.66s]  some of you are enrolled, but we did have a cap,
[1321.66s -> 1323.66s]  so we weren't able to enroll everyone.
[1323.66s -> 1325.82s]  And although for the people online,
[1325.82s -> 1328.08s]  you can follow it at home.
[1328.08s -> 1330.58s]  All the lecture materials and assignments are online,
[1330.86s -> 1332.34s]  so you can look at them.
[1332.34s -> 1334.78s]  The lectures are also recorded
[1334.78s -> 1336.06s]  and will be put on YouTube,
[1336.06s -> 1340.90s]  although there will be some number of weak lag there.
[1340.90s -> 1344.04s]  And also, we'll offer this class next year,
[1344.04s -> 1346.64s]  so if you were not able to take it this year,
[1347.56s -> 1349.58s]  don't fret, there will be next time.
[1350.94s -> 1354.90s]  Okay, so the class has five assignments,
[1355.88s -> 1358.18s]  and each of the assignments,
[1358.22s -> 1361.14s]  we don't provide scaffolding code,
[1361.14s -> 1365.74s]  in a sense that you're literally give you a blank file,
[1365.74s -> 1368.20s]  and you're supposed to build things up,
[1369.42s -> 1373.74s]  and in the spirit of building from scratch.
[1373.74s -> 1375.98s]  But we're not that mean.
[1375.98s -> 1378.70s]  We do provide unit tests and some adapter interfaces
[1378.70s -> 1383.30s]  that allow you to check correctness of different pieces,
[1383.30s -> 1385.06s]  and also the assignment write-up,
[1385.06s -> 1385.90s]  if you walk through it,
[1385.90s -> 1389.10s]  it does do it for sort of a gentle job of doing that.
[1389.10s -> 1391.14s]  But you're kind of on your own
[1391.14s -> 1394.60s]  for making good software design decisions
[1394.60s -> 1396.42s]  and figuring out what you name your functions
[1396.42s -> 1398.40s]  and how to organize your code,
[1398.40s -> 1400.72s]  which is a useful skill, I think.
[1403.24s -> 1406.08s]  So one strategy, I think, for all assignments
[1406.08s -> 1407.70s]  is that there is a piece of assignment
[1407.70s -> 1410.38s]  which is just implement the thing
[1410.38s -> 1412.06s]  and make sure it's correct.
[1412.06s -> 1415.78s]  That, mostly you can do locally on your laptop.
[1416.62s -> 1417.66s]  You shouldn't need compute for that.
[1417.66s -> 1420.62s]  And then you should, we have a cluster
[1420.62s -> 1425.42s]  that you can run for benchmarking both accuracy and speed.
[1425.42s -> 1428.06s]  So I want everyone to kind of embrace this idea
[1428.06s -> 1432.02s]  of that you want to use as a small data set
[1432.02s -> 1433.54s]  or as few resources as possible
[1433.54s -> 1437.00s]  to prototype before running large jobs.
[1437.00s -> 1437.98s]  You shouldn't be debugging
[1437.98s -> 1441.10s]  with one billion parameter models on the cluster,
[1441.10s -> 1442.82s]  if you can help it.
[1442.82s -> 1446.90s]  Okay, there's some assignments
[1446.90s -> 1449.26s]  which will have a leaderboard,
[1449.26s -> 1451.78s]  which usually is of the form,
[1451.78s -> 1454.16s]  do things to make perplexity go down
[1454.16s -> 1455.94s]  given a particular training budget.
[1455.94s -> 1459.62s]  Last year it was, I think, pretty exciting
[1459.62s -> 1462.58s]  for people to try to, you know,
[1462.58s -> 1465.14s]  try different things that you either learn from the class
[1465.14s -> 1466.76s]  or you read online.
[1468.74s -> 1471.32s]  And then finally, I guess this year is,
[1472.16s -> 1474.76s]  you know, this was less of a problem last year
[1474.76s -> 1476.50s]  because I guess Copilot wasn't as good,
[1476.50s -> 1478.76s]  but you know, Curse is pretty good.
[1478.76s -> 1481.76s]  So I think our general strategy is that,
[1481.76s -> 1484.50s]  you know, AI tools are, you know,
[1484.50s -> 1486.04s]  can take away from learning
[1486.04s -> 1488.96s]  because there are cases where it can just solve the thing
[1488.96s -> 1490.80s]  you want it to do.
[1490.80s -> 1493.56s]  But, you know, I think you can obviously
[1493.56s -> 1496.24s]  use them judiciously, but use at your own risk.
[1496.24s -> 1497.30s]  You're kind of responsible
[1497.30s -> 1499.90s]  for your own learning experience here.
[1500.90s -> 1503.78s]  Okay, so we do have a cluster,
[1503.78s -> 1506.14s]  so thank you Together AI for providing
[1506.14s -> 1508.58s]  a bunch of H100s for us.
[1508.58s -> 1511.26s]  There's a guide to, please read it carefully
[1511.26s -> 1513.82s]  to learn how to use a cluster.
[1513.82s -> 1516.46s]  And start your assignments early
[1516.46s -> 1519.70s]  because the cluster will fill up
[1519.70s -> 1520.86s]  towards the end of a deadline
[1520.86s -> 1524.18s]  as everyone's trying to get their large runs in.
[1527.02s -> 1527.86s]  Okay.
[1528.62s -> 1530.54s]  Any questions about that?
[1530.54s -> 1532.42s]  You mentioned it was a five unit class.
[1532.42s -> 1533.98s]  Are we able to sign up for it
[1533.98s -> 1535.10s]  for like three to five units
[1535.10s -> 1536.82s]  and then notice that it was off?
[1536.82s -> 1537.82s]  Right, so the question is,
[1537.82s -> 1539.98s]  can you sign up for less than five units?
[1539.98s -> 1542.10s]  I think administratively,
[1542.10s -> 1543.54s]  if you have to sign up for less,
[1543.54s -> 1546.62s]  that is possible, but it's the same class
[1546.62s -> 1548.18s]  and the same workload, yeah.
[1550.56s -> 1552.66s]  Any other questions?
[1556.02s -> 1556.86s]  Okay.
[1557.86s -> 1561.02s]  So in this part, I'm gonna go through
[1561.02s -> 1563.26s]  all the different components of the course
[1563.26s -> 1565.10s]  and just give a broad overview,
[1565.10s -> 1568.34s]  a preview of what you're gonna experience.
[1568.34s -> 1570.26s]  So remember, it's all about efficiency,
[1570.26s -> 1572.82s]  given hardware and data,
[1572.82s -> 1576.42s]  how do you train the best model given your resources?
[1576.42s -> 1579.08s]  So for example, if I give you a common crawl dump,
[1579.08s -> 1581.78s]  a web dump, and 32 H100s for two weeks,
[1581.78s -> 1582.94s]  what should you do?
[1584.10s -> 1587.50s]  There are a lot of different design decisions.
[1588.02s -> 1589.54s]  There's questions about the tokenizer,
[1589.54s -> 1592.82s]  the architecture, systems optimizations you can do,
[1592.82s -> 1594.80s]  data things you can do,
[1594.80s -> 1597.06s]  and we've organized the class
[1597.06s -> 1600.78s]  into these five units or pillars.
[1600.78s -> 1604.34s]  So I'm gonna go through each of them in turn
[1605.80s -> 1607.48s]  and talk about what we'll cover,
[1607.48s -> 1610.42s]  what the assignment will involve,
[1610.42s -> 1612.54s]  and then I'll kind of wrap up.
[1613.46s -> 1615.94s]  Okay, so the goal of the basics unit
[1615.94s -> 1619.70s]  is just get a basic version of a full pipeline working.
[1619.70s -> 1622.42s]  So here you implement a tokenizer,
[1622.42s -> 1624.82s]  model architecture, and training.
[1624.82s -> 1625.98s]  So I'll just say a bit more
[1625.98s -> 1627.50s]  about what these components are.
[1627.50s -> 1632.50s]  So a tokenizer is something that converts between strings
[1633.02s -> 1635.34s]  and sequences of integers.
[1635.34s -> 1637.46s]  Intuitively, you can think about the integers
[1637.46s -> 1642.46s]  corresponding to breaking up the string into segments
[1642.78s -> 1645.26s]  and mapping each segment to an integer.
[1645.26s -> 1648.54s]  And the idea is that your sequence of integers
[1648.54s -> 1650.42s]  is what goes into the actual model,
[1650.42s -> 1652.80s]  which has to be like a fixed dimension.
[1654.00s -> 1655.28s]  Okay, so in this course,
[1655.28s -> 1660.14s]  we'll talk about the byte pair encoding BPE tokenizer,
[1660.14s -> 1665.14s]  which is relatively simple and still is used.
[1668.46s -> 1673.46s]  There are a promising set of methods
[1673.46s -> 1674.98s]  on tokenizer-free approaches.
[1675.54s -> 1679.22s]  So these are methods that just start with the raw bytes
[1679.22s -> 1681.54s]  and don't do tokenization,
[1681.54s -> 1683.68s]  and develop a particular architecture
[1683.68s -> 1685.30s]  that just takes the raw bytes.
[1686.72s -> 1689.98s]  This work is promising, but so far,
[1689.98s -> 1693.04s]  I haven't seen it being scaled to the frontier yet.
[1693.04s -> 1694.68s]  So we'll go with BPE for now.
[1695.98s -> 1699.66s]  Okay, so once you've tokenized your sequence or strings
[1699.66s -> 1702.26s]  into a sequence of integers,
[1702.26s -> 1704.18s]  now we define a model architecture
[1704.18s -> 1706.30s]  over these sequences.
[1706.30s -> 1710.82s]  So the starting point here is original transformer.
[1710.82s -> 1712.92s]  That's what is the backbone
[1712.92s -> 1716.60s]  of basically all frontier models.
[1718.10s -> 1720.90s]  And here's the architectural diagram.
[1720.90s -> 1722.20s]  We won't go into the details here,
[1722.20s -> 1725.80s]  but there's a tension piece,
[1725.80s -> 1730.80s]  and then there's a MLP layer with some normalization.
[1731.76s -> 1736.76s]  So a lot has actually happened since 2017.
[1737.72s -> 1740.32s]  I think there's a sort of sense to which,
[1740.32s -> 1741.60s]  oh, the transformer is invented,
[1741.60s -> 1743.76s]  and then everyone's just using the transformer.
[1743.76s -> 1745.60s]  And to a first approximation, that's true.
[1745.60s -> 1747.44s]  We're still using the same recipe,
[1747.44s -> 1751.92s]  but there have been a bunch of smaller improvements
[1751.92s -> 1753.96s]  that do make a substantial difference
[1753.96s -> 1755.26s]  when you add them all up.
[1755.26s -> 1759.16s]  So for example, there is the activation,
[1759.16s -> 1760.84s]  non-linear activation function,
[1760.84s -> 1763.44s]  so SWGLU, which we saw a little bit before.
[1763.44s -> 1766.50s]  Positional embeddings, there's new positional embeddings,
[1767.44s -> 1769.40s]  these rotary positional embeddings,
[1769.40s -> 1771.14s]  which we'll talk about.
[1772.36s -> 1776.32s]  Normalization, instead of using a layer norm,
[1776.32s -> 1778.24s]  we're gonna look at something called RMS norm,
[1778.24s -> 1780.68s]  which is similar but simpler.
[1780.68s -> 1782.84s]  There's a question of where you place the normalization,
[1782.84s -> 1786.32s]  which has been changed from the original transformer.
[1786.32s -> 1790.96s]  The MLP use, the canonical version is a dense MLP,
[1790.96s -> 1794.04s]  and you can replace that with mixture of experts.
[1794.04s -> 1797.16s]  Attention is something that has actually been
[1797.16s -> 1800.40s]  gaining a lot of attention, I guess.
[1800.40s -> 1803.60s]  There's full attention, and then there's
[1803.60s -> 1805.84s]  sliding window attention and linear attention.
[1805.84s -> 1809.60s]  All of these are trying to prevent a quadratic blowup.
[1809.60s -> 1812.68s]  There's also lower dimensional versions,
[1812.68s -> 1815.84s]  like GQA and MLA, which we'll get to
[1815.84s -> 1820.24s]  in a second, or not in a second, but in a future lecture.
[1820.24s -> 1823.32s]  And then the most kind of maybe radical thing
[1823.32s -> 1826.72s]  is other alternatives to the transformer,
[1826.72s -> 1828.72s]  like state-space models, like Hyena,
[1828.72s -> 1832.32s]  where they're not doing attention,
[1832.32s -> 1835.76s]  but some other sort of operation,
[1835.76s -> 1839.10s]  and sometimes you get best of both worlds
[1839.10s -> 1841.66s]  by mixing, making a hybrid model
[1841.66s -> 1844.10s]  that mixes these in with transformers.
[1846.04s -> 1848.00s]  Okay, so once you define your architecture,
[1848.00s -> 1852.88s]  you need a train, so design decisions include optimizer.
[1852.88s -> 1857.04s]  So AdamW, which is a variant, basically Adam fixed up,
[1858.44s -> 1862.42s]  is still very prominent, so we'll mostly work with that,
[1862.42s -> 1864.88s]  but it is worth mentioning that there is
[1864.88s -> 1867.22s]  more recent optimizers like Muon and SOAP
[1867.22s -> 1869.30s]  that have shown promise.
[1870.30s -> 1873.96s]  Learning rate schedule, batch size,
[1874.02s -> 1877.26s]  whether you do regularization or not, hyperparameters.
[1877.26s -> 1880.20s]  There's a lot of details here,
[1880.20s -> 1884.90s]  and I think this class is one where the details do matter
[1884.90s -> 1887.90s]  because you can easily have order of magnitude difference
[1887.90s -> 1890.64s]  between a well-tuned architecture
[1890.64s -> 1894.38s]  and something that's just like a vanilla transformer.
[1894.38s -> 1897.68s]  So in assignment one, basically you'll implement
[1897.68s -> 1900.02s]  the BPE tokenizer.
[1900.04s -> 1904.12s]  I'll warn you that this is actually the part
[1904.12s -> 1907.92s]  that seems to have been a lot of,
[1907.92s -> 1910.28s]  surprising maybe a lot of work for people,
[1910.28s -> 1914.02s]  so just, you know, you're warned.
[1914.02s -> 1916.96s]  And you also implement the transformer,
[1916.96s -> 1920.40s]  cross MP3 P loss, AdamW optimizer and training group.
[1920.40s -> 1924.70s]  So again, the whole stack, and we're not making you
[1924.70s -> 1927.48s]  implement PyTorch from scratch,
[1927.50s -> 1930.34s]  so you can use PyTorch, but you can't use
[1930.34s -> 1934.58s]  the transformer implementation for PyTorch.
[1934.58s -> 1938.48s]  There's a small list of functions that you can use
[1938.48s -> 1940.62s]  and you can only use those.
[1940.62s -> 1944.78s]  Okay, so we're gonna have some tiny stories
[1944.78s -> 1947.22s]  and open web text data sets that you'll train on,
[1947.22s -> 1949.62s]  and then there will be a leaderboard
[1949.62s -> 1952.32s]  to minimize the open web text perplexity.
[1952.32s -> 1955.16s]  We'll give you 90 minutes on a H100
[1955.16s -> 1956.54s]  and see what you can do.
[1956.56s -> 1961.46s]  So this is last year, so see, we have the top,
[1961.46s -> 1964.50s]  so this is the number to beat for this year.
[1964.50s -> 1965.34s]  Okay.
[1966.94s -> 1969.50s]  All right, so that's the basics.
[1969.50s -> 1974.38s]  Now, after basics, I mean, in some sense you're done,
[1974.38s -> 1978.12s]  right, like you have ability to train a transformer.
[1978.12s -> 1980.22s]  What else do you need?
[1980.22s -> 1985.22s]  So the system part really goes into how you can optimize
[1986.10s -> 1988.98s]  this further, so how do you get the most out of hardware?
[1988.98s -> 1991.34s]  And for this, we need to take a closer look
[1991.34s -> 1995.38s]  at the hardware and how we can leverage it.
[1995.38s -> 1997.82s]  So there's kernels, parallelism, and inference
[1997.82s -> 2000.88s]  are the three components of this unit.
[2000.88s -> 2005.18s]  So, okay, so to first talk about kernels,
[2005.18s -> 2008.46s]  let's talk a little bit about what a GPU looks like.
[2008.46s -> 2013.36s]  Okay, so a GPU, which we'll get much more into,
[2013.36s -> 2018.08s]  is basically a huge array of these, you know,
[2018.08s -> 2022.80s]  little units that do floating point operations.
[2024.02s -> 2027.28s]  And maybe the one thing to note is that
[2027.28s -> 2032.28s]  this is the GPU chip, and here is the memory
[2032.56s -> 2034.38s]  that's actually off-chip.
[2034.38s -> 2036.52s]  And then there's some other memory,
[2036.52s -> 2040.18s]  like L2 caches and L1 caches on-chip.
[2040.18s -> 2045.18s]  And so the basic idea is that compute has to happen here.
[2045.54s -> 2047.68s]  Your data might be somewhere else,
[2047.68s -> 2050.28s]  and how do you basically organize your compute
[2050.28s -> 2054.50s]  so that you can be most efficient?
[2054.50s -> 2059.50s]  So one quick analogy is imagine that your memory is,
[2061.14s -> 2064.18s]  where you can store your data and model parameters,
[2064.18s -> 2069.18s]  is like a warehouse, and your compute is like the factory.
[2070.26s -> 2074.56s]  And what ends up being a big bottleneck
[2074.56s -> 2078.46s]  is just data movement costs, right?
[2078.46s -> 2081.94s]  So the thing that we have to do is
[2081.94s -> 2083.82s]  how do you organize the compute,
[2083.82s -> 2085.86s]  like even a matrix multiplication,
[2085.86s -> 2089.78s]  to maximize the utilization of the GPUs
[2089.78s -> 2092.16s]  by minimizing the data movement?
[2092.16s -> 2094.38s]  And there's a bunch of techniques like fusion
[2094.38s -> 2098.26s]  and tiling that allow you to do that.
[2098.26s -> 2100.58s]  So we'll get all into the details of that.
[2100.58s -> 2104.66s]  And to implement and leverage a kernel,
[2104.66s -> 2106.26s]  we're gonna look at Triton.
[2106.26s -> 2107.42s]  There's other things you can do
[2107.42s -> 2110.14s]  with various levels of sophistication,
[2110.14s -> 2112.34s]  but we're gonna use Triton, which is developed
[2112.34s -> 2115.54s]  by OpenAI and a popular way to build kernels.
[2115.54s -> 2117.22s]  Okay, so we're gonna write some kernels.
[2117.22s -> 2118.82s]  That's for one GPU.
[2118.82s -> 2122.98s]  So now, in general, you have these big runs
[2122.98s -> 2125.98s]  take, you know, tens, thousands,
[2126.02s -> 2129.42s]  if not tens of thousands of GPUs.
[2129.42s -> 2131.90s]  But even at eight, it kind of starts becoming interesting
[2131.90s -> 2134.50s]  because you have a lot of GPUs,
[2134.50s -> 2137.14s]  they're connected to some CPU nodes,
[2137.14s -> 2141.30s]  and they also are directly connected via NVSwitch, NVLink.
[2145.98s -> 2147.98s]  It's the same idea, right?
[2147.98s -> 2149.58s]  Now, the only thing is that data movement
[2149.58s -> 2152.08s]  between GPUs is even slower.
[2152.60s -> 2156.76s]  And so we need to figure out how to put model parameters
[2159.76s -> 2163.28s]  and activations and gradients and put them on the GPUs
[2163.28s -> 2167.44s]  and do the computation to minimize the amount of movement.
[2168.72s -> 2170.80s]  And then, so we're gonna explore different
[2170.80s -> 2173.12s]  type of techniques like data parallelism
[2173.12s -> 2176.24s]  and tensor parallelism and so on.
[2176.24s -> 2181.08s]  So that's all I'll say about that.
[2181.12s -> 2184.96s]  And finally, inference is something that we didn't
[2184.96s -> 2187.68s]  actually do last year in the class,
[2188.68s -> 2190.60s]  although we had a guest lecture.
[2190.60s -> 2194.88s]  But this is important because inference
[2194.88s -> 2197.44s]  is how you actually use a model, right?
[2197.44s -> 2200.04s]  It's basically the task of generating tokens
[2200.04s -> 2202.84s]  given a prompt, given a trained model.
[2202.84s -> 2205.64s]  And it also turns out to be really useful
[2205.64s -> 2208.48s]  for a bunch of other things besides just chatting
[2208.48s -> 2212.12s]  with your favorite model.
[2212.12s -> 2213.84s]  You need it for reinforcement learning,
[2213.84s -> 2217.80s]  test time compute, which has been very popular lately,
[2217.80s -> 2219.52s]  and even evaluating models.
[2219.52s -> 2221.44s]  You need to do inference.
[2221.44s -> 2224.84s]  So we're gonna spend some time talking about inference.
[2224.84s -> 2227.20s]  Actually, if you think about globally,
[2227.20s -> 2231.16s]  the cost that's spent on inference
[2231.16s -> 2236.16s]  is eclipsing the cost that it is used to train models.
[2237.16s -> 2240.56s]  Because training, despite it being very intensive,
[2240.56s -> 2242.28s]  is ultimately a one-time cost,
[2242.28s -> 2246.44s]  and inference is cost scales with every use.
[2246.44s -> 2249.48s]  And the more people use your model,
[2249.48s -> 2252.40s]  the more you'll need inference to be efficient.
[2253.28s -> 2258.28s]  Okay, so in inference, there's two phases.
[2259.20s -> 2260.52s]  There's a pre-fill and a decode.
[2260.52s -> 2262.48s]  Pre-fill is you take the prompt
[2262.48s -> 2263.96s]  and you can run it through the model
[2263.96s -> 2266.68s]  and get some activations.
[2266.68s -> 2269.96s]  And then decode is you go autoregressively one by one
[2269.96s -> 2272.08s]  and generate tokens.
[2272.08s -> 2275.16s]  So pre-fill, all the tokens are given
[2275.16s -> 2276.88s]  so you can process everything at once.
[2276.88s -> 2280.16s]  So this is exactly what you see at training time.
[2280.16s -> 2282.76s]  And generally, this is a good setting to be in
[2282.76s -> 2286.48s]  because it's naturally parallel
[2286.48s -> 2288.16s]  and you're mostly compute bound.
[2288.16s -> 2290.80s]  What makes inference, I think, special and difficult
[2290.80s -> 2293.20s]  is that this autoregressive decoding,
[2293.20s -> 2295.12s]  you need to generate one token at a time,
[2295.12s -> 2297.84s]  and it's hard to actually saturate all your GPUs
[2297.84s -> 2299.40s]  and it becomes memory bound
[2299.40s -> 2302.56s]  because you're constantly moving data around.
[2302.56s -> 2306.60s]  And we'll talk about a few ways to speed the models up,
[2306.60s -> 2308.48s]  just speed inference up.
[2308.48s -> 2310.48s]  You can use a cheaper model.
[2310.48s -> 2312.60s]  You can use this really cool technique
[2312.60s -> 2314.68s]  called speculative decoding where you use a cheaper model
[2314.68s -> 2319.68s]  to sort of scout ahead and to generate multiple tokens.
[2319.88s -> 2322.48s]  And then if these tokens happen to be good
[2323.28s -> 2325.08s]  for some definition good,
[2325.08s -> 2328.64s]  you can have the full model just score in
[2328.64s -> 2331.12s]  and accept them all in parallel.
[2332.60s -> 2334.52s]  And then there's a bunch of systems optimizations
[2334.52s -> 2335.88s]  that you can do as well.
[2337.00s -> 2340.88s]  Okay, so after the systems, oh, okay, assignment two.
[2340.88s -> 2344.88s]  So you're gonna implement a kernel.
[2344.88s -> 2348.04s]  You're gonna implement some parallelism.
[2348.04s -> 2353.04s]  So data parallel is very natural,
[2353.20s -> 2354.88s]  and so we'll do that.
[2356.64s -> 2359.80s]  Some of the model parallelism, like FSDP,
[2359.80s -> 2363.52s]  turns out to be a bit kind of complicated
[2363.52s -> 2365.04s]  to do from scratch, so we'll do
[2365.04s -> 2367.08s]  sort of a baby version of that.
[2368.08s -> 2372.36s]  But I encourage you to learn about the full version.
[2372.36s -> 2374.16s]  We'll go over the full version in class,
[2374.16s -> 2378.12s]  but implementing from scratch might be a bit too much.
[2379.96s -> 2381.32s]  And then I think an important thing
[2381.32s -> 2384.48s]  is getting in the habit of always benchmarking profile.
[2384.48s -> 2388.04s]  I think that's actually probably the most important thing
[2388.04s -> 2389.64s]  is that you can implement things,
[2389.64s -> 2392.72s]  but unless you have feedback
[2392.72s -> 2394.60s]  on how well your implementation is going
[2394.60s -> 2395.96s]  and where the bottlenecks are,
[2395.96s -> 2398.16s]  you're just gonna be kind of flying blind.
[2399.16s -> 2404.16s]  Okay, so unit three is scaling laws.
[2406.48s -> 2409.20s]  And here the goal is you wanna do experiments
[2409.20s -> 2411.40s]  at small scale and figure things out
[2411.40s -> 2414.36s]  and then predict the hyperparameters
[2414.36s -> 2417.20s]  and lots at large scale.
[2417.20s -> 2421.12s]  So here's a fundamental question.
[2421.12s -> 2424.76s]  So if I give you a FLOPS budget,
[2424.76s -> 2427.28s]  what model size should you use?
[2427.28s -> 2428.32s]  If you use a larger model,
[2428.32s -> 2429.92s]  that means you can train on less data,
[2429.92s -> 2431.00s]  and if you use a smaller model,
[2431.00s -> 2432.28s]  you can train on more data.
[2432.28s -> 2434.28s]  So what's the right balance here?
[2434.28s -> 2437.60s]  And this has been studied quite extensively
[2437.60s -> 2439.52s]  and figured out by a series of paper
[2439.52s -> 2441.68s]  from OpenAI and DeepMind.
[2441.68s -> 2444.36s]  So if you hear the term Chinchilla optimal,
[2444.36s -> 2446.52s]  this is what this is referring to.
[2446.52s -> 2451.44s]  And the basic idea is that for every compute budget
[2451.44s -> 2454.64s]  number of FLOPS, you can vary the number
[2454.64s -> 2457.20s]  of parameters of your model.
[2457.96s -> 2460.40s]  And then you measure how good that model is.
[2460.40s -> 2462.28s]  So for every level of compute,
[2462.28s -> 2467.28s]  you can get the optimal parameter count.
[2467.44s -> 2471.88s]  And then what you do is you can fit a curve
[2471.88s -> 2475.68s]  to extrapolate and see if you had,
[2475.68s -> 2479.04s]  let's say, one E22 FLOPS,
[2479.04s -> 2480.60s]  what would be the parameter size?
[2480.60s -> 2482.20s]  And it turns out these minimum,
[2482.20s -> 2486.24s]  when you plot them, it's actually remarkably linear.
[2487.32s -> 2491.40s]  Which leads to this very actually simple
[2491.40s -> 2493.68s]  but useful rule of thumb,
[2493.68s -> 2498.68s]  which is that if you have a particular model of size N,
[2502.00s -> 2503.60s]  if you multiply by 20,
[2503.60s -> 2505.96s]  that's the number of tokens you should train on,
[2505.96s -> 2507.00s]  essentially.
[2507.00s -> 2511.08s]  So that means if I say 1.4 billion parameter model
[2511.08s -> 2513.72s]  should be trained on 28 billion tokens.
[2514.56s -> 2515.60s]  Okay?
[2515.60s -> 2518.00s]  But this doesn't take into account inference cost.
[2518.00s -> 2521.76s]  This is literally how can you train the best model
[2521.76s -> 2523.88s]  regardless of how big that model is.
[2523.88s -> 2525.08s]  So there's some limitations here,
[2525.08s -> 2527.36s]  but it's nonetheless been extremely useful
[2527.36s -> 2528.56s]  for model development.
[2529.48s -> 2533.04s]  So in this assignment, this is kind of fun
[2533.04s -> 2536.88s]  because we define a quote-unquote training API,
[2536.88s -> 2539.40s]  which you can query with a particular set
[2539.40s -> 2540.24s]  of hyperparameters.
[2540.24s -> 2544.96s]  You specify the architecture and batch size and so on.
[2544.96s -> 2549.96s]  And we return you a loss that your decisions will get you.
[2550.76s -> 2554.88s]  Okay, so your job is you have a FLOPS budget
[2554.88s -> 2556.72s]  and you're gonna try to figure out
[2556.72s -> 2559.32s]  how to train a bunch of models
[2559.32s -> 2561.00s]  and then gather the data.
[2561.00s -> 2564.24s]  You're gonna fit a scaling law to the gathered data.
[2564.24s -> 2567.04s]  And then you're going to submit your prediction
[2567.04s -> 2570.88s]  on what you would choose to be the hyperparameters,
[2570.88s -> 2575.76s]  what model size and so on at a larger scale.
[2576.64s -> 2577.48s]  Okay?
[2577.48s -> 2579.52s]  So this is a case where you have to be really,
[2579.52s -> 2581.56s]  we wanna put you in this position
[2581.56s -> 2583.68s]  where there's some stakes.
[2583.68s -> 2586.24s]  I mean, this is not like burning real compute,
[2586.24s -> 2590.32s]  but once you run out of your FLOPS budget, that's it.
[2590.32s -> 2592.52s]  So you have to be very careful
[2592.52s -> 2596.96s]  in terms of how you prioritize what experiments to run,
[2596.96s -> 2598.52s]  which is something that the frontier labs
[2598.52s -> 2600.60s]  have to do all the time.
[2600.60s -> 2602.36s]  And there will be a leaderboard for this,
[2602.36s -> 2603.80s]  which is minimize FLOPS,
[2603.80s -> 2606.00s]  minimize loss given your FLOPS budget.
[2608.48s -> 2609.32s]  Question?
[2610.28s -> 2613.84s]  So those are links from 2024.
[2613.84s -> 2615.68s]  So if we're working ahead,
[2615.68s -> 2618.80s]  should we expect assignments to change over time,
[2618.80s -> 2621.20s]  or are these gonna be the final assignments right now?
[2621.20s -> 2625.36s]  Yeah, so the question is that these links are from 2024.
[2625.36s -> 2627.00s]  The rough assignments,
[2627.00s -> 2630.20s]  the rough structure will be the same from 2025.
[2630.20s -> 2631.80s]  There will be some modifications,
[2631.80s -> 2632.64s]  but if you look at these,
[2632.64s -> 2635.16s]  you should have a pretty good idea of what to expect.
[2638.80s -> 2642.04s]  Okay, so let's go into data now.
[2642.96s -> 2646.88s]  Okay, so up until now, you have scaling laws,
[2646.88s -> 2647.72s]  you have systems,
[2647.72s -> 2651.40s]  you have your transformer implementation, everything.
[2651.40s -> 2653.48s]  You're really kind of good to go.
[2653.48s -> 2657.84s]  But data, I would say, is a really kind of key ingredient
[2657.84s -> 2661.28s]  that I think differentiates in some sense.
[2661.28s -> 2664.36s]  And the question to ask here is,
[2664.36s -> 2666.76s]  what do I want this model to do?
[2666.76s -> 2669.08s]  Right, because what the model does
[2669.08s -> 2673.12s]  is mostly determined by the data.
[2673.12s -> 2675.56s]  If I train on multilingual data,
[2675.56s -> 2677.08s]  it will have multilingual capabilities.
[2677.08s -> 2680.16s]  If I train on code, it'll have code capabilities.
[2680.16s -> 2682.32s]  It's very natural.
[2682.32s -> 2685.36s]  And usually data sets are a conglomeration
[2685.36s -> 2687.12s]  of a lot of different pieces.
[2687.12s -> 2689.44s]  There's, you know, this is from a pile,
[2689.44s -> 2691.36s]  which is four years ago,
[2691.36s -> 2693.68s]  but the same idea, I think, holds.
[2693.68s -> 2696.24s]  You know, you have data from the web.
[2696.24s -> 2698.36s]  This is Common Crawl.
[2698.36s -> 2701.08s]  You have SAC Exchange, Wikipedia, GitHub,
[2701.08s -> 2703.80s]  and different sources which are curated.
[2704.72s -> 2707.16s]  And so in the data section,
[2707.16s -> 2709.60s]  we're gonna start talking about evaluation,
[2709.60s -> 2710.60s]  which is, given a model,
[2710.60s -> 2713.56s]  how do you evaluate whether it's any good?
[2713.56s -> 2717.20s]  So we're gonna talk about perplexity measures,
[2717.20s -> 2720.40s]  standard kind of standardized testing, like MMLU.
[2721.64s -> 2725.08s]  If you have models that generate utterances
[2725.08s -> 2728.20s]  for instruction following, how do you evaluate that?
[2728.20s -> 2733.00s]  There's also decisions about if you can ensemble
[2733.00s -> 2735.56s]  or do chain of thought at test time,
[2735.56s -> 2738.44s]  you know, how does that affect your evaluation?
[2738.48s -> 2743.48s]  And then, you know, you can talk about entire systems,
[2743.56s -> 2746.00s]  evaluation of entire system, not just the language model,
[2746.00s -> 2748.48s]  because language models often get, these days,
[2748.48s -> 2750.88s]  plugged into some agentic system or something.
[2753.72s -> 2757.84s]  Okay, so now, after establishing evaluation,
[2757.84s -> 2760.16s]  let's look at data curation.
[2760.16s -> 2762.44s]  So this is, I think, an important point
[2762.44s -> 2763.64s]  that people don't realize.
[2763.64s -> 2766.04s]  I often hear people say,
[2766.12s -> 2769.64s]  oh, we're training the model on the internet.
[2769.64s -> 2771.92s]  This just doesn't make sense, right?
[2771.92s -> 2775.72s]  Data doesn't just fall from the sky
[2775.72s -> 2776.56s]  and there's the internet
[2776.56s -> 2780.08s]  that you can pipe into your model.
[2781.52s -> 2785.72s]  Data has to always be actively acquired somehow.
[2786.60s -> 2791.60s]  So even if you, just as an example of,
[2792.60s -> 2794.68s]  I always tell people, look at the data.
[2795.52s -> 2797.16s]  And so let's look at some data.
[2797.16s -> 2802.16s]  So this is some Common Crawl data.
[2802.56s -> 2804.56s]  I'm gonna take 10 documents
[2804.56s -> 2807.28s]  and I think, hopefully, this works.
[2807.28s -> 2809.24s]  Okay, I think the rendering is off,
[2809.24s -> 2812.40s]  but you can kind of see,
[2813.48s -> 2818.48s]  this is a sort of random sample of Common Crawl.
[2819.08s -> 2824.08s]  And you can see that this is maybe
[2827.84s -> 2829.80s]  not exactly the data.
[2829.80s -> 2831.64s]  Oh, here's some actually real text here.
[2831.64s -> 2833.04s]  Okay, that's cool.
[2833.04s -> 2835.76s]  But if you look at most of Common Crawl,
[2835.76s -> 2836.76s]  this is a different language,
[2836.76s -> 2839.88s]  but you can also see this as very spammy sites.
[2839.88s -> 2842.04s]  And you'll quickly realize that
[2842.04s -> 2845.62s]  a lot of the web is just trash.
[2845.62s -> 2849.62s]  And so, well, okay, maybe that's not that surprising,
[2849.62s -> 2852.18s]  but it's more trash than you would actually expect,
[2852.18s -> 2853.70s]  I promise.
[2853.70s -> 2857.54s]  So what I'm saying is that there's a lot of work
[2857.54s -> 2859.38s]  that needs to happen in data.
[2859.38s -> 2861.38s]  So you can crawl the internet,
[2861.38s -> 2865.34s]  you can take books, archives, papers, GitHub,
[2866.62s -> 2868.10s]  and there's actually a lot of processing
[2868.10s -> 2869.18s]  that needs to happen.
[2870.58s -> 2872.78s]  There's also legal questions about what data
[2872.78s -> 2875.32s]  you can train on, which we'll touch on.
[2876.22s -> 2878.46s]  Nowadays, a lot of frontier models
[2878.46s -> 2880.78s]  have to actually buy data
[2880.78s -> 2882.38s]  because the data on the internet
[2882.38s -> 2886.14s]  that's publicly accessible is actually,
[2886.14s -> 2888.74s]  turns out to be a bit limited
[2888.74s -> 2893.14s]  for the really frontier performance.
[2893.14s -> 2895.18s]  And also, I think it's important to remember
[2895.18s -> 2896.98s]  that this data that's scraped,
[2896.98s -> 2899.10s]  it's not actually text, right?
[2899.10s -> 2901.58s]  First of all, it's HTML, or it's PDFs,
[2901.58s -> 2904.14s]  or in the case of code, it's just directories.
[2904.14s -> 2906.74s]  So there has to be an explicit process
[2906.74s -> 2910.58s]  that takes this data and turns it into text.
[2910.58s -> 2912.98s]  Okay, so we're gonna talk about the transformation
[2912.98s -> 2916.30s]  from HTML to text.
[2917.24s -> 2920.84s]  And this is gonna be a lossy process.
[2920.84s -> 2925.06s]  So the trick is how can you preserve the content
[2925.06s -> 2926.50s]  and some of the structure
[2927.42s -> 2932.20s]  without basically just having an HTML.
[2933.20s -> 2936.16s]  Filtering, as you could surmise,
[2936.16s -> 2937.88s]  is going to be very important,
[2937.88s -> 2939.36s]  both for getting high-quality data
[2939.36s -> 2941.92s]  but also removing harmful content.
[2941.92s -> 2944.88s]  Generally, people train classifiers to do this.
[2944.88s -> 2948.60s]  The duplication is also an important step,
[2948.60s -> 2950.08s]  which we'll talk about.
[2950.08s -> 2952.68s]  Okay, so assignment four is all about data.
[2952.68s -> 2957.02s]  We're gonna give you the raw common crawl dump,
[2957.02s -> 2959.92s]  so you can see just how bad it is.
[2959.96s -> 2963.60s]  And you're gonna train classifiers, dedupe,
[2963.60s -> 2964.92s]  and then there's gonna be a leaderboard
[2964.92s -> 2968.68s]  where you're gonna try to minimize perplexity
[2968.68s -> 2969.92s]  given your token budget.
[2970.80s -> 2974.26s]  So now let's, now you have the data,
[2974.26s -> 2976.88s]  you've built all your fancy kernels,
[2976.88s -> 2979.76s]  you've trained, now you can really train models.
[2979.76s -> 2981.96s]  But at this point, what you'll get
[2981.96s -> 2986.96s]  is a model that can complete the next token, right?
[2987.76s -> 2990.84s]  And this is called essentially a base model,
[2990.84s -> 2992.12s]  and I think about it as a model
[2992.12s -> 2993.60s]  that has a lot of raw potential,
[2993.60s -> 2997.44s]  but it needs to be aligned or modified some way.
[2997.44s -> 3000.04s]  And alignment is a process of making it useful.
[3000.04s -> 3005.04s]  So alignment captures a lot of different things,
[3005.04s -> 3007.88s]  but three things I think it captures
[3007.88s -> 3009.76s]  is that you want to get the language model
[3009.76s -> 3011.96s]  to follow instructions, right?
[3011.96s -> 3013.00s]  Completing the next token
[3013.00s -> 3015.22s]  is not necessarily following the instruction.
[3015.30s -> 3017.14s]  Just complete the instruction
[3017.14s -> 3019.66s]  or whatever it thinks will follow the instruction.
[3021.02s -> 3026.02s]  You get to here specify the style of the generation,
[3026.10s -> 3028.14s]  whether you want it to be long or short,
[3028.14s -> 3029.54s]  whether you want bullets,
[3029.54s -> 3034.08s]  whether you want it to be witty or have sass or not.
[3034.08s -> 3038.86s]  And when you play with chatgbt versus croc,
[3038.86s -> 3041.74s]  you'll see that there's different alignment
[3041.74s -> 3043.50s]  that has happened.
[3043.50s -> 3045.54s]  And then also safety.
[3045.54s -> 3047.38s]  One important thing is for these models
[3047.38s -> 3051.20s]  to be able to refuse answers that can be harmful,
[3051.20s -> 3053.78s]  so that's where alignment also kicks in.
[3053.78s -> 3057.38s]  So there's generally two phases of alignment.
[3057.38s -> 3059.46s]  There's supervised fine-tuning,
[3059.46s -> 3062.78s]  and here the goal is, I mean, it's very simple.
[3062.78s -> 3067.78s]  You basically gather a set of user assistant pairs,
[3069.86s -> 3071.74s]  so prompt response pairs,
[3071.78s -> 3076.18s]  and then you do supervised learning, okay?
[3076.18s -> 3079.02s]  And the idea here is that the base model
[3079.02s -> 3082.26s]  already has sort of the raw potential,
[3082.26s -> 3087.26s]  so just fine-tuning it on a few examples is sufficient.
[3087.82s -> 3089.14s]  Of course, the more examples you have,
[3089.14s -> 3090.90s]  the better the results,
[3090.90s -> 3092.98s]  but there's papers like this one
[3092.98s -> 3095.58s]  that shows even like a thousand examples
[3095.58s -> 3098.90s]  suffices to give you instruction-following capabilities
[3098.90s -> 3100.38s]  from a good base model.
[3101.30s -> 3103.90s]  Okay, so this part is actually very simple,
[3103.90s -> 3107.10s]  and it's not that different from pre-training,
[3107.10s -> 3108.90s]  because it's just you're given text
[3108.90s -> 3111.84s]  and you just maximize the probability of the text.
[3113.90s -> 3116.14s]  So the second part is a bit more interesting
[3116.14s -> 3118.50s]  from an algorithmic perspective.
[3118.50s -> 3122.14s]  So the idea here is that even with the SFT phase,
[3122.14s -> 3125.70s]  you will have a decent model.
[3125.70s -> 3127.14s]  And now, how do you improve it?
[3127.14s -> 3128.86s]  Well, you can get more SFT data,
[3128.86s -> 3130.22s]  but that can be very expensive,
[3131.02s -> 3134.42s]  because you have to have someone sit down and annotate data.
[3134.42s -> 3138.82s]  So the goal of learning from feedback
[3138.82s -> 3142.46s]  is that you can leverage lighter forms of annotation
[3143.66s -> 3146.66s]  and have the algorithms do a bit more work.
[3146.66s -> 3148.98s]  Okay, so one type of data you can learn from
[3148.98s -> 3150.48s]  is preference data.
[3150.48s -> 3154.26s]  So this is where you generate multiple responses
[3154.26s -> 3156.78s]  from a model to a given prompt, like A or B,
[3156.78s -> 3160.30s]  and the user rates whether A or B is better.
[3160.30s -> 3163.82s]  And so the data might look like it generates
[3164.76s -> 3166.62s]  what's the best way to train a language model,
[3166.62s -> 3169.02s]  use a large data set or use a small data set,
[3169.02s -> 3171.38s]  and of course the answer should be A.
[3171.38s -> 3176.26s]  So that is a unit of expressing preferences.
[3176.26s -> 3178.62s]  Another type of supervision you could have
[3178.62s -> 3180.18s]  is using verifiers.
[3180.18s -> 3182.90s]  So for some domains, you're lucky enough to have
[3182.90s -> 3185.90s]  a formal verifier, like for math or code,
[3185.94s -> 3189.54s]  or you can use learn verifiers where you train
[3189.54s -> 3193.54s]  an actual language model to rate the response.
[3196.74s -> 3199.58s]  And of course this relates to evaluation again.
[3200.62s -> 3203.82s]  Algorithms, this is, we're in the realm
[3203.82s -> 3205.30s]  of reinforcement learning.
[3205.30s -> 3210.30s]  So one of the earliest algorithms that was developed
[3210.40s -> 3215.40s]  that was applied to instruction tuning models was PPO.
[3216.10s -> 3218.02s]  Proximal Policy Optimization.
[3219.54s -> 3221.90s]  It turns out that if you just have preference data,
[3221.90s -> 3224.66s]  there's a much simpler algorithm called DPO
[3224.66s -> 3225.98s]  that works really well.
[3227.38s -> 3229.94s]  But in general, if you wanted to learn
[3229.94s -> 3231.70s]  from verifiers data, you have to,
[3231.70s -> 3235.30s]  it's not preference data, so you have to embrace RL fully
[3235.30s -> 3240.30s]  and there's this method which we'll do in this class
[3241.50s -> 3244.74s]  which called group relative preference optimization
[3244.74s -> 3247.18s]  which simplifies PPO and makes it more efficient
[3247.18s -> 3249.70s]  by removing the value function developed by DeepSeq
[3249.70s -> 3251.82s]  which seems to work pretty well.
[3253.90s -> 3257.38s]  Okay, so assignment five implements supervised tuning,
[3257.38s -> 3261.28s]  DPO and GRPO, and of course evaluate.
[3262.96s -> 3263.80s]  Question?
[3264.94s -> 3267.34s]  You gave me, I think it's kind of a quote
[3267.34s -> 3269.54s]  from the course evaluation about assignment one.
[3269.54s -> 3271.02s]  Did people have similar things to say
[3271.02s -> 3273.34s]  about assignments two through five or?
[3273.58s -> 3277.34s]  The question is, assignment one seems a bit daunting.
[3277.34s -> 3278.74s]  What about the other ones?
[3278.74s -> 3281.02s]  I would say that assignment one and two
[3281.02s -> 3283.78s]  are definitely the most heavy and hardest.
[3284.92s -> 3289.50s]  Assignment three is a bit more of a breather
[3289.50s -> 3292.18s]  and assignment four and five, at least last year,
[3292.18s -> 3297.14s]  were I would say a notch below assignment one and two.
[3297.14s -> 3298.92s]  Although, I don't know, it depends on,
[3298.92s -> 3301.78s]  we haven't fully worked out the details for this.
[3303.86s -> 3306.62s]  Yeah, it does get better.
[3308.46s -> 3313.02s]  Okay, so just to recap of the different pieces here.
[3314.92s -> 3317.38s]  Remember efficiency is this driving principle.
[3318.26s -> 3321.38s]  And there's a bunch of different design decisions.
[3321.38s -> 3326.16s]  And you can, I think if you view efficiency,
[3326.16s -> 3327.54s]  everything through a lens of efficiency,
[3327.54s -> 3329.96s]  I think a lot of things kind of make sense.
[3330.96s -> 3334.76s]  And importantly, I think we are,
[3334.76s -> 3336.32s]  it's worth pointing out there,
[3336.32s -> 3340.02s]  we are currently in this compute constrained regime,
[3340.02s -> 3340.88s]  at least this class,
[3340.88s -> 3343.80s]  and most people who are somewhat GPU poor.
[3343.80s -> 3344.92s]  So we have a lot of data,
[3344.92s -> 3346.88s]  but we don't have that much compute.
[3346.88s -> 3349.60s]  And so these design decisions will reflect
[3349.60s -> 3351.20s]  squeezing the most out of the hardware.
[3351.20s -> 3353.00s]  So for example, data processing.
[3353.00s -> 3355.08s]  We're filtering fairly aggressively
[3355.08s -> 3356.84s]  because we don't want to waste precious compute
[3356.84s -> 3359.58s]  on bad or irrelevant data.
[3360.10s -> 3364.50s]  Tokenization, like it's nice to have a model over bytes
[3364.50s -> 3365.34s]  that's very elegant,
[3365.34s -> 3367.08s]  but it's very compute inefficient
[3367.08s -> 3369.34s]  with today's model architectures.
[3369.34s -> 3371.38s]  So we have to do tokenization too,
[3371.38s -> 3373.46s]  as an efficiency gain.
[3373.46s -> 3376.32s]  Model architecture, there are a lot of design decisions
[3376.32s -> 3378.50s]  there that are essentially motivated
[3378.50s -> 3381.40s]  by efficiency training.
[3381.40s -> 3384.10s]  I think the fact that most of what we're doing to do
[3384.10s -> 3386.06s]  is just a single epoch.
[3386.06s -> 3388.42s]  This is clearly, we're in a hurry.
[3388.42s -> 3390.38s]  We just need to see more data
[3390.38s -> 3393.70s]  as opposed to spend a lot of time on any given data point.
[3393.70s -> 3396.18s]  Scaling laws is completely about efficiency.
[3396.18s -> 3399.54s]  We use less compute to figure out the hyperparameters.
[3399.54s -> 3404.54s]  And alignment is maybe a little bit different,
[3405.54s -> 3408.98s]  but the connection to efficiency is that
[3408.98s -> 3413.04s]  if you can put resources into alignment,
[3413.04s -> 3418.04s]  then you actually require smaller base models.
[3419.30s -> 3422.26s]  So there's sort of two paths.
[3422.26s -> 3425.26s]  If your use case is fairly narrow,
[3425.26s -> 3426.82s]  you can probably use a smaller model,
[3426.82s -> 3428.86s]  you align it or fine tune it,
[3428.86s -> 3430.30s]  and you can do well.
[3430.30s -> 3432.74s]  But if your use cases are very broad,
[3432.74s -> 3434.74s]  then there might not be a substitute
[3434.74s -> 3436.50s]  for training a big model.
[3437.58s -> 3439.18s]  So that's today.
[3439.18s -> 3443.98s]  So increasingly now, at least for frontier labs,
[3443.98s -> 3446.10s]  they're becoming data constrained.
[3446.10s -> 3448.18s]  Which is interesting because I think
[3448.78s -> 3452.54s]  the design decisions will presumably completely change.
[3452.54s -> 3455.18s]  Well, I mean, compute will always be important,
[3455.18s -> 3457.22s]  but I think the design decisions will change.
[3457.22s -> 3462.22s]  For example, taking one epoch of your data
[3462.50s -> 3464.58s]  I think doesn't really make sense
[3464.58s -> 3465.74s]  if you have more compute.
[3465.74s -> 3467.30s]  Why wouldn't you take more epochs at least
[3467.30s -> 3469.54s]  or do something smarter?
[3469.54s -> 3474.22s]  Or maybe there will be different architectures, for example,
[3474.22s -> 3476.62s]  because a transformer was really motivated
[3476.62s -> 3478.66s]  by compute efficiency.
[3479.98s -> 3482.18s]  So that's something to kind of ponder.
[3482.18s -> 3483.54s]  Still it's about efficiency,
[3483.54s -> 3486.46s]  but the design decisions reflect what regime you're in.
[3489.74s -> 3494.74s]  Okay, so now I'm going to dive into the first unit.
[3498.06s -> 3500.10s]  Before that, any questions?
[3500.10s -> 3505.10s]  We have a slide for Ed.
[3508.18s -> 3510.10s]  The question is if we have a slide for Ed.
[3510.10s -> 3511.22s]  We will have a slide.
[3511.22s -> 3513.66s]  We'll send out details after the slides.
[3515.54s -> 3516.38s]  Yeah.
[3516.38s -> 3517.46s]  Will students auditing the course
[3517.46s -> 3520.18s]  also have access to the same material?
[3520.18s -> 3522.70s]  The question is students auditing the class
[3522.70s -> 3527.70s]  will have access to all the online materials, assignments,
[3528.70s -> 3533.14s]  and will give you access to Canvas
[3533.14s -> 3536.42s]  so you can watch the lecture videos.
[3538.62s -> 3541.78s]  Yeah, what's the grading of the assignments?
[3541.78s -> 3543.78s]  What's the grading of the assignments?
[3544.70s -> 3545.58s]  Good question.
[3545.58s -> 3548.54s]  So there will be a set of unit tests
[3548.54s -> 3550.70s]  that you will have to pass.
[3550.70s -> 3551.66s]  So part of the grading is just
[3551.66s -> 3553.82s]  did you implement this correctly?
[3553.82s -> 3555.90s]  There will be also parts of the grade
[3555.90s -> 3558.18s]  which will, did you implement a model
[3558.18s -> 3560.50s]  that achieved a certain level of loss
[3560.50s -> 3562.54s]  or is efficient enough?
[3562.54s -> 3566.30s]  In the assignment, every problem part
[3566.30s -> 3568.82s]  has a number of points associated with it,
[3568.82s -> 3572.02s]  and so that gives you a fairly granular level
[3572.02s -> 3574.18s]  of what grading looks like.
[3578.54s -> 3581.94s]  Okay, let's jump into tokenization.
[3581.94s -> 3585.22s]  Okay, so Andre Capati has this really nice video
[3585.22s -> 3586.86s]  on tokenization, and in general,
[3586.86s -> 3588.62s]  he makes a lot of these videos
[3588.62s -> 3592.62s]  that actually inspired a lot of this class,
[3592.62s -> 3595.54s]  how you can build things from scratch.
[3595.54s -> 3597.98s]  So you should go check out some of his videos.
[3599.62s -> 3603.58s]  So tokenization, as we talked about it,
[3603.58s -> 3605.62s]  is the process of taking raw text,
[3605.62s -> 3608.94s]  which is generally represented as Unicode strings,
[3608.94s -> 3613.94s]  and turning it into a set of integers, essentially,
[3614.54s -> 3618.06s]  and where each integer represents a token.
[3618.90s -> 3622.22s]  Okay, so we need a procedure that encodes strings to tokens
[3622.22s -> 3624.98s]  and decodes them back into strings.
[3626.30s -> 3629.38s]  And the vocabulary size is just the number of values
[3629.38s -> 3630.90s]  that a token takes out,
[3630.90s -> 3633.30s]  the number of, the range of the integers.
[3634.54s -> 3637.30s]  Okay, so just to give you an example
[3637.30s -> 3638.78s]  of how tokenizers work,
[3638.78s -> 3642.14s]  let's play around with this really nice website,
[3642.14s -> 3644.26s]  which allows you to look at different tokenizers
[3644.26s -> 3646.98s]  and just type in something like,
[3646.98s -> 3651.98s]  hello, hello, or whatever.
[3653.50s -> 3657.34s]  Maybe I'll do this.
[3658.22s -> 3661.46s]  And one thing it does is it shows you
[3661.46s -> 3662.70s]  the list of integers.
[3662.70s -> 3664.06s]  This is the output of tokenizer.
[3664.06s -> 3667.86s]  It also nicely maps out the decomposition
[3667.86s -> 3671.22s]  of the original string into a bunch of segments.
[3672.18s -> 3674.54s]  And a few things to kind of note.
[3674.54s -> 3678.50s]  First of all, the space is part of a token.
[3678.50s -> 3680.02s]  So unlike classical NLP,
[3680.02s -> 3682.34s]  where the space just kind of disappears,
[3682.34s -> 3684.58s]  everything is accounted for.
[3684.58s -> 3687.10s]  These are meant to be kind of reversible operations,
[3687.10s -> 3688.02s]  tokenization.
[3689.62s -> 3693.26s]  And by convention, for whatever reason,
[3693.26s -> 3697.78s]  the space is usually preceding the token.
[3697.78s -> 3702.26s]  Also notice that hello is a completely different token
[3702.26s -> 3704.54s]  than space hello,
[3704.54s -> 3708.42s]  which you might make you a little bit squeamish,
[3708.42s -> 3711.62s]  but it seems, and it can cause problems,
[3711.62s -> 3713.98s]  but that's just how it is.
[3713.98s -> 3714.82s]  Question?
[3714.82s -> 3716.38s]  I was gonna ask, is the space being leading
[3716.38s -> 3718.26s]  instead of trailing intentional,
[3718.26s -> 3720.66s]  or is it just an artifact of the BPE process?
[3721.62s -> 3722.46s]  So the question is,
[3722.46s -> 3726.30s]  is the spacing before intentional or not?
[3726.82s -> 3731.82s]  So in the BPE process I will talk about,
[3732.18s -> 3734.22s]  you actually pre-tokenize,
[3734.22s -> 3738.82s]  and then you tokenize each part.
[3738.82s -> 3739.98s]  And I think the pre-tokenizer,
[3739.98s -> 3742.10s]  it does put the space in the front.
[3742.10s -> 3744.22s]  So it is built into the algorithm.
[3744.22s -> 3745.42s]  You could put it at the end,
[3745.42s -> 3748.02s]  but I think it probably makes more sense
[3748.02s -> 3750.86s]  to put it in the beginning,
[3750.86s -> 3753.86s]  but I actually don't.
[3753.90s -> 3756.14s]  Well, I guess it could go either way.
[3756.98s -> 3757.82s]  It makes sense.
[3758.94s -> 3762.42s]  Okay, so then if you look at numbers,
[3762.42s -> 3765.10s]  you see that the numbers are chopped down
[3765.10s -> 3768.54s]  into different pieces.
[3769.46s -> 3771.90s]  It's a little bit kind of interesting
[3771.90s -> 3773.02s]  that it's left to right,
[3773.02s -> 3774.82s]  so it's definitely not grouping by thousands
[3774.82s -> 3776.20s]  or anything like semantic.
[3777.10s -> 3779.10s]  But anyway, I encourage you to kind of play with it
[3779.10s -> 3783.06s]  and get a sense of what these existing tokenizers
[3783.06s -> 3784.10s]  look like.
[3784.10s -> 3787.18s]  So this is a tokenizer for GPT-4.0, for example.
[3789.62s -> 3792.40s]  So there's some observations that we made.
[3793.78s -> 3796.84s]  So if you look at the GPT-2 tokenizer,
[3796.84s -> 3799.60s]  which we'll use this kind of as a reference.
[3800.82s -> 3803.02s]  Okay, let me see if I can...
[3805.78s -> 3806.62s]  Hopefully this is,
[3806.62s -> 3809.54s]  let me know if this is getting too small in the back.
[3809.54s -> 3810.94s]  You could take a string.
[3811.94s -> 3814.94s]  If you apply the GPT-2 tokenizer,
[3814.94s -> 3817.18s]  you get your indices,
[3817.18s -> 3819.22s]  so it maps strings to indices,
[3819.22s -> 3823.54s]  and then you can decode to get back the string.
[3823.54s -> 3825.38s]  And this is just a sanity check
[3825.38s -> 3829.78s]  to make sure that you actually round trips.
[3829.78s -> 3831.78s]  Another thing that's, I guess,
[3831.78s -> 3834.22s]  interesting to look at is this compression ratio,
[3834.22s -> 3837.62s]  which is if you look at the number of bytes
[3837.62s -> 3839.18s]  divided by the number of tokens.
[3839.22s -> 3843.14s]  So how many bytes are represented by a token?
[3843.14s -> 3846.14s]  And the answer here is 1.6.
[3847.10s -> 3850.28s]  So every token represents 1.6 bytes of data.
[3851.90s -> 3853.94s]  Okay, so that's just a GPT tokenizer,
[3853.94s -> 3855.14s]  that open-ended trend.
[3856.62s -> 3858.58s]  To motivate kind of BPE,
[3858.58s -> 3861.66s]  I wanna go through a sequence of attempts.
[3861.66s -> 3863.82s]  So suppose you wanted to do tokenization.
[3863.82s -> 3867.18s]  What would be sort of the simplest thing?
[3867.18s -> 3868.26s]  The simplest thing is probably
[3868.26s -> 3870.02s]  character-based tokenization.
[3870.02s -> 3874.06s]  A Unicode string is a sequence of Unicode characters,
[3874.06s -> 3877.62s]  and each character can be converted into an integer,
[3877.62s -> 3879.34s]  called a code point.
[3879.34s -> 3882.30s]  So A maps to 97,
[3882.30s -> 3886.98s]  the world emoji maps to 127,757,
[3886.98s -> 3889.58s]  and you can see that it converts back.
[3889.58s -> 3891.62s]  So you can define a tokenizer
[3891.62s -> 3896.62s]  which simply maps each character
[3898.58s -> 3899.86s]  into a code point.
[3900.98s -> 3901.82s]  Okay?
[3902.70s -> 3905.62s]  So what's one problem with this?
[3908.14s -> 3908.98s]  Yeah?
[3908.98s -> 3910.62s]  The compression ratio is one.
[3910.62s -> 3912.98s]  The compression ratio is one.
[3912.98s -> 3915.82s]  So that's, well, actually the compression ratio
[3915.82s -> 3919.70s]  is not quite one because a character is not a byte.
[3919.70s -> 3922.74s]  But it's maybe not as good as you want.
[3922.74s -> 3925.98s]  One problem with that, if you look at some code points,
[3925.98s -> 3927.90s]  they're actually really large, right?
[3929.06s -> 3934.06s]  So you're basically allocating each one slot
[3934.34s -> 3937.62s]  in your vocabulary for every character, uniformly.
[3937.62s -> 3940.70s]  And some characters appear way more frequently than others,
[3940.70s -> 3943.62s]  so this is not a very effective use
[3943.62s -> 3945.66s]  of your kind of budget.
[3946.78s -> 3948.30s]  Okay?
[3948.30s -> 3950.70s]  So the vocabulary size is huge.
[3950.70s -> 3954.90s]  I mean, the vocabulary size being 127 is actually a big deal
[3954.90s -> 3959.46s]  but the bigger problem is that some characters are rare
[3959.46s -> 3961.82s]  and this is inefficient use of a vocab.
[3964.02s -> 3969.02s]  Okay, so the compression ratio is 1.5 in this case
[3969.46s -> 3972.54s]  because it's the tokens, sorry,
[3972.54s -> 3974.26s]  the number of bytes per token
[3974.26s -> 3978.02s]  and a character can be multiple bytes.
[3979.22s -> 3982.42s]  Okay, so that was a very kind of naive approach.
[3983.42s -> 3986.86s]  On the other hand, you can do byte-based tokenization.
[3986.86s -> 3989.66s]  Okay, so Unicode strings can be represented
[3989.66s -> 3994.06s]  in a sequence of bytes because every string
[3996.86s -> 4000.02s]  can just be converted into bytes.
[4000.02s -> 4004.82s]  Okay, so some, you know, A is already just kind of one byte
[4004.82s -> 4009.82s]  but some characters take up as many as four bytes
[4009.86s -> 4013.42s]  and this is using the UTF-8 kind of encoding of Unicode.
[4013.42s -> 4016.02s]  There's other encodings but this is the most common one
[4016.02s -> 4017.58s]  that's dynamic.
[4018.86s -> 4021.14s]  So let's just convert everything into bytes
[4022.46s -> 4023.74s]  and see what happens.
[4026.10s -> 4027.78s]  So if you do it into bytes,
[4027.78s -> 4030.94s]  now all the indices are between zero and 256
[4030.94s -> 4033.82s]  because there are only 256 possible values
[4033.82s -> 4035.34s]  for a byte by definition.
[4036.22s -> 4039.02s]  So your vocabulary is very, you know, small
[4039.06s -> 4042.78s]  and each byte is, I guess not all bytes are equally used
[4042.78s -> 4044.62s]  but, you know, it's not too,
[4044.62s -> 4047.34s]  you don't have that many sparsity, you know, problems.
[4048.26s -> 4050.62s]  But what's the problem with byte-based encoding?
[4053.74s -> 4056.94s]  Yeah, long sequences.
[4056.94s -> 4058.62s]  So this is, I mean, in some ways,
[4058.62s -> 4060.54s]  I really wish byte coding would work.
[4060.54s -> 4062.94s]  It's the most elegant thing
[4062.94s -> 4065.94s]  but you have long sequences,
[4065.98s -> 4069.46s]  your compression ratio is one, one byte per token
[4070.34s -> 4072.14s]  and this is just terrible.
[4072.14s -> 4073.74s]  A compression ratio of one is terrible
[4073.74s -> 4076.78s]  because your sequences will be really long,
[4076.78s -> 4079.82s]  attention is quadratic naively in the sequence length
[4079.82s -> 4082.46s]  so this is, you're just gonna have a bad time
[4082.46s -> 4083.82s]  in terms of efficiency.
[4085.02s -> 4088.34s]  Okay, so that wasn't really good.
[4089.38s -> 4093.78s]  So now the thing that you might think about is,
[4093.82s -> 4097.90s]  well, maybe we kind of have to be adaptive here, right?
[4097.90s -> 4099.94s]  Like, you know, we can't allocate a character
[4099.94s -> 4101.26s]  or a byte per token
[4101.26s -> 4103.94s]  but maybe some tokens can represent lots of bytes
[4103.94s -> 4106.34s]  and some tokens can represent few bytes.
[4106.34s -> 4109.18s]  So one way to do this is word-based tokenization
[4109.18s -> 4113.70s]  and this is something that was actually very classic in NLP.
[4113.70s -> 4118.42s]  So here's a string and you can just split it
[4119.50s -> 4123.42s]  into a sequence of segments, okay?
[4123.42s -> 4125.94s]  And you can call each of these tokens.
[4125.94s -> 4128.54s]  So you just use a regular expression,
[4128.54s -> 4130.58s]  here's a different regular expression
[4130.58s -> 4134.02s]  that GPT-2 uses to pre-tokenize
[4134.02s -> 4138.54s]  and it just splits your string
[4138.54s -> 4140.78s]  into a sequence of strings.
[4143.06s -> 4146.38s]  So, and then what you do with each segment
[4146.38s -> 4148.50s]  is you assign each of these to an integer
[4148.50s -> 4151.78s]  and then you're done, okay?
[4151.82s -> 4154.50s]  So, what's the problem with this?
[4157.58s -> 4161.22s]  Yeah, so the problem is that your vocabulary size
[4161.22s -> 4163.22s]  is sort of unbounded.
[4163.22s -> 4165.06s]  Well, not, maybe not quite unbounded
[4165.06s -> 4168.34s]  but you don't know how big it is, right?
[4168.34s -> 4170.50s]  Because on a given new input,
[4170.50s -> 4175.86s]  you might get a segment that just you've never seen before
[4175.86s -> 4177.66s]  and that's actually kind of a big problem.
[4177.66s -> 4180.98s]  This is actually, word-based is a really big pain in the butt
[4181.02s -> 4184.74s]  because some real words are rare
[4184.74s -> 4189.34s]  and actually it's really annoying
[4189.34s -> 4193.70s]  because new words have to receive this unk token
[4193.70s -> 4196.70s]  and if you're not careful about how you compute
[4196.70s -> 4200.18s]  the perplexity, then you're just gonna mess up.
[4201.50s -> 4204.98s]  So, word-based isn't,
[4204.98s -> 4207.94s]  I think it captures the right intuition of adaptivity
[4208.74s -> 4211.38s]  but it's not exactly what we want here.
[4212.34s -> 4216.78s]  So, here we're finally gonna talk about the BPE encoding
[4216.78s -> 4218.62s]  or byte pair encoding.
[4218.62s -> 4221.66s]  So, this was actually a very old algorithm
[4221.66s -> 4225.06s]  developed by Philip Gage in 94 for data compression
[4226.74s -> 4229.66s]  and it was first introduced into NLP
[4229.66s -> 4232.26s]  for neural machine translation.
[4232.26s -> 4235.66s]  So, before papers that did machine translation
[4235.74s -> 4240.46s]  or basically all NLP used word-based tokenization.
[4240.46s -> 4243.50s]  And again, word-based was a pain,
[4243.50s -> 4247.14s]  so this paper pioneered this idea,
[4247.14s -> 4250.82s]  well, we can use this nice algorithm for 94
[4250.82s -> 4254.82s]  and we can just make the tokenization kind of round trip
[4254.82s -> 4258.82s]  and we don't have to deal with unks or any of that stuff.
[4258.82s -> 4262.38s]  And then finally, this entered the kind of language modeling
[4263.10s -> 4268.10s]  through GPT-2, which was trained on using the BPE tokenizer.
[4271.62s -> 4274.38s]  Okay, so the basic idea is instead of defining
[4274.38s -> 4278.78s]  some sort of preconceived notion of how to split up,
[4278.78s -> 4281.78s]  we're gonna train the tokenizer on raw text.
[4281.78s -> 4284.70s]  That's the basic kind of insight, if you will.
[4284.70s -> 4288.74s]  And so, organically, common sequences
[4288.74s -> 4290.98s]  that span multiple characters,
[4291.06s -> 4293.42s]  we're gonna try to represent as one token
[4293.42s -> 4295.78s]  and rare sequences are gonna be represented
[4295.78s -> 4297.38s]  by multiple tokens.
[4299.10s -> 4301.34s]  There's a sort of a slight detail
[4301.34s -> 4304.70s]  which is for efficiency, the GPT-2 paper
[4304.70s -> 4307.14s]  uses word-based tokenizer as a sort of pre-processing
[4307.14s -> 4308.22s]  to break it up into segments
[4308.22s -> 4310.34s]  and then runs BPE on each of the segments,
[4310.34s -> 4313.94s]  which is what you're gonna do in this class as well.
[4313.94s -> 4316.38s]  The algorithm BPE is actually very simple.
[4316.38s -> 4319.62s]  So, we first convert the string into a sequence of bytes,
[4319.62s -> 4321.26s]  which we already did when we talked
[4321.26s -> 4322.78s]  about byte-based tokenization,
[4322.78s -> 4324.94s]  and now we're gonna successively merge
[4324.94s -> 4328.30s]  the most common pair of adjacent tokens
[4328.30s -> 4329.14s]  over and over again.
[4329.14s -> 4331.30s]  So, the intuition is that if a pair of tokens
[4331.30s -> 4334.14s]  shows up a lot, then we're going to compress it
[4334.14s -> 4334.98s]  into one token.
[4334.98s -> 4337.74s]  We're still gonna dedicate space for that.
[4337.74s -> 4339.14s]  Okay, so let's walk through
[4339.14s -> 4340.22s]  what this algorithm looks like.
[4340.22s -> 4343.74s]  So, we're gonna use this cat and hat as an example,
[4343.74s -> 4348.74s]  and we're gonna convert this into a sequence
[4349.26s -> 4350.62s]  of integers.
[4350.62s -> 4352.38s]  These are the bytes.
[4353.50s -> 4357.18s]  And then we're gonna keep track of what we've merged.
[4357.18s -> 4362.10s]  So, remember, merges is a map from two integers,
[4362.10s -> 4366.34s]  which can represent bytes or other pre-existing tokens,
[4366.34s -> 4368.18s]  and we're gonna create a new token.
[4369.10s -> 4373.54s]  And the vocab is just gonna be a handy way
[4373.54s -> 4376.90s]  to represent the index to bytes.
[4376.90s -> 4380.30s]  Okay, so we're going to, the BV algorithm,
[4380.30s -> 4381.14s]  I mean, it's very simple,
[4381.14s -> 4382.70s]  so I'm just actually gonna run through the code.
[4382.70s -> 4385.74s]  You're gonna do this num merges of times.
[4385.74s -> 4388.14s]  So, num merges is three in this case.
[4388.14s -> 4392.38s]  We're gonna first count up the number of occurrences
[4392.38s -> 4393.22s]  of pairs of bytes.
[4393.22s -> 4395.98s]  So, hopefully this doesn't become too small.
[4395.98s -> 4400.98s]  So, we're gonna just step through this sequence,
[4400.98s -> 4403.50s]  and we're gonna see that, okay, so,
[4403.50s -> 4407.10s]  once 116, 104, we're gonna increment that count.
[4407.10s -> 4409.38s]  104, 101, increment that count.
[4409.38s -> 4410.70s]  We're gonna go through the sequence,
[4410.70s -> 4413.86s]  and we're gonna count up the bytes.
[4414.74s -> 4415.94s]  Okay?
[4415.94s -> 4419.06s]  So now, after we have these counts,
[4419.06s -> 4423.06s]  we're going to find the pair
[4423.06s -> 4425.46s]  that occurs the most number of times.
[4426.50s -> 4428.26s]  So, I guess there's multiple ones,
[4428.26s -> 4431.46s]  but we're just gonna break ties and say 116 and 104.
[4432.46s -> 4434.26s]  Okay, so that occurred twice.
[4435.46s -> 4437.38s]  So, now we're gonna merge that pair.
[4437.38s -> 4441.66s]  So, we're gonna create a new slot in our vocab,
[4441.66s -> 4444.90s]  which is going to be 256.
[4444.90s -> 4447.14s]  So, so far it's been zero through 255,
[4447.14s -> 4450.14s]  but now we're expanding the vocab to 256,
[4450.14s -> 4453.74s]  and we're gonna say, every time we see 116 and 104,
[4453.74s -> 4455.70s]  we're gonna replace it with 256.
[4458.78s -> 4459.62s]  Okay?
[4460.62s -> 4465.62s]  And then we're going to just apply that merge
[4465.70s -> 4468.06s]  to our training set.
[4468.06s -> 4473.06s]  So, after we do that, the 116, 104 became 256,
[4474.66s -> 4477.50s]  and this 256, remember, occurred twice.
[4478.46s -> 4482.26s]  Okay, so now we're just gonna loop through this algorithm
[4482.26s -> 4483.54s]  one more time.
[4483.54s -> 4488.22s]  The second time, it decided to merge 256 and 101.
[4489.86s -> 4494.26s]  And now I'm gonna replace that in indices.
[4494.26s -> 4497.02s]  And notice that the indices is gonna shrink, right?
[4497.02s -> 4499.42s]  Because our compression ratio is getting better
[4499.42s -> 4503.14s]  as we make room for more vocabulary items
[4503.14s -> 4506.82s]  and we have a greater vocabulary to represent everything.
[4506.82s -> 4509.22s]  Okay, so let me do this one more time.
[4510.66s -> 4514.10s]  And then the next merge is 257, three,
[4514.10s -> 4516.10s]  and this is shrinking one more time.
[4517.10s -> 4518.38s]  Okay?
[4518.42s -> 4519.62s]  And then now we're done.
[4520.94s -> 4523.90s]  Okay, so let's try out this tokenizer.
[4523.90s -> 4527.58s]  So we have this string, the quick brown fox.
[4528.94s -> 4533.78s]  We're gonna encode into a sequence of indices.
[4533.78s -> 4537.58s]  And then we're gonna use our BP tokenizer to decode.
[4537.58s -> 4541.46s]  Let's actually step through what that looks like.
[4544.62s -> 4546.86s]  This, well actually, maybe decoding
[4546.86s -> 4548.86s]  isn't actually interesting, sorry.
[4548.86s -> 4550.74s]  I should have gone through the encode.
[4550.74s -> 4552.14s]  Let's go back to encode.
[4554.46s -> 4558.30s]  So encode, you take a string, you convert to indices,
[4558.30s -> 4561.50s]  and you just replay the merges.
[4561.50s -> 4563.74s]  Importantly, in the order that it occurred.
[4563.74s -> 4567.66s]  So I'm gonna replay these merges,
[4567.66s -> 4572.66s]  and then I'm going to get my indices.
[4572.66s -> 4575.06s]  I'm going to get my indices, okay?
[4575.06s -> 4579.14s]  And then verify that this works.
[4579.14s -> 4582.50s]  Okay, so that was, it's pretty simple.
[4584.46s -> 4587.50s]  It's because it's simple, it was also very inefficient.
[4587.50s -> 4589.78s]  For example, encode loops over the merges.
[4589.78s -> 4593.14s]  You should only loops over the merges that matter.
[4593.14s -> 4595.54s]  And there's some other bells and whistles,
[4595.54s -> 4598.50s]  like there's special tokens, pre-tokenization.
[4598.50s -> 4600.42s]  And so in your assignment,
[4600.42s -> 4603.14s]  you're going to essentially take this
[4603.14s -> 4604.94s]  as a starting point and, or I mean,
[4604.94s -> 4607.70s]  I guess you should implement your own from scratch.
[4607.70s -> 4611.58s]  But your goal is to make the implementation fast.
[4611.58s -> 4613.94s]  And you can paralyze it if you want.
[4613.94s -> 4615.14s]  You can go have fun.
[4616.90s -> 4619.26s]  Okay, so summary of tokenization.
[4619.26s -> 4621.78s]  So tokenizer maps between strings
[4621.78s -> 4623.94s]  and sequences of integers.
[4624.90s -> 4628.62s]  We looked at character-based, byte-based, word-based.
[4628.66s -> 4632.22s]  They're highly suboptimal for various reasons.
[4632.22s -> 4636.10s]  BPE is a very old algorithm from 94
[4636.10s -> 4638.66s]  that still proves to be effective heuristic.
[4638.66s -> 4640.18s]  And the important thing is it looks
[4640.18s -> 4643.30s]  at your corpus statistics to make sensible decisions
[4643.30s -> 4647.06s]  about how to best adaptively allocate vocabulary
[4647.06s -> 4649.46s]  to represent sequences of characters.
[4651.06s -> 4654.14s]  And you know, I hope that one day
[4654.14s -> 4655.74s]  I won't have to give this lecture
[4655.74s -> 4657.26s]  because we'll just have architectures
[4657.30s -> 4659.90s]  that map from bytes.
[4659.90s -> 4663.46s]  But until then, we'll have to deal with tokenization.
[4664.50s -> 4665.98s]  Okay, so that's it for today.
[4665.98s -> 4668.46s]  Next time, we're gonna dive into the details
[4668.46s -> 4671.34s]  of PyTorch and give you the building blocks
[4671.34s -> 4674.34s]  and pay attention to resource accounting.
[4674.34s -> 4677.78s]  All of you have presumably implemented PyTorch programs,
[4677.78s -> 4679.46s]  but we're gonna really look at
[4679.46s -> 4681.26s]  where all the flops are going.
[4681.26s -> 4682.62s]  Okay, see you next time.
[4687.26s -> 4688.26s]  Thank you.
