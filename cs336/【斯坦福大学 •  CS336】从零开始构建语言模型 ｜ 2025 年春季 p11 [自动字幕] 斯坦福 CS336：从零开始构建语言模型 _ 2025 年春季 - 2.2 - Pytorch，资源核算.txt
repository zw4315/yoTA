# Detected language: en (p=1.00)

[0.00s -> 10.00s]  Okay, so last lecture, I gave an overview of language models
[10.00s -> 12.00s]  and what it means to build them from scratch
[12.00s -> 13.00s]  and why we want to do that.
[13.00s -> 15.00s]  I also talked about tokenization,
[15.00s -> 18.00s]  which is going to be the first half of the first assignment.
[18.00s -> 24.00s]  Today's lecture will be going through actually building a model.
[24.00s -> 29.00s]  We'll discuss the primitives in PyTorch that are needed.
[29.00s -> 33.00s]  We're going to start with tensors, build models, optimizers, and training loop,
[33.00s -> 37.00s]  and we're going to place close attention to efficiency,
[37.00s -> 43.00s]  in particular, how we're using resources, both memory and compute.
[43.00s -> 49.00s]  Okay, so to motivate things a bit, here's some questions.
[49.00s -> 52.00s]  These questions are going to be answerable by napkin math,
[52.00s -> 55.00s]  so get your napkins out.
[55.00s -> 60.00s]  So how long would it take to train a 70 billion parameter dense transformer model
[60.00s -> 65.00s]  on 15 trillion tokens on 1,024 H100s?
[65.00s -> 68.00s]  Okay, so I'm just going to sketch out the sort of,
[68.00s -> 71.00s]  give you a flavor of the type of things that we want to do.
[71.00s -> 75.00s]  Okay, so here's how you go about reasoning it.
[75.00s -> 82.00s]  You count the total number of flops needed to train,
[82.00s -> 87.00s]  so that's six times the number of parameters times the number of tokens.
[87.00s -> 89.00s]  Okay, and where does that come from?
[89.00s -> 92.00s]  That will be what we'll talk about in this lecture.
[92.00s -> 99.00s]  You can look at the promised number of flops per second that H100 gives you,
[99.00s -> 101.00s]  the MFU, which is something we'll see later.
[101.00s -> 104.00s]  Let's just set it to 0.5.
[104.00s -> 108.00s]  And you can look at the number of flops per day
[108.00s -> 112.00s]  that your hardware is going to give you at this particular MFU,
[112.00s -> 120.00s]  so 1,024 of them for one day.
[120.00s -> 123.00s]  And then you just divide the total number of flops you need
[123.00s -> 127.00s]  to train the model by the number of flops that you're supposed to get.
[127.00s -> 132.00s]  Okay, and that gives you about 144.
[132.00s -> 137.00s]  Okay, so this is very simple calculations at the end of the day.
[137.00s -> 142.00s]  We're going to go through a bit more where these numbers come from,
[142.00s -> 146.00s]  and in particular where the six times number of parameters
[146.00s -> 149.00s]  times number of tokens comes from.
[149.00s -> 151.00s]  Okay, so here's the question.
[151.00s -> 155.00s]  What is the largest model you can train on H8, H100, using Adam, W,
[155.00s -> 158.00s]  if you're not being too clever?
[158.00s -> 165.00s]  Okay, so H100 has 80 gigabytes of HBM memory.
[166.00s -> 170.00s]  The number of bytes per parameter that you need for the parameters,
[170.00s -> 173.00s]  the gradients, optimizers, state is 16,
[173.00s -> 176.00s]  and we'll talk more about where that comes from.
[176.00s -> 180.00s]  And the number of parameters is basically the total amount of memory
[180.00s -> 184.00s]  divided by the number of bytes you need per parameter,
[184.00s -> 189.00s]  and that gives you about 40 billion parameters.
[189.00s -> 192.00s]  Okay, and this is very rough
[192.00s -> 195.00s]  because it doesn't take you into activations,
[195.00s -> 197.00s]  which depends on batch size and sequence length,
[197.00s -> 199.00s]  which I'm not really going to talk about,
[199.00s -> 202.00s]  but will be important for assignment one.
[202.00s -> 206.00s]  Okay, so this is a rough back of the calculation,
[206.00s -> 208.00s]  and this is something that you're probably not used to doing.
[208.00s -> 210.00s]  You just implement model, you train it,
[210.00s -> 211.00s]  and what happens, happens,
[211.00s -> 214.00s]  but remember that efficiency is the name of the game,
[214.00s -> 215.00s]  and to be efficient,
[215.00s -> 218.00s]  you have to know exactly how many flops you're actually expending
[218.00s -> 220.00s]  because when these numbers get large,
[220.00s -> 222.00s]  these directly translate into dollars,
[222.00s -> 226.00s]  and you want that to be as small as possible.
[226.00s -> 229.00s]  Okay, so we'll talk more about the details
[229.00s -> 232.00s]  of how these numbers arise.
[234.00s -> 237.00s]  You know, we will not actually go over the transformer,
[237.00s -> 242.00s]  so Tatsu's going to talk over the conceptual overview
[242.00s -> 244.00s]  of that next time,
[244.00s -> 247.00s]  and there's many ways you can learn about a transformer
[247.00s -> 249.00s]  if you haven't already looked at it.
[249.00s -> 250.00s]  There's assignment one.
[250.00s -> 251.00s]  If you do assignment one,
[251.00s -> 253.00s]  you'll definitely know what a transformer is,
[253.00s -> 255.00s]  and the handle actually does a pretty good job
[255.00s -> 257.00s]  of walking through all the different pieces.
[257.00s -> 258.00s]  There's a mathematical description.
[258.00s -> 260.00s]  If you like pictures, there's pictures.
[260.00s -> 264.00s]  There's a lot of stuff you can look on online.
[264.00s -> 267.00s]  But instead, I'm going to work with simpler models
[267.00s -> 269.00s]  and really talk about the primitives
[269.00s -> 271.00s]  and the resource accounting piece.
[271.00s -> 273.00s]  Okay, so remember last time I said
[273.00s -> 275.00s]  what kinds of knowledge can you learn?
[275.00s -> 278.00s]  So mechanics, in this lecture,
[278.00s -> 280.00s]  it's going to be just PyTorch
[280.00s -> 282.00s]  and understanding how PyTorch works
[282.00s -> 285.00s]  at a fairly primitive level.
[285.00s -> 287.00s]  So that will be pretty straightforward.
[287.00s -> 290.00s]  Mindset is about resource accounting,
[290.00s -> 291.00s]  and it's not hard.
[291.00s -> 293.00s]  You just have to do it.
[293.00s -> 296.00s]  And intuitions, unfortunately,
[296.00s -> 298.00s]  this is just going to be broad strokes for now.
[298.00s -> 300.00s]  Actually, there's not really much intuition
[300.00s -> 301.00s]  that I'm going to talk about
[301.00s -> 304.00s]  in terms of how anything we're doing
[304.00s -> 305.00s]  translates to good models.
[305.00s -> 310.00s]  This is more about the mechanics and mindset.
[310.00s -> 315.00s]  Okay, so let's start with memory accounting.
[315.00s -> 317.00s]  And then I'll talk about compute accounting,
[317.00s -> 320.00s]  and then we'll build up, bottom up.
[320.00s -> 324.00s]  Okay, so the best place to start is a tensor.
[324.00s -> 326.00s]  So tensors are the building block
[326.00s -> 328.00s]  for storing everything in deep learning.
[328.00s -> 332.00s]  Parameters, gradients, optimizers, data, activations,
[332.00s -> 334.00s]  and there's sort of these atoms.
[334.00s -> 338.00s]  You can read lots of documentation about them.
[338.00s -> 341.00s]  You're probably very familiar with how to create tensors.
[341.00s -> 344.00s]  There's creating tensors different ways.
[344.00s -> 347.00s]  You can also create a tensor and not initialize it
[347.00s -> 351.00s]  and use some special initialization
[351.00s -> 356.00s]  for the parameters, if you want.
[356.00s -> 361.00s]  Okay, so those are tensors.
[361.00s -> 363.00s]  So let's talk about memory
[363.00s -> 366.00s]  and how much memory tensors take up.
[366.00s -> 370.00s]  So every tensor that we'll probably be interested in
[370.00s -> 372.00s]  is stored as a floating point number.
[372.00s -> 375.00s]  And so there's many ways to represent floating point.
[375.00s -> 380.00s]  So the most default way is float32.
[380.00s -> 384.00s]  And float32 has 32 bits.
[384.00s -> 387.00s]  They're allocated one for sine, eight for exponent,
[387.00s -> 390.00s]  and 23 for the fractions.
[390.00s -> 392.00s]  So exponent gives you dynamic range
[392.00s -> 395.00s]  and fraction gives you different,
[395.00s -> 397.00s]  basically specifies different values.
[397.00s -> 401.00s]  So float32 is also known as FP32,
[401.00s -> 404.00s]  or single precision,
[404.00s -> 409.00s]  is sort of the gold standard in computing.
[409.00s -> 412.00s]  Some people also refer to float32 as full precision.
[412.00s -> 414.00s]  That's a little bit confusing
[414.00s -> 417.00s]  because full is really depending on who you're talking to.
[417.00s -> 419.00s]  If you're talking to a scientific computing person,
[419.00s -> 424.00s]  they'll kind of laugh at you when you say float32 is really full
[424.00s -> 427.00s]  because they'll use float64 or even more.
[427.00s -> 429.00s]  But if you're talking to a machine learning person,
[429.00s -> 432.00s]  float32 is the max you'll ever probably need to go
[432.00s -> 437.00s]  because deep learning is kind of sloppy like that.
[437.00s -> 440.00s]  Okay, so let's look at the memory.
[440.00s -> 442.00s]  So the memory is very simple.
[442.00s -> 444.00s]  It's determined by the number of values you have in your tensor
[444.00s -> 446.00s]  and the data type of each value.
[446.00s -> 450.00s]  Okay, so if you create a torch tensor of a four by eight matrix,
[450.00s -> 455.00s]  the default will give you a type of float32.
[455.00s -> 459.00s]  The size is four by eight,
[459.00s -> 462.00s]  and the number of elements is 32.
[462.00s -> 465.00s]  Each element size is four bytes.
[465.00s -> 467.00s]  32 bits is four bytes.
[467.00s -> 470.00s]  And the memory usage is simply
[470.00s -> 473.00s]  the number of elements times the number of,
[473.00s -> 475.00s]  size of each element,
[475.00s -> 477.00s]  and that will give you 128 bytes.
[477.00s -> 480.00s]  Okay, so this should be pretty easy.
[480.00s -> 482.00s]  And just to give some intuition,
[482.00s -> 486.00s]  if you get the one matrix in the P4 layer of GPT-3
[486.00s -> 488.00s]  is this number by this number,
[488.00s -> 491.00s]  and that gives you 2.3 gigabytes.
[491.00s -> 494.00s]  Okay, so that's one matrix.
[494.00s -> 498.00s]  These matrices can be pretty big.
[498.00s -> 502.00s]  Okay, so float32 is the default.
[502.00s -> 505.00s]  But of course, these matrices get big,
[505.00s -> 509.00s]  so naturally you want to make them smaller,
[509.00s -> 510.00s]  so you use less memory.
[510.00s -> 512.00s]  And also it turns out if you make them smaller,
[512.00s -> 515.00s]  you also make it go faster, too.
[515.00s -> 522.00s]  Okay, so another representation is called float16,
[522.00s -> 527.00s]  and as the name suggests, it's 16 bits
[527.00s -> 531.00s]  where both the exponent and the fraction are shrunk down
[531.00s -> 536.00s]  from eight to five and 23 to 10.
[536.00s -> 539.00s]  Okay, so this is known as half precision,
[539.00s -> 542.00s]  and it cuts down half the memory.
[542.00s -> 546.00s]  And that's all great except for
[546.00s -> 551.00s]  the dynamic range for these float16 isn't great.
[551.00s -> 554.00s]  So for example, if you try to make a number
[554.00s -> 561.00s]  like one e minus eight in float16,
[561.00s -> 564.00s]  it basically rounds out to zero,
[564.00s -> 566.00s]  and you get underflow.
[566.00s -> 568.00s]  Okay, so the float16 is not great
[568.00s -> 571.00s]  for representing very small numbers,
[571.00s -> 574.00s]  or very big numbers, as a matter of fact.
[574.00s -> 578.00s]  So if you use float16 for training,
[578.00s -> 581.00s]  for small models it's probably going to be okay,
[581.00s -> 585.00s]  but for large models when you're having lots of matrices
[585.00s -> 588.00s]  and you can get instability or underflow or overflow
[588.00s -> 591.00s]  and bad things happen.
[591.00s -> 596.00s]  Okay, so one thing that has happened,
[596.00s -> 599.00s]  which is nice, is there's been another representation
[599.00s -> 603.00s]  of bfloat16, which stands for brain float.
[603.00s -> 606.00s]  This was developed in 2018 to address the issue
[606.00s -> 609.00s]  that for deep learning,
[609.00s -> 611.00s]  we actually care about dynamic range
[611.00s -> 615.00s]  more than we care about this fraction.
[615.00s -> 618.00s]  So basically bf16 allocates more to the exponent
[618.00s -> 620.00s]  and less to the fraction.
[620.00s -> 624.00s]  Okay, so it uses the same memory as float16,
[624.00s -> 628.00s]  but it has a dynamic range of float32.
[628.00s -> 630.00s]  Okay, so that sounds really good,
[630.00s -> 634.00s]  and it actually catches at this resolution,
[634.00s -> 636.00s]  which is determined by this fraction is worse,
[636.00s -> 639.00s]  but this doesn't matter as much for deep learning.
[639.00s -> 642.00s]  So now if you try to create a tensor
[642.00s -> 646.00s]  with one E minus eight in bf16,
[646.00s -> 649.00s]  then you get something that's not zero.
[651.00s -> 654.00s]  Okay, so you can dive into the details.
[654.00s -> 655.00s]  I'm not going to go into this,
[655.00s -> 658.00s]  but you can stare at the actual full specs
[658.00s -> 661.00s]  of all the different floating point operations.
[662.00s -> 665.00s]  Okay, so bf16 is basically what
[665.00s -> 668.00s]  you will typically use to do computations
[668.00s -> 671.00s]  because it's sort of good enough
[671.00s -> 673.00s]  for free-for-all computation.
[673.00s -> 677.00s]  It turns out that for storing optimizer states
[677.00s -> 680.00s]  and parameters, you still need float32
[680.00s -> 684.00s]  for otherwise your training will go haywire.
[686.00s -> 688.00s]  So if you're bold,
[688.00s -> 693.00s]  so now we have something called fp8, or 8-bit,
[693.00s -> 695.00s]  and as the name suggests,
[695.00s -> 700.00s]  this was developed in 2022 by NVIDIA,
[700.00s -> 704.00s]  so now they have, essentially,
[704.00s -> 708.00s]  if you look at fp and bf16, it's like this,
[708.00s -> 710.00s]  and fp, wow, you really don't have
[710.00s -> 712.00s]  that many bits to store stuff, right?
[712.00s -> 714.00s]  So it's very crude.
[714.00s -> 716.00s]  There's two sort of variants,
[716.00s -> 719.00s]  depending on if you want to have more resolution
[719.00s -> 721.00s]  or more dynamic range.
[723.00s -> 725.00s]  And I'm not going to say too much about this,
[725.00s -> 728.00s]  but fp8 is supported by H100.
[728.00s -> 731.00s]  It's not really available on a previous generation.
[733.00s -> 736.00s]  But at a high level, training with float32,
[736.00s -> 741.00s]  which is, I think, is what you would do
[741.00s -> 743.00s]  if you're not trying to optimize too much,
[743.00s -> 745.00s]  and it's sort of safe.
[745.00s -> 747.00s]  It requires more memory.
[747.00s -> 752.00s]  You can go down to fp8 or bf16,
[754.00s -> 756.00s]  but you can get some instability.
[756.00s -> 758.00s]  Basically, I don't think you would probably want
[758.00s -> 762.00s]  to use a float16 at this point for deep learning.
[765.00s -> 769.00s]  And you can become more sophisticated
[769.00s -> 773.00s]  by looking at particular places in your pipeline,
[774.00s -> 776.00s]  either forward pass or backward pass
[776.00s -> 779.00s]  or optimizers or gradient accumulation,
[779.00s -> 782.00s]  and really figure out what the minimum precision
[782.00s -> 784.00s]  you need at this particular places,
[784.00s -> 787.00s]  and that's called, gets into mixed precision training.
[787.00s -> 791.00s]  So for example, some people like to use float32
[791.00s -> 794.00s]  for the attention to make sure
[794.00s -> 797.00s]  that doesn't kind of get messed up,
[797.00s -> 799.00s]  but for simple feedforward passes
[799.00s -> 802.00s]  with MATMUL's bf16 is fine.
[804.00s -> 805.00s]  Okay.
[805.00s -> 808.00s]  Pause a bit for questions.
[808.00s -> 810.00s]  So we talked about tensors,
[810.00s -> 814.00s]  and we looked at, depending on what representation,
[814.00s -> 817.00s]  how much storage they take.
[817.00s -> 818.00s]  Yeah?
[818.00s -> 820.00s]  Can you just clarify about the mixed position?
[820.00s -> 822.00s]  Like, when would you use 32 in the p-flow?
[822.00s -> 824.00s]  Yeah, so the question is,
[824.00s -> 829.00s]  when would you use float32 or bf16?
[830.00s -> 833.00s]  I don't have time to get into the exact details,
[833.00s -> 835.00s]  and it sort of varies depending on the model size
[835.00s -> 839.00s]  and everything, but generally for the parameters
[839.00s -> 842.00s]  and optimizers states, you use float32.
[842.00s -> 843.00s]  You can think about bf16
[843.00s -> 845.00s]  as something that's more transitory.
[845.00s -> 848.00s]  Like, you basically take your parameters,
[848.00s -> 849.00s]  you cast it to bf16,
[849.00s -> 852.00s]  and you kind of run ahead with that model.
[852.00s -> 855.00s]  But then the thing that you're gonna accumulate over time,
[855.00s -> 858.00s]  you want to have higher precision.
[858.00s -> 859.00s]  Yeah.
[861.00s -> 862.00s]  Okay.
[862.00s -> 865.00s]  So, now let's talk about compute.
[865.00s -> 867.00s]  So that was memory.
[868.00s -> 869.00s]  So,
[871.00s -> 875.00s]  compute obviously depends on what the hardware is.
[875.00s -> 877.00s]  By default, tensors are stored in CPU.
[877.00s -> 879.00s]  So, for example, if you just, in PyTorch,
[879.00s -> 882.00s]  say x equals torch at zero is 3232,
[882.00s -> 884.00s]  then it'll put it on your CPU.
[884.00s -> 887.00s]  It'll be in the CPU memory.
[888.00s -> 890.00s]  Of course, that's no good,
[890.00s -> 892.00s]  because if you're not using your GPU,
[892.00s -> 894.00s]  then you're gonna be orders of magnitude too slow.
[894.00s -> 897.00s]  So, you need to explicitly say in PyTorch
[897.00s -> 899.00s]  that you need to move it to the GPU,
[899.00s -> 900.00s]  and this is,
[902.00s -> 905.00s]  it's actually, just to make it very clear in pictures,
[905.00s -> 907.00s]  there's a CPU, it has RAM,
[907.00s -> 912.00s]  and that has to be moved over to the GPU.
[912.00s -> 915.00s]  There's a data transfer which is caught,
[915.00s -> 917.00s]  which takes some work,
[919.00s -> 920.00s]  takes some time.
[920.00s -> 921.00s]  Okay?
[921.00s -> 924.00s]  So, whenever you have a tensor in PyTorch,
[924.00s -> 926.00s]  you should always keep in your mind
[926.00s -> 927.00s]  where is this residing,
[927.00s -> 929.00s]  because just looking at the variable
[929.00s -> 930.00s]  or just looking at the code,
[930.00s -> 931.00s]  you can't always tell.
[931.00s -> 933.00s]  And if you want to be careful about
[933.00s -> 936.00s]  computation and data movement,
[936.00s -> 939.00s]  you have to really know where it is.
[939.00s -> 942.00s]  You can probably do things like assert
[942.00s -> 944.00s]  where it is in various places of code
[944.00s -> 947.00s]  just to document or be sure.
[948.00s -> 949.00s]  Okay, so,
[952.00s -> 955.00s]  so let's look at what hardware we have.
[955.00s -> 959.00s]  So, we have, in this case, we have one GPU.
[959.00s -> 962.00s]  This was run on the H100 clusters
[962.00s -> 964.00s]  that you guys have access to,
[964.00s -> 969.00s]  and this GPU is a H100,
[970.00s -> 973.00s]  80 gigabytes of high bandwidth memory,
[975.00s -> 979.00s]  and it gives you the cache size and so on.
[979.00s -> 980.00s]  Okay, so,
[982.00s -> 986.00s]  so if you have, remember the X is on CPU,
[987.00s -> 991.00s]  you can move it just by specifying two,
[991.00s -> 994.00s]  which is kind of a general PyTorch function.
[994.00s -> 996.00s]  You can also create a tensor directly on a GPU
[996.00s -> 999.00s]  so you don't have to move it at all.
[999.00s -> 1002.00s]  And if everything goes well,
[1003.00s -> 1006.00s]  I'm looking at the memory allocated before and after,
[1006.00s -> 1009.00s]  the difference should be exactly
[1010.00s -> 1011.00s]  two 32 by 32
[1014.00s -> 1017.00s]  matrices of four byte floats.
[1017.00s -> 1019.00s]  Okay, so it's A192.
[1021.00s -> 1024.00s]  Okay, so this is a sanity check that
[1024.00s -> 1027.00s]  the code is doing what is advertised.
[1028.00s -> 1032.00s]  Okay, so now you have your tensors on the GPU.
[1033.00s -> 1034.00s]  What do you do?
[1035.00s -> 1038.00s]  So there's many operations that you'll be needing
[1038.00s -> 1040.00s]  for assignment one and in general
[1040.00s -> 1042.00s]  to do any deep learning application,
[1042.00s -> 1044.00s]  and most tensors you just create
[1044.00s -> 1047.00s]  by performing operations on other tensors,
[1047.00s -> 1051.00s]  and each operation has some memory and compute footprints,
[1051.00s -> 1054.00s]  so let's make sure we understand that.
[1055.00s -> 1059.00s]  So first of all, what is actually a tensor in PyTorch?
[1061.00s -> 1063.00s]  Tensors are like a mathematical object.
[1063.00s -> 1067.00s]  In PyTorch, they're actually pointers
[1067.00s -> 1070.00s]  into some allocated memory.
[1070.00s -> 1073.00s]  Okay, so if you have, let's say,
[1073.00s -> 1076.00s]  a matrix, four by four matrix,
[1076.00s -> 1080.00s]  what it actually looks like is a long array.
[1081.00s -> 1084.00s]  And what the tensor has is metadata
[1084.00s -> 1088.00s]  that specifies how to get to address into that array.
[1088.00s -> 1091.00s]  And the metadata is going to be two numbers,
[1091.00s -> 1094.00s]  a stride for each, or actually,
[1094.00s -> 1098.00s]  one number per dimension of the tensor.
[1098.00s -> 1101.00s]  In this case, because there's two dimensions,
[1101.00s -> 1104.00s]  it's stride zero and stride one.
[1105.00s -> 1109.00s]  Stride zero specifies
[1109.00s -> 1112.00s]  if you were in dimension zero,
[1112.00s -> 1116.00s]  to get to the next row, to increment that index,
[1116.00s -> 1118.00s]  how many do you have to skip?
[1118.00s -> 1121.00s]  And so going down the rows, you skip four,
[1121.00s -> 1124.00s]  so stride zero is four.
[1124.00s -> 1129.00s]  And to go to the next column, you skip one.
[1129.00s -> 1133.00s]  So stride one is one, okay?
[1133.00s -> 1135.00s]  So with that, to find an element,
[1135.00s -> 1138.00s]  let's say one, two, one comma two,
[1138.00s -> 1142.00s]  it simply just multiply the indexes by the stride,
[1142.00s -> 1147.00s]  and you get to your index, which is six here.
[1147.00s -> 1150.00s]  So that would be here or here.
[1150.00s -> 1152.00s]  Okay, so that's basically what's going on
[1152.00s -> 1156.00s]  underneath the hood for tensors.
[1157.00s -> 1160.00s]  Okay, so this is relevant
[1160.00s -> 1164.00s]  because you can have multiple tensors
[1164.00s -> 1166.00s]  that use the same storage.
[1166.00s -> 1168.00s]  And this is useful because you don't want to
[1168.00s -> 1170.00s]  copy the tensor all over the place.
[1170.00s -> 1174.00s]  So imagine you have a two by three matrix here.
[1174.00s -> 1178.00s]  Many operations don't actually create a new tensor,
[1178.00s -> 1180.00s]  they just create a different view.
[1180.00s -> 1181.00s]  And it doesn't make a copy,
[1181.00s -> 1186.00s]  so you have to make sure that your mutations,
[1186.00s -> 1188.00s]  if you start mutating one tensor,
[1188.00s -> 1191.00s]  it's gonna cause the other one to mutate.
[1191.00s -> 1198.00s]  Okay, so for example, if you just get row zero,
[1198.00s -> 1202.00s]  okay, so remember y is this tensor,
[1202.00s -> 1205.00s]  sorry, x is one, two, three, four, five, six,
[1205.00s -> 1209.00s]  and y is x zero, which is just the first row.
[1209.00s -> 1212.00s]  Okay, and you can sort of double check,
[1212.00s -> 1214.00s]  there's this function in row that says
[1214.00s -> 1217.00s]  if you look at the underlying storage,
[1217.00s -> 1219.00s]  whether these two tensors have the same storage or not.
[1219.00s -> 1223.00s]  Okay, so this definitely doesn't copy the tensor,
[1223.00s -> 1226.00s]  it just creates a view.
[1226.00s -> 1229.00s]  You can get column one.
[1229.00s -> 1233.00s]  This also doesn't copy the tensor.
[1233.00s -> 1235.00s]  Oops, don't need to do that.
[1235.00s -> 1239.00s]  You can call a view function which can take any tensor
[1239.00s -> 1244.00s]  and look at it in terms of different dimensions.
[1244.00s -> 1247.00s]  Two by three.
[1247.00s -> 1250.00s]  Actually, this should be maybe the other way around,
[1250.00s -> 1253.00s]  as a three by two tensor.
[1253.00s -> 1258.00s]  So that also doesn't change doing copying.
[1258.00s -> 1261.00s]  You can transpose.
[1261.00s -> 1263.00s]  That also doesn't copy.
[1263.00s -> 1268.00s]  And then, like I said, if you start mutating x,
[1268.00s -> 1271.00s]  then y actually gets mutated as well,
[1271.00s -> 1273.00s]  because x and y are just pointers
[1273.00s -> 1276.00s]  into the same underlying storage.
[1276.00s -> 1281.00s]  Okay, so one thing that you have to be careful of
[1281.00s -> 1284.00s]  is that some views are contiguous,
[1284.00s -> 1286.00s]  which means that if you run through the tensor,
[1286.00s -> 1292.00s]  it's just going through this array in your storage.
[1292.00s -> 1294.00s]  But some are not.
[1294.00s -> 1297.00s]  So in particular, if you transpose it,
[1297.00s -> 1300.00s]  what does it mean when you're transposing it?
[1300.00s -> 1303.00s]  You're sort of going down now.
[1303.00s -> 1305.00s]  If you imagine going through the tensor,
[1305.00s -> 1307.00s]  you're kind of skipping around.
[1307.00s -> 1310.00s]  And if you have a non-contiguous tensor,
[1310.00s -> 1314.00s]  then if you try to further view it in a different way,
[1314.00s -> 1317.00s]  then this is not going to work.
[1317.00s -> 1318.00s]  Okay?
[1318.00s -> 1321.00s]  So in some cases, if you have a non-contiguous tensor,
[1321.00s -> 1323.00s]  you can make it contiguous first,
[1323.00s -> 1328.00s]  and then you can apply whatever viewing operation you want to it.
[1328.00s -> 1334.00s]  And then, in this case, x and y do not have the same storage,
[1334.00s -> 1340.00s]  because contiguous, in this case, makes a copy.
[1340.00s -> 1347.00s]  Okay, so this is just ways of slicing and dicing a tensor.
[1347.00s -> 1350.00s]  Views are free, so feel free to use them,
[1350.00s -> 1355.00s]  define different variables to make it easier to read your code,
[1355.00s -> 1359.00s]  because they're not allocating any memory.
[1359.00s -> 1363.00s]  But remember that contiguous, or reshape,
[1363.00s -> 1366.00s]  which is basically contiguous.view,
[1366.00s -> 1372.00s]  can create a copy, and so just be careful what you're doing.
[1372.00s -> 1380.00s]  Okay, questions before moving on?
[1380.00s -> 1384.00s]  All right, so hopefully a lot of this will be reviewed
[1384.00s -> 1388.00s]  for those of you who have done a lot of PyTorch before,
[1388.00s -> 1391.00s]  but it's helpful to just do it systematically
[1391.00s -> 1393.00s]  and make sure we're on the same page.
[1393.00s -> 1399.00s]  So, here's some operations that do create new tensors.
[1399.00s -> 1401.00s]  And in particular, element-wise operations
[1401.00s -> 1403.00s]  all create new tensors, obviously,
[1403.00s -> 1407.00s]  because you need it somewhere else to store the new value.
[1407.00s -> 1412.00s]  There's a, you know, triangular U is also an element operation
[1412.00s -> 1417.00s]  that comes in handy when you want to create a causal attention mask,
[1417.00s -> 1420.00s]  which you'll need for your assignment.
[1420.00s -> 1426.00s]  But nothing is that interesting here.
[1426.00s -> 1429.00s]  Okay, so let's talk about matmalls.
[1429.00s -> 1431.00s]  So the bread and butter of deep learning
[1431.00s -> 1433.00s]  is matrix multiplications.
[1433.00s -> 1436.00s]  And I'm sure all of you have done a matrix multiplication,
[1436.00s -> 1438.00s]  but just in case, this is what it looks like.
[1438.00s -> 1441.00s]  You take a 16 by 32 times a 32 by 2 matrix,
[1441.00s -> 1445.00s]  you get a 16 by 2 matrix.
[1446.00s -> 1453.00s]  But in general, when we do our machine learning application,
[1453.00s -> 1457.00s]  all operations you want to do in a batch.
[1457.00s -> 1459.00s]  And in the case of language models,
[1459.00s -> 1462.00s]  this usually means for every example in a batch
[1462.00s -> 1465.00s]  and for every sequence in a batch, you want to do something.
[1465.00s -> 1468.00s]  Okay, so generally what you're going to have
[1468.00s -> 1471.00s]  instead of just a matrix is you're going to have a tensor
[1471.00s -> 1474.00s]  where the dimensions are typically batch, sequence,
[1474.00s -> 1476.00s]  or whatever thing you're trying to do.
[1476.00s -> 1482.00s]  In this case, it's a matrix for every token in your data set.
[1482.00s -> 1488.00s]  And so PyTorch is nice enough to make this work well for you.
[1488.00s -> 1493.00s]  So when you take this four-dimensional tensor
[1493.00s -> 1497.00s]  and this matrix, what actually ends up happening
[1497.00s -> 1501.00s]  is that for every batch, every example and every token,
[1501.00s -> 1505.00s]  you're multiplying these two matrices.
[1505.00s -> 1507.00s]  Okay, and then the result is that you get
[1507.00s -> 1512.00s]  your resulting matrix for each of the first two elements.
[1512.00s -> 1515.00s]  So this is just, there's nothing fancy going on,
[1515.00s -> 1518.00s]  but this is just a pattern that I think
[1518.00s -> 1521.00s]  is helpful to think about.
[1524.00s -> 1528.00s]  Okay, so I'm going to take a little bit of a digression
[1528.00s -> 1531.00s]  and talk about INOPS.
[1531.00s -> 1535.00s]  And so the motivation for INOPS is the following.
[1535.00s -> 1541.00s]  So normally, in PyTorch, you define some tensors,
[1541.00s -> 1543.00s]  and then you see stuff like this,
[1543.00s -> 1547.00s]  where you take x and multiply by y transpose minus two minus one.
[1547.00s -> 1550.00s]  And you kind of look at this and you say,
[1550.00s -> 1552.00s]  okay, what is minus two?
[1552.00s -> 1555.00s]  Well, I think that's the sequence,
[1555.00s -> 1557.00s]  and then minus one is this hidden
[1557.00s -> 1559.00s]  because you're indexing backwards.
[1559.00s -> 1561.00s]  And it's really easy to mess this up,
[1561.00s -> 1562.00s]  because if you look in your code
[1562.00s -> 1565.00s]  and you see minus one, minus two,
[1565.00s -> 1567.00s]  you're kind of, if you're good,
[1567.00s -> 1569.00s]  you write a bunch of comments,
[1569.00s -> 1572.00s]  but then the comments can get out of date with the code
[1572.00s -> 1576.00s]  and then you have a bad time debugging.
[1576.00s -> 1580.00s]  So the solution is to use INOPS here.
[1580.00s -> 1584.00s]  So this is inspired by Einstein's summation notation.
[1585.00s -> 1589.00s]  And the idea is that we're just gonna name all the dimensions
[1589.00s -> 1594.00s]  instead of relying on indices, essentially.
[1594.00s -> 1597.00s]  Okay, so there's a library called JAX typing,
[1597.00s -> 1601.00s]  which is helpful for,
[1601.00s -> 1606.00s]  as a way to specify the dimensions in the types.
[1606.00s -> 1610.00s]  So normally, in PyTorch, you would just define,
[1610.00s -> 1612.00s]  write your code, and then you would comment,
[1612.00s -> 1614.00s]  oh, here's what the dimensions would be.
[1614.00s -> 1618.00s]  So if you use JAX typing, then you have this notation,
[1618.00s -> 1620.00s]  where as a string, you just write down
[1620.00s -> 1621.00s]  what the dimensions are.
[1621.00s -> 1624.00s]  So this is a slightly kind of more natural way
[1624.00s -> 1627.00s]  of documenting.
[1627.00s -> 1629.00s]  Now, notice that there's no enforcement here, right?
[1629.00s -> 1632.00s]  Because PyTorch types are sort of a little bit
[1632.00s -> 1635.00s]  of a lie in PyTorch.
[1635.00s -> 1639.00s]  So you can use a checker, right?
[1639.00s -> 1644.00s]  Yeah, you can use a check, but not by default.
[1647.00s -> 1651.00s]  Okay, so let's look at the einsam.
[1651.00s -> 1655.00s]  So einsam is basically matrix multiplication on steroids
[1655.00s -> 1657.00s]  with good bookkeeping.
[1657.00s -> 1660.00s]  So here's our example here.
[1660.00s -> 1663.00s]  We have X, which is, let's just think about this as,
[1663.00s -> 1665.00s]  you have a batch dimension, you have a sequence dimension,
[1665.00s -> 1671.00s]  and then you have four hidden, and Y is the same size.
[1671.00s -> 1674.00s]  You originally had to do this thing,
[1674.00s -> 1680.00s]  and now what you do instead is you basically write down
[1680.00s -> 1685.00s]  the dimensions, names of the dimensions of the two tensors,
[1685.00s -> 1688.00s]  so batch sequence one hidden, batch sequence two hidden,
[1688.00s -> 1691.00s]  and you just write what you,
[1691.00s -> 1694.00s]  dimensions should appear in the output.
[1694.00s -> 1697.00s]  Okay, so I write batch here because I just want to
[1697.00s -> 1701.00s]  basically, you know, carry that over.
[1701.00s -> 1704.00s]  And then I write sequence one and sequence two.
[1704.00s -> 1706.00s]  And notice that I don't write hidden,
[1706.00s -> 1709.00s]  and any dimension that is not named in output
[1709.00s -> 1710.00s]  is just summed over.
[1710.00s -> 1714.00s]  And any dimension that is named is sort of just
[1714.00s -> 1716.00s]  iterated over.
[1716.00s -> 1717.00s]  Okay?
[1717.00s -> 1721.00s]  So once you get used to this, this is actually very,
[1721.00s -> 1724.00s]  very, you know, helpful, and maybe it looks,
[1724.00s -> 1726.00s]  if you're seeing this for the first time,
[1726.00s -> 1728.00s]  it might seem a bit, you know, strange and long,
[1728.00s -> 1730.00s]  but trust me, once you get used to it,
[1730.00s -> 1734.00s]  it'll be better than doing minus two, minus one.
[1734.00s -> 1737.00s]  If you're a little bit, you know, slicker,
[1737.00s -> 1740.00s]  you can use dot dot dot to represent
[1740.00s -> 1742.00s]  broadcasting over any number of dimensions.
[1742.00s -> 1746.00s]  So in this case, instead of writing batch,
[1746.00s -> 1748.00s]  I can just write dot dot dot,
[1748.00s -> 1750.00s]  and this would handle the case where
[1750.00s -> 1753.00s]  instead of maybe batch, I have batch one, batch two,
[1753.00s -> 1756.00s]  or some other arbitrary long sequence.
[1758.00s -> 1759.00s]  Yeah, question?
[1759.00s -> 1761.00s]  Does port compile this,
[1761.00s -> 1765.00s]  like, is it guaranteed to compile to a position?
[1765.00s -> 1766.00s]  I guess.
[1766.00s -> 1767.00s]  So the question is,
[1767.00s -> 1771.00s]  is it guaranteed to compile to something efficient?
[1771.00s -> 1775.00s]  This, I think the short answer is yes.
[1775.00s -> 1778.00s]  I don't know if you have any, you know, nuances.
[1778.00s -> 1779.00s]  We'll figure out the best way to reduce,
[1779.00s -> 1782.00s]  the best order of dimensions to reduce,
[1782.00s -> 1783.00s]  and then use that.
[1783.00s -> 1784.00s]  If you use it within port compile,
[1784.00s -> 1785.00s]  only do that one time,
[1785.00s -> 1787.00s]  and then reuse the same implementation
[1787.00s -> 1788.00s]  over and over again.
[1788.00s -> 1791.00s]  It'll be better than anything designed by hand.
[1791.00s -> 1792.00s]  Yeah.
[1794.00s -> 1795.00s]  Okay.
[1796.00s -> 1797.00s]  So,
[1798.00s -> 1800.00s]  so let's look at reduce.
[1800.00s -> 1802.00s]  So reduce operates on one tensor,
[1802.00s -> 1805.00s]  and it basically aggregates some dimension
[1805.00s -> 1807.00s]  or dimensions of the tensor.
[1807.00s -> 1810.00s]  So you have this tensor before you would write mean
[1810.00s -> 1813.00s]  to sum over the final dimension,
[1813.00s -> 1816.00s]  and now you basically say,
[1816.00s -> 1819.00s]  actually, okay, so this replaces with sum.
[1819.00s -> 1825.00s]  So reduce, and again, you say hidden,
[1825.00s -> 1827.00s]  and hidden is disappeared,
[1827.00s -> 1830.00s]  so which means that you are aggregating
[1830.00s -> 1832.00s]  over that dimension.
[1832.00s -> 1835.00s]  Okay, so you can check that this indeed kind of works
[1835.00s -> 1836.00s]  over here.
[1838.00s -> 1843.00s]  Okay, so maybe one final example of this
[1843.00s -> 1849.00s]  is sometimes in a tensor,
[1849.00s -> 1852.00s]  one dimension actually represents multiple dimensions,
[1852.00s -> 1854.00s]  and you want to unpack that
[1854.00s -> 1856.00s]  and operate over one of them and pack it back.
[1856.00s -> 1861.00s]  So in this case, let's say you have batch sequence,
[1861.00s -> 1863.00s]  and then this eight-dimensional vector
[1863.00s -> 1866.00s]  is actually a flattened representation
[1866.00s -> 1870.00s]  of number of heads times some hidden dimension.
[1870.00s -> 1873.00s]  Okay, so and then you have a vector
[1873.00s -> 1877.00s]  that needs to operate on that hidden dimension.
[1877.00s -> 1880.00s]  So you can do this very elegantly
[1880.00s -> 1886.00s]  using INOPs by calling rearrange,
[1886.00s -> 1889.00s]  and this basically, you can think about it,
[1889.00s -> 1891.00s]  we saw a view before,
[1891.00s -> 1894.00s]  kind of like a fancier version
[1894.00s -> 1896.00s]  which basically looks at the same data,
[1896.00s -> 1900.00s]  but differently.
[1900.00s -> 1902.00s]  So here it basically says
[1902.00s -> 1905.00s]  this dimension is actually heads in hidden one.
[1905.00s -> 1909.00s]  I'm gonna explode that into two dimensions.
[1909.00s -> 1915.00s]  And you have to specify the number of heads here
[1915.00s -> 1919.00s]  because there's multiple ways to split a number into two.
[1919.00s -> 1923.00s]  Let's see, this might be a little bit long.
[1923.00s -> 1927.00s]  Okay, maybe it's not worth looking at right now.
[1927.00s -> 1933.00s]  And given that x, you can perform your transformation
[1933.00s -> 1935.00s]  using line sum.
[1935.00s -> 1940.00s]  So this is something hidden one, which corresponds to x,
[1940.00s -> 1943.00s]  and then hidden one, hidden two, which corresponds to w,
[1943.00s -> 1947.00s]  and that gives you something hidden two.
[1947.00s -> 1950.00s]  Okay?
[1950.00s -> 1953.00s]  And then you can rearrange back.
[1953.00s -> 1956.00s]  So this is just the inverse of breaking up.
[1956.00s -> 1958.00s]  So you have your two dimensions
[1958.00s -> 1960.00s]  and you group it into one.
[1960.00s -> 1962.00s]  So that's just a flattening operation
[1962.00s -> 1966.00s]  that's with everything, all the other dimensions,
[1966.00s -> 1971.00s]  kind of left alone.
[1971.00s -> 1975.00s]  Okay, so there is a tutorial for this
[1975.00s -> 1977.00s]  that I would recommend you go through
[1977.00s -> 1979.00s]  and it gives you a bit more.
[1979.00s -> 1981.00s]  So you don't have to use this
[1981.00s -> 1983.00s]  because you're building it from scratch
[1983.00s -> 1985.00s]  so you can kind of do anything you want,
[1985.00s -> 1988.00s]  but in assignment one we do give you guidance
[1988.00s -> 1995.00s]  and it's something probably to invest in.
[1995.00s -> 1999.00s]  Okay.
[1999.00s -> 2006.00s]  So now let's talk about computation cost
[2006.00s -> 2007.00s]  of tensile operations.
[2007.00s -> 2011.00s]  So we introduce a bunch of operations
[2011.00s -> 2014.00s]  and how much do they cost?
[2014.00s -> 2017.00s]  So a floating point operation
[2017.00s -> 2020.00s]  is any operation floating point
[2020.00s -> 2023.00s]  like addition or multiplication.
[2023.00s -> 2027.00s]  These are kind of the main ones
[2027.00s -> 2032.00s]  that are gonna, I think, matter in terms of flop count.
[2032.00s -> 2036.00s]  One thing that is sort of a pet peeve of mine
[2036.00s -> 2037.00s]  is that when you say flops,
[2037.00s -> 2039.00s]  it's actually unclear what you mean.
[2039.00s -> 2042.00s]  So you could mean flops with a lowercase s
[2042.00s -> 2046.00s]  which stands for number of floating operations.
[2046.00s -> 2049.00s]  This measures amount of computation that you've done.
[2049.00s -> 2054.00s]  Or you could mean flops also written with an uppercase S
[2054.00s -> 2056.00s]  which means floating points per second
[2056.00s -> 2059.00s]  which is used to measure the speed of hardware.
[2059.00s -> 2064.00s]  So we're not gonna in this class use uppercase S
[2064.00s -> 2066.00s]  because I find that very confusing
[2066.00s -> 2068.00s]  and just write slash S
[2068.00s -> 2072.00s]  to denote that it's a floating point per second.
[2072.00s -> 2074.00s]  Okay.
[2074.00s -> 2077.00s]  Okay, so just to give you some intuition about flops,
[2077.00s -> 2082.00s]  GBD3 took about three E23 flops.
[2082.00s -> 2085.00s]  GBD4 was two E25 flops.
[2086.00s -> 2087.00s]  Speculation.
[2087.00s -> 2089.00s]  And there was a US executive order
[2089.00s -> 2092.00s]  that any foundation model with over one E26 flops
[2092.00s -> 2094.00s]  had to be reported to government
[2094.00s -> 2097.00s]  which now has been revoked.
[2098.00s -> 2101.00s]  But the EU has still, they're going,
[2101.00s -> 2103.00s]  still has something that hasn't,
[2103.00s -> 2107.00s]  the EUAI Act which is one E25 which hasn't been revoked.
[2109.00s -> 2113.00s]  So some intuitions,
[2113.00s -> 2118.00s]  A100 has a peak performance of 312 teraflops per second.
[2121.00s -> 2124.00s]  And H100 has a peak performance
[2124.00s -> 2127.00s]  of 1979 teraflops per second
[2127.00s -> 2132.00s]  with sparsity and approximately 50% without.
[2132.00s -> 2135.00s]  And if you look at,
[2135.00s -> 2138.00s]  the Nvidia has these specification sheets
[2138.00s -> 2141.00s]  so you can see that the flops
[2141.00s -> 2144.00s]  actually depends on what you're trying to do.
[2144.00s -> 2150.00s]  So if you're using Fp32, it's actually really, really bad.
[2150.00s -> 2154.00s]  Like if you run Fp32 on H100,
[2154.00s -> 2158.00s]  you're not getting its orders of magnitude worse
[2158.00s -> 2161.00s]  than if you're doing Fp16
[2161.00s -> 2165.00s]  and if you're willing to go down to Fp8,
[2165.00s -> 2167.00s]  then it can be even faster.
[2167.00s -> 2170.00s]  And for the first read that I didn't realize
[2170.00s -> 2173.00s]  but there's an asterisk here and this means with sparsity.
[2173.00s -> 2175.00s]  So usually you're in,
[2175.00s -> 2178.00s]  a lot of the matrices we have in this class are dense
[2178.00s -> 2180.00s]  so you don't actually get this.
[2180.00s -> 2182.00s]  You get something like, you know.
[2182.00s -> 2184.00s]  It's exactly half that number.
[2184.00s -> 2186.00s]  Exactly half, okay.
[2190.00s -> 2191.00s]  Okay, so.
[2193.00s -> 2196.00s]  So now you can do a bucket of calculations.
[2196.00s -> 2199.00s]  Eight H100s for two weeks is
[2200.00s -> 2204.00s]  just eight times the number of flops per second
[2204.00s -> 2208.00s]  times the number of seconds in a week.
[2208.00s -> 2210.00s]  Actually this might be one week.
[2210.00s -> 2212.00s]  Okay, so that's one week.
[2212.00s -> 2216.00s]  And that's 4.7 times e to the 21
[2217.00s -> 2220.00s]  which is some number.
[2220.00s -> 2223.00s]  And you can kind of contextualize the flop counts
[2223.00s -> 2225.00s]  with the other model counts.
[2227.00s -> 2228.00s]  Yeah.
[2228.00s -> 2231.00s]  So that means if, so what does sparsity mean?
[2231.00s -> 2234.00s]  That means if your matrices are sparse.
[2234.00s -> 2236.00s]  Is it specific, like structure sparsity?
[2236.00s -> 2238.00s]  It's like two out of four elements
[2238.00s -> 2241.00s]  in each like group of four elements is zero.
[2241.00s -> 2243.00s]  That's the only case where you get that speed.
[2243.00s -> 2245.00s]  No one uses it.
[2245.00s -> 2249.00s]  Yeah, it's a marketing department uses it.
[2252.00s -> 2253.00s]  Okay.
[2254.00s -> 2257.00s]  So let's go through a simple example.
[2257.00s -> 2259.00s]  So remember we're not gonna touch the transformer
[2259.00s -> 2261.00s]  but I think even a linear model gives us
[2261.00s -> 2264.00s]  a lot of the building blocks and intuitions.
[2264.00s -> 2267.00s]  So suppose we have end points.
[2267.00s -> 2269.00s]  Each point is d-dimensional.
[2269.00s -> 2271.00s]  And the linear model is just gonna match,
[2271.00s -> 2275.00s]  map each d-dimensional vector to a k-dimensional vector.
[2275.00s -> 2279.00s]  Okay, so let's set some number of points
[2279.00s -> 2284.00s]  is b, dimension is d, k is the number of outputs.
[2285.00s -> 2288.00s]  And let's create our data matrix x,
[2288.00s -> 2293.00s]  our weight matrix w.
[2293.00s -> 2296.00s]  And the linear model is just some matmul.
[2296.00s -> 2300.00s]  So nothing, you know, too interesting going on.
[2300.00s -> 2308.00s]  And, you know, the question is how many flops was that?
[2308.00s -> 2313.00s]  And the way you would look at this is you say,
[2313.00s -> 2317.00s]  well, when you do the matrix multiplication,
[2317.00s -> 2322.00s]  you have basically for every i, j, k triple,
[2322.00s -> 2326.00s]  I have to multiply two numbers together.
[2326.00s -> 2332.00s]  And I also have to add that number to the total.
[2332.00s -> 2338.00s]  Okay, so the answer is two times the,
[2338.00s -> 2341.00s]  basically the product of all the dimensions involved.
[2341.00s -> 2343.00s]  So the left dimension, the middle dimension,
[2343.00s -> 2346.00s]  and the right dimension.
[2346.00s -> 2347.00s]  Okay?
[2347.00s -> 2350.00s]  So this is something that you should just kind of remember.
[2350.00s -> 2352.00s]  If you're doing a matrix multiplication,
[2352.00s -> 2355.00s]  the number of flops is two times
[2355.00s -> 2358.00s]  the product of the three dimensions.
[2359.00s -> 2364.00s]  Okay, so the flops of other operations are,
[2364.00s -> 2370.00s]  usually you can linear in the size of the matrix or tensor.
[2370.00s -> 2373.00s]  And in general, no other operation you encounter
[2373.00s -> 2377.00s]  in deep learning is as expensive as matrix multiplication
[2377.00s -> 2379.00s]  for large enough matrices.
[2379.00s -> 2382.00s]  So this is why I think a lot of the napkin math
[2382.00s -> 2385.00s]  is very simple because we're only looking at
[2385.00s -> 2391.00s]  the matrix multiplications that are performed by the model.
[2391.00s -> 2393.00s]  Now, of course there are regimes
[2393.00s -> 2396.00s]  where if your matrices are small enough,
[2396.00s -> 2398.00s]  then the cost of other things starts to dominate,
[2398.00s -> 2401.00s]  but generally that's not a good regime you want to be in
[2401.00s -> 2403.00s]  because the hardware is designed for
[2403.00s -> 2405.00s]  big multiplication multiplication,
[2405.00s -> 2408.00s]  so sort of by, it's a little bit circular,
[2408.00s -> 2410.00s]  but by kind of, we end up in this regime
[2410.00s -> 2413.00s]  where we only consider models
[2413.00s -> 2418.00s]  where the mammals are the dominant, you know, cost.
[2419.00s -> 2423.00s]  Okay, any questions about this number?
[2423.00s -> 2426.00s]  Two times the product of the three dimensions.
[2426.00s -> 2428.00s]  This is just a useful thing.
[2428.00s -> 2431.00s]  Would the algorithm of matrix multiplication always be the same
[2431.00s -> 2434.00s]  because the chip might have optimized that?
[2434.00s -> 2437.00s]  They're always correlated the same?
[2439.00s -> 2441.00s]  Yeah, so the question is like,
[2442.00s -> 2444.00s]  essentially does this depend
[2444.00s -> 2448.00s]  on the matrix multiplication algorithm?
[2449.00s -> 2454.00s]  In general, I guess we'll look at this next week
[2454.00s -> 2456.00s]  or the week after when we look at kernels.
[2456.00s -> 2459.00s]  I mean, actually there's a lot of optimization
[2459.00s -> 2460.00s]  that goes under the hood
[2460.00s -> 2464.00s]  when it comes to matrix multiplications,
[2464.00s -> 2466.00s]  and there's a lot of specialization
[2466.00s -> 2468.00s]  depending on the shape.
[2469.00s -> 2474.00s]  So I would say this is just a kind of a crude estimate
[2474.00s -> 2479.00s]  that is basically like the right order of magnitude.
[2483.00s -> 2486.00s]  Okay, so, yeah?
[2486.00s -> 2488.00s]  Additions and multiplications are considered equivalent?
[2488.00s -> 2492.00s]  Yeah, additions and multiplications are considered equivalent.
[2493.00s -> 2497.00s]  So one way I find helpful to interpret this,
[2497.00s -> 2500.00s]  so this is just a matrix multiplication,
[2500.00s -> 2504.00s]  but I'm gonna try to give a little bit of meaning to this,
[2504.00s -> 2506.00s]  which is why I've set up this
[2506.00s -> 2509.00s]  as kind of a little toy machine learning problem.
[2509.00s -> 2513.00s]  So B is really, stands for the number of data points,
[2513.00s -> 2516.00s]  and dk is the number of parameters.
[2516.00s -> 2518.00s]  So for this particular model,
[2518.00s -> 2521.00s]  the number of flops that's required for a forward pass
[2521.00s -> 2524.00s]  is two times the number of tokens or number of data points
[2524.00s -> 2527.00s]  times the number of parameters.
[2527.00s -> 2531.00s]  So this turns out to actually generalize to transformers.
[2531.00s -> 2534.00s]  There's an asterisk there because
[2536.00s -> 2539.00s]  there's the sequence length and other stuff,
[2539.00s -> 2542.00s]  but this is roughly right
[2542.00s -> 2545.00s]  if your sequence length isn't too large.
[2545.00s -> 2557.00s]  So, okay, so now this is just a number of floating point operations, right?
[2557.00s -> 2561.00s]  So how does this actually translate to a walk-like time,
[2561.00s -> 2564.00s]  which is presumably the thing you actually care about?
[2564.00s -> 2567.00s]  How long do you have to wait for your run?
[2567.00s -> 2570.00s]  So let's time this.
[2570.00s -> 2574.00s]  So I have this function that is just going to
[2579.00s -> 2580.00s]  do it five times,
[2580.00s -> 2584.00s]  and I'm going to perform the matrix multiply operation.
[2584.00s -> 2587.00s]  We'll talk a little bit later about this
[2587.00s -> 2591.00s]  two weeks from now, why the other code is here.
[2591.00s -> 2595.00s]  But for now, we get an actual time.
[2595.00s -> 2599.00s]  So that matrix took 0.16 seconds,
[2601.00s -> 2604.00s]  and the actual flops per second,
[2604.00s -> 2607.00s]  which is how many flops did it do per second,
[2607.00s -> 2611.00s]  is 5.4E13.
[2613.00s -> 2617.00s]  So now you can compare this with the marketing materials
[2619.00s -> 2621.00s]  for the A100 and H100.
[2622.00s -> 2625.00s]  And as we look at the spec sheet,
[2625.00s -> 2628.00s]  the flops depends on the data type,
[2628.00s -> 2632.00s]  and we see that the promise flops per second,
[2633.00s -> 2638.00s]  which for H100, for, I guess this is for float 32,
[2641.00s -> 2644.00s]  is 67 teraflops, as we looked.
[2645.00s -> 2650.00s]  So that is the number of promise flops per second we had.
[2655.00s -> 2657.00s]  And now if you look at the,
[2659.00s -> 2663.00s]  there's a helpful notion called model flops utilization,
[2663.00s -> 2666.00s]  or MFU, which is the actual number of flops
[2666.00s -> 2669.00s]  divided by the promise flops.
[2669.00s -> 2671.00s]  So you take the actual number of flops,
[2671.00s -> 2675.00s]  remember which is what you actually witnessed,
[2675.00s -> 2677.00s]  the number of floating point operations
[2677.00s -> 2680.00s]  that are useful for your model,
[2682.00s -> 2685.00s]  divided by the actual time it took,
[2685.00s -> 2687.00s]  divided by this promise flops per second,
[2687.00s -> 2690.00s]  which is from the glossy brochure,
[2690.00s -> 2692.00s]  you can get a MFU of 0.8.
[2697.00s -> 2700.00s]  So usually you see people talking about their MFUs,
[2700.00s -> 2703.00s]  and something greater than 0.5
[2703.00s -> 2706.00s]  is usually considered to be good,
[2706.00s -> 2709.00s]  and if you're like 5% MFU,
[2709.00s -> 2711.00s]  that's considered to be really bad.
[2711.00s -> 2713.00s]  You usually can't get close to,
[2713.00s -> 2715.00s]  that close to 90 or 100,
[2718.00s -> 2720.00s]  because this is sort of ignoring
[2720.00s -> 2723.00s]  all sort of communication and overhead.
[2723.00s -> 2727.00s]  It's just like the literal computation of the flops.
[2728.00s -> 2731.00s]  Okay, and usually MFU is much higher
[2731.00s -> 2735.00s]  if the matrix multiplications dominate.
[2735.00s -> 2739.00s]  Okay, so that's MFU, any questions about this?
[2740.00s -> 2741.00s]  Yeah?
[2741.00s -> 2744.00s]  You're using the promise flop per sec,
[2744.00s -> 2746.00s]  not considering the sparse.
[2746.00s -> 2749.00s]  So this promise flop per sec
[2749.00s -> 2752.00s]  is not considering this as sparse.
[2752.00s -> 2753.50s]  Interesting, yeah.
[2755.00s -> 2757.50s]  One note is this is actually,
[2759.00s -> 2761.00s]  there's also something called hardware
[2761.00s -> 2763.00s]  to flop serialization,
[2763.00s -> 2766.00s]  and the motivation here is that
[2769.00s -> 2771.50s]  we're trying to look at the,
[2774.00s -> 2776.00s]  it's called model because we're looking at
[2776.00s -> 2779.00s]  the number of effective useful operations
[2779.00s -> 2781.50s]  that the model is performing.
[2783.00s -> 2785.50s]  And so it's a way of kind of standardizing.
[2785.50s -> 2790.00s]  It's not the actual number of flops that are done,
[2790.00s -> 2792.00s]  because you could have optimization in your code
[2792.00s -> 2794.00s]  that cache a few things
[2794.00s -> 2798.00s]  or redo, you know, recomputation of some things,
[2798.00s -> 2802.00s]  and in some sense, you're still computing the same model.
[2802.00s -> 2805.00s]  So what matters is that this is sort of
[2805.00s -> 2807.00s]  trying to look at the model complexity,
[2807.00s -> 2808.00s]  and you shouldn't be penalized
[2808.00s -> 2811.00s]  just because you were clever in your MFU
[2811.00s -> 2814.00s]  or clever and you didn't actually do the flops,
[2814.00s -> 2816.00s]  but you said you did.
[2817.00s -> 2821.00s]  Okay, so you can also do the same with BF16.
[2824.00s -> 2826.50s]  And here, we see that for BF,
[2830.00s -> 2833.50s]  the time is actually much better, right?
[2835.00s -> 2837.00s]  So .03 instead of .16,
[2838.00s -> 2841.50s]  so the actual flops per second is higher.
[2844.00s -> 2846.00s]  Even accounting for sparsity,
[2846.00s -> 2849.50s]  the promise flops is still quite high,
[2849.50s -> 2853.00s]  so the MFU is actually lower for BF16.
[2856.50s -> 2859.00s]  This is maybe surprisingly low,
[2862.00s -> 2866.00s]  but sometimes the promise flops is a bit optimistic.
[2868.00s -> 2871.00s]  So always benchmark your code,
[2871.00s -> 2872.50s]  and don't just kind of assume
[2872.50s -> 2876.00s]  that you're gonna get certain levels of performance.
[2876.00s -> 2878.00s]  Okay, so just to summarize,
[2880.00s -> 2884.00s]  matrix multiplications dominate the compute,
[2885.00s -> 2887.00s]  and the general rule of thumb is that
[2887.00s -> 2891.00s]  it's two times the product of the dimensions, flops.
[2892.00s -> 2896.00s]  The flops per second, floating points per second,
[2898.00s -> 2902.00s]  depends on the hardware and also the data type.
[2903.00s -> 2905.00s]  So the fancier the hardware you have,
[2905.00s -> 2908.00s]  the higher it is, the smaller the data type,
[2908.00s -> 2911.00s]  the usually the faster it is.
[2912.00s -> 2915.00s]  And MFU is a useful notion to look at
[2916.00s -> 2919.00s]  how well you're essentially squeezing
[2920.00s -> 2921.16s]  your hardware.
[2924.00s -> 2925.00s]  Yeah?
[2925.00s -> 2926.00s]  I've heard that often,
[2926.00s -> 2928.00s]  in order to get the maximum utilization,
[2928.00s -> 2931.00s]  you want to use these tensor cores on the machine,
[2931.00s -> 2935.00s]  and so does PyTorch by default use these tensor cores?
[2935.00s -> 2938.00s]  Are these tensor cores accounting for that?
[2938.00s -> 2940.00s]  Yeah, so the question is,
[2940.00s -> 2942.00s]  what about those tensor cores?
[2942.00s -> 2945.00s]  So if you go to this spec sheet,
[2946.00s -> 2948.00s]  spec sheet, you'll see that
[2951.00s -> 2954.00s]  these are all on the tensor core.
[2954.00s -> 2956.00s]  So the tensor core is basically
[2956.00s -> 2959.00s]  a specialized hardware to do matmals.
[2967.00s -> 2969.00s]  So by default it should use it,
[2969.00s -> 2972.00s]  and especially if you're using PyTorch compile,
[2972.00s -> 2974.00s]  it should also generate the code
[2974.00s -> 2977.00s]  that will use the hardware properly.
[2984.00s -> 2989.00s]  Okay, so let's talk a little bit about gradients.
[2989.00s -> 2992.00s]  And the reason is that we've only looked
[2992.00s -> 2995.00s]  at matrix multiplication, or in other words,
[2995.00s -> 2998.00s]  basically feed-forward, forward passes
[2998.00s -> 3000.00s]  and the number of flops,
[3000.00s -> 3001.00s]  but there's also a computation
[3001.00s -> 3003.00s]  that comes from computing gradients,
[3003.00s -> 3007.00s]  and we want to track down how much that is.
[3010.00s -> 3013.00s]  So just to consider a simple example,
[3013.00s -> 3015.00s]  a simple linear model
[3015.00s -> 3018.00s]  where you take the prediction of a linear model
[3018.00s -> 3022.00s]  and you look at the MSE with respect to five.
[3024.00s -> 3025.00s]  So not a very interesting loss,
[3025.00s -> 3027.00s]  but I think it's illustrative
[3027.00s -> 3029.50s]  for looking at the gradients.
[3031.00s -> 3033.00s]  Okay, so remember in the forward pass,
[3033.00s -> 3036.00s]  you have your X, you have your W,
[3036.00s -> 3040.00s]  which you want to compute the gradient with respect to,
[3040.00s -> 3044.00s]  you make a prediction by taking a linear product,
[3044.00s -> 3047.00s]  and then you have your loss, okay?
[3049.00s -> 3053.00s]  And in the backward pass, you just call loss backwards,
[3053.00s -> 3056.00s]  and in this case, the gradient,
[3058.00s -> 3062.00s]  which is this variable attached to the tensor,
[3062.00s -> 3065.00s]  turns out to be what you want.
[3066.00s -> 3071.00s]  Okay, so everyone has done gradients in PyTorch before.
[3073.00s -> 3076.00s]  So let's look at how many flops
[3076.00s -> 3079.00s]  are required for computing gradients.
[3080.00s -> 3085.00s]  Okay, so let's look at a slightly more complicated model.
[3090.00s -> 3093.00s]  So now it's a two-layer linear model
[3095.00s -> 3098.00s]  where you have X, which is B by D,
[3098.00s -> 3102.00s]  times W1, which is D by D, so that's the first layer,
[3105.00s -> 3108.00s]  and then you take your hidden activations,
[3108.00s -> 3113.00s]  H1, and you pass it through another linear layer, W2,
[3113.00s -> 3115.00s]  and to get a k-dimensional vector,
[3115.00s -> 3118.00s]  and you do some, compute some loss.
[3121.00s -> 3123.00s]  Okay, so this is a two-layer linear network,
[3123.00s -> 3125.00s]  and just as a kind of review,
[3125.00s -> 3129.00s]  if you look at the number of forward flops,
[3129.00s -> 3133.00s]  what you had to do was you have to multiply,
[3133.00s -> 3137.00s]  look at W1, you have to multiply X by W1,
[3138.00s -> 3143.00s]  and add it to your H1, and you have to take H1 and W2,
[3145.00s -> 3148.00s]  and you have to add it to your H2.
[3149.00s -> 3152.00s]  Okay, so the total number of flops, again,
[3152.00s -> 3155.00s]  is two times the product of all the dimensions
[3155.00s -> 3159.00s]  in your matmul, plus two times the product dimensions
[3159.00s -> 3161.00s]  in your matmul for the second matrix.
[3161.00s -> 3162.00s]  Okay, in other words,
[3162.00s -> 3166.00s]  two times the total number of parameters in this case.
[3167.00s -> 3170.00s]  Okay, so what about the backward pass?
[3170.00s -> 3174.00s]  So this part will be a little bit more involved.
[3175.00s -> 3180.00s]  So we can recall the model X to H1 to H2 and the loss.
[3181.00s -> 3182.00s]  So in the backward pass,
[3182.00s -> 3185.00s]  you have to compute a bunch of gradients,
[3185.00s -> 3187.00s]  and the gradients that are relevant
[3187.00s -> 3190.00s]  is you have to compute the gradient
[3190.00s -> 3193.00s]  with respect to H1, you know, H2,
[3196.00s -> 3198.00s]  W1 and W2 of the loss.
[3201.00s -> 3204.00s]  So D loss, each of these variables.
[3204.00s -> 3207.00s]  Okay, so how long does it take to compute that?
[3207.00s -> 3210.00s]  Let's just look at W2 for now.
[3210.00s -> 3213.00s]  Okay, so the things that touch W2,
[3214.00s -> 3218.00s]  you can compute by looking at the chain rule.
[3219.00s -> 3222.00s]  So W2 grad, so the gradient with,
[3222.00s -> 3226.00s]  of D loss, D W2 is you sum H1 times the gradient
[3237.00s -> 3240.00s]  of the loss with respect to H2.
[3240.00s -> 3244.00s]  Okay, so that's just a chain rule for W2,
[3245.00s -> 3249.00s]  and this is, so all the gradients are the same size
[3250.00s -> 3252.00s]  as the underlying vectors.
[3255.00s -> 3257.00s]  So this turns out to be,
[3260.00s -> 3264.00s]  it essentially looks like a matrix multiplication,
[3264.00s -> 3267.00s]  and so the same calculus holds,
[3267.00s -> 3271.00s]  which is that it's two times the number of,
[3271.00s -> 3275.00s]  the product of all the dimensions, B times D times K.
[3276.00s -> 3280.00s]  Okay, but this is only the gradient with respect to W2.
[3283.00s -> 3287.00s]  We also need to compute the gradient with respect to H1,
[3287.00s -> 3291.00s]  because we have to keep on back propagating to W1
[3291.00s -> 3292.00s]  and so on.
[3293.00s -> 3296.50s]  Okay, so that is going to be, you know,
[3299.00s -> 3302.00s]  the product of W2, you know, times,
[3306.00s -> 3307.00s]  you know, H2.
[3308.00s -> 3311.00s]  Sorry, I think this should be that grad of H2.
[3311.00s -> 3312.00s]  H2 dot grad.
[3314.00s -> 3318.00s]  So that turns out to also be essentially,
[3318.00s -> 3322.00s]  you know, looks like the matrix multiplication,
[3322.00s -> 3325.00s]  and it's the same number of flops
[3325.00s -> 3328.00s]  for computing the gradient of H1.
[3331.00s -> 3333.00s]  Okay, so when you add the two,
[3333.00s -> 3335.00s]  so that's just for W2.
[3336.00s -> 3338.00s]  You do the same thing for W1,
[3338.00s -> 3341.00s]  and that's, which has D times D parameters,
[3341.00s -> 3344.00s]  and when you add it all up, it's,
[3346.00s -> 3350.00s]  so for this, for W2, the amount of computation
[3351.00s -> 3354.00s]  was four times B times D times K,
[3355.00s -> 3359.00s]  and for W1, it's also four times B times D times D,
[3359.00s -> 3361.00s]  because W1 is D by D.
[3368.00s -> 3369.00s]  Okay?
[3370.00s -> 3373.00s]  So, I know there's a lot of symbols here.
[3373.00s -> 3377.00s]  I'm going to try also to give you a visual account
[3377.00s -> 3379.00s]  for this, so this is from a blog post
[3379.00s -> 3383.00s]  that I think may work better, we'll see.
[3384.00s -> 3387.00s]  Okay, I have to wait for the animation to loop back.
[3387.00s -> 3390.00s]  So, basically this is one layer of the neural network
[3390.00s -> 3392.00s]  where it has, you know, the hidden,
[3392.00s -> 3395.00s]  and then the weights to the next layer,
[3395.00s -> 3399.00s]  and so I have to, okay, the problem with this animation
[3399.00s -> 3401.00s]  is I have to wait.
[3401.00s -> 3404.00s]  Okay, ready, set, okay.
[3404.00s -> 3408.00s]  So first, I have to multiply W and A,
[3408.00s -> 3410.00s]  and I have to add it to this, that's a forward pass,
[3410.00s -> 3412.00s]  and now I'm going to multiply this, these two,
[3412.00s -> 3415.00s]  and then add it to that,
[3415.00s -> 3418.00s]  and I'm going to multiply and then add it to that.
[3418.00s -> 3419.00s]  Okay.
[3419.00s -> 3422.00s]  Any questions?
[3423.00s -> 3426.00s]  I wish there were a way to slow this down.
[3426.00s -> 3430.00s]  But, you know, the details maybe I'll let you
[3430.00s -> 3433.00s]  kind of ruminate on, but the high level
[3433.00s -> 3436.00s]  is that there's two times the number of parameters
[3436.00s -> 3438.00s]  for the forward pass, and four times the number
[3438.00s -> 3440.00s]  of parameters for the backward pass,
[3440.00s -> 3443.00s]  and we can just kind of work it out
[3443.00s -> 3444.00s]  via the chain rule here.
[3444.00s -> 3445.00s]  Yeah?
[3445.00s -> 3448.00s]  For the homeworks, are we also using the,
[3448.00s -> 3450.00s]  you said some PyTorch implementation is allowed,
[3450.00s -> 3453.00s]  some isn't, are we allowed to use rad,
[3453.00s -> 3457.00s]  or are we doing the, like, entirely by hand
[3457.00s -> 3459.00s]  to the gradient?
[3459.00s -> 3461.00s]  So the question is, in the homework,
[3461.00s -> 3463.00s]  are you going to compute gradients by hand,
[3463.00s -> 3464.00s]  and the answer is no.
[3464.00s -> 3467.00s]  You're going to just use PyTorch gradient.
[3467.00s -> 3469.00s]  This is just to break it down
[3469.00s -> 3473.00s]  so we can do the counting flops.
[3474.00s -> 3479.00s]  Okay, any questions about this before I move on?
[3485.00s -> 3488.00s]  Okay, just to summarize, the forward pass is,
[3488.00s -> 3492.00s]  for this particular model, is two times the number
[3492.00s -> 3494.00s]  of data points times the number of parameters,
[3494.00s -> 3497.00s]  and backwards is four times the number of data points
[3497.00s -> 3499.00s]  times the number of parameters, which means
[3499.00s -> 3501.00s]  that total is six times the number of data
[3501.00s -> 3503.00s]  times parameters, okay?
[3503.00s -> 3507.00s]  And that explains why there was a six in the beginning
[3507.00s -> 3511.00s]  when I asked the motivating question.
[3511.00s -> 3516.00s]  So now this is for a simple linear model.
[3516.00s -> 3518.00s]  It turns out that many models,
[3518.00s -> 3523.00s]  this is basically the bulk of the computation,
[3523.00s -> 3528.00s]  when essentially every computation you do has,
[3528.00s -> 3532.00s]  you know, touches essentially a new parameter, roughly.
[3533.00s -> 3536.00s]  And obviously this doesn't hold,
[3536.00s -> 3538.00s]  you can find models where this doesn't hold
[3538.00s -> 3539.00s]  because you can have like one parameter
[3539.00s -> 3543.00s]  through parameter sharing and have a billion flops,
[3543.00s -> 3547.00s]  but that's generally not what models look like.
[3549.00s -> 3554.00s]  Okay, so let me move on.
[3556.00s -> 3560.00s]  So far I've basically finished talking
[3560.00s -> 3562.00s]  about the resource accounting.
[3562.00s -> 3563.00s]  So we looked at tensors,
[3563.00s -> 3565.00s]  we looked at some computation on tensors,
[3565.00s -> 3569.00s]  we looked at how much tensors take to store
[3569.00s -> 3573.00s]  and also how many flops tensors take
[3573.00s -> 3576.00s]  when you do various operations on them.
[3576.00s -> 3580.00s]  Now let's start building up different models.
[3580.00s -> 3585.00s]  I think this part isn't necessarily going to be
[3585.00s -> 3589.00s]  that conceptually interesting or challenging,
[3589.00s -> 3593.00s]  but it's more for maybe just completeness.
[3593.00s -> 3599.00s]  Okay, so parameters in PyTorch are stored
[3599.00s -> 3603.00s]  as these nn parameter objects.
[3603.00s -> 3608.00s]  Let's talk a little bit about parameter initialization.
[3608.00s -> 3617.00s]  So if you have, let's say, a parameter
[3617.00s -> 3623.00s]  that has, okay, so you generate, okay, sorry,
[3623.00s -> 3625.00s]  your w parameter is an input dimension
[3625.00s -> 3628.00s]  by hidden dimension matrix.
[3628.00s -> 3630.00s]  You're still in the linear model case,
[3630.00s -> 3631.00s]  so let's just turn in an input
[3631.00s -> 3634.00s]  and let's feed it through the output.
[3634.00s -> 3641.00s]  Okay, so rand n unit of Gaussian seems innocuous.
[3641.00s -> 3643.00s]  What happens when you do this
[3643.00s -> 3645.00s]  is that if you look at the output,
[3645.00s -> 3648.00s]  you get some pretty large numbers, right?
[3648.00s -> 3654.00s]  And this is because when you have the number grows
[3654.00s -> 3658.00s]  as essentially the square root of the hidden dimension,
[3658.00s -> 3661.00s]  and so when you have large models,
[3661.00s -> 3663.00s]  this is going to blow up,
[3663.00s -> 3668.00s]  and training can be very unstable.
[3668.00s -> 3673.00s]  So typically what you want to do is initialize
[3673.00s -> 3676.00s]  in a way that's invariant to hidden,
[3676.00s -> 3678.00s]  or at least when you're guaranteed
[3678.00s -> 3680.00s]  that it's not going to blow up.
[3680.00s -> 3682.00s]  And one simple way to do this
[3682.00s -> 3685.00s]  is just rescale by the one over square root
[3685.00s -> 3689.00s]  of number of inputs.
[3689.00s -> 3692.00s]  So basically let's redo this.
[3692.00s -> 3695.00s]  W equals a parameter where I simply divide
[3695.00s -> 3700.00s]  by the square root of the input dimension.
[3700.00s -> 3703.00s]  And then now when you feed it through the output,
[3703.00s -> 3706.00s]  now you get things that are stable around,
[3706.00s -> 3713.00s]  this will actually concentrate to something like normal zero one.
[3713.00s -> 3716.00s]  Okay, so this is basically,
[3716.00s -> 3718.00s]  this has been explored pretty extensively
[3718.00s -> 3720.00s]  in deep learning literature.
[3720.00s -> 3724.00s]  It's known up to a constant as savior initialization.
[3724.00s -> 3727.00s]  And typically I guess it's fairly common
[3727.00s -> 3729.00s]  if you want to be extra safe.
[3729.00s -> 3731.00s]  You don't trust the normal because it doesn't have,
[3731.00s -> 3734.00s]  it has unbounded tails and you just say,
[3734.00s -> 3736.00s]  I'm going to truncate to minus three three
[3736.00s -> 3738.00s]  so I don't get any large values
[3738.00s -> 3741.00s]  and I don't want any to mess with that.
[3741.00s -> 3743.00s]  Okay.
[3746.00s -> 3748.00s]  Okay, so.
[3751.00s -> 3756.00s]  So let's build just a simple model.
[3756.00s -> 3761.00s]  It's going to have d dimensions and two layers.
[3761.00s -> 3764.00s]  There's this, I just made up this name, Cruncher.
[3764.00s -> 3768.00s]  It's a custom model which is a deep linear network
[3768.00s -> 3772.00s]  which has n num layers layers
[3772.00s -> 3777.00s]  and each layer is a linear model
[3777.00s -> 3781.00s]  which has essentially just a matrix multiplication.
[3781.00s -> 3783.00s]  Okay.
[3783.00s -> 3788.00s]  So the parameters of this model
[3788.00s -> 3797.00s]  looks like I have layers for the first layer
[3797.00s -> 3800.00s]  which is a d by d matrix.
[3800.00s -> 3802.00s]  The second layer which is also a d by d matrix
[3802.00s -> 3807.00s]  and then I have a head or a final layer.
[3807.00s -> 3812.00s]  Okay, so if I get the number of parameters
[3812.00s -> 3816.00s]  of this model, then it's going to be
[3816.00s -> 3819.00s]  d squared plus d squared plus d.
[3819.00s -> 3822.00s]  Okay, so nothing too surprising there.
[3822.00s -> 3825.00s]  And I'm going to move it to the GPU
[3825.00s -> 3828.00s]  because I want this to run fast
[3828.00s -> 3831.00s]  and I'm going to generate some random data
[3831.00s -> 3833.00s]  and feed it through the data
[3833.00s -> 3837.00s]  and the forward pass is just going through the layers
[3837.00s -> 3841.00s]  and then finally applying the head.
[3843.00s -> 3846.00s]  Okay, so with that model,
[3846.00s -> 3849.00s]  let's try to, I'm going to use this model
[3849.00s -> 3851.00s]  and do some stuff with it.
[3851.00s -> 3854.00s]  But just one kind of general digression.
[3854.00s -> 3860.00s]  Randomness is something that is sort of,
[3860.00s -> 3862.00s]  can be annoying in some cases
[3862.00s -> 3864.00s]  if you're trying to reproduce a bug, for example.
[3864.00s -> 3866.00s]  It shows up in many places,
[3866.00s -> 3868.00s]  initialization, dropout, data ordering.
[3869.00s -> 3872.00s]  And just some best practices.
[3872.00s -> 3876.00s]  We recommend you always pass, fix a random seed
[3876.00s -> 3879.00s]  so you can reproduce your model
[3879.00s -> 3881.00s]  or at least as well as you can.
[3883.00s -> 3886.00s]  And in particular, having a different random seed
[3886.00s -> 3889.00s]  for every source of randomness is nice
[3889.00s -> 3891.00s]  because then you can, for example,
[3891.00s -> 3894.00s]  fix initialization or fix the data ordering
[3894.00s -> 3896.00s]  but very other things.
[3896.00s -> 3900.00s]  Determinism is your friend when you're debugging.
[3900.00s -> 3905.00s]  And in code, unfortunately there's many places
[3905.00s -> 3907.00s]  where you can use randomness
[3907.00s -> 3911.00s]  and just be cognizant of which one you're using
[3911.00s -> 3914.00s]  and if you want to be safe,
[3914.00s -> 3916.00s]  just set the seed to for all of them.
[3919.00s -> 3923.00s]  Data loading, I guess I'll go through this quickly.
[3923.00s -> 3929.00s]  It's not, it'll be useful for your assignment.
[3929.00s -> 3932.00s]  So in language modeling, data is typically
[3932.00s -> 3934.00s]  just a sequence of integers
[3934.00s -> 3937.00s]  because this is, remember, output by the tokenizer
[3937.00s -> 3941.00s]  and you serialize them into,
[3941.00s -> 3944.00s]  you can serialize them into NumPy arrays.
[3944.00s -> 3949.00s]  And one, I guess, thing that's maybe useful
[3949.00s -> 3952.00s]  is that you don't want to load all your data
[3952.00s -> 3955.00s]  into memory at once because, for example,
[3955.00s -> 3958.00s]  the llama data is 2.8 terabytes.
[3958.00s -> 3961.00s]  But you can sort of pretend to load it
[3961.00s -> 3964.00s]  by using this handy function called memmap
[3964.00s -> 3967.00s]  which gives you essentially a variable
[3967.00s -> 3970.00s]  that is mapped to a file.
[3970.00s -> 3973.00s]  So when you try to access the data,
[3973.00s -> 3977.00s]  it actually on demand loads the file.
[3977.00s -> 3981.00s]  And then using that, you can create a data loader
[3981.00s -> 3988.00s]  that samples data from your batch.
[3988.00s -> 3992.00s]  So I'm gonna skip over that, just in the interest of time.
[3992.00s -> 3995.00s]  Let's talk a little bit about optimizers.
[3995.00s -> 3998.00s]  So we've defined our model.
[4001.00s -> 4005.00s]  So there's many optimizers,
[4005.00s -> 4008.00s]  just kind of maybe going through
[4008.00s -> 4010.00s]  the intuitions behind some of them.
[4010.00s -> 4012.00s]  So of course there's stochastic gradient descent.
[4012.00s -> 4014.00s]  You compute the gradient of your batch.
[4014.00s -> 4015.00s]  You take a step in that direction.
[4015.00s -> 4017.00s]  No questions asked.
[4017.00s -> 4019.00s]  There's an idea called momentum
[4019.00s -> 4023.00s]  which dates back to classic optimization, Nesterov,
[4023.00s -> 4029.00s]  where you have a running average of your gradients
[4029.00s -> 4033.00s]  and you update against the running average
[4033.00s -> 4036.00s]  instead of your instantaneous gradient.
[4036.00s -> 4039.00s]  And then you have Adagrad,
[4039.00s -> 4043.00s]  which you scale the gradients
[4043.00s -> 4054.00s]  by the average over the square of the gradients.
[4054.00s -> 4056.00s]  You also have RMSprop,
[4056.00s -> 4059.00s]  which is an improved version of Adagrad
[4059.00s -> 4061.00s]  which uses exponential averaging
[4061.00s -> 4064.00s]  rather than just like a flat average.
[4064.00s -> 4067.00s]  And then finally Adam, which appeared in 2014,
[4067.00s -> 4071.00s]  which is essentially combining RMSprop and momentum.
[4071.00s -> 4073.00s]  So that's why you're maintaining
[4073.00s -> 4077.00s]  both your running average of your gradients
[4077.00s -> 4080.00s]  but also running average of your gradients squared.
[4080.00s -> 4086.00s]  Okay, so since you're gonna implement Adam in homework one,
[4086.00s -> 4087.00s]  I'm not gonna do that.
[4087.00s -> 4091.00s]  Instead, I'm gonna implement Adagrad.
[4091.00s -> 4098.00s]  So the way you implement an optimizer in PyTorch
[4098.00s -> 4100.00s]  is that you override the optimizer class
[4100.00s -> 4105.00s]  and you have to, let's see,
[4105.00s -> 4108.00s]  maybe I'll get to the implementation
[4108.00s -> 4111.00s]  once we've stepped through it.
[4111.00s -> 4115.00s]  So let's define some data,
[4115.00s -> 4119.00s]  compute the forward pass on the loss,
[4119.00s -> 4122.00s]  and then you compute the gradients.
[4122.00s -> 4127.00s]  And then when you call optimizer.step,
[4127.00s -> 4132.00s]  this is where the optimizer actually is active.
[4132.00s -> 4139.00s]  So what this looks like is your parameters are grouped by,
[4139.00s -> 4142.00s]  for example, you have one for the layer zero, layer one,
[4142.00s -> 4145.00s]  and then the final weights.
[4145.00s -> 4151.00s]  And you can access a state, which is a dictionary,
[4151.00s -> 4157.00s]  from parameters to whatever you want to store
[4157.00s -> 4160.00s]  as the optimizer state.
[4160.00s -> 4163.00s]  The gradient of that parameter you assume
[4163.00s -> 4169.00s]  is already calculated by the backward pass.
[4169.00s -> 4175.00s]  And now you can do things like in Adagrad,
[4175.00s -> 4179.00s]  you're storing the sum of the gradient squareds.
[4179.00s -> 4182.00s]  So you can get that G2 variable
[4182.00s -> 4186.00s]  and you can update that based on the square of the gradient.
[4186.00s -> 4190.00s]  So this is element-wise squaring of the gradient
[4190.00s -> 4193.00s]  and you put it back into the state.
[4193.00s -> 4196.00s]  So then obviously your optimizer is responsible
[4196.00s -> 4200.00s]  for updating the parameters and this is just the,
[4200.00s -> 4202.00s]  you update the learning rate times the gradient
[4202.00s -> 4205.00s]  divided by this scaling.
[4205.00s -> 4208.00s]  So now this state is kept over
[4208.00s -> 4214.00s]  across multiple invocations of the optimizer.
[4217.00s -> 4223.00s]  Okay, so, and then at the end of your optimizer step
[4223.00s -> 4228.00s]  you can free up the memory just to,
[4228.00s -> 4231.00s]  which is, I think, going to actually be more important
[4231.00s -> 4235.00s]  when you look, when we talk about model parallelism.
[4235.00s -> 4238.00s]  Okay, so let's talk about the memory requirements
[4238.00s -> 4241.00s]  of the optimizer states.
[4241.00s -> 4244.00s]  And actually, basically at this point, everything.
[4244.00s -> 4250.00s]  So, you need to, the number of parameters in this model
[4250.00s -> 4253.00s]  is D squared times the number of layers
[4253.00s -> 4257.00s]  plus D for the final head.
[4257.00s -> 4263.00s]  Okay, the number of activations,
[4263.00s -> 4265.00s]  so this is something we didn't do before,
[4265.00s -> 4268.00s]  but now for this simple model it's fairly easy to do.
[4268.00s -> 4275.00s]  It's just B times, you know, D times the number of layers
[4275.00s -> 4277.00s]  you have for every layer, for every data point,
[4277.00s -> 4281.00s]  for every dimension you have to hold the activations.
[4281.00s -> 4287.00s]  For the gradients, this is the same as the number of parameters.
[4287.00s -> 4290.00s]  And the number of optimizer states,
[4290.00s -> 4297.00s]  and for Adagrad it's, you'll remember we had to store
[4297.00s -> 4302.00s]  the gradient squared, so that's another copy of the parameters.
[4302.00s -> 4308.00s]  So, putting it all together, we have the total memory
[4308.00s -> 4313.00s]  is assuming, you know, FP32, which means four bytes,
[4313.00s -> 4317.00s]  times the number of parameters, number of activations,
[4317.00s -> 4320.00s]  number of gradients, and number of optimizer states.
[4320.00s -> 4324.00s]  Okay, and that gives us, you know, some number,
[4324.00s -> 4329.00s]  which is 496 here.
[4330.00s -> 4333.00s]  Okay, so this is a fairly simple calculation.
[4333.00s -> 4336.00s]  In the assignment one, you're going to do this
[4336.00s -> 4339.00s]  for the transformer, which is a little bit more involved
[4339.00s -> 4342.00s]  because you have to, there's not just matrix multiplications,
[4342.00s -> 4345.00s]  but there's many matrices, there's a tension,
[4345.00s -> 4347.00s]  and there's all these other things.
[4347.00s -> 4351.00s]  But the general form of the calculation is the same.
[4351.00s -> 4354.00s]  You have parameters, activations, gradients,
[4354.00s -> 4358.00s]  and optimizer states.
[4359.00s -> 4367.00s]  Okay, and the, so, and the flops required, again,
[4367.00s -> 4372.00s]  for this model is six times the number of tokens,
[4372.00s -> 4375.00s]  or number of data points times the number of parameters,
[4375.00s -> 4380.00s]  and, you know, that basically concludes the resource accounting
[4380.00s -> 4383.00s]  for this particular model.
[4383.00s -> 4387.00s]  And if, for reference, if you're curious about
[4387.00s -> 4390.00s]  working this out for transformers,
[4390.00s -> 4394.00s]  you can consult some of these articles.
[4394.00s -> 4398.00s]  Okay, so in the remaining time, I think,
[4398.00s -> 4402.00s]  maybe I'll pause for questions.
[4402.00s -> 4404.00s]  We talked about building up the tensors,
[4404.00s -> 4407.00s]  and then we built a kind of a very small model,
[4407.00s -> 4411.00s]  and, you know, we talked about optimization
[4411.00s -> 4413.00s]  and how many, how much memory
[4413.00s -> 4416.00s]  and how much compute was required.
[4418.00s -> 4419.00s]  Yeah?
[4419.00s -> 4425.00s]  So the question is, why do you need to store the activations?
[4425.00s -> 4427.00s]  So, naively, you need to store the activations
[4427.00s -> 4432.00s]  because when you're doing the p-core pass,
[4432.00s -> 4435.00s]  the gradients of, let's say,
[4435.00s -> 4437.00s]  the first layer depend on the activation.
[4437.00s -> 4438.00s]  So the gradients of the i-th layer
[4438.00s -> 4441.00s]  depends on the activation there.
[4441.00s -> 4443.00s]  Now, if you're smarter,
[4443.00s -> 4446.00s]  you don't have to store the activations,
[4446.00s -> 4447.00s]  or you don't have to store all of them.
[4447.00s -> 4449.00s]  You can recompute them,
[4449.00s -> 4451.00s]  and that's something a technique will,
[4451.00s -> 4453.00s]  called activation checkpointing,
[4453.00s -> 4455.00s]  which we can talk about later.
[4459.00s -> 4462.00s]  Okay, so let's just do this quick,
[4462.00s -> 4464.00s]  you know, actually, there's not much to say here,
[4464.00s -> 4466.00s]  but, you know, here's your typical, you know,
[4466.00s -> 4471.00s]  training loop where you define the model,
[4471.00s -> 4473.00s]  define the optimizer,
[4473.00s -> 4478.00s]  and you get the data, feed forward, backward,
[4478.00s -> 4481.00s]  and take a step in a parameter space.
[4483.00s -> 4490.00s]  And, I guess, it'd be more interesting,
[4490.00s -> 4492.00s]  I guess next time I should show, like,
[4492.00s -> 4496.00s]  an actual 1B plot, which isn't available on this version.
[4496.00s -> 4503.00s]  But, so, one note about checkpointing,
[4503.00s -> 4507.00s]  so training language models takes a long time,
[4507.00s -> 4511.00s]  and you certainly will crash at some point,
[4511.00s -> 4513.00s]  so you don't want to lose all your progress.
[4513.00s -> 4516.00s]  So you want to periodically save your model to disk,
[4516.00s -> 4518.00s]  and just to be very clear,
[4518.00s -> 4520.00s]  the thing you want to save is
[4520.00s -> 4524.00s]  both the model and the optimizer,
[4524.00s -> 4527.00s]  and probably, you know, which iteration you're on,
[4527.00s -> 4531.00s]  I should add that, and then you can just load it up.
[4533.00s -> 4536.00s]  One, maybe, final note, and is,
[4536.00s -> 4540.00s]  I alluded to kind of mixed precision training.
[4542.00s -> 4545.00s]  You know, choice of the data type
[4545.00s -> 4546.00s]  has different trade-offs.
[4546.00s -> 4551.00s]  If you have higher precision, it's more accurate and stable,
[4551.00s -> 4556.00s]  but it's more expensive, and lower precision is vice versa.
[4556.00s -> 4559.00s]  And, as we mentioned before,
[4559.00s -> 4563.00s]  by default, the recommendations use float 32,
[4563.00s -> 4568.00s]  but try to use BF16 or even FP8 whenever possible.
[4568.00s -> 4572.00s]  So you can use lower precision for the feed forward pass,
[4572.00s -> 4575.00s]  but float 32 for the rest.
[4575.00s -> 4578.00s]  And this is an idea that goes back to the, you know,
[4578.00s -> 4581.00s]  2017, there's exploring mixed precision training.
[4581.00s -> 4585.00s]  PyTorch has some tools that automatically allow you
[4585.00s -> 4588.00s]  to do mixed precision training,
[4588.00s -> 4590.00s]  because it can be sort of annoying
[4590.00s -> 4594.00s]  to have to specify which parts of your model
[4594.00s -> 4597.00s]  it needs to be, you know, what precision.
[4597.00s -> 4599.00s]  Generally, you define your model as, you know,
[4599.00s -> 4601.00s]  sort of this clean modular thing,
[4601.00s -> 4605.00s]  and specifying the precision is sort of like a,
[4605.00s -> 4610.00s]  you know, something that needs to cut across that.
[4610.00s -> 4614.00s]  And one, I guess maybe one kind of general comment
[4614.00s -> 4618.00s]  is that people are pushing the envelope
[4618.00s -> 4620.00s]  on what precision is needed.
[4620.00s -> 4622.00s]  There's some papers that show
[4622.00s -> 4627.00s]  you can actually use FP8 all the way through.
[4627.00s -> 4632.00s]  There's, I guess one of the challenges is,
[4632.00s -> 4634.00s]  of course, when you have lower precision,
[4634.00s -> 4636.00s]  it gets very numerically unstable,
[4636.00s -> 4639.00s]  but then you can do various tricks to, you know,
[4639.00s -> 4643.00s]  control the numerics of your model during training
[4643.00s -> 4646.00s]  so that you don't get into these, you know, bad regimes.
[4646.00s -> 4650.00s]  So this is where I think the systems
[4650.00s -> 4654.00s]  and the model architecture design kind of are synergistic
[4654.00s -> 4659.00s]  because you want to design models now that we have,
[4659.00s -> 4662.00s]  a lot of model design is just governed by hardware.
[4662.00s -> 4665.00s]  So even the transformer, as we mentioned last time,
[4665.00s -> 4667.00s]  is governed by having GPUs.
[4667.00s -> 4670.00s]  And now, if we notice that NVIDIA chips
[4670.00s -> 4675.00s]  have the property that if lower precision,
[4675.00s -> 4678.00s]  even like INT4, for example, is one thing,
[4678.00s -> 4681.00s]  now if you can make your model training
[4681.00s -> 4685.00s]  actually work on INT4, which is, I think, quite hard,
[4685.00s -> 4688.00s]  then you can get massive speed-ups
[4688.00s -> 4691.00s]  and your model will be more efficient.
[4692.00s -> 4697.00s]  Now, there's another thing which we'll talk about later,
[4697.00s -> 4699.00s]  which is, you know, often you'll train your model
[4699.00s -> 4702.00s]  using more sane floating point,
[4702.00s -> 4705.00s]  but when it comes to inference, you can go crazy
[4705.00s -> 4709.00s]  and you take your model and then you can quantize it
[4709.00s -> 4711.00s]  and get a lot of the gains
[4711.00s -> 4714.00s]  from very, very aggressive quantization.
[4714.00s -> 4717.00s]  So somehow training is a lot more difficult
[4717.00s -> 4720.00s]  to do with low precision, but once you have a trained model,
[4720.00s -> 4724.00s]  it's much easier to make it low precision.
[4724.00s -> 4727.00s]  Okay, so I will wrap up there.
[4727.00s -> 4731.00s]  Just to conclude, we have talked about
[4731.00s -> 4734.00s]  the different primitives to use to train a model,
[4734.00s -> 4737.00s]  building up from tensors all the way to the training loop.
[4737.00s -> 4741.00s]  We talked about memory accounting and FLOPS accounting
[4741.00s -> 4743.00s]  for these simple models.
[4743.00s -> 4746.00s]  Hopefully, once you go through assignment one,
[4746.00s -> 4748.00s]  all of these concepts will be really solid
[4748.00s -> 4751.00s]  because you'll be applying these ideas
[4751.00s -> 4754.00s]  for the actual transformer.
[4754.00s -> 4756.00s]  Okay, see you next time.
