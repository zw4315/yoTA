# Detected language: en (p=1.00)

[0.00s -> 8.00s]  So this is lecture 10, we're going to take a brief respite
[8.00s -> 13.00s]  from scaling laws and we're going to talk about inference.
[13.00s -> 17.00s]  So the question is inference is a very simple one.
[17.00s -> 19.00s]  Given a fixed model that we've trained,
[19.00s -> 24.00s]  generate responses given prompts, okay?
[24.00s -> 27.00s]  First we're going to start by understanding
[27.00s -> 30.00s]  what the implications of inference are
[30.00s -> 33.00s]  and the workload that it entails
[33.00s -> 35.00s]  and then we're going to talk about ways
[35.00s -> 38.00s]  of making inference faster.
[38.00s -> 40.00s]  And throughout this lecture you're going to see
[40.00s -> 44.00s]  that there's a lot of, inference is a very deep topic.
[44.00s -> 47.00s]  It's actually, we didn't do inference last year in lecture
[47.00s -> 49.00s]  so this is the first year we're doing it
[49.00s -> 50.00s]  but there's actually many, many topics
[50.00s -> 52.00s]  that could span multiple lectures
[52.00s -> 55.00s]  which I'll try to condense into one.
[55.00s -> 58.00s]  So inference shows up in multiple different places.
[58.00s -> 61.00s]  The most obvious place is if you actually want to use a model.
[61.00s -> 63.00s]  You want to use it to chat,
[63.00s -> 67.00s]  if you're using cursor or something to do code completion,
[67.00s -> 69.00s]  if you're running batch data processing job
[69.00s -> 71.00s]  using your language model,
[71.00s -> 73.00s]  all of these cases demand inference
[73.00s -> 77.00s]  because you need to generate tokens from your actual model.
[77.00s -> 79.00s]  But it also shows up in other contexts
[79.00s -> 82.00s]  if you want to even evaluate your model,
[82.00s -> 84.00s]  this is to say on instruction following,
[84.00s -> 86.00s]  you need to do inference.
[86.00s -> 89.00s]  There is a lot of interest in test time compute
[89.00s -> 94.00s]  which means thinking more before you actually output
[94.00s -> 96.00s]  some, the final answer
[98.00s -> 100.00s]  and that's also more inference
[100.00s -> 102.00s]  because thinking is basically generate tokens.
[102.00s -> 105.00s]  And then finally, even training itself,
[105.00s -> 107.00s]  if you're using reinforcement learning,
[107.00s -> 110.00s]  you need to sample responses
[110.00s -> 112.00s]  and evaluate them based on some reward
[112.00s -> 115.00s]  and that also requires inference.
[115.00s -> 118.00s]  So inference isn't just I want to put up a chatbot demo.
[118.00s -> 120.00s]  Inference actually is going to underlie
[120.00s -> 124.00s]  many of the basic functions of a language model
[124.00s -> 125.00s]  and even though it's one lecture,
[125.00s -> 128.00s]  I want to stress how actually important it is
[128.00s -> 131.00s]  for many things and we'll probably come back to this
[131.00s -> 135.00s]  when we talk about alignment later in the class.
[137.00s -> 139.00s]  So now, inference is important.
[139.00s -> 141.00s]  So the theme of this class is efficiency
[141.00s -> 144.00s]  and efficiency clearly matters.
[144.00s -> 145.00s]  Training is a one-time cost
[145.00s -> 147.00s]  but inference you repeat multiple times.
[147.00s -> 152.00s]  So here's some, you know, anecdotal stats
[152.00s -> 155.00s]  on why inference is a big deal.
[155.00s -> 159.00s]  So Sam says OpenAI generates 100 billion words a day
[159.00s -> 162.00s]  which is quite a lot.
[162.00s -> 165.00s]  And even cursor which is not that new of a product
[165.00s -> 168.00s]  is allegedly generating a billion lines
[168.00s -> 171.00s]  of accepted code each day.
[171.00s -> 175.00s]  So it's just giving you an idea
[175.00s -> 178.00s]  of how much inference is accounting for
[178.00s -> 182.00s]  and costs of inference compared to training
[182.00s -> 185.00s]  are definitely increasing.
[185.00s -> 188.00s]  So how do you measure what inference,
[188.00s -> 190.00s]  good inference looks like?
[190.00s -> 194.00s]  So there's time to first token, TTFT.
[194.00s -> 197.00s]  So this is how long an individual user needs to wait
[197.00s -> 200.00s]  before any generation happens at all.
[200.00s -> 202.00s]  And this matters clearly for interactive applications.
[202.00s -> 203.00s]  If you have a big prompt
[203.00s -> 205.00s]  and then you have to wait there for 10 seconds,
[205.00s -> 208.00s]  that may not be a good user experience.
[208.00s -> 211.00s]  Latency is how fast tokens are arriving
[211.00s -> 215.00s]  after maybe the first token.
[215.00s -> 219.00s]  This also matters for interactive applications.
[219.00s -> 221.00s]  Throughput is something a bit different.
[221.00s -> 225.00s]  Throughput is how many tokens in general are generated
[225.00s -> 227.00s]  but not for overall users.
[227.00s -> 229.00s]  So this is particularly useful
[229.00s -> 231.00s]  in batch processing applications.
[231.00s -> 233.00s]  So you can think about the throughput is
[233.00s -> 236.00s]  high throughput doesn't mean low latency
[236.00s -> 240.00s]  because some requests might just take a very long time
[240.00s -> 242.00s]  and you still have high throughput.
[242.00s -> 244.00s]  Latency is kind of like the worst case
[244.00s -> 248.00s]  over any individual user.
[248.00s -> 250.00s]  So what do you need to think about
[250.00s -> 254.00s]  when you think about the efficiency of inference?
[254.00s -> 258.00s]  So in training, the key idea is that
[258.00s -> 260.00s]  you get to see all the tokens,
[260.00s -> 262.00s]  at least the supervised training,
[262.00s -> 265.00s]  which means that you can paralyze over the sequence.
[265.00s -> 267.00s]  This is exploited heavily in the transformer, right?
[267.00s -> 269.00s]  So you've done the transformer training.
[269.00s -> 271.00s]  You know that you basically construct these tensors
[271.00s -> 272.00s]  over the entire sequence
[272.00s -> 276.00s]  and it's just like tensor, tensor, tensor, matmals
[276.00s -> 279.00s]  and then you get your output.
[279.00s -> 282.00s]  But the key defining feature of inference,
[282.00s -> 283.00s]  at least for transformers,
[283.00s -> 285.00s]  is that you have to generate sequentially.
[285.00s -> 288.00s]  You can't paralyze because
[288.00s -> 293.00s]  the generation of a token depends on all of the past.
[293.00s -> 295.00s]  So this is gonna be the key thing
[295.00s -> 298.00s]  that's gonna make inference a lot harder
[298.00s -> 300.00s]  and in particular it's gonna be harder
[300.00s -> 303.00s]  to utilize all of the compute that's available
[303.00s -> 304.00s]  and it's gonna be memory limited
[304.00s -> 307.00s]  as we'll see in detail later.
[307.00s -> 309.00s]  So a lot of people are doing inference.
[309.00s -> 312.00s]  Anyone who actually has a product and platform
[312.00s -> 315.00s]  quickly realizes that these costs in doing large models
[315.00s -> 319.00s]  is gonna go up, so they spend a lot of time
[319.00s -> 323.00s]  and engineering effort trying to reduce that time.
[323.00s -> 325.00s]  So both providers serving closed models
[325.00s -> 329.00s]  and providers serving open-weight models
[329.00s -> 333.00s]  pay a lot of attention to inference.
[333.00s -> 335.00s]  More so than I think the average academic
[335.00s -> 337.00s]  because we're not actually serving any models.
[337.00s -> 338.00s]  We're just training and getting a score
[338.00s -> 340.00s]  and putting it in the paper.
[340.00s -> 341.00s]  But people who are actually serving models
[341.00s -> 345.00s]  pay a lot of attention to inference.
[345.00s -> 347.00s]  So there's also a bunch of open source packages
[347.00s -> 351.00s]  which are interesting to look at as well.
[353.00s -> 357.00s]  Okay, so I want to understand the inference workload
[359.00s -> 360.00s]  kind of in detail.
[360.00s -> 363.00s]  So I'm going to review briefly
[363.00s -> 365.00s]  just sort of this transformer math
[365.00s -> 368.00s]  that you did in assignment one
[368.00s -> 369.00s]  and we talked a little bit about it
[369.00s -> 372.00s]  during the first week of class.
[372.00s -> 377.00s]  So this is from the scaling JaxML book
[377.00s -> 381.00s]  which is something you guys should really take a look at.
[381.00s -> 382.00s]  I think it does an excellent job
[382.00s -> 386.00s]  of outlining many of the key concepts here.
[386.00s -> 389.00s]  And they have this really nice diagram
[389.00s -> 393.00s]  that shows essentially the computation graph
[393.00s -> 397.00s]  taken in input and having it go through attention
[397.00s -> 399.00s]  and the MLP layers.
[399.00s -> 401.00s]  In particular, we're going to use this notation
[401.00s -> 404.00s]  so just to kind of review this quickly.
[404.00s -> 407.00s]  So B is the number of sequences in your batch.
[407.00s -> 409.00s]  L is the number of layers.
[409.00s -> 411.00s]  T is the sequence length.
[411.00s -> 413.00s]  You can think about it as the number of tokens
[413.00s -> 417.00s]  you're going to generate or query using.
[417.00s -> 419.00s]  S is also the sequence length
[419.00s -> 423.00s]  but how many you're kind of conditioning on in your prompt.
[423.00s -> 424.00s]  V is the vocabulary.
[424.00s -> 426.00s]  D is the dimensionality of your model.
[426.00s -> 429.00s]  F is the MLP hidden dimension
[429.00s -> 431.00s]  which is usually four times D.
[431.00s -> 434.00s]  H is the attention head dimension.
[434.00s -> 436.00s]  N is the number of query heads
[436.00s -> 439.00s]  so generally N times H equals D.
[439.00s -> 443.00s]  And then in GQA, group query attention,
[443.00s -> 445.00s]  you have a different number of key value heads
[445.00s -> 446.00s]  as query heads.
[446.00s -> 448.00s]  Usually K is smaller than N.
[448.00s -> 453.00s]  And G is the number of groups.
[453.00s -> 456.00s]  So K times G equals N.
[456.00s -> 458.00s]  And this diagram shows that you take your X,
[458.00s -> 464.00s]  you feed through the Q KV matrices
[464.00s -> 468.00s]  and you do a bunch of things.
[469.00s -> 476.00s]  Okay, so remember that the flops required for a FIFA pass
[476.00s -> 479.00s]  is six times the number of tokens
[479.00s -> 482.00s]  which is B times T times the number of parameters
[482.00s -> 486.00s]  plus for the attention there's another order T.
[486.00s -> 490.00s]  So T times T is T squared dependence.
[491.00s -> 495.00s]  Okay, so let's also review arithmetic and intensity
[495.00s -> 497.00s]  which is going to help us characterize
[497.00s -> 500.00s]  when something is compute limited versus memory limited.
[500.00s -> 505.00s]  So just to start with the basic matmul.
[505.00s -> 509.00s]  So let's take a matrix X which is B by D
[509.00s -> 512.00s]  and a matrix W, D by F.
[512.00s -> 517.00s]  And just to give some color to this computation,
[517.00s -> 520.00s]  B is the batch size, D is the hidden dimension
[520.00s -> 524.00s]  and F is the upper projection matrix in the gated MLP.
[524.00s -> 528.00s]  So let's do count the number of flops
[528.00s -> 534.00s]  and memory read and writes for just doing X times W.
[534.00s -> 538.00s]  Okay, so we're going to start with initialize to zero.
[538.00s -> 541.00s]  And what one has to do for this is
[541.00s -> 547.00s]  we're going to read X from HBM.
[547.00s -> 550.00s]  So that means it incurs a memory cost
[550.00s -> 554.00s]  of two times B times D assuming everything is in BF16.
[554.00s -> 556.00s]  You also read W.
[556.00s -> 560.00s]  So that's two times D times F.
[560.00s -> 564.00s]  Then you do the matmul and that incurs
[564.00s -> 567.00s]  two times B times D times F flops.
[567.00s -> 569.00s]  So remember this is from the first lecture
[569.00s -> 572.00s]  so hopefully this is review.
[572.00s -> 573.00s]  And then you have to write it back out
[573.00s -> 577.00s]  which is you have to pay another transfer.
[577.00s -> 582.00s]  Okay, so the total number of flops is just the matmul.
[582.00s -> 585.00s]  And the number of bytes transferred is essentially
[585.00s -> 589.00s]  the size of all the matrices that are read and written.
[589.00s -> 593.00s]  And arithmetic intensity is basically the ratio.
[593.00s -> 596.00s]  So the ratio is this expression.
[596.00s -> 599.00s]  And in general, just to simplify things a bit,
[599.00s -> 604.00s]  generally the batch size is much less than D and F.
[604.00s -> 608.00s]  B may be hundreds and D and F might be thousands
[608.00s -> 611.00s]  or tens of thousands.
[611.00s -> 616.00s]  So I'm using SymPy here just to keep myself
[616.00s -> 619.00s]  from making silly mistakes.
[619.00s -> 623.00s]  So basically I'm letting C go to infinity
[623.00s -> 627.00s]  and D scales as C times B and F scales as C times B.
[627.00s -> 632.00s]  And that gets you a simplified equation of B.
[632.00s -> 635.00s]  So the arithmetic intensity is B
[635.00s -> 639.00s]  for this particular matrix multiplication.
[644.00s -> 648.00s]  So the way to interpret this is
[648.00s -> 653.00s]  how many flops are done per byte that was transferred.
[653.00s -> 658.00s]  So now the second part is you look at the accelerator
[658.00s -> 665.00s]  which for H100, flops per second is 989 teraflops,
[665.00s -> 670.00s]  memory bandwidth 3.3 terabytes per second.
[670.00s -> 672.00s]  And you divide and that gives you
[672.00s -> 675.00s]  what is called the accelerator intensity.
[675.00s -> 680.00s]  And if you look at the computation intensity,
[680.00s -> 683.00s]  which is B, if it's greater than the accelerator intensity
[683.00s -> 684.00s]  that means you're computer limited,
[684.00s -> 687.00s]  that means you're able to use all the GPUs or TPUs.
[687.00s -> 691.00s]  And if you're less than that, then you're memory limited,
[691.00s -> 693.00s]  which is, you know, bad.
[693.00s -> 695.00s]  And so you're computer limited
[695.00s -> 697.00s]  in this matrix multiplication case
[697.00s -> 702.00s]  if B is greater than 295 for H100.
[702.00s -> 706.00s]  And all of this is a bit idealized, the actual details.
[706.00s -> 711.00s]  This is giving you a kind of a first order approximation.
[711.00s -> 714.00s]  So in extreme case, so generally that means
[714.00s -> 717.00s]  if you use batches of size, let's say 300,
[717.00s -> 720.00s]  then you're going to be able to saturate the GPU.
[720.00s -> 722.00s]  But what happens if your batch is really small?
[722.00s -> 724.00s]  So in particular B equals one,
[724.00s -> 727.00s]  which essentially corresponds to a matrix vector product,
[727.00s -> 730.00s]  then the arithmetic intensity is basically one.
[730.00s -> 732.00s]  And that is really, really bad.
[732.00s -> 736.00s]  That means you're going to be memory limited
[736.00s -> 739.00s]  and which kind of makes sense
[739.00s -> 742.00s]  because basically you're reading and writing
[742.00s -> 745.00s]  this D times, or actually you're just reading
[745.00s -> 747.00s]  this D times F matrix,
[747.00s -> 748.00s]  and you're performing essentially
[748.00s -> 750.00s]  the same number of flops.
[750.00s -> 752.00s]  So the ratio between the flops and the reads
[752.00s -> 754.00s]  is the same, which gives you one,
[754.00s -> 755.00s]  and one is bad.
[755.00s -> 758.00s]  You want a lot of flops to be done for any memory read
[758.00s -> 762.00s]  because memory reads are slow.
[762.00s -> 765.00s]  But this is in essence what happens with generation.
[765.00s -> 767.00s]  Because you're proceeding token by token,
[767.00s -> 771.00s]  we'll see that basically your arithmetic intensity
[771.00s -> 773.00s]  is going to be like one.
[773.00s -> 777.00s]  And that's why generation is going to be memory limited
[777.00s -> 779.00s]  and not computer limited.
[779.00s -> 781.00s]  Okay, so this is a very simple example
[781.00s -> 782.00s]  that I think gets at the core
[782.00s -> 785.00s]  of why generation is going to be slow.
[785.00s -> 789.00s]  So maybe I'll pause and take any questions on this
[789.00s -> 793.00s]  just to make sure everyone's clear.
[793.00s -> 794.00s]  Yeah?
[795.00s -> 801.00s]  So I think I heard the question,
[801.00s -> 803.00s]  why don't we have a batch size more than one?
[803.00s -> 805.00s]  So I'll get to why you can,
[805.00s -> 808.00s]  but batch size is going to mean
[808.00s -> 811.00s]  batch size times sequence length later.
[817.00s -> 820.00s]  Okay, so in summary,
[820.00s -> 823.00s]  matrix multiplications are the core computation
[824.00s -> 827.00s]  so we just studied a matrix multiplication
[827.00s -> 829.00s]  and counted the number of flops it requires
[829.00s -> 832.00s]  over the number of reads and writes.
[832.00s -> 834.00s]  And we show that that ratio,
[834.00s -> 836.00s]  which is the arithmetic intensity,
[836.00s -> 838.00s]  depends on one of the dimensions,
[838.00s -> 841.00s]  in this case the batch dimension.
[841.00s -> 843.00s]  And that's why big matrices are good
[843.00s -> 846.00s]  because that can saturate your compute,
[846.00s -> 849.00s]  whereas if you have even a thin matrix,
[849.00s -> 851.00s]  B equals one, that's really bad
[851.00s -> 855.00s]  because you're spending a lot of time reading from memory
[855.00s -> 858.00s]  and not doing that much compute.
[860.00s -> 863.00s]  Okay, so now let's talk about
[863.00s -> 866.00s]  the arithmetic intensity of inference.
[868.00s -> 872.00s]  Okay, so let's just kind of get more into
[872.00s -> 874.00s]  the weeds of what inference looks like.
[874.00s -> 878.00s]  So the naive thing you can imagine doing,
[878.00s -> 882.00s]  all these nice pictures are taken from this book,
[882.00s -> 884.00s]  is that you have a transformer,
[884.00s -> 886.00s]  you give the prompt in,
[886.00s -> 892.00s]  it gives you logits over the vocabulary of the next token,
[892.00s -> 894.00s]  and you just sample from that.
[894.00s -> 895.00s]  And then once you get that,
[895.00s -> 898.00s]  you attach it to the prompt
[898.00s -> 900.00s]  and then you feed it through the transformer
[900.00s -> 903.00s]  and you look at the logit, sample again,
[903.00s -> 905.00s]  and you repeat.
[905.00s -> 909.00s]  So that's the sort of most naive thing to do.
[911.00s -> 915.00s]  And the complexity here is pretty bad
[915.00s -> 918.00s]  because each token you generate
[918.00s -> 923.00s]  is like N squared or T squared computation
[923.00s -> 925.00s]  through the transformer.
[927.00s -> 929.00s]  Okay, so that's no good.
[929.00s -> 932.00s]  But if you look at this closely,
[932.00s -> 936.00s]  you'll notice that you're doing a lot of redundant work.
[936.00s -> 943.00s]  All of these, the work in basically encoding the prefix
[943.00s -> 946.00s]  basically stays the same.
[946.00s -> 951.00s]  So this is for a bidirectional transformer
[951.00s -> 952.00s]  would be different,
[952.00s -> 955.00s]  but at least for an autoregressive causal transformer,
[955.00s -> 958.00s]  it is the case that you should be able
[958.00s -> 960.00s]  to share a lot between prefixes.
[960.00s -> 963.00s]  And so the solution is you cache.
[963.00s -> 966.00s]  And you cache in the IBM HPM
[966.00s -> 969.00s]  because that's where you have enough space to store stuff.
[969.00s -> 971.00s]  So this is what it looks like
[971.00s -> 975.00s]  if you have a KV cache schematically.
[975.00s -> 978.00s]  So you take your prompt.
[978.00s -> 982.00s]  The prefilled step is you feed it through the transformer
[982.00s -> 985.00s]  and you compute this KV cache.
[985.00s -> 989.00s]  And then you generate the logits over the next token.
[989.00s -> 994.00s]  And then you put that into,
[994.00s -> 998.00s]  you know, take that generated token and the cache
[998.00s -> 1000.00s]  and then you can feed it through the transformer.
[1000.00s -> 1001.00s]  But you've already computed these,
[1001.00s -> 1002.00s]  so you don't have to do that again.
[1002.00s -> 1007.00s]  You just need to compute this new KV vector for this token.
[1007.00s -> 1010.00s]  And now that allows you to more quickly
[1010.00s -> 1014.00s]  generate the next token and so on.
[1014.00s -> 1016.00s]  So basically you're filling up this KV cache
[1016.00s -> 1019.00s]  which corresponds to the tokens
[1019.00s -> 1021.00s]  that you've either prefilled with
[1021.00s -> 1023.00s]  or that you've generated so far.
[1023.00s -> 1026.00s]  Okay, so instead of T squared per token,
[1026.00s -> 1030.00s]  it's going to be more like T.
[1030.00s -> 1032.00s]  Okay, so concretely the KV cache is
[1032.00s -> 1034.00s]  for every sequence in your batch,
[1034.00s -> 1036.00s]  for every token in your sequence,
[1036.00s -> 1038.00s]  for every layer of the transformer,
[1038.00s -> 1039.00s]  for every head,
[1039.00s -> 1042.00s]  you're going to store an H-dimensional vector.
[1042.00s -> 1045.00s]  So you might think that this is going to take a lot of memory
[1045.00s -> 1048.00s]  and you wouldn't be wrong.
[1048.00s -> 1050.00s]  Okay, so there's two stages of inference.
[1050.00s -> 1052.00s]  So prefill is you're giving your prompt,
[1052.00s -> 1053.00s]  encoded in a vector.
[1053.00s -> 1055.00s]  So this is just like what you do in training.
[1055.00s -> 1058.00s]  It's paralyzable, it's fast, you compute limited,
[1058.00s -> 1059.00s]  life is good.
[1059.00s -> 1061.00s]  And then you're doing generation,
[1061.00s -> 1063.00s]  which is you're generating response tokens
[1063.00s -> 1065.00s]  one by one, sequentially.
[1065.00s -> 1067.00s]  And this is the part that's going to give us
[1067.00s -> 1071.00s]  a lot of trouble in terms of efficiency.
[1071.00s -> 1074.00s]  So now let's compute the flops and memory IO
[1074.00s -> 1077.00s]  for both, for the transformer.
[1077.00s -> 1079.00s]  So we're going to break it down into MLP layers
[1079.00s -> 1082.00s]  and the attention layers.
[1082.00s -> 1085.00s]  And just for notation-wise,
[1085.00s -> 1088.00s]  we're going to do this computation with S being
[1088.00s -> 1090.00s]  the number of tokens we're conditioning on,
[1090.00s -> 1091.00s]  think about the length of the prompt,
[1091.00s -> 1094.00s]  and T is the number of tokens we're generating
[1094.00s -> 1098.00s]  or are querying using.
[1098.00s -> 1101.00s]  And in prefill, T is going to be S
[1101.00s -> 1105.00s]  because we're not generating T tokens,
[1105.00s -> 1109.00s]  but we're sort of querying using each of these tokens
[1109.00s -> 1113.00s]  and a generation where T is just one.
[1113.00s -> 1117.00s]  Okay, so hopefully the matrix multiplication
[1117.00s -> 1118.00s]  is so fresh in your head
[1118.00s -> 1121.00s]  because this is going to be essentially that,
[1121.00s -> 1122.00s]  but a little bit more complicated
[1122.00s -> 1124.00s]  because it's a transformer.
[1124.00s -> 1127.00s]  So we're going to count the flops and bytes generated.
[1127.00s -> 1130.00s]  So first we're going to take X,
[1130.00s -> 1137.00s]  which is a T by D matrix.
[1137.00s -> 1141.00s]  I think maybe these T's should be S's, but anyway.
[1141.00s -> 1148.00s]  So that involves doing a bunch of transfers.
[1148.00s -> 1150.00s]  Basically the size of that matrix times two
[1150.00s -> 1153.00s]  because B of 16.
[1153.00s -> 1156.00s]  Then there's the three-way matrices,
[1156.00s -> 1159.00s]  the up projection, the gate, and the down projection.
[1159.00s -> 1164.00s]  They're all the same size up to transposition.
[1164.00s -> 1167.00s]  So you need to transfer those.
[1167.00s -> 1173.00s]  Then you do the up projection.
[1173.00s -> 1175.00s]  That's some number of flops.
[1175.00s -> 1178.00s]  So B times D times C times F.
[1178.00s -> 1181.00s]  So whenever you multiply two tensors,
[1181.00s -> 1184.00s]  basically the contracting dimension only gets counted once,
[1184.00s -> 1188.00s]  because the other dimensions you just kind of gather together.
[1188.00s -> 1190.00s]  You need to write it out.
[1190.00s -> 1194.00s]  You also have the gate, which is the same thing.
[1194.00s -> 1195.00s]  You write it out.
[1195.00s -> 1200.00s]  You compute your nonlinearity.
[1200.00s -> 1203.00s]  You multiply some stuff and you down project.
[1203.00s -> 1207.00s]  And that's a B times T times D times F,
[1207.00s -> 1210.00s]  which is basically the same number of flops.
[1210.00s -> 1214.00s]  And then you write out the result.
[1214.00s -> 1218.00s]  Okay, so if you look at the counting,
[1218.00s -> 1221.00s]  I guess maybe I'll just, you know,
[1221.00s -> 1224.00s]  you can check the results.
[1224.00s -> 1225.00s]  Actually you don't need to check it
[1225.00s -> 1230.00s]  because this is sim pi and it's guaranteed to be correct.
[1230.00s -> 1234.00s]  So, but again, we're gonna assume that B times D
[1234.00s -> 1238.00s]  is much smaller than D and F.
[1238.00s -> 1241.00s]  And we get that the intensity is B times T.
[1241.00s -> 1245.00s]  Okay, so this is analogous to the matrix multiplication case
[1245.00s -> 1249.00s]  where the arithmetic intensity, which we want to be high,
[1249.00s -> 1251.00s]  depends on how large your batch is
[1251.00s -> 1255.00s]  and how many tokens you're essentially generating.
[1255.00s -> 1258.00s]  Okay, so now if you look at the two stages, pre-fill,
[1258.00s -> 1260.00s]  life is good, remember,
[1260.00s -> 1262.00s]  because we can just make BT large enough.
[1262.00s -> 1264.00s]  You use a batch size.
[1264.00s -> 1267.00s]  Even a batch size of one actually is maybe okay
[1267.00s -> 1270.00s]  if you have long enough sequence.
[1270.00s -> 1272.00s]  So that's not a problem.
[1272.00s -> 1275.00s]  Now generation, this is where it becomes
[1275.00s -> 1278.00s]  a little bit harder because you're generating
[1278.00s -> 1281.00s]  one token at a time, so T is one.
[1281.00s -> 1285.00s]  Right, so if T is one, that means for BT to be large,
[1285.00s -> 1287.00s]  you need B to be large.
[1287.00s -> 1291.00s]  And B is essentially the number of concurrent requests.
[1291.00s -> 1293.00s]  Okay, so this is kind of interesting
[1293.00s -> 1296.00s]  because your sort of efficiency depends on
[1296.00s -> 1298.00s]  having large batch sizes.
[1298.00s -> 1300.00s]  Because, I mean, intuitively it makes sense.
[1300.00s -> 1303.00s]  If you can take a lot of requests, batch them together,
[1303.00s -> 1307.00s]  then you can get better efficiency, at least throughput.
[1310.00s -> 1313.00s]  But this also depends on what B is
[1313.00s -> 1317.00s]  because if you're only getting a few requests at a time,
[1317.00s -> 1319.00s]  then you're not going to be able
[1319.00s -> 1322.00s]  to use your hardware very efficiently.
[1322.00s -> 1325.00s]  And this talk speaks to the sort of
[1325.00s -> 1327.00s]  the very dynamic aspect of inference,
[1327.00s -> 1330.00s]  which we'll come back to later in the lecture.
[1330.00s -> 1333.00s]  Okay, so now what about attention?
[1333.00s -> 1336.00s]  Turns out attention is even worse
[1336.00s -> 1338.00s]  for reasons I'll try to get into.
[1338.00s -> 1343.00s]  So let's do the counting, flops, bytes transferred.
[1343.00s -> 1351.00s]  Okay, so I'm going to read the QKB matrices from HBM.
[1352.00s -> 1356.00s]  I'm going to compute the attention,
[1356.00s -> 1360.00s]  which is a matrix, which is Q times K.
[1360.00s -> 1364.00s]  And the number of flops is B times S times T times D.
[1364.00s -> 1367.00s]  So remember, S and T are the same during a pre-fill,
[1367.00s -> 1372.00s]  so that's your sequence length squared times B times D.
[1372.00s -> 1377.00s]  And then I'm sort of only looking at the matrix multiplications
[1377.00s -> 1381.00s]  because the flops from the other steps don't really matter.
[1381.00s -> 1384.00s]  And then you project out to, oh, sorry,
[1384.00s -> 1388.00s]  you take a combination of this and this.
[1388.00s -> 1391.00s]  Actually, this is mathematically incorrect
[1391.00s -> 1393.00s]  because there's some softmaxes there,
[1393.00s -> 1397.00s]  but the essence of the matballs are the same.
[1397.00s -> 1399.00s]  So that's the same number of flops,
[1399.00s -> 1403.00s]  and then you write to HBM.
[1403.00s -> 1406.00s]  Okay, so, and here I'm assuming
[1406.00s -> 1408.00s]  there would be more bytes transferred
[1408.00s -> 1410.00s]  if you didn't use flash attention.
[1410.00s -> 1412.00s]  Flash attention means that you don't have to
[1412.00s -> 1416.00s]  keep on writing back to HBM intermediate steps.
[1416.00s -> 1421.00s]  But the order is actually not really affected.
[1421.00s -> 1424.00s]  So qualitatively, it doesn't really matter
[1424.00s -> 1426.00s]  whether you use flash attention or not,
[1426.00s -> 1429.00s]  but the math here depends on the constants matter.
[1429.00s -> 1433.00s]  But let's look at the flops and the bytes transferred.
[1433.00s -> 1438.00s]  And if you divide and simplify,
[1438.00s -> 1441.00s]  you get this rather nice expression.
[1441.00s -> 1442.00s]  I mean, nice in that it's simple,
[1442.00s -> 1445.00s]  not nice in that it's good efficiency,
[1445.00s -> 1450.00s]  which is S times T divided by S plus T.
[1450.00s -> 1454.00s]  Okay, so let's try to interpret this a bit.
[1454.00s -> 1458.00s]  So in pre-fill, T equals S.
[1458.00s -> 1462.00s]  So that means your pre-fill intensity is order S.
[1462.00s -> 1464.00s]  So that's good, right?
[1464.00s -> 1468.00s]  Because as long as you have long enough sequences,
[1468.00s -> 1472.00s]  then you're good to go.
[1472.00s -> 1476.00s]  And generally the sequences can assume are kind of long enough.
[1476.00s -> 1479.00s]  During generation, however,
[1479.00s -> 1484.00s]  you'll see that the intensity is essentially one.
[1484.00s -> 1488.00s]  S over S plus one, but that's basically one.
[1488.00s -> 1492.00s]  And remember, one is really bad.
[1492.00s -> 1496.00s]  Okay, so, but notice like what,
[1496.00s -> 1499.00s]  there's no dependence on B at all.
[1499.00s -> 1503.00s]  So unlike in MLPs, remember in MLPs,
[1503.00s -> 1509.00s]  the generation, the pre-fill was BT, which is great,
[1509.00s -> 1512.00s]  and the error intensity was B,
[1512.00s -> 1514.00s]  which was not great because it depends on
[1514.00s -> 1517.00s]  the whims of your users and workloads,
[1517.00s -> 1520.00s]  but still could be larger than one.
[1520.00s -> 1525.00s]  Whereas for attention, it's actually just always less than one.
[1525.00s -> 1527.00s]  No matter how many, how long your sequences are,
[1527.00s -> 1531.00s]  how many users there are, it's always one.
[1531.00s -> 1534.00s]  So why is this, intuitively,
[1534.00s -> 1539.00s]  that there's no dependence on B, the batch dimension?
[1539.00s -> 1543.00s]  So the reason is that in the MLP layers,
[1543.00s -> 1548.00s]  intuitively, every sequence hits the same MLP weights.
[1548.00s -> 1551.00s]  So whereas in an attention layer,
[1551.00s -> 1556.00s]  each sequence has its own KV cache, right?
[1556.00s -> 1559.00s]  Because the KV cache is sequence specific,
[1559.00s -> 1565.00s]  which means that you can't really use,
[1565.00s -> 1567.00s]  in MLP case, you can kind of read all the weights
[1567.00s -> 1570.00s]  and then you process a batch intuitively.
[1570.00s -> 1572.00s]  Whereas in an attention case,
[1572.00s -> 1574.00s]  every sequence kind of requires additional memory.
[1574.00s -> 1579.00s]  You don't get any kind of savings if you batch them up.
[1579.00s -> 1583.00s]  Mathematically, I guess you can look at it through here,
[1583.00s -> 1586.00s]  where the number of flops, there's a B here,
[1586.00s -> 1589.00s]  which is expected, but the number of bytes transferred
[1589.00s -> 1594.00s]  is B times, there's a scaling in B,
[1594.00s -> 1599.00s]  so when you divide, that B cancels.
[1599.00s -> 1604.00s]  Whereas over here, there is a B here,
[1604.00s -> 1607.00s]  but we're assuming that the F dominates,
[1607.00s -> 1611.00s]  so when you divide, basically there's no B
[1611.00s -> 1613.00s]  essentially left in the denominator.
[1613.00s -> 1615.00s]  So you can look at it mathematically,
[1615.00s -> 1621.00s]  or you can just kind of reason about it intuitively as,
[1621.00s -> 1625.00s]  for the attention, the KV cache is sort of
[1625.00s -> 1630.00s]  every sequence's own unique snowflake.
[1630.00s -> 1634.00s]  Okay, so the summary is, pre-fill is compute-limited,
[1634.00s -> 1637.00s]  where generation is memory-limited.
[1637.00s -> 1640.00s]  The MLP arithmetic intensity is B,
[1640.00s -> 1642.00s]  which to make good enough,
[1642.00s -> 1645.00s]  you need a bunch of concurrent requests,
[1645.00s -> 1647.00s]  but attention intensity is one,
[1647.00s -> 1650.00s]  and it's also possible to improve that.
[1650.00s -> 1654.00s]  Okay, I'll pause a bit for any questions.
[1656.00s -> 1659.00s]  Okay, so let's move on.
[1661.00s -> 1665.00s]  So now we know that inference due thanks to generation
[1665.00s -> 1669.00s]  is memory-limited, let's try to study
[1669.00s -> 1674.00s]  the throughput and latency, at least in theory.
[1675.00s -> 1683.00s]  So let's focus on, let's see, actually, okay.
[1683.00s -> 1686.00s]  So we're gonna make some assumptions,
[1686.00s -> 1689.00s]  so all of this is sort of napkin math
[1689.00s -> 1690.00s]  is a little bit stylized,
[1690.00s -> 1693.00s]  but it gives you roughly the right kind of scaling
[1693.00s -> 1695.00s]  and the right way to think about things.
[1695.00s -> 1698.00s]  So we're gonna assume that communication and compute
[1698.00s -> 1703.00s]  can be perfectly overlapped, which is obviously false,
[1703.00s -> 1710.00s]  but it's good enough for making these qualitative estimates.
[1710.00s -> 1714.00s]  So what we're gonna do is we're gonna instantiate
[1714.00s -> 1719.00s]  the latency and throughput for ALAMA2-13b on H100.
[1719.00s -> 1724.00s]  Okay, so for a 13b, here are the values.
[1724.00s -> 1728.00s]  So let's just put the sequence length to be 1,000,
[1728.00s -> 1731.00s]  hidden dimension to be, model dimension to be 5,000,
[1732.00s -> 1735.00s]  four times, I don't know if that's, that's not four times,
[1735.00s -> 1738.00s]  but anyway, f is some multiple of that,
[1738.00s -> 1743.00s]  number of heads, number of key value,
[1743.00s -> 1746.00s]  I guess query heads, number of key value heads,
[1746.00s -> 1748.00s]  which for ALAMA2 is the same,
[1748.00s -> 1751.00s]  we'll get to that point later, and so on.
[1751.00s -> 1755.00s]  And for the memory bandwidth of H100, that's the number.
[1755.00s -> 1758.00s]  Okay, so that's the config,
[1758.00s -> 1763.00s]  and we're gonna compute the memory, latency, and throughput.
[1763.00s -> 1767.00s]  Okay, so, oh, okay.
[1770.00s -> 1774.00s]  So first, let's just quickly get the number of parameters.
[1774.00s -> 1776.00s]  You guys did this in assignment one,
[1776.00s -> 1780.00s]  so I won't belabor this, but it's some expression
[1780.00s -> 1785.00s]  that depends on all the different variables,
[1786.00s -> 1791.00s]  and to store the parameters, we're gonna use BF16,
[1792.00s -> 1795.00s]  because inference is generally gonna be 16 bit,
[1795.00s -> 1798.00s]  not 32 bit, so we're gonna multiply by two,
[1798.00s -> 1803.00s]  so that's the memory that the parameters take, okay?
[1803.00s -> 1806.00s]  We don't need gradients, we don't need optimizer states
[1806.00s -> 1807.00s]  because we're not training,
[1807.00s -> 1811.00s]  but we do have to store the KV cache,
[1811.00s -> 1814.00s]  which are some of the activations,
[1814.00s -> 1817.00s]  not all the activations, but some of them,
[1817.00s -> 1820.00s]  for every sequence of length S,
[1820.00s -> 1823.00s]  and how much we have to store per sequence
[1823.00s -> 1825.00s]  is basically the sequence lengths
[1825.00s -> 1829.00s]  times the number of key value heads
[1832.00s -> 1833.00s]  times the dimension of that head
[1833.00s -> 1834.00s]  times the number of layers
[1834.00s -> 1839.00s]  times basically two for basically both the key and the value
[1839.00s -> 1842.00s]  and two for BF16, okay?
[1842.00s -> 1846.00s]  So that's how much the cache size takes,
[1846.00s -> 1849.00s]  and so the total memory is batch size
[1849.00s -> 1852.00s]  times the cache per sequence
[1852.00s -> 1855.00s]  plus the memory, plus the parameter size.
[1855.00s -> 1861.00s]  So now latency is going to be determined by memory I.
[1861.00s -> 1862.00s]  Remember, it's memory limited,
[1862.00s -> 1865.00s]  so we're just gonna compute
[1865.00s -> 1869.00s]  how much memory needs to be transferred
[1869.00s -> 1871.00s]  into the GPU to do this computation,
[1871.00s -> 1875.00s]  and it's simply memory over the memory bandwidth.
[1875.00s -> 1879.00s]  And throughput is essentially the inverse of latency,
[1879.00s -> 1880.00s]  but scaled up by B
[1880.00s -> 1886.00s]  because we're looking at generating B tokens in parallel.
[1886.00s -> 1892.00s]  Okay, so now if we substitute our llama2 config,
[1892.00s -> 1894.00s]  we'll see that the number of parameters
[1894.00s -> 1897.00s]  checks out its 13 billion.
[1897.00s -> 1902.00s]  Roughly the memory, latency, and throughput
[1902.00s -> 1904.00s]  have these expressions.
[1904.00s -> 1906.00s]  So memory grows obviously.
[1906.00s -> 1908.00s]  This is the parameter size.
[1908.00s -> 1913.00s]  This is the key value cache size times B.
[1913.00s -> 1915.00s]  Latency also goes up as a function of B.
[1915.00s -> 1919.00s]  Throughput increases,
[1919.00s -> 1921.00s]  but you'll see that it increases up to a point.
[1921.00s -> 1924.00s]  The B shows up in both the numerator and the denominator.
[1924.00s -> 1927.00s]  So there's limits to how much you can stretch throughput,
[1927.00s -> 1929.00s]  even if you could fit everything in memory.
[1929.00s -> 1933.00s]  Okay, so those are the expressions
[1933.00s -> 1936.00s]  for latency, throughput, and memory
[1936.00s -> 1939.00s]  for this particular model.
[1939.00s -> 1942.00s]  So now let's instantiate with different batch sizes.
[1942.00s -> 1946.00s]  So if B equals one,
[1946.00s -> 1950.00s]  then the latency is about eight milliseconds.
[1950.00s -> 1953.00s]  So every eight milliseconds you generate a token.
[1953.00s -> 1958.00s]  And the throughput is 124 tokens per second.
[1958.00s -> 1961.00s]  Okay, so that's 13 B on H100
[1961.00s -> 1964.00s]  if you're using batch size of one.
[1964.00s -> 1969.00s]  So now what happens if you use batch size of 16?
[1969.00s -> 1975.00s]  So you'll see that the memory usage increases
[1975.00s -> 1977.00s]  because you need to store the KB cache
[1977.00s -> 1981.00s]  for all 64 sequences now.
[1981.00s -> 1984.00s]  The latency goes up
[1984.00s -> 1986.00s]  because you kind of have to,
[1986.00s -> 1988.00s]  instead of just processing one,
[1988.00s -> 1991.00s]  you have to kind of wait for everything to finish.
[1991.00s -> 1995.00s]  But the throughput also goes up actually quite a lot.
[1995.00s -> 1998.00s]  Okay, so you're seeing kind of this immediate tradeoff
[1998.00s -> 2001.00s]  between latency and throughput.
[2001.00s -> 2004.00s]  If you're on low latency, you just use one.
[2005.00s -> 2006.00s]  B equals one.
[2006.00s -> 2007.00s]  But if you want high throughput,
[2007.00s -> 2010.00s]  you want larger B in general.
[2010.00s -> 2013.00s]  What happens if you use batch size of even larger,
[2013.00s -> 2016.00s]  so 256?
[2016.00s -> 2019.00s]  You'll see that low latency goes up,
[2019.00s -> 2020.00s]  throughput goes up,
[2020.00s -> 2023.00s]  but you see that throughput isn't going up that much
[2023.00s -> 2026.00s]  because you get diminishing returns after a while.
[2026.00s -> 2028.00s]  But the most kind of,
[2028.00s -> 2030.00s]  you can actually do this on H100
[2030.00s -> 2033.00s]  because if you look at the memory,
[2033.00s -> 2036.00s]  it's 240 gigs.
[2036.00s -> 2039.00s]  So it doesn't even fit.
[2039.00s -> 2040.00s]  Okay, so batch size,
[2040.00s -> 2043.00s]  you can only increase to a certain point
[2043.00s -> 2045.00s]  because of memory.
[2045.00s -> 2046.00s]  Okay, so just to recap,
[2046.00s -> 2048.00s]  there's a tradeoff between latency and throughput.
[2048.00s -> 2051.00s]  Smaller batch sizes, you have better latency.
[2051.00s -> 2055.00s]  Larger batch sizes, you have better throughput.
[2057.00s -> 2061.00s]  And finally, last week we talked about parallelism for training.
[2061.00s -> 2066.00s]  And it was kind of complicated and annoying.
[2066.00s -> 2069.00s]  At least one type of parallelism for inference
[2069.00s -> 2073.00s]  is really, really nice and simple.
[2073.00s -> 2076.00s]  You just launch M copies of a model.
[2076.00s -> 2080.00s]  No communication because you don't need to update the models.
[2080.00s -> 2081.00s]  The latency is the same,
[2081.00s -> 2084.00s]  and the throughput increases by M.
[2084.00s -> 2087.00s]  So that's pretty good.
[2087.00s -> 2089.00s]  Always remember that.
[2089.00s -> 2091.00s]  Don't forget the easy things.
[2091.00s -> 2095.00s]  Now there are cases where if you have a large enough model
[2095.00s -> 2099.00s]  then maybe it doesn't even fit on a single GPU
[2099.00s -> 2102.00s]  and you need to shard the model.
[2102.00s -> 2103.00s]  And in this case,
[2103.00s -> 2106.00s]  you also want to start sharding the KV cache
[2106.00s -> 2110.00s]  in some cases to get better efficiency.
[2110.00s -> 2115.00s]  So for more details, check out this book chapter.
[2117.00s -> 2120.00s]  Okay, so the time to first token,
[2120.00s -> 2123.00s]  which is a metric I mentioned earlier,
[2123.00s -> 2127.00s]  is essentially a function of the prefill.
[2127.00s -> 2130.00s]  It's basically how long does it take to encode the prompt.
[2130.00s -> 2133.00s]  And usually this is compute-limited
[2133.00s -> 2137.00s]  so you're basically going as fast as you can.
[2137.00s -> 2139.00s]  And there's not much you can do about it
[2139.00s -> 2142.00s]  given a fixed architecture.
[2143.00s -> 2146.00s]  And, well, okay, so sorry.
[2146.00s -> 2153.00s]  You can improve it if you reduce the batch size still.
[2153.00s -> 2156.00s]  But if you want to improve the throughput,
[2156.00s -> 2162.00s]  you have to increase the batch size.
[2162.00s -> 2164.00s]  Okay, so any questions about that?
[2164.00s -> 2169.00s]  So this was on computing the throughput and latency.
[2169.00s -> 2172.00s]  And because of the memory-limited argument
[2172.00s -> 2176.00s]  that I gave in the previous part,
[2176.00s -> 2179.00s]  I just focus on memory
[2179.00s -> 2181.00s]  and compute how many bytes need to be sent,
[2181.00s -> 2186.00s]  and that gives me a rough bound on the latency.
[2186.00s -> 2188.00s]  In practice, there are some regimes
[2188.00s -> 2189.00s]  where compute does matter,
[2189.00s -> 2191.00s]  but I'm sort of ignoring that,
[2191.00s -> 2195.00s]  just to keep things simple.
[2195.00s -> 2197.00s]  Okay, question?
[2197.00s -> 2202.00s]  Yes, this is assuming a single GPU.
[2217.00s -> 2218.00s]  Yeah, so the question is,
[2218.00s -> 2219.00s]  if you have multiple users
[2219.00s -> 2220.00s]  and you're batching them together,
[2220.00s -> 2221.00s]  they might arrive at different times.
[2221.00s -> 2223.00s]  They're going to finish at different times.
[2223.00s -> 2224.00s]  So we're going to get to that.
[2224.00s -> 2227.00s]  That's going to be a special issue
[2227.00s -> 2230.00s]  that we're going to have to deal with.
[2230.00s -> 2232.00s]  Any other questions?
[2237.00s -> 2240.00s]  Okay, so now we have a good handle
[2240.00s -> 2243.00s]  on what the inference workload looks like.
[2243.00s -> 2245.00s]  We looked at the arithmetic intensity.
[2245.00s -> 2249.00s]  We looked at the transformer inference
[2249.00s -> 2251.00s]  with respect to arithmetic intensity.
[2251.00s -> 2253.00s]  We saw that it was memory-limited
[2253.00s -> 2256.00s]  thanks to the tension where the KV cache
[2256.00s -> 2258.00s]  has to be special for every sequence,
[2258.00s -> 2260.00s]  and then using that, we can compute throughput
[2260.00s -> 2263.00s]  and latency, which are the main inference metrics
[2263.00s -> 2265.00s]  that we care about.
[2265.00s -> 2268.00s]  Now, how do we make things better?
[2268.00s -> 2272.00s]  Okay, so there are some things that you can do
[2272.00s -> 2274.00s]  that are lossless.
[2274.00s -> 2275.00s]  You can write better kernels.
[2275.00s -> 2277.00s]  You can improve your systems,
[2277.00s -> 2282.00s]  but I would say that there's a lot you can do
[2282.00s -> 2286.00s]  if you're willing to take shortcuts.
[2286.00s -> 2288.00s]  And these are kind of really interesting
[2288.00s -> 2292.00s]  because technically this lecture is on inference,
[2292.00s -> 2295.00s]  but secretly it's on model architectures
[2295.00s -> 2297.00s]  because what you'll see is that
[2297.00s -> 2299.00s]  a lot of the changes in model architecture
[2299.00s -> 2302.00s]  are going to have direct impact on inference
[2302.00s -> 2307.00s]  and were actually inspired by needing to do inference quickly.
[2307.00s -> 2313.00s]  Okay, so the big bottleneck here is the KV cache, right?
[2313.00s -> 2315.00s]  Because remember, memory-limited,
[2315.00s -> 2319.00s]  which means that the less memory stuff takes,
[2319.00s -> 2321.00s]  then the faster you go.
[2321.00s -> 2325.00s]  Not just because of flops, even though that's department,
[2325.00s -> 2327.00s]  but mostly due to memory
[2327.00s -> 2330.00s]  because it's mostly about memory transfers.
[2330.00s -> 2332.00s]  If that's one thing you take away from this lecture,
[2332.00s -> 2337.00s]  it's all about the memory for speed.
[2337.00s -> 2341.00s]  Okay, so the problem is that
[2341.00s -> 2344.00s]  if you just start walking away at the KV cache,
[2344.00s -> 2346.00s]  you might lose accuracy.
[2346.00s -> 2350.00s]  So how can you make sure you don't lose too much accuracy
[2350.00s -> 2356.00s]  but still maintain your KV cache small?
[2356.00s -> 2361.00s]  So there's a bunch of ideas I'm going to go through
[2361.00s -> 2364.00s]  that all essentially try to change the architecture
[2364.00s -> 2365.00s]  to reduce your KV cache.
[2365.00s -> 2369.00s]  Some of these ideas I think you've seen,
[2369.00s -> 2371.00s]  but I'll go through them kind of in this
[2371.00s -> 2373.00s]  sort of more systematic way.
[2373.00s -> 2377.00s]  So there's this idea called grouped query attention.
[2377.00s -> 2380.00s]  So multi-headed attention, which is the vanilla transformer,
[2380.00s -> 2384.00s]  keeps around basically a number of heads
[2384.00s -> 2386.00s]  and for each of that number
[2386.00s -> 2391.00s]  you have same number of keys, values, and queries.
[2391.00s -> 2394.00s]  There was one time a multi-query attention
[2394.00s -> 2399.00s]  in which you only have one key and one value,
[2399.00s -> 2401.00s]  basically one key value head.
[2401.00s -> 2404.00s]  It turned out that that was not very expressive,
[2404.00s -> 2406.00s]  so there was this sort of intermediate point
[2406.00s -> 2411.00s]  where you have a reduced number of keys and values
[2411.00s -> 2414.00s]  and then you have more queries.
[2414.00s -> 2415.00s]  So why are we doing this?
[2415.00s -> 2418.00s]  Well, remember we want to reduce the KV cache size,
[2418.00s -> 2421.00s]  so the fewer keys and values there are, the better.
[2421.00s -> 2423.00s]  So the batch size and the sequence length
[2423.00s -> 2425.00s]  doesn't get changed,
[2425.00s -> 2430.00s]  and the dimensionality of these vectors don't change,
[2430.00s -> 2432.00s]  but it's the number of key value heads
[2432.00s -> 2435.00s]  that we're reducing.
[2435.00s -> 2436.00s]  Okay?
[2436.00s -> 2438.00s]  So that's basically the idea,
[2438.00s -> 2443.00s]  and this paper shows that you do get latency
[2443.00s -> 2447.00s]  and throughput improvements, so times per sample,
[2447.00s -> 2451.00s]  and as you increase the number of groups,
[2451.00s -> 2455.00s]  then up to eight or so,
[2455.00s -> 2458.00s]  basically there's a negligible,
[2458.00s -> 2463.00s]  it's really fast compared to the full attention,
[2463.00s -> 2465.00s]  and as you increase the number of groups,
[2465.00s -> 2468.00s]  obviously you kind of end up at the original.
[2468.00s -> 2472.00s]  So that's latency and throughput improvements,
[2472.00s -> 2477.00s]  and just to actually do this kind of more rigorously,
[2477.00s -> 2481.00s]  so we have our llama to 13B model,
[2481.00s -> 2486.00s]  and if we compute the statistics,
[2486.00s -> 2490.00s]  this is using a batch size of 64.
[2490.00s -> 2494.00s]  Remember, this is what we got.
[2494.00s -> 2498.00s]  I guess I should have printed out latency here.
[2498.00s -> 2505.00s]  And then, if you run it with GQA,
[2505.00s -> 2508.00s]  you see that the memory is reduced
[2508.00s -> 2511.00s]  and the throughput goes way up.
[2511.00s -> 2513.00s]  So this is actually great.
[2513.00s -> 2518.00s]  So this is what happens if I take the llama to 13B architecture
[2518.00s -> 2522.00s]  and I just reduce for every query head,
[2522.00s -> 2524.00s]  I'm going, sorry, for every key value head,
[2524.00s -> 2526.00s]  I have five query heads.
[2526.00s -> 2530.00s]  That's what a one to five ratio means.
[2530.00s -> 2533.00s]  So this also means we can use a larger batch size,
[2533.00s -> 2536.00s]  because remember, last time we tried to do 256,
[2536.00s -> 2540.00s]  it didn't even fit in H100's memory.
[2540.00s -> 2545.00s]  So now we can actually comfortably fit into the H100 memory,
[2545.00s -> 2547.00s]  and then we can further improve the throughput
[2547.00s -> 2549.00s]  by using a larger batch size.
[2549.00s -> 2552.00s]  So you can see kind of a lot of different effects here
[2552.00s -> 2555.00s]  by reducing the number of key value pairs,
[2555.00s -> 2558.00s]  the memory of the KV cache reduces.
[2558.00s -> 2562.00s]  That means the throughput and latency go up automatically
[2562.00s -> 2564.00s]  because fewer memory transfers,
[2564.00s -> 2566.00s]  and furthermore, the secondary effect,
[2566.00s -> 2569.00s]  I can increase the batch size within the GPU
[2569.00s -> 2575.00s]  and that further improves the throughput.
[2575.00s -> 2578.00s]  Okay, so that's wonderful.
[2578.00s -> 2581.00s]  We have to also make sure the accuracy doesn't drop.
[2581.00s -> 2584.00s]  So this is this original paper
[2584.00s -> 2590.00s]  that shows that this is full attention, this is GQA,
[2590.00s -> 2592.00s]  the time is much less,
[2592.00s -> 2596.00s]  but the accuracy is basically the same.
[2596.00s -> 2597.00s]  Okay?
[2597.00s -> 2601.00s]  Now, what actually happened, so I don't,
[2601.00s -> 2605.00s]  so llama2 did not use this ratio,
[2605.00s -> 2609.00s]  but llama3 actually picked up GQA
[2609.00s -> 2613.00s]  and probably motivated by the kind of inference cost.
[2613.00s -> 2615.00s]  Actually, llama2, I think the 70,
[2615.00s -> 2620.00s]  the large model did have GQA, but not the smaller ones.
[2620.00s -> 2623.00s]  Okay, so that's GQA.
[2623.00s -> 2628.00s]  There's another way to reduce the key value cache,
[2628.00s -> 2629.00s]  and this comes from DeepSeq,
[2629.00s -> 2632.00s]  so this is actually from the DeepSeq V2 paper,
[2632.00s -> 2635.00s]  and it's called multi-head latent engine,
[2635.00s -> 2638.00s]  which Tatsu lectured about previously,
[2638.00s -> 2641.00s]  but I'll try to talk about it in the context of inference
[2641.00s -> 2643.00s]  and its implications.
[2643.00s -> 2647.00s]  So the basic idea is, here's full attention,
[2647.00s -> 2652.00s]  and GQA says, I'm gonna use fewer keys and values.
[2652.00s -> 2656.00s]  MLA says, I'm not gonna change the number of key and values,
[2656.00s -> 2660.00s]  I'm going to project these into a lower dimensional space.
[2660.00s -> 2663.00s]  So it's another way of shrinking the KV size,
[2663.00s -> 2667.00s]  but just in a, I guess, in a different dimension.
[2667.00s -> 2673.00s]  So instead of using N times H dimensions for each,
[2673.00s -> 2677.00s]  for the KV cache of each token,
[2677.00s -> 2680.00s]  I'm gonna project out to C dimensions,
[2680.00s -> 2682.00s]  and this is what DeepSeq did.
[2682.00s -> 2684.00s]  It's actually quite an aggressive reduction
[2684.00s -> 2686.00s]  from 16,000 to 512.
[2686.00s -> 2689.00s]  Only wrinkle is that this is not compatible with Rope,
[2689.00s -> 2691.00s]  so they need to add a few more dimensions
[2691.00s -> 2693.00s]  to put Rope back in.
[2693.00s -> 2699.00s]  But overall, this is actually quite promising
[2699.00s -> 2702.00s]  from a KV reduction perspective.
[2702.00s -> 2703.00s]  I'm not gonna do the math,
[2703.00s -> 2706.00s]  but you can just trust me that you can see
[2706.00s -> 2709.00s]  kind of how the KV cache would be reduced a lot,
[2709.00s -> 2711.00s]  and you get the same kind of latency
[2711.00s -> 2713.00s]  and throughput advantages.
[2713.00s -> 2715.00s]  And in terms of accuracy,
[2715.00s -> 2720.00s]  they actually showed that compared to GQA,
[2720.00s -> 2723.00s]  the MH, sorry, the,
[2724.00s -> 2728.00s]  actually, maybe I'm showing the wrong thing here.
[2730.00s -> 2732.00s]  MH, okay.
[2732.00s -> 2737.00s]  I meant to show that the MLA actually improves,
[2737.00s -> 2739.00s]  but this table does not show that,
[2739.00s -> 2741.00s]  so I'll have to dig that up later.
[2741.00s -> 2746.00s]  But anyway, MLA does preserve the accuracy as well.
[2747.00s -> 2751.00s]  Okay, so there's another idea which says,
[2751.00s -> 2756.00s]  well, you know, GQA basically shares,
[2756.00s -> 2761.00s]  you can think about it as a sharing key value vectors,
[2761.00s -> 2766.00s]  right, within a token and within a sequence.
[2766.00s -> 2769.00s]  But we can also look at something
[2769.00s -> 2771.00s]  called cross-layer attention,
[2771.00s -> 2773.00s]  which, there's a paper on this,
[2773.00s -> 2776.00s]  I think many people have been kind of thinking about this
[2776.00s -> 2778.00s]  and doing this, so I don't know
[2778.00s -> 2780.00s]  if this is actually the first paper.
[2780.00s -> 2783.00s]  But basically, if you look at the Transformer IA diagram,
[2783.00s -> 2787.00s]  you have the key value projection of one layer,
[2787.00s -> 2789.00s]  and then you have the next layer,
[2789.00s -> 2794.00s]  and these key value vectors are separate, usually.
[2795.00s -> 2798.00s]  But the idea here with CLA is that
[2798.00s -> 2801.00s]  we're just gonna use the same key value projection
[2801.00s -> 2805.00s]  across layers, that's why it's called cross-layer attention.
[2809.00s -> 2811.00s]  So just as GQA shares across heads,
[2811.00s -> 2814.00s]  CLA shares across layers.
[2814.00s -> 2819.00s]  So here, they show that they empirically improve
[2819.00s -> 2822.00s]  the Pareto frontier of accuracy
[2822.00s -> 2825.00s]  and the KV cache size.
[2825.00s -> 2829.00s]  So KV cache size, which relates to throughput and latency,
[2829.00s -> 2831.00s]  you want it to be small,
[2831.00s -> 2835.00s]  and you want the perplexity also to be small,
[2835.00s -> 2840.00s]  so they're able to improve that.
[2843.00s -> 2847.00s]  Okay, so notice that, for example,
[2847.00s -> 2853.00s]  64 heads, the cache size gets reduced,
[2853.00s -> 2857.00s]  but the validation perplexity does go up a little bit.
[2857.00s -> 2860.00s]  But overall, there's kind of advantage
[2860.00s -> 2864.00s]  in making that trade-off.
[2866.00s -> 2870.00s]  Okay, so there's yet another way to do things.
[2870.00s -> 2874.00s]  So local attention, which has been explored
[2874.00s -> 2878.00s]  actually quite a bit, since even there's a longformer,
[2878.00s -> 2881.00s]  there's an OpenAI paper, and then Mistral,
[2881.00s -> 2885.00s]  and I think many others use this as well.
[2885.00s -> 2887.00s]  It's a very, I guess, a natural idea.
[2887.00s -> 2889.00s]  Instead of, if you look at a full attention diagram,
[2889.00s -> 2892.00s]  it's dense, N squared,
[2892.00s -> 2896.00s]  and that's where a lot of your complexity comes from.
[2896.00s -> 2899.00s]  And basically, the idea is you're going to
[2899.00s -> 2905.00s]  just attend to only the past K tokens,
[2905.00s -> 2908.00s]  which means that in a KV cache,
[2908.00s -> 2910.00s]  as you're generating the sequence,
[2910.00s -> 2911.00s]  you don't have to remember everything.
[2911.00s -> 2914.00s]  As soon as the token kind of falls outside
[2914.00s -> 2916.00s]  your window that you have attention,
[2916.00s -> 2919.00s]  you can just throw it away.
[2919.00s -> 2921.00s]  So local attention is very,
[2921.00s -> 2926.00s]  you could say that the KV cache size remains constant
[2926.00s -> 2929.00s]  as opposed to growing with the sequence length.
[2929.00s -> 2931.00s]  So this is really good, right?
[2931.00s -> 2934.00s]  Because it means that for even long sequences,
[2934.00s -> 2939.00s]  you can have quite a small cache.
[2939.00s -> 2942.00s]  Okay?
[2942.00s -> 2947.00s]  But the problem is that this still hurts accuracy,
[2947.00s -> 2949.00s]  because if you just think about it,
[2949.00s -> 2952.00s]  why are we doing attention instead of RNNs
[2952.00s -> 2955.00s]  is that we needed to have
[2955.00s -> 2958.00s]  long-range model run, long-range dependencies,
[2958.00s -> 2960.00s]  and this is, in some sense,
[2960.00s -> 2962.00s]  even the call to attention is a little bit
[2962.00s -> 2964.00s]  kind of overselling this.
[2964.00s -> 2967.00s]  This is only looking at the local context,
[2967.00s -> 2970.00s]  which is not very expressive.
[2970.00s -> 2974.00s]  So what you do here is you can interleave local attention
[2974.00s -> 2979.00s]  with full global attention, hybrid layers.
[2979.00s -> 2981.00s]  So for example, character AI used,
[2981.00s -> 2986.00s]  for every six layers, they had one global layer
[2986.00s -> 2989.00s]  and five local layers.
[2989.00s -> 2993.00s]  So it looks something in addition to cross-layer attention.
[2993.00s -> 2995.00s]  So it looks something like this,
[2995.00s -> 2998.00s]  where full attention, every layer,
[2998.00s -> 3002.00s]  you have to store the kvcache,
[3002.00s -> 3008.00s]  and for what they did is that for every six layers,
[3008.00s -> 3010.00s]  you have the full attention,
[3010.00s -> 3013.00s]  but in between, you have this local attention,
[3013.00s -> 3019.00s]  and on top of that, they have kvcache sharing locally,
[3019.00s -> 3023.00s]  both for the local attention and the global attention.
[3023.00s -> 3026.00s]  So this is like all the tricks kind of,
[3026.00s -> 3028.00s]  not all the tricks, but many of the tricks
[3028.00s -> 3032.00s]  kind of combine together.
[3032.00s -> 3035.00s]  So in summary, these are a few ways
[3035.00s -> 3037.00s]  to reduce the kvcache size,
[3037.00s -> 3040.00s]  because remember, inference is memory limited.
[3040.00s -> 3042.00s]  So you want to reduce the cache size,
[3042.00s -> 3044.00s]  but you don't want to hurt accuracy too much.
[3044.00s -> 3046.00s]  And there's many ways to do it.
[3046.00s -> 3050.00s]  You can lower the dimensionality of the kvcache.
[3050.00s -> 3052.00s]  You can have few kvcache vectors.
[3052.00s -> 3057.00s]  You can reduce the dimensionality of a kvvector.
[3057.00s -> 3061.00s]  You can share the kvcache across layers.
[3061.00s -> 3064.00s]  And also, you can use local attention
[3064.00s -> 3068.00s]  on some of the layers.
[3068.00s -> 3071.00s]  Okay, any questions about the set of tricks
[3071.00s -> 3075.00s]  for reducing the kvcache?
[3075.00s -> 3076.00s]  Yeah?
[3076.00s -> 3077.00s]  Yeah.
[3092.00s -> 3097.00s]  Yeah, so the question is, are the weights shared?
[3097.00s -> 3100.00s]  So the kvcache is shared, but the weights are shared.
[3100.00s -> 3105.00s]  So what happens is the weights for doing the projection
[3105.00s -> 3110.00s]  need to be shared, so there's some consistency.
[3110.00s -> 3115.00s]  Yeah, there's another question.
[3115.00s -> 3118.00s]  The context size is too large,
[3118.00s -> 3121.00s]  and then it increases the kvcache as well.
[3121.00s -> 3125.00s]  The context size, when you pump the context
[3125.00s -> 3128.00s]  that is given to the large-length model,
[3128.00s -> 3131.00s]  it increases the kvcache size.
[3131.00s -> 3135.00s]  So are you trying reducing those context sizes?
[3135.00s -> 3138.00s]  Yeah, so the question is, if you have really long context,
[3138.00s -> 3140.00s]  let's say your prompt is huge,
[3140.00s -> 3145.00s]  that's going to intrinsically take a lot of kvcache.
[3145.00s -> 3149.00s]  So all these tricks can try to reduce that.
[3149.00s -> 3152.00s]  You can do more aggressive things that,
[3152.00s -> 3155.00s]  like there's ideas like GIST tokens
[3155.00s -> 3157.00s]  or ways to summarize the prompt,
[3157.00s -> 3159.00s]  which we're not going to talk about in this class,
[3159.00s -> 3163.00s]  but there's ways that address the long prompt situation as well.
[3163.00s -> 3172.00s]  Okay, so now I'm going to talk about even more radical ways
[3172.00s -> 3177.00s]  of making inference go faster by changing the transformer.
[3177.00s -> 3182.00s]  So the kvcache, these are basically variants of the transformer.
[3182.00s -> 3190.00s]  But maybe you can actually go outside the transformer
[3190.00s -> 3194.00s]  and do better, because the transformer wasn't really designed
[3194.00s -> 3197.00s]  with heavy inference workloads in mind.
[3197.00s -> 3200.00s]  They were just trying to train a good model efficiently.
[3200.00s -> 3202.00s]  It was mostly about training efficiency.
[3202.00s -> 3206.00s]  And the autoregression, as we sort of pointed out,
[3206.00s -> 3209.00s]  is really causing this bottleneck here,
[3209.00s -> 3214.00s]  with autoregression plus the full attention.
[3214.00s -> 3217.00s]  So we're going to talk about two directions,
[3217.00s -> 3219.00s]  state-space models and diffusion models.
[3219.00s -> 3222.00s]  This is going to be fairly quick.
[3222.00s -> 3226.00s]  So the idea of state-space models
[3226.00s -> 3230.00s]  is actually drawing ideas from signal processing
[3230.00s -> 3231.00s]  and control theory.
[3231.00s -> 3234.00s]  Initially, the motivation was trying to model
[3234.00s -> 3239.00s]  long context sequences without suffering the n squared blowup.
[3239.00s -> 3241.00s]  So it wasn't necessary about inference speed,
[3241.00s -> 3243.00s]  but it turns out if you solve that problem,
[3243.00s -> 3245.00s]  you get faster inference, too.
[3245.00s -> 3248.00s]  So there's a kind of early paper on S4.
[3248.00s -> 3252.00s]  Which uses classical state-space models,
[3252.00s -> 3256.00s]  which are basically these kind of linear dynamical systems,
[3256.00s -> 3261.00s]  which have been used to kind of model long contexts
[3261.00s -> 3266.00s]  and sort of shoehorning them into kind of a modern neural setup.
[3266.00s -> 3275.00s]  This work is nice in that it has sort of this RNN kind of interpretation
[3275.00s -> 3277.00s]  due to the linearity structure
[3277.00s -> 3283.00s]  and also has a convolution interpretation as well.
[3283.00s -> 3287.00s]  So they published this paper
[3287.00s -> 3291.00s]  and showed that it worked really well
[3291.00s -> 3295.00s]  on these long context synthetic tasks.
[3295.00s -> 3298.00s]  But what they found is that,
[3298.00s -> 3301.00s]  I guess what was discovered is that
[3301.00s -> 3307.00s]  they don't really work well for language modeling.
[3307.00s -> 3309.00s]  And that's obviously kind of a disappointment
[3309.00s -> 3311.00s]  because a lot of the value of transformers
[3311.00s -> 3314.00s]  is being able to do language well.
[3314.00s -> 3316.00s]  So in a series of papers,
[3316.00s -> 3321.00s]  they sort of identified a set of kind of synthetic tasks
[3321.00s -> 3326.00s]  that captured the essence of why these models weren't working well.
[3326.00s -> 3328.00s]  And that's basically these associative recall tasks.
[3328.00s -> 3331.00s]  So here's a synthetic task where you're given basically
[3331.00s -> 3334.00s]  a sequence of key-value pairs.
[3334.00s -> 3337.00s]  And the goal is to predict,
[3337.00s -> 3340.00s]  basically look up the key and output the value.
[3340.00s -> 3344.00s]  So in some sense, it's kind of a logically a trivial task,
[3344.00s -> 3345.00s]  but it's long sequence
[3345.00s -> 3347.00s]  because I can have a lot of key-value pairs
[3347.00s -> 3351.00s]  and I'm going to have to look far back.
[3351.00s -> 3353.00s]  It can be arbitrary long-independence.
[3353.00s -> 3354.00s]  And you can see that local attention
[3354.00s -> 3355.00s]  is not going to work very well
[3355.00s -> 3358.00s]  because it's just going to remember the last few sequences.
[3358.00s -> 3362.00s]  And the problem with state-space models
[3362.00s -> 3363.00s]  is that they're sort of good
[3363.00s -> 3366.00s]  for these kind of signal processing tasks,
[3366.00s -> 3369.00s]  but really this is like you need to go isolate
[3369.00s -> 3372.00s]  a particular key-value pair and pull out the answer.
[3372.00s -> 3375.00s]  And for those type of tasks, it didn't really work.
[3375.00s -> 3377.00s]  So there's a bunch of work.
[3377.00s -> 3378.00s]  I'm not citing.
[3378.00s -> 3381.00s]  There's like Hyena, H3, and then Mamba,
[3382.00s -> 3387.00s]  which basically tweaked or changed the HSSM
[3387.00s -> 3390.00s]  to basically handle these associative recall tasks.
[3390.00s -> 3392.00s]  And eventually it worked better.
[3392.00s -> 3397.00s]  Up to kind of 1B scale was matching transformers.
[3397.00s -> 3402.00s]  And the idea of Mamba has been popular and scaled up
[3403.00s -> 3408.00s]  even to a 52B MOE by AI21 folks.
[3409.00s -> 3414.00s]  Notice that in this case, they still had to use a transformer.
[3414.00s -> 3419.00s]  So a transformer, but only every, I guess, eight layers,
[3419.00s -> 3420.00s]  they had a transformer.
[3420.00s -> 3422.00s]  The rest of them were Mamba layers.
[3422.00s -> 3428.00s]  And so that still led to a fairly big savings and speed up.
[3432.00s -> 3436.00s]  But more recently, there's this kind of revival
[3436.00s -> 3439.00s]  of this older idea called linear attention,
[3439.00s -> 3444.00s]  where instead of, let's see if I can make this bigger.
[3446.00s -> 3448.00s]  It's actually a kind of a very simple idea.
[3448.00s -> 3449.00s]  So you know what local attention
[3449.00s -> 3452.00s]  or sliding window attention is.
[3452.00s -> 3457.00s]  Linear attention is this idea that you essentially,
[3460.00s -> 3463.00s]  so basically in the attention computation,
[3463.00s -> 3468.00s]  there's a key and a query and you dot product them
[3468.00s -> 3470.00s]  and you take the exp of that,
[3470.00s -> 3473.00s]  which is basically giving you a exp kernel.
[3473.00s -> 3476.00s]  So you can basically take a Taylor expansion of that
[3476.00s -> 3479.00s]  and write that computation as basically dot products
[3479.00s -> 3481.00s]  of some nonlinear map.
[3481.00s -> 3485.00s]  So then what you essentially have is you can think about
[3485.00s -> 3488.00s]  for every key value position,
[3488.00s -> 3491.00s]  you are basically applying some sort of nonlinearity
[3491.00s -> 3492.00s]  blowing up into some space,
[3492.00s -> 3495.00s]  and then doing some linear computation over it.
[3495.00s -> 3497.00s]  And because it's linear attention,
[3497.00s -> 3500.00s]  it actually kind of behaves like an RNN
[3500.00s -> 3504.00s]  and it's linear in the sequence length
[3504.00s -> 3506.00s]  rather than quadratic.
[3506.00s -> 3507.00s]  I know that was a little bit fast,
[3507.00s -> 3512.00s]  but I just want to give you sort of the taste of it.
[3512.00s -> 3515.00s]  And so this idea has been actually
[3515.00s -> 3517.00s]  scaled up quite successfully.
[3517.00s -> 3521.00s]  So there's this organization called Minimax
[3521.00s -> 3525.00s]  that's training pretty legitimate models
[3525.00s -> 3530.00s]  up to 456 billion parameter MOEs.
[3530.00s -> 3534.00s]  And they use this basically linear attention idea.
[3534.00s -> 3538.00s]  Now they have to use full attention
[3538.00s -> 3541.00s]  still once in a while, it seems.
[3541.00s -> 3545.00s]  I don't think people have been able to
[3545.00s -> 3548.00s]  get around having some full attention,
[3548.00s -> 3551.00s]  but at least it seems like people have been able
[3551.00s -> 3554.00s]  to get rid of most of full attention.
[3554.00s -> 3557.00s]  Most of the layers are not full attention anymore.
[3557.00s -> 3560.00s]  They're just either linear layers or local attention layers
[3560.00s -> 3564.00s]  which are much, much more efficient.
[3564.00s -> 3570.00s]  Okay, so the linear plus local attention
[3570.00s -> 3574.00s]  now are actually yielding serious state of the art models.
[3574.00s -> 3577.00s]  And it's probably safe to say that
[3578.00s -> 3581.00s]  I don't know what exactly the close model providers
[3581.00s -> 3584.00s]  are doing, but I would suspect that there would be
[3584.00s -> 3587.00s]  at least as kind of advanced in terms of
[3587.00s -> 3591.00s]  as efficient as this in leveraging sparsity.
[3593.00s -> 3595.00s]  So it's kind of an interesting question
[3595.00s -> 3598.00s]  when people ask, well, is attention all you need
[3598.00s -> 3600.00s]  as transformers?
[3600.00s -> 3603.00s]  Well, yes and no.
[3603.00s -> 3605.00s]  I guess in some sense there is still
[3605.00s -> 3607.00s]  a sense squared there.
[3607.00s -> 3609.00s]  Maybe we'll be able to get rid of it,
[3609.00s -> 3611.00s]  but most of the transformer has been
[3611.00s -> 3613.00s]  pretty radically changed by having
[3613.00s -> 3616.00s]  other much lighter weight components.
[3616.00s -> 3618.00s]  And you're still able to get
[3618.00s -> 3621.00s]  much of the same kind of accuracies.
[3622.00s -> 3626.00s]  And all this is really helpful for inference
[3626.00s -> 3629.00s]  because on these non-full attention layers
[3629.00s -> 3633.00s]  you're basically replacing the ordered T KV cache
[3633.00s -> 3637.00s]  which grows as a sequencing with something that's constant.
[3638.00s -> 3643.00s]  And there's papers that follow up,
[3643.00s -> 3647.00s]  I think on the based paper, where did that go?
[3647.00s -> 3650.00s]  There's some, either in this paper or in follow up work
[3650.00s -> 3654.00s]  analyzing basically the trade-off between the KV size
[3654.00s -> 3659.00s]  and the ability to do various types of recall tasks.
[3659.00s -> 3662.00s]  Which makes sense because if you don't store very much
[3662.00s -> 3664.00s]  you won't be able to solve certain tasks,
[3664.00s -> 3666.00s]  but there is this trade-off curve
[3666.00s -> 3669.00s]  that you can try to play with.
[3670.00s -> 3676.00s]  Okay, so that's all I'll say about the safe space models.
[3676.00s -> 3679.00s]  So now let's talk about a completely different style
[3679.00s -> 3682.00s]  of generation models, diffusion models.
[3682.00s -> 3684.00s]  So diffusion models have been very popular
[3684.00s -> 3686.00s]  in image generation, but they turn out to be
[3686.00s -> 3689.00s]  fairly tricky to get working in text.
[3689.00s -> 3693.00s]  Although there recently have been some advances here.
[3693.00s -> 3696.00s]  So the idea of diffusion is that you,
[3696.00s -> 3698.00s]  instead of generating all regressively,
[3698.00s -> 3700.00s]  you just generate every token in parallel.
[3700.00s -> 3704.00s]  So obviously if you only do that via some simple layer
[3704.00s -> 3705.00s]  it's not gonna be very good.
[3705.00s -> 3707.00s]  You can't generate all the words in parallel
[3707.00s -> 3709.00s]  and expect it to be coherent.
[3709.00s -> 3711.00s]  But what you do is you iterate
[3711.00s -> 3714.00s]  and you keep on refining this generation
[3714.00s -> 3719.00s]  until it gets to your final generation that you output.
[3719.00s -> 3725.00s]  And the idea behind generating in parallel,
[3725.00s -> 3729.00s]  you no longer are regressively bound
[3729.00s -> 3732.00s]  and that generating all tokens in parallel
[3732.00s -> 3734.00s]  can be done in parallel.
[3734.00s -> 3738.00s]  So you get to saturate your GPUs relatively easy
[3738.00s -> 3741.00s]  as long as your context length is large enough.
[3741.00s -> 3744.00s]  So recently there's this,
[3744.00s -> 3749.00s]  Insection Labs has produced some pretty interesting models.
[3749.00s -> 3751.00s]  There's not much written about them,
[3751.00s -> 3753.00s]  but you can see kind of a demo
[3753.00s -> 3755.00s]  of the generation and process.
[3755.00s -> 3759.00s]  It just kind of generates code instantaneously,
[3759.00s -> 3761.00s]  but it's obviously kind of broken code
[3761.00s -> 3766.00s]  and then it kind of refines over time.
[3767.00s -> 3771.00s]  So, and this is one of their benchmarks
[3771.00s -> 3774.00s]  that show that, at least on coding,
[3774.00s -> 3776.00s]  I'm not sure about other tasks,
[3776.00s -> 3778.00s]  that if you look at that tokens per second,
[3778.00s -> 3780.00s]  these models are like way out here
[3780.00s -> 3784.00s]  in terms of speed compared to anything that's transformer.
[3784.00s -> 3789.00s]  Even Jamba, remember, is like a hybrid Mamba.
[3789.00s -> 3792.00s]  Transformer architecture is quite slow
[3792.00s -> 3795.00s]  compared to these diffusion models.
[3795.00s -> 3798.00s]  So now whether diffusion models will be
[3798.00s -> 3800.00s]  kind of general purpose and powerful enough
[3800.00s -> 3802.00s]  and all of these, that remains to be seen,
[3802.00s -> 3805.00s]  but I think you have such a lead on
[3805.00s -> 3808.00s]  kind of the tokens speed here
[3808.00s -> 3811.00s]  that even if you,
[3811.00s -> 3813.00s]  I think you can put more compute
[3813.00s -> 3817.00s]  and kind of recover some of the accuracy losses
[3817.00s -> 3820.00s]  if you need to.
[3820.00s -> 3823.00s]  Okay, so the summary here is that
[3823.00s -> 3826.00s]  I think this whole kind of architecture,
[3826.00s -> 3829.00s]  novel architecture thing is actually really
[3829.00s -> 3832.00s]  exciting for inference
[3832.00s -> 3835.00s]  because they allow you to
[3835.00s -> 3838.00s]  sidestep kind of fundamental obstacles, right?
[3838.00s -> 3840.00s]  So if you're dealing with a tension,
[3840.00s -> 3843.00s]  you just have this fundamental KB cache obstacle
[3843.00s -> 3845.00s]  that you can quantize, you can optimize,
[3845.00s -> 3848.00s]  but it's still there.
[3848.00s -> 3851.00s]  And so by making a kind of safe space model,
[3851.00s -> 3853.00s]  you're shrinking that to a constant size,
[3853.00s -> 3855.00s]  and as long as you can keep up the accuracy,
[3855.00s -> 3858.00s]  which is big if, then you win big time.
[3858.00s -> 3860.00s]  Same with the diffusion models.
[3860.00s -> 3862.00s]  Autoregressive generation is a key bottleneck.
[3862.00s -> 3865.00s]  Now if you just generate things in parallel,
[3865.00s -> 3869.00s]  now all of a sudden you kind of change the game completely.
[3869.00s -> 3871.00s]  So there's much more work to be done here
[3871.00s -> 3875.00s]  in proving inference.
[3875.00s -> 3877.00s]  So as you can see now,
[3877.00s -> 3880.00s]  the inference game is much broader
[3880.00s -> 3882.00s]  than it seems at first sight.
[3882.00s -> 3884.00s]  It's not really about kind of necessarily
[3884.00s -> 3886.00s]  the systems optimizations to make it fast,
[3886.00s -> 3887.00s]  although you obviously need those,
[3887.00s -> 3890.00s]  but I think the real gains are coming from
[3890.00s -> 3894.00s]  like real radical changes in architecture.
[3894.00s -> 3896.00s]  Okay, so about 10 minutes left,
[3896.00s -> 3899.00s]  I'll go through these quickly.
[3899.00s -> 3900.00s]  Quantization and model pruning.
[3900.00s -> 3903.00s]  So quantization, the key idea is
[3903.00s -> 3906.00s]  just reduce the precision of the numbers, okay?
[3906.00s -> 3910.00s]  So very easy to do,
[3910.00s -> 3913.00s]  and the thought is that less memory
[3913.00s -> 3917.00s]  means less bytes transferred higher,
[3917.00s -> 3921.00s]  sorry, there should be lower latency, higher throughput.
[3921.00s -> 3924.00s]  And you do have to worry about accuracy, of course.
[3924.00s -> 3926.00s]  That's the trade-off.
[3926.00s -> 3931.00s]  If you look at the different types of formats,
[3931.00s -> 3936.00s]  FP32 used for training, not used for inference, really.
[3936.00s -> 3939.00s]  BF16 is sort of the default for inference.
[3939.00s -> 3942.00s]  You can go down to FP8 or INT8,
[3942.00s -> 3947.00s]  which now is less accurate,
[3947.00s -> 3950.00s]  but much cheaper than even FP8.
[3950.00s -> 3953.00s]  So people do do a bunch of inference in INT8,
[3953.00s -> 3954.00s]  which if you look at the range,
[3954.00s -> 3958.00s]  I mean, it's an integer between 127 minus 128,
[3958.00s -> 3961.00s]  which is not that, it's pretty low precision.
[3961.00s -> 3964.00s]  And people are even going down to INT4,
[3964.00s -> 3966.00s]  which is, they're not,
[3966.00s -> 3970.00s]  okay, so INT4 is pretty low.
[3970.00s -> 3973.00s]  There's also other ways you can do,
[3973.00s -> 3975.00s]  okay, so you can,
[3975.00s -> 3982.00s]  so once you kind of decide that you want to quantize,
[3982.00s -> 3986.00s]  I guess you could do several things.
[3986.00s -> 3988.00s]  You can train with the quantization,
[3988.00s -> 3990.00s]  but obviously that means you need to retrain models.
[3990.00s -> 3992.00s]  And more, I guess, commonly,
[3992.00s -> 3993.00s]  you do post-training quantization,
[3993.00s -> 3995.00s]  where you take an existing model
[3995.00s -> 3996.00s]  and you try to quantize it
[3996.00s -> 4000.00s]  and try not to screw things up too much.
[4000.00s -> 4004.00s]  So there's a paper called element INT8,
[4004.00s -> 4007.00s]  which I'll talk through kind of briefly.
[4007.00s -> 4010.00s]  So in quantization,
[4010.00s -> 4014.00s]  basically what happens is that you take your vector,
[4014.00s -> 4016.00s]  which is, let's say, FP16,
[4016.00s -> 4020.00s]  and then you need to figure out the dynamic range, right?
[4020.00s -> 4021.00s]  If you want to pack it into INT8,
[4021.00s -> 4023.00s]  you need to figure out what the largest value is,
[4023.00s -> 4024.00s]  and once you figure that out,
[4024.00s -> 4029.00s]  you can kind of divide by that and multiply by 128,
[4029.00s -> 4031.00s]  and then you get your integers.
[4031.00s -> 4035.00s]  And then if you need to de-quantize,
[4035.00s -> 4038.00s]  then you kind of go the other way.
[4038.00s -> 4040.00s]  So basically, quantization means that,
[4040.00s -> 4042.00s]  remember, memory is the bandwidth, right?
[4042.00s -> 4043.00s]  So, bottleneck.
[4043.00s -> 4048.00s]  So all your transfers are happening in INT8,
[4048.00s -> 4051.00s]  but when you actually do the,
[4051.00s -> 4056.00s]  I guess you sometimes have to upcast to a floating point
[4056.00s -> 4059.00s]  to actually do the arithmetic.
[4059.00s -> 4062.00s]  Okay, so the problem with INT8
[4062.00s -> 4065.00s]  is that not everything fits nicely,
[4065.00s -> 4067.00s]  and you have these outliers,
[4067.00s -> 4070.00s]  which appear in larger networks that screw things up.
[4070.00s -> 4074.00s]  So what this paper did is that you take this matrix,
[4074.00s -> 4076.00s]  you identify the really large outlier values,
[4076.00s -> 4078.00s]  and then you handle them separately
[4078.00s -> 4081.00s]  using full 16-bit precision,
[4081.00s -> 4086.00s]  and then do most of the vast majority in INT8.
[4086.00s -> 4092.00s]  So this works well, but is actually a bit slower.
[4092.00s -> 4094.00s]  So the motivation here wasn't the inference speed,
[4094.00s -> 4098.00s]  but more to even be able to fit your model into memory.
[4098.00s -> 4100.00s]  There's another paper called
[4100.00s -> 4102.00s]  Activation-Aware Quantization,
[4102.00s -> 4105.00s]  and here the idea is that
[4105.00s -> 4110.00s]  you're kind of quantizing the weights,
[4110.00s -> 4113.00s]  but you're gonna figure out which weights
[4113.00s -> 4117.00s]  to quantize based on the activations.
[4117.00s -> 4119.00s]  You know, really quickly,
[4119.00s -> 4122.00s]  you're going down to actually INT3,
[4122.00s -> 4125.00s]  and this obviously reduces memory by quite a bit
[4125.00s -> 4128.00s]  and leads to a 3x speedup.
[4128.00s -> 4131.00s]  And so the general idea here is that
[4131.00s -> 4134.00s]  you get a trained model,
[4134.00s -> 4137.00s]  and it just happens that some of the weights
[4137.00s -> 4140.00s]  or activations are gonna be abnormally large,
[4140.00s -> 4142.00s]  so for those you handle separately,
[4142.00s -> 4145.00s]  and then everything else you can kind of work in
[4145.00s -> 4147.00s]  low precision.
[4147.00s -> 4150.00s]  Okay, talk about model pruning.
[4150.00s -> 4153.00s]  Ideas vary like quantization.
[4154.00s -> 4156.00s]  The basic idea is very simple.
[4156.00s -> 4158.00s]  You just rip out parts of an expensive model
[4158.00s -> 4159.00s]  to make it cheaper,
[4159.00s -> 4161.00s]  and then you fix it up.
[4161.00s -> 4163.00s]  So in this NVD paper,
[4163.00s -> 4167.00s]  what they do is they first identify important
[4167.00s -> 4171.00s]  either layers or heads or hidden dimensions
[4171.00s -> 4173.00s]  using a small calibration size.
[4173.00s -> 4177.00s]  They use some simple scores to compute that,
[4177.00s -> 4181.00s]  and then you just remove the unimportant layers
[4181.00s -> 4183.00s]  or hidden units or heads,
[4183.00s -> 4185.00s]  and then now if you just take that model,
[4185.00s -> 4188.00s]  it's gonna be clearly worse.
[4188.00s -> 4191.00s]  So then the last step is you distill the original model
[4191.00s -> 4193.00s]  into the pruned model.
[4193.00s -> 4195.00s]  So you kind of repair the model
[4195.00s -> 4197.00s]  from the initialization, which is your prune.
[4197.00s -> 4199.00s]  So you're not starting from scratch.
[4199.00s -> 4201.00s]  You're starting from something that's worse
[4201.00s -> 4203.00s]  but hopefully not worse
[4203.00s -> 4205.00s]  and hopefully retains a lot of the same
[4205.00s -> 4208.00s]  structural properties of the original model.
[4208.00s -> 4212.00s]  So it's just maybe not calibrated in some sense.
[4212.00s -> 4214.00s]  And the results are pretty good on that.
[4214.00s -> 4217.00s]  So they have these 15 billion parameter models
[4217.00s -> 4220.00s]  that they're able to reduce to 8B
[4220.00s -> 4222.00s]  with hardly any drop in this,
[4222.00s -> 4225.00s]  I guess at least according to MMLU,
[4225.00s -> 4228.00s]  and then down to 4B with some drop,
[4228.00s -> 4232.00s]  but you're also going down quite a bit to a 4B model.
[4233.00s -> 4239.00s]  Okay, so maybe just summarize this taking shortcuts idea.
[4239.00s -> 4242.00s]  You can reduce inference complexity
[4242.00s -> 4243.00s]  without hurting accuracy.
[4243.00s -> 4244.00s]  You can do it from scratch
[4244.00s -> 4246.00s]  where you just define a fresh architecture
[4246.00s -> 4249.00s]  that's by construction fast and just train it.
[4249.00s -> 4252.00s]  Or you can distill, you define architecture,
[4252.00s -> 4255.00s]  you can take a slow model
[4255.00s -> 4258.00s]  and you figure out some sort of scheme
[4258.00s -> 4262.00s]  to initialize the new model with the old model
[4262.00s -> 4265.00s]  and then you basically do distillation.
[4270.00s -> 4275.00s]  Okay, so now all of these are a little bit unsatisfying
[4275.00s -> 4277.00s]  because they're lossy.
[4277.00s -> 4279.00s]  So you get massive speed ups,
[4279.00s -> 4281.00s]  but you always wonder,
[4281.00s -> 4285.00s]  well, maybe this model isn't as actually good as original.
[4285.00s -> 4287.00s]  So speculative decoding or speculative sampling
[4287.00s -> 4292.00s]  allows you to basically have your cake and eat it too.
[4292.00s -> 4294.00s]  So recall there's two stages of inference.
[4294.00s -> 4298.00s]  You pre-fill, which you're given a sequence,
[4298.00s -> 4300.00s]  you encode all these tokens in parallel,
[4300.00s -> 4302.00s]  there's a compute limited, which is great.
[4302.00s -> 4305.00s]  Notice that this also gives you log probabilities
[4305.00s -> 4307.00s]  for each of the tokens.
[4307.00s -> 4310.00s]  And then there's generation, which is one token at a time.
[4310.00s -> 4312.00s]  It's memory limited, it's slow.
[4312.00s -> 4316.00s]  So in other words, checking is faster than generation.
[4317.00s -> 4318.00s]  So intuitively this makes sense,
[4318.00s -> 4320.00s]  but hopefully now you also appreciate
[4320.00s -> 4324.00s]  the math behind why this is true.
[4324.00s -> 4327.00s]  And the speculative sampling idea is actually really,
[4327.00s -> 4330.00s]  really again, it was proposed in parallel
[4330.00s -> 4334.00s]  by these two independent teams from Google.
[4334.00s -> 4338.00s]  And the idea is to use a cheap draft model P
[4341.00s -> 4344.00s]  to just run ahead and generate some tokens.
[4344.00s -> 4346.00s]  And then you're gonna evaluate those tokens
[4346.00s -> 4348.00s]  with a target model.
[4348.00s -> 4352.00s]  And because evaluation of given tokens is just pre-filled,
[4352.00s -> 4354.00s]  so you can do that in parallel, which is fast.
[4354.00s -> 4357.00s]  And then you accept it if it looks good.
[4357.00s -> 4361.00s]  So this is what it looks like in real life.
[4361.00s -> 4363.00s]  So if you're using a big model,
[4363.00s -> 4365.00s]  generating one token at a time, that's slow.
[4365.00s -> 4369.00s]  But in speculative decoding, you have a draft model
[4369.00s -> 4373.00s]  that's racing ahead, generating a lot of tokens,
[4373.00s -> 4377.00s]  and using the big model to essentially verify.
[4377.00s -> 4380.00s]  And sometimes it'll reject, and sometimes it'll accept,
[4380.00s -> 4383.00s]  and the acceptance rate basically determines
[4383.00s -> 4386.00s]  how fast of a speed up you have.
[4387.00s -> 4391.00s]  Okay, so here is the more formal algorithm.
[4391.00s -> 4395.00s]  So you're gonna have a look ahead of K.
[4395.00s -> 4397.00s]  So you're gonna use your draft model
[4397.00s -> 4400.00s]  and generate K tokens autoregressively.
[4400.00s -> 4403.00s]  So this is hopefully fast because your draft model's small.
[4403.00s -> 4406.00s]  And then you're given these K tokens that you generated,
[4406.00s -> 4408.00s]  and I'm gonna score them based on,
[4408.00s -> 4409.00s]  I'm gonna compute the probability
[4409.00s -> 4412.00s]  under the target model Q.
[4412.00s -> 4415.00s]  Now I'm gonna decide whether I wanna accept this or not.
[4415.00s -> 4417.00s]  So I go through each token,
[4417.00s -> 4420.00s]  and I'm going to essentially accept it
[4420.00s -> 4424.00s]  with probability Q over P.
[4424.00s -> 4427.00s]  And the one just is, make sure this,
[4428.00s -> 4430.00s]  probabilities are between zero and one.
[4430.00s -> 4432.00s]  If this kind of looks like,
[4432.00s -> 4434.00s]  if people are familiar with Metropolis Hastings,
[4434.00s -> 4437.00s]  this is kind of where this kind of comes from.
[4437.00s -> 4441.00s]  So intuitively, you're sampling with P,
[4441.00s -> 4442.00s]  so you need to divide that out,
[4442.00s -> 4444.00s]  because you don't want P, you want Q.
[4444.00s -> 4447.00s]  So this is kind of importance weight on this.
[4447.00s -> 4449.00s]  So if you accept it, then great,
[4449.00s -> 4450.00s]  you kind of move on,
[4450.00s -> 4453.00s]  and you look at the next draft token and so on.
[4453.00s -> 4455.00s]  And if you don't accept it,
[4455.00s -> 4460.00s]  then you're going to sample from the target model,
[4461.00s -> 4462.00s]  the slow model,
[4462.00s -> 4464.00s]  but you kind of do this correction
[4464.00s -> 4466.00s]  where you've already tried to sample using P,
[4466.00s -> 4468.00s]  so you don't need to do that anymore,
[4468.00s -> 4471.00s]  you subtract it out and you sample from Q.
[4471.00s -> 4474.00s]  So this is basically kind of a rejection sampling
[4474.00s -> 4478.00s]  with a proposal P and a target, sorry, target Q.
[4478.00s -> 4483.00s]  The only difference is that you are sampling,
[4483.00s -> 4485.00s]  you're injection sampling,
[4485.00s -> 4487.00s]  if you reject, then you reject,
[4487.00s -> 4489.00s]  and you just try again and you try again.
[4489.00s -> 4493.00s]  And here, we don't want to keep on kind of looping forever,
[4493.00s -> 4495.00s]  because if you reject, we're just going to say,
[4495.00s -> 4496.00s]  okay, fine, we'll bite the bullet
[4496.00s -> 4500.00s]  and just sample from the more expensive model.
[4502.00s -> 4504.00s]  So the cool thing here is that
[4504.00s -> 4507.00s]  you're guaranteed to get an exact sample
[4507.00s -> 4509.00s]  from the target model.
[4509.00s -> 4511.00s]  Okay, so those of you familiar with sampling,
[4511.00s -> 4513.00s]  this shouldn't be too surprising.
[4513.00s -> 4515.00s]  You're able to use kind of prior information
[4515.00s -> 4518.00s]  to speed up sampling,
[4518.00s -> 4522.00s]  but in a language modeling context, this is kind of nice.
[4522.00s -> 4524.00s]  I'm going to skip the, this is not really a proof,
[4524.00s -> 4526.00s]  this is just kind of some derivation
[4526.00s -> 4529.00s]  to show that for a case of vocab two,
[4529.00s -> 4532.00s]  why these formulas kind of give you
[4532.00s -> 4536.00s]  the right unbiased sampling procedure.
[4536.00s -> 4539.00s]  And it works pretty well,
[4539.00s -> 4542.00s]  so the accuracy is, you know,
[4542.00s -> 4546.00s]  it should be actually the same, since it's the same model,
[4546.00s -> 4548.00s]  but maybe there's some randomness there.
[4548.00s -> 4550.00s]  But the speedup is, you're getting a factor of two,
[4550.00s -> 4552.00s]  speedup, essentially.
[4556.00s -> 4558.00s]  So in practice, what you do is
[4558.00s -> 4561.00s]  you have something like a 70b model,
[4561.00s -> 4564.00s]  and your draft model is much, much smaller.
[4564.00s -> 4567.00s]  And if your target model is 80b, 8b,
[4567.00s -> 4569.00s]  then your draft model might be 1b.
[4569.00s -> 4571.00s]  And you generally want to make the draft model
[4571.00s -> 4574.00s]  as close to the target as possible,
[4574.00s -> 4577.00s]  and so if you're doing some distillation,
[4577.00s -> 4580.00s]  that could make it even better.
[4580.00s -> 4584.00s]  There's a bunch of, this is a pretty hot area
[4584.00s -> 4586.00s]  of research and inference.
[4586.00s -> 4589.00s]  There's a lot of ways to improve this process.
[4589.00s -> 4593.00s]  You can use Medusa, which is this way
[4593.00s -> 4595.00s]  to have the draft model instead of
[4595.00s -> 4597.00s]  generally auto-regressively sample multiple tokens
[4597.00s -> 4600.00s]  in parallel, or EGLE, where you're actually
[4600.00s -> 4603.00s]  taking high-level features of the target model
[4603.00s -> 4605.00s]  and pumping them into the draft model to generate.
[4605.00s -> 4608.00s]  So the draft model doesn't actually have to stand alone.
[4608.00s -> 4610.00s]  It can be kind of glum onto the target model
[4610.00s -> 4612.00s]  to help it generate.
[4613.00s -> 4617.00s]  So, summary, exact sampling from the target model,
[4617.00s -> 4621.00s]  thanks to math, and this exploits the symmetry
[4621.00s -> 4624.00s]  between checking and generation.
[4625.00s -> 4628.00s]  Or pre-fill and generation.
[4628.00s -> 4631.00s]  And there's actually a lot of room for innovation
[4631.00s -> 4634.00s]  on the draft model, which can,
[4634.00s -> 4637.00s]  everything that we've talked about before,
[4637.00s -> 4639.00s]  where you can have different radical architectures,
[4639.00s -> 4643.00s]  different ways of quantizing, all of those apply.
[4643.00s -> 4647.00s]  The only thing is that you get to basically
[4647.00s -> 4650.00s]  guarantee that you're getting an exact sample.
[4650.00s -> 4653.00s]  Okay, so now I'll go, I'm out of time,
[4653.00s -> 4657.00s]  but quickly go through the question that came up earlier,
[4657.00s -> 4661.00s]  which is that in practice when you're surveying,
[4661.00s -> 4664.00s]  there's live traffic, requests come at different times,
[4664.00s -> 4666.00s]  they finish at different times,
[4666.00s -> 4669.00s]  they have, some of them have shared prefixes,
[4669.00s -> 4671.00s]  some of them don't, they have different lengths.
[4671.00s -> 4676.00s]  So it's very heterogeneous in comparison to training,
[4676.00s -> 4679.00s]  where you get basically a dense block of tokens,
[4679.00s -> 4682.00s]  and you're basically gonna push it through your GPU
[4682.00s -> 4684.00s]  at full speed.
[4684.00s -> 4686.00s]  So what do you do in this case?
[4686.00s -> 4690.00s]  So there's a series of papers that kind of explore this,
[4690.00s -> 4694.00s]  and the basic idea is, so the last two parties
[4694.00s -> 4698.00s]  are more of a kind of systems level contribution.
[4698.00s -> 4702.00s]  So the idea is that you don't wait for batches,
[4702.00s -> 4707.00s]  the train leaves, the train doesn't wait for you.
[4707.00s -> 4711.00s]  So when a new batch comes, you're just gonna put it in.
[4711.00s -> 4719.00s]  Which means that the worker that's generating tokens
[4719.00s -> 4722.00s]  needs to kind of hand control back to the scheduler
[4722.00s -> 4723.00s]  every step.
[4723.00s -> 4725.00s]  So you generate a token, come back to the scheduler
[4725.00s -> 4731.00s]  and say if there's new requests, then they get stuck in,
[4731.00s -> 4733.00s]  and then it kind of continues.
[4733.00s -> 4735.00s]  So you're kind of not wasting any time
[4735.00s -> 4737.00s]  waiting around for requests.
[4737.00s -> 4740.00s]  Now there's a kind of a problem with batching,
[4740.00s -> 4742.00s]  I think, which is behind the question.
[4742.00s -> 4745.00s]  Batching works when everything's the same dimensionality,
[4745.00s -> 4747.00s]  but every request might be a different length.
[4747.00s -> 4749.00s]  So there's this idea of selective batching
[4749.00s -> 4754.00s]  where you basically break up your computation for attention,
[4754.00s -> 4756.00s]  everything has to be handled separately,
[4756.00s -> 4758.00s]  but for your MLPs, remember,
[4758.00s -> 4760.00s]  which are the bulk of the computation,
[4760.00s -> 4763.00s]  you can actually take tensors of different sizes
[4763.00s -> 4765.00s]  and you just flatten them.
[4765.00s -> 4767.00s]  And because they don't interact,
[4767.00s -> 4771.00s]  they can just be kind of along for the ride
[4771.00s -> 4772.00s]  in the batch dimension.
[4772.00s -> 4773.00s]  Okay, I know that was fast,
[4773.00s -> 4777.00s]  but I'll just quickly go over page attention now.
[4777.00s -> 4779.00s]  This is the paper behind VLM,
[4779.00s -> 4782.00s]  which some of you probably have used.
[4782.00s -> 4785.00s]  And this addresses the memory usage problem.
[4785.00s -> 4787.00s]  So if you have a KV cache
[4787.00s -> 4789.00s]  and prompts are coming in and finishing,
[4789.00s -> 4791.00s]  then your cache is gonna get fragmented.
[4791.00s -> 4797.00s]  So you're gonna allocate a bunch of space for a request,
[4797.00s -> 4799.00s]  but you don't know how many tokens are gonna generate it.
[4799.00s -> 4802.00s]  So there's gonna be internal fragmentation
[4802.00s -> 4804.00s]  and then there's also gonna be external fragmentation
[4804.00s -> 4808.00s]  where there's padding between the requests and responses.
[4808.00s -> 4809.00s]  So that's no good.
[4809.00s -> 4811.00s]  So the page attention basically says,
[4811.00s -> 4813.00s]  remember operating systems?
[4813.00s -> 4816.00s]  We have, and how virtual memory works,
[4816.00s -> 4820.00s]  we divide a KV cache into a sequence of contiguous blocks
[4820.00s -> 4825.00s]  and then we just put them wherever we find wide space.
[4825.00s -> 4828.00s]  So if you have two requests coming in,
[4828.00s -> 4830.00s]  then they might just,
[4830.00s -> 4833.00s]  the first request might be here, here, and here,
[4833.00s -> 4835.00s]  and the second request might be here and here.
[4835.00s -> 4836.00s]  So the blocks are the things
[4836.00s -> 4837.00s]  that you're gonna keep contiguous
[4837.00s -> 4844.00s]  and that's gonna allow you time to coalesce your memory.
[4844.00s -> 4847.00s]  So you can also play these tricks
[4847.00s -> 4850.00s]  where if you have sharing of prefixes,
[4850.00s -> 4854.00s]  then there's another idea from operating systems
[4854.00s -> 4856.00s]  which is copy-on-write.
[4856.00s -> 4859.00s]  So you basically maintain reference counters
[4859.00s -> 4862.00s]  for how many basically sequences
[4862.00s -> 4864.00s]  are using this particular block
[4864.00s -> 4866.00s]  and then if you need to kind of diverge
[4866.00s -> 4869.00s]  and have blocks go in different directions,
[4869.00s -> 4872.00s]  then you copy and you reduce the reference count.
[4872.00s -> 4874.00s]  There's a bunch of other VLM optimizations
[4874.00s -> 4876.00s]  which I won't go through,
[4876.00s -> 4878.00s]  but basically the summary is,
[4878.00s -> 4882.00s]  remembering your operating systems classes,
[4882.00s -> 4885.00s]  you can apply them to inference as well.
[4885.00s -> 4887.00s]  Okay, so quick summary.
[4887.00s -> 4890.00s]  Inference is really, really important.
[4890.00s -> 4894.00s]  And the characteristics are distinct from training.
[4894.00s -> 4897.00s]  You're memory limited and it's also dynamic,
[4897.00s -> 4901.00s]  so which leads to a bunch of new challenges.
[4901.00s -> 4903.00s]  We saw a whole host of different techniques
[4903.00s -> 4905.00s]  around new architectures, quantization, pruning,
[4906.00s -> 4909.00s]  distillation, speculative decoding.
[4909.00s -> 4913.00s]  There's ideas from systems which can allow you
[4913.00s -> 4917.00s]  to better use your memory, overlap communication
[4917.00s -> 4919.00s]  and compute and things like that.
[4919.00s -> 4924.00s]  But I would say that there's probably even more opportunity
[4924.00s -> 4927.00s]  in sort of the modeling and architecture
[4927.00s -> 4928.00s]  because if you think about it,
[4928.00s -> 4930.00s]  all you, you don't, inference,
[4930.00s -> 4933.00s]  not only is inference in a particular model.
[4933.00s -> 4935.00s]  How do I run this particular model?
[4935.00s -> 4937.00s]  But who cares running about that particular model?
[4937.00s -> 4939.00s]  You care about delivering good accuracy
[4939.00s -> 4941.00s]  given your resource budget.
[4941.00s -> 4945.00s]  So a lot of these ideas that are trying to
[4945.00s -> 4949.00s]  reduce the KV cache, changing the transformer
[4949.00s -> 4953.00s]  are basically ways to sidestep the problem
[4953.00s -> 4955.00s]  and say, well, I have something that's more efficient
[4955.00s -> 4957.00s]  and I can train it in a way
[4957.00s -> 4961.00s]  that gets me better accuracy, then I win.
[4961.00s -> 4964.00s]  Okay, so that's all I have, and I will see you next time.
[4964.00s -> 4966.00s]  Then we're back to scaling laws.
