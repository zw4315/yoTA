# Detected language: en (p=1.00)

[0.00s -> 5.00s]  I'm gonna talk a little bit about scaling laws.
[6.84s -> 8.70s]  Originally I think we were gonna talk about inference,
[8.70s -> 11.84s]  but I'll take a few minutes to start on scaling laws
[11.84s -> 12.98s]  and then we'll kind of figure out
[12.98s -> 14.44s]  where we'll go from there.
[15.62s -> 19.08s]  Okay, so the whole point of scaling laws
[19.08s -> 21.12s]  is kind of, well, to begin with,
[21.12s -> 22.26s]  I want you to put yourself
[22.26s -> 24.28s]  into the following scenario, right?
[24.28s -> 26.00s]  So you have a very rich friend
[26.00s -> 29.12s]  and he or she has given you 10,000,
[29.14s -> 32.02s]  actually let's say 100,000 H100s for a month
[32.02s -> 35.92s]  and you have to build the best open source LM that you can.
[35.92s -> 38.40s]  So this is a somewhat hard task
[38.40s -> 41.10s]  and we've given you some of the tools that you need
[41.10s -> 43.40s]  to make progress on this question.
[43.40s -> 46.46s]  So you can put together your infra team
[46.46s -> 47.40s]  and your systems people
[47.40s -> 50.50s]  and you can put together a distributed training framework.
[50.50s -> 51.90s]  In the next assignment after that,
[51.90s -> 54.20s]  you're gonna put together a great pre-training dataset
[54.20s -> 55.50s]  and then you kind of know all about
[55.50s -> 56.58s]  architectures and so on.
[56.58s -> 58.54s]  So you kind of know, you have all the pieces
[59.20s -> 61.26s]  and so we can turn the crank and we can run the big model.
[62.56s -> 65.04s]  And in the first couple lectures,
[65.04s -> 67.84s]  we talked about all the other various decisions
[67.84s -> 69.80s]  you might make along this journey, right?
[69.80s -> 70.84s]  Like what's the architecture?
[70.84s -> 71.94s]  What's the hyperparameters?
[71.94s -> 73.98s]  Like how are you gonna do all these things?
[73.98s -> 77.32s]  Well, I think in some ways the answer I gave you
[77.32s -> 79.12s]  from those early lectures was
[79.12s -> 81.06s]  just pick what other people have done, right?
[81.06s -> 83.92s]  Like just follow llama or whatever other models.
[83.92s -> 85.50s]  But in a way, that's a very boring answer
[85.50s -> 87.52s]  because it doesn't let you push the frontiers, right?
[87.94s -> 89.74s]  If you're in like a big frontier lab
[89.74s -> 91.38s]  and you're gonna build the best model,
[91.38s -> 92.88s]  you don't want to just copy other people,
[92.88s -> 94.72s]  you want to innovate, right?
[94.72s -> 98.32s]  So how do we innovate and get these optimized solutions
[98.32s -> 99.22s]  in the first place?
[99.22s -> 103.06s]  So that's kind of gonna be the point of scaling laws.
[103.06s -> 104.56s]  What we want to do is we want to build
[104.56s -> 106.92s]  simple predictive laws
[106.92s -> 109.42s]  for the behavior of language models.
[109.42s -> 112.26s]  And scaling laws are basically this whole idea
[112.26s -> 115.80s]  of being able to take small models, scale them up,
[116.32s -> 119.52s]  and be able to do that in order to improve your engineering.
[119.52s -> 121.26s]  So one way of thinking about this
[121.26s -> 124.12s]  is the old unpleasant way of doing deep learning
[124.12s -> 126.50s]  is just train a bunch of big models,
[126.50s -> 128.36s]  tune your hyperparameters so that your big models
[128.36s -> 130.24s]  are good, right?
[130.24s -> 132.50s]  That's just gonna cost tons and tons of compute.
[132.50s -> 135.34s]  Like you can't really easily do that.
[135.34s -> 137.98s]  And so I think the new optimism,
[137.98s -> 140.34s]  and if you're sort of following a lot of these
[140.34s -> 141.98s]  developments on scaling,
[141.98s -> 143.38s]  you kind of think of this as all right,
[143.38s -> 145.12s]  we're gonna train a bunch of small models,
[145.54s -> 147.34s]  we're gonna learn a lot of things from those small models,
[147.34s -> 149.04s]  and then we're gonna extrapolate them back up
[149.04s -> 150.58s]  to bigger models, right?
[150.58s -> 152.58s]  So we're gonna take our smallest models
[152.58s -> 155.82s]  at the left side of this sort of compute scale here,
[155.82s -> 157.52s]  and I'm gonna learn a lot about what to do,
[157.52s -> 159.56s]  and then I'm gonna nail it in one go
[159.56s -> 160.96s]  when I build my big model.
[163.12s -> 165.10s]  And the first place I want to start with
[165.10s -> 166.80s]  is just kind of the history
[166.80s -> 168.80s]  and the background of scaling laws.
[168.80s -> 170.56s]  And I want to contextualize this
[170.56s -> 172.90s]  because I think when people talk about scaling laws,
[172.92s -> 174.96s]  often this is done in like very like
[175.86s -> 177.72s]  messianic like AGI terms.
[177.72s -> 179.32s]  So like scaling laws just tell you
[179.32s -> 182.02s]  that these amazing things are log linear forever
[182.02s -> 185.36s]  and we will achieve super intelligence or something.
[185.36s -> 187.76s]  But I think scaling laws are actually much more grounded
[187.76s -> 189.26s]  and have a lot of interesting history.
[189.26s -> 190.54s]  And so I'm gonna start there
[190.54s -> 192.30s]  to sort of try to convince you
[192.30s -> 194.20s]  that scaling laws aren't necessarily
[194.20s -> 196.50s]  just fitting lines on log-log plots,
[196.50s -> 198.00s]  although that is a very big part
[198.00s -> 199.48s]  of what we're gonna do.
[199.50s -> 202.94s]  And then I'm gonna do basically very easy steps.
[202.94s -> 205.44s]  I'm gonna try to convince you that at least for data,
[205.44s -> 207.00s]  scaling laws are a very natural thing
[207.00s -> 208.60s]  to think about and to expect.
[210.54s -> 212.34s]  So as a person that's kind of brought up
[212.34s -> 214.18s]  in statistical machine learning,
[214.18s -> 215.52s]  my starting point is gonna be
[215.52s -> 216.92s]  statistical machine learning, right?
[216.92s -> 219.08s]  Like what is scaling laws?
[219.08s -> 221.02s]  In some ways, scaling laws are telling us
[221.02s -> 222.62s]  as we increase the amount of data
[222.62s -> 224.16s]  or we change the model size,
[224.16s -> 227.96s]  we expect certain behaviors out of the model, right?
[227.98s -> 230.94s]  And if you go back to something like machine learning 101,
[230.94s -> 233.04s]  and if you remember your VC dimensions
[233.04s -> 235.82s]  and Rademacher complexities and so on,
[235.82s -> 238.06s]  in some ways that's the theory version of exactly this.
[238.06s -> 241.66s]  So I have on the top, a generalization bound
[241.66s -> 246.66s]  for the excess risk of learning
[247.06s -> 249.20s]  amongst a finite set of K hypotheses.
[249.20s -> 251.10s]  And we see that that should scale
[251.10s -> 254.26s]  as one over square root of M, right?
[254.26s -> 255.94s]  In some ways that's a theoretical version
[255.96s -> 258.16s]  of a scaling law where we're making predictions
[258.16s -> 260.12s]  about how fast our errors should decay
[260.12s -> 261.70s]  as a function of that.
[262.96s -> 264.12s]  On the bottom, we might have something
[264.12s -> 265.50s]  a little bit more exotic.
[265.50s -> 267.00s]  If we're doing generative modeling
[267.00s -> 268.74s]  and where our generative model
[268.74s -> 271.56s]  is a really flexible non-parametric class,
[271.56s -> 273.40s]  what we might do instead is we might fit
[273.40s -> 275.24s]  some sort of smooth density.
[275.24s -> 278.36s]  So in this case, our prediction is that
[278.36s -> 282.64s]  the L2 sort of error of estimating a density
[282.64s -> 285.24s]  is going to be upper bounded by some polynomial
[285.26s -> 288.06s]  and to the beta over two beta plus one, right?
[288.06s -> 290.98s]  This is what some people might call non-parametric rates.
[290.98s -> 293.50s]  So theorists have been thinking for a very long time
[293.50s -> 296.74s]  about how sample size especially should relate to error,
[296.74s -> 299.02s]  right, this is a very classic problem
[299.02s -> 301.56s]  that people have thought about in machine learning theory.
[301.56s -> 302.78s]  But these are upper bounds,
[302.78s -> 304.98s]  not actual realized loss values.
[304.98s -> 306.58s]  And really scaling laws are in some sense
[306.58s -> 309.72s]  the leap from thinking about kind of the theoretical side
[309.72s -> 313.42s]  of how should data and model size relate to performance
[313.42s -> 315.02s]  and going to the empirical side of saying,
[315.88s -> 317.16s]  our bounds are bad,
[317.16s -> 319.92s]  but maybe we can actually fit these things empirically.
[321.80s -> 326.00s]  And this is a fun trivia fact or arguable trivia fact.
[326.00s -> 328.22s]  What is the first scaling laws paper?
[328.22s -> 330.40s]  And actually not many papers cite this one,
[330.40s -> 334.00s]  but I think probably the right first scaling law papers
[334.00s -> 338.50s]  is a paper from 1993 NeurIPS from Bell Labs.
[338.50s -> 340.12s]  And you might recognize some of these names,
[340.12s -> 343.44s]  these are kind of theorists and some of the people
[343.46s -> 344.86s]  that have done really classic work
[344.86s -> 346.26s]  in machine learning theory,
[346.26s -> 348.64s]  like Vapnik and Corina Cortez and others.
[349.50s -> 353.10s]  And I take an excerpt because I was reading this paper,
[353.10s -> 355.86s]  actually just preparing this lecture earlier,
[356.90s -> 359.70s]  and it just struck me how ahead of its time
[359.70s -> 361.16s]  in many ways this paper was, right?
[361.16s -> 363.94s]  It's saying, training classifiers on large databases
[363.94s -> 365.98s]  is very computationally demanding
[365.98s -> 367.70s]  and we need to figure out which ones are good
[367.70s -> 369.22s]  before actually training them.
[369.22s -> 370.26s]  And so what we're going to do
[370.26s -> 372.66s]  is we're gonna propose a new predictive method
[372.68s -> 374.56s]  that predicts how good a model is gonna be
[374.56s -> 376.40s]  without actually training the whole thing, right?
[376.40s -> 379.40s]  And that sounds a lot like scaling laws.
[379.40s -> 381.10s]  And you'll see this later,
[381.10s -> 382.88s]  but they have a functional form
[382.88s -> 385.60s]  that's basically like, oh, the test error of a model
[385.60s -> 388.44s]  is expressible as some irreducible error
[388.44s -> 390.32s]  plus a polynomially decaying term.
[390.32s -> 391.14s]  And you're like, huh,
[391.14s -> 393.08s]  that looks a lot like a modern scaling law.
[393.08s -> 394.40s]  And they even do the thing
[394.40s -> 397.40s]  where they train a bunch of small models,
[397.40s -> 399.00s]  they fit their curves, and they're like,
[399.00s -> 400.96s]  oh, we can accurately predict the behavior
[400.96s -> 402.66s]  of the model further out.
[402.66s -> 404.74s]  So as with many things, I guess,
[404.74s -> 407.78s]  scaling laws partially thought about
[407.78s -> 409.92s]  at Bell Labs way back when.
[411.22s -> 413.70s]  And of course, there's others that I think
[413.70s -> 416.70s]  have thought about related ideas in scaling,
[416.70s -> 417.74s]  not just scaling laws,
[417.74s -> 420.10s]  but also really the modern mindset, I think,
[420.10s -> 422.06s]  of thinking about scaling.
[422.06s -> 424.18s]  There's another paper that often gets mentioned
[424.18s -> 426.02s]  in sort of the history of scaling laws,
[426.02s -> 429.18s]  Banco and Brill, who was studying
[429.18s -> 432.00s]  how does the performance of a certain kind of NLP system
[432.00s -> 433.92s]  scale with the amount of data?
[433.92s -> 435.96s]  And they have what looks like
[435.96s -> 437.76s]  very often a modern scaling law,
[437.76s -> 441.92s]  log axis data on the x-axis, performance on the y-axis.
[441.92s -> 444.64s]  And they're basically arguing, well, look,
[444.64s -> 447.08s]  we can get really dramatic performance improvements
[447.08s -> 448.52s]  just by scaling up data.
[448.52s -> 449.84s]  It's very predictable.
[449.84s -> 452.60s]  And maybe we should consider the trade-offs spent
[452.60s -> 455.28s]  between spending time and money on algorithm development
[455.28s -> 456.72s]  versus just collecting more data.
[456.72s -> 458.18s]  And you're like, huh, that sounds a lot like
[458.20s -> 461.26s]  what a lot of this pre-training stuff is thinking about.
[462.56s -> 465.28s]  And then finally, one of the things that I think
[465.28s -> 468.96s]  people have thought about recently and in the past is,
[468.96s -> 470.16s]  is this thing really predictable?
[470.16s -> 471.84s]  What are the right functional forms?
[471.84s -> 474.48s]  And as early as like 2012,
[474.48s -> 476.32s]  people were really thinking about, all right,
[476.32s -> 478.32s]  are these things actually predictable?
[478.32s -> 481.24s]  Is power law, for example, power three and power four,
[481.24s -> 483.32s]  are those really the right functional forms
[483.32s -> 486.40s]  for predicting the behavior of models?
[486.40s -> 489.54s]  And of course, all of this, just to remind you,
[489.54s -> 492.46s]  is thinking about the behavior of models on the y-axis,
[492.46s -> 495.48s]  the capabilities, as a function of the amount of data
[495.48s -> 497.38s]  that you have on the x-axis.
[497.38s -> 498.74s]  So that's the relationship that I think
[498.74s -> 500.12s]  has been really classically studied,
[500.12s -> 502.92s]  what you might call data scaling in all these cases.
[503.90s -> 506.66s]  And if you're interested in kind of the earliest
[506.66s -> 509.60s]  large-scale neural scaling law paper,
[509.60s -> 512.94s]  that would probably be Hessnitz et al. in 2017.
[512.94s -> 516.18s]  I believe they were at Baidu when they did this work.
[516.18s -> 519.24s]  They showed that for a range of tasks,
[519.24s -> 522.56s]  machine translation, speech,
[522.56s -> 524.90s]  and I think like some vision tasks,
[524.90s -> 526.64s]  they showed that essentially error rates
[526.64s -> 528.40s]  fall as a power law.
[528.40s -> 529.92s]  And they even have this nice plot
[529.92s -> 531.68s]  that I really like to refer to
[531.68s -> 533.50s]  when people are discussing scaling laws,
[533.50s -> 535.10s]  that really your expectation should be
[535.10s -> 537.10s]  that there's three different regions
[537.10s -> 538.60s]  in the behavior of a model, right?
[538.60s -> 540.80s]  Initially, you start out at best guess,
[540.80s -> 542.36s]  you then enter into a region
[542.36s -> 544.64s]  where you're kind of predictably scaling the model,
[544.64s -> 545.96s]  that's the power law region,
[546.58s -> 547.82s]  and then there's another asymptotic region
[547.82s -> 548.98s]  where you're approaching essentially
[548.98s -> 551.54s]  the irreducible error of your model class.
[552.40s -> 555.62s]  And I'll kind of highlight that I think
[555.62s -> 557.18s]  there's been, in the last few years,
[557.18s -> 559.46s]  a lot of talk of new phenomena,
[559.46s -> 561.78s]  things like, oh, emerging capabilities,
[561.78s -> 564.02s]  or like scaling compute being a new thing,
[564.02s -> 567.18s]  or systems being really important.
[567.18s -> 570.14s]  But had you been reading sort of Hessnitz in 2017,
[570.14s -> 571.42s]  carefully, you would have seen
[571.42s -> 573.90s]  essentially all of these things.
[573.92s -> 577.40s]  They say, actually, it's really hard to do predictions
[577.40s -> 580.12s]  by scaling law when models are at random performance,
[580.12s -> 583.12s]  because suddenly you can leave the random region.
[583.12s -> 584.96s]  They talk about computational limits.
[584.96s -> 586.96s]  Actually, if we can scale,
[586.96s -> 589.96s]  it means actually scaling by compute is really important.
[589.96s -> 592.08s]  And then finally, they even say things like,
[592.08s -> 594.24s]  maybe we should do things like quantization,
[594.24s -> 596.04s]  because if we have predictable scaling,
[596.04s -> 597.12s]  then that means we should be willing
[597.12s -> 599.64s]  to pay for model accuracy with compute, right?
[599.64s -> 601.84s]  These are all very, very modern ideas
[601.86s -> 604.26s]  that I think a lot of the early scaling law papers,
[604.26s -> 607.14s]  I think, kind of understood fairly intuitively,
[607.14s -> 608.90s]  because once you see these plots,
[608.90s -> 610.42s]  you kind of see that actually,
[610.42s -> 612.26s]  with predictable resource investment,
[612.26s -> 614.38s]  you get predictable capabilities improvements, right?
[614.38s -> 618.46s]  So that's in some sense the core,
[618.46s -> 620.58s]  not quite history, but I think context
[621.58s -> 624.22s]  that has really shaped scaling laws.
[625.06s -> 627.14s]  All right, any questions so far on kind of the context?
[627.14s -> 629.58s]  This is mainly just kind of data scaling,
[629.58s -> 632.04s]  but I wanted to make sure we go over it carefully.
[634.52s -> 635.36s]  Yes?
[636.28s -> 639.56s]  Like, it's pretty natural for scaling.
[639.56s -> 641.84s]  I was wondering, is there cases
[641.84s -> 644.84s]  where there isn't scaling where that doesn't get better?
[644.84s -> 646.92s]  Yeah, so the question was,
[646.92s -> 650.48s]  it's natural, or maybe arguably natural,
[650.48s -> 651.32s]  to expect scaling.
[651.32s -> 652.98s]  Are there cases where we don't get scaling
[652.98s -> 654.84s]  or we get different kinds of scaling?
[655.86s -> 658.34s]  And I think one way of thinking about this is,
[658.36s -> 660.48s]  if you're measuring kind of training laws,
[660.48s -> 663.16s]  or like, you know, held out versions of training laws,
[663.16s -> 664.92s]  then I think scaling is very natural, right?
[664.92s -> 667.64s]  Like all of classical, statistical theory says,
[667.64s -> 668.88s]  you know, things should converge,
[668.88s -> 671.00s]  and when they converge, eventually they will get better,
[671.00s -> 674.08s]  right, at some sort of very asymptotic sense.
[674.08s -> 676.92s]  But we do see non-scaling behavior.
[676.92s -> 680.00s]  There was a really interesting competition a few years back
[681.28s -> 683.40s]  called like the Inverse Scaling Prize,
[683.40s -> 684.52s]  where they were looking for things
[684.52s -> 687.20s]  that like scale inversely as models got better.
[688.02s -> 689.88s]  And a lot of these are very niche things,
[689.88s -> 692.10s]  like, you know, models tend to copy better,
[692.10s -> 694.14s]  and so if you wanna like suppress copying behavior,
[694.14s -> 695.14s]  it becomes really hard
[695.14s -> 697.78s]  for really strong models, for example.
[697.78s -> 699.42s]  But I think one sort of like thing
[699.42s -> 702.02s]  that ties a lot of that together is, you know,
[702.02s -> 704.14s]  if you go really far out of distribution,
[704.14s -> 706.84s]  where the behavior is not well-specified by the data,
[706.84s -> 708.34s]  then you can get all sorts of behaviors,
[708.34s -> 709.30s]  like no scaling at all,
[709.30s -> 711.06s]  or inverse scaling, or what have you, right?
[711.06s -> 712.74s]  So in some sense, you can think of this
[712.74s -> 714.04s]  as like the extension of the classic
[714.04s -> 716.10s]  like deep learning robustness problems.
[717.20s -> 720.62s]  Cool, okay.
[720.62s -> 725.14s]  So now I'm gonna talk about scaling behaviors of LLMs,
[725.14s -> 726.90s]  like just essentially going through
[726.90s -> 728.94s]  several kinds of empirical results.
[728.94s -> 732.18s]  I'm gonna walk you through data scaling in particular
[732.18s -> 734.26s]  and some examples just to convince you
[734.26s -> 736.38s]  that this is a very natural object to expect.
[736.38s -> 738.26s]  And then we'll talk about model size,
[738.26s -> 740.58s]  which is a different kind of a thing.
[740.58s -> 745.58s]  So, scaling laws, I think, are fairly well-established,
[748.40s -> 750.40s]  and they seem to appear very, very often
[750.40s -> 752.40s]  in kind of many variables, right?
[752.40s -> 756.16s]  You see scaling in compute on the x-axis.
[756.16s -> 758.44s]  These are all taken from Kaplan's scaling law paper,
[758.44s -> 760.60s]  which I'll refer to extensively in this lecture.
[760.60s -> 762.92s]  So the x-axis here is log compute.
[762.92s -> 765.56s]  Y-axis here is log test loss.
[765.56s -> 768.10s]  And on the right, you see similar kinds of scaling,
[768.10s -> 769.28s]  both for data set size,
[769.30s -> 772.14s]  so this is the amount of data and parameters.
[772.14s -> 774.18s]  One subtlety I'll mention here as I sort of
[774.18s -> 776.48s]  talk through this is when we scale things
[776.48s -> 778.00s]  like data set size or parameters,
[778.00s -> 780.36s]  we're always assuming that the other variable,
[780.36s -> 782.42s]  in this case, if you're scaling data set size,
[782.42s -> 784.54s]  the model size is much, much, much bigger
[784.54s -> 786.50s]  than you can saturate with the data set size, right?
[786.50s -> 788.58s]  Because obviously, if you have way more data
[788.58s -> 790.72s]  than parameters, eventually you're gonna
[790.72s -> 791.94s]  sort of asymptote out, right?
[791.94s -> 793.56s]  So in all of these, we're trying to avoid
[793.56s -> 794.82s]  the asymptotic regime.
[796.06s -> 798.30s]  They hold in also pretty non-standard settings.
[798.32s -> 800.40s]  They'll hold for downstream tasks,
[800.40s -> 801.76s]  they'll hold out of distribution,
[801.76s -> 805.40s]  which is what's being shown here from the Kaplan paper.
[805.40s -> 808.96s]  And so, in some ways, power law relationships
[808.96s -> 813.28s]  seem to appear more often than we might initially expect,
[813.28s -> 816.28s]  especially for these OOD or other variables.
[817.12s -> 819.76s]  So I wanna talk through data scaling laws first
[819.76s -> 821.24s]  because I think they're the most intuitive.
[821.24s -> 822.08s]  Like, at the very least,
[822.08s -> 824.96s]  I think the theory for that is fairly clear.
[824.96s -> 826.72s]  And so, to be precise,
[826.72s -> 828.82s]  when I say something like data scaling,
[828.82s -> 831.60s]  what I mean is just some sort of simple formula
[831.60s -> 835.38s]  that maps data set size, which I'm gonna refer to as n,
[835.38s -> 837.74s]  to our excess error, right?
[837.74s -> 840.92s]  Excess error is the error beyond the irreducible regime.
[841.90s -> 846.50s]  And if you recall that figure I referred to in Hestness,
[846.50s -> 847.84s]  what we are gonna expect is
[847.84s -> 850.18s]  monotonic, logistic-looking curves.
[850.18s -> 853.30s]  And really, our interest is primarily going to be
[853.30s -> 856.06s]  in the power law region to the irreducible error region.
[856.20s -> 858.20s]  Of course, it's very interesting to also ask questions
[858.20s -> 860.36s]  about what happens in the small data regions
[860.36s -> 862.50s]  as we leave random guessing.
[862.50s -> 864.68s]  But that's much, much harder to reason about,
[864.68s -> 866.84s]  whereas I think this right tail, actually,
[866.84s -> 869.72s]  I can hopefully convince you that this part
[869.72s -> 871.90s]  is actually a very, very natural thing
[871.90s -> 873.40s]  to expect power law scaling.
[875.44s -> 877.66s]  So, okay, right.
[877.66s -> 880.36s]  So the first empirical observation that we have, right,
[880.36s -> 881.44s]  and this is kind of the thing
[881.44s -> 882.92s]  that I'm gonna convince you is natural,
[882.92s -> 886.74s]  is when we plot on the x-axis data set size
[886.74s -> 889.22s]  and on the y-axis test loss,
[889.22s -> 892.86s]  then on the log-log plot, model performance is linear.
[893.74s -> 895.46s]  You might call this scale-free,
[895.46s -> 896.66s]  or you might call it power law.
[896.66s -> 901.66s]  These are more sort of physics-oriented terminology.
[902.86s -> 906.58s]  And sort of this was established by many people,
[906.58s -> 908.50s]  but you might refer to Kaplan
[908.50s -> 911.58s]  to see many examples of this.
[911.58s -> 914.76s]  So I think, as sort of the previous question
[914.76s -> 915.80s]  sort of brought up, right,
[915.80s -> 917.64s]  we kind of expect error to be monotone.
[917.64s -> 921.28s]  We train on more data, error goes down, fairly obvious.
[921.28s -> 922.68s]  The part that is less obvious
[922.68s -> 925.62s]  is the precise functional form of this scaling, right?
[925.62s -> 927.28s]  So when I say it's a power law,
[927.28s -> 929.72s]  it's linear in log-log space.
[929.72s -> 932.00s]  And then, so what is the implication of that, right?
[932.00s -> 934.12s]  If something is linear in log-log,
[934.12s -> 936.74s]  that means that there's a polynomial relationship
[936.74s -> 940.52s]  between your x-axis and your y-axis, right?
[940.52s -> 943.16s]  And why is polynomial decay natural?
[943.16s -> 945.14s]  Well, I'm gonna walk you through two examples,
[945.14s -> 946.62s]  and both of those are gonna result
[946.62s -> 948.76s]  in some fairly natural polynomial decay.
[949.66s -> 952.26s]  I'm gonna start with the simplest possible example,
[952.26s -> 955.70s]  right, like this is just gonna be even stats 101
[955.70s -> 958.34s]  rather than machine learning 101.
[958.34s -> 959.54s]  So what I wanna do
[959.54s -> 962.26s]  is I wanna estimate the mean of a data set, right?
[962.26s -> 963.58s]  And estimating the mean
[963.58s -> 966.62s]  is a task of estimating a parameter, right?
[966.62s -> 968.30s]  I can ask for what's the scaling law,
[968.30s -> 970.68s]  what's the error of my mean estimation task
[970.68s -> 972.04s]  as a function of data, right?
[972.04s -> 972.96s]  So I can write that down.
[972.96s -> 975.44s]  Well, my input comes from a Gaussian,
[975.44s -> 977.28s]  and the task is to estimate the average.
[977.28s -> 979.76s]  I've written those out in the blue box above.
[979.76s -> 981.12s]  And what's the error?
[981.12s -> 983.56s]  Well, by sort of very standard arguments, right,
[983.56s -> 986.04s]  the average is gonna be also distributed as a Gaussian
[986.04s -> 987.84s]  with the standard deviation divided by n.
[987.84s -> 990.04s]  So I'm gonna get sigma squared over n
[990.04s -> 991.32s]  is my estimation error, right?
[991.32s -> 994.38s]  This is the expected squared error of my estimate.
[995.28s -> 997.68s]  And if you look at this, this is polynomial in n,
[997.70s -> 999.70s]  and just to really drive the point home,
[999.70s -> 1001.26s]  you take the log of both sides of this,
[1001.26s -> 1002.86s]  log of the error on the left,
[1002.86s -> 1007.58s]  and log of sort of n on the right-hand side,
[1007.58s -> 1008.70s]  I get exactly log of error
[1008.70s -> 1012.58s]  is equal to negative log n plus two log sigma, right?
[1012.58s -> 1014.46s]  So this is exactly the kind of thing we expect,
[1014.46s -> 1016.06s]  and we expect a slope of one
[1016.06s -> 1018.84s]  if we were to fit a scaling law for mean estimation.
[1020.98s -> 1024.60s]  So now, equipped with this new knowledge,
[1024.60s -> 1026.50s]  you might say, all right, I'm gonna go around
[1026.52s -> 1028.28s]  and I'm gonna look at what the rates are
[1028.28s -> 1029.80s]  for estimating different things,
[1029.80s -> 1031.92s]  and that will tell me about what I should expect
[1031.92s -> 1033.04s]  for data scaling.
[1033.04s -> 1035.64s]  And so you might say, oh, what I expect is one over n.
[1035.64s -> 1037.40s]  You might expect one over square root of n
[1037.40s -> 1039.96s]  for agnostic learning, and so on and so forth.
[1039.96s -> 1043.00s]  So we should expect to see some pretty nice round numbers
[1043.00s -> 1044.92s]  on the slope here, right, of a log-log plot.
[1044.92s -> 1047.76s]  I should expect to see like one or a .5.
[1047.76s -> 1049.60s]  What do we actually find empirically
[1049.60s -> 1052.32s]  when we look across these papers, right?
[1052.32s -> 1053.30s]  Just to sort of call them out,
[1053.30s -> 1055.48s]  in Hestness, for machine translation,
[1055.48s -> 1058.26s]  we see negative 0.13.
[1058.26s -> 1061.04s]  For speech, we see negative 0.3.
[1061.04s -> 1063.18s]  And for language modeling,
[1063.18s -> 1067.22s]  we see an exponent of negative 0.095, right?
[1067.22s -> 1070.80s]  Those are all much, much slower than the one over n
[1070.80s -> 1072.74s]  or one over square root of n rates
[1072.74s -> 1073.94s]  that you might expect
[1073.94s -> 1076.94s]  when you're just fitting simple functions.
[1076.94s -> 1079.18s]  So why might this be, okay?
[1079.18s -> 1081.42s]  This will be the last math slide of this lecture,
[1081.42s -> 1082.78s]  and then we can go to just fitting lines
[1082.78s -> 1084.86s]  on log-log plots the rest of the time.
[1084.88s -> 1087.68s]  But this will hopefully drive the point home
[1087.68s -> 1091.10s]  of why we might see these particular slopes.
[1091.10s -> 1092.40s]  So we know that neural nets
[1092.40s -> 1094.04s]  aren't just estimating the mean, right?
[1094.04s -> 1095.92s]  Or it's not even fitting a linear regression, right?
[1095.92s -> 1098.20s]  They can fit arbitrary functions, right?
[1098.20s -> 1099.64s]  So let's turn that into an example,
[1099.64s -> 1101.00s]  and let's work through that example.
[1101.00s -> 1104.64s]  So my input is x1 through xn.
[1104.64s -> 1105.68s]  So I have n samples,
[1105.68s -> 1108.60s]  and I'm gonna put them uniformly in the 2D unit box.
[1108.60s -> 1111.10s]  And I wanna estimate some random, not random,
[1111.10s -> 1114.12s]  some arbitrary regression function, y equals f, right?
[1114.22s -> 1116.06s]  And I'll assume f is smooth and so on
[1116.06s -> 1117.62s]  if you really wanna be precise, right?
[1117.62s -> 1120.46s]  But there's some regularity conditions here.
[1120.46s -> 1124.34s]  A simple approach to estimating a regression function f
[1124.34s -> 1127.42s]  is just to cut the 2D space up into small boxes.
[1127.42s -> 1128.72s]  And within each box,
[1128.72s -> 1131.54s]  I can measure the average of the y values, right?
[1131.54s -> 1133.74s]  Like a very simple non-parametric regressor
[1133.74s -> 1135.08s]  is to just cut the space up
[1135.08s -> 1137.34s]  and then to estimate what's gonna happen.
[1137.34s -> 1139.22s]  Now, informally, if we pick,
[1139.22s -> 1141.38s]  you know, I'm gonna have square root m boxes.
[1141.38s -> 1143.82s]  Now each box is gonna get square root of n samples,
[1144.36s -> 1146.84s]  and now my error is gonna be one over square root of n.
[1146.84s -> 1148.60s]  And if you sort of follow this logic
[1148.60s -> 1150.16s]  through to more dimensions,
[1150.16s -> 1151.60s]  you'll see that in d dimensions,
[1151.60s -> 1153.20s]  this is gonna be error is equal to n
[1153.20s -> 1154.72s]  to the negative one over d.
[1154.72s -> 1156.76s]  And then sort of my overall scaling,
[1156.76s -> 1159.32s]  if I were to take log-log plots of the whole thing,
[1159.32s -> 1161.92s]  is I expect the slope of negative one over d, right?
[1163.00s -> 1165.32s]  And so why did I walk you through this example, right?
[1165.32s -> 1166.44s]  I walked you through this example
[1166.44s -> 1168.84s]  because if you have flexible function classes,
[1168.84s -> 1171.36s]  what people call non-parametric function classes,
[1171.36s -> 1172.92s]  you expect dimension dependence,
[1172.94s -> 1176.02s]  and therefore the slope of the scaling law
[1176.02s -> 1178.26s]  to actually move sort of much more slowly.
[1178.26s -> 1180.38s]  And in some sense, the slope is telling you
[1180.38s -> 1183.70s]  almost precisely kind of the intrinsic dimensionality
[1183.70s -> 1186.46s]  or the ease of learning this task.
[1186.46s -> 1188.34s]  And people have argued this more formally
[1188.34s -> 1190.86s]  or sort of more literally.
[1190.86s -> 1192.74s]  There's been several sort of theories
[1192.74s -> 1194.66s]  slash empirical papers arguing
[1194.66s -> 1196.60s]  that really the reason why we get
[1196.60s -> 1199.42s]  these sort of exotic or non-standard rates of learning
[1199.42s -> 1201.22s]  is that it is closely connected
[1201.24s -> 1203.50s]  to the intrinsic dimensionality of the data.
[1203.50s -> 1205.52s]  And the sort of, for example, the plots
[1205.52s -> 1207.00s]  of these predictions, the dash lines,
[1207.00s -> 1210.12s]  and these purple circles are somewhat close.
[1210.12s -> 1213.12s]  Although, you don't want to read too much into this
[1213.12s -> 1215.12s]  because estimation of intrinsic dimension
[1215.12s -> 1216.80s]  is an extremely difficult problem,
[1216.80s -> 1221.44s]  and as difficult as modeling the data overall.
[1221.44s -> 1222.48s]  Okay?
[1222.48s -> 1223.76s]  Oh, yes?
[1223.76s -> 1225.36s]  I guess it's related to the point you made
[1225.36s -> 1230.36s]  with the answers, but how do you generate data
[1231.14s -> 1234.38s]  that has an underlying intrinsic dimension
[1234.38s -> 1236.54s]  at all from a simulation perspective?
[1236.54s -> 1239.54s]  Yeah, so the results here, well, if you want,
[1239.54s -> 1241.50s]  for example, to generate data,
[1241.50s -> 1242.74s]  that's actually not too hard.
[1242.74s -> 1244.42s]  You could write down a function
[1244.42s -> 1247.02s]  that takes in five variables, right?
[1247.02s -> 1248.98s]  And then that would be, as long as all five
[1248.98s -> 1251.58s]  of those variables don't cancel each other,
[1251.58s -> 1253.06s]  that's a five-dimensional surface,
[1253.06s -> 1254.10s]  and you can add a little bit of noise
[1254.10s -> 1255.46s]  and you're good to go.
[1255.46s -> 1257.34s]  The difficulty here is that they're actually
[1257.34s -> 1260.48s]  doing things like training on CIFAR,
[1260.48s -> 1263.12s]  and then they're trying to estimate
[1263.12s -> 1264.52s]  the intrinsic dimensionality of CIFAR.
[1264.52s -> 1266.32s]  That's a much harder task.
[1269.48s -> 1273.44s]  Okay, and data scaling laws are quite useful.
[1274.60s -> 1276.36s]  I was going at this from a,
[1276.36s -> 1279.12s]  let me explain to you scaling laws perspective,
[1279.12s -> 1281.36s]  but you can actually use scaling laws
[1281.36s -> 1283.16s]  to do many interesting things, right?
[1283.16s -> 1285.84s]  You can make engineering decisions of various kinds
[1285.84s -> 1286.96s]  using data scaling laws,
[1287.50s -> 1289.26s]  and people do, in fact, do this.
[1290.30s -> 1293.30s]  For example, you might say, well,
[1293.30s -> 1295.66s]  how does data set composition affect performance,
[1295.66s -> 1297.14s]  not just data set size?
[1297.14s -> 1299.96s]  Well, if you're changing the test set,
[1301.38s -> 1303.40s]  Kaplan et al. has a really nice figure showing,
[1303.40s -> 1306.26s]  actually, data composition only affects the offset,
[1306.26s -> 1307.30s]  not the slope.
[1307.30s -> 1309.34s]  And what that would mean is it says
[1309.34s -> 1311.82s]  if you want to pick a really good data set,
[1311.82s -> 1313.66s]  you don't have to necessarily train your models
[1313.66s -> 1314.66s]  at a huge scale.
[1314.66s -> 1316.96s]  You can scale them down and do your data selection
[1316.96s -> 1319.08s]  experiments on much smaller models.
[1320.08s -> 1323.32s]  And the shape of the expected,
[1323.32s -> 1324.88s]  as we mix different data,
[1324.88s -> 1327.28s]  we might expect certain kinds of shapes,
[1327.28s -> 1329.72s]  and you can use regression and other kinds of techniques
[1329.72s -> 1330.84s]  to try to figure out, for example,
[1330.84s -> 1333.44s]  optimal data mixing using scaling laws,
[1333.44s -> 1336.60s]  and people have written several papers on this topic.
[1336.60s -> 1340.80s]  Although, as with all data selection research,
[1340.82s -> 1345.18s]  a lot of this seems fairly tricky to execute reliably.
[1346.68s -> 1348.34s]  There's other also interesting questions
[1348.34s -> 1349.46s]  that you might ask, right?
[1349.46s -> 1351.78s]  There's a lot of discussion these days about,
[1351.78s -> 1354.26s]  are we running out of data, right, on the internet?
[1354.26s -> 1356.26s]  And so, once you start asking those questions,
[1356.26s -> 1358.64s]  the other interesting and important question is,
[1358.64s -> 1360.06s]  well, can we just keep training
[1360.06s -> 1361.62s]  on the same data we have?
[1361.62s -> 1363.80s]  What's the diminishing returns property of that, right?
[1363.80s -> 1366.02s]  And so, there's interesting work
[1366.02s -> 1369.12s]  extending scaling laws to multi-epoch training.
[1370.10s -> 1373.50s]  Basically arguing that there's an effective sample size,
[1373.50s -> 1375.74s]  and after about four epochs,
[1375.74s -> 1377.70s]  you have rapidly diminishing returns
[1377.70s -> 1379.58s]  as you repeat more and more data.
[1379.58s -> 1383.46s]  And by modifying the usual scaling law,
[1383.46s -> 1385.98s]  you can basically get a version
[1385.98s -> 1388.58s]  where you have amount of effective data
[1388.58s -> 1391.14s]  and unique tokens that diminish out
[1391.14s -> 1393.30s]  as you increase the amount of repetition.
[1395.70s -> 1398.30s]  Finally, I think one interesting combination
[1398.32s -> 1401.08s]  of these two ideas is if you're thinking about
[1401.08s -> 1404.88s]  sort of data selection in the large data regime.
[1404.88s -> 1406.14s]  Imagine you're going to be training
[1406.14s -> 1408.88s]  on trillions and trillions of tokens, right?
[1408.88s -> 1410.44s]  Now, what would be better?
[1410.44s -> 1412.96s]  Would it be better to repeat high quality sources,
[1412.96s -> 1416.36s]  like Wikipedia and perhaps your secret pirated books,
[1416.36s -> 1420.42s]  10 times, or would it be better to include new data?
[1420.42s -> 1422.24s]  The fact that you can either repeat data
[1422.24s -> 1424.00s]  or you can include more data
[1424.00s -> 1426.20s]  now has multiple sort of axes
[1426.20s -> 1429.50s]  on which you can sort of optimize your data mixture.
[1429.50s -> 1432.42s]  And there's also been some interesting data scaling work,
[1432.42s -> 1434.12s]  this one from CMU folks,
[1434.12s -> 1437.14s]  on essentially trading off between repeating data
[1437.14s -> 1441.30s]  versus picking lower quality data that's new, right?
[1441.30s -> 1445.18s]  And so all of this really is a really natural extension
[1445.18s -> 1447.30s]  of what I sort of already taught you,
[1447.30s -> 1449.02s]  which is if you assume
[1449.02s -> 1451.64s]  that there's a predictive power law relationship, right,
[1451.64s -> 1453.38s]  and that this power law relationship
[1453.38s -> 1455.90s]  holds sort of on a per mixture basis,
[1455.92s -> 1458.72s]  then you can fit these sort of scaling law extrapolations
[1458.72s -> 1460.60s]  and then get an estimate of how good your data
[1460.60s -> 1462.72s]  is going to be at scale, right?
[1464.88s -> 1468.00s]  So that's the starting point, which is data scaling,
[1468.00s -> 1470.36s]  right, and hopefully I've convinced you at this point
[1470.36s -> 1472.72s]  both sort of empirically and conceptually
[1472.72s -> 1475.12s]  that it's natural to have, you know,
[1475.12s -> 1478.88s]  log linear relationships between data and error.
[1479.80s -> 1482.28s]  This relationship seems to hold very robustly
[1482.28s -> 1485.84s]  across domains, across different kinds of models.
[1485.86s -> 1487.46s]  And you can kind of have a nice, clean,
[1487.46s -> 1490.38s]  theoretical understanding of what is happening here.
[1491.82s -> 1493.50s]  And once you do this, you can use this
[1493.50s -> 1494.50s]  for all sorts of purposes,
[1494.50s -> 1498.22s]  like picking optimal data mixtures or whatever else.
[1498.22s -> 1499.38s]  Okay, yes?
[1499.38s -> 1503.78s]  How is the model size picked on the data scaling process?
[1503.78s -> 1508.46s]  Yeah, so as I was kind of saying back in,
[1508.46s -> 1511.88s]  well, not this slide, but let's see, back in this slide,
[1512.42s -> 1516.34s]  when we think about kind of the data size scaling,
[1516.34s -> 1519.82s]  the model is always picked to be really, really large.
[1519.82s -> 1522.60s]  So the data is not saturating your model, right?
[1523.68s -> 1525.84s]  And you want to kind of avoid being
[1525.84s -> 1527.98s]  in this irreducible error regime.
[1527.98s -> 1530.32s]  So the model is always picked to be large enough
[1530.32s -> 1531.94s]  that you're in the power law region
[1531.94s -> 1533.74s]  whenever you're only varying data.
[1534.74s -> 1537.58s]  So is it just one model size for like all of them?
[1537.58s -> 1539.38s]  Is that one really, really big model size
[1539.38s -> 1541.96s]  for each point on a different size model?
[1541.96s -> 1544.56s]  Yeah, for example, for this plot in particular,
[1544.56s -> 1547.04s]  it's like one big model size.
[1547.04s -> 1548.16s]  When you're looking at, for example,
[1548.16s -> 1550.28s]  compute scaling on this axis,
[1550.28s -> 1552.14s]  then data and model scale jointly
[1552.14s -> 1554.34s]  at some preordained ratio.
[1556.42s -> 1557.40s]  Cool.
[1557.40s -> 1558.50s]  Any other questions?
[1559.60s -> 1561.04s]  Good, okay, excellent.
[1563.82s -> 1564.66s]  All right.
[1565.64s -> 1570.22s]  So now, I think we get to move from data scaling
[1570.22s -> 1572.70s]  to, in my opinion, slightly more mysterious
[1572.70s -> 1573.66s]  kinds of scaling.
[1575.16s -> 1578.10s]  And we're gonna talk about model scaling next.
[1579.04s -> 1581.46s]  And I think this is a more practical engineering
[1581.46s -> 1584.28s]  set of questions that we're now gonna try to answer.
[1584.28s -> 1587.10s]  So you're in charge of building and shipping
[1587.10s -> 1588.90s]  a really large language model.
[1588.90s -> 1591.42s]  And there's a lot of interesting ideas out there, right?
[1591.42s -> 1594.32s]  Like you could train the latest state space model.
[1594.84s -> 1595.82s]  You could train a transformer.
[1595.82s -> 1597.92s]  You could use Adam, you could use SGD, right?
[1597.92s -> 1599.50s]  People invent all sorts of new tricks.
[1599.50s -> 1602.34s]  Which ones are worth scaling up and which ones are not?
[1603.22s -> 1606.94s]  You could also take your limited compute resources
[1606.94s -> 1608.14s]  and spend them on different things.
[1608.14s -> 1609.62s]  You can train models for longer,
[1609.62s -> 1611.46s]  or you could train bigger models, right?
[1611.46s -> 1614.10s]  For give and flop, you can trade between these two.
[1614.98s -> 1616.40s]  And you could also do things like go
[1616.40s -> 1618.14s]  and collect more data versus get more GPUs.
[1618.14s -> 1620.62s]  There's a lot of different sort of things that you can do.
[1620.62s -> 1622.54s]  And the scaling laws allow you
[1622.54s -> 1624.32s]  to have a pretty simple procedure
[1624.32s -> 1626.36s]  to just answer all of these questions, right?
[1626.36s -> 1629.10s]  So I'll go through the classic
[1629.10s -> 1630.70s]  sort of Kaplan scaling law paper.
[1630.70s -> 1632.06s]  If you're interested in these topics,
[1632.06s -> 1633.06s]  I encourage you to read it.
[1633.06s -> 1635.34s]  It's just kind of a gold mine
[1635.34s -> 1636.94s]  of all these kinds of observations.
[1636.94s -> 1639.74s]  Some of it is old, but it's, I think,
[1639.74s -> 1641.82s]  still unmatched in the thoroughness
[1641.82s -> 1644.10s]  of all of the things that I really studied
[1644.10s -> 1646.74s]  in a fairly nice, unified setting.
[1646.74s -> 1651.00s]  So architecture-wise, you might start by asking
[1651.00s -> 1652.76s]  transformers versus LSTMs, right?
[1652.76s -> 1653.60s]  Which one's better?
[1653.60s -> 1655.36s]  Well, you know, the brute force way
[1655.36s -> 1659.16s]  might be to scale up LSTMs up to like GPT-3 level,
[1659.16s -> 1662.58s]  and then you can figure out whether it's good or not.
[1662.58s -> 1664.72s]  The scaling law way is much simpler, right?
[1664.72s -> 1667.16s]  You basically train a bunch of LSTMs and transformers
[1667.16s -> 1670.68s]  across many different compute thresholds or compute levels,
[1670.68s -> 1672.36s]  and then you kind of see what happens
[1672.36s -> 1673.42s]  as you scale them up.
[1673.42s -> 1675.62s]  And I think the trends here are fairly clear, right?
[1675.62s -> 1678.76s]  Like no matter how many layers you have on your LSTMs,
[1678.76s -> 1680.52s]  there's a pretty big gap, right?
[1680.52s -> 1682.24s]  Pretty big constant factor gap
[1682.24s -> 1684.32s]  between transformers and LSTMs, right?
[1684.32s -> 1685.98s]  And remember this is in log scale,
[1685.98s -> 1687.64s]  so this is kind of saying something like,
[1687.64s -> 1688.84s]  I don't know what the exact numbers are,
[1688.84s -> 1692.28s]  but imagine this is like 15 times less efficient, right?
[1692.28s -> 1694.44s]  Then no matter where you are on this plot,
[1694.44s -> 1697.48s]  the LSTM is, let's say, 15 times less compute efficient
[1697.48s -> 1698.56s]  than a transformer, right?
[1698.56s -> 1701.22s]  So there's a constant factor compute penalty
[1701.22s -> 1704.60s]  to using LSTMs, at least in this plot.
[1706.00s -> 1707.08s]  You could zoom out and say,
[1707.08s -> 1709.64s]  well, there's a lot more architectures,
[1709.64s -> 1713.74s]  which ones are really good and worth doing?
[1713.74s -> 1715.44s]  And sort of some of the classic papers,
[1715.44s -> 1719.10s]  this one is by E.K. and others at Google,
[1719.10s -> 1721.62s]  have done exactly this kind of scaling work
[1721.62s -> 1725.12s]  where they took a bunch of architectures on the right here
[1725.12s -> 1727.12s]  and they basically scaled them up.
[1727.12s -> 1729.00s]  So the x-axis is the amount of compute,
[1729.00s -> 1731.96s]  the red line is basically each architecture,
[1731.96s -> 1735.06s]  and the green line is the transformer baseline, right?
[1735.06s -> 1735.90s]  And they asked like,
[1735.92s -> 1738.58s]  can any of these alternative architectures match
[1738.58s -> 1742.50s]  or outscale the transformer, right?
[1743.38s -> 1744.90s]  And what do they end up?
[1744.90s -> 1746.34s]  Well, actually, the only thing
[1746.34s -> 1748.18s]  that seems like really strongly and reliably
[1748.18s -> 1751.46s]  meet the transformer is gated linear units
[1751.46s -> 1752.70s]  and mixture of experts.
[1752.70s -> 1755.22s]  And once you know it, that's exactly the kind of stuff
[1755.22s -> 1756.70s]  that people are doing today, right?
[1756.70s -> 1758.86s]  And so this is kind of the scaling law version
[1758.86s -> 1760.68s]  of that same idea of saying like,
[1760.68s -> 1762.70s]  how would you have come to the conclusion
[1762.70s -> 1765.34s]  that we should be doing switch transformers in GLU,
[1765.66s -> 1767.58s]  and for example, not the performer, right?
[1767.58s -> 1770.02s]  And the scaling law provides some clear evidence
[1770.02s -> 1772.34s]  of why you might want to do that.
[1775.12s -> 1777.98s]  Optimizer choice, I think, follows a similar thing.
[1777.98s -> 1780.10s]  This one's from Hestness.
[1780.10s -> 1781.62s]  They compare SGD and Adam.
[1781.62s -> 1783.66s]  They find, very similar to before,
[1783.66s -> 1786.30s]  this kind of constant factor gap in compute,
[1786.30s -> 1787.90s]  in this case, data set size,
[1787.90s -> 1790.46s]  but of course, that translates to compute
[1790.46s -> 1793.82s]  in the effectiveness of Adam versus SGD.
[1794.82s -> 1797.30s]  RHN, in this case, is recurrent highway nets.
[1797.30s -> 1799.48s]  You can sort of ignore the details here.
[1799.48s -> 1801.42s]  You kind of see the point of how you would do
[1801.42s -> 1804.06s]  this analysis rather than the specific results
[1804.06s -> 1805.16s]  that are shown here.
[1808.54s -> 1810.22s]  In the beginning, I also said something like,
[1810.22s -> 1812.10s]  oh, depth versus width.
[1812.10s -> 1813.88s]  What should the aspect ratios be?
[1813.88s -> 1816.98s]  That was one of the hyperparameter topics we talked about
[1816.98s -> 1819.82s]  and we see sort of similar sort of analysis,
[1819.82s -> 1821.78s]  but in scaling law form from Kaplan,
[1821.78s -> 1824.22s]  I think this one's intriguing to me, at least,
[1824.22s -> 1826.68s]  because we might think that deeper layers
[1826.68s -> 1829.80s]  get dramatically better, that there's clear separation
[1829.80s -> 1831.30s]  between the number of layers,
[1831.30s -> 1832.78s]  but we see, at least here,
[1832.78s -> 1835.70s]  that there's actually a lot of sort of slop.
[1835.70s -> 1837.12s]  One layer is really bad,
[1837.12s -> 1839.26s]  but a lot of the other sort of layer choices
[1839.26s -> 1841.10s]  sort of remain pretty stable.
[1841.10s -> 1843.02s]  And hopefully, this is reminiscent
[1843.02s -> 1844.56s]  of kind of that slide I showed
[1844.56s -> 1846.26s]  back in the architecture lecture where I said,
[1846.26s -> 1849.74s]  well, the aspect ratio, the ratio of width to depth,
[1849.74s -> 1853.90s]  roughly something like four to 16 or something
[1853.90s -> 1855.26s]  was a pretty natural number,
[1855.26s -> 1856.94s]  but there's a really wide basin
[1856.94s -> 1858.42s]  in which you're approximately optimal
[1858.42s -> 1861.38s]  and the scaling law analysis also backs that up.
[1862.24s -> 1865.18s]  One important subtlety that I do want to point out,
[1865.18s -> 1867.94s]  and this one bites people every now and then,
[1867.94s -> 1869.86s]  is that not all parameters are equal.
[1869.86s -> 1873.66s]  Often, you want to do parameter scaling analyses,
[1874.54s -> 1876.06s]  but if you were to say,
[1876.06s -> 1879.50s]  count embedding parameters as part of your model,
[1879.50s -> 1881.76s]  well, you get a pretty different scaling law.
[1881.76s -> 1883.58s]  You get this kind of weird looking thing
[1883.58s -> 1886.04s]  that slightly bends over here,
[1886.04s -> 1888.66s]  whereas if you only consider the non-embedding parameter,
[1888.66s -> 1890.42s]  you see that much cleaner result
[1890.42s -> 1891.82s]  that I showed you before.
[1891.82s -> 1893.54s]  So embedding layer parameters
[1893.54s -> 1895.22s]  don't really behave the same,
[1895.22s -> 1896.82s]  and they don't show the same kinds
[1896.82s -> 1900.14s]  of sort of log linear scaling
[1900.14s -> 1903.62s]  as the non-embedding parameters when you account for them.
[1904.94s -> 1905.98s]  And there's sort of related work
[1905.98s -> 1908.50s]  on saying not all parameters are the same
[1908.50s -> 1911.50s]  on recent papers on scaling mixtures of experts,
[1911.50s -> 1913.60s]  where they're also sort of trying to figure out
[1913.60s -> 1915.38s]  what does it mean to be a parameter
[1915.38s -> 1917.54s]  when you have such sparsely activated parameters.
[1917.54s -> 1918.98s]  And in those kinds of papers,
[1918.98s -> 1920.86s]  they sort of try to derive, essentially,
[1920.86s -> 1923.20s]  things like equivalent number of dense parameters
[1923.20s -> 1925.18s]  in order to sort of try to normalize
[1926.02s -> 1927.82s]  the number of parameters in an MOE.
[1932.30s -> 1934.10s]  I've showed you this plot earlier
[1934.10s -> 1935.34s]  in the hyperparameter selection,
[1935.34s -> 1937.98s]  but hopefully now, actually, you see the full context,
[1938.34s -> 1939.30s]  not just the original,
[1939.30s -> 1941.94s]  sort of the hyperparameter choice question.
[1943.50s -> 1945.86s]  We know that in many cases,
[1945.86s -> 1948.78s]  I'll go back, let's say, to like here,
[1948.78s -> 1951.14s]  often what we'll see is scaling log curves
[1951.14s -> 1952.58s]  that look like the following.
[1952.58s -> 1955.24s]  You'll often see that the slope of the curves
[1955.24s -> 1957.62s]  remain very similar, they're non-crossing,
[1957.62s -> 1960.22s]  and that there's sort of constant factor offsets
[1960.22s -> 1961.46s]  between these curves.
[1961.46s -> 1962.98s]  And whenever this is true,
[1962.98s -> 1965.30s]  what you can then do is you can take a slice
[1965.30s -> 1966.94s]  at a particular level of compute
[1966.94s -> 1968.74s]  or a particular set of hyperparameters
[1968.74s -> 1971.62s]  and analyze the hyperparameter trade-offs very carefully,
[1971.62s -> 1974.42s]  assuming and sort of be sort of safe
[1974.42s -> 1976.04s]  in sort of scaling that up.
[1976.04s -> 1978.86s]  And so, when you go to Kaplan's paper,
[1978.86s -> 1982.22s]  you'll see exactly these kinds of analyses being done,
[1982.22s -> 1984.10s]  especially, I think, the center one,
[1984.10s -> 1987.26s]  the aspect ratio plot is definitely worth looking at.
[1987.26s -> 1989.66s]  They're not just sort of scaling up and down models,
[1989.66s -> 1991.06s]  they're actually taking different slices,
[1991.06s -> 1992.42s]  so different sized models,
[1992.42s -> 1995.90s]  50 million, 270 million, 1.5 billion,
[1995.90s -> 1998.14s]  and they're looking at how the aspect ratio
[1998.14s -> 1999.58s]  changes the loss.
[1999.58s -> 2000.90s]  And they kind of see that, oh, actually,
[2000.90s -> 2004.18s]  the shape of the curve, not just the scaling slopes,
[2004.18s -> 2005.80s]  actually remain similar.
[2005.80s -> 2008.14s]  And this means that I can pick an aspect ratio
[2008.14s -> 2012.14s]  between 10 to 100, and anything in between
[2012.14s -> 2015.18s]  will work fine at all of these different scales.
[2015.18s -> 2017.86s]  And so this is, I think, important to think about.
[2017.86s -> 2019.90s]  I think, initially, when you're trained
[2019.90s -> 2022.46s]  in sort of deep learning model training,
[2022.46s -> 2023.94s]  you think about hyperparameter tuning,
[2023.94s -> 2025.72s]  but you want to be sort of scale aware
[2026.56s -> 2027.38s]  in how you're tuning your hyperparameters,
[2027.38s -> 2030.12s]  and that's a really big difference in mindset, I think,
[2030.12s -> 2032.52s]  between kind of the scaling loss style approach
[2032.52s -> 2033.88s]  and sort of maybe what you've been trained
[2033.88s -> 2036.12s]  or what you've, you know, naturally think about
[2036.12s -> 2038.28s]  in terms of, oh, let's just tune these models
[2038.28s -> 2039.52s]  at a small scale, right?
[2039.52s -> 2041.36s]  And so the same is being done
[2041.36s -> 2043.52s]  kind of for feed forward ratio
[2043.52s -> 2044.84s]  and for attention head dimension.
[2044.84s -> 2047.34s]  You know, you're varying various aspects of scale,
[2047.34s -> 2048.44s]  and you're trying to see
[2048.44s -> 2051.14s]  whether sort of the minima remains similar.
[2051.14s -> 2056.14s]  Okay.
[2057.38s -> 2059.98s]  Another important thing, next,
[2059.98s -> 2061.18s]  actually maybe not next lecture,
[2061.18s -> 2064.18s]  but next lecture, I'm gonna talk about
[2064.18s -> 2066.18s]  sort of practical case studies, almost,
[2066.18s -> 2068.08s]  of how people have scaled up models.
[2069.20s -> 2072.70s]  And we'll actually see that batch size and learning rate
[2072.70s -> 2074.70s]  are actually two really tricky things
[2074.70s -> 2075.98s]  that you have to deal with carefully
[2075.98s -> 2077.70s]  when you scale models up, right?
[2077.70s -> 2079.06s]  So when you scale models up,
[2079.06s -> 2080.98s]  you're gonna have to maybe think about,
[2081.70s -> 2082.54s]  the optimal learning rate
[2082.54s -> 2084.58s]  will be different across model scales.
[2084.58s -> 2085.42s]  And if you're doing that,
[2085.42s -> 2087.58s]  then actually also maybe the optimal batch size
[2087.58s -> 2089.04s]  might end up varying as well, right?
[2089.04s -> 2091.42s]  Because those two are often co-linked.
[2091.42s -> 2092.36s]  And so we need to think about
[2092.36s -> 2094.78s]  what the right way of scaling batch sizes
[2094.78s -> 2096.42s]  and how batch size interacts with scale
[2096.42s -> 2097.26s]  and also learning rates.
[2097.26s -> 2100.58s]  So I'll talk about those for the next couple slides.
[2101.50s -> 2103.18s]  So batch size from the systems lecture,
[2103.18s -> 2104.06s]  hopefully you remember,
[2104.06s -> 2106.50s]  it has diminishing returns past a certain point.
[2106.50s -> 2108.54s]  So up until a certain point,
[2109.26s -> 2112.90s]  when the batch size is smaller than the noise scale,
[2112.90s -> 2114.98s]  we're on the left hand side here,
[2114.98s -> 2117.64s]  increasing batch size is almost equivalent
[2117.64s -> 2119.04s]  to taking more gradient steps.
[2119.04s -> 2121.00s]  So that's roughly saying,
[2121.00s -> 2122.68s]  if I double my batch size,
[2122.68s -> 2125.52s]  it's as good as taking two gradient steps.
[2125.52s -> 2127.34s]  And that's a really, really good place to be, right?
[2127.34s -> 2129.70s]  Because now you've got the systems power
[2129.70s -> 2132.34s]  of being able to paralyze across the batch, right?
[2132.34s -> 2135.74s]  While having the optimization efficiency of taking two steps.
[2135.74s -> 2137.10s]  But past a certain point,
[2137.10s -> 2139.26s]  you're going to have ineffective scaling, right?
[2139.26s -> 2141.22s]  Where now your sort of noise scale
[2141.22s -> 2142.62s]  and your batch size are the same,
[2142.62s -> 2146.30s]  and the additional samples in your batch that you're taking,
[2146.30s -> 2148.78s]  they're not reducing useful noise,
[2148.78s -> 2151.08s]  it's getting dominated by kind of the curvature
[2151.08s -> 2153.26s]  of the bias term, so to speak,
[2153.26s -> 2157.62s]  of the curvature of your optimization landscape.
[2157.62s -> 2160.14s]  And one really useful thing to think about,
[2160.14s -> 2162.96s]  useful sort of analysis object,
[2162.96s -> 2165.14s]  is this notion of a critical batch size.
[2165.18s -> 2167.34s]  And the critical batch size you can think of
[2167.34s -> 2168.98s]  is kind of this threshold point
[2168.98s -> 2170.78s]  where we go from perfect scaling
[2170.78s -> 2173.86s]  to strong diminishing returns, right?
[2173.86s -> 2175.82s]  And you can sort of analyze this in theory
[2175.82s -> 2177.56s]  and sort of open AI papers
[2177.56s -> 2179.82s]  on critical batch sizes do this,
[2179.82s -> 2183.18s]  but you could also analyze this empirically.
[2183.18s -> 2185.54s]  And this is another thing that's been studied
[2185.54s -> 2188.96s]  sort of in the scaling law kind of way.
[2188.96s -> 2190.10s]  You can kind of see,
[2191.14s -> 2194.58s]  you can estimate the point at which sort of progress flows.
[2194.86s -> 2196.02s]  So you can estimate empirically
[2196.02s -> 2199.06s]  what the critical batch size point trade-off points are.
[2199.06s -> 2203.34s]  And you can also basically train bigger and better models.
[2203.34s -> 2205.18s]  And one really interesting thing is,
[2205.18s -> 2207.62s]  as you try to improve the loss,
[2207.62s -> 2209.34s]  so you're going left side here,
[2209.34s -> 2210.82s]  so you're making losses better and better
[2210.82s -> 2212.64s]  and better and better and better,
[2212.64s -> 2215.66s]  your critical batch size ends up getting smaller, right?
[2215.66s -> 2217.56s]  So the smaller the loss target,
[2217.56s -> 2221.46s]  the bigger the overall batch size that you can be.
[2222.46s -> 2225.42s]  And so one of the things that this leads to is,
[2225.42s -> 2229.10s]  for example, if you look at the LAMA3 training report,
[2229.10s -> 2230.74s]  you'll actually see, for example,
[2230.74s -> 2232.26s]  that they'll increase the batch size
[2232.26s -> 2233.10s]  after a certain point,
[2233.10s -> 2235.48s]  or they'll do things like increase the batch size
[2235.48s -> 2236.32s]  as they train,
[2236.32s -> 2238.66s]  because as your loss target gets smaller,
[2238.66s -> 2241.68s]  your batch sizes can, in turn, get bigger.
[2244.18s -> 2247.26s]  So as we increase both compute and model size,
[2247.26s -> 2248.82s]  like what's the right thing to do,
[2248.82s -> 2250.94s]  once again, we can do kind of a scaling analysis.
[2251.34s -> 2252.74s]  This is from Kaplan.
[2252.74s -> 2254.62s]  And you can try to figure out,
[2254.62s -> 2256.82s]  as we increase the amount of compute,
[2256.82s -> 2259.76s]  what is the optimal batch size?
[2259.76s -> 2261.46s]  And what we kind of see is that,
[2262.62s -> 2265.06s]  as we increase the amount of compute,
[2265.06s -> 2268.38s]  we can actually have reasonable sort of parallelism.
[2268.38s -> 2271.18s]  The number of total steps can stay the same,
[2271.18s -> 2272.50s]  at least within this compute threshold,
[2272.50s -> 2275.38s]  the number of total steps can stay the same
[2275.38s -> 2276.98s]  while sort of getting the batches
[2276.98s -> 2277.82s]  bigger and bigger and bigger.
[2277.82s -> 2279.38s]  And if you fix the amount of batches,
[2279.38s -> 2280.74s]  of course, the number of steps
[2280.74s -> 2281.80s]  is gonna go up and up and up.
[2281.80s -> 2283.30s]  So this is good news, hopefully,
[2283.30s -> 2284.86s]  for data parallel processing.
[2286.14s -> 2287.74s]  So that's the batch size story.
[2287.74s -> 2289.74s]  The thing you should maybe remember,
[2289.74s -> 2291.38s]  because I think critical batch sizes
[2291.38s -> 2293.00s]  are kind of a messy concept,
[2293.00s -> 2296.34s]  is that A, there's a sort of diminishing returns point,
[2296.34s -> 2298.34s]  the critical batch size, that's one thing.
[2298.34s -> 2300.34s]  The second one is that it does seem to follow
[2300.34s -> 2301.62s]  a pretty predictable scaling,
[2301.62s -> 2304.06s]  often as a function of your target loss.
[2304.06s -> 2306.38s]  And given that, you can figure out
[2306.38s -> 2309.10s]  what is the right trade-offs that I can make
[2309.68s -> 2310.76s]  in terms of systems efficiency
[2310.76s -> 2312.70s]  and my optimization progress.
[2315.10s -> 2318.82s]  As I said before, the other aspect of this
[2318.82s -> 2320.98s]  is you've got your batch size
[2320.98s -> 2322.14s]  and then you've got your learning rate.
[2322.14s -> 2325.34s]  And those two are fairly closely linked with each other.
[2325.34s -> 2327.54s]  And I'm gonna talk about mu p
[2327.54s -> 2329.34s]  at much more extensive length
[2329.34s -> 2332.14s]  in the next part of the scaling lecture.
[2332.14s -> 2334.54s]  But this is kind of a really important,
[2334.54s -> 2335.82s]  I think, broader idea.
[2335.82s -> 2337.10s]  So you could do one of two things,
[2337.10s -> 2338.48s]  and I think this figure will allow me
[2338.72s -> 2339.56s]  to talk about both of these.
[2339.56s -> 2342.44s]  So let's look at this left plot first,
[2342.44s -> 2344.32s]  what's labeled standard practice.
[2344.32s -> 2346.44s]  So when you train a transformer,
[2346.44s -> 2347.52s]  what you're basically gonna see
[2347.52s -> 2349.00s]  is something like this left thing here,
[2349.00s -> 2350.28s]  this standard practice.
[2351.44s -> 2353.24s]  So the optimal learning rate
[2353.24s -> 2355.12s]  is gonna be at different points.
[2355.12s -> 2356.64s]  And the wider the model,
[2356.64s -> 2358.20s]  as you increase your model size
[2358.20s -> 2360.90s]  and your MLPs get wider and wider and wider,
[2360.90s -> 2363.56s]  the optimal learning rate is gonna be pretty small.
[2363.56s -> 2365.28s]  And as you make your model smaller
[2365.28s -> 2366.52s]  and smaller and smaller,
[2366.52s -> 2368.08s]  your losses, of course, are gonna go up
[2368.64s -> 2370.76s]  because your model is less expressive.
[2370.76s -> 2372.76s]  But also the optimal learning rate
[2372.76s -> 2374.48s]  is gonna also go up, right?
[2375.36s -> 2377.88s]  And often people say there's a rule of thumb,
[2377.88s -> 2379.32s]  it's like one over the width
[2379.32s -> 2380.44s]  is the right rate at which
[2380.44s -> 2382.76s]  you should scale the learning rate.
[2382.76s -> 2385.48s]  More advanced people will actually fit,
[2385.48s -> 2386.88s]  basically take these curves,
[2386.88s -> 2389.56s]  find the minimum, and then fit a scaling law
[2389.56s -> 2390.88s]  on the optimal learning rate.
[2390.88s -> 2392.32s]  And so there, we can see that this is
[2392.32s -> 2394.10s]  a predictable decay in learning rate,
[2394.10s -> 2395.44s]  and maybe we can fit a scaling law.
[2395.48s -> 2396.68s]  And I'll talk about this more
[2396.68s -> 2398.12s]  in the next set of lectures.
[2399.20s -> 2401.22s]  But an alternative, one that I think
[2401.22s -> 2403.04s]  many people have started to adopt,
[2403.04s -> 2404.84s]  and I think is a really interesting thing
[2404.84s -> 2406.88s]  to think about, is that you can
[2406.88s -> 2409.76s]  actually re-parametrize the model.
[2409.76s -> 2411.84s]  And in particular, you can do things
[2411.84s -> 2416.00s]  like scale the learning rates
[2416.00s -> 2418.72s]  of different layers based on the width.
[2418.72s -> 2421.68s]  You can scale the variance of the initialization
[2421.68s -> 2424.04s]  based on the width of the model,
[2424.04s -> 2427.00s]  as well as multiply the output in the forward path
[2427.00s -> 2428.66s]  of different layers of the model.
[2430.04s -> 2432.40s]  And if you do this in a way that
[2432.40s -> 2435.76s]  is dependent on sort of the width of the model,
[2435.76s -> 2438.92s]  you end up with a parameterization of the model
[2438.92s -> 2442.32s]  whose learning rate is supposed to be more stable,
[2442.32s -> 2444.04s]  or at least in the original paper,
[2444.04s -> 2446.64s]  exactly stable across scale.
[2446.64s -> 2448.66s]  So you tune your learning rate once,
[2448.66s -> 2450.20s]  and you don't have to do anything else.
[2450.20s -> 2451.88s]  That optimum directly transfers,
[2451.88s -> 2453.92s]  actually you tune it here on the smallest one,
[2454.76s -> 2457.36s]  and that directly transfers to the very largest scale.
[2457.36s -> 2459.42s]  And this is the idea called NuP.
[2460.32s -> 2462.92s]  There have been sort of, this original paper
[2462.92s -> 2464.84s]  that I'm showing you is called With NuP.
[2464.84s -> 2466.62s]  There's been other variants.
[2466.62s -> 2468.36s]  Meta, with the release of LLama4,
[2468.36s -> 2471.18s]  claims to have invented something called MetaP,
[2471.18s -> 2473.84s]  which I'm not quite sure what it is yet.
[2473.84s -> 2476.04s]  But you can sort of see that a lot of labs
[2476.04s -> 2477.56s]  are kind of thinking about this, right?
[2477.56s -> 2479.48s]  Because if you're gonna have to
[2479.48s -> 2482.28s]  rely on predicting what the optimum learning rate is,
[2482.28s -> 2484.66s]  then you have to do all sorts of tricky scaling law fits,
[2484.66s -> 2486.44s]  and maybe this is very unstable.
[2486.44s -> 2488.48s]  But if you can re-parametrize your model,
[2488.48s -> 2490.40s]  then well, maybe you don't have to do
[2490.40s -> 2491.92s]  any sort of re-tuning at all.
[2491.92s -> 2493.80s]  Of course, that's way more optimistic
[2493.80s -> 2495.44s]  than what happens in practice,
[2495.44s -> 2497.36s]  but hopefully this gives you a sense of
[2497.36s -> 2499.94s]  why this is really cool and really interesting, right?
[2499.94s -> 2501.76s]  Scale-aware initializations.
[2503.92s -> 2504.96s]  Cool.
[2504.96s -> 2506.16s]  Any questions up until this point?
[2506.16s -> 2507.70s]  I feel like I've sort of gone through
[2507.70s -> 2511.60s]  a whole bunch of scaling architecture and parameter stuff,
[2511.76s -> 2514.12s]  so maybe I'll stop for a moment here
[2514.12s -> 2516.28s]  in case anyone has any questions.
[2516.28s -> 2517.12s]  Yeah.
[2517.12s -> 2519.20s]  I really get the intuition behind
[2519.20s -> 2521.88s]  like if we want a lower loss target,
[2521.88s -> 2524.08s]  we want to increase the batch size.
[2524.08s -> 2526.40s]  I understand that.
[2526.40s -> 2529.62s]  Yeah, so when you have a lower loss target,
[2532.30s -> 2533.14s]  yeah.
[2533.14s -> 2537.08s]  So what you want to do, right,
[2537.08s -> 2539.32s]  is the smaller the loss target,
[2539.32s -> 2540.84s]  the kind of more sensitive things are.
[2540.88s -> 2542.70s]  And in the same way that you're going to be
[2542.70s -> 2544.76s]  lowering your learning rate,
[2544.76s -> 2546.80s]  you want to also increase your batch size
[2546.80s -> 2548.32s]  in order to de-noise.
[2548.32s -> 2551.04s]  The more sensitive the target that you have,
[2551.04s -> 2552.68s]  the sort of more precise your gradients
[2552.68s -> 2554.08s]  potentially have to be.
[2554.08s -> 2556.08s]  One way of thinking about it is
[2556.08s -> 2558.76s]  as you're cooling down and your learning rate is going up,
[2558.76s -> 2560.48s]  maybe your batch size should increase as well
[2560.48s -> 2562.38s]  because the learning rate and batch sizes
[2562.38s -> 2565.08s]  sort of affect each other inversely.
[2565.08s -> 2566.28s]  Yeah.
[2566.28s -> 2571.28s]  I'm not sure there is a sort of related opening
[2578.00s -> 2580.32s]  I scaling paper for sort of multimodal models,
[2580.32s -> 2582.16s]  but I'm not, I don't remember what that says
[2582.16s -> 2583.96s]  about critical batch size for those.
[2583.96s -> 2584.80s]  Yeah.
[2585.72s -> 2586.56s]  Yes.
[2588.56s -> 2589.40s]  The noise scale?
[2589.40s -> 2590.56s]  Yeah.
[2590.56s -> 2592.48s]  The noise scale, at least in sort of this figure
[2592.48s -> 2593.84s]  if that's what you're asking about,
[2593.84s -> 2596.12s]  this is a kind of theoretical analysis.
[2597.00s -> 2598.80s]  It's basically about the gradient noise that you expect
[2598.80s -> 2601.06s]  from random sampling within the batch.
[2601.06s -> 2604.04s]  So this is not like a precisely,
[2604.04s -> 2606.40s]  empirically measured quantity of these things.
[2609.56s -> 2610.40s]  Okay.
[2613.54s -> 2614.44s]  All right.
[2614.44s -> 2616.92s]  So one thing I'll caution,
[2616.92s -> 2618.28s]  and I think this is a big caution
[2618.28s -> 2620.20s]  for a lot of scaling law works,
[2620.20s -> 2622.44s]  is that scaling laws are very nicely behaved
[2622.44s -> 2624.32s]  for log losses, right?
[2624.32s -> 2628.76s]  So we train on next token prediction cross entropies.
[2628.76s -> 2632.30s]  When your scaling law targets are those cross entropies,
[2632.30s -> 2634.14s]  very easy works very well.
[2635.14s -> 2638.16s]  But if you're trying to do downstream tasks, right,
[2638.16s -> 2640.64s]  you're trying to like directly scale on benchmarks,
[2640.64s -> 2642.68s]  behavior is much less predictable.
[2642.68s -> 2644.48s]  So here on the left side,
[2644.48s -> 2648.12s]  this is from Yike's paper comparing lots of different
[2648.12s -> 2650.84s]  sort of like hyper parameters and architectures.
[2650.84s -> 2652.44s]  You see that the number of parameters,
[2652.44s -> 2654.58s]  which in this case is a surrogate for compute,
[2654.58s -> 2656.68s]  and the negative log perplexity
[2656.68s -> 2659.16s]  is very nicely linearly correlated.
[2659.16s -> 2660.80s]  And what this is basically saying is,
[2660.80s -> 2663.60s]  well, it doesn't matter what your like depth or width
[2663.60s -> 2666.60s]  or like precise setting of the hyper parameters are,
[2666.60s -> 2667.92s]  the only thing that really matters
[2667.92s -> 2669.68s]  is your total compute expenditure, right?
[2669.68s -> 2672.12s]  This is a very simple and nice story.
[2672.12s -> 2673.80s]  But then you take these models,
[2673.80s -> 2675.16s]  this was back in 2023,
[2675.16s -> 2678.60s]  so people were still kind of doing super glue accuracy.
[2679.64s -> 2681.22s]  And you basically say like,
[2681.26s -> 2683.82s]  okay, but what's the downstream performance of these models?
[2683.82s -> 2686.10s]  And while now, we don't see a very nice
[2686.10s -> 2687.66s]  linear relationship anymore, right?
[2687.66s -> 2689.82s]  We see this like totally different thing
[2689.82s -> 2691.70s]  where certain models are much better than others
[2691.70s -> 2694.70s]  and certain architectures are better than others.
[2694.70s -> 2696.20s]  And so you might not expect
[2696.20s -> 2698.94s]  exactly this kind of scaling property.
[2698.94s -> 2701.26s]  And we've seen variants of this story
[2701.26s -> 2703.30s]  play out in many different places.
[2703.30s -> 2706.34s]  If you follow the literature on state space models,
[2706.34s -> 2708.50s]  that's one thing that we've seen.
[2708.50s -> 2710.02s]  In state space models,
[2710.02s -> 2712.52s]  we see really nice predictable scaling
[2712.52s -> 2713.82s]  like the ones on the left,
[2713.82s -> 2716.22s]  but often for certain capabilities
[2716.22s -> 2719.36s]  like in context learning or for QA,
[2719.36s -> 2722.26s]  people have shown that these models maybe do less well.
[2722.26s -> 2726.78s]  So it's important to not take this perplexity scaling
[2726.78s -> 2728.50s]  as the same thing as downstream scaling
[2728.50s -> 2730.34s]  and you want to be a little bit cautious
[2730.34s -> 2732.84s]  whenever you're doing these kinds of analyses.
[2735.16s -> 2736.46s]  Okay.
[2736.46s -> 2739.90s]  So maybe this is not surprising to some of you,
[2740.74s -> 2744.70s]  but hopefully this is surprising and convincing,
[2744.70s -> 2746.58s]  which is that if you want to make
[2746.58s -> 2747.86s]  lots of engineering decisions
[2747.86s -> 2751.14s]  like hyperparameter choices, architecture decisions,
[2751.14s -> 2752.94s]  we can do a lot of that before training, right?
[2752.94s -> 2755.12s]  Like we can train these models at small scale
[2755.12s -> 2757.78s]  across several orders of magnitude compute
[2757.78s -> 2759.26s]  and then you scale that up
[2759.26s -> 2762.34s]  in order to try to predict the behavior of models, right?
[2762.34s -> 2765.10s]  So the scaling law-based design procedure is pretty simple.
[2765.10s -> 2766.78s]  You train a few smaller models
[2766.78s -> 2768.16s]  and these smaller models should span
[2768.16s -> 2770.14s]  a couple orders of magnitude compute.
[2770.14s -> 2772.20s]  You establish a scaling law of some kind.
[2772.20s -> 2775.16s]  So you see that at least on the models that you train
[2775.16s -> 2778.08s]  that there's a clear log-log linear relationship
[2778.08s -> 2780.00s]  and then based on this prediction,
[2780.00s -> 2782.44s]  you can set optimal hyper-impraters.
[2782.44s -> 2784.20s]  In many cases, in fact,
[2784.20s -> 2786.12s]  these scaling laws won't really vary too much.
[2786.12s -> 2787.76s]  Their slopes will actually be the same
[2787.76s -> 2790.06s]  in which case sort of the corollary to this
[2790.06s -> 2792.20s]  is you can just train a few smaller models
[2792.20s -> 2794.24s]  and the results of those small models
[2794.24s -> 2797.90s]  will transfer surprisingly well to larger models
[2797.90s -> 2799.82s]  in many of these cases, but not all of them,
[2799.82s -> 2802.58s]  learning rate being an important exception, for example.
[2803.66s -> 2805.68s]  Okay, so that's how you do things
[2805.68s -> 2808.74s]  like hyper-parameter selection and architecture selection.
[2808.74s -> 2811.26s]  Now I wanna talk about one very important use
[2811.26s -> 2812.74s]  of scaling laws, one that's had kind of
[2812.74s -> 2816.98s]  an outsized influence on how we pick sizes of models,
[2816.98s -> 2819.22s]  how we think about data efficiency
[2819.22s -> 2821.00s]  and so on of these models.
[2821.00s -> 2824.46s]  So I think back in the earlier days
[2824.46s -> 2827.34s]  when people were beginning to scale up these models,
[2827.34s -> 2829.70s]  there's a really core question that you need to ask.
[2829.70s -> 2832.82s]  Do we need more data or do we need bigger models?
[2832.82s -> 2837.82s]  In some sense, back in 2021 to 2023 or something,
[2838.94s -> 2841.12s]  data was way more abundant than compute.
[2841.12s -> 2843.58s]  So we didn't need to worry about
[2843.58s -> 2845.28s]  the total data limitations.
[2845.28s -> 2847.82s]  And so the one limiting resource is compute.
[2847.82s -> 2849.86s]  Your total number of flops for your training budget,
[2849.86s -> 2851.80s]  that's kind of the limiting resource.
[2851.80s -> 2853.58s]  And you can then spend that resource
[2853.58s -> 2854.42s]  in many different ways.
[2854.42s -> 2856.46s]  You can spend it on training on lots of data
[2856.46s -> 2858.82s]  with a small model, or you can train
[2858.82s -> 2861.58s]  one giant model on very little data.
[2861.58s -> 2864.02s]  And both of those extremes seem very wasteful.
[2864.02s -> 2866.30s]  Like if you have a teeny tiny model,
[2866.30s -> 2867.54s]  pumping in tons and tons of data
[2867.54s -> 2869.82s]  doesn't seem useful in reverse.
[2869.82s -> 2872.06s]  If you have a giant model with like 10 tokens,
[2872.06s -> 2874.36s]  also doesn't seem very useful.
[2874.36s -> 2877.70s]  And so this was sort of a core question for many people.
[2877.70s -> 2881.10s]  And so simultaneously several authors
[2881.10s -> 2885.10s]  sort of proposed sort of joint data model scaling laws
[2885.10s -> 2886.66s]  to try to answer this question.
[2886.66s -> 2888.00s]  And so what are those, right?
[2888.00s -> 2890.26s]  I've been talking about scaling laws
[2890.26s -> 2891.82s]  in essentially one variable
[2891.82s -> 2893.50s]  exclusively up until this point.
[2893.50s -> 2894.86s]  And that one variable has varied.
[2894.86s -> 2898.06s]  It has sometimes been parameters or data or compute.
[2898.06s -> 2900.94s]  But we've not looked at joint scaling, right?
[2900.94s -> 2902.86s]  And so data model scaling laws
[2902.86s -> 2904.30s]  are things that look like this.
[2904.30s -> 2906.58s]  These two sort of equations here
[2906.58s -> 2909.84s]  are both like functionally equivalent to first order
[2909.84s -> 2913.04s]  and describe the trade-off between the amount of data
[2913.04s -> 2914.94s]  and the amount of models.
[2915.78s -> 2918.24s]  So the top one from Rosenfeld is basically saying
[2918.24s -> 2920.66s]  there is a part of the error, one part of it,
[2920.66s -> 2922.70s]  that decays polynomially in data.
[2922.70s -> 2923.74s]  There's a part of the error
[2923.74s -> 2926.46s]  that decays polynomially in the model size.
[2926.46s -> 2928.24s]  And then there's an irreducible error term
[2928.24s -> 2930.70s]  that cannot be removed even if I scale
[2930.70s -> 2932.94s]  both the data size and the model to infinity, right?
[2932.94s -> 2935.02s]  Same effect with Kaplan.
[2935.02s -> 2937.78s]  But here they're sort of thinking about irreducible error
[2937.78s -> 2939.90s]  rather than reducible error.
[2939.90s -> 2941.86s]  And so there's no constant term here.
[2942.86s -> 2945.26s]  So this seems kind of arbitrary
[2945.26s -> 2949.76s]  because I don't think there's any sort of top-down reason
[2949.76s -> 2951.94s]  why this has to be the correct functional form.
[2951.94s -> 2954.22s]  But this provides surprisingly good fits
[2954.22s -> 2957.18s]  to the joint error that you see in data and model.
[2957.18s -> 2959.58s]  So this is from, I believe, Rosenfeld.
[2959.58s -> 2963.00s]  They show this nice 3D plot of this is the amount of data.
[2963.00s -> 2966.02s]  This is the amount, this is the size of the model.
[2966.02s -> 2968.10s]  And this is the loss on the y-axis.
[2968.10s -> 2970.58s]  And the surface that's being fit is their functional form.
[2970.58s -> 2972.66s]  The dots are their runs.
[2972.66s -> 2974.50s]  It might be a little hard to see from the back,
[2974.50s -> 2977.38s]  but the surface fits the dots almost exactly.
[2979.90s -> 2982.62s]  And despite the fact that this functional form
[2982.62s -> 2985.34s]  is kind of ad hoc, like it's pulled out of a hat,
[2985.34s -> 2987.40s]  it is surprisingly accurate.
[2987.40s -> 2990.06s]  This one's from Rosenfeld as well
[2990.06s -> 2993.62s]  where they basically say, okay, I'm only gonna train
[2993.62s -> 2995.58s]  on essentially the small half, right?
[2995.58s -> 2998.42s]  Models that are small and data that is small, right?
[2998.42s -> 3000.06s]  So on this sort of left bottom.
[3000.58s -> 3003.22s]  And I'm gonna extrapolate to models that are sort of
[3003.22s -> 3005.82s]  both large and trained with more models.
[3005.82s -> 3008.68s]  And how good is that fit of like joint extrapolation?
[3008.68s -> 3010.34s]  Well, quite good, right?
[3010.34s -> 3011.58s]  So if you look at the error,
[3011.58s -> 3016.58s]  my sort of real values are on the x-axis,
[3017.00s -> 3019.78s]  my predictions of the error on the y-axis,
[3019.78s -> 3021.40s]  and they're sort of almost exactly right
[3021.40s -> 3024.40s]  both on like sort of ImageNet and on Wikitex.
[3025.34s -> 3026.70s]  So this seems pretty good.
[3027.22s -> 3031.74s]  And so for a fixed compute budget,
[3031.74s -> 3032.82s]  now what can we do?
[3032.82s -> 3034.66s]  We go back to, for example, Kaplan,
[3034.66s -> 3036.90s]  we see similar things being done here.
[3036.90s -> 3040.32s]  We see sort of joint scaling of compute and data.
[3040.32s -> 3043.30s]  So in this case, parameters are on the x-axis,
[3043.30s -> 3045.46s]  the colors represent compute,
[3045.46s -> 3047.98s]  and so there's sort of a third axis of data
[3047.98s -> 3049.58s]  that's being implicitly varied
[3049.58s -> 3051.42s]  in order to vary the total amount of compute.
[3051.42s -> 3055.22s]  So as you go shift on these curves,
[3055.26s -> 3057.62s]  the parameters are being varied
[3057.62s -> 3059.46s]  while the compute is being held constant,
[3059.46s -> 3061.50s]  and so the amount of data is gonna vary.
[3065.30s -> 3068.42s]  So Chinchilla, I think many of you have hopefully heard of
[3069.30s -> 3072.70s]  is probably the reference in solving this problem, right?
[3072.70s -> 3075.82s]  So both Rosenfeld and Kaplan came up with
[3075.82s -> 3078.34s]  kind of this joint scaling functional form,
[3078.34s -> 3080.94s]  and then both of them sort of noticed
[3080.94s -> 3083.62s]  that it was possible to use these functional forms
[3083.62s -> 3085.18s]  to optimize the trade-off
[3085.18s -> 3087.86s]  between compute and data in various ways.
[3088.90s -> 3090.34s]  But for various reasons,
[3091.30s -> 3092.46s]  basically it's hard to fit
[3092.46s -> 3094.54s]  these sort of functional forms precisely,
[3094.54s -> 3098.38s]  and the details like the sort of learning great shapes
[3098.38s -> 3100.50s]  being different are important,
[3100.50s -> 3103.72s]  and so Kaplan sort of had one estimate
[3103.72s -> 3106.34s]  that was quite far off from what was later
[3106.34s -> 3108.58s]  and sometimes validated to be optimal.
[3108.58s -> 3110.66s]  And so the Chinchilla paper
[3110.66s -> 3112.42s]  by a bunch of Google authors
[3112.86s -> 3114.90s]  was an attempt to really empirically try to nail down
[3114.90s -> 3116.42s]  what is the right trade-off
[3117.34s -> 3120.42s]  between the amount of tokens and the model size
[3120.42s -> 3123.22s]  assuming that your goal is to get the best model
[3123.22s -> 3126.22s]  for the smallest amount of trading flops, right?
[3126.22s -> 3127.74s]  So they have three different approaches,
[3127.74s -> 3129.62s]  approach one, two, and three
[3129.62s -> 3131.46s]  for basically fitting different curves
[3131.46s -> 3133.08s]  and making scaling predictions.
[3133.08s -> 3136.12s]  These blue dots are the models that they trained,
[3136.12s -> 3138.26s]  and basically the lines are predicting
[3138.26s -> 3141.60s]  different optimal parameter sizes for different flops,
[3141.64s -> 3144.76s]  and hopefully most of you kind of know the Chinchilla ratio,
[3144.76s -> 3147.80s]  that's something like 20 tokens per parameter,
[3147.80s -> 3149.64s]  and that comes from exactly this, right?
[3149.64s -> 3151.20s]  Like if you take each of these points
[3151.20s -> 3152.68s]  and you multiply it by 20,
[3152.68s -> 3154.38s]  you're gonna get roughly the flop,
[3154.38s -> 3156.80s]  or sorry, multiply it by 20, you'll get the token count,
[3156.80s -> 3159.20s]  and so if you multiply the parameters by that,
[3159.20s -> 3160.52s]  you'll get the flops.
[3163.54s -> 3166.46s]  The difference between sort of the Kaplan results,
[3166.46s -> 3169.10s]  which were basically estimating one set
[3169.10s -> 3171.44s]  of token to parameter ratios,
[3172.14s -> 3173.56s]  and sort of the Chinchilla ones,
[3173.56s -> 3177.08s]  one of the reasons is because of learning rate schedules.
[3177.08s -> 3180.56s]  We know that we train models with cosine learning rates,
[3180.56s -> 3182.24s]  so cosine learning rates are gonna look
[3182.24s -> 3183.84s]  something like this, it goes up,
[3183.84s -> 3185.08s]  and then it comes back down,
[3185.08s -> 3186.60s]  and then it's gonna cool down all the way
[3186.60s -> 3189.44s]  to a minimum learning rate at your bottom.
[3189.44s -> 3192.68s]  But one thing about cosine learning rates
[3192.68s -> 3194.64s]  that sort of trips everyone up all the time
[3194.64s -> 3196.78s]  is you can't truncate them early.
[3196.78s -> 3198.24s]  For a cosine learning rate,
[3198.24s -> 3200.40s]  you have to sort of go all the way to the end
[3200.40s -> 3201.92s]  in order to get a valid model, right?
[3201.92s -> 3204.52s]  You have to get a cool down phase all the way to the end.
[3204.52s -> 3206.24s]  If I truncate a model in the middle,
[3206.24s -> 3209.24s]  this is not the same as starting a model from scratch
[3209.24s -> 3211.12s]  and training it with a cosine learning rate
[3211.12s -> 3212.80s]  somewhere in the middle.
[3212.80s -> 3215.24s]  And this was one of the sort of contributing factors,
[3215.24s -> 3217.12s]  there were others as well,
[3217.12s -> 3220.64s]  leading to the Kaplan estimates being pretty far off
[3220.64s -> 3223.80s]  from the later sort of more improved estimates
[3223.80s -> 3226.74s]  provided by the Chinchilla paper.
[3227.74s -> 3231.50s]  So, what do the Chinchilla authors actually do?
[3231.50s -> 3233.42s]  Well, they have three different methods
[3233.42s -> 3236.50s]  of trying to estimate the optimum trade-off
[3236.50s -> 3238.46s]  between tokens to models.
[3238.46s -> 3241.38s]  And each of these methods are gonna sort of provide
[3241.38s -> 3243.08s]  different scaling coefficients, right?
[3243.08s -> 3245.66s]  Scaling coefficients for the model size
[3245.66s -> 3248.46s]  and scaling coefficients for the data size.
[3248.46s -> 3250.78s]  And kind of surprisingly, in this case,
[3250.78s -> 3253.22s]  they're getting 0.5 on both of these
[3253.22s -> 3254.82s]  for methods one and two.
[3254.82s -> 3257.54s]  And method three is providing pretty different
[3257.54s -> 3258.96s]  or slightly different estimates.
[3258.96s -> 3260.64s]  They're about off by 0.03.
[3261.58s -> 3263.62s]  But we'll talk about that a little bit later.
[3263.62s -> 3266.78s]  Kaplan and all you see is way off
[3266.78s -> 3268.54s]  than any of the three estimates.
[3269.62s -> 3270.94s]  So we'll go over each of these methods.
[3270.94s -> 3272.74s]  Each of these makes sense.
[3272.74s -> 3275.44s]  They make sort of different assumptions about scaling.
[3275.44s -> 3278.62s]  But they end up with very, very similar estimates
[3278.62s -> 3279.82s]  at the very end here.
[3279.82s -> 3284.82s]  So, method one on Chinchilla is to basically
[3285.78s -> 3287.90s]  take the minimum over curves.
[3287.90s -> 3289.70s]  And so what does that mean?
[3289.70s -> 3292.50s]  Well, you basically overlay all of the different
[3292.50s -> 3293.94s]  training curves that you have.
[3293.94s -> 3298.94s]  So you can see here on the x-axis is different flops.
[3299.06s -> 3302.66s]  On the y-axis is sort of the training loss.
[3302.66s -> 3305.66s]  And I have models trained at many different sizes.
[3305.66s -> 3307.60s]  And of course, each of these sizes
[3307.60s -> 3310.36s]  are gonna be trained with different amount of tokens.
[3310.36s -> 3313.44s]  And so they're gonna reach a different sort of total flop
[3313.44s -> 3316.12s]  as they sort of go through training.
[3316.12s -> 3317.88s]  Now, what I'm going to do is I'm gonna look
[3317.88s -> 3319.36s]  at the lower envelope, right?
[3319.36s -> 3321.88s]  The set of sort of points or checkpoints
[3321.88s -> 3325.64s]  that prove to be optimal under any compute budget.
[3325.64s -> 3327.96s]  And I can take these models and I can look at,
[3327.96s -> 3331.76s]  okay, what were the actual parameter sizes of these models?
[3331.76s -> 3333.80s]  And you can see that sort of the total compute
[3333.80s -> 3336.32s]  on the x-axis here and the number of parameters
[3336.32s -> 3338.00s]  as well as the corresponding tokens
[3338.00s -> 3341.80s]  all forms a relatively nice scaling law, right?
[3341.80s -> 3344.56s]  And so this is kind of the minimum envelope method.
[3344.56s -> 3348.60s]  It's basically saying I expect the minimum training loss
[3348.60s -> 3351.12s]  where I optimize over all the model sizes
[3351.12s -> 3353.52s]  to actually be optimum in flops.
[3353.52s -> 3357.98s]  And sort of to call back to some earlier papers, right?
[3357.98s -> 3361.80s]  If you look back at the earlier sort of Kaplan paper
[3361.80s -> 3362.96s]  and other scaling laws,
[3362.96s -> 3364.96s]  you see exactly this already being done.
[3364.96s -> 3367.88s]  You see different models being trained
[3367.88s -> 3369.42s]  with different sort of parameters
[3369.42s -> 3371.20s]  and different compute scales.
[3371.20s -> 3373.72s]  And we're taking sort of the minimum across these
[3373.72s -> 3374.56s]  and we've already seen
[3374.56s -> 3376.04s]  that the minimum forms a scaling law.
[3376.04s -> 3378.26s]  So this is building on this observation
[3378.26s -> 3380.80s]  that the minimum across many different training curves
[3380.80s -> 3385.80s]  across compute should form a power line.
[3387.16s -> 3390.38s]  So under that assumption, you can get fairly nice fits.
[3390.38s -> 3392.40s]  And this gives one estimate
[3392.40s -> 3394.92s]  that is quite consistent with others.
[3394.92s -> 3396.08s]  Of .5, .5.
[3397.52s -> 3399.64s]  Now the other one, this I think,
[3399.64s -> 3402.12s]  if you were to pick a single canonical way
[3402.12s -> 3404.38s]  to do the Chinchilla analysis,
[3404.38s -> 3405.92s]  this would probably be the one.
[3405.92s -> 3407.56s]  And in some ways, I think this is the most
[3407.56s -> 3409.16s]  conceptually straightforward one,
[3410.08s -> 3412.30s]  which is the isoflop analysis.
[3412.30s -> 3414.04s]  So to do the isoflop analysis,
[3414.04s -> 3416.60s]  what you do is you pick a bunch of compute scales.
[3416.60s -> 3419.78s]  So each of these colors is a different amount of compute.
[3419.78s -> 3421.42s]  And what I'm going to do is
[3421.42s -> 3423.22s]  for each of these compute scales,
[3423.22s -> 3426.14s]  I can essentially have models with smaller parameters
[3426.14s -> 3427.34s]  trained with more data
[3427.34s -> 3429.82s]  or more parameters trained with less data.
[3429.82s -> 3432.70s]  So I'm going to sweep over my model sizes
[3432.70s -> 3434.30s]  for each of these flops.
[3434.30s -> 3437.66s]  And then I can look at the minimum of each of these curves
[3437.66s -> 3439.94s]  I can either pick the minimum point explicitly,
[3439.94s -> 3441.26s]  sort of non-parametrically,
[3441.26s -> 3443.58s]  or I could fit quadratics onto each of these
[3443.58s -> 3446.52s]  and get the minimum point of the quadratic.
[3446.52s -> 3449.10s]  But in either case, the argument is fairly simple.
[3449.10s -> 3450.94s]  The argument is, it should be the case
[3450.94s -> 3454.22s]  that this minimum itself follows a predictable scaling law.
[3454.22s -> 3456.30s]  And thus, I can extract from it
[3456.30s -> 3459.20s]  sort of the optimum sort of parameters per flop.
[3459.20s -> 3461.56s]  So that's the minimum points across all of these.
[3461.56s -> 3465.18s]  And I can also extract the optimal number of tokens
[3465.18s -> 3466.02s]  per flop.
[3466.02s -> 3467.90s]  I can read that out by sort of dividing
[3467.90s -> 3470.78s]  my flops budget by the number of parameters.
[3470.78s -> 3472.50s]  So I can get those simultaneously.
[3472.50s -> 3473.52s]  And you can see that, once again,
[3473.52s -> 3476.10s]  this gives very clean sort of results
[3476.10s -> 3478.40s]  that are consistent with method one.
[3478.40s -> 3480.46s]  So we can compare that with before this says,
[3481.02s -> 3483.38s]  for the eventual Chinchilla model budget,
[3483.38s -> 3485.92s]  you want 63 billion parameters.
[3485.92s -> 3488.22s]  This one's at 67 billion parameters.
[3488.22s -> 3489.46s]  The two are quite close.
[3492.68s -> 3494.02s]  Okay.
[3494.02s -> 3499.02s]  The last one, honestly, is just a little bit messier.
[3499.34s -> 3502.86s]  And this goes back to kind of that Rosenfeld paper.
[3502.86s -> 3506.04s]  If you have a functional form, like this one, right,
[3506.04s -> 3507.96s]  like this, from Rosenfeld,
[3507.96s -> 3509.82s]  a very natural instinct is to say,
[3509.82s -> 3511.38s]  I'm just gonna train a bunch of models
[3511.38s -> 3513.56s]  varying both N and M, right?
[3513.56s -> 3514.82s]  And I'm just gonna do curve fitting.
[3514.82s -> 3517.22s]  I'm gonna fit this curve onto whatever I get,
[3517.22s -> 3519.28s]  the thing I get out of my models, right?
[3519.28s -> 3520.70s]  So I'm gonna train a bunch of models
[3520.70s -> 3522.42s]  and fit that 3D shape.
[3522.42s -> 3523.78s]  And we know from Rosenfeld,
[3523.78s -> 3526.64s]  it's reasonable to some extent to fit these, right?
[3526.64s -> 3529.46s]  So you've got all these dots, which are the models.
[3529.46s -> 3533.00s]  I fitted a curve that's this sort of heat map color
[3533.00s -> 3534.94s]  that you see on the left.
[3534.94s -> 3537.02s]  And then you can sort of back out
[3537.02s -> 3539.06s]  what the implied ISO flop should look like
[3539.10s -> 3541.06s]  from these dashed lines.
[3541.06s -> 3543.46s]  But if you look at this, hopefully you see
[3543.46s -> 3546.42s]  that the scaling law fits and sort of the curve fits here
[3546.42s -> 3548.34s]  are just not quite as good
[3549.22s -> 3552.50s]  as the fits in the other plots, right?
[3552.50s -> 3554.82s]  And if you look at the coefficients,
[3554.82s -> 3557.26s]  the Chinchilla method three
[3557.26s -> 3559.06s]  just gives way different estimates
[3559.06s -> 3560.42s]  in terms of the model size
[3560.42s -> 3563.84s]  and total token count than the others, right?
[3563.84s -> 3566.62s]  And actually, this was a mystery to me for a long time.
[3566.62s -> 3567.70s]  I think some of my students were like,
[3567.94s -> 3569.54s]  why is method three so different?
[3569.54s -> 3570.38s]  And I said, I don't know.
[3570.38s -> 3572.58s]  Maybe scaling laws are just sometimes noisy.
[3573.64s -> 3575.02s]  I don't know how many of you know this,
[3575.02s -> 3577.18s]  but this is a really fun trivia fact,
[3577.18s -> 3580.38s]  or not trivia fact, fun piece of trivia, let's say.
[3581.30s -> 3584.70s]  So last year, some folks at Epoch AI,
[3584.70s -> 3586.70s]  I don't know what motivated them to do this,
[3586.70s -> 3589.20s]  were curious enough about this result
[3589.20s -> 3592.82s]  that they went and tried to replicate method three.
[3592.82s -> 3596.62s]  And it was very difficult to replicate it
[3596.62s -> 3598.62s]  because you don't have the original data
[3598.62s -> 3600.10s]  for all of these training runs.
[3600.10s -> 3602.62s]  So they actually went to the extreme
[3602.62s -> 3604.90s]  of actually looking at the plots
[3604.90s -> 3606.62s]  and using sort of a forensic tool
[3606.62s -> 3610.18s]  to extract the values of the points from the plots.
[3610.18s -> 3611.34s]  And based on that,
[3611.34s -> 3615.10s]  they could actually replicate the original result.
[3615.10s -> 3617.02s]  And kind of the funny thing is they showed
[3617.02s -> 3619.86s]  that actually the curve fitting was the bad part.
[3619.86s -> 3621.98s]  Like their data and their approach was good,
[3621.98s -> 3623.20s]  but actually when they fit the curve,
[3623.20s -> 3624.98s]  they didn't necessarily do it right.
[3625.06s -> 3628.26s]  And so the original fit had residuals.
[3628.26s -> 3629.78s]  If you're familiar with regression,
[3629.78s -> 3631.46s]  your residual should be zero mean centered
[3631.46s -> 3634.82s]  because otherwise you should be offsetting your predictions
[3634.82s -> 3636.42s]  to make it zero center.
[3636.42s -> 3637.76s]  Their residuals are non-zero,
[3637.76s -> 3639.18s]  and then they fit it better.
[3639.18s -> 3640.72s]  And then when they did fit it better,
[3640.72s -> 3644.70s]  well actually their optimal estimate
[3644.70s -> 3647.30s]  almost exactly matched methods one and two.
[3647.30s -> 3649.10s]  And so this is one of those funny cases
[3649.10s -> 3652.08s]  where actually the original authors
[3652.08s -> 3654.62s]  had both the idea and the data right,
[3655.14s -> 3657.06s]  but because of a minor issue in curve fitting,
[3657.06s -> 3658.34s]  they kind of had it wrong
[3658.34s -> 3660.84s]  and the replication actually makes it more correct
[3660.84s -> 3661.68s]  than before.
[3661.68s -> 3663.68s]  Usually replication sort of disprove things,
[3663.68s -> 3664.58s]  but in this case,
[3664.58s -> 3665.86s]  actually the replication just showed
[3665.86s -> 3668.36s]  that the original result was correct all along,
[3668.36s -> 3670.46s]  which is I think a pretty cool result.
[3672.30s -> 3676.18s]  Okay, so the final thing I want to talk about
[3676.18s -> 3678.22s]  with kind of this set of Chinchilla results
[3678.22s -> 3681.76s]  is we're talking about training optimal scaling.
[3681.76s -> 3683.70s]  So you have a fixed FOPS budget.
[3683.70s -> 3686.90s]  I want the best possible model possible.
[3686.90s -> 3690.10s]  But really, I think that the story has really shifted.
[3690.10s -> 3691.58s]  When sort of Chinchilla was written
[3691.58s -> 3693.22s]  and the Kaplan paper was written,
[3694.18s -> 3696.32s]  LLMs were not really a product yet.
[3696.32s -> 3698.14s]  And so really the name of the game was
[3698.14s -> 3700.38s]  everyone wanted the most biggest, flashiest,
[3700.38s -> 3701.98s]  most intelligent model,
[3701.98s -> 3703.86s]  but they didn't care about the inference cost
[3703.86s -> 3706.26s]  of actually deploying these systems.
[3706.26s -> 3709.46s]  But nowadays, what we really care about
[3709.46s -> 3711.06s]  is inference costs, right?
[3711.06s -> 3712.66s]  Because these systems are actually products,
[3712.66s -> 3713.94s]  they generate revenue,
[3713.94s -> 3716.58s]  you have a cost associated with the revenue.
[3716.58s -> 3718.24s]  And so we've seen over time
[3718.24s -> 3720.10s]  that actually the tokens per parameter
[3720.10s -> 3721.50s]  has steadily grown, right?
[3721.50s -> 3724.90s]  Like GPT-3 was two tokens per parameter.
[3724.90s -> 3727.38s]  Chinchilla moved us to 20 tokens per parameter.
[3727.38s -> 3729.54s]  And for a bit, people played around
[3729.54s -> 3731.78s]  with sort of 20 tokens per parameter stuff.
[3731.78s -> 3734.86s]  But then very quickly, people realized
[3734.86s -> 3738.22s]  actually what we care about is really good intelligence
[3738.22s -> 3739.86s]  at really small parameter sizes.
[3739.86s -> 3742.10s]  And so people have really started to scale up
[3742.48s -> 3744.66s]  the number of tokens per parameter very, very rapidly.
[3744.66s -> 3747.70s]  And I think I saw yesterday that, for example,
[3747.70s -> 3749.52s]  the most recent QEM models
[3749.52s -> 3752.22s]  were trained on 30 trillion tokens, right?
[3752.22s -> 3754.10s]  People are really pushing the limits
[3754.10s -> 3756.70s]  on the tokens to parameter ratio
[3756.70s -> 3759.62s]  because really you would much rather pay the upfront cost
[3759.62s -> 3761.98s]  than to pay the ongoing operating cost
[3761.98s -> 3766.40s]  of running inference on a really big, expensive model.
[3768.74s -> 3769.58s]  Cool.
[3770.18s -> 3774.80s]  Last thing that is kind of a fun side thing
[3774.80s -> 3776.76s]  that I want to end with is to say,
[3778.12s -> 3781.22s]  these results are pretty robust and easy to replicate.
[3781.22s -> 3783.94s]  A few years back, one of my students, Ishan,
[3783.94s -> 3786.50s]  was really interested in really pushing
[3786.50s -> 3788.62s]  diffusion models for text forward.
[3788.62s -> 3789.96s]  And so one of the things that we had to do
[3789.96s -> 3792.10s]  was to say this is a whole new kind of model.
[3792.10s -> 3793.30s]  We don't know what the optimal
[3793.30s -> 3794.56s]  token to parameter ratio is.
[3794.56s -> 3797.14s]  We don't know if this thing even reliably scales
[3797.18s -> 3800.10s]  with a totally different kind of generative model.
[3800.10s -> 3801.14s]  What do we do?
[3801.14s -> 3805.28s]  Well, turns out, if you just fit the same kind of playbook
[3805.28s -> 3808.14s]  of saying, oh, we're gonna do isoflop analyses
[3808.14s -> 3809.24s]  for autoregressive models,
[3809.24s -> 3811.06s]  we get almost exactly the chinchilla thing
[3811.06s -> 3812.86s]  without too much effort.
[3812.86s -> 3815.22s]  You do the same kind of analysis on diffusion models.
[3815.22s -> 3817.70s]  Wow, we see very similar kinds of curves
[3817.70s -> 3818.98s]  even though it's a pretty different
[3818.98s -> 3820.46s]  generative model entirely.
[3820.46s -> 3823.10s]  And then if you plot the minimum across of these,
[3823.10s -> 3825.72s]  well, you see very predictable scaling for both
[3825.72s -> 3827.84s]  separated by a constant offset.
[3827.84s -> 3829.52s]  I don't bring this up to say
[3829.52s -> 3832.76s]  because I want to particularly push to you diffusion models
[3832.76s -> 3835.92s]  but just as a really random sort of case study
[3835.92s -> 3838.44s]  or example to say these scaling laws
[3838.44s -> 3839.60s]  don't necessarily need to be these
[3839.60s -> 3841.16s]  very cherry picked examples.
[3841.16s -> 3843.60s]  They seem to happen pretty naturally
[3843.60s -> 3845.20s]  as you're sort of working on new models
[3845.20s -> 3846.80s]  or working on new environments.
[3849.48s -> 3851.04s]  So, okay.
[3851.88s -> 3856.88s]  This is to put together this last part, right?
[3857.92s -> 3860.92s]  Log linearity is not just about sort of one dimensional
[3860.92s -> 3862.56s]  things where we think about data.
[3862.56s -> 3864.16s]  They extend to sort of model parameters.
[3864.16s -> 3865.88s]  They extend to total compute.
[3865.88s -> 3868.88s]  And so that lets us make all sorts of hyperparameter
[3868.88s -> 3869.88s]  in other decisions.
[3869.88s -> 3871.92s]  That's kind of this first part.
[3871.92s -> 3873.36s]  And they're also letting us make
[3873.36s -> 3874.96s]  really smart resource trade-offs, right?
[3874.96s -> 3876.28s]  They let us make trade-offs between
[3876.28s -> 3878.22s]  sort of big models versus more data.
[3879.18s -> 3881.82s]  And we saw that in kind of this Chinchilla analysis.
[3881.82s -> 3883.30s]  And you know, it's kind of remarkable
[3883.30s -> 3887.86s]  how cleanly things like the ISO flop analysis turn out.
[3889.48s -> 3890.42s]  So, all right.
[3890.42s -> 3894.74s]  That's all I got for basic scaling laws.
[3894.74s -> 3898.90s]  We did a recap of Kaplan as well as Chinchilla today.
[3898.90s -> 3900.78s]  And hopefully now you're on board
[3900.78s -> 3903.42s]  with this idea of data scaling, model scaling
[3903.42s -> 3905.70s]  and using scaling laws to sort of optimize
[3905.70s -> 3907.14s]  all the aspects of your model
[3907.14s -> 3908.78s]  without actually going all the way
[3908.78s -> 3910.94s]  to large scale training runs.
[3910.94s -> 3912.86s]  Thanks, and I'll see you all Thursday.
