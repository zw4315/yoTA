# Detected language: en (p=1.00)

[0.00s -> 6.00s]  The nearest neighbor algorithm is arguably the simplest single object tracking algorithm.
[6.00s -> 13.00s]  In spite of its simplicity, it performs comparable to more advanced algorithms in simple scenarios.
[13.00s -> 18.00s]  In this video, we introduce this algorithm and explain some of its properties.
[18.00s -> 26.00s]  Since both nearest neighbor and probabilistic data association end every recursion by approximating the posterior density as Gaussian,
[26.00s -> 32.00s]  we always start the next recursion by assuming that the posterior at the previous time is Gaussian.
[32.00s -> 36.00s]  For simplicity, we here assume that the motion model is linear and Gaussian.
[36.00s -> 40.00s]  It therefore follows that the predicted density is also linear and Gaussian,
[40.00s -> 44.00s]  and we have closed form expressions for the first two moments of the predicted density.
[44.00s -> 50.00s]  Note that we can handle non-linear motion models by approximating the predicted density as Gaussian,
[50.00s -> 52.00s]  for instance using an extended Kalman filter.
[52.00s -> 57.00s]  We would like to be able to compare how different algorithms approximate the posterior density.
[57.00s -> 66.00s]  We therefore sometimes use the superscript nn when the approximation to the posterior density is computed using the nearest neighbor algorithm.
[66.00s -> 73.00s]  To obtain simple expressions for the posterior at time k, we assume that the probability of detection is constant,
[73.00s -> 77.00s]  and that the object likelihood gk is linear and Gaussian.
[77.00s -> 83.00s]  As before, we obtain simple expressions even if the clutter intensity lambda c is a non-linear function of z.
[83.00s -> 92.00s]  We can now express the posterior density under the assumption that the predicted density computed by the nearest neighbor algorithm is the true predicted density.
[92.00s -> 96.00s]  Note that this is the standard approach in assumed density filtering.
[96.00s -> 102.00s]  Once we have introduced the approximations, we proceed as if the approximations are the true densities.
[102.00s -> 110.00s]  Given that the predicted density is Gaussian, the posterior density is a Gaussian mixture with one term for every possible association at time k.
[110.00s -> 119.00s]  That is, it contains mk plus one terms where the weight w theta k denotes the probability of association theta k,
[119.00s -> 129.00s]  and p theta k with sub-indices k given k is the posterior density given the measurements up to time k and the association theta k.
[129.00s -> 136.00s]  Note that the equations that I'm about to present were discussed in more detail in the video about the update step for linear and Gaussian models.
[136.00s -> 146.00s]  In several of the upcoming videos, we use this breathe accent to indicate that it is the posterior density at time k before we introduce additional approximations.
[146.00s -> 154.00s]  In the nearest neighbor case, this is the Gaussian mixture posterior, assuming that the nearest neighbor predicted density is the true density.
[154.00s -> 162.00s]  The density p breathe is never actually computed by the nearest neighbor algorithm, and I've only included it here to motivate the algorithm.
[162.00s -> 167.00s]  Under the above assumptions, the density of xk given theta k is Gaussian.
[167.00s -> 172.00s]  When theta k is zero, its moments are the same as the moments of the predicted density.
[172.00s -> 179.00s]  When theta k is greater than zero, we obtain the moments of p theta k using a simple Kalman filter update,
[179.00s -> 192.00s]  assuming that z theta k is the object measurement, whereas the unnormalized weight is pd times the predicted likelihood of z theta k divided by the cluttered intensity at z theta k.
[192.00s -> 203.00s]  The bottom line with these equations is simply that if the posterior density at time k minus one is Gaussian, the posterior at time k is a Gaussian mixture with mk plus one terms.
[203.00s -> 207.00s]  This situation is true for both the nearest neighbor and the pda filters.
[207.00s -> 212.00s]  And the question is, how can we approximate this Gaussian mixture as a Gaussian density?
[212.00s -> 221.00s]  The basic idea behind the nearest neighbor algorithm is to find the most probable hypothesis and ignore all the other hypotheses.
[221.00s -> 227.00s]  That is, the posterior density is computed by assuming that the most likely hypothesis is the true hypothesis.
[227.00s -> 231.00s]  We can describe this procedure in terms of an explicit algorithm.
[231.00s -> 238.00s]  As a first step, we compute the unnormalized weights w tilde for all the hypotheses theta k.
[238.00s -> 247.00s]  We can then use these to find the most probable hypothesis by identifying the index theta for which w tilde is maximized.
[247.00s -> 257.00s]  In theory, we really want to maximize the normalized weights, but since the only difference is a proportionality constant, this maximization gives the same result.
[257.00s -> 268.00s]  Given the most probable association, we compute the posterior mean and covariance for that hypothesis and assume that these moments are the true posterior moments.
[268.00s -> 275.00s]  It may be interesting to note that we never actually need to compute the posterior moments for any of the other hypotheses.
[275.00s -> 282.00s]  Of course, once we have computed the posterior moments, we assume that the posterior density is Gaussian with those moments.
[282.00s -> 291.00s]  To visualize what these densities looked like, we return to the toy example that we first studied in the video about the conceptual solution to single object tracking.
[291.00s -> 302.00s]  Both the measurements and the state are scalar variables, we have a Gaussian prior at time one, and both pi k and g k are assumed linear and Gaussian.
[302.00s -> 309.00s]  Also, both the probability of detection and the clutter intensity are assumed constant within the area of interest.
[309.00s -> 323.00s]  The measurements at the first three time steps are identical to before, but we now consider a longer time sequence and receive two measurements at time four, one measurement at time five, and two measurements at time six.
[323.00s -> 334.00s]  As you can see, at all times we observe one measurement somewhere in an interval from 1.3 to 3, and this will be reflected in the posterior density of our state.
[334.00s -> 339.00s]  And we eventually become convinced that the state is somewhere close to 3.
[339.00s -> 345.00s]  To gain insights into the properties of the nearest neighbor algorithm, we visualize four different densities.
[345.00s -> 351.00s]  The first is the predicted density according to the nearest neighbor algorithm, illustrated in a red point dashed curve.
[351.00s -> 357.00s]  The second is the exact posterior without any approximations, which is the solid black curve.
[357.00s -> 366.00s]  Please note that it's not generally possible to compute the true posterior, but this toy example is sufficiently small and simple to enable us to do this.
[366.00s -> 371.00s]  The third density is p-brieve, which is a magenta colored dashed curve.
[371.00s -> 379.00s]  At time one, this density is identical to the exact posterior, since the predicted density is the true predicted density.
[379.00s -> 386.00s]  Finally, we also show the posterior according to the nearest neighbor algorithm, which is the green curve with squares on it.
[386.00s -> 392.00s]  Apart from these densities, the figure also shows the MK plus one hypothesis at time k.
[392.00s -> 400.00s]  We do this by plotting the MK measurements and a circle that represents the hypothesis that the object is undetected.
[400.00s -> 405.00s]  In this case, the circle is positioned at the predicted value of zk.
[405.00s -> 408.00s]  The most probable hypothesis is marked in red.
[408.00s -> 415.00s]  At time one, this is the hypothesis that the measurement at 1.7 is an object measurement.
[415.00s -> 422.00s]  The nearest neighbor algorithm therefore acts as if we knew that the measurement at 1.7 is an object measurement.
[422.00s -> 430.00s]  That is, even though all three hypotheses are reasonable, the algorithm ignores the two less probable hypotheses.
[430.00s -> 436.00s]  And this gives rise to an approximation of the posterior that underestimates the uncertainties.
[436.00s -> 443.00s]  At time two, the most likely hypothesis is that the measurement at 1.3 is an object measurement.
[443.00s -> 448.00s]  And here the nearest neighbor approximates the true posterior very well.
[448.00s -> 455.00s]  At time three, we again have two measurements, and both appear to be reasonable object measurements.
[455.00s -> 467.00s]  The true posterior is therefore bimodal, whereas the nearest neighbor algorithm approximates it as unimodal by pretending that we know that the measurement at 2.3 is an object measurement.
[467.00s -> 471.00s]  As we proceed, we obtain more measurements between 2.6 and 3.
[471.00s -> 478.00s]  And both the true posterior and the nearest neighbor algorithm become confident that the state is more or less close to 3.
[478.00s -> 484.00s]  Overall, we can see that the nearest neighbor algorithm approximates the true posterior density fairly well.
[484.00s -> 488.00s]  And it would probably also give rise to reasonable state estimates.
