# Detected language: en (p=1.00)

[0.50s -> 3.40s]  The most challenging aspect of object tracking
[3.40s -> 5.48s]  is arguably that the associations
[5.48s -> 8.78s]  between measurements and objects is unknown.
[8.78s -> 10.40s]  But what about if we assume
[10.40s -> 12.44s]  that the associations were given?
[12.44s -> 16.18s]  How can we then perform the prediction and update steps?
[16.18s -> 19.16s]  This is what we will describe in this video.
[19.16s -> 21.14s]  You might think that this is a special case
[21.14s -> 22.30s]  of little interest,
[22.30s -> 25.18s]  since the associations are generally unknown.
[25.18s -> 26.76s]  However, it turns out
[26.76s -> 29.98s]  that virtually all multi-object tracking solutions,
[29.98s -> 31.64s]  at least for point objects,
[31.64s -> 35.08s]  make use of the type of solutions that we present here.
[35.08s -> 37.02s]  And you'll see these equations repeated
[37.02s -> 39.80s]  over and over again throughout this course.
[39.80s -> 41.82s]  Let us make the simplifying assumption
[41.82s -> 43.72s]  that the data associations are known.
[43.72s -> 46.00s]  This implies that once we have observed
[46.00s -> 48.04s]  the measurement matrix at time k,
[48.04s -> 51.72s]  we know the object measurement matrix capital OK.
[51.72s -> 53.40s]  Note that we are still considering
[53.40s -> 55.94s]  single object tracking of point objects,
[55.94s -> 58.32s]  which means that the matrix OK
[58.32s -> 61.52s]  either contains zero or one measurement vector.
[61.52s -> 63.06s]  Another way to think about this
[63.06s -> 65.30s]  is that we are performing single object tracking
[65.30s -> 67.06s]  without any clutter,
[67.06s -> 69.90s]  but where the object may or may not be detected.
[69.90s -> 71.32s]  The objective in this video
[71.32s -> 74.00s]  is to show how we can recursively compute
[74.00s -> 76.34s]  the posterior density under these assumptions.
[76.34s -> 79.82s]  That is, to show how we can recursively compute
[79.82s -> 81.98s]  the posterior density of xk,
[82.02s -> 86.38s]  given the object detections up until and including time k.
[86.38s -> 89.54s]  As usual, we split these recursive computations
[89.54s -> 91.74s]  in a prediction and an update step.
[91.74s -> 93.68s]  Let us start with a prediction step,
[93.68s -> 96.02s]  which I hope you will find quite simple.
[96.02s -> 97.50s]  Given that we have a motion model,
[97.50s -> 99.26s]  how can we then perform prediction?
[99.26s -> 102.96s]  Well, as usual, we use the Chapman-Kolmogorov equation,
[102.96s -> 104.50s]  which means that we take the product
[104.50s -> 106.86s]  of the posterior at time k minus one
[106.86s -> 108.02s]  and the motion model,
[108.02s -> 111.28s]  and we then integrate out the state at time k minus one.
[111.28s -> 113.80s]  Note that the motion model has not really changed
[113.80s -> 114.72s]  compared to last week,
[114.72s -> 116.54s]  and the prediction step is also performed
[116.54s -> 118.16s]  in exactly the same way.
[118.16s -> 121.00s]  For instance, if we have linear and Gaussian models,
[121.00s -> 123.50s]  the prediction step would exactly follow the equations
[123.50s -> 124.66s]  of a Kalman filter,
[124.66s -> 126.68s]  such that we get the predicted mean
[126.68s -> 129.52s]  by taking the posterior mean at time k minus one
[129.52s -> 132.72s]  and multiply with the motion model matrix f,
[132.72s -> 134.88s]  and we get the predicted covariance matrix
[134.88s -> 137.96s]  by taking fp f transpose
[137.96s -> 140.96s]  plus the covariance of the motion noise, q.
[141.00s -> 143.16s]  The conclusion from this is that the prediction step
[143.16s -> 145.08s]  is performed in a standard manner,
[145.08s -> 146.62s]  but what about the update step?
[146.62s -> 148.64s]  The question we'd like to answer is,
[148.64s -> 151.20s]  given a measurement model of the following form,
[151.20s -> 153.58s]  how can we then perform the update step?
[153.58s -> 156.66s]  In principle, this is completely straightforward.
[156.66s -> 159.00s]  You may recall that Bayes' rule tells us
[159.00s -> 160.84s]  that the posterior is proportional
[160.84s -> 162.84s]  to the prior time set likelihood.
[162.84s -> 166.44s]  In this context, this means that the posterior at time k
[166.44s -> 169.52s]  is the predicted density, p of xk,
[169.52s -> 173.00s]  given the object detections up until time k minus one,
[173.00s -> 175.00s]  which is the prior in this context,
[175.00s -> 179.18s]  times p of capital Ok, given xk,
[179.18s -> 182.56s]  which is the likelihood or the measurement model.
[182.56s -> 185.00s]  If we plug in the expression for the measurement model,
[185.00s -> 187.48s]  we get two possible expressions.
[187.48s -> 191.08s]  The first possibility is that the matrix capital Ok
[191.08s -> 193.44s]  does not contain any measurements,
[193.44s -> 195.60s]  which means that the object was not detected
[195.60s -> 197.36s]  and the posterior is then proportional
[197.40s -> 201.44s]  to the predicted density times one minus pd of xk.
[202.32s -> 205.76s]  A second possibility is that the matrix Ok
[205.76s -> 208.24s]  contains a measurement vector Ok,
[208.24s -> 210.28s]  which is then the object detection,
[210.28s -> 212.12s]  and the posterior is then proportional
[212.12s -> 215.70s]  to the predicted density times pd of xk
[215.70s -> 218.84s]  times the measurement model for the vector Ok.
[218.84s -> 221.12s]  The fact that there are two possible cases
[221.12s -> 223.84s]  may give this expression a complicated look,
[223.84s -> 225.20s]  but note that we have assumed
[225.20s -> 227.16s]  that the matrix Ok is known,
[227.80s -> 230.56s]  and then we always know which of the two expressions to use.
[230.56s -> 232.84s]  I'd like to emphasize that this equation
[232.84s -> 235.28s]  will appear many times in this course,
[235.28s -> 237.72s]  and it is very useful if you can gain intuition
[237.72s -> 239.20s]  for it already now.
[239.20s -> 241.92s]  In fact, different versions of this equation
[241.92s -> 244.60s]  are used in all algorithms that we will learn.
[244.60s -> 246.12s]  Let us look at two examples
[246.12s -> 248.64s]  to understand these equations in more detail.
[248.64s -> 252.04s]  In many cases, pd of xk is assumed constant
[252.04s -> 253.52s]  within the area of interest,
[253.52s -> 255.72s]  and then we can remove it from these equations.
[255.72s -> 257.84s]  However, I'd like to start with an example
[257.84s -> 261.68s]  where we highlight why it cannot always be ignored.
[261.68s -> 264.96s]  To understand why the factor pd of xk is included,
[264.96s -> 267.44s]  let us consider an example specifically designed
[267.44s -> 270.32s]  to illustrate that it can be informative.
[270.32s -> 273.40s]  To get started, suppose the predicted density is Gaussian
[273.40s -> 275.82s]  with zero mean and variance one.
[275.82s -> 278.54s]  Let us also assume that the object detection Ok
[278.54s -> 281.06s]  is independent of the state xk,
[281.06s -> 284.20s]  and that it has some distribution p of Ok,
[284.20s -> 286.24s]  regardless of the value of xk.
[286.24s -> 288.28s]  This means that the vector Ok
[288.28s -> 291.78s]  does not actually carry any information about xk,
[291.78s -> 294.96s]  which is arguably an unusual but possible case.
[294.96s -> 296.74s]  To make the example even more interesting,
[296.74s -> 298.80s]  let us assume that the probability of detection
[298.80s -> 300.98s]  varies significantly in the area
[300.98s -> 302.92s]  where we believe that the object is.
[302.92s -> 305.74s]  Specifically, we'll assume that it is zero
[305.74s -> 307.60s]  whenever xk is negative,
[307.60s -> 310.22s]  and one when xk is positive,
[310.22s -> 312.98s]  such that it changes value from zero to one
[312.98s -> 315.08s]  when xk is equal to zero,
[315.08s -> 318.42s]  which happens to be the mean of our Gaussian prior.
[318.42s -> 321.18s]  Note that this implies that if we observe a detection,
[321.18s -> 323.96s]  then xk must be positive,
[323.96s -> 325.82s]  and if we don't observe a detection,
[325.82s -> 327.96s]  xk must be negative.
[327.96s -> 330.06s]  Let us now look at the expression for the posterior
[330.06s -> 331.46s]  and check if it makes sense.
[331.46s -> 333.30s]  The general expression for the posterior
[333.30s -> 336.02s]  that we saw on the previous slide is as follows.
[336.02s -> 338.26s]  Before we discuss the details of this equation,
[338.26s -> 340.94s]  I'd like to point out that the measurement likelihood
[340.94s -> 344.50s]  for the vector ok does not depend on xk in this example,
[344.50s -> 347.70s]  which means that this factor can be crossed out and removed.
[347.70s -> 350.66s]  That is, under the assumption that the matrix ok
[350.66s -> 352.20s]  contains a vector ok,
[352.20s -> 353.94s]  the posterior is simply proportional
[353.94s -> 357.10s]  to the predicted density times pd of xk.
[357.10s -> 359.22s]  Let us now look at the equations in more detail
[359.22s -> 360.46s]  and start with the assumption
[360.46s -> 362.14s]  that the object is undetected.
[362.14s -> 363.94s]  If the object is undetected,
[363.94s -> 367.78s]  we know from this equation that xk must be negative.
[367.78s -> 369.42s]  When the object is undetected,
[369.42s -> 372.06s]  the matrix ok is empty,
[372.06s -> 373.78s]  and the posterior is proportional
[373.78s -> 376.96s]  to the predicted density times one minus pd.
[376.96s -> 379.82s]  As you can see, one minus pd is zero
[379.82s -> 381.96s]  whenever xk is positive,
[381.96s -> 384.56s]  and one whenever xk is negative.
[384.56s -> 386.90s]  We are therefore multiplying the predicted density
[386.90s -> 389.94s]  with zero for all positive xk,
[389.94s -> 392.30s]  and one for all negative xk.
[392.30s -> 393.82s]  Given that the predicted density
[393.82s -> 397.36s]  is the Gaussian density illustrated using this blue curve,
[397.36s -> 399.96s]  the posterior density must have the same shape
[399.96s -> 402.76s]  as the blue curve for negative values of xk
[402.76s -> 406.62s]  and take the value zero for all positive values of xk,
[406.62s -> 409.68s]  since the posterior density must integrate to one.
[409.68s -> 411.22s]  It turns out that the posterior
[411.22s -> 414.36s]  is the green dashed curve illustrated here,
[414.36s -> 417.16s]  where we have multiplied the blue curve by two
[417.16s -> 419.12s]  for negative values of xk
[419.12s -> 422.32s]  and by zero for all positive values of xk.
[422.32s -> 423.64s]  Using similar arguments,
[423.64s -> 426.68s]  we can consider the case when the object is detected
[426.72s -> 429.92s]  and the matrix ok contains a measurement vector
[429.92s -> 431.80s]  and conclude that the posterior density
[431.80s -> 434.02s]  is this magenta colored curve,
[434.02s -> 436.74s]  which is zero when xk is negative
[436.74s -> 438.12s]  and proportional to the prior
[438.12s -> 440.32s]  for all positive values of xk.
[440.32s -> 441.60s]  To conclude this example,
[441.60s -> 443.56s]  I would like to point out two things.
[443.56s -> 446.16s]  First of all, this example demonstrates
[446.16s -> 449.16s]  that the probability of detection can be informative
[449.16s -> 451.44s]  and that it cannot always be ignored.
[451.44s -> 453.38s]  Second, it is important to note
[453.38s -> 455.84s]  that this is a toy example specifically designed
[455.88s -> 458.88s]  to illustrate that pd of xk may be important
[458.88s -> 461.68s]  and in most situations, the measurement vector ok
[461.68s -> 463.54s]  is far more informative
[463.54s -> 465.24s]  than the probability of detection.
[465.24s -> 466.68s]  As a second example,
[466.68s -> 469.96s]  suppose the probability of detection is instead constant,
[469.96s -> 471.60s]  which means that it does not convey
[471.60s -> 473.36s]  any information about xk.
[473.36s -> 476.20s]  The general expression for the posterior is as follows.
[476.20s -> 479.10s]  Where we have a factor one minus pd of xk
[479.10s -> 481.12s]  when the object is undetected
[481.12s -> 484.30s]  and a factor pd of xk when it is detected.
[484.30s -> 485.14s]  Under the assumption
[485.14s -> 487.62s]  that the probability of detection is a constant,
[487.62s -> 489.78s]  both of these factors are constants
[489.78s -> 492.98s]  that can be absorbed into the proportionality constant.
[492.98s -> 494.98s]  We can therefore simplify the expression
[494.98s -> 498.06s]  by removing the factors that contain pd of xk.
[498.06s -> 500.82s]  As you can see, if the object is undetected,
[500.82s -> 503.38s]  the posterior is identical to the prior.
[503.38s -> 505.26s]  And this is probably what you would expect
[505.26s -> 508.06s]  in situations where we fail to detect the object.
[508.06s -> 510.38s]  Similarly, when the object is detected,
[510.38s -> 512.42s]  we multiply the predicted density
[512.42s -> 515.22s]  with a likelihood function of the object measurement.
[515.22s -> 516.22s]  What you see here
[516.22s -> 518.70s]  just looks like the standard equations for the posterior
[518.70s -> 521.06s]  when we observe a measurement vector ok.
[521.06s -> 522.64s]  One way to summarize this
[522.64s -> 526.18s]  is that we perform a standard update using the vector ok,
[526.18s -> 528.14s]  but only if the object is detected.
[528.14s -> 529.78s]  To clarify even further
[529.78s -> 532.98s]  what I meant with standard update on the previous slide,
[532.98s -> 534.98s]  we can look at a linear and Gaussian example
[534.98s -> 537.10s]  for which the update step is simple.
[537.10s -> 539.42s]  Specifically, suppose the predicted density
[539.46s -> 543.46s]  is a Gaussian density with mean x bar k given k minus one
[543.46s -> 546.62s]  and a covariance pk given k minus one
[546.62s -> 548.78s]  and a linear and Gaussian measurement model
[548.78s -> 552.10s]  with matrix hk and covariance rk.
[552.10s -> 555.26s]  In that case, the posterior density is also Gaussian.
[555.26s -> 557.10s]  If the object is undetected,
[557.10s -> 559.68s]  such that the matrix ok is empty,
[559.68s -> 562.26s]  the posterior has the same mean and covariance
[562.26s -> 563.70s]  as the predicted density.
[563.70s -> 566.10s]  If the object is instead detected,
[566.10s -> 569.58s]  the posterior density is proportional to a Gaussian prior
[569.58s -> 572.86s]  times a linear and Gaussian measurement likelihood,
[572.86s -> 574.86s]  which means that the posterior mean and covariance
[574.86s -> 577.22s]  are given by a standard Kalman filter update.
[577.22s -> 578.26s]  You can pause the video
[578.26s -> 580.82s]  if you want to look at the Kalman filter equations,
[580.82s -> 582.30s]  but the bottom line is that we perform
[582.30s -> 584.26s]  a standard Kalman filter update
[584.26s -> 585.74s]  if the object is detected
[585.74s -> 587.74s]  and we leave the predicted density as it is
[587.74s -> 589.94s]  if the object is undetected.
[589.94s -> 591.50s]  Let us finish by visualizing
[591.50s -> 594.18s]  what the prediction and update steps might look like
[594.18s -> 596.06s]  in an even more specific example.
[596.06s -> 598.42s]  Suppose we have a two-dimensional state vector
[598.42s -> 601.30s]  representing position and velocity along a line,
[601.30s -> 603.54s]  that we are using a constant velocity model,
[603.54s -> 605.78s]  and that the initial prior is Gaussian.
[605.78s -> 606.98s]  As a measurement model,
[606.98s -> 609.22s]  we assume that we have a probability of detection
[609.22s -> 613.62s]  of 0.85, which also means that Pd is constant.
[613.62s -> 615.90s]  Finally, we assume that when the object is detected,
[615.90s -> 619.54s]  we observe its position with some additive Gaussian noise.
[619.54s -> 620.94s]  In this illustration,
[620.94s -> 624.20s]  the green curve represents the prior at time zero,
[624.20s -> 627.80s]  the blue curve, the predicted density at time one,
[627.80s -> 631.60s]  the black line represents an observation of the position,
[631.60s -> 635.76s]  and the magenta line represents the posterior at time one.
[635.76s -> 638.32s]  This is the relation that we generally expect to see
[638.32s -> 640.48s]  where the prediction shifts the PDF
[640.48s -> 642.56s]  and increases the uncertainties,
[642.56s -> 645.48s]  whereas the update step decreases the uncertainties
[645.48s -> 647.92s]  and shifts the density towards the regions
[647.92s -> 649.92s]  that match the measurements better.
[649.92s -> 652.56s]  At later times, the green curve instead
[652.56s -> 655.64s]  represents the posterior at time k minus one.
[655.64s -> 657.72s]  But the relation between the different densities
[657.72s -> 658.72s]  is similar.
[658.72s -> 661.40s]  At time k equals three,
[661.40s -> 662.96s]  the relations are instead different
[662.96s -> 665.48s]  since the object happens to be undetected.
[665.48s -> 667.12s]  In this case, the predicted density
[667.12s -> 668.68s]  is not actually visible
[668.68s -> 671.28s]  since it is identical to the posterior density.
[671.28s -> 672.20s]  As you can see,
[672.20s -> 674.72s]  the posterior uncertainties at time k equals three
[674.72s -> 677.36s]  are larger than at time k equals two,
[677.36s -> 679.72s]  and they would continue to increase over time
[679.72s -> 681.96s]  if the object remains undetected.
[682.00s -> 684.52s]  However, at time k equals four,
[684.52s -> 686.16s]  the object is again detected,
[686.16s -> 689.56s]  and the posterior uncertainties therefore decrease again.
[689.56s -> 691.20s]  To summarize, in this video,
[691.20s -> 693.56s]  we have studied the prediction and update steps
[693.56s -> 696.40s]  under the assumption that the data associations are known.
[696.40s -> 698.60s]  These equations are used extensively
[698.60s -> 699.92s]  in the rest of this course,
[699.92s -> 701.04s]  so please try to make sure
[701.04s -> 702.76s]  that you understand them properly.
[702.76s -> 705.96s]  The prediction step is actually identical to last week,
[705.96s -> 709.00s]  whereas the update equation takes different forms
[709.00s -> 712.04s]  depending on if the object is detected or undetected.
