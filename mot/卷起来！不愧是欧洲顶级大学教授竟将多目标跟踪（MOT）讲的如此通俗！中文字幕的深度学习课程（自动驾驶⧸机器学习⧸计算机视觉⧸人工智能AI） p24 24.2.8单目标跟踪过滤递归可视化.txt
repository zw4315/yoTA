# Detected language: en (p=1.00)

[0.00s -> 6.00s]  Welcome. The purpose with this section is to derive exact filtering equations.
[6.00s -> 18.00s]  What we mean by this is a set of equations that describe how to recursively compute the posterior of the state at time k, given measurements up until and including time k.
[18.00s -> 26.00s]  We refer to these equations as a conceptual solution, since it is intractable to directly use them in a filter.
[26.00s -> 37.00s]  Still, having the exact equations is a perfect starting point for developing tractable filtering algorithms that approximate the exact solution.
[37.00s -> 41.00s]  We will present a number of such algorithms later on.
[41.00s -> 55.00s]  The purpose with this video is to visualize the conceptual filtering solution in order to build intuition for it and to understand the overall structure before we zoom in on the underlying technical details.
[56.00s -> 66.00s]  In particular, we will illustrate the results from the prediction and update steps in a toy example where we have unknown data association hypotheses.
[66.00s -> 73.00s]  Before we look at an example, let us present the overall structure of the filtering densities in single object tracking.
[73.00s -> 85.00s]  To do this, we use z 1 to k to denote the sequence of measurements up to time k, and θ 1 to k to denote the sequence of data association hypotheses up to time k.
[85.00s -> 92.00s]  The objective in single object tracking is to compute the posterior density of xk, given all the measurements up to time k.
[92.00s -> 99.00s]  It turns out that this density can be decomposed into one term for every possible sequence of data association hypotheses.
[99.00s -> 108.00s]  In each term, the first factor is the density of the object state, given all the measurements, and a specific sequence of data association hypotheses.
[108.00s -> 115.00s]  That is, this is the density when we assume that we know which of the measurements in the sequence that are object detections.
[115.00s -> 124.00s]  As you may recall, we had a video on single object tracking with known associations, and the recursions described in that video can be used to compute this density.
[124.00s -> 131.00s]  Each term also has a second factor, which is the probability of the data association hypotheses, given the measurements.
[131.00s -> 138.00s]  This factor scales the different terms and gives a smaller weight to unlikely sequences of hypotheses.
[138.00s -> 146.00s]  To arrive at this expression, we first realize that it would be easier for us to express the posterior distribution if the data association hypotheses were known,
[146.00s -> 152.00s]  and we then use the law of total probability to introduce θ 1 to k into the distribution.
[152.00s -> 168.00s]  Finally, we note that the joint distribution of xk and θ 1 to k, given z 1 to k, can be factorized into the distribution of xk, given θ 1 to k, and z 1 to k, times the distribution of θ 1 to k, given z 1 to k.
[168.00s -> 173.00s]  As you can see, we managed to derive the decomposition in just two steps.
[173.00s -> 179.00s]  You should also note that derivation precisely follows the steps that we used in the first video about the measurement model.
[179.00s -> 188.00s]  First, we identify variables that can simplify things for us, then we use the law of total probability to introduce them, and finally we factorize the distribution.
[188.00s -> 195.00s]  By the way, the same technique is also used to drive the Chapman-Kolmogorov equations, so I'm confident that you've seen this before.
[195.00s -> 203.00s]  The predicted density has a similar structure and can also be decomposed into one term for every possible sequence of data association hypotheses.
[203.00s -> 211.00s]  The weights are also the same, since the probabilities of the different data association hypotheses do not change during the prediction step.
[211.00s -> 220.00s]  The only difference is that the densities in each term are the predicted densities of xk plus 1, given measurements and hypotheses up to time k.
[220.00s -> 226.00s]  To obtain this predicted density, we simply use Chapman-Kolmogorov on the corresponding filtering densities.
[226.00s -> 232.00s]  And you can return to the video about single object tracking with known associations if you're uncertain about this part.
[232.00s -> 240.00s]  To simplify the visualizations, we consider a very simple toy example where both the object state and the measurements are scalar.
[240.00s -> 247.00s]  We also assume that both gk and pi k, describing the object measurements and motion, are linear and Gaussian.
[247.00s -> 258.00s]  The exact details are less important to understand our visualizations of the prediction and update steps, but I'll walk you through them such that you can check that the visualizations are reasonable.
[258.00s -> 266.00s]  We assume that the initial prior is Gaussian with mean 0.5 and variance 0.2, which means that we have a fairly informative prior.
[266.00s -> 275.00s]  The motion is assumed to be a random walk such that the new state is the previous state plus Gaussian noise with mean 0 and variance 0.35.
[275.00s -> 286.00s]  The object is detected with probability 0.9 and, if detected, we observe an object measurement which is the state plus Gaussian noise with mean 0 and variance 0.2.
[286.00s -> 293.00s]  The clutter intensity is 0.4 in the region where we have collected all our measurements and zero outside that region.
[293.00s -> 312.00s]  Finally, we only consider the first three time steps and we assume that we observe the detections minus 1.3 and 1.7 at time 1, the detection 1.3 at time 2 and the detections minus 0.3 and 2.3 at time 3.
[312.00s -> 326.00s]  Please note that, since the probability of detection is constant and both πk and gk are linear and Gaussian, we can perform filtering, under the assumption that the associations are known, using the Kalman filter equations.
[326.00s -> 333.00s]  The only minor difference is that we do not always obtain an object measurement and at those times we simply skip the update.
[333.00s -> 339.00s]  We are ready to visualize the filtering recursions and let us start with the update step at time k equal 1.
[339.00s -> 348.00s]  At time 1 we observe two measurements, minus 1.3 and 1.7, and these are marked by asterisks in the figure.
[348.00s -> 355.00s]  For visualization purposes, I have also included a circle which represents the hypothesis that the object is undetected.
[355.00s -> 363.00s]  In these two figures you can see the prior at time 1 and the posterior at time 1, given that we have observed these two measurements.
[363.00s -> 367.00s]  Before we continue, we can try to understand this posterior in more detail.
[367.00s -> 372.00s]  The number of possible associations at time 1 is the number of measurements, M1 plus 1.
[372.00s -> 375.00s]  In this case we have two measurements and three possible associations.
[375.00s -> 379.00s]  We can reason about the three possibilities on an intuitive level.
[379.00s -> 389.00s]  First, it is not unreasonable that the detection at 1.7 is the object detection and that would indicate that the object is closer to 1.7 than we thought initially.
[389.00s -> 399.00s]  Second, it is also possible that the measurement at minus 1.3 is the object detection and in that case it is more likely that the object is closer to minus 1.3 than we thought.
[399.00s -> 408.00s]  Third, the object could also be undetected and in that case we have not gained any new information about the state and we would just have to rely on the prior.
[408.00s -> 419.00s]  Considering that we don't know which association is true, the posterior density needs to acknowledge all three possibilities and this yields the posterior density that you see here.
[419.00s -> 429.00s]  On a more technical level, we refer to these possibilities as hypotheses and, as you know, we can decompose the posterior into one term for each hypothesis.
[429.00s -> 433.00s]  We will use red dashed curves to visualize the individual terms.
[434.00s -> 443.00s]  At the moment, the red curve shows the posterior of X1 given that the object is undetected, scaled by the probability that it is undetected.
[443.00s -> 449.00s]  But the fact that we are currently considering the hypothesis that the object is undetected is indicated by the red circle.
[449.00s -> 458.00s]  Since the object is undetected, the red curve is therefore proportional to the prior since the posterior and the prior densities are identical when the object is undetected.
[458.00s -> 464.00s]  But the red curve takes smaller values since the probability that the object is undetected is smaller than 1.
[464.00s -> 476.00s]  Similarly, under the hypothesis that the object detection is minus 1.3, we obtain a posterior which is shifted towards minus 1.3 and which has a smaller variance than the prior.
[476.00s -> 488.00s]  Finally, under the hypothesis that the object detection is 1.7, we obtain a posterior which is shifted towards 1.7 and that again has a smaller variance than the prior.
[488.00s -> 493.00s]  In all three cases, the distributions are scaled by the probability of the corresponding hypothesis.
[493.00s -> 500.00s]  As you can see from the equation, the complete posterior is obtained by summing up the contributions from the three hypotheses.
[500.00s -> 506.00s]  We have now obtained the posterior at time 1 and the next step in our filter is to perform prediction until time 2.
[506.00s -> 514.00s]  In these figures, the solid green curves show the posterior at time 1 as well as the predicted density of x2 given z1.
[514.00s -> 521.00s]  The red dashed curves illustrate the contributions from the individual hypothesis to these densities.
[521.00s -> 533.00s]  A short and intuitive description of the prediction step is that we go through the possible associations one at a time and ask ourselves what would the predicted density be if this was the true association.
[533.00s -> 539.00s]  Having done that, we obtain the complete posterior by summing up the densities weighted by their probabilities.
[539.00s -> 543.00s]  Right now, we are looking at the contribution from the undetected hypothesis.
[543.00s -> 552.00s]  As you can see, that curve flattens out during the prediction step due to the increased variance in the predicted density given that the object was undetected.
[552.00s -> 561.00s]  However, the integral under the curve doesn't change and the reason for that is that the probability that the object was undetected hasn't changed.
[561.00s -> 567.00s]  Similarly, for the other two hypotheses, the uncertainties also increase during the prediction step.
[567.00s -> 574.00s]  We can see that the overall predicted density, illustrated in green, is slightly smoother than the posterior at time 1.
[574.00s -> 582.00s]  Please note that the number of data association hypotheses does not change during the prediction step since there are no new observations.
[582.00s -> 586.00s]  Let us now look at how we can perform the update step at time 2.
[586.00s -> 596.00s]  Importantly, for every hypothesis at time 1, we now have two possible associations at time 2, corresponding to the object being detected or undetected.
[596.00s -> 607.00s]  In general, we get m1 plus 1 times m2 plus 1 hypotheses in total, which in this example is 3 times 2 or 6 hypotheses.
[607.00s -> 615.00s]  In these figures, the solid green curves show the predicted density of x2 given z1, as well as the posterior at time 2.
[615.00s -> 621.00s]  As before, the red dashed curves show the contributions from the individual hypotheses.
[621.00s -> 629.00s]  In this case, several of the sequences of our associations have very small probabilities and do not contribute much to the posterior.
[629.00s -> 639.00s]  For instance, the probability that the object was undetected at both time 1 and time 2 is quite small, which makes this red dashed curve very low.
[639.00s -> 645.00s]  Since we assume that the object is undetected, it actually has the same shape as the red curve to the left.
[645.00s -> 651.00s]  But since the probability that the object is undetected at both times is small, it is scaled by a small number.
[651.00s -> 657.00s]  The probability that the object is first undetected and then detected is slightly larger,
[657.00s -> 666.00s]  and under this hypothesis you can see that the updated density is shifted in the direction of the measurement at time 2, and that it has a smaller variance.
[666.00s -> 671.00s]  You can go through the other four sequences of hypotheses and make similar observations.
[671.00s -> 679.00s]  You can for instance see that the posterior is dominated by two hypotheses, under which the object is assumed to be detected at time 2.
[679.00s -> 686.00s]  To compute the predicted density at time k equal 3, we perform the same type of calculations as in the previous prediction step.
[686.00s -> 694.00s]  Specifically, for every hypothesis, uncertainties are increased, but the probability of the different hypotheses remains identical,
[694.00s -> 699.00s]  since we have not obtained any new evidence in favor of the different hypotheses.
[699.00s -> 702.00s]  We also note that the number of hypotheses remain constant.
[702.00s -> 708.00s]  I'd like to clarify that the predictions that we have presented in this video obeys the Chapman-Kolmogorov equations,
[708.00s -> 713.00s]  and that the update presented on this slide obeys Bayes' rule.
[713.00s -> 722.00s]  That is, those equations are still valid, but the decomposition used here enables us to predict and update these fairly complicated densities
[722.00s -> 727.00s]  by applying the Kalman filter equations to every term in the summation.
[727.00s -> 734.00s]  Even though it is great that we can split the prediction and update steps into a number of simple Kalman filter calculations,
[734.00s -> 743.00s]  exact filtering still generally becomes intractable after a few time steps, since the number of hypotheses grows very quickly.
[743.00s -> 751.00s]  It should also be noted that the models are often non-linear, and the posterior density conditioned on a sequence of data association hypotheses
[751.00s -> 756.00s]  may then have to be approximated using, for instance, an extended Kalman filter.
[756.00s -> 759.00s]  Let us summarize the conclusions regarding the number of hypotheses.
[759.00s -> 765.00s]  At a single time instance, we have MK plus one data association hypotheses.
[765.00s -> 772.00s]  Among these, we have one hypothesis for each measurement, plus one hypothesis for the event that the object is undetected.
[772.00s -> 783.00s]  At time k, the number of possible association sequences is the product of M1 plus one, times M2 plus one, and so on, until MK plus one.
[783.00s -> 785.00s]  And this grows quickly with time.
[785.00s -> 793.00s]  For instance, if we observe four measurements at all times, we have almost 10 million hypotheses after only 10 time steps.
[793.00s -> 800.00s]  If MK is the same at all times, the number of possible association sequences grows exponentially as a function of k.
[800.00s -> 806.00s]  And it is therefore often informally stated that the number of hypotheses grows exponentially with time.
