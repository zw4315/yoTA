# Detected language: en (p=1.00)

[0.00s -> 8.00s]  We wish to solve the constraint minimization problem shown here.
[8.00s -> 17.00s]  So we want to minimize the assignment cost under the constraints that each object is associated to either a detection or a misdetection,
[17.00s -> 22.00s]  and each detection is associated to at most one object.
[22.00s -> 30.00s]  Some solvers for this problem are the Hungarian, the auction, and the Junker, Volgin, and Kestanov, or JVC, algorithms,
[30.00s -> 35.00s]  which can be used to find the single best solution, A*.
[35.00s -> 43.00s]  Murti's algorithm can be used to find the M best assignments, ranked in order of increasing assignment cost.
[43.00s -> 52.00s]  And lastly, there is Gibbs sampling, which is a suboptimal but computationally efficient alternative to finding the M best assignments.
[52.00s -> 57.00s]  Explaining exactly how these solvers work is beyond the scope of this course.
[57.00s -> 64.00s]  So here we will take them as off-the-shelf algorithms that can help us deal with the data association problem.
[64.00s -> 68.00s]  In this video, we will just briefly explain what it is that they do.
[68.00s -> 78.00s]  Common to all of them is that they take as an input a cost matrix and return assignment matrices that correspond to valid data associations.
[78.00s -> 85.00s]  Hungarian auction and JVC are all algorithms that compute the optimal assignment A*.
[85.00s -> 90.00s]  In other words, the one that corresponds to a data association theta*.
[90.00s -> 95.00s]  For which the weight is larger than the weight for any other valid association.
[95.00s -> 104.00s]  Murti's algorithm is an example of an algorithm that computes a number of ranked assignments, A*.1 to A*.M.
[104.00s -> 112.00s]  Where the weight of the first one is larger than the weight of the second one, and so on and so forth until the last one.
[112.00s -> 118.00s]  And the weight of the last one is larger than the weight of any other valid association.
[119.00s -> 125.00s]  Gibbs sampling is a so-called Markov chain Monte Carlo method, or MCMC method.
[125.00s -> 133.00s]  That can be used to find M assignments A*.1 to A*.M that have high likelihoods or high weights.
[133.00s -> 137.00s]  However, there are no guarantees when using Gibbs sampling.
[137.00s -> 147.00s]  It is possible that there is one or more associations that have high enough weights to be among the M associations with highest weights of all.
[147.00s -> 156.00s]  But the benefit of using Gibbs sampling is that it is computationally very efficient compared to, for example, Murti's algorithm.
[156.00s -> 161.00s]  And empirical evidence shows that it can yield quite good tracking performance.
[161.00s -> 168.00s]  So to summarize, the goal is to find a subset of data associations that have high weights.
[168.00s -> 180.00s]  And we do this by casting the task as an optimal assignment problem, where the input is a cost matrix and the output is assignments that correspond to different valid associations.
[180.00s -> 188.00s]  And some algorithms for finding these associations are Hungarian, auction and JVC for finding the single best association,
[188.00s -> 198.00s]  Murti's algorithm for finding the M best associations, and Gibbs sampling as an efficient alternative for finding M good associations.
