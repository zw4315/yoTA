# Detected language: en (p=1.00)

[0.00s -> 5.52s]  The Poisson point process is very important in multi-object tracking, and it is often
[5.52s -> 9.36s]  used to model several different aspects of the problem.
[9.36s -> 14.00s]  In particular, it is the standard model for clutter, and actually the only model
[14.00s -> 18.12s]  for clutter that we consider in this entire course.
[18.12s -> 23.36s]  In this video, we present some of the key properties of Poisson point processes,
[23.36s -> 27.80s]  and provide enough technical details to derive the complete measurement model in the next
[27.80s -> 28.80s]  two videos.
[29.04s -> 34.52s]  Like I've already mentioned, the Poisson point process really is the default model for clutter.
[34.52s -> 40.28s]  We use uppercase CK to denote the clutter matrix, and lowercase CK with a superscript
[40.28s -> 42.68s]  to denote the vectors in the matrix.
[42.68s -> 48.08s]  Also, MKC denotes the number of clutter detections at time k.
[48.08s -> 52.56s]  For the specific Poisson point process introduced in the previous video, where the expected
[52.56s -> 57.84s]  number of clutter detections per unit volume was λ, everywhere inside the field of view,
[57.84s -> 62.96s]  the number of clutter detections is Poisson distributed with mean λ times v.
[62.96s -> 68.48s]  Also, given the number of detections, the vectors in the matrix CK are independent and
[68.48s -> 73.32s]  identically distributed, and they are all uniformly distributed over the field of view.
[73.32s -> 78.16s]  I've here used boldface v to denote the set of vectors in our field of view.
[78.16s -> 82.76s]  To clarify what we mean by this, let us look at an algorithm that generates samples from this
[82.76s -> 84.28s]  Poisson point process.
[84.32s -> 91.80s]  So, to obtain a sample from this Poisson point process CK, we initialize CK to be an empty matrix.
[91.80s -> 95.72s]  We then generate MKC from a Poisson distribution.
[95.72s -> 100.72s]  Finally, we can use a for loop to generate all the vectors in the matrix.
[100.72s -> 107.00s]  To do this, we simply generate a vector CKi from this uniform distribution, and then
[107.00s -> 111.40s]  we include that vector as a new column in the matrix CK.
[111.40s -> 114.64s]  We have seen a first example of a Poisson point process.
[114.64s -> 121.16s]  One way to parameterize a general Poisson point process is using its intensity function λc.
[121.16s -> 126.80s]  For instance, sometimes we might expect more clutter detections in one region than another,
[126.80s -> 131.36s]  and we can model this using an intensity function which is larger, where we expect
[131.36s -> 135.72s]  more clutter detections, and smaller in the other regions.
[135.72s -> 139.88s]  Another way to parameterize a Poisson point process is using a combination of the rate
[140.36s -> 146.12s]  λ bar c, which is a non-negative number representing the expected number of detections,
[146.12s -> 152.60s]  and fc of c, representing the spatial probability density function of the clutter vectors.
[152.60s -> 157.64s]  As you can see, the rate is the integral over the intensity function λc of c,
[157.64s -> 162.28s]  and the spatial PDF is a normalized version of λc of c.
[162.28s -> 166.52s]  It's easy to verify that the spatial PDF integrates to one.
[166.52s -> 171.56s]  From these equations, we can see that we can compute the rate and the spatial PDF from the
[171.56s -> 177.00s]  intensity function. However, the intensity function can also be computed from the rate
[177.00s -> 180.28s]  and the spatial PDF by multiplying the two together.
[180.28s -> 185.40s]  It is clear from the expression of the spatial PDF that the rate then cancels out.
[185.40s -> 189.64s]  Which of the two parameterizations that we use therefore simply depends on the task at hand,
[189.64s -> 191.72s]  and the one we happen to find more convenient.
[192.28s -> 198.36s]  For the Poisson point process that we saw before, the intensity function is λ everywhere inside the
[198.36s -> 204.52s]  field of view, and 0 outside the field of view. Similarly, the rate is λ times v,
[205.16s -> 210.84s]  and the spatial PDF is 1 over v everywhere inside the field of view, and 0 outside.
[211.48s -> 216.36s]  Here, the rate tells us that the expected number of clutter detections is λ times v,
[216.44s -> 222.20s]  and the spatial PDF tells us that the vectors are uniformly distributed over the field of view.
[222.20s -> 227.64s]  We previously presented an algorithm to generate samples from the specific Poisson point process
[227.64s -> 231.48s]  introduced in the last video. For general intensity functions,
[231.48s -> 235.08s]  the algorithm to generate samples is essentially identical.
[235.08s -> 239.48s]  We start by initializing ck to be an empty matrix, and then determine the number of
[239.48s -> 245.72s]  clutter detections by generating mkc from a Poisson distribution with mean λ bar c.
[245.72s -> 252.60s]  Finally, we generate the vectors cki from the spatial PDF fc and add the vectors to the matrix.
[252.60s -> 257.48s]  The main difference compared to before is that the spatial PDF does not have to be uniform,
[257.48s -> 260.20s]  but the general algorithm contains the same steps.
[260.20s -> 263.88s]  We can now look at the Poisson point process distribution.
[263.88s -> 267.00s]  Suppose we want to evaluate the distribution for a matrix
[267.00s -> 273.64s]  containing mkc different vectors denoted ck1, ck2, and so on.
[273.64s -> 278.36s]  It turns out that the distribution of ck is identical to the joint distribution of
[278.36s -> 284.04s]  ck and mkc. We will elaborate on this step on the home page by providing a short proof,
[284.04s -> 291.56s]  but intuitively speaking the equation holds because mkc was in some sense already present
[291.56s -> 294.52s]  since it is the number of column vectors in ck.
[294.52s -> 299.80s]  The joint distribution of mkc and ck can now be factorized into the product of the
[299.80s -> 307.56s]  distribution of mkc times the distribution of ck given mkc, which are two distributions that we know.
[307.56s -> 312.76s]  The first corresponds to the probability of obtaining precisely mkc vectors,
[312.76s -> 319.08s]  which is the Poisson distribution with parameter λ bar c evaluated at mkc.
[319.08s -> 325.32s]  The second factor is the probability to obtain the vectors ck1, ck2, and so on,
[325.32s -> 331.08s]  given the value of mkc. And since the vectors are independently generated from fc, this is
[331.08s -> 338.12s]  the product over i of fc cki. For our current purposes, this expression is actually enough,
[338.12s -> 343.16s]  but there is an alternative way to express the distribution that will appear every now and then
[343.16s -> 349.24s]  as we continue. To see this, we note that this Poisson distribution is e to the power of minus
[349.24s -> 357.32s]  λ bar c times λ bar c to the power of mkc. All of this divided by mkc factorial.
[357.32s -> 364.04s]  Whereas the spatial distribution of fc is λc over λ bar c. As you can see,
[364.04s -> 371.16s]  λ bar c to the power of mkc cancels out, since we divide by λ bar c in each factor
[371.16s -> 377.88s]  in this product, which becomes precisely 1 divided by λ bar c to the power of mkc.
[377.88s -> 385.88s]  The only remaining factors are therefore e to the power of minus λ bar c divided by mkc factorial
[385.88s -> 392.60s]  times the product over i of the intensity function λc of cki. It's good to learn to recognize
[392.60s -> 397.16s]  this expression for the Poisson point process distribution, since you will see it every now
[397.16s -> 401.80s]  and then in the upcoming weeks. Just to end with something visual, let us look at samples from
[401.80s -> 406.20s]  a Poisson point process. For simplicity, we have returned to the Poisson point process
[406.20s -> 410.76s]  described previously, where samples are uniformly distributed over the field of view,
[410.76s -> 416.20s]  and the expected number of detections is 3.2. As you know, the spatial distribution is uniform
[416.20s -> 420.44s]  over this area. However, it may be more interesting to look at the number of detections
[420.44s -> 425.72s]  that we obtain. In this example, we know that the expected number of detections is 3.2,
[425.72s -> 431.48s]  but as you can see, the number of elements varies significantly from zero detections to
[431.48s -> 445.64s]  six detections, in the few realizations observed here.
