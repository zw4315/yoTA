# Detected language: en (p=1.00)

[0.00s -> 5.46s]  Hello, in this video, we're going to talk about gating for a known number of objects.
[5.46s -> 9.88s]  In single object tracking, we saw that gating can be used to reduce the number of data
[9.88s -> 15.06s]  association possibilities, and the same thing holds when we have n objects.
[15.06s -> 19.24s]  We saw earlier that in n object tracking, we have a very high number of possible
[19.24s -> 20.90s]  data association.
[20.90s -> 24.92s]  And just like in single object tracking, it would be good if we had a simple way
[24.92s -> 30.52s]  to reject or disregard data associations that we think are very unlikely.
[30.52s -> 34.48s]  So in other words, data associations that are very improbable.
[34.48s -> 40.12s]  In this video, we will show that by generalizing gating to n objects, we can drastically reduce
[40.12s -> 42.04s]  the number of data associations.
[42.04s -> 46.80s]  We can lower the computational burden of the tracking algorithm, and we can also partition
[46.80s -> 52.28s]  the data association problem into smaller subproblems, such that each subproblem has
[52.28s -> 54.80s]  fewer objects and fewer detections.
[54.80s -> 60.40s]  And the fewer objects and detections, the fewer the valid data associations are.
[60.40s -> 65.96s]  And it then follows that dealing with the data associations becomes computationally cheaper.
[65.96s -> 70.88s]  The basic idea when we have n objects is the same as when we had a single object.
[70.88s -> 76.44s]  We form a gate around each predicted measurement, and then we consider only detections within
[76.44s -> 77.48s]  the gates.
[77.48s -> 83.32s]  And by doing so, we can often see a drastic reduction in the number of associations.
[83.32s -> 86.36s]  And we will show examples of this in this video.
[86.36s -> 91.32s]  And just like in single object tracking, if the object densities are Gaussian, we consider
[91.32s -> 92.96s]  ellipsoidal gating.
[92.96s -> 96.36s]  There are multiple different ways in which we can do the gating.
[96.36s -> 101.16s]  However, given that Gaussian object densities are common in object tracking, we're going
[101.16s -> 103.88s]  to focus on ellipsoidal gating.
[103.88s -> 109.00s]  Let's start by reminding ourselves how ellipsoidal gating for Gaussian object densities is
[109.00s -> 110.00s]  defined.
[110.04s -> 116.04s]  The ellipsoidal gating distance for detection J and object I under hypothesis H is denoted
[116.04s -> 122.04s]  D and is defined as the residual transpose times the inverse innovation covariance times
[122.04s -> 123.04s]  the residual.
[123.04s -> 128.30s]  Here, the residual, which is sometimes also called the innovation, is the difference
[128.30s -> 134.32s]  between the detection J and the predicted detection Z-hat for object I.
[134.40s -> 140.44s]  So we have a gating threshold G, and this corresponds to some gate probability PG.
[140.44s -> 145.76s]  The relationship between G and PG is the same in an object tracking as it is in single
[145.76s -> 148.92s]  object tracking, and we do not repeat it here.
[148.92s -> 155.36s]  For an hypothesis H, if the gating distance is smaller than or equal to the gate G,
[155.36s -> 159.62s]  the detection J is considered as a possible detection from object I.
[159.74s -> 164.98s]  And if the distance is larger than G, then the possible association of detection J and
[164.98s -> 168.32s]  object I is ruled out for this hypothesis.
[168.32s -> 173.62s]  We can compare the gating to the log likelihood of the association that we used in the cost
[173.62s -> 178.10s]  matrix when we formulated the data association as an assignment problem.
[178.10s -> 182.70s]  The log likelihood is reproduced here, and we can see that the gating distance actually
[182.70s -> 184.62s]  occurs in this expression.
[184.62s -> 187.90s]  We have negative one-half times the gating distance.
[187.90s -> 193.18s]  So it follows that the larger the distance is, the smaller the log likelihood is.
[193.18s -> 198.30s]  And since we're interested in finding data associations that have large sums of log
[198.30s -> 204.60s]  likelihoods, it makes sense to reject associations that correspond to small log likelihoods.
[204.60s -> 208.98s]  And the ellipsoidal gating allows us to do precisely this.
[208.98s -> 215.42s]  So if a detection J falls outside the gate of an object I for some hypothesis H, then
[215.42s -> 221.70s]  the corresponding log likelihood is set to equal negative infinity, which is an approximation
[221.70s -> 224.42s]  of the logarithm of likelihood zero.
[224.42s -> 227.40s]  So let's have a look at a gating example.
[227.40s -> 231.56s]  We have a 2D scenario, so the detections are 2D vectors.
[231.56s -> 237.08s]  And on the right here, we have illustrated 15 example detections as red squares.
[237.08s -> 239.50s]  Now assume that there are six objects.
[239.50s -> 245.18s]  The corresponding predicted measurements, Z-hat, are illustrated on the right as blue circles.
[245.18s -> 250.90s]  For 15 detections and 6 objects, there are more than 6 million valid data associations.
[250.90s -> 255.00s]  The matrix with log likelihood is shown on the bottom left here.
[255.00s -> 259.58s]  And note that we formulated data association as an assignment problem.
[259.58s -> 263.10s]  The cost matrix is the negative of the matrix shown here.
[263.10s -> 268.02s]  The color scale shown ranges from blue, which corresponds to low likelihood, and
[268.02s -> 270.70s]  to red, which corresponds to high likelihood.
[270.70s -> 274.86s]  So associations that are not allowed, in other words, that have been set equal to
[274.86s -> 276.90s]  negative infinity, are white.
[276.90s -> 281.38s]  And if you want to, you could pause the video here and verify that the longer the
[281.38s -> 286.30s]  distance is between the detection and the predicted detection in the top right image,
[286.30s -> 289.90s]  the lower the log likelihood is in the bottom left image.
[289.90s -> 295.08s]  If we draw the ellipsoidal gates centered at the corresponding predicted detections,
[295.08s -> 299.88s]  we see that for each object, some detections are inside the gate and other detections
[299.88s -> 301.38s]  are outside the gate.
[301.38s -> 306.92s]  For any detection that is outside the gate, we set the log likelihood to negative infinity.
[306.92s -> 309.32s]  And this is illustrated at the bottom right.
[309.32s -> 313.56s]  We see that several of the elements in the matrix are now negative infinity.
[313.56s -> 318.28s]  And if we were to use these log likelihoods to solve an optimal assignment problem,
[318.28s -> 325.16s]  then any association that associates to a detection outside a gate would have a cost equal to infinity.
[325.16s -> 330.92s]  In the top right image, we have also drawn lines between each predicted detection and
[330.92s -> 333.56s]  the detections that fall inside the gate.
[333.56s -> 338.62s]  And by following these lines, we see that three clusters, or groups, of objects and
[338.62s -> 340.44s]  detections have formed.
[340.44s -> 345.76s]  And we can use this to lower the computational cost of dealing with the data association
[345.76s -> 349.54s]  by doing something that is called grouping by gating.
[349.54s -> 354.88s]  For this example, we have three separate groups of detections and predicted detections.
[354.88s -> 360.52s]  And the basic idea in grouping by gating is to use the gating to group the detections
[360.52s -> 362.80s]  and objects into smaller groups.
[362.80s -> 367.72s]  And the motivation for doing this is that handling the data association for each group
[367.72s -> 373.12s]  is computationally cheaper than handling the data association for all detections and all
[373.12s -> 375.16s]  objects at the same time.
[375.16s -> 381.32s]  So in the example, we have six objects, 15 detections, and therefore more than 6 million
[381.32s -> 382.90s]  valid associations.
[382.90s -> 386.20s]  In the first group, we have one object and two detections.
[386.20s -> 391.20s]  And the log likelihood matrix for this group is given by extracting the relevant rows and
[391.20s -> 394.92s]  columns from the matrix with all objects and detections.
[394.92s -> 398.74s]  So here we have object 1 and detections 1 and 12.
[398.74s -> 402.38s]  And then we also need the element that corresponds to missed detection.
[402.38s -> 406.58s]  And this gives us a log likelihood matrix with one row, three columns.
[406.58s -> 409.66s]  And for this, we have three valid associations.
[409.66s -> 413.20s]  In the second group, we have two objects and four measurements.
[413.20s -> 416.70s]  The log likelihood matrix for the group is obtained in the same way.
[416.70s -> 419.70s]  We extract the relevant rows and columns.
[419.70s -> 422.58s]  And now we have two rows, six columns.
[422.58s -> 426.74s]  And that gives us 21 valid associations for the group.
[426.74s -> 432.26s]  And for the third group in this example, we have three objects, six detections, and
[432.26s -> 435.66s]  there are 229 valid associations.
[435.66s -> 441.06s]  So by doing grouping by gating, we have reduced the number of valid associations to three
[441.06s -> 447.40s]  times 21 times 229, which equals 14,427.
[447.40s -> 452.26s]  And this is several orders of magnitude smaller than 6.3 million.
[452.26s -> 456.60s]  And actually, if we consider the gating in each group, in other words, that for
[456.60s -> 460.90s]  each group, there are some associations for which we have set the log likelihood to
[460.90s -> 468.54s]  negative infinity, then we get three times 11 times 41, which is equal to 1353 valid
[468.54s -> 469.70s]  associations.
[469.70s -> 472.54s]  And this is another order of magnitude smaller.
[472.54s -> 477.60s]  So this example clearly shows us that the gating can help us to drastically reduce
[477.60s -> 479.82s]  the number of valid associations.
[479.82s -> 485.42s]  To summarize, gating for n objects allows us to disregard unlikely data associations.
[485.42s -> 490.78s]  And it follows that we can reduce the number of hypotheses in the posterior n object density.
[490.78s -> 495.26s]  When the object densities are Gaussian, ellipsoidal gates are a natural choice.
[495.26s -> 501.42s]  Finally, we want to point out that the gating threshold, or equivalently, the gating probability
[501.42s -> 503.02s]  must be chosen carefully.
[503.02s -> 508.38s]  If the gate is too small, we will actually reject associations whose probability is not
[508.38s -> 509.38s]  negligible.
[509.38s -> 513.74s]  And if the gate is too large, we will not reject so many associations, and the
[513.74s -> 516.62s]  computational savings will not be as large.
[516.62s -> 521.30s]  So what the best choice of G and PGR does not have a simple answer.
[521.30s -> 525.66s]  These are tuning parameters in object tracking, and they have to be adapted such that the
[525.66s -> 531.18s]  tracking algorithm has good enough performance and also a reasonable computational cost.
