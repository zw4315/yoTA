# Detected language: en (p=1.00)

[0.00s -> 8.18s]  In an earlier video, we presented general update equations for single object tracking.
[8.18s -> 11.82s]  Since those equations are general, we can of course use them here.
[11.82s -> 16.26s]  Still, we now assume that the predicted density has a specific form, and we will
[16.26s -> 21.14s]  actually re-derive the update equations, to show how to express the posterior on
[21.14s -> 22.44s]  the desired form.
[22.44s -> 27.06s]  We first recall the measurement model, ignoring the terms outside the brackets that do not
[27.06s -> 28.52s]  depend on xk.
[28.52s -> 35.46s]  1-PD represents the hypothesis that the object is undetected, whereas the other MK terms
[35.46s -> 41.00s]  represent the MK different hypotheses where one of the measurements is in object detection.
[41.00s -> 46.34s]  We note that the measurement likelihood is a sum with one term for every possible association
[46.34s -> 47.48s]  at time k.
[47.48s -> 53.94s]  The predicted density is instead a sum with one term for every possible sequence of associations
[53.94s -> 55.68s]  up to time k-1.
[55.68s -> 61.20s]  As usual, the posterior at time k is proportional to the predicted density times the likelihood
[61.20s -> 62.20s]  function.
[62.20s -> 66.82s]  Since both the predicted density and the likelihood contain a summation, the product
[66.82s -> 71.62s]  contains one term for every pair of terms in the two factors.
[71.62s -> 76.26s]  First, the factors outside the brackets in the likelihood function can be absorbed into
[76.26s -> 78.06s]  the proportionality constant.
[78.06s -> 82.44s]  To express the rest of the product, we take the predicted density times the different
[82.44s -> 83.94s]  terms in the likelihood.
[83.94s -> 89.60s]  For the term 1-PD times the predicted density, we obtain the following sum.
[89.60s -> 95.44s]  For the remaining MK terms in the likelihood, we obtain a double sum over the sequence
[95.44s -> 99.78s]  θ1 to k-1 and the association θk.
[99.78s -> 102.80s]  We now have a large sum with many different terms.
[102.80s -> 107.16s]  However, every term is a product of one term in the likelihood, which corresponds
[107.16s -> 112.42s]  to specific θk, and one term in the predicted density, which corresponds to sequence
[112.44s -> 115.14s]  of associations up to time k-1.
[115.14s -> 121.52s]  That is, every term corresponds to a pair of hypotheses, θ1 to k-1 and θk.
[121.52s -> 127.42s]  And for every pair, we obtain a new hypothesis, defining the sequence of associations up
[127.42s -> 130.80s]  to time k, here denoted θ1 to k.
[130.80s -> 135.92s]  It should therefore be possible to view this entire sum as a summation over θ1 to
[135.92s -> 136.92s]  k.
[136.92s -> 140.24s]  So, the posterior density is proportional to this summation.
[140.24s -> 147.42s]  We would now like to express the posterior as a sum over θ1 to k of weights times densities.
[147.42s -> 151.42s]  Using tricks from earlier videos, we can express the weights and densities.
[151.42s -> 156.56s]  The unnormalized weight of a single term is generally equal to the integral of the
[156.56s -> 161.18s]  original function, whereas the density is proportional to the same function.
[161.18s -> 165.98s]  For the hypothesis where θk is equal to zero, and the specific sequence of associations
[165.98s -> 170.14s]  up to time k-1, the unnormalized weight is the integral of the weight
[170.14s -> 175.72s]  times the predicted density for the specific association sequence times 1 minus pd, where
[175.72s -> 178.20s]  the weight can be factored out from the integral.
[178.20s -> 183.00s]  Similarly, the posterior density given the same sequence of associations is proportional
[183.00s -> 190.60s]  to the predicted density times 1 minus pd, where the weight w θ1 to k-1 is ignored
[190.60s -> 191.94s]  since it is a constant.
[191.94s -> 196.68s]  As you can see, these are essentially the standard expressions when the object is undetected
[196.68s -> 197.68s]  at time k.
[197.86s -> 202.84s]  But the unnormalized weight is now scaled by the weight of θ1 to k-1.
[202.84s -> 204.50s]  So why is this factor included?
[204.50s -> 210.04s]  Well, the weights reflect how reasonable or probable a sequence of associations is.
[210.04s -> 216.16s]  This factor tells us that it is not enough to consider if θk equals zero is reasonable,
[216.16s -> 221.26s]  but we also need to consider if the earlier parts of the sequence of associations are
[221.26s -> 222.26s]  reasonable.
[222.26s -> 227.18s]  The weights and densities for the hypothesis where θk is larger than zero are obtained
[227.18s -> 228.50s]  in a similar manner.
[228.50s -> 232.96s]  The unnormalized weight is the integral of this function, whereas the densities for these
[232.96s -> 236.24s]  hypotheses are proportional to the same function.
[236.24s -> 240.16s]  If you look at the densities, you can see that it is the same type of update
[240.16s -> 244.46s]  that we have performed previously, and the only difference is that the predicted density
[244.46s -> 249.96s]  given θ1 to k-1 is used as a prior to the specific update.
[249.96s -> 254.24s]  It's illuminating to describe the update step under the assumptions that the predicted
[254.24s -> 259.32s]  density is Gaussian, the probability of detection is constant, and the object likelihood is
[259.32s -> 260.70s]  linear and Gaussian.
[260.70s -> 264.66s]  With very few adjustments, the equations are identical to the expressions that we have
[264.66s -> 265.66s]  seen previously.
[265.66s -> 269.80s]  Apart from a slight change in notation, the main difference is that the unnormalized
[269.80s -> 274.12s]  weights are scaled by the weight for θ1 to k-1.
[274.12s -> 280.06s]  The posterior density of Xk, given θ1 to k, is identical to the predicted density
[280.16s -> 284.24s]  when θk states that the object is undetected at time k.
[284.24s -> 291.62s]  If θk is greater than zero, the posterior density of Xk given θ1 to k has mean mu
[291.62s -> 292.84s]  and covariance p.
[292.84s -> 297.34s]  Since we are given the predicted density, it is sufficient to perform a single Kalman
[297.34s -> 301.34s]  filter update using Zθk as measurement.
[301.34s -> 305.02s]  The expressions for the unnormalized weights are similar to before.
[305.02s -> 310.36s]  To compute the predicted likelihood, we need the predicted measurement Z-bar and the predicted
[310.36s -> 315.88s]  measurement covariance S, and these are computed as part of the same Kalman filter update.
[315.88s -> 322.08s]  As usual, the predicted likelihood is small if Zk of θk is far from the predicted
[322.08s -> 323.08s]  measurement.
[323.08s -> 327.46s]  For completeness, here are the equations for the Kalman filter update that gives
[327.46s -> 331.58s]  you the predicted mean and covariance of the measurements, as well as the posterior
[331.58s -> 336.58s]  mean and covariance of the state, for a given sequence of data associations.
[336.58s -> 343.34s]  The equations look a bit more complicated due to the super indices θ1 to k-1 and θ1
[343.34s -> 347.00s]  to k, but it is really the standard Kalman filter update.
[347.00s -> 350.38s]  Let us visualize the equations to make things more concrete again.
[350.38s -> 355.74s]  The green curves show the predicted density at time 2 and the posterior at time 2.
[355.74s -> 360.66s]  In this case, the predicted density contains three hypotheses since we have two measurements
[360.66s -> 362.02s]  at time 1.
[362.02s -> 367.30s]  Since we only have one measurement at time 2, we have two possible associations at time
[367.30s -> 368.30s]  2.
[368.30s -> 371.16s]  Either the object was detected or it was undetected.
[371.16s -> 377.02s]  This gives us six possible sequences of associations that we need to consider, and we can visualize
[377.02s -> 381.18s]  how the different sequences contribute to the overall posterior.
[381.18s -> 385.72s]  The most likely hypothesis is that the object was detected at both time instances
[385.72s -> 391.48s]  and that the object measurements are 1.7 at time 1 and 1.3 at time 2.
[391.48s -> 397.12s]  The hypotheses that involve the measurements at minus 1.3 are very unlikely.
[397.12s -> 402.14s]  This is partly due to the prior mean at time 1 being 0.5, and partly because the
[402.14s -> 406.78s]  measurements at minus 1.3 and 1.3 are far apart.
[406.78s -> 412.00s]  More specifically, the predicted likelihood for the measurement at time 2 is very small
[412.00s -> 416.34s]  if we assume that minus 1.3 was the object detection at time 1.
[416.34s -> 417.90s]  We are now ready to wrap up.
[417.90s -> 422.44s]  We have learned about tricks to normalize mixtures of densities and a few other things.
[422.44s -> 428.08s]  Still, our main goal was to present a conceptual solution to recursively compute the posterior
[428.08s -> 429.50s]  at time k.
[429.50s -> 434.38s]  We have seen that the posterior can be decomposed into a summation that contains one term
[434.38s -> 437.40s]  for every possible data association sequence.
[437.40s -> 441.54s]  An important advantage with this is that we can use a simple Kalman filter to compute
[441.54s -> 445.06s]  the densities and weights if the models are linear in Gaussian.
[445.06s -> 449.18s]  Also, even when the models are not linear in Gaussian, we can often use an extended
[449.18s -> 454.10s]  Kalman filter or some other type of Gaussian filter to approximate the densities and the
[454.10s -> 455.10s]  weights.
[455.10s -> 459.70s]  This is very useful, but since the number of hypotheses grows very quickly, we still
[459.70s -> 462.72s]  need to introduce additional approximations.
[462.72s -> 464.02s]  And that is the upcoming topic.
