# Detected language: en (p=1.00)

[0.00s -> 8.82s]  As we saw earlier, we wish to minimize the sum of the negative log weights.
[8.82s -> 14.62s]  Given a hypothesis, the unnormalized association weights are given by these two expressions
[14.62s -> 20.58s]  for misdetection association and detection association, respectively.
[20.58s -> 28.14s]  So given hypothesis H, we define the log likelihood of misdetecting object I and
[28.14s -> 33.94s]  the log likelihood of associating object I to detection J as shown here.
[33.94s -> 39.78s]  So this is just the logarithm of the corresponding unnormalized association weights.
[39.78s -> 46.10s]  So if we consider an example with constant probability of detection, uniform clutter,
[46.10s -> 50.70s]  and linear Gaussian models, then we get the log likelihoods shown here.
[50.70s -> 58.14s]  And here, Z-hat is the predicted detection, and S is the innovation covariance.
[58.14s -> 64.46s]  So the goal is to minimize the sum of negative log weights, and this can be rewritten by
[64.46s -> 71.06s]  writing the association cost using these log likelihoods and an assignment matrix A,
[71.06s -> 76.46s]  provided that the assignment matrix A corresponds to the association theta.
[76.46s -> 81.34s]  So what we need now is a cost matrix that allows the cost to be written as the
[81.34s -> 88.46s]  trace of A transpose L, while also facilitating finding assignments that correspond to unique
[88.46s -> 90.22s]  valid associations.
[90.22s -> 96.50s]  We can achieve this by defining the cost matrix as an n-by-m-plus-n matrix, where
[96.50s -> 102.24s]  the left submatrix contains the negative likelihoods for the detections, and the right
[102.24s -> 108.44s]  submatrix contains the negative log likelihoods for the missed detections along the diagonal.
[108.44s -> 114.30s]  The off-diagonal elements in the right submatrix are equal to negative logarithm
[114.30s -> 120.28s]  of zero, which is actually undefined, but we set this equal to infinity.
[120.28s -> 124.80s]  So in other words, this corresponds to setting the likelihood to zero.
[124.80s -> 130.28s]  And by doing this, we avoid that the solver returns multiple assignments that correspond
[130.32s -> 132.68s]  to the same valid association.
[132.68s -> 138.76s]  And you should note that if we have a valid association, then the corresponding elements
[138.76s -> 142.68s]  in the assignment matrix A are always zero.
[142.68s -> 147.46s]  With an assignment matrix and a cost matrix, as on the previous slide, the assignment
[147.46s -> 153.44s]  cost can then be expressed as the trace of A transpose L, which is what we want
[153.44s -> 160.12s]  for an assignment problem, and any non-unique assignments will have a cost equal to infinity.
[160.12s -> 166.08s]  For an hypothesis, the association weight for the association matrix A and the cost
[166.08s -> 172.30s]  matrix L is the exponential function of the negative trace of A transpose L.
[172.30s -> 178.20s]  So let's take an example with two objects and a single measurement, where we have three
[178.20s -> 180.40s]  possible data associations.
[180.40s -> 186.44s]  The prior densities are Gaussian, the clutter is uniform, and the likelihood is Gaussian.
[186.44s -> 191.40s]  And for this example, we get the cost matrix that is shown here.
[191.40s -> 196.96s]  So we note that the single detection is slightly closer to the mean for the object
[196.96s -> 200.74s]  one than it is to the mean for object two.
[200.74s -> 206.80s]  And this means that associating to object one has a bit higher likelihood than associating
[206.80s -> 208.62s]  to object two.
[208.62s -> 210.68s]  And we can see this in the cost matrix.
[210.68s -> 216.80s]  The corresponding assignment cost is lower than it is for assigning to object two.
[216.80s -> 222.48s]  Earlier, we found the assignment matrices for the three valid data associations when
[222.48s -> 225.20s]  we have two objects and one measurement.
[225.20s -> 230.90s]  And using the cost matrix, we can compute the cost of the three different assignments.
[230.90s -> 236.56s]  And we can see that associating the detection to object one has lowest cost, and two
[236.56s -> 239.30s]  missed detections has highest cost.
[239.30s -> 244.50s]  And if we compute the corresponding normalized weights, so the probabilities of the data
[244.50s -> 249.92s]  associations, we see that the weight of associating the detection to object one has
[249.92s -> 254.70s]  highest probability, and the probability of two missed detections is lowest.
