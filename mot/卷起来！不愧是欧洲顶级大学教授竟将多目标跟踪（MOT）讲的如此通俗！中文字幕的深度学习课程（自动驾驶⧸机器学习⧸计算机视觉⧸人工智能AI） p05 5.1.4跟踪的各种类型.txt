# Detected language: en (p=1.00)

[0.00s -> 8.96s]  The first type of tracking is called point object tracking.
[8.96s -> 14.00s]  For this type, we have at most one detection per object per time step.
[14.00s -> 20.48s]  At most one means that we either get no detection or a single detection.
[20.48s -> 26.36s]  And in object tracking literature, this is often called the point object assumption.
[26.36s -> 29.40s]  So we can have a look at a simple illustration.
[29.40s -> 34.84s]  We have an autonomous vehicle, it has a forward facing sensor with a field of view illustrated
[34.84s -> 37.40s]  by the circular segment.
[37.40s -> 41.96s]  And in the field of view of the sensor, there are two vehicles.
[41.96s -> 45.48s]  In point object tracking, we get at most one detection.
[45.48s -> 49.80s]  So here this is illustrated by one of the vehicles being detected and the other is
[49.80s -> 52.96s]  not detected.
[52.96s -> 58.76s]  One example of point object tracking is the example that we've looked at previously.
[58.76s -> 61.28s]  Objects are detected in images.
[61.28s -> 64.92s]  Another example is aerospace surveillance using radar.
[64.92s -> 70.52s]  In fact, radar used for aerospace surveillance is an application that has been very common
[70.52s -> 77.20s]  historically and a lot of object tracking literature assumes point object tracking.
[77.20s -> 81.08s]  The second type of tracking is called extended object tracking.
[81.08s -> 85.44s]  In this case, it is still possible that we get no detections for an object, but
[85.44s -> 89.48s]  there can also be more than one detection per object.
[89.48s -> 93.40s]  So let's consider this simple illustration scenario again.
[93.40s -> 99.60s]  In extended object tracking, we have multiple detections per vehicle.
[99.60s -> 105.24s]  And that means that in extended object tracking, it is possible to use the multiple detections
[105.24s -> 111.56s]  to estimate the object's extent in addition to the position and motion parameters.
[111.56s -> 118.92s]  And by extent, we typically mean the shape and the size of the object.
[118.92s -> 123.04s]  So if we look at the vehicle on the left in this illustration, we see that the five
[123.04s -> 126.18s]  detections approximately form an L shape.
[126.18s -> 130.76s]  The size of this L shape is related to the length and the width of the vehicle.
[130.76s -> 135.00s]  And we can use the multiple detections to estimate the length and the width of the
[135.00s -> 136.88s]  vehicle.
[136.88s -> 141.88s]  If we consider instead the vehicle on the right, we only have two detections and there
[141.88s -> 147.80s]  are no detections located on either the left side or the right side of the vehicle.
[147.80s -> 152.42s]  And in this case, we cannot estimate the length of the vehicle, but we can estimate
[152.42s -> 154.88s]  the width.
[154.88s -> 161.44s]  So some examples of extended object tracking scenarios are when LIDAR sensors are used or
[161.44s -> 165.74s]  when automotive radar sensors are used.
[165.74s -> 169.88s]  The third type of tracking is called group object tracking.
[169.88s -> 176.80s]  For this type, we have several objects that are treated as a single entity, a group object.
[176.80s -> 181.58s]  Often each member of the group can be detected in the sensor data, and therefore there
[181.58s -> 186.48s]  is a possibility of multiple detections per group.
[186.48s -> 191.14s]  So one example of group object tracking is shown here on the right.
[191.14s -> 197.12s]  So a camera was positioned overlooking a footpath and pedestrians were detected in
[197.12s -> 198.72s]  the image data.
[198.72s -> 203.18s]  A ground plane projection of the detections is shown in the top image from a top-down
[203.18s -> 205.86s]  perspective or bird's eye view.
[205.86s -> 210.74s]  Also shown are the group estimates visualized as ellipses.
[210.74s -> 215.20s]  And each ellipse is meant to encircle the members of the group.
[215.20s -> 219.96s]  And the numbers printed next to each group is the number of detections that we expect
[219.96s -> 222.72s]  from that group.
[222.72s -> 228.16s]  And in the bottom image, the elliptical group estimates have been projected into the image
[228.16s -> 231.60s]  and are visualized as elliptical cylinders.
[231.60s -> 236.76s]  So as you can see, the single individual on the left of the image is also given a
[236.76s -> 238.16s]  group estimate.
[238.16s -> 244.76s]  So in this particular example, group is given a fairly broad interpretation where a single
[244.80s -> 251.48s]  person is still considered a group, albeit with a single group member.
[251.48s -> 256.64s]  Group tracking can be applied to many different sensors and in many different scenarios.
[256.64s -> 261.56s]  It could, for example, be that we are able to track individual objects, but we are
[261.56s -> 266.80s]  more interested in studying group behavior, and therefore we track groups rather than
[266.80s -> 269.04s]  individual objects.
[269.04s -> 274.20s]  Alternatively, it could be the case that it is not possible to discern the individual
[274.24s -> 279.00s]  objects, and we are therefore forced to track groups.
[279.00s -> 283.84s]  The fourth type of tracking is called tracking with multipath propagation.
[283.84s -> 287.96s]  Again, we have a possibility of multiple measurements per object.
[287.96s -> 292.88s]  In this case, it's due to a phenomenon that is called multipath propagation.
[292.88s -> 297.76s]  And what this means is illustrated using examples on this slide.
[297.76s -> 302.28s]  We have over the horizon radar and tracking using automotive radar.
[302.28s -> 307.96s]  A short explanation of over the horizon radar is that the radar signal transmitted by the
[307.96s -> 312.96s]  ship on the right reflects off of different layers in the atmosphere, illustrated here
[312.96s -> 314.32s]  by the dashed curves.
[314.32s -> 319.60s]  It is therefore possible to reach objects that are located below the horizon and are not
[319.60s -> 322.72s]  in direct line of sight.
[322.72s -> 327.44s]  A second example of tracking with multipath propagation is vehicle tracking using
[327.44s -> 329.04s]  automotive radar.
[329.04s -> 334.32s]  Here, the radar signal is reflected off of the road surface and then hits, for example,
[334.32s -> 337.04s]  the undercarriage of the vehicle.
[337.04s -> 339.72s]  In this way, we get a radar detection.
[339.72s -> 344.60s]  However, it's not the closest point of the object that we detect, which in this case
[344.60s -> 347.36s]  would be the back of the vehicle.
[347.36s -> 351.96s]  And then the last type of tracking that we're going to discuss is called tracking with
[351.96s -> 354.68s]  unresolved objects.
[354.84s -> 359.48s]  In this type of tracking, we can have multiple objects per detection.
[359.48s -> 365.16s]  In other words, it's possible that multiple objects cause a single detection.
[365.16s -> 372.16s]  And one example where this can happen is vehicle tracking with automotive radar.
[372.16s -> 377.12s]  Here we have an autonomous vehicle that is traveling on a highway, and this highway
[377.12s -> 380.56s]  has multiple lanes going in the same direction.
[380.56s -> 385.44s]  So in front of the autonomous vehicle, there are two cars that travel at approximately
[385.44s -> 388.72s]  the same speed right next to each other.
[388.72s -> 394.12s]  In such a scenario, it sometimes happens that the detector only gives a single detection
[394.12s -> 399.76s]  for the two vehicles, and that this detection is located in between the two vehicles,
[399.76s -> 403.56s]  as illustrated here.
[403.56s -> 407.90s]  This phenomenon is also called tracking with merged measurements.
[407.90s -> 411.10s]  The radar signal reflects off of both vehicles.
[411.10s -> 416.94s]  However, in the detector, the reflected signal energies become merged, and the detector
[416.94s -> 421.86s]  only gives a single detection for the two vehicles.
[421.86s -> 426.42s]  So, we have discussed the following.
[426.42s -> 432.22s]  Point object tracking, extended object tracking, group object tracking, tracking with multipath
[432.22s -> 438.42s]  propagation, and lastly, tracking with unresolved measurements.
[438.42s -> 443.98s]  In this course, we're going to focus on the first type, which is point object tracking.
[443.98s -> 446.62s]  Here we have at most one measurement per object.
[446.62s -> 452.78s]  That is, each object is either not detected at all or detected by one measurement.
[452.78s -> 457.02s]  The other four types of tracking are outside the scope of this course.
[457.02s -> 461.46s]  The point object tracking that we teach in the course, however, can be generalized
[461.50s -> 464.98s]  in a straightforward way to the other types of tracking.
[464.98s -> 469.58s]  So note that extended object tracking, group object tracking, and tracking with multipath
[469.58s -> 474.14s]  propagation all involve multiple detections per object.
[474.14s -> 480.46s]  However, the modeling requirements within these different types of tracking are different.
[480.46s -> 486.50s]  And lastly, tracking with unresolved or merged measurements involve multiple objects per
[486.50s -> 488.54s]  detection.
