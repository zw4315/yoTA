# Detected language: en (p=1.00)

[0.00s -> 8.92s]  The conceptual idea behind the J-PDA update can be explained as follows.
[8.92s -> 14.76s]  We start with an independent prior density, and the marginal posterior for object i can
[14.76s -> 18.40s]  be described using the marginal association probabilities.
[18.40s -> 24.32s]  So we have beta i zero times the prior density, because without a detection, there
[24.32s -> 27.02s]  is nothing to update the prediction with.
[27.02s -> 32.64s]  And then we add a sum over all measurements, where we have beta i j times the posterior
[32.64s -> 37.74s]  density that results from updating the prior for object i with measurement j.
[37.74s -> 42.34s]  So you should note that these marginal posteriors are multimodal.
[42.34s -> 44.86s]  In other words, they are mixture densities.
[44.86s -> 51.02s]  Next, for each object, we merge the marginal posterior using some merging function that
[51.02s -> 54.38s]  merges the densities into a single density.
[54.38s -> 61.46s]  The typical choice here is to use moment matching that minimizes the Kullback-Leibler divergence.
[61.46s -> 67.06s]  For Gaussian densities, this corresponds to matching the mean and matching the covariance.
[67.06s -> 73.38s]  And lastly, the posterior density for the n objects is the product of the merged marginal
[73.38s -> 74.38s]  posteriors.
[74.38s -> 80.06s]  And the basics of the J-PDA recursion is that for each time step, again, we start
[80.06s -> 83.26s]  with a Chapman-Kolmogorov prediction for each object.
[83.26s -> 87.76s]  If we have Gaussian object densities and linear Gaussian transition density, then this
[87.76s -> 90.70s]  comes down to the Kalman filter prediction.
[90.70s -> 95.98s]  Next, in the update, we start by computing the marginal association probabilities.
[95.98s -> 101.14s]  And given the association probabilities for each object, we compute the merged marginal
[101.14s -> 102.14s]  posterior.
[102.14s -> 106.34s]  We will show later what the merged mean and covariances are if we have Gaussian
[106.34s -> 110.02s]  object densities and linear Gaussian measurement model.
[110.02s -> 113.86s]  Just like in GNN, we may wish to compute object estimates.
[113.86s -> 118.58s]  And for J-PDA filters, the expected value is a common choice for the estimator.
[118.58s -> 123.98s]  Again, for Gaussian densities, this is a convenient choice because the expected value
[123.98s -> 127.18s]  is one of the two parameters of the Gaussian density.
[127.18s -> 131.34s]  We can show what the predicted and updated merged Gaussian parameters are for linear
[131.34s -> 132.34s]  Gaussian models.
[132.34s -> 137.82s]  So we let Pd be constant, the clutter intensity is uniform, and the measurement likelihood
[137.82s -> 140.92s]  and transition density are linear and Gaussian.
[140.92s -> 145.54s]  If the initial prior is Gaussian, then the posterior density will be parameterized by
[145.54s -> 148.16s]  the mean and the covariance for each object.
[148.16s -> 151.62s]  For the prediction, the Kalman prediction is used.
[151.62s -> 155.18s]  Because we have seen this many times already, we will not show it again.
[155.18s -> 159.74s]  Instead we will focus on the merged mean and covariance that we have in the update.
[159.74s -> 165.86s]  So given the predicted parameters, the updated mean for object i is computed by first
[165.90s -> 171.22s]  computing an innovation for each measurement j, denoted epsilon ij.
[171.22s -> 178.22s]  Next, an expected innovation called epsilon is computed as the sum of beta ij times
[178.22s -> 179.66s]  epsilon ij.
[179.66s -> 185.06s]  And the updated merged mean is given by the Kalman update with the expected innovation.
[185.06s -> 191.30s]  You should note that if the marginal probability of no association, beta i0, is very large,
[191.30s -> 197.34s]  or almost equal to 1, then the marginal probabilities for the measurement will be almost zero,
[197.34s -> 199.60s]  because they all have to sum to one.
[199.60s -> 205.26s]  And then the expected innovation will be a vector where all elements are almost zero.
[205.26s -> 210.98s]  And it then follows that the merged mean is equal more or less to the predicted mean.
[210.98s -> 216.10s]  And this is what we expect if the probability of no association is very high.
[216.10s -> 219.74s]  The updated merged covariance is the sum of three parts.
[219.74s -> 226.14s]  So first, we have beta i0 times the prior covariance, so the probability of no association
[226.14s -> 230.78s]  and the covariance that we have if there is no measurement that we can update with.
[230.78s -> 235.46s]  Second is 1 minus beta i0 times p bar.
[235.46s -> 240.02s]  This is the probability of associating to any of the detections and the covariance
[240.02s -> 243.34s]  that results from updating with a measurement.
[243.34s -> 245.80s]  And the third part is p tilde.
[245.80s -> 251.94s]  If the gated measurements are quite similar, or close together, their corresponding innovations
[251.94s -> 257.60s]  will also be similar, and their spread will therefore be small, leading to a small p tilde.
[257.60s -> 262.80s]  Similarly, if the gated measurements are different, their innovations will be different,
[262.80s -> 266.24s]  and this will lead to a large spread and a large p tilde.
[266.24s -> 271.16s]  So it can be shown that this update for the mean and the covariance matches the mean
[271.16s -> 274.76s]  and the covariance of the marginal posterior mixture density.
[274.76s -> 280.12s]  But the mathematics of this are quite tedious, so we will not show this in this video.
[280.12s -> 283.76s]  We can illustrate the J-PDA merging using an example.
[283.76s -> 287.72s]  So here we have the six objects and 15 measurements again.
[287.72s -> 293.40s]  This time, we're going to focus on object six and the two measurements that fall inside its gate.
[293.40s -> 298.62s]  For this object, the prior density is now shown in blue, and we have two detections
[298.62s -> 300.36s]  shown as red squares.
[300.36s -> 306.76s]  We have marginal association probabilities for mist detection equal to 0.09,
[306.76s -> 316.56s]  association to measurement one equal to 0.77, and association to measurement 15 equal to 0.14.
[316.56s -> 320.04s]  Here we show the Gaussians that would result from mist detection,
[320.04s -> 324.76s]  association to measurement one, and association to measurement 15.
[324.76s -> 329.16s]  And the width of the line that shows the covariance matrix is proportional to the marginal
[329.16s -> 330.92s]  association probability.
[330.92s -> 334.72s]  So we have the largest width for the association to measurement one,
[334.72s -> 338.88s]  since that probability was highest, 0.77.
[338.88s -> 345.96s]  And lastly, we have the merged mean and covariance shown by an orange triangle and an orange ellipse.
[345.96s -> 349.96s]  The merged mean is closest to the mean resulting from measurement one,
[349.96s -> 354.48s]  because that association probability was highest, 0.77.
[354.48s -> 360.20s]  The merged covariance is larger than the covariance for association to measurement one due to
[360.20s -> 366.56s]  the fact that with probability 0.23, the correct association was either measurement
[366.56s -> 369.44s]  15 or a mist detection.
[369.44s -> 374.84s]  We noted earlier that computing the marginal association probabilities involved a sum over
[374.84s -> 380.52s]  all valid associations, and as we know, this can have very high computational cost
[380.52s -> 382.52s]  or even be intractable.
[382.52s -> 387.36s]  In object tracking literature, some methods can be found for approximating the marginal
[387.36s -> 389.24s]  association probabilities.
[389.24s -> 396.12s]  For example, there are the methods called cheap JPDA, suboptimal JPDA, and fast JPDA.
[396.12s -> 400.16s]  It goes beyond the scope of this course to explain these different methods.
[400.16s -> 403.08s]  But if you're interested, you can look into them.
[403.08s -> 408.52s]  Another way to approximate the marginal association probabilities is to use a method to find
[408.52s -> 413.20s]  the M best associations, and then use only those associations.
[413.20s -> 420.16s]  So using this method, the larger we choose M, the more accurate the approximation of beta ij will be.
