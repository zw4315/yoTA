# Detected language: en (p=1.00)

[0.00s -> 15.00s]  In this video, we present the prediction and update equations under the assumption that the posterior at time k-1 is a Gaussian mixture with hypotheses indexed from 1 to capital H k-1.
[15.00s -> 27.00s]  At time k, we obtain a posterior, which is a Gaussian mixture, and we index the hypotheses from 1 to the total number of hypotheses.
[27.00s -> 34.00s]  The equations presented here are closely related to the conceptual solution to the prediction and update steps,
[34.00s -> 43.00s]  and the difference is mainly that we index the hypotheses in a different manner that is more suitable for developing Gaussian sum filters.
[43.00s -> 55.00s]  In Gaussian sum filtering, we assume that the posterior at time k-1 is a Gaussian mixture with capital H k-1 terms, weights W k-1 of H k-1,
[55.00s -> 63.00s]  and Gaussian densities P k-1 given k-1 of H k-1 of X k-1.
[63.00s -> 68.00s]  Using derivations similar to what you've seen in videos about the conceptual solution,
[68.00s -> 75.00s]  one can show that the predicted density is a Gaussian mixture with the same number of terms and the same weights,
[75.00s -> 78.00s]  whereas the density for every hypothesis has changed.
[78.00s -> 85.00s]  One can also show that the posterior density at time k is a Gaussian mixture with updated weights and densities,
[85.00s -> 92.00s]  and that we obtain MK plus 1 new hypotheses for every hypothesis at time k-1.
[92.00s -> 98.00s]  As usual, the posterior before we introduce new approximations is denoted with the sign breathe.
[98.00s -> 105.00s]  In this video, we present the equations for the predicted density P k given k-1 for a given hypothesis,
[105.00s -> 109.00s]  as well as the updated weights and densities in P breathe.
[109.00s -> 114.00s]  To obtain the predicted density for a given hypothesis, we can use the Chapman-Kolmogorov equation,
[114.00s -> 122.00s]  stating that the predicted density is the motion model pi k times the posterior at time k-1,
[122.00s -> 126.00s]  integrated over all possible states at time k-1.
[126.00s -> 131.00s]  In the special case where the posterior for a given hypothesis is Gaussian at time k-1,
[131.00s -> 136.00s]  and the motion model is linear and Gaussian, the predicted density is also Gaussian.
[136.00s -> 142.00s]  Further, its mean and covariance are given by the Kalman filter prediction equations,
[142.00s -> 148.00s]  meaning that the predicted mean is f k-1 times the mean at time k-1,
[148.00s -> 153.00s]  and the predicted covariance is f p f transpose plus q.
[153.00s -> 159.00s]  Note that the equations presented on this slide are completely analogous to the prediction in the conceptual solution.
[159.00s -> 167.00s]  In the update step, we obtain a new hypothesis h k, for every pair of hypotheses h k-1 and theta k.
[167.00s -> 170.00s]  This is again analogous to the conceptual solution.
[170.00s -> 178.00s]  The difference is that instead of h k-1, we then had a sequence of associations up to time k-1,
[178.00s -> 183.00s]  and instead of h k, we had a sequence of associations up to time k.
[183.00s -> 188.00s]  The weight for a combination of hypotheses h k-1 and theta k,
[188.00s -> 195.00s]  is proportional to the weight of h k-1 times this integral if theta k is equal to zero,
[195.00s -> 199.00s]  and this expression if theta k is greater than zero.
[199.00s -> 201.00s]  These equations should also look familiar.
[201.00s -> 205.00s]  Note that the right hand side is the expression for the unnormalized weight,
[205.00s -> 209.00s]  and that we have written that the final weights are proportional to this expression.
[209.00s -> 213.00s]  To find the proportionality constant and normalize the weights,
[213.00s -> 219.00s]  we should sum the right hand side over all combinations of h k-1 and theta k.
[219.00s -> 223.00s]  We also want to find the posterior density, given a hypothesis h k.
[223.00s -> 228.00s]  That is, given a combination of h k-1 and theta k.
[228.00s -> 234.00s]  When theta k is equal to zero, that density is simply proportional to the predicted density,
[234.00s -> 238.00s]  given h k-1 times one minus p d.
[238.00s -> 245.00s]  When theta k is greater than zero, that density is instead proportional to the predicted density times p d,
[245.00s -> 250.00s]  times the measurement likelihood g k for z theta k.
[250.00s -> 252.00s]  We have seen similar equations before,
[252.00s -> 256.00s]  and under certain assumptions they simplify into factors and densities
[256.00s -> 259.00s]  that we can obtain from a Kalman filter update.
[259.00s -> 264.00s]  The specific assumptions needed for the equations to simplify are that p d is constant,
[264.00s -> 268.00s]  and that the object likelihood g k is linear in Gaussian.
[268.00s -> 272.00s]  The expression for the weight of h k then takes the following form.
[272.00s -> 278.00s]  If theta k is zero, it is the weight of h k-1 times one minus p d.
[278.00s -> 283.00s]  And if theta k is greater than zero, it is the weight of h k-1 times p d,
[283.00s -> 287.00s]  times the predicted likelihood, divided by the clouded intensity.
[287.00s -> 293.00s]  Also, under these assumptions, the posterior density given h k is a Gaussian density.
[293.00s -> 297.00s]  If theta k is equal to zero, this Gaussian density has the same mean and covariance
[297.00s -> 301.00s]  as the predicted density under hypothesis h k-1.
[301.00s -> 307.00s]  If theta k is greater than zero, its mean and covariance are obtained using a standard Kalman filter update,
[307.00s -> 312.00s]  under the assumptions that the predicted density has the moments defined by h k-1,
[312.00s -> 316.00s]  and that z theta k is the object measurement.
[316.00s -> 321.00s]  All of these equations are analogous to the ones that we found in the conceptual solution,
[321.00s -> 327.00s]  where we also obtained m k plus one new hypothesis for every predicted hypothesis.
[327.00s -> 331.00s]  On the previous slides, we described equations that can be used to compute
[331.00s -> 335.00s]  the weight, mean and covariance of any hypothesis h k.
[335.00s -> 341.00s]  In principle, the weight, mean and covariance is enough to understand what the hypothesis is.
[341.00s -> 345.00s]  But to store it in a computer, it is useful to give h k a number
[345.00s -> 351.00s]  and refer to it as, say, hypothesis number one, five or ten.
[351.00s -> 354.00s]  How we number the hypothesis is in some sense arbitrary,
[354.00s -> 357.00s]  as long as each hypothesis has its own number.
[357.00s -> 361.00s]  Here, we also want to ensure that the hypotheses are numbered
[361.00s -> 364.00s]  from one up to the total number of hypotheses.
[364.00s -> 370.00s]  Here are two possible ways to select the number for h k that both have reasonable properties.
[370.00s -> 372.00s]  As an example, we can look at the first alternative
[372.00s -> 377.00s]  and see how that behaves when there are two hypotheses at time k minus one
[377.00s -> 380.00s]  and only one measurement at time k.
[380.00s -> 383.00s]  This means that theta k can take the value zero or one
[383.00s -> 387.00s]  and that there are four possible hypotheses at time k.
[387.00s -> 390.00s]  Using the first alternative to assign the numbers to h k,
[390.00s -> 395.00s]  we see that when h k minus one is one and theta k is zero,
[395.00s -> 397.00s]  h k takes the value one.
[397.00s -> 401.00s]  When h k minus one is two and theta k is zero,
[401.00s -> 403.00s]  h k takes the value two.
[403.00s -> 406.00s]  When h k minus one is one and theta k is one,
[406.00s -> 408.00s]  h k takes the value three.
[408.00s -> 413.00s]  Finally, when h k minus one is two and theta k is one,
[413.00s -> 415.00s]  h k takes the value four.
[415.00s -> 419.00s]  We note that every pair of hypotheses, h k minus one and theta k,
[419.00s -> 423.00s]  maps to a unique number h k between one and four.
[423.00s -> 427.00s]  And in general, every pair maps to a unique number h k
[427.00s -> 430.00s]  between one and the total number of hypotheses,
[430.00s -> 432.00s]  which is the property that we want.
[432.00s -> 434.00s]  What I've presented here is really just a way
[434.00s -> 437.00s]  to bookkeep the hypotheses in a computer.
[437.00s -> 439.00s]  But it can be helpful to know about
[439.00s -> 442.00s]  if you want to implement these algorithms from scratch.
