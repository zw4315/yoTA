# Detected language: en (p=1.00)

[0.00s -> 8.00s]  The nearest neighbor algorithm approximates the posterior as Gaussian by ignoring all but one hypothesis.
[8.00s -> 14.00s]  This has the disadvantage that we underestimate the posterior uncertainties.
[14.00s -> 26.00s]  In this video, we present the probabilistic data association algorithm which approximates the posterior as Gaussian by instead merging all hypotheses in the posterior.
[26.00s -> 35.00s]  Compared to nearest neighbor, the probabilistic data association algorithm does not underestimate the uncertainties to the same extent.
[35.00s -> 44.00s]  Like the nearest neighbor algorithm, the probabilistic data association algorithm approximates the posterior as Gaussian after both the update and the prediction step.
[44.00s -> 51.00s]  Of course, if the models are linear in Gaussian, the prediction step can be performed without introducing additional approximations.
[51.00s -> 60.00s]  We will again assume that we have a constant pd and a linear in Gaussian object likelihood gk, whereas we can handle general intensity functions.
[60.00s -> 72.00s]  Under the assumption that the predicted density is this Gaussian density given by the PDA prediction, the posterior after the update denoted pb is a Gaussian mixture with mk plus one terms.
[72.00s -> 80.00s]  These weights and densities are computed in the same way as the weights and densities in pb for the nearest neighbor algorithm.
[80.00s -> 85.00s]  But they generally take different values since they are computed based on a different predicted density.
[85.00s -> 96.00s]  To clarify that they are different, we could have introduced a super index PDA on w theta k and p theta k, but I've omitted that here to keep the notation simpler.
[96.00s -> 102.00s]  Again, the question is how to approximate this Gaussian mixture as a Gaussian density.
[102.00s -> 113.00s]  The basic idea in probabilistic data association is to use merging and approximate the posterior as a Gaussian density with the same mean and covariance as pb.
[113.00s -> 120.00s]  That is, we replace the Gaussian mixture with a Gaussian density that has the same mean and covariance as the Gaussian mixture.
[120.00s -> 129.00s]  In other words, we set the posterior mean x-bar k given k to the expected value of xk, where xk is distributed according to pb.
[129.00s -> 137.00s]  Also, the posterior covariance pk given k is the covariance of xk, where xk is distributed according to pb.
[137.00s -> 143.00s]  Of course, we then approximate the posterior distribution as Gaussian with this mean and this covariance.
[143.00s -> 150.00s]  One can show that this minimizes something called the Kullback-Leibler divergence between pb and the Gaussian approximation.
[150.00s -> 156.00s]  The Kullback-Leibler divergence is commonly used to measure similarity between densities.
[156.00s -> 163.00s]  And this result tells us that this is the best among all Gaussian approximations, in this specific sense.
[163.00s -> 170.00s]  Still, this is mostly a side note, and if you haven't heard about the Kullback-Leibler divergence before, you can ignore this comment.
[170.00s -> 175.00s]  Anyway, the basic idea behind the PDA algorithm is to select the moments like this.
[175.00s -> 180.00s]  But to obtain an algorithm, we still need to figure out how to compute these two moments.
[180.00s -> 194.00s]  One can show that the expected value of this Gaussian mixture is the sum over the expected values x hat theta k of xk given theta k, weighted by the probabilities w theta k of the different components.
[194.00s -> 199.00s]  You can think of this as the expected value of the expected value of the Gaussian components.
[199.00s -> 204.00s]  The covariance of a Gaussian mixture is slightly more involved and contains two parts.
[204.00s -> 209.00s]  The first is the weighted sum of the covariances of the different Gaussian components.
[209.00s -> 211.00s]  You can think of this as the average covariance.
[211.00s -> 219.00s]  The second part is a weighted sum over the squared differences between the mean of the mixture and the mean of the individual component.
[219.00s -> 224.00s]  This part can be viewed as a measure on how much the means are spread out.
[224.00s -> 230.00s]  Let us look at an example where we have a Gaussian mixture and want to compute its mean and covariance.
[230.00s -> 238.00s]  Suppose the Gaussian mixture is such that with probability 0.5, we have a Gaussian density with mean minus 3 and variance 2.
[238.00s -> 244.00s]  And with probability 0.5, we have a Gaussian density with mean 3 and variance 2.
[244.00s -> 246.00s]  This density is illustrated in this figure.
[246.00s -> 257.00s]  According to the result on the previous slide, the expected value of x is then 0.5 times minus 3 plus 0.5 times 3, which is 0.
[257.00s -> 265.00s]  Considering that x takes values around minus 3 and 3 with equal probability, it makes sense that the expected value is 0.
[265.00s -> 272.00s]  The covariance of x separates into two parts where the first is the average variance of the two components.
[272.00s -> 276.00s]  Since both have variance 2, the expected value should be 2.
[276.00s -> 283.00s]  And if we plug in the values, this part is 0.5 times 2 plus 0.5 times 2.
[283.00s -> 286.00s]  And as expected, the result is 2.
[286.00s -> 295.00s]  The second part is the spread of the means, which is 0.5 times 3 squared plus 0.5 times minus 3 squared.
[295.00s -> 302.00s]  Here 3 is the distance between the mean of the Gaussian mixture, which is 0, and the mean of the individual components.
[302.00s -> 304.00s]  This part sums to 9.
[304.00s -> 308.00s]  And this reflects the fact that the two components are fairly well separated.
[308.00s -> 312.00s]  The variance of x is 9 plus 2, which is 11.
[312.00s -> 319.00s]  And as you can see, the majority of this variance is due to the spread in the mean of the two components in the mixture.
[319.00s -> 326.00s]  If we want, we can now approximate p of x by a Gaussian density p-hat, which has the same moments as p.
[326.00s -> 329.00s]  The density p-hat is illustrated in the figure.
[329.00s -> 338.00s]  Obviously, any Gaussian approximation will introduce errors, but at least this approximation seems to capture the Gaussian mixture reasonably well.
[338.00s -> 345.00s]  Coming back to PDA filtering, we can summarize the steps that we need to perform in order to implement the algorithm.
[345.00s -> 348.00s]  The update step can be separated into three steps.
[348.00s -> 355.00s]  First, we compute the parameters of the Gaussian mixture, which are the weights, means, and covariances of the different components.
[355.00s -> 360.00s]  Second, we compute the posterior mean using a weighted sum of the individual means.
[360.00s -> 367.00s]  Third, we compute the covariance by computing both the average covariance and the average spread of the means.
[367.00s -> 374.00s]  As you can see, assuming that we know how to compute the Gaussian mixture, the PDA algorithm is very simple to implement.
