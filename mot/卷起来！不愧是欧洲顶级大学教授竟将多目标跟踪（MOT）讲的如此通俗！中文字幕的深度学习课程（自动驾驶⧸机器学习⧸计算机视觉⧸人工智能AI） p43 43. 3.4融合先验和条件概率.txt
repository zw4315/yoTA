# Detected language: en (p=1.00)

[0.00s -> 11.76s]  The data association prior, that is the density of the association theta and the number of
[11.76s -> 17.14s]  measurements m, given the object states, can be expressed as shown here.
[17.14s -> 19.46s]  This prior has three parts.
[19.46s -> 24.28s]  The first part gives us the probability of detecting a specific set of all the n
[24.28s -> 25.28s]  objects.
[26.00s -> 31.64s]  And you should know that this is not just the probability of detecting mo objects out of the n objects.
[31.64s -> 37.20s]  This is the probability of detecting a specific set of objects and miss detecting the rest.
[37.20s -> 43.52s]  So it points out which specific objects are detected and which are miss detected.
[43.52s -> 48.40s]  The second part is the probability of having mc clutter detections.
[48.40s -> 51.88s]  And as we have discussed earlier, the clutter is Poisson.
[51.96s -> 57.96s]  So the probability of having mc clutter detections is given by the Poisson pmf.
[57.96s -> 63.12s]  And lastly, we have the probability of the specific association, which is one over the
[63.12s -> 68.52s]  number of ways to select which objects are detected, that's given by the binomial
[68.52s -> 74.64s]  coefficient m over mo and multiplied by the number of ways we can associate given
[74.64s -> 77.12s]  by mo factorial.
[77.12s -> 79.92s]  We can also rewrite this expression as follows.
[79.92s -> 83.48s]  It might not be easy to see why we can rewrite this way.
[83.48s -> 87.52s]  So you may wish to verify this using pen and paper once you have finished watching
[87.52s -> 90.08s]  this video.
[90.08s -> 95.80s]  For the association condition likelihood, we make a simplifying assumption, which is that
[95.80s -> 101.56s]  given the data association theta and the number of measurements m, all of the measurements
[101.56s -> 103.52s]  are independent.
[103.52s -> 108.38s]  And under this assumption, we get that the density of the measurements given the object
[108.38s -> 113.82s]  states, the data association and the number of measurements can be described as a product
[113.82s -> 120.86s]  over the measurements associated to clutter multiplied by a product over the objects associated
[120.86s -> 123.14s]  to detections.
[123.14s -> 127.58s]  As we saw in the beginning of this section, the n object measurement likelihood can be
[127.58s -> 132.46s]  expressed as a sum over data associations, where we have the association conditioned
[132.46s -> 135.46s]  likelihood and the association prior.
[136.22s -> 143.10s]  So if we insert the expressions from the two previous slides, we get this expression shown here.
[143.10s -> 146.80s]  This can be rewritten as the following expression.
[146.80s -> 149.54s]  We have a sum over the data associations.
[149.54s -> 154.58s]  Then we have e to the power of negative lambda bar divided by m factorial.
[154.58s -> 161.38s]  We have a product of the clutter Poisson intensity for the detections associated to clutter.
[161.42s -> 166.62s]  We have a product of the probability of missed detection for the objects that are not associated
[166.62s -> 168.42s]  to any detection.
[168.42s -> 173.74s]  And lastly, we have a product of the probability of detection and the measurement likelihood
[173.74s -> 179.14s]  g for the objects that have been associated to a detection.
[179.14s -> 184.30s]  And again, it might not be simple to see that these two expressions are actually equivalent.
[184.30s -> 188.70s]  Once you've finished watching the video, you can use pen and paper to verify that
[188.70s -> 192.22s]  these expressions are indeed equivalent.
[192.22s -> 197.98s]  So we can see that for each data association theta, the measurement likelihood for n objects
[197.98s -> 201.70s]  that we have derived accounts for all of the n objects.
[201.70s -> 206.78s]  We have the product over the missed detections and the product over the detections associated
[206.78s -> 207.78s]  to objects.
[207.78s -> 211.14s]  And together, this is all of the n objects.
[211.14s -> 216.26s]  We can also see that the likelihood that we have derived accounts for all the measurements.
[216.26s -> 220.26s]  We have the product over the clutter and the product over the measurements associated
[220.26s -> 221.26s]  to objects.
[221.26s -> 225.42s]  And together, this is all of the measurements.
[225.42s -> 230.38s]  And lastly, from the sum over all possible data associations, we have that the measurement
[230.38s -> 235.62s]  likelihood that we have derived considers all possible ways to assign detections to
[235.62s -> 237.18s]  the objects.
[237.18s -> 244.30s]  So we can take this n object measurement likelihood and rewrite it in a similar way as we rewrote
[244.30s -> 247.46s]  the measurement likelihood for a single object.
[247.46s -> 252.02s]  And the simple trick is to multiply by one.
[252.02s -> 256.62s]  So here we have the product of the Poisson intensities for the measurements that we
[256.62s -> 262.82s]  have associated to objects and then divided by itself.
[262.82s -> 267.30s]  And this allows us to express the n object measurement likelihood with the product of
[267.30s -> 272.94s]  the Poisson intensities for all measurements and for the measurements associated to objects
[272.94s -> 276.62s]  we have now divided by the Poisson intensity.
[276.62s -> 283.26s]  And what we see now is that for each data association theta, we have e to the power
[283.26s -> 288.90s]  of negative lambda bar divided by m factorial and then a product over all measurements
[288.90s -> 291.20s]  of the Poisson intensity.
[291.20s -> 295.48s]  So this is independent of the data association.
[295.48s -> 300.94s]  So this allows us to write the n object measurement likelihood up to proportionality
[300.94s -> 303.62s]  as the expression shown here.
[303.62s -> 307.50s]  And you should note that this expression includes both the missed detections and
[307.50s -> 308.42s]  the detections.
