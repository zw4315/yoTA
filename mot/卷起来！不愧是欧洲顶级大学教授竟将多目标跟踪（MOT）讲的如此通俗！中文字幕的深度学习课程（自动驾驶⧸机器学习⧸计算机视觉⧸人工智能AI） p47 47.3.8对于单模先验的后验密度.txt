# Detected language: en (p=1.00)

[0.00s -> 6.00s]  Hi! We've previously talked about the prior in n-object tracking as well as the measurement likelihood.
[6.00s -> 11.00s]  Now we're going to have a look at what the base posterior density is in n-object tracking.
[11.00s -> 15.00s]  Let's consider a unimodal independent prior.
[15.00s -> 21.00s]  And that means that the prior for the n-object state is a product of independent densities for each object.
[21.00s -> 28.00s]  Earlier, we derived an expression for the measurement likelihood in n-object tracking, which is reproduced here.
[28.00s -> 35.00s]  We have a sum over the data associations with a product for the missed detections and a product for the detections.
[35.00s -> 43.00s]  The posterior density is given by the base update and is proportional to the product of the likelihood and the prior.
[43.00s -> 50.00s]  If we insert the expressions on the previous slide into the base posterior, then we get the expression shown here.
[50.00s -> 61.00s]  So, for each data association, there is a product over the missed detection likelihood, a product over the detection likelihood, and a product over the prior densities.
[61.00s -> 66.00s]  And we can rewrite this expression such that we have two products instead.
[66.00s -> 74.00s]  One with the missed detection likelihood times the corresponding prior, and one with the detection likelihood times the prior.
[74.00s -> 80.00s]  So, for each data association, we have missed detected objects and detected objects.
[80.00s -> 88.00s]  And we can understand this posterior density for the n-objects as being made up of unnormalized single-object densities.
[88.00s -> 92.00s]  So, naturally, we want the posterior density to be normalized.
[92.00s -> 101.00s]  And one way to obtain a normalized density is to use the normalization that we learned about earlier in single-object tracking.
[101.00s -> 110.00s]  So, to normalize a density, if p is proportional to some function g, then we can normalize p by dividing g by its integral.
[110.00s -> 120.00s]  So, in the posterior density, we have four missed detected objects that the density that we want to normalize is equal to 1 minus pd times the prior.
[120.00s -> 130.00s]  And the normalized posterior is then given by dividing this function g with w tilde, where w tilde is the integral of g.
[130.00s -> 142.00s]  And similarly, for detected objects, the density that we wish to normalize is pd times the likelihood g divided by the clutter intensity lambda and multiplied by the prior.
[142.00s -> 145.00s]  And we normalize this in the same way.
[145.00s -> 153.00s]  Using this normalization on the individual object density gives us the following posterior n-object density.
[153.00s -> 163.00s]  Where for each data association, we have a product of the weights w tilde and the normalized posterior densities pi theta i.
[163.00s -> 174.00s]  The weights w tilde are given by this expression, and the posterior densities for each object given the data association are given by this expression.
[174.00s -> 181.00s]  So, now we have expressed the posterior n-object density using normalized single-object densities.
[181.00s -> 187.00s]  However, we do not yet have a normalized n-object density, which is what we want.
[187.00s -> 194.00s]  To normalize the posterior n-object density, we start by writing it as a mixture of n-object densities.
[194.00s -> 200.00s]  We can write this product as two products, one for the weights and one for the single-object densities.
[200.00s -> 210.00s]  And the product of the weights for each single-object association can be defined as the weight for the n-object association.
[210.00s -> 221.00s]  And similarly, the product of the independent single-object posterior densities is an n-object posterior density conditioned on the data association theta.
[221.00s -> 226.00s]  We now have an unnormalized mixture density for the n-objects.
[226.00s -> 231.00s]  And we can obtain a normalized density by normalizing the weights as follows.
[231.00s -> 239.00s]  The normalized weight is given by the unnormalized weight w tilde divided by the sum of weights w tilde.
[239.00s -> 243.00s]  So finally, we have a normalized posterior density.
[243.00s -> 250.00s]  Now, we derive this expression by first normalizing the single-object posterior densities.
[250.00s -> 256.00s]  However, it's actually possible to normalize the posterior n-object density directly.
[256.00s -> 259.00s]  And we will have a look at how to do this now.
[259.00s -> 266.00s]  The base update is the likelihood times the prior divided by the normalizing factor p of z,
[266.00s -> 274.00s]  where the integral with respect to the n-object state is defined as an integral over each individual object state.
[274.00s -> 280.00s]  So the normalizing factor is the integral with respect to the likelihood and the prior.
[280.00s -> 289.00s]  And if we insert the expression for the likelihood, we see that the normalizing factor can be written as a sum over data associations,
[289.00s -> 296.00s]  where we have an integral of the joint measurement and data association likelihood and the prior.
[296.00s -> 306.00s]  And this is a marginalization of the joint measurement and data association density, where we marginalize the data association theta.
[306.00s -> 312.00s]  If we input the expressions for the likelihood and the prior, we get the following expression.
[312.00s -> 317.00s]  And earlier, we defined this as w tilde.
[317.00s -> 322.00s]  And if we compare to the previous expression for the normalized posterior weights,
[322.00s -> 329.00s]  we see that this normalizing factor is indeed in the denominator for the normalized weights.
[329.00s -> 336.00s]  Let's have a look at an example where we have a Gaussian prior and a linear Gaussian measurement likelihood.
[336.00s -> 340.00s]  We then get the following posterior with Gaussian object densities.
[340.00s -> 348.00s]  So here we have unnormalized object densities, and we have to apply the normalization trick that we just used.
[348.00s -> 352.00s]  Doing so gives us the mixture posterior shown here.
[352.00s -> 358.00s]  Given an association theta, for each object, the mean and the covariance are given by the Kalman update.
[358.00s -> 365.00s]  If a detection is associated to the object, in other words, if theta i is not equal to zero.
[365.00s -> 370.00s]  And if a detection is not associated, if theta i is equal to zero,
[370.00s -> 376.00s]  then the mean and the covariance for the object are just equal to the prior mean and covariance.
[376.00s -> 385.00s]  The unnormalized weight for the data association is equal to the probability of missed detection to the power of the number of missed detected objects
[385.00s -> 392.00s]  times the probability of detection divided by the clutter intensity to the power of the number of detected objects.
[392.00s -> 397.00s]  And lastly, for each detected object, we have the predicted likelihood,
[397.00s -> 402.00s]  where sad hat is the predicted measurement and S is the innovation covariance.
[402.00s -> 408.00s]  Just like we visualize the measurement likelihood, we can visualize the posterior density.
[408.00s -> 418.00s]  So we would like to consider the same scenario with two objects with scalar states and two measurements, minus 1.6 and 1 respectively.
[418.00s -> 423.00s]  For two objects and two measurements, we have seven possible data associations.
[423.00s -> 429.00s]  We have a measurement model for the probability of detection, 0.85.
[429.00s -> 434.00s]  The clutter intensity is equal to 0.3 from minus 5 to 5.
[434.00s -> 440.00s]  And the measurement likelihood, G, is Gaussian with variance 0.2.
[440.00s -> 454.00s]  In the example, the prior is Gaussian with mean minus 2.5 and variance 0.36 for the first object and mean 2.5 and variance 0.36 for the second object.
[454.00s -> 461.00s]  For the visualizations, for a given data association, we will have a look at the joint density for both objects,
[461.00s -> 465.00s]  as well as the marginal densities for each object.
[465.00s -> 468.00s]  We're not going to look at all seven data associations.
[468.00s -> 473.00s]  Instead, we will focus on three that have high posterior weights.
[473.00s -> 479.00s]  And we will also have a look at the full posterior, both the joint posterior and the marginals.
[479.00s -> 483.00s]  Starting with the prior, it looks like shown here.
[483.00s -> 489.00s]  On the left, we have the joint prior, and on the right, we have the corresponding marginal densities.
[489.00s -> 496.00s]  We can clearly see that the variances are equal for the two objects, but the means are different.
[496.00s -> 502.00s]  First, let's consider the data association where both objects are misdetected.
[502.00s -> 509.00s]  On the left here, we can see the prior, both the joint prior and the marginal prior.
[509.00s -> 514.00s]  In the middle, we have the joint likelihood for the measurements and this data association.
[514.00s -> 519.00s]  And because neither object is detected, the likelihood is uniform.
[519.00s -> 529.00s]  And finally, on the right, we have the posterior density for this data association, both the joint posterior and the marginal posteriors.
[529.00s -> 537.00s]  We see that the posterior is equal to the prior, which is natural given that neither object was associated to a measurement.
[537.00s -> 540.00s]  In other words, both were misdetected.
[540.00s -> 547.00s]  Note also that in the title for the marginal posterior, we have written the unnormalized weight, W tilde.
[547.00s -> 551.00s]  We have also plotted the measurements for this data association.
[551.00s -> 558.00s]  And because neither object is detected, both measurements belong to the clutter, denoted C.
[558.00s -> 567.00s]  Now, let's have a look at the data association where the first measurement is associated to the first object and the second object is misdetected.
[567.00s -> 574.00s]  Again, we have the prior on the left, the likelihood in the middle and the posterior on the right.
[574.00s -> 582.00s]  Here we see that in the posterior density for the first object, we have updated using the associated measurement.
[582.00s -> 588.00s]  And the mean is shifted closer to the associated measurement and the variance is smaller.
[588.00s -> 596.00s]  However, for the second object, which was not associated to any measurement, the posterior is equal to the prior.
[596.00s -> 606.00s]  Lastly, we have the data association where the first object is associated to the first measurement and the second object is associated to the second measurement.
[606.00s -> 614.00s]  In this case, both object densities are updated using the respective associated measurements.
[614.00s -> 621.00s]  If we look at the full posterior instead, we have the symmetric likelihood in the middle.
[621.00s -> 628.00s]  And on the right, we have the posterior, which is a mixture density, a mixture of Gaussians.
[628.00s -> 641.00s]  This mixture density is dominated by the associations that either associate measurement 1 to object 1 or measurement 2 to object 2 or associate both of them.
[641.00s -> 652.00s]  In the figure with the marginal posteriors, we have plotted both the Gaussian mixtures as solid lines and we have also plotted the Gaussian components using dashed lines.
[652.00s -> 661.00s]  Both marginal Gaussian mixtures have multiple mixture components. However, they are dominated by the two that can be seen easily.
[661.00s -> 670.00s]  For the first object, that is the component that corresponds to a misdetection and the component that corresponds to an association to measurement 1.
[670.00s -> 678.00s]  And for the second object, it is the components that correspond to a misdetection and an association to measurement 2.
[678.00s -> 684.00s]  We can also have a look at the normalized posterior weights shown here in the middle.
[684.00s -> 691.00s]  This shows that three of the seven data associations have weights more or less equal to zero.
[692.00s -> 702.00s]  And the reason for this is that for Gaussian models, associating to a measurement closer to the prior mean has higher likelihood than associating to a measurement further away.
[702.00s -> 708.00s]  If we increase the probability of detection, we get the results that are shown here.
[708.00s -> 719.00s]  We see that now the association of object 1 to measurement 1 and object 2 to measurement 2 has a much higher posterior weight, almost 0.8.
[719.00s -> 727.00s]  We can also see that the posterior weight of the data association in which both objects were misdetected is almost zero.
[727.00s -> 739.00s]  This is due to the fact that with a higher probability of detection, the likelihood of misdetections becomes lower and it follows that the corresponding normalized posterior weights are lower.
[739.00s -> 748.00s]  In the posterior mixture densities, we can see that the mixtures are now dominated by the Gaussian components that correspond to this association.
[748.00s -> 754.00s]  If we instead have a lower probability of detection, then we get the results shown now.
[754.00s -> 761.00s]  In this case, the data association where both objects are misdetected has highest posterior weight.
[761.00s -> 769.00s]  And if we look at the posterior Gaussian mixture density, we see that it no longer is dominated by just one Gaussian.
[769.00s -> 781.00s]  If we show these three different settings for this problem next to each other, we can see the differences both in terms of the posterior Gaussian mixtures and also in terms of the posterior weights.
[781.00s -> 790.00s]  We will return to this example scenario when we talk about different tracking algorithms and how they approximate the posterior mixture density.
