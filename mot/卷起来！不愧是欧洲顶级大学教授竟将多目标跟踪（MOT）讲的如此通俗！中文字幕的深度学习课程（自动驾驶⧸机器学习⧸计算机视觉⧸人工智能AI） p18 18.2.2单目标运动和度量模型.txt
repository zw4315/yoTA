# Detected language: en (p=1.00)

[0.00s -> 9.00s]  To develop our tracking algorithms, we use models, and in this video we present commonly used single object models for motion and measurements.
[9.00s -> 17.00s]  That is, we will describe models for how an object moves, as well as the distribution for the measurements that we receive from a single object.
[17.00s -> 24.00s]  To obtain a complete model, we also need a model for clutter, but that model is described later.
[24.00s -> 40.00s]  The models that we present here are very important in object tracking, and the single object measurement model, which now includes the possibility that an object may be undetected, is a standard model which is going to be used for almost the entire course.
[40.00s -> 49.00s]  We will start some of these videos with a few remarks on background knowledge that you may or may not be familiar with, and which is useful to know about to understand the material.
[49.00s -> 55.00s]  The Bernoulli distribution is one such example, since that is used to describe the measurement model in single object tracking.
[55.00s -> 64.00s]  A Bernoulli distribution can be used to represent, for instance, the successfulness of an experiment, where 1 means that it was successful, and 0 means that it failed.
[64.00s -> 74.00s]  It could also be used to represent the outcome, when you toss a possibly biased coin, where x equals 1 means heads, and x equals 0 means tails.
[74.00s -> 85.00s]  Importantly, it can also be used to represent the event of a detection, where x equals 1 indicates that the object is detected, and x equals 0 means that the object is not detected.
[85.00s -> 95.00s]  The Bernoulli distribution is actually central to both single object tracking and multi-object tracking, and will be used to model several different aspects in upcoming weeks.
[95.00s -> 106.00s]  In general, if x is Bernoulli distributed with probability p between 0 and 1, it takes the value 1 with probability p, and the value 0 with probability 1 minus p.
[106.00s -> 123.00s]  If we look at the probability mass function of x when p is 0.8, the probability that x is 0 is 0.2, the probability that x is 1 is 0.8, and the probability that it takes any other value, say 2, 3 or 4 is 0.
[123.00s -> 128.00s]  Let us start with the single object motion model, which actually hasn't changed since last week.
[128.00s -> 139.00s]  We use xk to denote the object state at time k, and assume that it is a Markov chain that evolves according to some motion model, p of xk given xk minus 1.
[139.00s -> 148.00s]  As you have already seen, we normally use p to denote the densities, and you actually need to look at the input variables to know which densities we are referring to.
[148.00s -> 152.00s]  This gives us a compact notation that we normally find sufficiently clear.
[152.00s -> 163.00s]  However, for some densities that appear frequently, we also introduce a specific notation, and for the motion model we use pi k of xk given xk minus 1.
[163.00s -> 174.00s]  In difference to the p notation, pi k is only used to denote the motion model for a single object, and you don't need to look at the input variables to figure out that we are referring to the motion model.
[174.00s -> 182.00s]  As you know, it is common to assume that xk is some function fk minus 1 of xk minus 1, plus Gaussian noise with zero mean.
[182.00s -> 199.00s]  If the covariance matrix of the additive noise is denoted qk minus 1, this implies that pi k of xk given xk minus 1 is the Gaussian density with mean fk minus 1 of xk minus 1, and covariance qk minus 1, evaluated at xk.
[199.00s -> 210.00s]  To make this even more concrete, you can imagine that the motion model is a constant velocity model, constant acceleration model, or a bicycle model, but we are of course not limited to using those specific models.
[210.00s -> 221.00s]  The bottom line of this slide is that the motion model is of the same type as last week, and we also make the standard assumptions regarding conditional independently that you make in state space models.
[221.00s -> 230.00s]  Let us now look at the single object measurement model, that is, the model that we will use to describe the distribution of the object detections from a single object.
[230.00s -> 235.00s]  Note that this is not the complete measurement model, since we also need a model for the clutter detections.
[235.00s -> 242.00s]  However, in this video and the next, we will ignore clutter detections, and instead focus on the object detections.
[242.00s -> 246.00s]  We assume that the object is detected with probability pd of xk.
[246.00s -> 253.00s]  If detected, we receive a measurement denoted ok from the distribution p of ok given xk.
[253.00s -> 266.00s]  Similarly to the pi notation used for a single object motion model, we introduce a specific notation gk of ok given xk to denote the distribution of the detections from a single object.
[266.00s -> 275.00s]  Similarly to the motion model, it is common to assume that the measurement ok is a function hk of xk plus some Gaussian noise with zero mean.
[275.00s -> 289.00s]  If the covariance matrix of the additive noise is denoted rk, this implies that gk of ok given xk is a Gaussian density with mean hk of xk and covariance rk, evaluated at ok.
[289.00s -> 294.00s]  The dimensionality and adaptation of ok will depend on the sensor that we are using.
[294.00s -> 298.00s]  But you can for instance imagine that the sensor observes the range and angle to the object.
[298.00s -> 307.00s]  Now, it is important to note that the measurement model described here is quite different to the models that we discussed last week, since the object is not always detected.
[307.00s -> 316.00s]  This is illustrated to the right, where you can see red object detections of a car at all times except at the time in the middle, where the object is not detected.
[317.00s -> 326.00s]  It is useful to have a single variable that can represent both whether or not the object is detected, as well as the object measurement when it is detected.
[326.00s -> 331.00s]  During the first part of this course, we use matrix notations to represent all measurements.
[331.00s -> 336.00s]  If the object is detected, the matrix ok is identical to the object measurement.
[336.00s -> 348.00s]  If the object is not detected, we set ok to this symbol representing the event that the matrix does not contain any measurement vectors and we refer to this symbol as an empty matrix.
[348.00s -> 354.00s]  We use vertical bars on both sides of the matrix to denote the number of column vectors in the matrix.
[354.00s -> 373.00s]  Clearly, according to the model, ok can contain at most one vector and given xk, it is easy to see that the number of column vectors in ok is Bernoulli distributed and that it takes the value 1 with probability pd of xk and the value 0 with probability 1 minus pd of xk.
[373.00s -> 382.00s]  As you can see, the probability that an object generates more than one detection is assumed to be 0 and we refer to this as a point object model.
[382.00s -> 393.00s]  The model described here is the standard measurement model for single objects and it is the model that we will use during most of this course, even though we will later represent the measurements by sets.
[393.00s -> 399.00s]  As discussed last week, in some contexts objects can generate several detections.
[399.00s -> 408.00s]  We refer to models where objects can generate multiple detections as extended object models and we return to discuss such models in the last week of this course.
[408.00s -> 413.00s]  But until then, we will consistently assume that single objects can generate at most one detection.
[413.00s -> 418.00s]  We can now describe the distribution of the object measurement matrix given the state xk.
[418.00s -> 429.00s]  Evaluating this distribution when ok does not contain any measurements is the same as asking what the probability is that the object is undetected and this is 1 minus pd of xk.
[430.00s -> 446.00s]  If we instead evaluate the distribution when the measurement matrix capital Ok is the vector lowercase ok, we get the probability that the object is detected pd of xk times the distribution of the measurement vector ok given xk.
[447.00s -> 474.00s]  Please note that the measurement model described here elegantly captures both the probability of detection and, if detected, the distribution of the measurement, where the factors 1 minus pd and pd account for the probability that the matrix is empty or non-empty, whereas the factor gk of Ok given xk, roughly speaking, accounts for the probability of the specific measurement vector, given that the object is detected.
[474.00s -> 480.00s]  Formally, given xk, the set of measurement vectors in Ok is a Bernoulli random finite set.
[480.00s -> 491.00s]  We will provide more details about random finite sets in week 4 and this week we keep the discussion about random finite sets to bare minimum in order to focus more on single object tracking.
[492.00s -> 497.00s]  One way to get a better understanding for a distribution is to look at how we can generate samples from it.
[497.00s -> 501.00s]  For the above measurement model, this is quite simple and illuminating.
[501.00s -> 519.00s]  To obtain a sample of Ok given xk, we initialize the matrix as empty, then we generate a uniformly distributed random number between 0 and 1, in MATLAB this is simply denoted rand, and check if that number is smaller than pd of xk, which happens with probability pd.
[519.00s -> 525.00s]  If this happens, we generate a measurement vector Ok and put that into the matrix capital Ok.
[525.00s -> 530.00s]  Another way to gain intuition for a distribution is to look at samples from the distribution.
[530.00s -> 535.00s]  Let us therefore consider a specific example and generate a few samples.
[535.00s -> 550.00s]  In this example, we assume that the probability of detection is 0.85 and that the spatial pdf of the object measurements is Gaussian with mean 3,2 and covariance matrix 0.3 times the identity matrix.
[550.00s -> 554.00s]  We have illustrated this pdf using a circle in this figure.
[554.00s -> 566.00s]  If we look at samples from this distribution, we can see that the matrix normally contains a measurement and that the measurement vector appears in the vicinity of the mean vector 3,2.
