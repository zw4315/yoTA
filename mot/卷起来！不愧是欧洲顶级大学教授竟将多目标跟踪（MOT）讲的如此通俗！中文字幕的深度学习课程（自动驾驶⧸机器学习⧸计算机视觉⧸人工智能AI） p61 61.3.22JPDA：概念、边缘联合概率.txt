# Detected language: en (p=1.00)

[0.00s -> 6.24s]  Hi! In this video, we will describe the Joint Probabilistic Data Association filter,
[6.24s -> 10.16s]  which is most often called by its abbreviation, J-PDA.
[10.16s -> 14.96s]  This tracking algorithm is a generalization of PDA for single object tracking.
[15.52s -> 19.76s]  The basic idea in the J-PDA filter is that in each update,
[19.76s -> 23.20s]  we merge the different hypotheses in the posterior density,
[23.20s -> 26.16s]  and we do so by merging the marginal density.
[26.16s -> 31.36s]  In order to do so, we first compute so-called marginal association probabilities,
[31.36s -> 36.32s]  and we compute them jointly, meaning that we compute them for all objects at the same time.
[36.32s -> 41.44s]  A motivation for merging the hypotheses into a single one is that we can keep some of the
[41.44s -> 47.04s]  relative simplicity of an algorithm that has a single hypothesis in the posterior density,
[47.04s -> 51.68s]  while still retaining some information from all of the multiple hypotheses.
[51.68s -> 57.76s]  And if we compare in GNN, we lose the information in the pruned hypothesis completely.
[57.76s -> 63.68s]  The posterior density is approximated by a single hypothesis density that is computed using
[63.68s -> 68.00s]  marginal association probabilities, and we will define shortly what this means.
[68.00s -> 73.28s]  The marginal association probabilities, denoted beta, are computed in each time step,
[73.28s -> 79.84s]  and at time k they are conditioned on the marginal probabilities computed from time 1 to time k-1.
[79.84s -> 85.52s]  The J-PDA density has a single hypothesis and is parameterized by the object densities.
[85.52s -> 88.00s]  So if we have Gaussian densities in models,
[88.00s -> 93.20s]  then the J-PDA density is parameterized by the Gaussian means and covariances.
[93.20s -> 99.28s]  The marginal probability that object i is associated to measurement j at time k
[99.28s -> 105.84s]  is denoted beta with a super index ij, and is computed as the sum of the probabilities,
[105.84s -> 111.28s]  or weights, for the valid associations for which theta i is equal to j.
[111.28s -> 116.00s]  So this is proportional to the sum of the unnormalized weights, w tilde.
[116.00s -> 120.00s]  The marginal probability that object i was not detected at time k
[120.00s -> 123.68s]  is denoted beta with super index i and 0.
[123.68s -> 129.20s]  And this can be computed as 1 minus the sum of marginal association probabilities for all
[129.20s -> 135.04s]  measurements, or equivalently, we can compute it as the sum of the probabilities for the valid
[135.04s -> 140.64s]  associations for which theta i is equal to 0. So we can note one thing immediately,
[140.64s -> 144.40s]  and that is that computing the marginal association probabilities
[144.40s -> 149.76s]  requires a summation over all valid associations. And as we have seen many times now,
[149.76s -> 153.12s]  this can involve an intractable number of valid associations.
[153.12s -> 156.00s]  And we will come back to how this can be approximated.
[156.00s -> 161.12s]  Another thing that is important to note is that the marginal association probabilities
[161.12s -> 166.48s]  must be computed jointly. And we can illustrate why this is using an example.
[166.48s -> 170.96s]  So let's consider a scenario where we have between one and four objects.
[170.96s -> 174.64s]  The objects have scalar states, and we have just a single measurement.
[174.64s -> 178.88s]  The measurement model is that pd is equal to 0.85,
[178.88s -> 184.64s]  flutter intensity is 0.3, and the Gaussian measurement likelihood has variance 0.2.
[184.64s -> 189.04s]  The four objects have the priors shown here, so they have different means and different
[189.04s -> 195.52s]  covariances. Lastly, what we will visualize is the probability of not associating to the detection
[195.52s -> 198.64s]  and the probability of associating to the detection.
[198.64s -> 204.00s]  So let's first consider that we have just a single object. The prior is shown on the left.
[204.00s -> 207.76s]  In the middle, we have four different values of the measurement said,
[207.76s -> 213.28s]  the marginal probability of not associating to the single detection. So in other words,
[213.28s -> 218.00s]  missed detection. And on the right, we have the marginal probability of associating
[218.00s -> 224.00s]  to the single detection. So we see that when the measurement said is closer to the mean of the
[224.00s -> 232.16s]  object density, then beta10 is low and beta11 is high, which is natural with a Gaussian likelihood.
[232.16s -> 238.16s]  Now we have instead two objects. Because the means of the prior densities are quite far apart
[238.16s -> 243.52s]  with respect to the prior covariances, the two objects have a very small effect on the respective
[243.52s -> 250.48s]  marginal association probabilities. Beta10 and beta11, they're essentially the same as when
[250.48s -> 257.20s]  we had just a single object. If we add a third object with a prior mean close to the prior mean
[257.20s -> 263.68s]  of object one, we can observe a large effect on beta10. That is the blue line in the middle
[263.68s -> 270.08s]  and beta11, the blue line on the right. And if we add a fourth object with prior mean close
[270.08s -> 276.64s]  to the prior mean of object two, we see the similar effect on beta20 and beta21,
[276.64s -> 281.60s]  the dashed red lines in the middle and right figures. So what these examples illustrate is
[281.60s -> 287.12s]  that we cannot compute the marginal association probabilities independently for each object,
[287.12s -> 291.68s]  we have to do it jointly. So in this example, we had just a single detection,
[291.68s -> 296.48s]  because it becomes difficult to visualize the marginal association probabilities for multiple
[296.48s -> 302.00s]  detections. But the conclusion that the marginal association probabilities must be
[302.00s -> 307.60s]  computed jointly still holds. And next, we're going to consider an example with multiple
[307.60s -> 313.52s]  detections. Let's take the same 15 2d measurements that were used to illustrate gating for n
[313.52s -> 318.64s]  objects shown on the right here as red squares, we have six objects. And on the right, we have
[318.64s -> 324.40s]  illustrated the respective predicted measurements and gates and illustrated on the right are now
[324.40s -> 330.16s]  lines between each pair of predicted measurement and measurement. So in other words, between each
[330.16s -> 335.28s]  object and measurement pair. So some of these associations are more probable than others. And
[335.28s -> 341.36s]  we can illustrate this by making the color intensity proportional to the marginal association
[341.36s -> 346.00s]  probabilities. So the more intense the orange colored line is, the higher the marginal
[346.00s -> 351.52s]  association probability is, and vice versa. Some of these associations have a very low
[351.52s -> 358.16s]  probability and can just be seen as faint gray lines. Of course, we can apply ellipsoidal gating
[358.16s -> 362.96s]  to this problem and group the objects and measurements before we compute the marginal
[362.96s -> 368.56s]  association probabilities. So here, the gating has led to rejecting associations that had very low
[368.56s -> 373.52s]  probability, and the corresponding lines from predicted measurement to measurement could hardly
[373.52s -> 379.04s]  be seen in the previous illustration. If we just select a gate that is large enough, this will
[379.04s -> 382.56s]  lead to negligible errors when we compute the probabilities.
