# Detected language: en (p=1.00)

[0.00s -> 5.12s]  Hi, in the next couple of videos, we're going to present a few different tracking algorithms
[5.12s -> 6.44s]  for n objects.
[6.44s -> 10.72s]  But first, in this video, we will explain what problem these algorithms are trying
[10.72s -> 14.44s]  to solve, and the different methods that they build upon.
[14.44s -> 18.28s]  Let's illustrate the problem that we wish to solve with an example.
[18.28s -> 22.84s]  So here we have two objects with one dimensional states, and we have a timeline that goes
[22.84s -> 24.64s]  from time one to time 10.
[24.64s -> 28.84s]  So here the states for the two objects are shown as orange squares, and they're connected
[28.88s -> 33.76s]  over time by orange lines that show how the object state evolves over time.
[33.76s -> 37.68s]  We have noisy object detections, and they're shown here as blue circles.
[37.68s -> 42.26s]  And there are also missed detections, which we can see for the object on the left at
[42.26s -> 46.64s]  time five and time nine, and for the object on the right, we have a missed detection
[46.64s -> 48.52s]  at time six.
[48.52s -> 53.04s]  And there are also cluttered detections, or false detections, which are illustrated
[53.04s -> 55.24s]  here by red triangles.
[55.24s -> 60.32s]  So the purpose of n object tracking is to process these detections and estimate a posterior
[60.32s -> 62.48s]  density for the object states.
[62.48s -> 67.80s]  However, we don't know the true object states, and we do not know which detections
[67.80s -> 70.74s]  are from objects and which are clutter.
[70.74s -> 75.48s]  So what we have to deal with is more like what is shown here, detections illustrated
[75.48s -> 76.48s]  as red circles.
[76.48s -> 81.50s]  So assuming that we know that there are two objects, we have to process these measurements
[81.50s -> 83.76s]  and estimate the object states.
[83.76s -> 87.98s]  We want to process the measurements and estimate the posterior density, which is a mixture
[87.98s -> 93.00s]  density that has weights or probabilities for different data association sequences and
[93.00s -> 96.72s]  state densities conditioned on the data association sequences.
[96.72s -> 101.96s]  So due to the rapidly increasing number of mixture components, the exact posterior is
[101.96s -> 105.40s]  intractable, and we have to do approximations.
[105.40s -> 110.68s]  So different tracking algorithms correspond to different approximations of the posterior
[110.68s -> 111.68s]  density.
[111.68s -> 116.24s]  The methods for dealing with a large number of hypotheses can be divided into two main
[116.24s -> 119.50s]  categories called pruning and merging.
[119.50s -> 123.12s]  Pruning means that we truncate hypotheses with small weights.
[123.12s -> 127.16s]  In other words, we remove hypotheses that have a low probability.
[127.16s -> 132.48s]  This can be understood as approximating the weights of the truncated hypotheses as zero
[132.48s -> 136.84s]  and then renormalizing the remaining non-zero weights.
[136.84s -> 141.40s]  Merging means that we merge several hypotheses into a single hypothesis.
[141.40s -> 147.24s]  The mixture density for the multiple hypotheses is then approximated by a single density.
[147.24s -> 153.04s]  A good tool for merging is to merge such that the Kullback-Leibler divergence is minimized.
[153.04s -> 156.76s]  And we don't have to choose to do either pruning or merging.
[156.76s -> 160.08s]  Instead, we can combine them in several different ways.
[160.08s -> 165.44s]  For example, we can prune all hypotheses except for a fixed number of hypotheses.
[165.44s -> 168.62s]  We can prune only the hypotheses that have small weights.
[168.62s -> 173.82s]  We can merge hypotheses that are similar in some sense, where similarity can be measured,
[173.82s -> 176.78s]  for example, by the Kullback-Leibler divergence.
[176.78s -> 180.74s]  And we can also merge all hypotheses, regardless of how similar they are.
[180.74s -> 185.94s]  Quite often, tracking algorithms that include merging starts by pruning hypotheses that
[185.94s -> 190.54s]  have small weights and only applies the merging to the remaining hypotheses.
[190.54s -> 195.74s]  We're going to present three different tracking algorithms, one in which we are greedy,
[195.74s -> 200.90s]  which means that in each time step, we take the best association and prune all others,
[200.90s -> 206.54s]  another in which we merge all hypotheses into a single hypothesis, and the third in
[206.54s -> 211.46s]  which we maintain multiple hypotheses with high weights and prune the rest.
[211.46s -> 216.74s]  The greedy algorithms are based on computing optimal assignments, and the posterior density
[216.74s -> 220.26s]  is approximated by a density with a single hypothesis.
[220.26s -> 224.64s]  The tracking algorithm that we will have a look at is called global nearest neighbor,
[224.64s -> 231.48s]  or GNN filter, and this is a generalization of nearest neighbor filter to N objects.
[231.48s -> 236.52s]  Merging all hypotheses into a single hypothesis can be based on computing what is called
[236.52s -> 241.32s]  marginal association probabilities, and we will learn what that is later.
[241.32s -> 245.74s]  But in this case, the posterior density is again approximated by a density that has
[245.74s -> 250.76s]  a single hypothesis, and the tracking algorithm that we will learn about is called joint
[250.76s -> 254.84s]  probabilistic data association, or J-PDA filter.
[254.84s -> 258.92s]  And this is a generalization of PDA to N objects.
[258.92s -> 263.68s]  And the last type of algorithm is based on computing the M-best assignments in each
[263.68s -> 264.68s]  time step.
[264.68s -> 269.44s]  So now the posterior is approximated by a density with multiple hypotheses, and the
[269.44s -> 274.88s]  resulting tracking algorithm is called multi-hypothesis tracker, or MHT.
[274.88s -> 281.58s]  And actually, GNN can be seen as a special case of MHT, where M is set to be equal to
[281.58s -> 282.58s]  one.
[282.58s -> 288.00s]  In common to all three algorithms is that we wish to approximate the posterior density.
[288.00s -> 294.36s]  So here we have illustrated the theoretically exact marginal posterior densities for two
[294.36s -> 295.36s]  objects.
[295.36s -> 299.68s]  On the left, we have the first object in blue, and on the right, the second object
[299.68s -> 300.70s]  in orange.
[300.70s -> 305.80s]  If we make a greedy approximation, which is what GNN does, then for this example, we
[305.80s -> 308.98s]  get the approximate marginal posteriors shown here.
[308.98s -> 313.62s]  The GNN posterior matches the main peaks of the exact marginal posteriors.
[313.62s -> 318.46s]  However, there's a fair amount of difference between the exact and the approximation,
[318.46s -> 322.94s]  especially for the second object on the right in orange color, there's a fair amount
[322.94s -> 327.82s]  of probability density that the GNN approximation does not capture.
[327.82s -> 334.12s]  If instead we merge all hypotheses, as in JPDA, we get the approximation shown here.
[334.12s -> 339.82s]  We see that for both objects, the approximation is arguably better than what it was for GNN.
[339.82s -> 342.50s]  However, there's still some difference.
[342.50s -> 347.62s]  Lastly, we can maintain multiple hypotheses, which is what MHT does.
[347.62s -> 352.38s]  And then we can get the approximation shown here, which out of the three we have just
[352.38s -> 357.20s]  looked at, is clearly most similar to the exact marginal posterior.
[357.20s -> 362.00s]  There's some difference, but the shape of the posterior is captured quite well.
[362.00s -> 368.44s]  Okay, so what we will learn about in the next few videos is GNN, JPDA, and MHT.
