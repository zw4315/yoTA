# Detected language: en (p=1.00)

[0.00s -> 11.00s]  You might ask yourself why the algorithm is called nearest neighbor filtering, instead of something like the single hypothesis filtering algorithm.
[11.00s -> 17.00s]  To understand the connection to the name, we can look at an alternative way to find the most likely hypothesis, theta star.
[17.00s -> 21.00s]  We can split the calculation of theta star into two steps.
[21.00s -> 27.00s]  In the first step, we find the most probable hypothesis among all the ones where the object is detected.
[27.00s -> 32.00s]  That is, in this step, we consider values of theta from 1 to mk.
[32.00s -> 35.00s]  We here refer to this hypothesis as theta plus.
[35.00s -> 41.00s]  In the second step, we compare the hypothesis that we found with the hypothesis that the object is undetected.
[41.00s -> 50.00s]  If the hypothesis theta plus is more probable than the hypothesis that the object is undetected, it must be the most probable of all hypotheses.
[50.00s -> 59.00s]  If the hypothesis that the object is undetected is more probable than theta plus, theta equals 0 is the most probable association.
[59.00s -> 70.00s]  One can show that finding theta plus is, in some sense, equivalent to finding the index of the measurement z, which is closest to the predicted density z bar k given k minus 1.
[70.00s -> 78.00s]  That is, in the first step of this algorithm, the algorithm looks for the nearest neighbor to z bar among all the measurements.
[78.00s -> 86.00s]  As long as the undetected hypothesis is not more likely, the nearest neighbor algorithm simply uses that measurement in a Kalman filter update.
[86.00s -> 93.00s]  Let us briefly try to understand in what sense theta plus finds the measurement which is closer to z bar.
[93.00s -> 98.00s]  This derivation is also useful to understand ellipsoidal gating that we introduce later.
[98.00s -> 105.00s]  So, theta plus is the most probable hypothesis, ignoring the possibility that theta could be 0.
[105.00s -> 113.00s]  The unnormalized weight, when theta is greater than 0, is pd times the predicted likelihood divided by the clutter intensity.
[113.00s -> 119.00s]  Clearly, pd takes the same value for all the theta and does not influence the maximizing argument theta.
[119.00s -> 130.00s]  If we also assume that the clutter intensity is the same for all the measurements, we can simplify the expression that we are trying to maximize by ignoring both pd and the clutter intensity.
[130.00s -> 139.00s]  We are left with the predicted likelihood, which is a Gaussian density with covariance Sk and mean z bar, evaluated at z theta.
[139.00s -> 151.00s]  As you can see, the variable theta only appears in this quadratic form and it is easy to see that we want this quadratic form to be as small as possible in order for the value of the density to be as large as possible.
[151.00s -> 159.00s]  To find theta that maximizes the Gaussian density, we can instead find theta that minimizes the quadratic form.
[159.00s -> 163.00s]  Which means that z theta should be as close as possible to z bar.
[163.00s -> 169.00s]  In principle, this is quite natural since we know that Gaussian densities take their maximum values at the mean.
[169.00s -> 180.00s]  However, we can also see that the covariance Sk will determine how sensitive the quadratic form is to differences between z theta and z bar in different dimensions.
[180.00s -> 190.00s]  In principle, the quadratic form is more sensitive in dimensions where the variance according to Sk is small and less sensitive in dimensions where the variance is large.
[190.00s -> 198.00s]  The conclusion from all of this is that the nearest neighbor algorithm essentially uses the measurement which is closest to z bar to update the predicted density.
[198.00s -> 206.00s]  However, this assumes that the clutter intensity is constant and that the undetected hypothesis is not the most likely hypothesis.
[206.00s -> 211.00s]  Also, our notion of distance is determined by the covariance matrix Sk.
[211.00s -> 217.00s]  Before we finish, I'd like to highlight that the nearest neighbor approximations can be problematic.
[217.00s -> 225.00s]  To do this, let us consider the same example as before, but where the measurements at time 4, 5 and 6 are now different.
[225.00s -> 234.00s]  Instead of consistently receiving measurements between 2 and 3, we now receive measurements between minus 1.3 and minus 0.7.
[234.00s -> 240.00s]  As you will see, this makes the nearest neighbor filtering approximation much less accurate.
[240.00s -> 248.00s]  We are still visualizing the same densities and hypotheses and for the first three time steps, everything is identical to before.
[248.00s -> 260.00s]  At time 3, the nearest neighbor algorithm ignores the possibility that the measurement at minus 0.3 could be the object measurement and it underestimates the uncertainties.
[260.00s -> 266.00s]  At time 4, we now receive one measurement at minus 0.7 and one at 3.
[266.00s -> 271.00s]  According to the exact posterior, both of them have a significant probability of being in object detection.
[271.00s -> 277.00s]  However, based on the approximation of the predicted density produced by the nearest neighbor algorithm,
[277.00s -> 286.00s]  there is a very large probability that the measurement at 3 is an object measurement and the nearest neighbor again ignores all other possibilities.
[286.00s -> 295.00s]  At time 5 and at time 6, we only receive one measurement, first at minus 1 and then at minus 1.3.
[295.00s -> 305.00s]  According to the true posterior, there is a very large probability that these measurements are object measurements and the density has its mass centered around minus 1.
[305.00s -> 318.00s]  The nearest neighbor algorithm, on the other hand, was already convinced that the object was around plus 3 and therefore disregards the measurements at minus 1 and minus 1.3 as clutter measurements
[318.00s -> 323.00s]  and therefore concludes that the object is undetected at time 5 and at time 6.
[323.00s -> 332.00s]  In situations where the algorithm is wrong about where the object is and therefore starts to disregard the object measurements as clutter, we say that we lost track of the object.
[332.00s -> 343.00s]  All algorithms can lose track of an object if the conditions are complicated, but simple algorithms, like the nearest neighbor algorithm, have a larger risk of doing so, even in relatively simple scenarios.
[343.00s -> 345.00s]  Let us summarize what we've learned.
[345.00s -> 352.00s]  The nearest neighbor algorithm is a simple algorithm that only maintains the most likely hypothesis in every update step.
[352.00s -> 359.00s]  The advantages with the nearest neighbor algorithm is that it's simple to implement and that it has low computational complexity.
[359.00s -> 366.00s]  An important disadvantage is that it ignores some hypotheses and therefore underestimates the uncertainties.
[366.00s -> 377.00s]  A consequence of this is that we can lose track of the object quite easily unless we are considering a simple scenario, for instance with large PD and few clutter detections.
