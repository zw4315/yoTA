# Detected language: en (p=1.00)

[0.00s -> 10.50s]  Let's consider an example in which we have a linear and Gaussian model, a constant probability
[10.50s -> 13.10s]  of detection, and uniform clutter.
[13.10s -> 17.30s]  For this model, we got the following n-object measurement likelihood.
[17.30s -> 20.70s]  For the objects that were misdetected, because the probability of detection is
[20.70s -> 25.76s]  independent of the object state, we get the probability of misdetection to the power
[25.76s -> 31.62s]  of the number of objects that were misdetected according to the data association.
[31.62s -> 36.16s]  So the total number of objects is n, and according to the data association, we have
[36.16s -> 41.66s]  MO object detections, and that gives us n minus MO misdetections.
[41.66s -> 45.38s]  And for the objects that were detected according to the association, we get the
[45.38s -> 51.18s]  probability of detection times the Gaussian likelihood divided by the clutter intensity.
[51.18s -> 56.24s]  Let's do a visualization of what this n-object measurement likelihood can look like.
[56.24s -> 60.88s]  We're going to take an example that resembles one of the examples that we had in the single
[60.88s -> 65.78s]  object tracking, but this time we have two objects rather than just one.
[65.78s -> 72.94s]  There are two measurements, Z1 and Z2, and they are equal to minus 1.6 and 1 respectively.
[72.94s -> 76.68s]  The probability of detection is equal to 0.85.
[76.68s -> 82.54s]  The clutter intensity is equal to 0.3 in the interval from minus 5 to 5, and the measurement
[82.54s -> 88.94s]  likelihood G is equal to a normal density with a mean value X and variance 0.2.
[88.94s -> 93.20s]  So for this particular example, there are seven different valid associations.
[93.20s -> 97.88s]  Later in the course, we will derive an expression for what the number of valid associations
[97.88s -> 102.10s]  is, given the number of objects and the number of measurements.
[102.10s -> 107.14s]  There is one way in which we can associate such that neither of the objects are detected.
[107.14s -> 110.26s]  In other words, both objects are misdetected.
[110.26s -> 114.80s]  In this case, the measurement likelihood evaluated for different values for the object
[114.80s -> 121.66s]  states X1 and X2 is constant, about 5 times 10 to the power of minus 5.
[121.66s -> 127.20s]  There are two ways in which we can associate such that the object with state X1 is detected
[127.20s -> 130.20s]  and the object with state X2 is misdetected.
[130.20s -> 136.52s]  And these two associations correspond to associating the object with state X1 either to measurement
[136.52s -> 139.72s]  Z1 or to measurement Z2.
[139.72s -> 145.52s]  In both cases, we get that the joint likelihood for the measurements and the association is
[145.52s -> 152.68s]  approximately equal to a constant multiplied by a Gaussian density dependent on X1.
[152.68s -> 157.52s]  Because the joint likelihood is independent of X2, we get the likelihood that is shown
[157.52s -> 159.14s]  in these two images.
[159.14s -> 164.54s]  The Gaussian density for X1 is centered around the associated measurement.
[164.54s -> 169.18s]  And in the X2 dimension, the joint likelihood is uniform.
[169.18s -> 173.94s]  Similarly, there are two ways in which we can associate such that the object with
[173.94s -> 179.06s]  state X1 is misdetected and the object with state X2 is detected.
[179.06s -> 183.96s]  In this case, the joint likelihood has a similar type of appearance when visualized.
[183.96s -> 190.08s]  The difference is now that the Gaussian is in the X2 dimension and in the X1 dimension
[190.08s -> 191.48s]  it is uniform.
[191.48s -> 195.76s]  Lastly, there are two different ways in which we can associate both measurements
[195.76s -> 196.88s]  to the objects.
[196.88s -> 204.12s]  We can associate Z1 to X1 and Z2 to X2, as on the left here, or we can associate
[204.12s -> 208.84s]  Z2 to X1 and Z1 to X2, as on the right.
[208.84s -> 214.08s]  In both of these cases, we get that the joint likelihood is Gaussian with mean according
[214.08s -> 216.20s]  to the association.
[216.20s -> 221.50s]  If we compute the sum of the joint likelihoods for the measurements and the associations,
[221.50s -> 225.60s]  then we get the measurement likelihood and that is visualized here.
[225.60s -> 230.20s]  For these parameter settings, we get that associating both measurements to the objects
[230.20s -> 231.20s]  dominates.
[231.20s -> 236.16s]  However, we can still see faint traces of the associations where just one object
[236.16s -> 242.48s]  was detected, as these vertical and horizontal lines.
[242.48s -> 250.08s]  If we increase the probability of detection from 0.85 to 0.95, then we get the measurement
[250.08s -> 252.60s]  likelihood that is visualized here.
[252.60s -> 257.60s]  In this case, because the probability of detection is higher, the probability of misdetection
[257.60s -> 263.40s]  is lower, and it follows that the two data associations that associated both measurements
[263.40s -> 265.62s]  dominate even more.
[265.62s -> 271.54s]  If instead we lower the probability of detection to 0.5, then we get the measurement likelihood
[271.54s -> 273.18s]  visualized here.
[273.18s -> 277.88s]  And in this case, the probability of misdetection is quite high, and it follows that the data
[277.88s -> 282.30s]  associations that associated just one of the measurements to the objects are much
[282.30s -> 284.20s]  more significant.
[284.20s -> 288.44s]  When we visualize the measurement likelihoods, we could see that they are symmetric with
[288.44s -> 296.00s]  respect to the line x1 equal to x2, so the line going diagonally across these images.
[296.00s -> 301.08s]  This type of symmetry holds also if we have more objects or more measurements.
[301.08s -> 305.68s]  However, that is not as straightforward to visualize, especially not when we have
[305.68s -> 307.66s]  more than two objects.
[307.66s -> 312.62s]  We can understand this symmetry being caused in part by the unknown data sources.
[312.62s -> 319.06s]  For example, we can consider the example where the probability of detection is 0.95.
[319.06s -> 325.50s]  In this case, the two measurements at negative 1.6 and 1 are an indication that there could
[325.50s -> 331.10s]  be an object close to negative 1.6 and another object close to 1.
[331.10s -> 336.66s]  However, the unknown data associations means that we do not know if it is x1 that is
[336.66s -> 343.36s]  close to negative 1.6 and x2 that is close to 1 or the other way around, so x1 is close
[343.36s -> 350.80s]  to 1 and x2 is close to negative 1.6, and therefore we get this symmetry.
[350.80s -> 355.96s]  Another way we can understand this symmetry is by considering the fact that the indexing
[355.96s -> 360.82s]  1 and 2 that we have given to the object states is actually arbitrary.
[360.82s -> 366.58s]  Again, the measurement tells us that there could be an object around minus 1.6 and there
[366.58s -> 369.34s]  could be another object around 1.
[369.34s -> 374.66s]  However, it is arbitrary which one of these objects we call object 1 and which one we
[374.66s -> 376.46s]  call object 2.
[376.46s -> 381.82s]  Both ways to assign this indexing will represent the same object states.
[381.82s -> 387.30s]  So we can illustrate what we mean by arbitrary indexing using an example from autonomous
[387.30s -> 388.30s]  vehicles.
[388.30s -> 394.00s]  So here we see an example with an autonomous vehicle and four other vehicles within the
[394.00s -> 396.18s]  sensor's field of view.
[396.18s -> 403.14s]  Because there are four vehicles, we can call the corresponding state vectors x1, x2, x3,
[403.14s -> 404.30s]  and x4.
[404.30s -> 409.72s]  So the question is now which vehicle is represented by state vector x1 and which
[409.72s -> 413.06s]  is represented by x2 and so on.
[413.06s -> 417.34s]  We could, for example, assign the state vectors from the top to the bottom as shown
[417.34s -> 418.34s]  here.
[418.34s -> 422.42s]  So we have x1, x2, x3, and x4.
[422.42s -> 426.94s]  Alternatively, we could assign the state vectors from the bottom to the top as shown
[426.94s -> 430.82s]  now, x1, x2, x3, x4.
[430.82s -> 435.58s]  Yet another alternative is to assign the state vectors from the left to the right
[435.58s -> 437.08s]  as shown now.
[437.08s -> 441.24s]  And as you understand, there are many ways in which we can assign these state vectors
[441.24s -> 445.22s]  x1 to x4 for these four vehicles.
[445.22s -> 450.60s]  And the key insight is that how we assign these state vectors, that's not going to
[450.60s -> 454.16s]  affect the actual state of the objects.
[454.16s -> 459.20s]  The objects are still going to have the same positions, the same speeds, the same headings,
[459.20s -> 460.60s]  and so on.
[460.60s -> 464.84s]  And hence we say that this object indexing is arbitrary.
[464.84s -> 469.20s]  So to summarize, we have talked about the standard point object measurement model
[469.20s -> 470.76s]  for multiple objects.
[470.76s -> 476.64s]  We have Poisson clutter, we have a probability of detection, and the object measurement likelihood.
[476.68s -> 481.60s]  We also talked about how the n-object data association vector theta is defined.
[481.60s -> 486.42s]  We then presented the n-object measurement likelihood and showed that just like in single
[486.42s -> 493.38s]  object tracking, the measurement likelihood includes a sum over all possible data associations.
[493.38s -> 497.84s]  And lastly, we gave some examples and showed that likelihood is symmetric.
