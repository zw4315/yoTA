# Detected language: en (p=1.00)

[0.00s -> 9.24s]  We can illustrate the GNN filter using a 1D example that is similar to the 1D examples
[9.24s -> 10.88s]  that we have looked at before.
[10.88s -> 13.18s]  We have two objects with scalar states.
[13.18s -> 19.84s]  Pd is equal to 0.85, the clutter intensity is 0.3 from minus 5 to 5, and the likelihood
[19.84s -> 22.82s]  is Gaussian with variance 0.2.
[22.82s -> 28.68s]  We have a random walk motion model with variance 0.25, the initial priors have mean
[28.68s -> 33.80s]  minus 2.5 and 2.5, and variance is 0.36.
[33.80s -> 38.92s]  And to visualize the results, we will consider the marginal densities and estimated expected
[38.92s -> 39.92s]  values.
[39.92s -> 44.52s]  If we simulate these models, we can for example get the sequences of detections
[44.52s -> 45.62s]  shown here.
[45.62s -> 50.96s]  So this is what we will process with a GNN filter in order to compute posterior densities
[50.96s -> 52.38s]  and object estimates.
[52.38s -> 55.32s]  We start with an initial prior at time 0.
[55.34s -> 60.96s]  The first object is on the left in blue, and the second object is on the right in orange.
[60.96s -> 66.08s]  First we predict to time 1, and at time 1 we get a set of detections.
[66.08s -> 68.94s]  In this case, there are 6 detections.
[68.94s -> 74.00s]  We compute an optimal association, which is illustrated here by coloring the associated
[74.00s -> 76.84s]  detection with the corresponding object color.
[76.84s -> 81.22s]  So on the left we have a blue detection associated to the first object whose density
[81.22s -> 85.46s]  is blue, and similarly on the right with the orange detection.
[85.46s -> 91.26s]  And lastly, we use the optimal association to update the object densities at time 1.
[91.26s -> 97.54s]  Next, we predict to time 2, we get a set of detections, this time we have 5 detections,
[97.54s -> 102.58s]  we compute an optimal assignment, and we use it to update the object densities.
[102.58s -> 105.06s]  So that's how the GNN recursion goes.
[105.06s -> 109.02s]  We predict, we compute an optimal assignment, and we update.
[109.02s -> 113.94s]  In this figure, we have visualized the GNN posterior densities as heat maps, so for each
[113.94s -> 119.58s]  discrete time step, higher color intensity corresponds to where more probability density
[119.58s -> 125.98s]  is located, and where it is white, neither object has any significant probability density.
[125.98s -> 131.06s]  The unassociated measurements are shown as white circles, and the associated ones as
[131.06s -> 132.48s]  colored squares.
[132.48s -> 137.72s]  And we see that for both objects, in most time steps a detection has been associated,
[137.72s -> 142.32s]  but there are also some time steps with no associated detection.
[142.32s -> 147.84s]  Here, we've illustrated the measurements and the posterior densities together with estimates
[147.84s -> 148.96s]  and ground truth.
[148.96s -> 153.64s]  So on top left, we have the measurements, on the top right is the posterior densities
[153.64s -> 159.10s]  that we just had a look at, in the bottom left, we have illustrated the posterior densities
[159.10s -> 162.16s]  together with estimates shown as black circles.
[162.16s -> 166.84s]  So the black lines show how the sequence of estimates for each object evolves.
[166.84s -> 171.96s]  And lastly, in the bottom right, we have a comparison of the object estimates and the
[171.96s -> 173.36s]  ground truth.
[173.36s -> 177.84s]  Due to the measurement noise and the process noise, the estimates are not perfect, but
[177.84s -> 180.94s]  they tend to follow the ground truth reasonably well.
[180.94s -> 186.04s]  In this slide, we show the same type of figures, but for a lower probability of detection.
[186.04s -> 190.12s]  Before it was 0.85, and here it is 0.5.
[190.12s -> 194.64s]  In order to make the comparison as easy as possible for us, we have the exact same
[194.64s -> 198.88s]  underlying ground truth for the object, we have the same cluttered detections, and
[198.88s -> 201.80s]  the same measurement noise for the object detections.
[201.80s -> 205.32s]  So what is different is only the simulated detection process.
[205.32s -> 207.88s]  Now we have more missed detections.
[207.88s -> 210.80s]  Some interesting differences can be seen.
[210.80s -> 215.76s]  Because we have several time steps in a row where detection is not associated, the
[215.76s -> 220.12s]  visualization of the marginal posterior densities are more smeared out.
[220.12s -> 225.04s]  And that is because the covariances are much larger when we predict and then do not have
[225.04s -> 228.24s]  any associated detection to update with.
[228.24s -> 232.48s]  Due to the fact that we have more missed detections, the estimates can be seen to be
[232.48s -> 238.68s]  worse than previously, which is most evident for the first object, colored in blue.
[238.68s -> 243.70s]  From time one to time seven, this object state is estimated to be smaller than what
[243.70s -> 244.92s]  the ground truth is.
[244.92s -> 250.20s]  In fact, this result is an illustration of a general property of GNN filters, which is
[250.20s -> 254.52s]  that they can perform worse when the signal to noise ratio is lower.
[254.52s -> 260.88s]  So low SNR is, for example, when PD is low, or the clutter intensity is very high,
[260.88s -> 263.70s]  or the measurement noise covariance is large.
[263.70s -> 270.16s]  So the fact that GNN performs worse for low SNR is related to how the exact posterior
[270.16s -> 272.48s]  density is approximated.
[272.48s -> 277.72s]  We can have a look at the posterior density approximation for an example with two objects,
[277.72s -> 282.60s]  two measurements equal to minus 1.6 and one, and a Gaussian prior.
[282.60s -> 286.78s]  And you might recognize that we have used this example previously to illustrate the
[286.78s -> 290.04s]  measurement likelihood and the posterior density.
[290.04s -> 295.34s]  So here, the exact marginal posterior is shown together with a GNN approximation for
[295.34s -> 298.58s]  three different values for the probability of detection.
[298.58s -> 303.24s]  And we also show the posterior weights for the different data associations.
[303.24s -> 310.20s]  When PD is 0.95, which is shown on the right, one of the data association hypotheses has
[310.20s -> 316.34s]  a posterior probability that is almost 0.8, which is quite a bit larger than the probabilities
[316.34s -> 318.12s]  of the other associations.
[318.12s -> 324.02s]  And therefore, approximating the posterior using the most probable association hypothesis
[324.02s -> 328.04s]  gives us a reasonable approximation of the exact posterior.
[328.04s -> 332.92s]  For object one, shown in blue, there is actually very little approximation error.
[332.92s -> 339.12s]  When PD is a bit lower, 0.85, which is shown in the middle, the most probable hypothesis
[339.12s -> 342.74s]  is the same as for PD equal to 0.95.
[342.74s -> 348.74s]  However, the probability of this hypothesis is no longer much larger than the second
[348.74s -> 349.82s]  largest one.
[349.82s -> 356.38s]  And subsequently, there is a larger difference between the exact posterior and the GNN approximation.
[356.40s -> 362.88s]  And lastly, when PD is even lower, 0.5, shown on the left, we have a similar situation.
[362.88s -> 367.90s]  The most probable hypothesis is that both objects are misdetected, and this has probability
[367.90s -> 369.98s]  just over 0.4.
[369.98s -> 374.52s]  So when we prune the remaining weights, the remaining hypothesis, that corresponds
[374.52s -> 378.56s]  to almost 60% of the probability of the hypothesis.
[378.56s -> 384.36s]  So we get a clear difference between the exact posterior and the GNN approximation.
[384.36s -> 387.80s]  We can summarize GNN with some pros and cons.
[387.80s -> 392.76s]  Positive is that GNN filters are computationally cheap tracking algorithms, and are relatively
[392.76s -> 394.48s]  simple to implement.
[394.48s -> 398.80s]  There is also empirical evidence that show that GNN filters work fairly well when the
[398.80s -> 402.08s]  signal-to-noise ratio, or SNR, is high.
[402.08s -> 407.62s]  So in other words, when PD is high, the clutter intensity lambda is low, and the measurement
[407.62s -> 409.60s]  noise covariance is small.
[409.62s -> 414.12s]  When it comes to the disadvantages of GNN filters, we have that it is actually not
[414.12s -> 419.98s]  guaranteed that by greedily taking the optimal assignment in each time step, we will get
[419.98s -> 423.78s]  the data association sequence that is most probable.
[423.78s -> 430.90s]  It can be that, at time k, another sequence of data associations is actually more probable
[430.90s -> 432.56s]  than the greedy sequence.
[432.56s -> 437.96s]  So this is related to the fact that a single n-object hypothesis is not always sufficient
[437.96s -> 441.44s]  to represent the uncertainty of the tracking scenario.
[441.44s -> 447.16s]  The GNN filter can give poor tracking performance when the SNR is moderate to low, or when
[447.16s -> 450.96s]  the SNR is high, but the objects are close together.
[450.96s -> 456.36s]  In such cases, multiple data association hypotheses will have high probability, and
[456.36s -> 461.88s]  by pruning all of them except for a single one, we get a poor approximation of the posterior
[461.88s -> 462.68s]  density.
