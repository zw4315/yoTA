# Detected language: en (p=1.00)

[0.00s -> 7.00s]  In the previous video, we presented general equations for the update step in single object tracking.
[7.00s -> 13.00s]  In particular, we presented detailed expressions for the probabilities of different hypotheses,
[13.00s -> 18.00s]  as well as the posterior density given a data association hypothesis.
[18.00s -> 26.00s]  In this video, we try to make sense of these equations by considering their expressions in a simple and important special case.
[26.00s -> 32.00s]  To obtain a closed form expression for the posterior density, we make three simplifying assumptions.
[32.00s -> 34.00s]  First, that the prior is Gaussian.
[34.00s -> 37.00s]  Second, that the probability of detection is constant.
[37.00s -> 42.00s]  And third, that the object measurement likelihood is linear and Gaussian.
[42.00s -> 46.00s]  Note that we don't need to make any simplifying assumptions on the clutter intensity.
[46.00s -> 52.00s]  We sometimes informally refer to a single object tracking model that satisfies these assumptions as linear and Gaussian,
[52.00s -> 58.00s]  even though we have also assumed a constant probability of detection and even though we have unknown data associations.
[58.00s -> 63.00s]  As usual, we decompose the posterior into one term for each data association hypothesis,
[63.00s -> 68.00s]  and we will now study the detailed expressions for w theta and p theta of x.
[68.00s -> 76.00s]  Let us start by considering p theta of x, which is the posterior of x given the data association theta and the measurements z.
[76.00s -> 84.00s]  We can actually compute p theta of x using the update step, presented in the video about single object tracking with known associations.
[84.00s -> 87.00s]  But it's easy to derive and I'll repeat it here for completeness.
[87.00s -> 94.00s]  In the previous video, we found that p theta of x is proportional to the following expression when theta is equal to zero,
[94.00s -> 97.00s]  and the following expression when theta is greater than zero.
[97.00s -> 102.00s]  Under the assumption that the probability of detection is constant, we obtain a simpler expression,
[102.00s -> 108.00s]  since the factors one minus pd and pd can be absorbed into the proportionality constant.
[108.00s -> 113.00s]  When theta is equal to zero, p theta of x is simply proportional to the prior density.
[113.00s -> 117.00s]  That is, we skip the update step if the object is undetected.
[117.00s -> 125.00s]  If theta is greater than zero, p theta of x is proportional to the prior times g of z theta given x.
[125.00s -> 130.00s]  That is, we update the prior using the likelihood g of z theta given x.
[130.00s -> 133.00s]  If theta states that z theta is the object measurement.
[133.00s -> 140.00s]  Note that we only use the assumption that pd is constant and the expression for p theta already looks much simpler.
[140.00s -> 148.00s]  To obtain closed form expressions, we also assume that p of x is a Gaussian density and that g of o given x is linear and Gaussian.
[148.00s -> 154.00s]  Stating that p theta of x is proportional to a Gaussian prior times a linear and Gaussian likelihood function
[154.00s -> 160.00s]  implies that p theta of x is the posterior when we update this prior by this likelihood,
[160.00s -> 163.00s]  which means that p theta of x is a Gaussian density.
[163.00s -> 170.00s]  In fact, we can use the Kalman filter equations to compute the mean and covariance of the Gaussian density p theta of x
[170.00s -> 174.00s]  and by now you should be familiar with these equations.
[174.00s -> 180.00s]  We've concluded that if theta is greater than zero, p theta of x can be computed using a Kalman filter update,
[180.00s -> 183.00s]  where we assume that z theta is the object measurement.
[183.00s -> 190.00s]  For visualization, let us return to the example that we studied in the video about the complete measurement model.
[190.00s -> 197.00s]  The only difference here is that we now have a Gaussian prior as well, in this case with mean 0.5 and variance 0.5,
[197.00s -> 202.00s]  and we are visualizing the posterior density instead of the measurement likelihood.
[202.00s -> 206.00s]  If you want, you can of course compare the posterior density with the likelihood in that function
[206.00s -> 211.00s]  and check if the relation between the prior likelihood and posterior looks reasonable.
[211.00s -> 220.00s]  What I want you to focus on here are the curves w0 p0 of x, w1 p1 of x, and w2 p2 of x.
[220.00s -> 227.00s]  The weights w0, w1, and w2 make it slightly complicated to see the shapes of the densities.
[227.00s -> 232.00s]  However, p1 of x is obtained from a Kalman filter update using z1,
[232.00s -> 239.00s]  p2 is obtained from a Kalman filter update using z2, whereas p0 of x is identical to the prior.
[240.00s -> 244.00s]  Let us now look at the part that you haven't seen before, namely how to compute the weights.
[244.00s -> 248.00s]  In particular, I would like you to understand the equations for the weights
[248.00s -> 253.00s]  and check if they are reasonable considering that the weights are the data of cessation probabilities.
[253.00s -> 261.00s]  We've found that the unnormalized weight is the integral of p of x times 1 minus pd of x when theta is equal to 0.
[261.00s -> 269.00s]  And when theta is greater than 0, the unnormalized weight is 1 over lambda z of z theta times this integral.
[269.00s -> 274.00s]  The factor lambda c of z theta is perhaps the least intuitive part of this equation.
[274.00s -> 279.00s]  One way to reason about it is that there are only two possible explanations of z theta.
[279.00s -> 282.00s]  It is either an object detection or a clutter detection.
[282.00s -> 289.00s]  It makes sense that the probability that z theta is clutter grows with the clutter intensity lambda c of z theta.
[289.00s -> 295.00s]  Since this is related to the probability that we receive a clutter detection in the vicinity of z theta.
[295.00s -> 305.00s]  Consequently, the probability that z theta is an object detection should therefore shrink as we increase lambda c of z theta, which is what we see here.
[305.00s -> 315.00s]  Now, if pd is constant, these two integrals simplify since the factors 1 minus pd and pd can be extracted out from the integrals.
[315.00s -> 321.00s]  For theta equals 0, the remaining integral is just the integral over p of x, which is 1.
[321.00s -> 324.00s]  And the unnormalized weight is therefore 1 minus pd.
[324.00s -> 332.00s]  For theta greater than 0, we are left with an integral over p of x times g of z theta given x.
[332.00s -> 339.00s]  The way pd enters these equations, we can see that the probability that the object is undetected decreases with pd.
[339.00s -> 342.00s]  And the probability that the object is detected grows with pd.
[343.00s -> 353.00s]  Note that these weights are unnormalized and that the final probabilities of the different hypotheses depend on the normalization factor, which is the sum of all the unnormalized weights.
[353.00s -> 363.00s]  The integral over p of x times g of z theta given x can be thought of as the predicted density of the measurement evaluated at z theta.
[363.00s -> 374.00s]  This may not be entirely obvious, but it has the same form as the Chapman-Kolmogorov equation with the minor difference that we have replaced the motion model pi with the measurement model g.
[374.00s -> 379.00s]  To obtain the final expression, we need to simplify the integral over p times g.
[379.00s -> 391.00s]  Given our linear and Gaussian assumptions, we get that the unnormalized weight, when theta is greater than 0, is pd divided by the intensity function at z theta times an integral.
[391.00s -> 405.00s]  Here, the integral is over a Gaussian density with mean mu and covariance p evaluated at x, times another Gaussian density with mean hx and covariance r evaluated at z theta.
[405.00s -> 408.00s]  This is integrated over all values of x.
[408.00s -> 414.00s]  Now, this integral is the tricky part, but it actually has a known and simple expression.
[414.00s -> 425.00s]  It turns out that it is identical to Gaussian density with mean h times mu and covariance h p h transpose plus r evaluated at z theta.
[425.00s -> 431.00s]  To remember this result, I personally find it helpful to write down the densities as algebraic equations.
[431.00s -> 452.00s]  If z theta is equal to hx plus v, where x is Gaussian with mean mu and covariance p, and v is Gaussian with mean 0 and covariance r, then we know that z theta is Gaussian with mean h times mu and covariance matrix h p h transpose plus r.
[452.00s -> 462.00s]  All you have to do now to obtain this equation is to realize that the integral on the left hand side actually expresses the distribution over z theta under these assumptions.
[462.00s -> 471.00s]  For theta greater than 0, the unnormalized weight is therefore pd times the predicted likelihood divided by lambda c of z theta.
[471.00s -> 484.00s]  We have already discussed pd and lambda c, so the last thing to make sense of is the predicted likelihood, which is the Gaussian density with mean z bar and covariance matrix S evaluated at z theta.
[484.00s -> 496.00s]  What this says is that the hypothesis obtains a small weight unless the measurement z theta is close to where we would expect to find the object measurement, where S determines what we mean by close.
[497.00s -> 505.00s]  If S tells us that we are very uncertain about where the measurement may be, the unnormalized weight is less sensitive to the value of z theta.
[505.00s -> 508.00s]  Let us visualize these results to make them more concrete.
[508.00s -> 522.00s]  We've concluded that the unnormalized weight is 1 minus pd if theta is equal to 0, and pd times the predicted likelihood divided by lambda c of z theta if theta is an integral between 1 and m.
[522.00s -> 535.00s]  If we return to the previous example, we see that since the prior has the mean 0.5 and G says that O is x plus some noise, it follows that z bar is 0.5.
[535.00s -> 546.00s]  Similarly, we find that S is just the variance of x plus the variance of the measurement noise, according to G, which is 0.5 plus 0.2, which is 0.7.
[547.00s -> 556.00s]  Since z bar is 0.5, we can see that z2 is close to z bar, whereas z1 is further away from z bar.
[556.00s -> 561.00s]  This makes the predicted likelihood much larger for z2 than for z1.
[561.00s -> 573.00s]  It is also clear from the visualization that w2 is larger than w1, since the area under w2 times p2 is much larger than the area under w1 times p1.
[573.00s -> 579.00s]  If z1 would have taken an even smaller value, w1 would have been close to 0.
[579.00s -> 583.00s]  In the video about the measurement model, we looked at how the likelihood changed with pd.
[583.00s -> 593.00s]  If we increase pd to 0.95, the main difference is that w0 is reduced, whereas w1 and w2 are increased slightly.
[593.00s -> 597.00s]  This is what we expected to see based on the expressions for w tilde.
[598.00s -> 609.00s]  If we instead decrease pd to 0.5, the probability that the object is undetected is increased and the posterior density no longer has two distinct peaks.
[609.00s -> 618.00s]  Under the assumption that the prior is Gaussian, pd is constant, and G of O given x is linear in Gaussian, we have obtained simple expressions.
[618.00s -> 627.00s]  The density p theta of x is either not updated at all, or obtained using a simple Kalman filter update using the measurement z theta.
[627.00s -> 636.00s]  The unnormalized weight is either 1 minus pd, or pd times the predicted likelihood divided by lambda c of z theta.
[636.00s -> 643.00s]  All of the parameters z bar, s, x at theta and p plus are obtained from a simple Kalman filter update.
[643.00s -> 646.00s]  Before we finish, I'd like to make an important remark.
[646.00s -> 653.00s]  It is common to model the object detections as a non-linear function H of x plus some Gaussian noise v.
[653.00s -> 667.00s]  Even though this prevents us from using a Kalman filter, we can still use a Gaussian filter, such as the extended Kalman filter or an unscented Kalman filter, to approximate x hat theta, p plus, z bar and s.
[667.00s -> 675.00s]  For the models that we consider, it is straightforward to do this, and this enables us to use the derived equations also for non-linear models.
[676.00s -> 678.00s]  Thank you for watching.
