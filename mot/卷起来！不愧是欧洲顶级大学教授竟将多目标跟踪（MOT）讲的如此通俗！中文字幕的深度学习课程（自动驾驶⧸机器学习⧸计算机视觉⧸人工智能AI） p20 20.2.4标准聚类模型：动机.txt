# Detected language: en (p=1.00)

[0.00s -> 7.00s]  In an earlier video, we learned how to model the object detections from a single object.
[7.00s -> 15.00s]  To obtain a complete measurement model that we can use for single object tracking, we also need a model for the clutter detections.
[15.00s -> 20.00s]  We use clutter detections as a general term for all kinds of pulse detections.
[20.00s -> 24.00s]  Sometimes these are due to noise inside the sensors,
[24.00s -> 32.00s]  but often they are caused by things in our environment that the sensor incorrectly perceives as an object that should be detected.
[32.00s -> 40.00s]  In this video, we discuss how we can model the clutter, starting out with an assumption that sensors have limited resolution,
[40.00s -> 44.00s]  and then considering what happens as we increase the resolution.
[44.00s -> 50.00s]  To present the clutter models, it is useful to be familiar with three scalar distributions,
[50.00s -> 56.00s]  namely the Bernoulli distribution, introduced in an earlier video, and two more introduced here.
[56.00s -> 61.00s]  The first is the binomial distribution, which is closely related to the Bernoulli distribution.
[61.00s -> 68.00s]  Imagine that we perform j independent trials, which are all Bernoulli distributed, with probability p.
[68.00s -> 75.00s]  Then the total number of successful trials is binomially distributed with parameters p and j.
[75.00s -> 81.00s]  For instance, imagine that we perform five trials, each with probability 0.8 of succeeding.
[81.00s -> 87.00s]  Then the total number of experiments is binomially distributed with this probability mass function.
[87.00s -> 93.00s]  For instance, the probability of succeeding with four out of five trials is slightly more than 0.4.
[93.00s -> 98.00s]  The probability of succeeding with all five trials is slightly larger than 0.3.
[98.00s -> 103.00s]  And the probability of succeeding six times in five attempts is zero.
[103.00s -> 110.00s]  If x is binomially distributed with parameters p and j, the probability that x is equal to i
[110.00s -> 119.00s]  is the binomial coefficient j choose i times p to the power of i times 1 minus p to the power of j minus i,
[119.00s -> 126.00s]  where the binomial coefficient is j factorial divided by i factorial times j minus i factorial.
[126.00s -> 132.00s]  The binomial coefficient is the number of ways that we can select j elements from a set of i elements,
[132.00s -> 135.00s]  when we do not care about the order of the j elements.
[135.00s -> 140.00s]  One specific property which is useful to understand the arguments in this video,
[140.00s -> 144.00s]  is that the expected value of x is then p times j.
[144.00s -> 148.00s]  To understand this, it may help to think about the case when j is equal to 1,
[148.00s -> 153.00s]  for which it is easy to verify that the expected value of x is p.
[153.00s -> 158.00s]  If j is two or larger, we perform several independent trials,
[158.00s -> 163.00s]  each with expected value p, which gives us p times j as the total expected value.
[163.00s -> 167.00s]  For instance, in this example, the expected value is four.
[167.00s -> 170.00s]  Another useful distribution is the Poisson distribution,
[170.00s -> 174.00s]  which is commonly used to model the number of events in a certain time interval.
[174.00s -> 179.00s]  It could be, for instance, the number of clients entering a supermarket during one hour.
[179.00s -> 183.00s]  We normally parameterize the Poisson distribution with a parameter lambda,
[183.00s -> 186.00s]  which is the expected value of the random variable.
[186.00s -> 190.00s]  If x is Poisson distributed with expected value lambda,
[190.00s -> 199.00s]  then the probability that x is i is lambda to the power of i times e to the power of minus lambda divided by i factorial.
[199.00s -> 206.00s]  We use Po of i semicolon lambda to denote this function on a more compact form.
[206.00s -> 211.00s]  If we plot this probability mass function as a function of i for lambda equal to four,
[211.00s -> 215.00s]  it looks as follows, and you can see that it is fairly spread out.
[215.00s -> 221.00s]  It generally holds that a Poisson distributed random variable has the same mean and variance,
[221.00s -> 227.00s]  which means that we cannot increase the expected value of x without also increasing its variance.
[227.00s -> 231.00s]  As a side note, both the Poisson distribution on this slide
[231.00s -> 235.00s]  and the binomial distribution on the previous slide have the expected value four.
[235.00s -> 239.00s]  And in spite of this, their probability mass functions are quite different.
[239.00s -> 241.00s]  Just to remind you about the bigger picture,
[241.00s -> 248.00s]  the observed measurement matrix Zk contains both objects measurements, denoted Ok,
[248.00s -> 254.00s]  and clutter measurements that we denote Ck, where both Ok and Ck are here matrices.
[254.00s -> 260.00s]  To form the matrix Zk, we simply grab all the measurement vectors in Ok and Ck
[260.00s -> 265.00s]  and randomly order them to obtain the column vectors in the matrix Zk.
[265.00s -> 274.00s]  We use capital pi to denote an operator that forms a matrix by randomly shuffling the input column vectors.
[274.00s -> 280.00s]  Of course, if one of the input variables is an empty matrix, the operator pi will ignore that input.
[280.00s -> 284.00s]  Let us look at an example to clarify what this means.
[284.00s -> 289.00s]  Suppose the vectors O1 and C1 are the input to pi.
[289.00s -> 292.00s]  These two vectors can then be ordered in two different ways,
[292.00s -> 301.00s]  and the output Z is either the matrix O1 C1 or the matrix C1 O1, depending on how we happen to order the vectors.
[301.00s -> 304.00s]  Both possibilities have the probability 0.5.
[304.00s -> 307.00s]  We have already introduced a model for the object detections,
[307.00s -> 314.00s]  and we are now going to develop a model for the clutter detections, which is needed to obtain a model for Zk.
[314.00s -> 320.00s]  Since Ck is a random matrix, the model for Ck needs to characterize
[320.00s -> 325.00s]  both the number of measurement vectors, which corresponds to the width of Ck,
[325.00s -> 329.00s]  as well as the distribution of the vectors, given how many they are.
[329.00s -> 336.00s]  For the development of the clutter model, we initially assume that our sensor has a field of view of volume V,
[336.00s -> 339.00s]  which is the part of the measurement space that our sensor can see,
[339.00s -> 345.00s]  and we use lambda to denote the expected number of clutter detections per unit volume.
[345.00s -> 348.00s]  For now, we assume that lambda is just a constant.
[348.00s -> 351.00s]  As an example, we can assume that the measurement space is two-dimensional,
[351.00s -> 354.00s]  for instance, a two-dimensional position.
[354.00s -> 360.00s]  In this example, the field of view is an area of size 2x2, such that the volume is 4,
[360.00s -> 363.00s]  and we also assume that lambda is 0.8.
[363.00s -> 369.00s]  This means that we would expect 4 times 0.8, or 3.2, clutter detections on average.
[369.00s -> 373.00s]  Note that we do not get any detections outside the field of view.
[373.00s -> 376.00s]  Of course, formally, this is just an area, but in this context,
[376.00s -> 380.00s]  we use the term volume independently of the dimensionality of the space.
[380.00s -> 384.00s]  To give you a first idea about the model we're about to develop,
[384.00s -> 387.00s]  I've generated three samples of Ck from the model.
[387.00s -> 391.00s]  The first sample is a matrix that contains three vectors.
[391.00s -> 395.00s]  The second sample is a matrix that contains four vectors,
[395.00s -> 399.00s]  and the third sample is a matrix that contains a single vector.
[399.00s -> 403.00s]  I obviously don't expect you to understand the entire model from these samples,
[403.00s -> 406.00s]  but you can at least note that both the number of vectors
[406.00s -> 409.00s]  and the vectors themselves are random.
[409.00s -> 412.00s]  Sensors generally have limited resolution,
[412.00s -> 415.00s]  which means that objects that are sufficiently close together
[415.00s -> 418.00s]  only generate at most one detection.
[418.00s -> 422.00s]  Let us look at one attempt to model clutter when we have limited resolution.
[422.00s -> 426.00s]  The first thing that we can do is to split the volume into resolution cells
[426.00s -> 430.00s]  and assume that we can obtain at most one detection from each cell.
[430.00s -> 434.00s]  The size of these cells should, of course, match the resolution of the sensors.
[434.00s -> 438.00s]  Suppose we split the volume into j cells and construct Ck
[438.00s -> 442.00s]  by randomly ordering the measurement vectors from all the different cells
[442.00s -> 444.00s]  and putting them into a matrix.
[444.00s -> 448.00s]  Let us also assume that the set of clutter detections from different cells are independent
[448.00s -> 451.00s]  and that the number of clutter detections in a single cell
[451.00s -> 456.00s]  is Bernoulli distributed with parameter lambda times v divided by j.
[456.00s -> 459.00s]  Of course, the Bernoulli parameter has to be smaller than one,
[459.00s -> 463.00s]  but as long as we select reasonable values for lambda and j,
[463.00s -> 465.00s]  that should be satisfied.
[465.00s -> 468.00s]  This means that we can get at most one detection from each cell,
[468.00s -> 472.00s]  and the probability of getting a detection is lambda,
[472.00s -> 474.00s]  the expected number of detections per volume,
[474.00s -> 479.00s]  times v divided by j, which is the volume of a single cell.
[479.00s -> 483.00s]  In this particular example, v divided by j is one,
[483.00s -> 488.00s]  and lambda times v divided by j is therefore 0.8.
[488.00s -> 491.00s]  Finally, if a cell contains a clutter detection,
[491.00s -> 494.00s]  the vector is uniformly distributed within that cell.
[494.00s -> 499.00s]  We note that since the number of detections in individual cells is Bernoulli distributed,
[499.00s -> 503.00s]  the total number of detections is binomially distributed
[503.00s -> 507.00s]  with parameters j and v times lambda divided by j.
[507.00s -> 512.00s]  And the expected number of detections is therefore simply v times lambda,
[512.00s -> 514.00s]  which in our example is 3.2.
[516.00s -> 519.00s]  In this course, we assume unlimited sensor resolution.
[519.00s -> 522.00s]  This assumption simplifies things substantially,
[522.00s -> 524.00s]  and it is often a good approximation,
[524.00s -> 527.00s]  especially for the sensors and settings encountered
[527.00s -> 529.00s]  when working with self-driving vehicles.
[529.00s -> 532.00s]  It may be difficult to envision a suitable clutter model
[532.00s -> 534.00s]  when we have unlimited resolution.
[534.00s -> 537.00s]  However, we can actually obtain a reasonable model
[537.00s -> 541.00s]  by starting out with a clutter model for limited sensor resolution,
[541.00s -> 544.00s]  and simply increase the number of resolution cells,
[544.00s -> 546.00s]  such that all cells become smaller.
[546.00s -> 549.00s]  Let us look at what happens as we increase the number of cells.
[549.00s -> 554.00s]  First, we realize that it's theoretically possible to obtain more clutter detections,
[554.00s -> 558.00s]  since there are more cells that might generate a clutter detection.
[558.00s -> 563.00s]  On the other hand, the probability of obtaining a detection in a single cell decreases.
[563.00s -> 567.00s]  And the expected number of detections is constantly v times lambda,
[567.00s -> 569.00s]  as we increase the value of j.
[569.00s -> 572.00s]  This follows from the fact that the number of clutter detections
[572.00s -> 577.00s]  is binomially distributed with parameters j and v times lambda over j,
[577.00s -> 579.00s]  discussed on the previous slide.
[579.00s -> 582.00s]  Also, and this is quite an interesting result,
[582.00s -> 586.00s]  the binomial distribution that describes the number of clutter detections
[586.00s -> 591.00s]  converges to Poisson distribution as the number of cells increases.
[591.00s -> 594.00s]  Coming back to our example, we can compare the Poisson distribution
[594.00s -> 598.00s]  to the binomial distribution as we increase the number of cells.
[598.00s -> 602.00s]  For 4 cells, the binomial distribution is very different
[602.00s -> 605.00s]  compared to a Poisson distribution with the same expected value.
[605.00s -> 607.00s]  But already with 16 cells,
[607.00s -> 611.00s]  the binomial distribution is much closer to the Poisson distribution.
[611.00s -> 614.00s]  If we increase the number of cells further,
[614.00s -> 616.00s]  the distributions become more and more similar.
[616.00s -> 620.00s]  Finally, in the limit as j goes to infinity,
[620.00s -> 624.00s]  the distribution of the set of measurements generated from this model
[624.00s -> 629.00s]  converges to Poisson point process, which is the standard model for clutter.
[629.00s -> 633.00s]  A technical detail here is that when we talk about point processes,
[633.00s -> 636.00s]  we normally refer to a random set of vectors.
[636.00s -> 642.00s]  According to our models, the set of vectors in Ck is a Poisson point process.
[642.00s -> 648.00s]  For convenience, we also refer to the matrix Ck itself as a Poisson point process.
[648.00s -> 651.00s]  That is, in the context of this course,
[651.00s -> 655.00s]  a Poisson point process could be either a random set or a random matrix,
[655.00s -> 659.00s]  and it will be clear from the context which one we are referring to.
[659.00s -> 662.00s]  As for the properties of the matrix Ck,
[662.00s -> 666.00s]  note that it contains a random number of random measurement vectors,
[666.00s -> 670.00s]  and its distribution is significantly more complicated
[670.00s -> 673.00s]  than the three scalar distributions that we discussed earlier.
[673.00s -> 677.00s]  You will learn more about Poisson point processes in the next video,
[677.00s -> 680.00s]  but hopefully you can already imagine some of the properties
[680.00s -> 683.00s]  that you get by shrinking all the cells.
