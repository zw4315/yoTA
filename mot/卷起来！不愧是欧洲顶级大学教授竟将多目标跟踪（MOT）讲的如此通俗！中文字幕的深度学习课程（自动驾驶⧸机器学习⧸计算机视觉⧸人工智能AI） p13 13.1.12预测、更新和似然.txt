# Detected language: en (p=1.00)

[0.00s -> 3.04s]  Given that we have a motion model and a measurement model,
[3.04s -> 6.14s]  let's return to the Bayesian filtering recursion.
[6.14s -> 8.60s]  The first step in the recursion is known as
[8.60s -> 11.08s]  the Chapman-Kolmogorov prediction.
[11.08s -> 14.42s]  Here, we take the density at time k minus one,
[14.42s -> 17.68s]  given measurements up to and including time k minus one,
[17.68s -> 19.92s]  and we also take the transition density,
[19.92s -> 22.54s]  and then we marginalize the previous state.
[22.54s -> 26.48s]  In other words, we marginalize the state at time k minus one.
[26.48s -> 28.56s]  What this does is that it gives us
[28.60s -> 30.40s]  the predicted state density.
[30.40s -> 33.92s]  In other words, the density of the state at time k,
[33.92s -> 38.04s]  given measurements up to and including time k minus one.
[38.04s -> 40.96s]  The second step is the base update.
[40.96s -> 43.06s]  Here, we take the predicted density,
[43.06s -> 45.20s]  the density of the state at time k,
[45.20s -> 48.44s]  given measurements up to and including time k minus one,
[48.44s -> 50.54s]  and we also take the measurement likelihood,
[50.54s -> 53.00s]  so the density of the measurement set,
[53.00s -> 55.60s]  given the state, and then we normalize this.
[55.60s -> 58.40s]  And this gives us the updated density,
[59.28s -> 61.52s]  the density of the object state x at time k,
[61.52s -> 64.40s]  given measurements up to and including time k.
[64.40s -> 67.08s]  So we can see that the predicted density
[67.08s -> 69.20s]  is used in the base update,
[69.20s -> 71.72s]  and the updated density is then used
[71.72s -> 73.40s]  in the next prediction,
[73.40s -> 76.40s]  and this gives us the Bayesian filtering recursion.
[76.40s -> 78.64s]  We predict the posterior density,
[78.64s -> 81.90s]  we do a base update, and then we repeat this.
[81.90s -> 83.44s]  In multiple object tracking,
[83.44s -> 86.32s]  the normalizing constant in the base update
[86.36s -> 89.20s]  is sometimes called the predicted likelihood.
[89.20s -> 92.46s]  What we have here is the predicted object density,
[92.46s -> 95.34s]  given measurements up to time k minus one,
[95.34s -> 97.06s]  and the measurement likelihood.
[97.06s -> 99.44s]  And these two densities multiplied
[99.44s -> 101.60s]  give us the joint density of the measurement
[101.60s -> 102.94s]  and the object state.
[102.94s -> 106.08s]  And when we then marginalize the object state,
[106.08s -> 108.80s]  we get just the density for the measurement.
[109.74s -> 111.60s]  So here we have an example.
[111.60s -> 114.12s]  We have a predicted likelihood that has been obtained
[114.12s -> 117.00s]  by marginalizing some predicted state density
[117.00s -> 118.86s]  and some measurement likelihood.
[118.86s -> 121.96s]  Now let's assume that we have two different measurements,
[121.96s -> 125.20s]  valued 2.5 and 6.9.
[125.20s -> 127.60s]  In a tracking context, we have to reason
[127.60s -> 129.72s]  about which of these two measurements
[129.72s -> 132.28s]  should be associated to the state.
[132.28s -> 134.40s]  We also have to reason about how probable
[134.40s -> 137.38s]  each alternative association is,
[137.38s -> 140.80s]  and we can do this using the predicted likelihood.
[140.80s -> 144.80s]  In this case, we find that the measurement equal to 2.5
[144.80s -> 148.40s]  has a predicted likelihood of about 0.3,
[148.40s -> 151.22s]  whereas the measurement equal to 6.9
[151.22s -> 154.72s]  has a likelihood of much, much less.
[154.72s -> 158.04s]  In other words, the measurement value 2.5
[158.04s -> 160.62s]  is the most likely measurement of these two.
[160.62s -> 162.90s]  And if we had to make a decision
[162.90s -> 164.34s]  which of these two measurements
[164.34s -> 166.40s]  we should associate to the object,
[166.40s -> 170.00s]  a good decision would be to take the most likely one.
[170.02s -> 172.08s]  Later in this course, we're going to learn
[172.08s -> 174.36s]  about how we can use the predicted likelihoods
[174.36s -> 177.94s]  for multiple objects to reason about data association.
