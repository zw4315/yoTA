# Detected language: en (p=1.00)

[0.00s -> 7.52s]  Kevin. Yeah, so this is going to be kind of a lecture with a different flavor than the ones
[7.52s -> 12.88s]  that you've seen before. In particular, it will be much more focused on understanding
[13.52s -> 18.64s]  the theoretical foundations of some of the reinforcement learning algorithms and
[18.64s -> 25.76s]  protocols that you've seen in class. Now, if we take a big step back and try to
[26.72s -> 31.44s]  take a look at all the algorithms that you've seen in class and think about potential applications
[31.44s -> 39.04s]  to the real world, you will see that there are still some challenges. One challenge is, for example,
[39.04s -> 48.80s]  designing a stable reinforcement learning algorithm. In particular, this might require some
[48.80s -> 57.04s]  designing of certain tricks to ensure stability of the reinforcement learning algorithms, and often
[57.04s -> 62.32s]  it translates into tuning some hyperparameters to achieve a certain performance.
[64.64s -> 71.12s]  Now, another key issue about applying reinforcement learning power diagrams to the real world
[71.12s -> 77.28s]  is data efficiency. In general, reinforcement learning algorithms are extremely data hungry,
[77.28s -> 83.52s]  and they do require much more data than, for example, algorithms that we commonly use in supervised
[83.52s -> 91.68s]  learning. Then there is the issue about generalization. Often, a specific algorithm is tuned
[91.68s -> 98.32s]  and it learns on a specific task, but it is much more difficult to design general class,
[99.52s -> 104.08s]  general purpose algorithms that can perform well across completely different tasks.
[107.76s -> 113.12s]  Another issue is computational efficiency. If you try to do some of the homework in the class,
[113.68s -> 119.68s]  you will see that sometimes it takes quite a long time to train. Now, all these issues
[120.80s -> 127.36s]  are kind of specific to reinforcement learning, but they are issues that sort of prevent
[127.92s -> 134.24s]  reinforcement learning from being applied more broadly in real world problems, where issues like
[134.24s -> 141.76s]  stability, convergence, and sample efficiency becomes really fundamental. In particular,
[141.76s -> 147.20s]  if you're interacting in the real world, somehow you would like some predictability for what the
[147.20s -> 153.92s]  algorithm is going to do, and also samples are generally quite expensive because they amount
[153.92s -> 162.40s]  to interaction with the real world. And so in this talk, we will try to take sort of a step back
[162.96s -> 168.80s]  and try to understand some of the foundations for the reinforcement learning algorithms.
[171.44s -> 176.64s]  And why do we want to develop sort of a theory for reinforcement learning? Well,
[178.32s -> 184.48s]  perhaps the most basic motivation is that really the key basic protocols that people use
[184.48s -> 190.72s]  in reinforcement learning are really algorithms that are motivated by theory. For example,
[190.72s -> 197.52s]  value iteration, policy iteration, upper confidence bound exploration, reinforced policy
[197.52s -> 204.08s]  gradients. Those algorithms are all algorithms that at least they are somewhat inspired by
[204.08s -> 209.44s]  theory, and oftentimes they come with guarantees, at least in their simplest form.
[211.44s -> 217.36s]  There are also some success stories of translating algorithms that were developed from theory
[217.36s -> 222.72s]  into something more practical. One example is randomized least square value iteration,
[222.72s -> 225.76s]  which you might know as a bootstrap DQN.
[227.84s -> 232.40s]  Moreover, theory can give you some consideration that apply not just to a specific
[232.40s -> 238.00s]  problem, the one that you're trying to solve, but maybe more broadly to a wide class of
[238.00s -> 242.80s]  problems, and I would say more broadly to the field over here. And also they help uncover
[243.52s -> 248.64s]  fundamental limits, for example, things that you cannot do, and we will see an example of that
[249.68s -> 259.12s]  today. Now, what question would we like to ask from a theoretical point of view? Well, ideally,
[260.88s -> 266.32s]  we would like to have some form of guarantees for an algorithm that we are studying. For
[266.32s -> 271.68s]  example, if you're proposing a new algorithm, you would like to understand whether it converges,
[271.68s -> 276.80s]  and that's kind of the primary concern that you might have when you go ahead and you want to
[276.80s -> 282.56s]  apply it to your problem. You would like to understand how to choose the hyperparameters,
[282.56s -> 288.40s]  whether there is any sort of ways or formula or trade-off that you need to make.
[290.64s -> 297.36s]  Another question is how much data does the algorithm need to collect in order to achieve
[297.36s -> 301.04s]  a certain level of performance? So, how many interactions?
[302.72s -> 308.16s]  And also you might be concerned with things like computational complexity, and so running time of
[308.16s -> 316.00s]  the algorithm. This is what you would like to study, but in reality, answering those questions
[316.00s -> 321.92s]  is extremely challenging. It's extremely difficult. For example, for most of the
[322.88s -> 329.60s]  deeper algorithms, it's not even possible to prove convergence, because at the end of the day,
[329.60s -> 335.84s]  the basic temporal difference scheme, or to deal with experience replay and target networks,
[335.84s -> 340.64s]  they are not always guaranteed to converge. And so immediately we have sort of a challenge,
[341.84s -> 347.84s]  and it turns out that answering those questions is generally extremely difficult. And so what
[347.92s -> 353.36s]  you will see today is that there is kind of a huge gap right now between the practical algorithms
[353.36s -> 358.40s]  that you've seen in the class and some of the considerations that we will go through today.
[360.48s -> 367.28s]  But at any rate, I will focus mostly on understanding the statistical aspects of RL,
[367.28s -> 374.08s]  and so how many samples do you need to learn certain problems. And I will look into sort of
[374.16s -> 381.20s]  three different macro topics. One is about trying to understand what the reinforcement learning
[381.20s -> 386.80s]  problems are easy and harder, and whether we can learn faster on easier problems.
[388.40s -> 394.88s]  And then I will focus on understanding the interplay between RL algorithms and function
[394.88s -> 402.08s]  approximation, so the issue of generalization. I will talk briefly about statistical limits,
[402.08s -> 407.92s]  what you cannot do with reinforcement learning algorithms, and also briefly about offline
[407.92s -> 415.28s]  reinforcement learning. Now let's get to sort of the first part, understanding what problems
[415.28s -> 421.04s]  are easy and what problems are hard. The second thing that we consider here is
[422.00s -> 428.88s]  the exploration problem. I think most of the class that you've gone through is about
[429.60s -> 435.92s]  exploration algorithms. Think about the standard online setting DQN, all these. And so in this
[435.92s -> 441.04s]  setting, you have a reinforcement learning agent that is starting with an empty data set,
[441.92s -> 447.12s]  and there is an interaction for h steps until the end, for example, of a game.
[447.92s -> 451.36s]  And this interaction is ongoing and it continues for a number of episodes.
[452.08s -> 455.92s]  And you would like to measure how quickly a reinforcement learning agent is learning.
[456.48s -> 461.20s]  Now, intuitively, the reinforcement learning agent starts with a policy that might be
[461.20s -> 465.52s]  suboptimal. If it is playing Atari, the first policy is going to be bad if you start with
[465.52s -> 470.80s]  an empty data set, but then progressively it is going to learn and play policies that are
[470.80s -> 477.92s]  better and better. What we would like to do is to measure the performance of the algorithm
[478.56s -> 484.24s]  and the standard way to do it. Let me try to move this thing.
[489.92s -> 497.20s]  The standard way to do it is to define a quantity that is called regret,
[498.56s -> 504.56s]  which you might have seen in class. And it's really the sum of the suboptimality gaps
[504.56s -> 510.32s]  of the policy played by the agent. Intuitively, at least if the problem is easy,
[511.60s -> 515.20s]  an algorithm that is learning will approach, in terms of performance,
[515.92s -> 521.20s]  the value of the optimal policy. But it will start in a way that it doesn't know much.
[521.20s -> 524.72s]  And so the initial value of the policy that it plays are going to be low.
[525.76s -> 531.28s]  And if we sum all the suboptimality gaps as a function of the episode,
[532.08s -> 538.80s]  well, that would amount to computing the integral of this curve, so the area shaded in orange.
[540.40s -> 543.44s]  And that's what we call the regret of the algorithm.
[547.28s -> 553.76s]  Our goal will be to try to design an algorithm that minimizes the regret.
[555.04s -> 560.24s]  Now, in most cases, you can't do that. For example, in deep RL, it's not so clear
[560.24s -> 566.72s]  that you can do that. And so we will focus on problems that are, for the first part of today,
[567.28s -> 571.68s]  with small state and action spaces. It's a problem where we have a tabular representation.
[573.68s -> 581.44s]  And if we go back to maybe 2010, 2011 and subsequent years, in the foundations of RL,
[581.44s -> 587.68s]  there was a huge push to try to design algorithms that could be as efficient as possible
[587.68s -> 593.92s]  on tabular problems. In particular, several algorithms have been proposed. And this,
[593.92s -> 598.16s]  they had some form of regret bound. There was a function of the state and action space,
[598.16s -> 605.36s]  in particular, its cardinality, the horizon, and the number of episodes.
[608.32s -> 617.12s]  Those regret bounds are useful because they apply broadly to any problem that is
[617.12s -> 621.20s]  a Markov decision process. You don't need to worry about the specifics of the problem,
[621.20s -> 627.60s]  as long as you have finite state and action space, you have a guarantee on these algorithms.
[629.60s -> 635.44s]  And this is also their limitation in the sense that it is not clear whether a certain
[635.44s -> 639.60s]  algorithm would perform better or worse if the problem had a certain structure.
[640.80s -> 645.68s]  And this is what we see in practice, that the performance of reinforcement learning algorithms
[645.68s -> 651.28s]  varies greatly, even for the same algorithm, on problems that are quite different.
[651.84s -> 657.76s]  And so we would like to start and try to derive some systematic understanding of what
[657.76s -> 663.28s]  problems are difficult and what problems are easy to explore in reinforcement learning.
[666.08s -> 674.96s]  Now, from an historical point of view, there has been a lot of effort into improving those
[674.96s -> 682.32s]  regret bounds until, essentially, we got one algorithm that, in terms of worst-case performance,
[682.88s -> 689.36s]  it was unimproveable, meaning that it had a performance guarantee across all problems that
[689.36s -> 695.68s]  was as good as possible given the lower bound that we knew, meaning that the performance is
[695.68s -> 703.20s]  not improvable without any limit. There is a fundamental limit that you cannot surpass.
[703.68s -> 709.76s]  At the same time, we know that there are classes of problems that are very different
[710.32s -> 718.08s]  from the type of contrived construction that creates the lower bound. One example
[718.08s -> 725.20s]  is a problem that has no dynamics or weak memory. A problem that has weak memory is a problem
[725.20s -> 731.76s]  where the action that you took in the past, they have really little impact on your state.
[732.56s -> 739.92s]  Think about a recommender system, which is a type of contextual bandit problem. Well, that is
[739.92s -> 748.08s]  a situation in which this weak memory sort of arises. In a recommender system, think about
[748.08s -> 755.52s]  a customer coming to Amazon. If you make a bad recommendation, intuitively, you might make a
[755.60s -> 761.52s]  certain customer unhappy, but this won't affect the next customer that you see.
[762.56s -> 770.64s]  That's a problem of weak memory. For bandit problems, we do know that there are specific
[770.64s -> 776.32s]  bandit algorithms to take advantage of the structure, and they are able to learn much faster
[776.32s -> 784.40s]  than classical Markov decision processes. Likewise, problems that are deterministic
[784.40s -> 788.48s]  are generally much easier. It's essentially a search problem.
[790.00s -> 794.88s]  Likewise, problems where you can only move locally in the state and action space are
[794.88s -> 800.80s]  generally problems that are easier, because if you make a mistake, you can still recover somehow.
[800.80s -> 808.32s]  One example is mountain car. Now, the question that we ask is if we treat these problems as
[809.20s -> 813.84s]  popular problems where we have an explicit representation of the state and action space
[813.84s -> 819.12s]  and the dynamics, what do these easy problems have in common? Can we identify some common
[819.12s -> 826.16s]  characteristics and try to measure how hard they are, and can we learn faster if the
[826.16s -> 832.48s]  problem belongs, if the actual problem instance that we face belongs to some of these subclasses?
[833.36s -> 840.48s]  We gave a positive answer to this, and we proposed first a problem-dependent complexity
[840.48s -> 844.96s]  and then an algorithm based on that that had certain specific properties.
[846.00s -> 850.32s]  First of all, we proposed some problem-dependent complexity measure
[850.96s -> 857.44s]  that characterizes the complexity of different reinforcement learning problems.
[858.40s -> 864.64s]  In particular, it is defined by the interaction of the system dynamics
[865.36s -> 874.24s]  and the value of the optimal policy. It is defined as the variant of the next state
[874.24s -> 884.00s]  optimal value function, and this is not something that the algorithm can compute if you do not know
[884.08s -> 888.48s]  the actual Markov decision process because the optimal value function is unknown and the
[888.48s -> 895.28s]  dynamics are also unknown, but nonetheless, you can design an algorithm that has a performance
[895.28s -> 901.44s]  bound that scale with this quantity, which is generally unknown, and the algorithm doesn't
[901.44s -> 908.56s]  need to know that. As a result, the algorithm is able to match the best performance for
[909.44s -> 914.00s]  tabular Markov decision processes, meaning that it is minimax-optimal, it is an improvable,
[914.56s -> 918.96s]  but compared to the state-of-the-art, it can also attain the optimal performance
[920.16s -> 924.64s]  if the problem belongs to a certain class of easier problems. For example,
[924.64s -> 929.52s]  if it is a contextual bandit problem, then the algorithm automatically matches
[930.80s -> 935.60s]  essentially the performance of basic UCB on contextual bandits.
[939.12s -> 946.08s]  In addition to being analytically small on certain problems and subclasses, you can evaluate the
[946.08s -> 954.88s]  quantity numerically, and it's going to appear here on problems that people have considered
[954.88s -> 961.68s]  before. It takes a value that is much smaller than sort of a worst-case value that was suggested
[961.68s -> 969.20s]  by prior bounds. So essentially, it is a quantity that it is bought analytically small on problems
[969.20s -> 975.12s]  that we care about, but also numerically small on problems that have been considered before.
[977.68s -> 981.20s]  Now, I want to pause one second and ask if there is
[981.20s -> 985.36s]  any technical question on this part before I move ahead.
[991.68s -> 1003.92s]  The intuition, well, it really depends on the type of problem. So, for example,
[1005.60s -> 1008.88s]  if a problem has weak memory, it's a contextual bandit problem,
[1010.32s -> 1016.88s]  what happens is that a mistake that you might make in a certain state, it doesn't really have
[1016.88s -> 1025.84s]  long-term consequences. And so the next state value function, it wouldn't be too much different
[1025.84s -> 1034.08s]  across different states. And so essentially this quantity has to be small. You might make an
[1034.08s -> 1040.00s]  error, but you only lose with the current customer, right? You don't mess up the entire
[1040.96s -> 1048.32s]  long-term plan, right? And so this quantity ends up being small. Think about this being
[1050.40s -> 1057.52s]  some challenge in estimating the effect of transitions, but the transitions can be highly
[1057.52s -> 1064.40s]  stochastic. For example, again, in bandits, they are highly stochastic, but still there is
[1064.40s -> 1069.68s]  not much variability in the value of the state that you end up with. In that case,
[1069.68s -> 1080.00s]  it's going to be small. The supremum, you can relax it as expectation over trajectories.
[1082.32s -> 1084.80s]  It is supremum in the actual work, but you can relax.
[1084.80s -> 1099.60s]  Okay. I want to give one slide that is perhaps a bit more technical
[1101.68s -> 1105.12s]  about how do we go about achieving something like this.
[1108.00s -> 1114.64s]  Well, exploration generally is typically achieved, at least for probably efficient algorithms,
[1114.64s -> 1122.96s]  by adding an exploration bonus to the experience reward. Think about DQN, exploration there is done
[1122.96s -> 1129.12s]  with epsilon greedy, at least in the most basic format. But if you want more sophisticated
[1129.12s -> 1136.40s]  schemes, think about UCB in bandit algorithms. Normally what's done is a bonus is added.
[1136.96s -> 1143.52s]  Now the bonus can take different forms. The most basic one, the prior route was using,
[1144.16s -> 1151.76s]  is something that scales with the inverse of the number of samples. It comes from often inequality.
[1153.12s -> 1159.84s]  But this type of exploration bonus is essentially problem independent, meaning that
[1160.48s -> 1166.56s]  it's not tied to any particular feature of the MDP. And so the algorithm would explore in
[1166.56s -> 1172.08s]  the same way regardless of the problem. And this won't give rise to problem dependent bounds.
[1172.64s -> 1180.16s]  Now the ideal choice that one would like to make is to use some form of Bernstein-based
[1180.16s -> 1186.96s]  concentration inequality, which does indeed contain something very similar to the quantity
[1186.96s -> 1192.96s]  that we want. It would give rise to problem dependent bounds, but there is one issue that
[1192.96s -> 1199.84s]  in general, the optimal action value function, you don't know what it is, and the transition
[1200.72s -> 1206.48s]  dynamics, you also don't know what it is. So although this choice of the bonus will be ideal,
[1208.08s -> 1213.84s]  it would not practically give rise, it's not something that you can do in practice.
[1215.68s -> 1222.08s]  The way around it is sort of intuitive, is to try to use the empirical dynamics and
[1222.08s -> 1226.88s]  some empirical estimate of the optimal value function. But there are several challenges
[1226.88s -> 1232.16s]  that arise if you try to do that. The main challenge is that generally those quantities,
[1232.72s -> 1237.28s]  they are unknown. Think about when you start initially, you know very, very little about
[1237.92s -> 1244.16s]  the dynamics. And so you have essentially no way to guess what these quantities are. And if you
[1244.16s -> 1251.44s]  take the wrong guess, essentially the algorithm might not be optimistic enough, it might not
[1251.44s -> 1259.36s]  explore enough, and it would just not find a good policy. So what you have to do is rather
[1259.36s -> 1266.32s]  to introduce some correction terms. Thankfully, those correction terms that try to correct for
[1266.32s -> 1275.68s]  your wrong estimates, they decay very quickly. So they decay at a faster rate. And so it is
[1275.68s -> 1282.88s]  as if the agent was applying sort of the correct Bernstein-based concentration inequality,
[1282.88s -> 1288.96s]  but with one correction term that is decaying very quickly. And the challenge here lies in
[1290.88s -> 1296.72s]  estimating the size of the correction, in particular because we have to correct some
[1296.72s -> 1304.00s]  value function that is different from the optimal one, and estimating those errors requires
[1304.00s -> 1312.32s]  estimating however propagates through the MDP from states that perhaps we haven't even visited
[1312.32s -> 1325.28s]  them at. And this choice essentially gives rise to those problem-dependent bounds. Now,
[1325.28s -> 1335.76s]  this is good because it does give you some initial sort of strong understanding of whether
[1335.76s -> 1341.84s]  it's possible to adapt to the problem difficulty, and whether it's possible to be at the same time
[1341.84s -> 1347.04s]  minimax optimal, but also instance optimal on the variety of problem classes that we are
[1347.04s -> 1354.00s]  interested in. But the big limitation here is that, of course, this thing applies only to small
[1354.00s -> 1363.76s]  state and action spaces. In practice, we would like to tackle problems that have a very large,
[1363.76s -> 1371.60s]  potentially continuous state and action space. And to be clear, what you've seen in the class
[1371.60s -> 1376.96s]  is always in this second category. As soon as you start using any form of functional approximation,
[1377.68s -> 1382.56s]  you are in this category. And so the next question that we will try to understand
[1383.28s -> 1390.08s]  is what can we say about reinforcement learning with function approximation? And the answer will
[1390.08s -> 1397.44s]  turn out to be a bit more negative than here. Here we made some sort of positive progress,
[1398.32s -> 1402.80s]  but here we will see that when you start to talk about reinforcement learning with
[1402.80s -> 1408.88s]  function approximation, even problems that seem to be easy, they might be very challenging.
[1408.88s -> 1416.88s]  And so to do a quick recap, practical problems, they always have a state space that
[1417.60s -> 1422.96s]  is extremely large. Most states have never visited. What we would like to do is to
[1422.96s -> 1429.28s]  introduce some form of function approximation that can add generalized knowledge from the
[1429.28s -> 1435.68s]  states that we have seen to states that we have not yet observed. And the hope is that
[1436.24s -> 1443.12s]  we do not need to learn what to do in every state. Rather, we need only a number of samples
[1443.12s -> 1448.96s]  that is roughly of the same order as the number of parameters in our model.
[1453.92s -> 1460.56s]  Now, the observation that, the full global observation, if you want, that we have is that
[1461.52s -> 1464.40s]  reinforcement learning algorithm, they use function approximation.
[1465.20s -> 1472.80s]  They still need a lot of samples compared to supervised learning. And so we would like to ask
[1472.80s -> 1478.72s]  a very basic question whether reinforcement learning is, for example, fundamentally
[1478.72s -> 1487.36s]  more difficult than classical supervised learning. And in order to study this question,
[1488.16s -> 1496.16s]  we consider a setting that is very similar to the offline reinforcement learning setting that
[1496.16s -> 1501.84s]  you have seen in the second part of the class. In the offline reinforcement learning setting,
[1501.84s -> 1508.48s]  you have some data set that is available and it consists of state, actions, reward, and
[1508.48s -> 1515.84s]  successor states. And we try to ask questions about, for example, policies that might be
[1515.84s -> 1520.88s]  different from the one that generated the data set. You might, for example,
[1520.88s -> 1526.08s]  want to try to identify the optimal policy or you might try to do off-policy evaluation.
[1529.60s -> 1536.08s]  The specific setting that we consider is one in which we allow some sort of data collection with
[1536.08s -> 1542.64s]  a static distribution before. And the reason to do that, to allow for some flexibility is
[1542.64s -> 1547.76s]  because if the data set is poor, intuitively we cannot do much. And that's not the algorithm
[1547.76s -> 1553.76s]  fault, it's just the data set. Maybe I have just data on a single statement. So we do consider
[1553.76s -> 1559.04s]  a case in which you can do some form of data collection with a static policy beforehand.
[1559.76s -> 1567.04s]  And then we try to understand whether we can successfully predict the value of a different
[1567.04s -> 1571.20s]  policy, for example, or extract the value of the optimal policy.
[1574.08s -> 1582.64s]  Now, our expectation is that if the action value function has a simple representation,
[1582.64s -> 1589.68s]  for example, if the action value function has a linear expansion and perhaps we even know the
[1589.68s -> 1596.56s]  feature extractor, then this should be an easy problem. Why? Well, it's just by analogy with
[1596.64s -> 1604.56s]  linear regression. If you are solving a regression problem and I give you a feature map and I promise
[1604.56s -> 1610.72s]  that the problem is realizable, so the target do have some linear expansion, perhaps with some
[1610.72s -> 1618.48s]  noise, then you can open a textbook in statistics and you will see that standard linear
[1618.48s -> 1625.84s]  regression can learn this problem very quickly. However, in reinforcement learning,
[1626.64s -> 1632.00s]  even problems that are linear, they don't seem to be so easy. In particular, there have been
[1633.28s -> 1640.00s]  examples of divergence of classical TD and fitted Q, even on problems that are linearly
[1640.00s -> 1647.44s]  realizable. And in fact, if you take a look at the analysis that are available for
[1648.56s -> 1654.40s]  some of the basic algorithms and protocols, you will see that they all make some assumptions
[1654.40s -> 1661.28s]  that seem to be much more stronger than just realizability. And so as a matter of fact,
[1662.40s -> 1672.08s]  we don't know in 2020, 2021, whether even the simplest linear setting is something that we can
[1672.08s -> 1678.32s]  provide stable algorithms for. Can we provide an algorithm that, for example, converges and
[1678.40s -> 1684.32s]  converges, yes, because you can use LSTD, but can we have any guarantee about, for example,
[1684.32s -> 1688.64s]  the amount of samples that are required to learn even in this simple setting, which is
[1688.64s -> 1693.76s]  the first step after tabular problems. And really to understand what's happening,
[1695.36s -> 1702.32s]  you need to compare supervised learning with reinforcement learning. And the key difference is
[1702.32s -> 1706.88s]  whether you're trying to make predictions for one time step or for many time step.
[1708.72s -> 1714.88s]  This is because if you're trying to make predictions for one time step and you start
[1714.88s -> 1720.48s]  with a data set that you might have recollected in an intelligent way, well, if you're trying to
[1720.48s -> 1726.08s]  just predict the first reward and you have the promise that the reward function is linear,
[1726.72s -> 1732.00s]  then we know that linear regression solves this problem very, very quickly. So we know
[1732.00s -> 1736.08s]  an algorithm and we know guarantees as well. And this is the most basic
[1736.80s -> 1745.20s]  machine learning algorithm that you can think of. However, our question is what happens
[1745.20s -> 1753.76s]  if we want to predict the value of a policy for multiple time steps with a guarantee
[1754.56s -> 1761.04s]  that that value is actually realizable, meaning that we have a feature extractor
[1762.00s -> 1766.96s]  that correctly predicts the value of the target policy for some data parameter.
[1767.84s -> 1774.56s]  It turns out that this problem is in the worst case extremely difficult, meaning that
[1776.32s -> 1782.48s]  as opposed to supervised learning, you can find problems where you have this beautiful linear
[1782.88s -> 1791.68s]  model and yet any algorithm would take a number of samples to make the correct predictions
[1791.68s -> 1799.36s]  that is exponential in the dimensionality of the feature extractor. And when I say predictions,
[1800.08s -> 1805.20s]  you can intend this broadly, meaning that the answer would remain the same
[1806.16s -> 1809.76s]  if you were trying to, for example, identify an optimal policy.
[1810.64s -> 1817.04s]  I want a policy that does better than random. You still need a number of samples that in the
[1817.04s -> 1822.08s]  worst case might be exponential in the dimensionality of the feature extractor.
[1822.80s -> 1830.16s]  And so we see that there is a strong separation between what is achievable in supervised learning,
[1830.16s -> 1835.44s]  which is concerned with making predictions. And so if you want a one-step prediction
[1836.16s -> 1843.76s]  and reinforcement learning, which considers sequential processes, as the horizon becomes
[1843.76s -> 1850.24s]  longer, the problems can become exponentially harder. This doesn't mean that all problems
[1850.24s -> 1855.36s]  are exponentially harder, but it does tell you that even problems that appear to be simple,
[1855.36s -> 1859.60s]  the problems that should be linear, and so they should be easily learnable,
[1860.40s -> 1866.80s]  you will not be able to find an algorithm that has guarantees even on those problems.
[1866.80s -> 1872.08s]  And so for you in order to learn, for an algorithm to learn, there has to be some
[1873.44s -> 1879.12s]  additional special structure. And indeed this is something that we sort of see,
[1880.16s -> 1887.04s]  that indeed poor sample complexity is a major issue in RL, and this issue is also related to
[1887.92s -> 1894.24s]  divergence. But the contribution here is really to identify that those issues are
[1894.24s -> 1898.56s]  algorithm independent. They have information theoretic, meaning that there is some
[1898.56s -> 1904.24s]  fundamental hardness in the reinforcement learning problem that applies broadly to all algorithms
[1904.24s -> 1908.48s]  that you can come up with. You will not be able to find an algorithm that is able to
[1908.48s -> 1914.88s]  solve all problems, even if they are as simple as linear. And this issue has been studied
[1914.88s -> 1922.32s]  more broadly by some other sort of important papers, and some have sort of similar results.
[1923.04s -> 1931.36s]  And if you want to sort of reinterpret this second section, you can also take a look at
[1931.36s -> 1937.52s]  that from the point of view of online RL. I might have an action value function, think about
[1937.52s -> 1943.20s]  what you have in DQN, and instead of having a deep neural network, you just have a simple linear
[1943.28s -> 1949.36s]  map. And I promise to you that the problem really does have a linear action value function.
[1949.92s -> 1955.52s]  Still, you will not be able to find an algorithm that can learn
[1956.72s -> 1960.88s]  polynomially faster in a problem that is linear.
[1963.36s -> 1967.84s]  And so the main takeaway here is that linear regression is easy to start, but the equivalent
[1967.84s -> 1974.88s]  in reinforcement learning from a model-free point of view is already out of reach. And so
[1974.88s -> 1982.00s]  we have to be sort of not too optimistic about the type of problems that we can solve.
[1984.72s -> 1989.04s]  And there has been indeed a big effort trying to understand what additional conditions are
[1989.04s -> 1995.76s]  necessary in order to have polynomial sample complexity as we have for many statistical
[1996.64s -> 2003.44s]  algorithms in statistics. Now, before we move forward, is there any question on
[2005.12s -> 2010.64s]  sort of this second section? Yeah.
[2025.76s -> 2038.88s]  I think the thing that is really tricky is that
[2041.52s -> 2048.88s]  this is really a model-free point of view, right? So we're looking at whether we have
[2048.88s -> 2056.64s]  enough information on the Q-values. But somehow you can have problems that are extremely complex,
[2058.88s -> 2065.68s]  but the action value function ends up being simple. It ends up being sort of linear.
[2065.68s -> 2072.32s]  So the actual counterexample has essentially a reward function that is very complex. It's
[2072.32s -> 2079.44s]  like a ReLU neural networks that is non-zero only in a very hidden area of the state space,
[2079.44s -> 2085.52s]  which is exponentially larger. The dynamics are very complex, and the dynamics are sort of
[2085.52s -> 2089.84s]  engineered in a way that they linearize the reward function in the sense that once you do
[2089.84s -> 2096.16s]  many steps of the Bellman backup, you end up with an actual value function that looks linear.
[2096.16s -> 2100.32s]  And so it looks like the problem is easy because that thing is really linear.
[2100.32s -> 2105.36s]  But what you're really trying to do is to identify where the reward function is non-zero
[2105.92s -> 2113.44s]  in a sort of an exponentially larger sphere. I don't know if I can say much more than this,
[2113.44s -> 2120.32s]  but it's really related to the high dimensionality that you have, that there's
[2120.32s -> 2125.92s]  a lot of space in high dimension. If you take random vectors in high dimension, that will almost
[2125.92s -> 2130.96s]  always be orthogonal. And so you can sort of hide information in very high dimension.
[2130.96s -> 2136.16s]  It's not something that is obvious in 2D or 3D. You really have to go high dimension.
[2141.60s -> 2145.68s]  Yeah, the policy pie is fixed. It could be, you know, think about
[2146.40s -> 2150.16s]  predict the value of the optimal points. It could be fixed or it could be, you know,
[2150.16s -> 2167.60s]  the optimal one. I do want to spend one slide talking about,
[2169.76s -> 2177.20s]  indeed, what happens with more general function approximation in terms of positive results.
[2177.20s -> 2180.80s]  Well, if you open some book about statistics, high dimensional statistics,
[2181.44s -> 2185.68s]  at least for regression, you will see that there are performance guarantees
[2186.96s -> 2191.68s]  that are a function of the very function class you're using. So if you're using kernel methods,
[2191.68s -> 2198.00s]  convex functions, or other things, you will have some performance bound, some trade-off between
[2198.00s -> 2203.04s]  approximation error and statistical complexity. And the statistical complexity
[2203.04s -> 2210.48s]  is normally expressed in sort of more notions like about the market complexity,
[2210.48s -> 2215.76s]  busy dimensions, and other things. But the same is not sufficient in a way.
[2215.76s -> 2223.12s]  So it looks like the interplay between Bellman operator and the function, the very same function
[2223.12s -> 2228.64s]  class that you use to model the action value function for 2D methods, but the interplay is
[2228.64s -> 2235.84s]  really kind of important. And so what people have focused on, to understand some foundations
[2235.84s -> 2240.80s]  overall, it's not just about the complexity of the function class. This is not sufficient.
[2240.80s -> 2245.60s]  Like we saw before, we have a linear map and that is already too hard. Well, there has to be
[2246.24s -> 2250.08s]  something that makes the problem sort of learnable. And that's really the interaction
[2250.08s -> 2254.16s]  between Bellman operator and the actual value function. The reason why that's essential for
[2254.16s -> 2259.12s]  2D methods is that you're taking an action value function, you're creating the Bellman
[2259.12s -> 2263.12s]  backup out of it, and you're fitting sort of the same, and you want that to be zero.
[2264.00s -> 2268.80s]  And so the interaction becomes critical. And so many notions have been proposed to try to
[2268.80s -> 2275.84s]  understand in what cases can you do this sort of learning in a way that is stable and
[2277.12s -> 2283.84s]  statistically efficient. But I won't go into that. Instead, I'm going to jump to
[2285.12s -> 2291.04s]  some offline reinforcement learning. Offline reinforcement learning, you've seen it already
[2291.04s -> 2296.96s]  in the classes, but just to do a quick recap, there's already a lot of data out there, so we
[2296.96s -> 2304.40s]  would like to leverage them. How can we do so without collecting further data? And the setting
[2304.40s -> 2309.36s]  is the same as that that you've seen in class. We have an historical data set of state actions
[2310.00s -> 2316.88s]  with reward and successful states, and the task is how do we find the policy with the highest value?
[2316.88s -> 2323.60s]  What does it even mean to find the policy with the highest value given a data set? Well,
[2323.60s -> 2329.76s]  the highest value, of course, is the policy that is the optimal policy, but your data set
[2329.76s -> 2336.32s]  may contain no information about the optimal policy. And so it's like we have to make sort of a
[2336.32s -> 2345.52s]  best effort, some best effort in trying to identify a good policy and lower our expectation
[2346.64s -> 2352.32s]  and perhaps not find the optimal policy. The main challenge, I think you have discussed
[2352.32s -> 2356.64s]  this in class as well, is that of distribution shift, meaning that
[2357.60s -> 2364.72s]  the data set, well, a best case scenario, which never happened, is one in which your data set has
[2366.08s -> 2372.72s]  uniform samples all over the state and action space, in which case you could just try to
[2372.72s -> 2378.72s]  evaluate policy by one, by two, and by three, and pick up the best. But normally what you've
[2378.72s -> 2386.16s]  given is projectories that they might be, for example, from humans, and so they're generally
[2386.16s -> 2394.32s]  narrowly concentrated, and that's what we call problem of partial coverage. And in the example
[2394.32s -> 2400.16s]  here, the data set may have a lot of information about pi1, no information about pi2,
[2400.16s -> 2405.60s]  and some information about pi3, and somehow you have to come up with and choose between the
[2405.60s -> 2411.36s]  three and figure out which policy is the best. And the thing that they want to sort of
[2411.36s -> 2418.80s]  highlight today is how do we even measure this coverage, how much information the data set
[2418.80s -> 2435.52s]  contains to find a good policy. And intuitively, the way to solve this problem is to find
[2436.32s -> 2442.48s]  what you've seen in class, or actually there are two ways if you want. One is to try to stay close
[2442.48s -> 2450.72s]  to the policies that generated the data set, some form of behavioral cloning. Another way
[2450.72s -> 2457.76s]  is to attempt to estimate the uncertainty about your predictions, so generally your data set
[2458.72s -> 2467.04s]  is generated by policies, certain policies that are sort of narrowly concentrated, and they give
[2467.04s -> 2473.92s]  you data about state actions, reward, and transitions, and you would try to sort of fit
[2473.92s -> 2479.44s]  some form of model and try to use the model to make predictions about the value of other
[2479.44s -> 2484.64s]  policies. Now the model doesn't actually need to be a model, you might do this in a model-free way,
[2484.72s -> 2490.24s]  but you're still using some data that has been generated by some policies and make predictions
[2490.24s -> 2496.80s]  about other policies. Now of course what you would like to do is to pick up the policy that
[2496.80s -> 2504.80s]  has the highest value, but that's not known. Instead you would like to return a policy
[2505.52s -> 2509.68s]  that looks like it has good value, but that you're also reasonably confident about.
[2510.40s -> 2520.08s]  And so one way to look at offline ARAD is as a procedure that tries to find some optimal trade-off
[2520.80s -> 2527.92s]  between value of the policy that is returned and the uncertainty about this policy.
[2527.92s -> 2535.04s]  Think about some bias-variance trade-off in statistics. You would like to have an algorithm
[2535.84s -> 2542.40s]  that has sort of an optimal bias-variance trade-off. The bias is generally unknown.
[2542.40s -> 2548.96s]  The variance you can try to estimate. In offline ARAD there is sort of a similar
[2550.08s -> 2557.60s]  notion if you want. You would like to balance the value of the policy that you return,
[2557.60s -> 2564.00s]  which is unknown to you, with its uncertainty. And so guarantees for offline reinforcement
[2564.00s -> 2570.80s]  learning algorithms, they generally look something like this. One algorithm should return
[2570.80s -> 2578.80s]  with very high probability the best trade-off between the value of the policy that returns
[2578.80s -> 2585.60s]  and its uncertainty, which essentially amounts to finding the point with the highest lower bound.
[2585.60s -> 2590.00s]  In a sense offline reinforcement learning, the one that you've also seen in the class,
[2590.00s -> 2594.32s]  in some way they try to get to this optimal trade-off.
[2596.08s -> 2602.80s]  Now one big question is what is this sort of constant C that depends on the policy? Well,
[2603.68s -> 2609.92s]  if you have seen concentration inequalities in statistics, you might be already familiar
[2609.92s -> 2615.28s]  with the term one divided by square root of n. It's what arises from, for example,
[2615.60s -> 2621.28s]  inequality. But here there is an extra coefficient that depends on the policy,
[2621.28s -> 2628.08s]  which should encapsulate the distribution shift. Now this coefficient depends on the
[2628.08s -> 2634.08s]  actual algorithm and it depends, for example, on the function classes that you're using
[2634.64s -> 2641.12s]  and the interaction with the Bellman operator. And as a concrete instantiation,
[2641.84s -> 2649.20s]  you can take, for example, softmax policies. Think about those that arise from a natural policy
[2649.20s -> 2656.08s]  gradient. And again, for simplicity, linear action value functions, and those are two distinct
[2656.08s -> 2663.60s]  parameters. And you can design algorithms that essentially try to solve this offline
[2663.60s -> 2669.60s]  reinforcement learning problem. And they will have some guarantees that are precisely of this
[2669.60s -> 2677.92s]  form where this coverage coefficient has a certain analytical expression. And the analytical
[2677.92s -> 2683.04s]  expression highlights really the interplay between the information that is contained in
[2683.04s -> 2690.72s]  the data set and the target policy that you're trying to estimate. In particular, the information
[2690.72s -> 2697.60s]  contained in the data set is reflected in the covariance matrix, which is a somewhat familiar
[2698.24s -> 2702.56s]  object from statistics, linear regression. You compute some covariance matrix,
[2702.56s -> 2706.40s]  the covariance contains the amount of information that you know about the problem,
[2706.96s -> 2712.96s]  and these interact with a certain norm in its inverse with the expected feature over
[2714.56s -> 2718.40s]  the target policy that you are considering for the optimization.
[2720.72s -> 2726.88s]  This quantity you know, but this one is generally not computable, right? So this sort
[2726.88s -> 2734.24s]  of tells you how the two interact to create confidence intervals for off-policy evaluation
[2734.24s -> 2742.00s]  that you can use to find a good policy. What is sort of surprising and perhaps not
[2742.00s -> 2749.12s]  surprising but important about this is that this coverage, which is also called concentrability,
[2750.08s -> 2755.76s]  it doesn't really, it doesn't have an expression in the state and action space.
[2756.64s -> 2761.04s]  If you open some of the papers that do statistical analysis, you would often find
[2761.04s -> 2766.64s]  a ratio between busy distribution, discounted busy distribution of the target policies
[2766.64s -> 2771.68s]  versus the behavioral policy, and that is a ratio in the state and action space.
[2772.32s -> 2776.72s]  This one has none of that. It's all projected down to a lower dimensional
[2777.28s -> 2784.72s]  feature space where coverage can indeed be sort of much larger. Think about having a covariance
[2784.72s -> 2791.68s]  matrix that is the identity that would certainly make the coverage coefficient be very small.
[2799.84s -> 2805.92s]  I don't think I want to talk about sort of how we achieve this and sort of the
[2805.92s -> 2811.76s]  technicalities. I think the important part is how would, for example, a guarantee in offline RL
[2811.76s -> 2816.56s]  look like in terms of actual statement, which is what you saw in the prior slide,
[2818.00s -> 2824.48s]  but if you want at a very high level, we're trying to avoid penalizing
[2826.16s -> 2832.96s]  actions directly and we want to retain a very sort of low statistical complexity and we operate
[2832.96s -> 2838.24s]  in the parameter space to compute these confidence intervals and all this is put
[2838.24s -> 2846.80s]  into a big actor critic algorithms that uses natural policy gradient and some pessimistic
[2846.80s -> 2854.48s]  version of TD with target networks where the parameters are moved in a way that computes
[2854.56s -> 2862.16s]  a pessimistic solution. I'm gonna skip the algorithm.
[2864.48s -> 2871.28s]  Now, one limitation of this study that you've seen is that it applies to the linear setting.
[2872.08s -> 2877.76s]  Of course, the question is what happens if I use a richer set of functions, for example,
[2878.88s -> 2882.64s]  offline reinforcement learning with more general function approximations
[2882.64s -> 2889.12s]  such as the ones that you've seen in class. Can we give any guarantees for those? The answer is
[2889.92s -> 2894.16s]  unfortunately there is a huge gap in the sense that the type of algorithms that you've seen in
[2894.16s -> 2899.28s]  the class is very difficult to prove guarantees for them because they may not converge. There are
[2899.28s -> 2907.04s]  variants that are sort of in a sense that you can provide guarantees for it. The big problem
[2907.04s -> 2914.00s]  is that it's not clear how you would implement them and that's kind of an issue of all over L
[2914.00s -> 2921.36s]  with general function approximation. If you want guarantees, it's not clear
[2922.32s -> 2928.08s]  how you would come up with an algorithm that you can actually implement and so oftentimes what
[2928.08s -> 2936.56s]  is analyzed is sort of a conceptual version that is not the same as the actual algorithm
[2936.64s -> 2938.32s]  that is implemented in practice.
[2943.12s -> 2948.64s]  Now, before we head to the conclusion, any question on this sort of third part?
[2951.12s -> 2952.00s]  Yeah, of course.
[2952.32s -> 2957.36s]  So this movement?
[2964.40s -> 2967.84s]  Well, the covariance, if this is a finite horizon problem,
[2967.84s -> 2971.36s]  the covariance can really change through time steps.
[2975.44s -> 2980.56s]  It's the covariance of the features, so sum of phi, phi transpose.
[2982.96s -> 2984.08s]  Feature expect.
[2988.08s -> 2992.88s]  Yeah, same as linear regression. It's the same object appears.
[3003.44s -> 3005.36s]  What do you mean by epsilon-optimal problems?
[3012.00s -> 3021.28s]  This won't be optimal, right, because it's offline RL, so it really depends on your data set.
[3022.72s -> 3028.00s]  The policy that you find may be very crappy if your data set doesn't have good information.
[3028.96s -> 3033.68s]  Suppose your data set is just from a policy that is narrowly concentrated and
[3034.64s -> 3043.12s]  the behavioral policy is bad and this feature matrix is like rank one,
[3043.12s -> 3046.56s]  it's all concentrated in one direction, then that doesn't really tell you much
[3047.84s -> 3052.16s]  and so you won't be able to find a good policy.
[3052.16s -> 3057.84s]  Somehow this is sort of reflected in precisely that statement, right, because
[3058.64s -> 3063.84s]  policies that are very good, they would have a coverage coefficient that is very large
[3064.48s -> 3068.96s]  and so you will not know their value and no algorithm would return that.
[3069.12s -> 3080.56s]  Yeah.
[3085.04s -> 3089.12s]  No, I think if you want, this is the epsilon that you're talking about, right?
[3090.40s -> 3092.48s]  This is the policy that we will return.
[3093.92s -> 3097.20s]  You can always think about the optimal policy in the supremum.
[3097.20s -> 3100.80s]  I want to evaluate this expression at the optimal policy, I can do that,
[3101.76s -> 3108.64s]  but then this guy will become the value, the sort of coverage about the optimal policy
[3108.64s -> 3110.96s]  and so this is your epsilon, right?
[3110.96s -> 3116.24s]  This is epsilon suboptimal compared to the optimal policy, but epsilon can be huge.
[3116.24s -> 3121.12s]  Basically I'm telling you the value of epsilon given the data set that we have.
[3121.20s -> 3129.12s]  DC is really a way to measure how much information I've contained in the data set
[3130.48s -> 3137.04s]  and as a result, what's the performance that you can expect.
[3145.28s -> 3146.80s]  At the initial state, I would say.
[3147.44s -> 3149.52s]  The value of the policy at the initial state.
[3150.48s -> 3157.04s]  That's sort of the one that you care about and your estimates may be more off in states
[3157.04s -> 3161.92s]  that for example you don't visit or even in states that you visit, but they might compensate.
[3161.92s -> 3164.56s]  What you really care is performance at the starting point.
[3164.56s -> 3168.96s]  Okay, yeah.
[3176.00s -> 3176.56s]  Right, right, right.
[3176.56s -> 3180.96s]  So I think you have seen something similar with CQL, I think.
[3183.28s -> 3189.76s]  Basically, on next let's plot, let's put different policies.
[3189.76s -> 3191.28s]  To be clear, you're going to have
[3191.28s -> 3195.44s]  uncountably many, right, but let's put them on a graph
[3196.08s -> 3199.68s]  and on the y, we're going to plot the value of the policies.
[3201.68s -> 3202.72s]  Yeah, yeah, yeah, yeah.
[3202.72s -> 3204.64s]  Expect this can be some of the world.
[3204.64s -> 3208.96s]  Now what you wish you knew is the actual value of the policies, right?
[3208.96s -> 3211.36s]  The green line is the value of the policy.
[3211.36s -> 3215.92s]  So if you know the actual MDP, you do value iteration and you will find this guy,
[3215.92s -> 3216.96s]  the optimal policy.
[3217.52s -> 3220.08s]  Unfortunately, what you have is a data set.
[3220.08s -> 3222.16s]  Now, how to use the data set is up to you.
[3222.80s -> 3226.40s]  Intuitively, if you're doing model-based RL, you can try to fit some model
[3226.40s -> 3228.00s]  and try to use that to make predictions.
[3228.00s -> 3229.68s]  So your model may be good, it might be bad.
[3230.24s -> 3232.64s]  It might generally be good to make predictions about
[3233.36s -> 3236.00s]  value of policies that generated the data set.
[3236.00s -> 3239.52s]  It might be very bad to predict value of different policies.
[3239.52s -> 3244.24s]  You might not use a model-based version and you may do something different.
[3244.24s -> 3247.76s]  For example, you may adopt a model-free approach like CQL.
[3248.72s -> 3254.64s]  And intuitively, if you could, what you would do is the following.
[3254.64s -> 3259.36s]  Try to come up with an estimator for the value of different policies
[3260.16s -> 3262.48s]  and try to measure also the uncertainty.
[3262.48s -> 3265.84s]  The uncertainty is really sort of this band here, right?
[3266.80s -> 3271.20s]  And this curve may move up and down depending on the data set.
[3272.48s -> 3276.16s]  But you would like to try to estimate the uncertainty about your predictions.
[3276.24s -> 3280.80s]  Intuitively, the uncertainty will be smaller for policy evaluation,
[3280.80s -> 3283.76s]  for the very policy that's generated the data set.
[3283.76s -> 3285.60s]  You have a bunch of data, it just takes leverage.
[3286.32s -> 3290.56s]  But it will be very bad for a policy that is very, very different,
[3290.56s -> 3295.28s]  that visits completely different areas of the state and national space, right?
[3295.28s -> 3297.52s]  Your data is narrowly concentrated.
[3297.52s -> 3300.16s]  You have no idea about the policy that does something
[3300.16s -> 3303.36s]  in a completely unknown area of the state and national space.
[3303.36s -> 3308.72s]  So even if that policy, by doing, you know, feed and kill, it looks good,
[3308.72s -> 3312.72s]  you have to take into account that you're extremely uncertain about this value.
[3312.72s -> 3314.40s]  And so somehow you would like to penalize it.
[3315.04s -> 3318.72s]  And so you would like to say, oh, this policy, like this, I'm too uncertain.
[3319.28s -> 3321.12s]  I'm going to assign it a very low value.
[3321.76s -> 3328.96s]  And if I do this procedure, the optimal trade-off is to maximize this lower bound,
[3328.96s -> 3332.32s]  a lower bound on the performance of the policies.
[3332.32s -> 3336.96s]  And this gives you sort of abstractly this expression here,
[3336.96s -> 3340.80s]  which will become concrete as soon as you consider a specific algorithm
[3340.80s -> 3344.80s]  and a specific type of, you know, function class and MDT.
[3344.80s -> 3347.04s]  And that will determine the C pi.
[3362.32s -> 3371.36s]  Yeah, yeah, yeah, yeah, yeah, of course, thank you.
[3374.96s -> 3378.08s]  I think what's important here, like if you want one takeaway,
[3378.08s -> 3383.92s]  is really how would a guarantee of an offline algorithm look like?
[3383.92s -> 3388.24s]  It would look something like this, some trade-off between values.
[3388.24s -> 3391.84s]  Of course, you want the highest value, but policies that have high value,
[3391.92s -> 3393.68s]  you might be very uncertain about that.
[3393.68s -> 3396.32s]  And so there's going to be some trade-off.
[3399.76s -> 3401.68s]  And just to make it, if you want more clear,
[3403.84s -> 3409.60s]  for the policy that having the data set, generally you have a lot of data, right?
[3410.56s -> 3415.04s]  And so this C pi is going to be something like one.
[3415.92s -> 3419.92s]  You have a lot of data, so n is big, this quantity is small.
[3420.88s -> 3424.40s]  And so what this expression tells you is that you should do better
[3425.28s -> 3427.68s]  than the policies that generated the data set.
[3428.48s -> 3431.92s]  If you design the algorithm correctly, this is sort of the minimum
[3431.92s -> 3432.80s]  that you would expect.
[3432.80s -> 3435.84s]  You want to do better than behavioral clone, right?
[3435.84s -> 3438.24s]  And this expression tells you exactly that.
[3438.24s -> 3443.12s]  If I put pi as a behavioral policy that generated the data set,
[3443.12s -> 3445.28s]  C pi will be small, it's going to be one.
[3445.28s -> 3447.04s]  This expression will be smaller.
[3447.12s -> 3450.72s]  And this tells me I do better than behavioral cloning,
[3450.72s -> 3452.00s]  which is what we would expect.
[3454.16s -> 3457.68s]  Okay, time to sum up.
[3460.32s -> 3461.68s]  We have seen sort of three things.
[3462.72s -> 3467.28s]  One is most problems in RL, they're not worst case,
[3467.28s -> 3469.92s]  they belong to a much easier class of problems.
[3471.44s -> 3475.28s]  We have seen that as soon as we move to function approximation,
[3475.28s -> 3478.96s]  RL is much more difficult than standard supervised learning.
[3480.08s -> 3484.56s]  And then we have seen what type of guarantees we can obtain
[3484.56s -> 3488.40s]  for offline reinforcement learning algorithms.
[3489.84s -> 3493.60s]  And to conclude, I think after the presentation,
[3493.60s -> 3494.56s]  this is much more clear.
[3494.56s -> 3498.08s]  There is a huge gap between sort of theory and practice.
[3499.36s -> 3504.08s]  I think working at the intersection is of course difficult, right?
[3504.08s -> 3507.04s]  Because you have to sort of please both communities.
[3507.60s -> 3512.96s]  But it might help make reinforcement learning more applicable
[3512.96s -> 3516.16s]  in the sense that there are going to be compromises to be made.
[3516.16s -> 3522.56s]  You won't sort of beat any benchmark or top any benchmark.
[3522.56s -> 3524.80s]  But you might be able to come up with some algorithm
[3524.80s -> 3530.40s]  that has some sort of analysis.
[3530.40s -> 3535.04s]  And at least in simple cases, some ability guarantees.
[3536.24s -> 3538.96s]  And this will be kind of critical
[3538.96s -> 3541.36s]  in order to apply reinforcement learning
[3541.36s -> 3543.20s]  to very different problems.
[3543.20s -> 3546.40s]  You would feel much more confident to apply an algorithm
[3546.40s -> 3549.92s]  if it is backed by some form of guarantees
[3549.92s -> 3551.76s]  that apply even in a restricted set.
[3553.52s -> 3557.04s]  And generally theory, you will not tell you
[3557.04s -> 3560.24s]  how to sort of tune hyperparameters and all that.
[3560.24s -> 3563.20s]  So it will not necessarily inform you
[3563.20s -> 3565.84s]  on the specific of any given application.
[3565.84s -> 3571.12s]  But it can give you sort of more broad insights
[3571.12s -> 3574.64s]  and foundation that apply more broadly to the field.
[3574.64s -> 3577.76s]  We have seen some fundamental lower bounds before.
[3578.96s -> 3582.56s]  And yeah, I think with this I conclude
[3582.56s -> 3584.80s]  and this is everything I had for today.
[3584.80s -> 3586.48s]  So thank you for your attention.
[3587.04s -> 3602.24s]  I'm going to ask you if there is any final question.
[3605.52s -> 3613.76s]  Thank you for coming here.
