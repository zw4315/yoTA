# Detected language: en (p=1.00)

[0.00s -> 8.00s]  In today's lecture, we're going to cover actor-critic algorithms. Actor-critic algorithms build
[8.00s -> 12.08s]  on the policy gradient framework that we discussed in the previous lecture, but they're
[12.08s -> 19.40s]  also augmented with learned value functions and Q functions. So to begin, let's recap
[19.40s -> 24.68s]  the policy gradients material from last time. Last time we learned about the reinforce
[24.68s -> 31.12s]  algorithm, which alternates between three steps. It samples a batch of trajectories by running the
[31.12s -> 37.04s]  current policy in the environment, so you sample n trajectories in this way, and then we use these
[37.04s -> 43.20s]  trajectories to compute an estimate of the policy gradient, which is calculated by averaging over
[43.20s -> 50.12s]  all of our samples, a sum over all time steps of grad log pi at that time step times the sum
[50.16s -> 55.76s]  of rewards from that step until the end, or the reward to go. And then we take this approximate
[55.76s -> 61.76s]  policy gradient, and we add it multiplied by some learning rate to our current parameter vector,
[61.76s -> 69.36s]  which corresponds to a gradient ascent optimization process. This algorithm follows
[69.36s -> 73.92s]  the basic anatomy of the reinforcement learning algorithm that we discussed before, where the
[73.92s -> 80.04s]  orange box corresponds to generating the samples, the green box corresponds to calculating the reward
[80.04s -> 86.20s]  to go at every time step for every sample, and the blue box corresponds to applying the gradient
[86.20s -> 94.28s]  ascent rule. Now in the lecture last time, I somewhat suggestively used the symbol Q hat
[94.28s -> 103.08s]  to denote the reward to go, and this choice was deliberate, because when you exploit this
[103.08s -> 107.64s]  causality property that I described, it turns out that the way that you should calculate your
[107.64s -> 114.20s]  policy gradient is by multiplying each grad log pi by the total reward that you expect to get
[114.20s -> 120.60s]  if you start in state SIT, then take action AIT, and then follow your policy. That's a very
[120.60s -> 125.08s]  reasonable interpretation of the policy gradient. You're essentially saying that you will increase
[125.08s -> 129.64s]  the probability of those actions that an in-expectation lead to high rewards and
[129.64s -> 134.52s]  decrease the probabilities of those actions that an in-expectation lead to low rewards.
[135.40s -> 142.52s]  But let's examine this Q hat term a little bit more closely. Q hat represents an estimate
[142.52s -> 149.40s]  of the expected reward if you take action AIT, in state SIT, and then follow your policy
[149.40s -> 154.36s]  until the end of the trajectory. But can we get a better estimate of this quantity?
[156.12s -> 160.76s]  Let's imagine that this curvy line represents one of the trajectories that you sampled,
[160.76s -> 168.76s]  and your Q hat is calculated at a particular time step. So this green circle represents
[168.76s -> 175.00s]  the state SIT, so the ith sample time step t, and at that point in time we're going to
[175.00s -> 180.76s]  calculate an estimate of our reward to go Q hat. And then we're going to multiply our grad log
[180.76s -> 186.04s]  pi by that reward to go. So the way that we calculate this estimate is by summing up
[186.04s -> 191.24s]  the rewards that we actually got along that trajectory. But that trajectory represents
[191.24s -> 195.96s]  just one of the many possibilities. So if we were to somehow accidentally land in the
[195.96s -> 200.36s]  same exact state again and then run our policy just like we did on this rollout,
[200.36s -> 205.16s]  we might get a different outcome simply because the policy and the MDP have some randomness
[205.16s -> 211.48s]  in them. So right now we're using a single-step estimate for the reward to go, but in reality
[212.36s -> 217.88s]  there are many possibilities for what might happen next. So we would have a better estimate of the
[217.88s -> 222.04s]  reward to go if we could actually compute a full expectation over all these different
[222.04s -> 227.56s]  possibilities. The reason that there are many possibilities is simply because there's randomness
[227.56s -> 232.84s]  in the system. Our policy has randomness and our MDP has randomness. But this randomness
[232.84s -> 237.08s]  can be quite significant, which means that our single sample estimate that we got by
[237.08s -> 241.72s]  summing up the rewards that we actually obtained in that trajectory might be quite far off from
[241.72s -> 250.20s]  the actual expected value. Now this problem I'm going to claim directly relates to the high
[250.20s -> 255.00s]  variance of a policy gradient, and I'd like all of you to take a moment to think about
[255.00s -> 262.20s]  what this has to do with variance. So the connection to variance is that the policy
[262.20s -> 268.60s]  gradient way of calculating the reward to go is a single sample estimate of a very complex
[268.60s -> 274.36s]  expectation. The fewer samples you use to estimate an expectation, the higher the variance
[274.36s -> 280.28s]  of your estimator will be. So a single sample estimator has very high variance. If we could
[280.28s -> 285.08s]  somehow generate a million samples starting from the same state action tuple, then we would have
[285.08s -> 290.76s]  much lower variance. If we could somehow calculate this expectation exactly, we would have much much
[290.76s -> 298.36s]  lower variance. So if we had access to the true expected reward to go, defined as the true expected
[298.36s -> 303.72s]  value of the sum of rewards that we get starting from state SIT and action AIT,
[304.28s -> 311.00s]  then the variance of our policy gradient would be much lower. And then if we had this Q function,
[311.00s -> 315.64s]  we could simply plug it in in place of Q hat and get a lower variance policy gradient.
[315.64s -> 323.00s]  Now in the previous lecture, we also learned about this thing called baselines, which could
[323.00s -> 328.76s]  lower the variance of a policy gradient even further. Can we apply a baseline even when we have
[328.76s -> 335.16s]  the true Q function? And the answer is that of course we can. So we can subtract some quantity
[335.16s -> 340.44s]  B. We learned last time that the average reward was a good choice for B, although not the
[340.44s -> 345.96s]  optimal choice. So what do we average? Well, we could average Q values. So we could say, well,
[345.96s -> 350.52s]  let's make B just be the average Q value at that time step over all the states and actions
[350.52s -> 355.00s]  that we saw, and then we will have this appealing property that the policy gradient will
[355.00s -> 359.72s]  increase the probability of actions that are better than average in terms of the reward to
[359.72s -> 364.04s]  go expectation and decrease the probability of actions that are worse than average.
[365.00s -> 371.24s]  But it turns out that we can lower the variance even further because the baseline can actually
[371.24s -> 376.12s]  depend on the state. It can't depend on the action that leads to bias, but you can make
[376.12s -> 382.84s]  it depend on the state. So if you make the baseline depend on the state, then the best
[382.84s -> 388.44s]  thing to do, or not the optimal thing to do but a better thing to do, would be to
[388.76s -> 394.84s]  compute the average reward over all the possibilities that start in that state.
[394.84s -> 398.76s]  So not just the average reward over all possibilities at that time step, but specifically
[398.76s -> 404.44s]  in that specific state. And if you average your Q values over all the actions in a particular
[404.44s -> 410.68s]  state, that's simply the definition of the value function. So a very good choice for the baseline
[410.68s -> 418.12s]  is the value function. So you can calculate your policy gradient as grad log pi multiplied by Q
[418.68s -> 425.64s]  SIT AIT minus V SIT. This is in fact a very intuitive quantity because the difference
[425.64s -> 432.04s]  between the Q value and the value function represents your estimate of how much better
[432.04s -> 438.44s]  the action AIT is on average than the average action you would take in the state SIT. So it
[438.44s -> 443.96s]  makes a lot of sense to multiply your grad log pi terms by this because it's directly saying
[443.96s -> 448.60s]  take the actions that are better than average in that state and increase their probability
[448.60s -> 452.52s]  and take the actions that are worse than average in that state and decrease their probability.
[455.00s -> 460.44s]  In fact, this Q minus V term is so important that we have a special name for it. We call it
[460.44s -> 464.84s]  the advantage function. The reason we call it the advantage function is that it represents
[464.84s -> 470.68s]  how advantageous the action AIT is as compared to the average performance that you would expect
[470.68s -> 479.16s]  the policy pi theta to get in the state SIT. Okay, so let's talk about state and state action
[479.16s -> 483.40s]  value functions. By the way, when I say state action value function or Q function, those mean
[483.40s -> 488.44s]  exactly the same thing, but sometimes saying state action value function can be a little clearer.
[490.52s -> 497.32s]  So our Q function or state action value function represents the total expected reward that you
[497.32s -> 502.52s]  expect to get if you start in state SIT, take action AIT, and then follow your policy.
[504.04s -> 509.80s]  We will often write the Q function with the superscript pi to emphasize that the Q function
[509.80s -> 515.96s]  depends on pi, so every policy will have a different Q function. The value function is
[515.96s -> 522.68s]  the expected value over all the actions in state SIT under your current policy of the Q value.
[522.68s -> 526.60s]  Another way of saying it is it's the total reward that you expect to get if you start in
[526.60s -> 533.00s]  state SIT and then follow your policy. The advantage function is the difference between these
[533.00s -> 538.28s]  two quantities, and the advantage function represents how much better the action AIT is
[538.28s -> 543.16s]  as compared to the average performance of your policy pi in state SIT.
[545.64s -> 549.16s]  So we can get a very good estimate of the policy gradient if we simply
[549.56s -> 556.76s]  multiply the grad log pi terms by the advantage value at SIT AIT.
[560.12s -> 565.08s]  Now of course in reality we won't have the correct value of the advantage, we'll have to
[565.08s -> 569.88s]  estimate it, for example using some function approximator, so the better our estimate of the
[569.88s -> 575.96s]  advantage, the lower our variance will be. Now it's also worth mentioning that the kind of
[576.68s -> 581.00s]  actor-critic methods that we'll discuss in today's lecture don't necessarily produce
[581.00s -> 587.00s]  unbiased estimates of the advantage function. So while the policy gradient we've discussed so far
[587.00s -> 592.20s]  has been unbiased, if your advantage function is incorrect, then your entire policy gradient
[592.20s -> 598.12s]  can also be biased. Usually we're okay with that because the enormous reduction in variance
[598.12s -> 603.24s]  is often worth the slight increase in bias that we incur from using approximate Q values and
[603.24s -> 614.04s]  value functions. So to summarize, the conventional policy gradient uses a kind of a Monte Carlo
[614.04s -> 618.68s]  estimate of the advantage calculated by using the one sample that you have in the remainder of
[618.68s -> 623.16s]  the current trajectory by summing up the rewards in your trajectory and subtracting a baseline.
[624.44s -> 629.48s]  This is an unbiased but high variance single sample estimate and we can replace it with
[629.48s -> 634.92s]  an approximate advantage function, which itself is usually calculated from an approximate Q function
[634.92s -> 639.96s]  or an approximate value function, and get a much lower variance estimate because now we're
[639.96s -> 645.24s]  potentially getting a better estimate for this expectation that does not rely on a single sample,
[645.24s -> 651.56s]  but often the resulting approximate value functions will have some bias, so we'll trade off lots of
[651.56s -> 658.76s]  variance for some small increase in bias. So the structure of the resulting algorithms will now
[658.76s -> 664.28s]  have a much more elaborate green box, so the orange box will be the same as before, we'll generate
[664.28s -> 669.16s]  samples by running our policy, the blue box will still be the same, we'll still use the policy
[669.16s -> 675.16s]  gradient to do gradient descent, but the green box will now involve fitting some kind of estimator,
[675.16s -> 682.44s]  either an estimator to Q pi or an estimator to V pi or A pi. So let's talk about that
[682.44s -> 688.36s]  next. Let's talk about fitting value functions. So we have three possible quantities, Q, V,
[688.36s -> 693.88s]  or A. Ultimately we want A, but the question we might ask is, well, what should we fit? Which of
[693.88s -> 700.44s]  these three should we fit, and what should we fit at two? What should our targets be? So should we fit
[700.44s -> 706.68s]  Q, V, or A? Take a moment to think about this choice and consider some of the pros and cons
[706.68s -> 716.68s]  of one choice or another. So the Q function is the expected value of the reward that we'll
[716.68s -> 721.56s]  get when we start from state St, take action At, and then follow our policy.
[723.80s -> 728.92s]  Now one very convenient property of this is that because St and At are not actually random
[728.92s -> 735.88s]  variables, we can rewrite the Q function as simply the current reward plus the expected value
[735.88s -> 740.12s]  of the reward in the future, because the current reward depends on St and At and they're not
[740.12s -> 750.28s]  random. So this equality is exact. And this quantity that we're adding is simply the expected
[750.28s -> 756.60s]  value of the value function at the state St plus one that we will get when we take action At
[756.60s -> 763.72s]  in state St. So we can similarly write the Q function in terms of the value function
[763.72s -> 768.76s]  as the current reward plus the expected value of the reward of the value function at the next
[768.76s -> 773.24s]  time step. And the expectation here is of course taken with respect to the transition dynamics.
[775.40s -> 780.84s]  Now we can make a small approximation where we could say that the actual state St plus one
[780.84s -> 785.80s]  that we saw in the current trajectory is kind of representative of the average St plus one that
[785.80s -> 790.04s]  we'll get. Now at this point we've made an approximation. This is not an exact equality.
[790.04s -> 794.76s]  We're essentially approximating the distribution over states at the next time step with, again,
[794.76s -> 800.28s]  a single sample estimator. But now it's a single sample estimator for just that one time step.
[800.28s -> 805.40s]  Everything after that is still integrated out as represented by the value function v pi.
[807.40s -> 814.92s]  So we've made this approximation and now we might wonder, well, okay, so we lost a little
[814.92s -> 819.24s]  bit. We still have lower variance but not quite as low as we had before. Why would we want to
[819.24s -> 824.36s]  do that? Well the reason that we would want to do that is because if we then substitute this
[824.36s -> 829.08s]  approximate equation for the Q value into the equation for the advantage, we get this very
[829.08s -> 834.44s]  appealing expression where the advantage is now approximately equal to the current reward plus
[834.44s -> 842.12s]  the next value minus the current value. This is still an approximation because, to be exact, this
[842.12s -> 847.64s]  v pi St plus one needs to be in expectation over all possible values of St plus one, whereas
[847.64s -> 852.52s]  we've just substituted the actual St plus one that we saw. But what's very appealing about this
[852.52s -> 859.64s]  equation is that now it depends entirely on v, and v is more convenient to learn than Q or A
[859.64s -> 864.20s]  because Q and A both depend on the state and the action, whereas v depends only on the state.
[865.16s -> 870.04s]  When your function approximator depends on fewer things, it's easier to learn because you won't
[870.04s -> 877.88s]  need as many samples. So maybe what we should do is just fit v pi of s. This is not the only
[877.88s -> 883.16s]  choice for actor-critic algorithms, and we will learn about actor-critic methods that use Q
[883.16s -> 888.12s]  functions as well later on in the course, but for now we'll talk about actor-critic algorithms
[888.12s -> 893.88s]  that just fit v pi of s and then use this equation to derive the advantage function
[893.88s -> 901.56s]  approximately. So when we fit v pi of s, we would have some kind of model, such as a neural
[901.56s -> 906.92s]  network, that maps states s to approximate values v hat pi of s.
[910.28s -> 913.32s]  And this network will have some parameters, which I'm going to call phi.
[915.56s -> 921.48s]  So let's talk about the process of fitting v pi of s. This process is sometimes referred
[921.48s -> 928.44s]  to as policy evaluation because v pi represents the value of the policy at every state, so
[928.44s -> 936.36s]  calculating the value is evaluation. In fact, if you think back to the lecture last week
[936.36s -> 941.32s]  on the definitions of reinforcement learning problems, you will remember that the reinforcement
[941.32s -> 946.68s]  learning objective itself can be expressed as the expected value of the value function
[946.68s -> 951.00s]  over the initial state distribution. So if you compute the value function, you can literally
[951.00s -> 955.80s]  evaluate how good your policy is just by averaging together the values at the initial states.
[959.32s -> 963.80s]  So that's the expression here. Our objective j theta can be expressed as just the expected
[963.80s -> 970.04s]  value of v pi at the initial states. So how can we perform policy evaluation?
[971.24s -> 976.84s]  Well, one thing that we could do is we could use Monte Carlo policy evaluation. In a sense,
[976.84s -> 981.64s]  this is what policy gradients do. In Monte Carlo policy evaluation, we run our policy many,
[981.64s -> 986.84s]  many times and then sum together the rewards obtained along the trajectories generated by
[986.84s -> 992.44s]  the policy and use that as an unbiased but high variance estimate of the policy's total reward.
[994.04s -> 1001.08s]  So we could say that the value at state s, t is approximately the sum over all the rewards
[1001.08s -> 1006.04s]  that we saw after visiting state t along the trajectory that visited state s, t.
[1007.48s -> 1014.60s]  So here is our rollout, here is the state s, t, and we're just going to sum all of the things
[1014.60s -> 1021.48s]  that we saw after that along that one trajectory. Now, ideally, what we would like to be able to do
[1021.48s -> 1026.52s]  is sum over all possible trajectories that could occur when you start from that state,
[1026.52s -> 1030.84s]  because there's more than one possibility. So we would like to sum over all of these things.
[1030.84s -> 1035.40s]  Unfortunately, in the model-free setting, this is generally impossible, because this requires us
[1035.40s -> 1040.52s]  to be able to reset back to the state s, t and run multiple trials starting from that state.
[1040.52s -> 1043.80s]  Generally, we don't assume that we're able to do this. We only assume that we're able
[1043.80s -> 1048.92s]  to run multiple trials from the initial state. So typically we can't do this, but if you have
[1048.92s -> 1054.84s]  access to a simulator that you can reset, you can technically calculate your Monte Carlo values
[1054.84s -> 1065.00s]  in this way. Okay, so what happens if we use a neural network function approximator for the
[1065.00s -> 1070.76s]  value function with this kind of Monte Carlo evaluation scheme? Well, we have our neural network
[1070.76s -> 1076.68s]  v-hat pi with parameters phi. We're going to, at every state that we visit, sum together
[1076.68s -> 1082.84s]  the remaining rewards, and that will produce our target values. But then, instead of plugging
[1082.84s -> 1087.56s]  those reward to-goes directly into our policy gradient, we'll actually fit a neural network
[1087.56s -> 1093.56s]  to those values, and that will actually reduce our variance, because even though we can't visit
[1093.56s -> 1099.88s]  the same state twice, our function approximator, our neural network, will actually realize
[1099.88s -> 1104.68s]  that different states that we visited in different trajectories are similar to one another. So,
[1104.68s -> 1109.56s]  even though this green state along the first trajectory will never be visited more than once
[1109.56s -> 1115.00s]  in continuous state spaces, if we have another trajectory rollout that is kind of nearby,
[1115.00s -> 1118.68s]  but then where something else happened later down the line in that trajectory,
[1118.68s -> 1124.52s]  the function approximator will realize that these two states are similar, and when it tries to
[1124.52s -> 1129.40s]  estimate the value at both of these states, the value of one will sort of leak into the value of
[1129.40s -> 1133.80s]  the other. That's essentially generalization. Generalization means that your function
[1133.80s -> 1139.08s]  approximator understands that nearby states should take on similar values. So if you
[1139.08s -> 1142.36s]  accidentally had a very different outcome in one of those states than you did in the other,
[1142.36s -> 1147.48s]  the function approximator will to some degree average those out and produce lower variance
[1147.48s -> 1151.72s]  estimates than you would have gotten if you just directly used that single sample value
[1151.72s -> 1158.52s]  in your policy gradient. So it's not as good as making multiple rollouts from the same state,
[1158.52s -> 1163.72s]  but it's still pretty good. So the way that we would do this is we would generate training data
[1163.72s -> 1169.56s]  by taking all of our rollouts, and for each state along every rollout we create a tuple
[1169.56s -> 1175.88s]  consisting of the state S-I-T and a label corresponding to the sum of rewards that we saw
[1175.88s -> 1183.56s]  starting from S-I-T for the rest of that rollout. And we're going to call these labels
[1183.56s -> 1190.92s]  y-i-t, and when I say target value, I mean y-i-t. So we'll get these tuples S-I-T, y-i-t, and then
[1190.92s -> 1196.28s]  we'll solve a supervised regression problem. We'll train our neural network value function
[1196.84s -> 1204.76s]  so that its parameters phi minimize the sum over all of our samples of the squared error
[1204.76s -> 1210.44s]  between the value function's prediction and the single sample Monte Carlo estimate of the value
[1210.44s -> 1218.68s]  at that state. Of course, if our function approximator massively over fits and produces
[1218.68s -> 1223.48s]  exactly the training label at every single state, then we wouldn't have gained much as
[1223.48s -> 1229.16s]  compared to just directly using the y-i-t values in our policy gradient. But if we get
[1229.16s -> 1233.40s]  generalization, meaning that our function approximator understands that two nearby states
[1233.40s -> 1237.72s]  should have similar values even if their labels are different, then we'll actually get lower
[1237.72s -> 1243.00s]  variance, because this function approximator will now average out the dissimilar labels
[1243.00s -> 1251.24s]  at similar states. But can we do it even better? So the ideal target that we would like to have
[1251.24s -> 1256.60s]  when training our value function is the true expected value of rewards starting from the state
[1256.60s -> 1263.16s]  S-I-T. Of course, we don't know this quantity. The Monte Carlo target that we used before uses
[1263.16s -> 1270.92s]  a single sample estimate of this quantity. But we can also use the relationship that we wrote out before
[1272.04s -> 1276.84s]  where we saw that a Q-function is simply equal to the reward of the current time step
[1276.84s -> 1283.88s]  plus the expected reward starting from the next time step. And if we write out this quantity,
[1283.88s -> 1288.76s]  then we can perform the same substitution as before and actually replace the second term
[1288.84s -> 1294.68s]  in the summation with our estimate of the value function. And this is a better lower
[1294.68s -> 1300.52s]  variance estimate of the reward to go than our single sample estimator. So this says,
[1300.52s -> 1306.44s]  let's use the actual reward that we saw the current time step plus the value at the actual
[1306.44s -> 1311.88s]  state that we saw the next time step. Now, of course, we don't know the value v pi,
[1312.44s -> 1317.96s]  so we're going to approximate that simply by using our previous function approximator.
[1317.96s -> 1324.28s]  So we'll assume that our previous v-hat pi phi was kind of okay, like maybe it wasn't great,
[1324.28s -> 1328.92s]  but it's probably better than nothing, so we can plug it in in place of v pi and get what is
[1328.92s -> 1334.52s]  called a bootstrap estimator. So here we're directly going to use the previous fitted
[1334.52s -> 1341.08s]  value function to estimate this quantity. So now our training data will consist of two poles
[1341.08s -> 1346.12s]  of the states that we saw S-I-T and labels that correspond to the reward that we actually
[1346.12s -> 1354.44s]  got at that time step, R-S-I-T-A-I-T plus the estimate of the value function
[1355.00s -> 1361.80s]  at the actual next state S-I-T plus one that we saw. Now this estimate v-hat might be incorrect,
[1361.80s -> 1366.52s]  but as we repeat this process, hopefully these values will get closer and closer to the correct
[1366.52s -> 1373.48s]  values, and because the v-hats are averaging together all possible future returns, we expect
[1373.48s -> 1380.44s]  their variance to be lower. So now our target value y-i-t is given by the sum, and our training
[1380.44s -> 1387.80s]  process, just like before, is going to be supervised regression onto these y-i-ts. This is sometimes
[1387.80s -> 1393.80s]  referred to as a bootstrap estimate, and the bootstrap estimate has lower variance because it's
[1393.80s -> 1399.72s]  using v-hat instead of a single sample estimator, but it also has higher bias because our v-hat
[1399.72s -> 1403.32s]  pi-phi might be incorrect. So that's the trade-off.
[1405.88s -> 1410.92s]  All right, so to conclude this portion of the lecture, what I want to do is give you a few
[1410.92s -> 1415.64s]  examples of policy evaluation just so that you get a better intuitive understanding of what the
[1415.64s -> 1421.24s]  heck policy evaluation actually means, because in many cases policy evaluation actually is a very
[1421.24s -> 1426.92s]  intuitive concept. For example, if we're training a reinforcement learning system with actor-critic
[1426.92s -> 1433.64s]  to play backgammon, this is from the TD Gammon paper in 1992, maybe our reward corresponds to
[1433.64s -> 1440.36s]  the outcome of the game. It's 1 if you win the game and 0 if you don't. Then our value function
[1440.36s -> 1446.04s]  is simply the expected outcome given the board state. If you get a 1 if you win the game and
[1446.04s -> 1449.88s]  0 if you lose, then the value function just directly predicts the probability that you'll win
[1449.88s -> 1456.20s]  the game given the state of the board right now. Very intuitive. Similarly, if you're training
[1456.20s -> 1461.80s]  a system to play Go and your reward is the game outcome, exactly the same thing. Your value
[1461.80s -> 1465.56s]  function is actually trying to predict how likely are you to win the game given the state
[1465.56s -> 1470.20s]  of the board right now. Now this is very convenient for board games because we know the
[1470.20s -> 1475.08s]  rules that govern these board games, so we can simulate what would happen if we make a move.
[1475.88s -> 1481.24s]  In fact, we can simulate every possible move, check its value, and then take the move that
[1481.24s -> 1486.36s]  leads to the highest value state. This is much cheaper than doing a full game tree because all you
[1486.36s -> 1491.16s]  have to do is predict one step into the future and then your value function tells you, given what
[1491.16s -> 1496.76s]  will happen after that one step, how likely are you to win the game, and then you take the
[1496.76s -> 1500.68s]  move that most increases your probability to win the game. So policy evaluation can have very
[1500.68s -> 1504.92s]  natural interpretations. In the next portion of the lecture we'll talk about how we can use
[1504.92s -> 1511.00s]  policy evaluation in a complete actor-critic algorithm to derive a new reinforcement learning
[1511.00s -> 1513.00s]  method.
