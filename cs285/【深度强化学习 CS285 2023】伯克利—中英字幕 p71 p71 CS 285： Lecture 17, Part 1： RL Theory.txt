# Detected language: en (p=1.00)

[0.00s -> 4.44s]  Welcome to lecture 17 of CS285.
[4.44s -> 8.94s]  Today, we're going to have a brief discussion of reinforcement learning theory.
[8.94s -> 12.24s]  And this is the only theory lecture in this course,
[12.24s -> 14.56s]  so I won't go into a great deal of depth.
[14.56s -> 17.20s]  My goal is to mainly just give you a sense for
[17.20s -> 19.16s]  the kinds of theoretical analysis that we can do in
[19.16s -> 21.36s]  reinforcement learning algorithms and the kinds of
[21.36s -> 24.68s]  conclusions that we might draw from this sort of analysis.
[24.68s -> 29.84s]  So what questions do we usually ask when we're doing reinforcement learning theory?
[30.20s -> 31.84s]  Well, there are a lot of different questions,
[31.84s -> 33.76s]  but here are a few common ones.
[33.76s -> 35.52s]  If you have an algorithm,
[35.52s -> 37.44s]  some kind of reinforcement learning algorithm,
[37.44s -> 39.92s]  and it's provided with n samples,
[39.92s -> 44.52s]  and it uses those n samples every one of k iterations,
[44.52s -> 46.84s]  how good is the result from this algorithm?
[46.84s -> 48.40s]  What does how good mean?
[48.40s -> 50.16s]  Well, let's say we're doing Q-learning.
[50.16s -> 53.68s]  We might ask, how much does the learned Q function,
[53.68s -> 55.96s]  Q hat k after k iterations,
[55.96s -> 58.24s]  differ from the true optimal Q function?
[58.40s -> 62.24s]  Could we, for example, show that Q hat k differs from
[62.24s -> 67.12s]  the optimal Q function Q star in some norm by at most epsilon?
[67.12s -> 71.24s]  Now, typically these algorithms have some kind of randomness in them,
[71.24s -> 73.64s]  for example, randomness in how we generate the samples.
[73.64s -> 75.56s]  So we can't really guarantee that we'll
[75.56s -> 78.28s]  always have a difference less than or equal to epsilon.
[78.28s -> 80.96s]  So typically we would have a guarantee that it's less than or
[80.96s -> 84.56s]  equal to epsilon with at least some probability one minus delta,
[84.56s -> 86.48s]  where delta is a small number.
[87.00s -> 89.72s]  So typically we would want to show that this is true if we
[89.72s -> 92.36s]  have at least some number of samples where the number of
[92.36s -> 96.16s]  samples depends on some function of epsilon and delta.
[96.16s -> 98.48s]  And typically we would want some well-behaved function.
[98.48s -> 103.00s]  For example, we might want n to increase logarithmically with delta.
[105.56s -> 108.76s]  Another question we could ask, which is a slightly different question,
[108.76s -> 114.60s]  is how does the policy induced by the Q function at the kth iteration
[114.60s -> 118.52s]  differ from the optimal policy in terms of its Q value?
[118.52s -> 120.84s]  So let me unpack the notation here.
[120.84s -> 126.64s]  So Q pi k is the true Q function, meaning the true expected
[126.64s -> 130.36s]  total reward induced by the policy pi k,
[130.36s -> 135.00s]  where pi k is the policy corresponding to Q hat k.
[137.52s -> 141.52s]  So asking what is the difference between Q pi k and Q stars
[141.56s -> 146.80s]  really amounts to asking how different is the expected reward
[146.80s -> 151.24s]  of the policy iteration k from the best expected reward you could get.
[151.24s -> 152.72s]  So this is really a measure of regret.
[154.72s -> 157.60s]  All right, so Q pi k is not the same thing as Q hat k,
[157.60s -> 160.80s]  because you could, for example, have Q hat k erroneously
[160.80s -> 162.60s]  overestimate the Q values.
[162.60s -> 168.64s]  Q pi k is the true Q value of the policy corresponding to Q hat k,
[168.64s -> 170.44s]  which is typically the argmax policy.
[171.52s -> 175.64s]  And there are other questions we could ask, too.
[175.64s -> 179.56s]  Another kind of question we could ask is if I use a particular
[179.56s -> 182.40s]  exploration algorithm, how high is my regret going to be?
[182.40s -> 187.56s]  This is a little bit more elaborate, but typically, you know,
[187.56s -> 190.56s]  we might want to show that, for example, some kind of exploration
[190.56s -> 194.48s]  procedure gives you regret that is logarithmic in T.
[194.48s -> 198.60s]  So what I have here is actually the full version of the bound
[198.60s -> 200.80s]  for an upper confidence bound exploration method.
[201.48s -> 205.04s]  And you can see that it has a linear term multiplied by delta,
[205.04s -> 206.72s]  but delta is a very small number.
[206.72s -> 211.80s]  And then most of the bad stuff comes from this first term,
[211.80s -> 216.40s]  which goes as the log of T.
[216.40s -> 218.28s]  But there are, of course, many other questions we could ask.
[218.28s -> 219.48s]  So these are just a few examples.
[221.16s -> 225.48s]  We'll mostly focus on sample complexity type questions today,
[225.48s -> 227.20s]  so these first few.
[227.20s -> 229.04s]  But keep in mind that there are other questions.
[231.20s -> 233.64s]  Now, when we're doing reinforcement learning analysis,
[233.64s -> 236.16s]  we do typically need to make fairly strong assumptions.
[236.16s -> 240.08s]  So analyzing full deep RL methods in the most general setting
[240.08s -> 241.16s]  is generally not possible.
[242.76s -> 247.16s]  So effective R analysis is very hard without strong assumptions.
[247.16s -> 250.36s]  And the trick is to make assumptions that admit interesting conclusions
[250.36s -> 253.64s]  without divorcing us too much from reality.
[255.16s -> 256.24s]  So some examples.
[256.84s -> 262.84s]  In exploration, performance of RL methods
[262.84s -> 266.72s]  is greatly complicated by exploration because how well you learn,
[266.72s -> 269.44s]  how many samples you need, very strongly depends on how likely
[269.44s -> 271.44s]  you are to find potentially sparse rewards.
[273.24s -> 277.16s]  Since theoretical guarantees typically address worst case performance
[277.16s -> 281.12s]  and worst case exploration is extremely hard,
[281.12s -> 285.48s]  we typically don't want to couple analysis of exploration
[285.52s -> 288.04s]  together with analysis of sample complexity
[288.04s -> 291.64s]  that addresses things like approximation error or sampling error.
[291.64s -> 295.12s]  So in studying exploration,
[295.12s -> 297.96s]  we want to show that some exploration method is good.
[297.96s -> 302.44s]  And typically good for exploration means that it will learn a good policy,
[302.44s -> 306.32s]  a policy that deviates from the optimal policy by some epsilon
[306.32s -> 311.12s]  in time that is polynomial, typically in the number of states,
[311.12s -> 314.28s]  the number of actions, and in one over one minus gamma,
[314.32s -> 315.32s]  which is the horizon.
[318.04s -> 321.68s]  But separately from studying exploration, we might also study learning.
[321.68s -> 323.64s]  So if we somehow abstract away exploration,
[323.64s -> 327.56s]  if we somehow pretend that exploration works well,
[327.56s -> 329.80s]  how many samples do we need to effectively learn a model
[329.80s -> 332.04s]  or value function that results in good performance?
[332.04s -> 334.32s]  And this is a slightly separate question.
[334.32s -> 337.44s]  So it might actually behoove us to separate exploration from learning
[337.44s -> 340.36s]  because if we always analyze exploration and learning together,
[340.36s -> 343.56s]  oftentimes the difficulty will be dominated by exploration.
[343.56s -> 346.28s]  But worst case exploration is extremely pessimistic.
[346.28s -> 349.04s]  Typical exploration is not nearly as bad as the worst case
[349.04s -> 351.48s]  if, for example, you have a well-shaped reward.
[351.48s -> 354.16s]  So in many cases, we might want to abstract away explorations
[354.16s -> 355.48s]  and basically get rid of it
[355.48s -> 358.60s]  so that we can study the sample complexity of learning.
[358.60s -> 360.48s]  One way we could do that, for example,
[360.48s -> 362.56s]  is with a generative model assumption.
[362.56s -> 366.12s]  So this assumption just says you don't actually have to explore.
[366.12s -> 369.92s]  You can just sample any state action you want in the entire MDP,
[369.96s -> 373.92s]  however much you want, for basically any state action tuple.
[373.92s -> 375.08s]  This is, of course, unrealistic.
[375.08s -> 377.68s]  This is not what real reinforcement learning algorithms do.
[377.68s -> 380.12s]  But making this kind of assumption can be very convenient
[380.12s -> 383.32s]  because it allows us to study how difficult learning is,
[383.32s -> 385.88s]  essentially, if we assume that exploration is easy.
[388.96s -> 390.44s]  So one way to do this is to basically say
[390.44s -> 391.88s]  we're going to have oracle exploration.
[391.88s -> 393.36s]  For every state action tuple,
[393.36s -> 398.48s]  we're going to sample PS prime given SA n times.
[398.48s -> 400.68s]  So we'll just literally just, like, uniform,
[400.68s -> 402.60s]  carpet-bomb the whole MDP.
[402.60s -> 405.68s]  And at that point, the problem is still not solved.
[405.68s -> 408.32s]  We still need to study the effect of sampling error.
[410.44s -> 411.92s]  Okay, so we've got some questions.
[411.92s -> 412.84s]  We've got some assumptions.
[412.84s -> 416.08s]  But before I really dive into the analysis,
[416.08s -> 417.68s]  one of the things I want to talk about is
[417.68s -> 419.28s]  what is the point of all this?
[420.64s -> 423.68s]  So we could say that maybe the point of all this analysis
[423.68s -> 425.68s]  is to prove that our RL algorithm
[425.68s -> 427.28s]  will work perfectly every time.
[428.28s -> 431.92s]  That's usually not possible with current D power L methods.
[431.92s -> 433.80s]  When we put in all the bells and whistles,
[433.80s -> 435.52s]  all the tricks of the trade,
[435.52s -> 436.64s]  we don't even end up with algorithms
[436.64s -> 438.40s]  that are guaranteed to converge every time,
[438.40s -> 439.80s]  much less to work perfectly.
[441.20s -> 443.12s]  Another goal is to maybe understand
[443.12s -> 446.48s]  how errors are affected by problem parameters.
[447.40s -> 450.04s]  For example, the larger discounts work better than small ones.
[450.04s -> 451.92s]  Is it easier to solve a problem with a large discount
[451.92s -> 454.44s]  or is it easier to solve a problem with a small discount?
[454.44s -> 456.48s]  Is it easier to solve a problem with a large state space
[456.52s -> 457.48s]  or a small one?
[457.48s -> 460.08s]  Should we take more iterations or fewer iterations?
[460.08s -> 461.96s]  If we want half the error,
[461.96s -> 464.04s]  do we need two times the number of samples
[464.04s -> 465.80s]  or do we need four times the number of samples
[465.80s -> 466.80s]  or something else?
[467.88s -> 470.80s]  These are all somewhat qualitative questions at some level.
[470.80s -> 473.52s]  And usually we use precise theory
[473.52s -> 476.08s]  to get imprecise qualitative conclusions
[476.08s -> 478.16s]  about how various factors influence the performance
[478.16s -> 480.96s]  of our algorithms under strong assumptions.
[480.96s -> 483.56s]  And then we try to make the assumptions reasonable enough
[483.56s -> 486.24s]  that these conclusions are likely to apply to real problems
[487.00s -> 489.20s]  but they're not guaranteed to apply to real problems.
[490.52s -> 492.52s]  So it's very important to understand this.
[492.52s -> 494.84s]  I think there's a kind of a tendency sometimes
[494.84s -> 497.36s]  for people to say, well, my theoretical result proves
[497.36s -> 498.76s]  that my algorithm will work every time
[498.76s -> 503.48s]  or I have a provably good RL method.
[503.48s -> 504.36s]  That's nonsense.
[504.36s -> 505.92s]  We never have a provably good RL method
[505.92s -> 509.84s]  and anybody who tells you so is not being forthright.
[509.84s -> 511.32s]  Theory in reinforcement learning
[511.32s -> 513.48s]  and really in most of machine learning
[513.48s -> 515.52s]  is actually about getting qualitative
[515.52s -> 517.68s]  and somewhat heuristic conclusions
[517.68s -> 521.80s]  by analyzing greatly simplified special cases.
[521.80s -> 523.08s]  So don't take someone seriously
[523.08s -> 525.24s]  if they say their RL algorithm has provable guarantees.
[525.24s -> 526.32s]  It never does.
[526.32s -> 528.08s]  The assumptions are always unrealistic
[528.08s -> 532.08s]  and theory is at best a rough guide to what might happen.
[532.08s -> 533.36s]  This is not unique to machine learning.
[533.36s -> 534.92s]  Of course, the same is true in other areas.
[534.92s -> 538.04s]  For example, in physics, you could have some theory
[538.04s -> 541.28s]  that describes the efficiency of the ideal engine.
[541.28s -> 543.88s]  Now, no current physical theory will allow you
[543.88s -> 546.68s]  to analyze the actual fuel efficiency
[546.68s -> 548.68s]  of a gasoline engine in a modern car.
[548.68s -> 550.48s]  It's just too complicated.
[550.48s -> 553.40s]  What it will tell you is some guidance
[553.40s -> 555.96s]  about the limitations and potentials
[555.96s -> 558.72s]  of idealized versions of that system.
[558.72s -> 559.72s]  And that's exactly what we do
[559.72s -> 561.28s]  in reinforcement learning theory.
[561.28s -> 564.40s]  We provide qualitative guidance
[564.40s -> 566.52s]  about the limitations and potentials
[566.52s -> 568.40s]  of idealized versions of the algorithms
[568.40s -> 569.52s]  that we actually use.
[571.76s -> 573.04s]  And what we look at
[573.08s -> 575.64s]  when we come up with a theoretical result
[575.64s -> 578.76s]  is not a guarantee that the method
[578.76s -> 581.16s]  will perfectly have a particular sample complexity.
[581.16s -> 584.20s]  What we look at is how does its behavior change
[584.20s -> 587.08s]  as we change different problem parameters?
[587.08s -> 588.92s]  Does it become more efficient or less efficient
[588.92s -> 590.96s]  as the state space gets larger?
[590.96s -> 593.32s]  Do we need more iterations or fewer iterations
[593.32s -> 597.84s]  as we increase the discount factor or the horizon?
[597.84s -> 600.68s]  And these kinds of qualitative questions,
[600.68s -> 602.28s]  they're actually very important.
[602.28s -> 604.60s]  They can guide our choice of parameters
[604.60s -> 605.92s]  and algorithm design decisions,
[605.92s -> 608.12s]  and we can get some qualitative guidance
[608.12s -> 611.24s]  on those things by doing theoretical analysis.
[612.84s -> 614.92s]  So having understood all that,
[614.92s -> 616.36s]  let's actually get into the meat
[616.36s -> 618.44s]  of some of the analysis that we can do.
[619.76s -> 623.16s]  So we'll start with some basic sample complexity analysis.
[623.16s -> 625.76s]  And a lot of what I'll present in this lecture
[625.76s -> 629.20s]  follows the RL theory textbook by Alec Agarwal and others
[629.20s -> 630.72s]  linked at the bottom of the slide,
[630.72s -> 632.92s]  as well as some lecture slides
[632.92s -> 635.36s]  that were made by Avril Kumar last year.
[636.32s -> 638.56s]  So we'll start with the oracle exploration assumption,
[638.56s -> 640.64s]  meaning that for every state action tuple,
[640.64s -> 643.32s]  we can sample the next state S prime n times.
[644.60s -> 646.80s]  And the algorithm that we'll start with
[646.80s -> 651.20s]  is a very basic kind of model-based algorithm.
[651.20s -> 654.00s]  Now, I use the term model-based rather loosely.
[654.00s -> 656.96s]  This is actually a pretty idealized algorithm.
[656.96s -> 659.40s]  So what we're going to do
[659.40s -> 662.56s]  is we're going to estimate the transition probabilities,
[662.56s -> 664.52s]  P hat S prime given SA,
[664.52s -> 666.64s]  simply by counting the number of times
[666.64s -> 669.48s]  that we transition into S prime from SA.
[669.48s -> 671.88s]  A very simple tabular estimation strategy.
[671.88s -> 673.84s]  So there's no function approximation here.
[673.84s -> 675.24s]  There's no neural net.
[675.24s -> 676.80s]  We're just doing tabular estimation.
[676.80s -> 678.56s]  We're literally counting how many times
[678.56s -> 680.64s]  we land on a particular state S prime.
[680.64s -> 681.92s]  This of course only makes sense
[681.92s -> 684.08s]  if the states are discrete and the actions are discrete,
[684.08s -> 686.44s]  and we can build a table with all these numbers.
[689.68s -> 691.76s]  And then what we're going to do is
[691.76s -> 694.36s]  we'll first focus just on policy evaluation.
[694.36s -> 696.96s]  So given some fixed policy pi,
[696.96s -> 699.20s]  let's just use P hat to estimate
[699.20s -> 700.80s]  the Q function Q hat pi.
[701.80s -> 705.00s]  So this idealized algorithm takes some policy
[705.00s -> 706.76s]  and then it uses this P hat
[706.76s -> 709.16s]  to exactly estimate Q hat pi.
[710.00s -> 712.00s]  Now Q hat pi is not the exact Q function,
[712.00s -> 714.76s]  but it is the exact Q function under P hat, right?
[714.76s -> 717.20s]  Because P hat fully determines an MDP.
[717.36s -> 719.68s]  And then we'll do something like, you know,
[719.68s -> 723.32s]  Q value iteration to estimate Q hat pi, okay?
[723.32s -> 725.60s]  So step two is exact,
[725.60s -> 728.80s]  but using an inexact model P hat, okay?
[728.80s -> 730.36s]  This is a very simple algorithm.
[730.36s -> 733.04s]  And our goal is basically going to be to understand
[733.04s -> 735.20s]  the error that is induced
[735.20s -> 737.20s]  by the fact that P hat is not perfect.
[739.64s -> 744.36s]  Now to kind of take stock of what we're doing here,
[744.36s -> 747.40s]  this is of course a rather simplistic RL method.
[747.40s -> 749.80s]  This is not how we would usually do RL.
[749.80s -> 752.12s]  And the main purpose of this analysis
[752.12s -> 755.04s]  is really to understand how sampling error
[755.04s -> 758.16s]  in estimating P hat propagates into Q functions.
[759.08s -> 760.92s]  So for a little bit of context with this,
[760.92s -> 763.00s]  you know, in supervised learning theory,
[763.00s -> 766.12s]  there are a lot of tools that we can use
[766.12s -> 767.36s]  to answer questions like,
[767.36s -> 770.04s]  if I'm trying to estimate a quantity like P hat
[770.04s -> 771.64s]  and I have some number of samples,
[771.64s -> 773.64s]  how accurate is my estimate going to be?
[774.56s -> 776.28s]  And by introducing step two,
[776.28s -> 777.24s]  what we're really trying to do
[777.24s -> 778.08s]  is we're trying to take
[778.08s -> 779.96s]  these standard supervised learning results
[779.96s -> 783.40s]  and we're trying to pass them through the RL machinery
[783.40s -> 786.64s]  to say, well, how do bounds on sampling error
[786.64s -> 788.12s]  from supervised learning translate
[788.12s -> 791.76s]  into bounds on sampling error for Q functions
[791.76s -> 793.12s]  when that Q function is the result
[793.12s -> 794.92s]  of some kind of Bellman backup?
[794.92s -> 796.92s]  So that's really going to be the flavor
[796.92s -> 799.48s]  of the analysis that I'll present.
[799.48s -> 802.32s]  So the questions that we'll ask,
[802.32s -> 805.00s]  how close is Q hat pi to Q pi,
[805.00s -> 806.20s]  meaning that we're going to estimate
[806.20s -> 807.72s]  the Q values of our policy pi,
[807.72s -> 811.32s]  how close is it to the true Q values of that policy?
[811.32s -> 813.16s]  So ideally what we want to show
[813.16s -> 815.84s]  is that over all states and actions,
[815.84s -> 817.88s]  if we take the infinity norm of the difference
[817.88s -> 820.00s]  between Q pi and Q pi hat,
[820.00s -> 823.16s]  that infinity norm should be bounded by epsilon
[823.16s -> 825.80s]  with some probability one minus delta.
[825.80s -> 827.68s]  And if the number of samples is larger
[827.68s -> 829.64s]  than some function of epsilon and delta,
[829.64s -> 831.72s]  or hopefully it's a nice function,
[831.72s -> 833.84s]  a well-behaved function that is not exponential
[833.84s -> 835.40s]  or something crazy like that.
[836.56s -> 838.92s]  The infinity norm is just the max, right?
[838.92s -> 840.76s]  So if you see me write an infinity norm,
[840.76s -> 843.12s]  what it really means is just the difference
[843.12s -> 845.76s]  between the two things in the argument
[845.76s -> 847.80s]  for the worst case the action tuple.
[852.52s -> 854.24s]  And it's good to use the infinity norm
[854.24s -> 856.88s]  because it gives us a bound on worst case performance.
[856.88s -> 861.64s]  Now we could ask another question.
[861.64s -> 866.40s]  How close is Q hat star if we learn it using P hat?
[866.40s -> 867.92s]  So if we don't just evaluate some policy
[867.92s -> 869.12s]  that's given to us,
[869.12s -> 872.92s]  but if we instead try to actually run Q value iteration,
[872.92s -> 876.20s]  like I should find the optimal Q function under P hat,
[877.36s -> 878.20s]  how close is it going to be
[878.20s -> 880.56s]  to the true optimal Q function?
[880.56s -> 884.00s]  So what's the difference between Q star and Q hat star?
[884.00s -> 884.96s]  And ideally we'd like to see
[885.00s -> 886.92s]  that that's bounded by some epsilon.
[888.12s -> 890.28s]  So Q hat star is the optimal Q function we learn
[890.28s -> 891.68s]  under our learned model,
[891.68s -> 892.52s]  which is basically what happens
[892.52s -> 894.32s]  if we do RL with this method.
[896.36s -> 897.48s]  And as I mentioned before,
[897.48s -> 899.96s]  we could also ask how good is the resulting policy,
[899.96s -> 901.92s]  which is not the same.
[901.92s -> 905.16s]  So if we take the policy pi hat,
[905.16s -> 908.12s]  which corresponds to Q hat star,
[908.12s -> 910.60s]  and we take the true Q function of pi hat,
[910.60s -> 912.04s]  how different is that from Q star?
[912.04s -> 914.52s]  Meaning how suboptimal is the policy we get
[915.04s -> 917.80s]  by running Q value iteration under P hat.
[919.36s -> 921.16s]  And that last question is really the one
[921.16s -> 922.84s]  that quantifies the performance of RL,
[922.84s -> 924.32s]  because that's really telling us
[924.32s -> 926.20s]  how much worse is the policy we get
[926.20s -> 927.56s]  under the model P hat
[927.56s -> 930.16s]  than the best policy we could have gotten anywhere.
[931.40s -> 933.08s]  Now it turns out that actually the first question,
[933.08s -> 934.76s]  the policy evaluation question,
[934.76s -> 936.32s]  gives us a tool that is very good
[936.32s -> 937.72s]  for answering the other two questions.
[937.72s -> 939.72s]  So we'll mostly focus on the first question,
[939.72s -> 943.20s]  how close is Q hat pi to Q pi for a given policy pi,
[943.24s -> 945.68s]  and then we'll see how to utilize that as a tool
[945.68s -> 947.32s]  to answer the other two questions.
[950.64s -> 953.84s]  Okay, so before I get into this,
[953.84s -> 956.80s]  let's introduce some standard tools
[956.80s -> 959.00s]  in supervised learning theory.
[959.00s -> 960.40s]  So all of this analysis has to do
[960.40s -> 962.72s]  with how the number of samples affects the error
[962.72s -> 964.80s]  in estimating some quantity.
[964.80s -> 966.80s]  In supervised learning,
[966.80s -> 969.88s]  we have inequalities that allow us to bound the error
[969.88s -> 972.56s]  for estimating some quantity using some number of samples,
[972.56s -> 975.04s]  and these are referred to as concentration inequalities,
[975.04s -> 977.96s]  because they quantify how quickly our estimate
[977.96s -> 980.12s]  of some random variable concentrates
[980.12s -> 983.32s]  around the true expected value of that variable.
[984.28s -> 986.40s]  So whenever we need to answer questions
[986.40s -> 988.80s]  about how close a learned function is to the true function
[988.80s -> 990.52s]  in terms of the number of samples,
[990.52s -> 992.52s]  we use concentration inequalities.
[993.64s -> 995.92s]  One of the most basic concentration inequalities,
[995.92s -> 997.88s]  and typically the first one that you would learn about
[997.88s -> 1000.36s]  if you take a machine learning theory class,
[1000.36s -> 1001.76s]  is Hoeffding's inequality.
[1002.88s -> 1005.00s]  So the full statement of Hoeffding's inequality
[1005.00s -> 1009.12s]  is given here, but it's a little bit opaque,
[1009.12s -> 1010.60s]  but it has a very simple interpretation.
[1010.60s -> 1012.84s]  So let me describe the full statement,
[1012.84s -> 1014.84s]  and then I'll provide a little bit of intuition
[1014.84s -> 1017.00s]  for what it's really saying.
[1017.00s -> 1019.88s]  So suppose that x1, x2 through xn
[1019.88s -> 1022.40s]  are a sequence of independent identically distributed
[1022.40s -> 1024.40s]  random variables with me and mu.
[1026.36s -> 1028.32s]  What are x1 through xn?
[1028.32s -> 1030.12s]  Well, these are your samples.
[1030.12s -> 1032.12s]  So you have some true distribution,
[1032.60s -> 1034.52s]  and that true distribution has a mean mu.
[1034.52s -> 1035.76s]  Now, we don't really know anything else
[1035.76s -> 1036.60s]  about that distribution.
[1036.60s -> 1038.04s]  We'll just say it has a mean mu,
[1038.04s -> 1040.48s]  and you take some samples from that distribution.
[1041.80s -> 1045.32s]  Let x bar n be the sample-wise average.
[1045.32s -> 1050.32s]  So x bar n is the sum over all the xi's divided by n.
[1050.80s -> 1052.88s]  So it's the average value.
[1052.88s -> 1054.28s]  Now, this is your estimate of the average, right?
[1054.28s -> 1056.04s]  The true average may not match this.
[1056.04s -> 1058.08s]  If you only generated like two samples,
[1058.08s -> 1059.32s]  averaging them together
[1059.32s -> 1061.92s]  doesn't give you their true average.
[1062.56s -> 1063.40s]  You might incur some error
[1063.40s -> 1065.36s]  because you have too few samples.
[1065.36s -> 1066.84s]  So what Hoeffding's inequality does
[1066.84s -> 1070.32s]  is it quantifies how much error you would get
[1070.32s -> 1071.36s]  as a function of n.
[1072.48s -> 1076.84s]  So suppose that each of these samples
[1076.84s -> 1081.80s]  is in the range from b minus to b plus, right?
[1081.80s -> 1083.92s]  So this is just saying that whatever their mean
[1083.92s -> 1084.96s]  or whatever their distribution is,
[1084.96s -> 1086.52s]  they can never be less than b minus
[1086.52s -> 1088.68s]  and they can never be larger than b plus.
[1089.68s -> 1093.40s]  And then we have the following two results.
[1093.40s -> 1097.40s]  Your sample-based estimate of the average, x bar n,
[1098.96s -> 1102.40s]  is greater than or equal to the true average
[1102.40s -> 1106.44s]  plus epsilon with probability that is at most
[1106.44s -> 1109.72s]  e to the negative two n epsilon squared
[1109.72s -> 1112.68s]  divided by b plus minus b minus squared.
[1114.40s -> 1116.84s]  So what this means is that the probability
[1117.00s -> 1119.32s]  that your sample-based estimate of the mean
[1119.32s -> 1120.56s]  differs from the true mean
[1120.56s -> 1122.72s]  in the positive direction by epsilon
[1122.72s -> 1126.20s]  is no greater than e to the negative two n epsilon squared
[1126.20s -> 1128.24s]  over b plus plus b minus squared.
[1129.52s -> 1130.48s]  This is actually very good.
[1130.48s -> 1133.40s]  This means that your probability of making a mistake
[1134.40s -> 1137.68s]  larger than epsilon decreases exponentially
[1137.68s -> 1139.08s]  in the number of samples.
[1141.32s -> 1143.08s]  And similarly, you have a bound on the other side,
[1143.08s -> 1144.64s]  the probability of your estimate
[1144.64s -> 1146.96s]  being less than mu minus epsilon
[1146.96s -> 1148.40s]  is also less than or equal to epsilon
[1148.40s -> 1150.24s]  to the negative two n epsilon squared
[1150.24s -> 1152.56s]  divided by b plus plus b minus squared.
[1153.64s -> 1156.88s]  So this describes how quickly your estimate x bar n
[1156.88s -> 1159.16s]  concentrates around the true mean mu
[1159.16s -> 1161.52s]  because as epsilon goes to zero,
[1161.52s -> 1164.44s]  then your estimate approaches mu
[1164.44s -> 1165.92s]  and here we see the probability
[1165.92s -> 1169.68s]  that your estimate will deviate from mu
[1169.68s -> 1170.84s]  by more than epsilon.
[1171.84s -> 1174.72s]  Now this has a few implications, right?
[1174.72s -> 1177.12s]  So if we estimate mu with n samples,
[1177.12s -> 1179.64s]  the probability that we're off by more than epsilon
[1179.64s -> 1182.36s]  is at most the thing on the right-hand side
[1182.36s -> 1186.28s]  of this inequality and we can equivalently reinterpret it
[1186.28s -> 1189.08s]  to say that if you want this probability to be delta,
[1189.08s -> 1191.32s]  so if you want the probability to be off
[1191.32s -> 1193.92s]  by more than epsilon to be at most delta,
[1193.92s -> 1196.60s]  meaning that you have an error less than epsilon
[1196.60s -> 1199.64s]  with probability at least one minus delta,
[1200.08s -> 1201.60s]  then you can simply solve for delta.
[1201.60s -> 1204.92s]  So you can say I want delta to be less than or equal to
[1204.92s -> 1207.84s]  two times e to the negative two n epsilon squared
[1207.84s -> 1209.56s]  over b plus plus b minus squared.
[1209.56s -> 1211.56s]  The reason the two is there is because you can be off
[1211.56s -> 1212.60s]  either in the positive direction
[1212.60s -> 1214.16s]  or in the negative direction.
[1215.24s -> 1217.72s]  And then you can just solve for delta.
[1217.72s -> 1219.68s]  So you can take the log of both sides
[1220.72s -> 1223.20s]  and then you can do a little bit of algebra,
[1223.20s -> 1224.40s]  rearrange these things.
[1224.40s -> 1226.12s]  So here in the first step,
[1226.12s -> 1229.24s]  what I did is I divided both sides by two and took the log
[1229.68s -> 1230.52s]  and then in the next step,
[1230.52s -> 1234.40s]  what I did is I divided both sides
[1234.40s -> 1236.96s]  by b plus minus b minus squared over two n
[1236.96s -> 1240.00s]  and negated both sides.
[1240.00s -> 1242.80s]  So that changes the less than or equal to
[1242.80s -> 1244.28s]  into greater than or equal to
[1244.28s -> 1245.72s]  and then you take the square root
[1245.72s -> 1250.36s]  and you see that you need b plus minus b minus
[1250.36s -> 1252.08s]  over square root of two n
[1252.08s -> 1254.28s]  times the square root of log two over delta
[1254.28s -> 1256.84s]  to be greater than or equal to epsilon.
[1256.84s -> 1258.56s]  So if you want some error epsilon
[1258.60s -> 1261.32s]  with probability delta
[1261.32s -> 1263.08s]  or you want the order to be less than epsilon
[1263.08s -> 1264.68s]  with probability one minus delta,
[1264.68s -> 1266.28s]  then the number of samples you need
[1266.28s -> 1268.12s]  scales as the square root of n.
[1271.52s -> 1273.80s]  Or you can also write down a function for n
[1273.80s -> 1275.60s]  in terms of epsilon and delta, same thing.
[1275.60s -> 1277.88s]  Just do a bunch of algebraic manipulation
[1277.88s -> 1279.76s]  to get n on one side
[1279.76s -> 1284.76s]  and you can see that if you have this number of samples,
[1285.60s -> 1289.56s]  then you will have error at most epsilon
[1289.56s -> 1291.64s]  with probability no larger than delta.
[1293.16s -> 1294.76s]  So one of the conclusions that you can get from this
[1294.76s -> 1297.36s]  is that error scales with one over square root of n.
[1298.44s -> 1300.24s]  That's pretty convenient.
[1300.24s -> 1301.72s]  So hopefully this gives you some idea
[1301.72s -> 1303.72s]  of how these concentration inequalities work.
[1303.72s -> 1306.44s]  You write down some equation for the probability
[1306.44s -> 1309.84s]  that you'll be off by epsilon
[1309.84s -> 1312.12s]  and then you can manipulate that equation
[1312.12s -> 1313.76s]  to solve for the probability delta
[1313.80s -> 1315.68s]  or to solve for the number of samples.
[1318.32s -> 1320.76s]  Now, in a lot of the analysis
[1320.76s -> 1322.60s]  that we do in reinforcement learning,
[1322.60s -> 1324.28s]  we're concerned with estimating probabilities
[1324.28s -> 1326.16s]  of categorical variables.
[1326.16s -> 1327.72s]  So P of S prime given SA
[1327.72s -> 1330.08s]  is a distribution over a categorical variable,
[1330.08s -> 1331.32s]  not a real value variable.
[1331.32s -> 1335.60s]  So Hoeffding's inequality applies to estimating the mean
[1335.60s -> 1337.56s]  of a continuous valued random variable.
[1338.40s -> 1341.08s]  But here, if we're estimating P hat,
[1341.08s -> 1342.64s]  we're actually concerned with our accuracy
[1342.64s -> 1344.48s]  in estimating the probability distribution
[1344.48s -> 1346.96s]  over a categorical variable, in this case, S prime.
[1348.84s -> 1351.80s]  So a similar kind of concentration inequality
[1351.80s -> 1353.04s]  can be derived for that.
[1354.04s -> 1357.92s]  Let's say that Z is some kind of discrete random variable
[1357.92s -> 1360.28s]  that takes values in one through D.
[1360.28s -> 1361.96s]  So D is the cardinality of Z.
[1361.96s -> 1363.96s]  D is the number of possible values.
[1363.96s -> 1365.64s]  And they're distributed according to Q.
[1365.64s -> 1367.72s]  So Q is a vector of probabilities
[1367.72s -> 1371.24s]  that are all greater than zero and sums to one.
[1371.24s -> 1373.36s]  And there are D values in Q.
[1373.36s -> 1378.20s]  So if you write it as a vector, where the jth entry
[1378.20s -> 1381.40s]  is the probability that Z takes on its jth value,
[1381.40s -> 1384.28s]  and you assume that you have n IID samples,
[1385.48s -> 1388.56s]  and that the empirical estimate is given
[1388.56s -> 1390.28s]  by basically counting the number of times
[1390.28s -> 1391.12s]  you get each value.
[1391.12s -> 1394.28s]  So exactly the way that we're estimating P hat up above.
[1394.28s -> 1396.16s]  Then you have a concentration inequality
[1396.16s -> 1399.00s]  that looks kind of similar to Hoeffding's inequality.
[1400.00s -> 1402.40s]  But for estimating the probabilities
[1402.40s -> 1405.64s]  of these random variables.
[1406.52s -> 1408.88s]  So it's really the second one that we care about.
[1408.88s -> 1410.20s]  So the first one is a probability
[1410.20s -> 1413.04s]  that your error in the two norm is gonna be bounded.
[1413.04s -> 1414.24s]  The second one has to do with the error
[1414.24s -> 1415.08s]  in the one norm.
[1415.08s -> 1416.04s]  And the error in the one norm,
[1416.04s -> 1417.56s]  that's total variation divergence.
[1417.56s -> 1419.04s]  That's the one we're gonna be using.
[1419.04s -> 1421.92s]  So if you look at the last line of this theorem,
[1421.92s -> 1424.96s]  the probability that your estimate of the probabilities
[1425.28s -> 1429.44s]  that Q hat minus the true probabilities Q
[1429.44s -> 1432.08s]  in the one norm, which is total variation divergence,
[1432.08s -> 1434.76s]  the probability that their total variation divergence
[1434.76s -> 1437.72s]  is greater than or equal to the square root of d
[1437.72s -> 1440.04s]  times one over root n plus epsilon
[1441.84s -> 1443.32s]  is less than or equal to e
[1443.32s -> 1445.52s]  to the negative n times epsilon squared.
[1446.56s -> 1448.52s]  And notice the similarity to Hoeffding's inequality.
[1448.52s -> 1449.76s]  So in Hoeffding's inequality,
[1449.76s -> 1451.84s]  we also had the probability be bounded
[1451.92s -> 1455.88s]  by e to the negative two n times epsilon squared
[1455.88s -> 1457.40s]  with some other coefficients.
[1457.40s -> 1460.12s]  So here we have a negative n epsilon squared
[1460.12s -> 1462.12s]  is just that the constants are different.
[1462.12s -> 1465.16s]  And the thing that we are greater than or equal to
[1466.28s -> 1467.92s]  is not epsilon.
[1467.92s -> 1471.56s]  It's now this root d one over root n plus epsilon thing.
[1472.68s -> 1473.88s]  But we can do all the same stuff.
[1473.88s -> 1475.84s]  We can take this quantity
[1475.84s -> 1478.28s]  and we can solve it for delta, for example.
[1478.28s -> 1480.60s]  And we know that delta is less than or equal to e
[1480.64s -> 1483.20s]  to the negative n epsilon squared.
[1483.20s -> 1484.76s]  We can solve it for epsilon
[1484.76s -> 1486.32s]  and we get epsilon is less than or equal to
[1486.32s -> 1489.76s]  one over root n times the square root
[1489.76s -> 1491.08s]  of log one over delta.
[1492.52s -> 1493.72s]  And we can solve it for n
[1493.72s -> 1495.08s]  and we get that n is less than or equal to
[1495.08s -> 1497.52s]  one over epsilon squared log one over delta.
[1497.52s -> 1498.56s]  So all the same stuff,
[1498.56s -> 1501.52s]  we can describe what the error will be
[1501.52s -> 1502.92s]  as a function of the number of samples
[1502.92s -> 1504.28s]  or how many samples we need
[1504.28s -> 1507.24s]  as a function of the error and the probability.
[1507.24s -> 1511.96s]  So if we just make a substitution,
[1511.96s -> 1514.32s]  substitute the symbols for p hat,
[1514.32s -> 1519.32s]  the cardinality of S prime is S, capital S,
[1519.72s -> 1521.28s]  well, the cardinality of capital S,
[1521.28s -> 1522.56s]  that's the number of states.
[1522.56s -> 1525.64s]  So if we just plug this directly into this inequality,
[1525.64s -> 1527.76s]  we know that if we use n samples
[1527.76s -> 1530.08s]  for every state action tuple,
[1530.08s -> 1532.08s]  then the total variation divergence,
[1532.08s -> 1535.36s]  the one norm between p hat S prime given S a
[1535.40s -> 1538.16s]  and the true p of S prime given S a
[1538.16s -> 1539.64s]  is less than or equal to the square root
[1539.64s -> 1541.36s]  of the number of states times one
[1541.36s -> 1543.92s]  over the square root of n plus epsilon
[1543.92s -> 1546.36s]  with probability one minus delta.
[1550.00s -> 1551.96s]  Now, this is when we,
[1551.96s -> 1554.60s]  in the case where we have n samples for each S a.
[1554.60s -> 1556.00s]  So the total number of samples
[1556.00s -> 1557.68s]  that we will be using to estimate this model
[1557.68s -> 1559.76s]  is n times the number of states
[1559.76s -> 1561.48s]  times the number of actions.
[1561.48s -> 1562.80s]  So just important to keep in mind
[1562.80s -> 1564.24s]  some of the constants here.
[1566.28s -> 1570.32s]  And if I do a little bit of symbolic manipulation on this,
[1570.32s -> 1572.76s]  just basically distribute the square root of S
[1572.76s -> 1575.72s]  into the parentheses, I get this.
[1575.72s -> 1578.68s]  And roughly I can say that this is bounded
[1578.68s -> 1581.08s]  by some constant times the square root
[1581.08s -> 1583.44s]  of the number of states times log one over delta
[1583.44s -> 1584.48s]  divided by n, right?
[1584.48s -> 1588.56s]  So it actually is one plus
[1590.20s -> 1591.76s]  the square root of log one over delta,
[1591.76s -> 1594.32s]  but as long as we don't care about
[1594.32s -> 1596.64s]  the constants, as long as you don't care about delta,
[1596.64s -> 1598.16s]  it's, we can write it as some C
[1598.16s -> 1599.76s]  and that'll make it a little bit convenient.
[1599.76s -> 1601.92s]  So then we have fewer terms flying around.
[1603.76s -> 1605.48s]  Okay, so now that we've got
[1605.48s -> 1607.40s]  our concentration inequalities out of the way,
[1607.40s -> 1610.08s]  and we can understand how accurate our learned model
[1610.08s -> 1613.36s]  is going to be as a function of the number of samples,
[1613.36s -> 1615.44s]  let's relate the error in p hat
[1615.44s -> 1618.12s]  to the error in q hat pi.
[1618.12s -> 1619.40s]  And this is where we actually get into
[1619.40s -> 1621.48s]  the RL centric part of the analysis.
[1621.48s -> 1623.72s]  So, so far, all of our discussion
[1623.76s -> 1626.48s]  dealt with just general machine learning theory.
[1626.48s -> 1627.76s]  Now we're gonna get into the parts
[1627.76s -> 1630.36s]  that are really specific to reinforcement learning.
[1631.68s -> 1636.04s]  So let's try to relate the model p to q pi.
[1636.04s -> 1637.80s]  For now, we won't worry about approximations
[1637.80s -> 1639.80s]  or anything, I just wanna write down some equations
[1639.80s -> 1642.16s]  that relate p to q pi for a fixed pi.
[1643.24s -> 1644.60s]  And we saw this actually before
[1644.60s -> 1645.84s]  when we talked about offline RL,
[1645.84s -> 1648.76s]  but I'll just repeat it here for convenience.
[1648.76s -> 1650.00s]  So this is the Bellman equation.
[1650.00s -> 1652.84s]  So q pi at some state s and action sa
[1652.84s -> 1654.76s]  is equal to the reward RSA
[1654.76s -> 1659.24s]  plus gamma times the expected value over s prime,
[1659.24s -> 1661.60s]  distributed according to p of s prime given sa
[1661.60s -> 1663.20s]  of v pi s prime.
[1664.68s -> 1665.96s]  And v pi s prime, of course,
[1665.96s -> 1670.00s]  is the expected value of q pi s prime a prime under pi.
[1671.56s -> 1673.08s]  And we can expand out the expectation,
[1673.08s -> 1674.52s]  just write it as a sum.
[1674.52s -> 1676.44s]  Writing it as a sum will make it easier
[1676.44s -> 1679.00s]  to turn this into a linear algebra equation
[1679.00s -> 1681.04s]  that we can then manipulate symbolically.
[1682.04s -> 1684.32s]  So if we write this in vector notation,
[1684.32s -> 1687.92s]  if we say that q is a vector with sa entries,
[1687.92s -> 1689.68s]  r is a vector with sa entries,
[1689.68s -> 1691.68s]  and p is a matrix, we can write q pi
[1691.68s -> 1694.04s]  is equal to r plus gamma p v pi.
[1694.88s -> 1697.52s]  So if we have, let's say, two states and two actions,
[1697.52s -> 1701.56s]  q pi is a big vector with sa entries.
[1701.56s -> 1703.68s]  So it has two times two, four entries,
[1703.68s -> 1706.00s]  if you have two states and two actions.
[1706.00s -> 1709.48s]  r is also a big vector with four entries, two by two.
[1710.40s -> 1711.52s]  And it doesn't actually matter
[1711.52s -> 1712.44s]  how you arrange these entries,
[1712.44s -> 1717.52s]  so you could imagine that it's like s1 a1, s1 a2, s2 a1, s2 a2,
[1717.52s -> 1719.32s]  or you could imagine that it's the other way around,
[1719.32s -> 1720.16s]  it doesn't really matter.
[1720.16s -> 1722.20s]  It's just something that is of length sa.
[1723.32s -> 1725.56s]  p is a matrix that describes
[1725.56s -> 1728.12s]  how a state-action tuple transitions into a state.
[1729.68s -> 1733.20s]  So p has sa rows,
[1733.20s -> 1734.52s]  so for the stuff on the right-hand side
[1734.52s -> 1735.92s]  of the conditioning bar,
[1735.92s -> 1738.16s]  and it has s columns,
[1738.16s -> 1740.36s]  because it gives you the probability of each state
[1740.36s -> 1741.76s]  given some state in action.
[1742.76s -> 1744.76s]  So the number of columns is the number of states,
[1744.76s -> 1745.60s]  the number of rows
[1745.60s -> 1747.72s]  is the number of possible state-action tuples.
[1749.68s -> 1751.64s]  And v is a vector with the number of entries
[1751.64s -> 1753.16s]  equal to the number of states.
[1753.16s -> 1755.16s]  And you can see that all the dimensionalities line up.
[1755.16s -> 1757.40s]  So you can multiply p by v pi,
[1757.40s -> 1760.48s]  and that gives you a vector with sa entries,
[1760.48s -> 1762.44s]  and then you can add that to r,
[1762.44s -> 1764.92s]  and that'll be the same dimensionality as q pi.
[1765.76s -> 1770.76s]  We can also write v as some matrix capital pi times q pi.
[1773.72s -> 1776.32s]  Remember that v is an expected value
[1776.32s -> 1779.16s]  with respect to the actions of the q function.
[1779.16s -> 1781.00s]  So pi here is a matrix
[1781.00s -> 1785.56s]  that now has s different rows and sa columns,
[1785.56s -> 1788.72s]  and every entry is the probability
[1788.72s -> 1791.88s]  of some action in some state.
[1794.92s -> 1799.92s]  So that means that q pi is equal to r
[1805.68s -> 1810.68s]  plus gamma times some matrix p pi times q pi,
[1811.48s -> 1813.72s]  where p pi is just what you get
[1813.72s -> 1817.44s]  by multiplying capital p by capital pi.
[1817.44s -> 1822.44s]  All right, so now with that out of the way,
[1825.40s -> 1830.40s]  we have q pi is equal to r plus gamma p pi q pi.
[1830.44s -> 1835.44s]  So p pi is just some kind of a matrix.
[1837.60s -> 1841.68s]  And what we can do is we can take this gamma p pi q pi
[1841.68s -> 1844.56s]  and throw it on the left-hand side of the equality.
[1844.56s -> 1847.08s]  So we can say that q pi minus gamma p pi q pi
[1847.64s -> 1848.48s]  is equal to r.
[1848.48s -> 1851.32s]  And now we can see that you have all the terms
[1851.32s -> 1853.96s]  involving q pi collected on the left-hand side.
[1853.96s -> 1857.36s]  So you can distribute out the matrix multiplying them
[1857.36s -> 1862.24s]  and you get i minus gamma p pi times q pi is equal to r.
[1862.24s -> 1865.32s]  It turns out, although it is not trivial to prove this,
[1865.32s -> 1866.84s]  that i minus gamma p pi
[1866.84s -> 1869.08s]  is actually always going to be invertible.
[1869.08s -> 1870.92s]  So you can write q pi as being equal
[1870.92s -> 1874.20s]  to i minus gamma p pi inverse times r.
[1874.20s -> 1876.36s]  And now we've related p to q pi.
[1876.40s -> 1879.00s]  So we can write q pi as a function of p,
[1880.20s -> 1883.00s]  and more specifically as a function of p pi.
[1883.00s -> 1886.76s]  It's a nonlinear function, but we have this relationship
[1886.76s -> 1889.68s]  and then we can use it to describe how errors in p
[1889.68s -> 1892.40s]  will affect errors in q, okay?
[1894.48s -> 1897.48s]  Now, this is true for any dynamics.
[1897.48s -> 1899.00s]  So just like we can write q pi
[1899.00s -> 1901.24s]  is equal to i minus gamma p pi inverse r,
[1901.24s -> 1903.40s]  we can also write q hat pi
[1903.40s -> 1906.84s]  is equal to i minus gamma p hat pi inverse times r.
[1906.84s -> 1910.04s]  Because remember q hat pi was obtained
[1910.04s -> 1915.04s]  just by solving the learned MDP determined by p hat.
[1918.44s -> 1920.56s]  So now we're gonna introduce a little lemma
[1920.56s -> 1923.80s]  that we're going to use to understand the relationship
[1923.80s -> 1927.32s]  between errors in p hat and errors in q hat.
[1927.32s -> 1928.64s]  We'll actually introduce two lemmas
[1928.64s -> 1930.80s]  and then we'll put them together and get our conclusion.
[1930.80s -> 1933.48s]  So the first lemma is what's called the simulation lemma.
[1933.48s -> 1937.72s]  The simulation lemma describes how a q function
[1937.72s -> 1941.24s]  in the true MDP q pi differs from the q function
[1941.24s -> 1943.76s]  in the learned MDP q hat pi.
[1945.28s -> 1947.48s]  This is the statement of the simulation lemma.
[1947.48s -> 1950.48s]  It might be a little opaque, but I'll unpack this shortly.
[1950.48s -> 1954.44s]  So the simulation lemma says that q pi minus q hat pi
[1954.44s -> 1959.40s]  is equal to gamma times i minus gamma p hat pi inverse
[1959.40s -> 1961.88s]  times p minus p hat times v pi.
[1964.20s -> 1966.68s]  So this part is the difference in probabilities.
[1966.68s -> 1967.52s]  This is basically the difference
[1967.52s -> 1970.16s]  between the true model and the learned model.
[1970.16s -> 1971.20s]  This is the true value.
[1971.20s -> 1973.56s]  So you can think of these as differences in probabilities
[1973.56s -> 1975.52s]  weighted by their value.
[1975.52s -> 1979.16s]  The value kind of roughly speaking
[1979.16s -> 1981.00s]  constitutes their relative importance.
[1982.36s -> 1983.96s]  And this is this evaluation operator.
[1983.96s -> 1985.52s]  This is the thing that takes reward functions
[1985.52s -> 1986.72s]  to q functions.
[1987.56s -> 1990.36s]  So roughly speaking, what we're doing is
[1990.36s -> 1993.24s]  we're taking our true value function.
[1993.24s -> 1995.84s]  We're converting into a kind of a pseudo reward
[1995.84s -> 1999.24s]  by passing it through the difference of the dynamics.
[1999.24s -> 2002.44s]  And then we are basically running q iteration
[2002.44s -> 2004.36s]  on the pseudo reward.
[2004.36s -> 2006.60s]  Okay, slightly opaque result,
[2006.60s -> 2008.68s]  but it'll be pretty useful later.
[2008.68s -> 2011.12s]  So first I'll prove the simulation lemma
[2011.12s -> 2012.44s]  and that'll give you kind of a taste
[2012.44s -> 2013.44s]  for some of the tools we use
[2013.44s -> 2015.20s]  in these algebraic manipulations.
[2015.20s -> 2016.40s]  Then I'll prove one more lemma
[2016.96s -> 2017.80s]  and then I'll put them together
[2017.80s -> 2019.76s]  to actually quantify the error in q hat pi.
[2021.72s -> 2024.00s]  So the way we do this is actually fairly mechanical.
[2024.00s -> 2026.28s]  We have q pi minus q hat pi,
[2026.28s -> 2029.16s]  and we're gonna replace q hat pi
[2029.16s -> 2031.72s]  with the equation up above.
[2031.72s -> 2032.56s]  Why?
[2032.56s -> 2034.04s]  Well, notice how the right hand side
[2034.04s -> 2036.80s]  of the simulation lemma doesn't contain q hat pi,
[2036.80s -> 2038.20s]  it contains v pi.
[2038.20s -> 2041.40s]  So we're gonna get v pi out of q pi, of course.
[2041.40s -> 2042.80s]  And it also contains p hat.
[2042.80s -> 2044.20s]  So we need something to get p hat in there.
[2044.20s -> 2046.00s]  So what we're gonna do is we're gonna take q hat pi
[2046.56s -> 2048.80s]  and we'll replace it by this equation,
[2048.80s -> 2051.56s]  i minus gamma p hat pi inverse r,
[2051.56s -> 2052.80s]  because that contains p hat
[2052.80s -> 2054.44s]  and let's get rid of q hat pi.
[2056.96s -> 2059.96s]  And for q pi, what we're going to do
[2059.96s -> 2063.68s]  is we're gonna stick in a p hat in there too,
[2063.68s -> 2064.92s]  because remember we need to get a p hat
[2064.92s -> 2066.40s]  in front of everything.
[2066.40s -> 2070.12s]  So we'll just stick in i minus gamma p hat pi inverse
[2070.12s -> 2072.00s]  times i minus gamma p hat pi, right?
[2072.00s -> 2075.64s]  Because that's a matrix inverse times itself,
[2076.16s -> 2078.88s]  which is identity, so we can always put that in.
[2078.88s -> 2080.08s]  Okay, so now we're getting somewhere.
[2080.08s -> 2082.72s]  We have an i minus gamma p hat pi inverse
[2082.72s -> 2086.40s]  in front of both terms, and we have a q pi in there.
[2086.40s -> 2088.96s]  So hopefully that'll go somewhere.
[2088.96s -> 2092.88s]  So now what I'm going to do is I'm gonna take that r
[2092.88s -> 2095.72s]  at the end and I'll replace the r
[2095.72s -> 2100.04s]  with i minus gamma p pi times q pi, right?
[2100.04s -> 2103.00s]  Because q pi is equal to i minus gamma p pi inverse r.
[2103.00s -> 2106.32s]  So I can multiply both sides by i minus gamma p pi,
[2106.32s -> 2110.88s]  and that'll turn that r into an i minus gamma p pi q pi.
[2110.88s -> 2112.04s]  So now I'm really getting somewhere.
[2112.04s -> 2113.16s]  I've got two terms.
[2113.16s -> 2115.76s]  They both have an i minus gamma p hat pi inverse
[2115.76s -> 2118.16s]  in front of them, just like the simulation lemma.
[2118.16s -> 2121.08s]  One of the terms has a p hat and the other one has a p.
[2124.60s -> 2127.40s]  So when I group them together, I can basically,
[2127.40s -> 2129.72s]  since they both have an i minus gamma p hat pi inverse
[2129.72s -> 2130.56s]  in front of them,
[2130.76s -> 2133.28s]  i minus gamma p hat pi inverse.
[2133.28s -> 2137.08s]  Then in parentheses, I have i minus gamma p hat pi
[2137.08s -> 2139.32s]  minus i minus gamma p pi,
[2139.32s -> 2142.20s]  and the whole thing is multiplied by q pi.
[2142.20s -> 2144.36s]  So the identities cancel out.
[2144.36s -> 2148.24s]  And since that second one becomes a minus minus gamma p pi
[2148.24s -> 2153.24s]  I can switch the order and I get p pi minus p hat pi.
[2153.56s -> 2154.84s]  That is all multiplied by gamma.
[2154.84s -> 2156.36s]  So I take the gamma out front
[2156.36s -> 2159.72s]  and that leaves me with gamma i minus gamma p hat pi
[2159.76s -> 2164.08s]  inverse times p pi minus p hat pi times q pi.
[2165.52s -> 2170.52s]  Now remember that p pi is just p times this matrix pi.
[2170.76s -> 2173.36s]  And it's the same matrix pi for both p and p hat.
[2174.40s -> 2176.00s]  So that gives me this equation.
[2177.00s -> 2181.92s]  And v pi is just capital pi times q pi.
[2182.96s -> 2185.40s]  So I can take the capital pi out,
[2185.40s -> 2186.64s]  replace that with v pi,
[2186.64s -> 2188.92s]  and then I finished proving the lemma.
[2190.56s -> 2194.52s]  So that's kind of the nature of the algebraic manipulation.
[2194.52s -> 2195.92s]  It's not terribly insightful,
[2195.92s -> 2197.68s]  but it gives us this useful lemma.
[2198.84s -> 2199.92s]  Now here's another useful lemma.
[2199.92s -> 2201.88s]  This one is gonna be even simpler.
[2201.88s -> 2206.12s]  Given p pi and any vector in RSA,
[2206.12s -> 2208.04s]  we're gonna have this relationship.
[2208.04s -> 2210.40s]  If we apply i minus gamma p pi inverse,
[2210.40s -> 2214.72s]  meaning this evaluation operator to the vector v,
[2214.72s -> 2217.16s]  the infinity norm of that is less than or equal
[2217.16s -> 2220.44s]  to the infinity norm of v divided by one minus gamma.
[2220.44s -> 2222.96s]  Meaning that applying i minus gamma p pi inverse
[2222.96s -> 2225.72s]  to some vector will blow up the infinity norm
[2225.72s -> 2227.16s]  of that vector by at most a factor
[2227.16s -> 2228.72s]  of one over one minus gamma.
[2230.12s -> 2234.40s]  So intuitively, the q function corresponding to a reward v
[2234.40s -> 2237.40s]  is at most one over one minus gamma times larger
[2237.40s -> 2239.56s]  in terms of the infinity norm.
[2239.56s -> 2241.24s]  By the way, we see a lot of these one over one
[2241.24s -> 2243.20s]  minus gamma terms flying around,
[2243.20s -> 2245.72s]  just to make it clear where these come from.
[2246.60s -> 2249.92s]  Usually when you have sums of discounted rewards,
[2249.92s -> 2251.48s]  you're gonna have sums that look like this.
[2251.48s -> 2253.80s]  You're gonna have a sum from t equals zero to infinity
[2253.80s -> 2257.16s]  of gamma to the t times some number c.
[2257.16s -> 2259.88s]  I mean, usually the c will depend on t,
[2259.88s -> 2262.64s]  but typically it'll be bounded by some quantity,
[2262.64s -> 2264.72s]  like it'll be bounded by the largest reward.
[2264.72s -> 2266.40s]  So you'll often end up with bounds
[2266.40s -> 2268.20s]  that have terms that look like a sum over
[2268.20s -> 2271.92s]  from t equals zero to infinity of gamma to the t times c.
[2271.92s -> 2275.52s]  And that's a c times a geometric series.
[2275.52s -> 2278.00s]  And that's equal to c over one minus gamma.
[2278.00s -> 2279.64s]  So when you have a geometric series like that,
[2279.64s -> 2281.72s]  it ends up equaling one over one minus gamma.
[2281.72s -> 2284.36s]  That's just kind of a standard math result.
[2284.36s -> 2286.36s]  And that's where all these one over one
[2286.36s -> 2288.40s]  minus gamma terms come from.
[2288.40s -> 2289.84s]  One way to interpret it is that
[2289.84s -> 2291.76s]  this is a kind of horizon term.
[2291.76s -> 2295.68s]  So if we had a finite horizon problem and gamma was one,
[2295.68s -> 2296.92s]  then the multiplier in front of c
[2296.92s -> 2298.84s]  would just be the value of the horizon.
[2298.84s -> 2301.20s]  So usually we think of one over one minus gamma
[2301.20s -> 2302.76s]  as kind of the effective horizon
[2302.76s -> 2304.52s]  of an infinite horizon problem.
[2304.80s -> 2307.00s]  How far out you go before you basically
[2307.00s -> 2308.92s]  stop seeing the effect of those rewards.
[2308.92s -> 2310.04s]  So that's why we end up with a lot of
[2310.04s -> 2312.16s]  one over one minus gamma terms flying around.
[2312.16s -> 2314.56s]  And whenever you see a one over one minus gamma term,
[2314.56s -> 2315.80s]  think horizon.
[2317.76s -> 2321.20s]  Okay, so let's go through the proof of this lemma.
[2321.20s -> 2322.92s]  We're gonna use w as shorthand
[2322.92s -> 2325.72s]  for i minus gamma p pi inverse v,
[2325.72s -> 2327.68s]  just so that our equations are short.
[2329.24s -> 2334.24s]  And that means that the infinity norm
[2334.32s -> 2338.88s]  of v is gonna be equal to the infinity norm
[2338.88s -> 2341.60s]  of i minus gamma p pi inverse times w.
[2350.32s -> 2352.44s]  Sorry that there's a little typo here
[2352.44s -> 2355.56s]  that should actually be i minus gamma p pi times w
[2355.56s -> 2356.64s]  without the inverse.
[2356.64s -> 2360.56s]  So this inverse here, this is a little mistake.
[2361.24s -> 2364.60s]  So this is greater than or equal to,
[2365.52s -> 2367.44s]  by the triangle inequality,
[2367.44s -> 2369.72s]  the infinity norm of i times w
[2369.72s -> 2374.72s]  minus gamma times the infinity norm of p pi times w.
[2375.16s -> 2377.08s]  That's just from triangle inequality.
[2377.08s -> 2380.24s]  So whenever we have the norm
[2380.24s -> 2382.48s]  of the difference of two vectors,
[2382.48s -> 2384.36s]  that is greater than or equal to the difference
[2384.36s -> 2386.08s]  of the norms of the two vectors.
[2386.52s -> 2391.52s]  And now what we can do is we can take
[2396.16s -> 2399.48s]  that second term p pi times w.
[2399.48s -> 2401.08s]  And we actually know that the infinity norm
[2401.08s -> 2406.00s]  of p pi times w is gonna be less than or equal
[2406.00s -> 2407.52s]  to the infinity norm of w.
[2409.20s -> 2410.16s]  Why?
[2410.16s -> 2415.16s]  Well, because p pi is a stochastic matrix.
[2416.20s -> 2420.68s]  So when you take a linear combination of some vector
[2420.68s -> 2424.24s]  with weights that are greater than or equal to zero
[2424.24s -> 2426.12s]  and sum to less than or equal to one,
[2427.38s -> 2430.76s]  then you can't increase the length of that vector.
[2430.76s -> 2434.20s]  So that's why you can take out the p pi term there.
[2434.20s -> 2436.44s]  So now you have the infinity norm
[2436.44s -> 2439.16s]  of w minus gamma times infinity norm of w.
[2439.16s -> 2442.02s]  So that's one minus gamma times infinity norm of w.
[2443.24s -> 2444.24s]  Now this is a bound, of course,
[2444.24s -> 2445.28s]  that's not necessarily equal to that.
[2445.32s -> 2447.48s]  That's just greater than or equal to that.
[2449.20s -> 2452.40s]  So if you divide everything through by one minus gamma,
[2452.40s -> 2455.88s]  then you get the infinity norm of v divided
[2455.88s -> 2457.92s]  by one minus gamma is greater than or equal
[2457.92s -> 2459.80s]  to the infinity norm of w.
[2459.80s -> 2463.80s]  And remember w is I minus gamma p pi inverse times v,
[2463.80s -> 2465.36s]  and that completes the proof.
[2466.64s -> 2468.84s]  So now putting these pieces together,
[2469.92s -> 2474.34s]  we've got our two lemmas, I minus gamma p pi inverse v
[2474.38s -> 2476.14s]  in the infinity norm is less than or equal to v
[2476.14s -> 2478.70s]  in the infinity norm over one minus gamma.
[2478.70s -> 2480.90s]  And we've got the simulation lemma.
[2480.90s -> 2485.26s]  So now what we'll do is we'll plug in p minus p hat v pi
[2485.26s -> 2488.82s]  from the simulation lemma into this second lemma
[2488.82s -> 2489.70s]  in place of v.
[2490.82s -> 2495.82s]  And that will tell us that the infinity norm
[2496.36s -> 2498.44s]  of q pi minus q hat pi,
[2499.62s -> 2501.02s]  that the infinity norm of the left hand side
[2501.02s -> 2504.22s]  of the simulation lemma is of course equal
[2504.98s -> 2505.82s]  to the infinity norm of the right hand side,
[2505.82s -> 2506.66s]  because if the left hand side is equal
[2506.66s -> 2507.48s]  to the right hand side,
[2507.48s -> 2509.34s]  then their infinity norms are equal.
[2509.34s -> 2513.90s]  But then if we apply this top left equation
[2515.34s -> 2519.30s]  using p minus p hat v pi as little v,
[2519.30s -> 2522.82s]  then this is less than or equal to gamma
[2522.82s -> 2523.90s]  that comes from the fact the whole thing
[2523.90s -> 2525.98s]  is multiplied by gamma over one minus gamma
[2525.98s -> 2527.84s]  that comes from the top left equation,
[2527.84s -> 2532.02s]  times p minus p hat times v pi in the infinity norm.
[2532.22s -> 2535.84s]  So now we've related q pi to q hat pi
[2535.84s -> 2538.46s]  to the difference between p and p hat.
[2538.46s -> 2540.90s]  But the difference is weighted by this v pi thing.
[2542.18s -> 2543.42s]  So what we can do is we can say,
[2543.42s -> 2548.42s]  well, if you have some kind of matrix times a vector,
[2551.18s -> 2553.10s]  their product is gonna be less than or equal
[2553.10s -> 2555.52s]  to the product of the largest entries.
[2556.70s -> 2558.54s]  So you could say that this is less than or equal
[2558.54s -> 2562.90s]  to gamma over one minus gamma of the max over SA.
[2562.90s -> 2567.38s]  So that's taking a max over the rows of the matrix
[2568.82s -> 2571.18s]  of the one norm between the difference
[2571.18s -> 2575.30s]  between the entries times the largest possible entry
[2575.30s -> 2576.30s]  in v pi.
[2576.30s -> 2578.36s]  This is a fairly crude bound, right?
[2578.36s -> 2580.90s]  This is gonna be pretty loose, but it is a bound.
[2584.02s -> 2585.56s]  And now we've turned this into a form
[2585.56s -> 2588.50s]  that uses quantities that we can actually get
[2589.42s -> 2590.62s]  from our concentration inequalities from before.
[2590.62s -> 2592.82s]  So if you remember our concentration inequality
[2592.82s -> 2596.98s]  was basically analyzing the max over SA
[2596.98s -> 2599.54s]  of the one norm, the total variation divergence
[2599.54s -> 2600.64s]  between p and p hat.
[2602.86s -> 2605.98s]  Can we bound v pi, the infinity norm of v pi?
[2605.98s -> 2608.30s]  Well, basically, yes, because v pi
[2608.30s -> 2610.48s]  is the sum of over rewards.
[2610.48s -> 2613.26s]  And if you have a sum from t equals zero to infinity
[2613.26s -> 2616.30s]  of gamma to the t times some reward rt,
[2616.30s -> 2617.96s]  well, let's just replace all the rewards
[2618.48s -> 2621.20s]  with the largest possible reward, let's call it r max.
[2621.20s -> 2624.00s]  And then we can use the same geometric series formula
[2624.00s -> 2627.46s]  and get a bound of one over one minus gamma times r max.
[2627.46s -> 2630.08s]  So values are always bounded by one over one minus gamma
[2630.08s -> 2631.12s]  times r max.
[2632.20s -> 2634.52s]  And we'll assume that r max is one,
[2634.52s -> 2636.16s]  it's actually fairly standard in this kind of analysis,
[2636.16s -> 2638.58s]  just assume that your rewards are between zero and one.
[2638.58s -> 2640.48s]  The reason that we do that is because,
[2640.48s -> 2642.68s]  well, rewards are invariant to additive
[2642.68s -> 2644.04s]  and multiplicative factors.
[2644.04s -> 2646.20s]  So you can assume some range on your reward,
[2646.20s -> 2647.68s]  pretty much without loss of generalization,
[2648.24s -> 2649.84s]  as long as your rewards are always finite.
[2650.76s -> 2653.40s]  So that allows us to get rid of this v pi infinity term.
[2653.40s -> 2656.24s]  And then we're left with this equation,
[2656.24s -> 2660.40s]  the q pi minus q hat pi is less than or equal to gamma
[2660.40s -> 2662.48s]  over one minus gamma squared
[2662.48s -> 2667.14s]  times the total variation divergence on p maximized over sa.
[2669.80s -> 2673.68s]  So now we can use that concentration equality from before,
[2673.68s -> 2676.48s]  which allows us to relate the total variation divergence.
[2678.20s -> 2679.60s]  The constant is actually gonna be different,
[2679.60s -> 2681.42s]  it's gonna be c two instead of just c.
[2681.42s -> 2683.40s]  The reason the constant changes is because
[2683.40s -> 2685.48s]  we're taking a max over all the states in action,
[2685.48s -> 2687.96s]  so that requires us to apply a union bound.
[2687.96s -> 2690.84s]  Remember that these are all things that are happening
[2690.84s -> 2692.60s]  with probability one minus delta.
[2692.60s -> 2694.42s]  So if you have sa different events,
[2694.42s -> 2696.58s]  each with probability one minus delta,
[2696.58s -> 2697.76s]  then if you wanna bound the max,
[2697.76s -> 2699.32s]  you need to use the union bound.
[2699.32s -> 2701.08s]  But that doesn't actually change any of the values
[2701.08s -> 2703.56s]  we care about, it mostly just changes the constants.
[2704.48s -> 2708.28s]  So this is the final result that we're left with.
[2708.28s -> 2710.04s]  And now remember what I said at the very beginning,
[2710.04s -> 2712.28s]  the purpose of doing all this analysis
[2712.28s -> 2715.68s]  is not to prove that anything will work super well,
[2715.68s -> 2718.58s]  but it's to understand how error will be affected
[2718.58s -> 2720.52s]  by various parameters of the problem.
[2721.52s -> 2722.90s]  So we can see that, you know,
[2722.90s -> 2726.20s]  some parts of the analysis are quite appealing.
[2726.20s -> 2728.52s]  For example, more samples leads to lower error,
[2728.52s -> 2732.60s]  and in particular, the values concentrate at a rate of
[2732.60s -> 2734.16s]  one over square root of n,
[2734.16s -> 2735.82s]  which is actually the same as in supervised learning.
[2735.82s -> 2738.92s]  So the effect of samples is the same.
[2740.82s -> 2742.56s]  But what is interesting is that the error grows
[2742.56s -> 2745.38s]  quadratically in one over one minus gamma.
[2745.38s -> 2746.68s]  So the gamma term in the numerator,
[2746.68s -> 2747.52s]  we don't really care about
[2747.52s -> 2748.64s]  because that's gonna be close to one,
[2748.64s -> 2751.40s]  but the denominator would care about a great deal.
[2751.40s -> 2753.12s]  And the important thing about this bound
[2753.12s -> 2754.96s]  is that the denominator is squared.
[2755.92s -> 2757.48s]  So you can think of it as error growing
[2757.48s -> 2759.16s]  quadratically in the horizon.
[2759.16s -> 2761.24s]  We've seen error growing quadratically in horizons before,
[2761.24s -> 2763.28s]  so that's not actually a new thing to us.
[2764.58s -> 2767.30s]  So basically what this tells us is that each backup
[2767.30s -> 2770.12s]  over our time horizon accumulates error,
[2770.12s -> 2772.56s]  and effectively accumulates it quadratically.
[2772.56s -> 2775.44s]  Now there are some algorithms and some fitting strategies
[2775.44s -> 2776.84s]  for which the error is not quadratic.
[2776.84s -> 2778.40s]  So it's not always quadratic,
[2778.40s -> 2780.96s]  but it is for this naive scheme that we analyzed.
[2782.48s -> 2785.24s]  All right, so that hopefully gives you a flavor
[2785.24s -> 2787.08s]  of how this analysis works.
[2787.12s -> 2791.68s]  Now, a few relatively simple implications of this analysis.
[2791.68s -> 2794.20s]  We showed that Q pi minus Q hat pi
[2794.20s -> 2796.52s]  is less than or equal to epsilon.
[2796.52s -> 2798.56s]  And epsilon is given by this equation.
[2799.48s -> 2803.16s]  What about the difference between Q star and Q hat star?
[2803.16s -> 2804.24s]  Now this is a little different, right?
[2804.24s -> 2806.54s]  Because in Q pi minus Q hat pi,
[2806.54s -> 2808.16s]  both Q function for the same policy,
[2808.16s -> 2810.00s]  but evaluated using different models.
[2810.00s -> 2811.58s]  Now we're asking what is the difference
[2811.58s -> 2815.16s]  between the optimal Q function under P
[2815.16s -> 2817.60s]  versus the optimal Q function under P hat?
[2817.60s -> 2820.52s]  And those will correspond to different policies.
[2820.52s -> 2825.04s]  So there's a really useful little identity
[2825.04s -> 2826.26s]  that we're gonna use.
[2826.26s -> 2828.60s]  If you'd have two functions, F and G,
[2828.60s -> 2830.30s]  and you wanna take the absolute value,
[2830.30s -> 2832.60s]  or in general, any kind of norm of the difference
[2832.60s -> 2833.88s]  between their supremums,
[2833.88s -> 2836.32s]  between the largest values they take on,
[2836.32s -> 2838.44s]  that's gonna be less than or equal to the supremum
[2838.44s -> 2840.08s]  of the difference.
[2840.08s -> 2840.92s]  Kind of makes sense, right?
[2840.92s -> 2844.06s]  Because if you're maximizing F and G separately,
[2844.10s -> 2846.70s]  that difference is gonna be only smaller
[2846.70s -> 2849.30s]  than if you're just directly maximizing their difference.
[2850.70s -> 2854.48s]  Now, the thing about optimal Q functions
[2854.48s -> 2856.16s]  is that they are, of course,
[2856.16s -> 2859.90s]  the supremum of Q pi over all the policies, right?
[2859.90s -> 2861.30s]  That's what an optimal Q function is.
[2861.30s -> 2862.74s]  It's the Q function of the policy
[2862.74s -> 2864.76s]  that has the largest Q values.
[2864.76s -> 2868.62s]  So you can replace Q star with a supremum over pi of Q pi
[2868.62s -> 2870.82s]  and you can replace Q hat star
[2870.82s -> 2873.08s]  with a supremum over pi of Q hat pi.
[2875.06s -> 2878.14s]  And then we apply this convenient identity
[2878.14s -> 2880.20s]  and we see that this is less than or equal
[2880.20s -> 2881.46s]  to the supremum over pi
[2881.46s -> 2884.26s]  of the difference of the Q functions.
[2884.26s -> 2887.14s]  But we know from our main result
[2887.14s -> 2890.18s]  that Q pi minus Q hat pi is less than or equal
[2890.18s -> 2892.62s]  to epsilon for all policies pi,
[2892.62s -> 2893.50s]  which means that this whole thing
[2893.50s -> 2895.78s]  is also less than or equal to epsilon.
[2895.78s -> 2897.20s]  Okay, so that's pretty convenient.
[2897.20s -> 2898.70s]  We just directly took our result
[2898.70s -> 2901.86s]  and we used it to relate with the optimal Q functions.
[2902.86s -> 2904.70s]  But this doesn't actually tell us
[2904.70s -> 2906.78s]  what the performance of the policy
[2906.78s -> 2911.38s]  that we find under P hat will be in the true MDP.
[2911.38s -> 2912.58s]  So that's the other question.
[2912.58s -> 2915.66s]  What is the difference between Q star
[2915.66s -> 2920.66s]  and the true Q function corresponding to pi hat star,
[2920.72s -> 2923.62s]  corresponding to the policy that we get
[2923.62s -> 2926.86s]  by doing the argmax action under Q hat star?
[2927.30s -> 2932.30s]  Here, the analysis is a little bit more involved.
[2932.90s -> 2934.62s]  So what we're going to do is we're going
[2934.62s -> 2937.60s]  to take this difference and we're gonna subtract
[2937.60s -> 2940.58s]  and add Q hat pi hat star.
[2940.58s -> 2943.50s]  So we can always insert minus X plus X
[2943.50s -> 2945.02s]  because that's equal to zero.
[2945.02s -> 2949.02s]  So we're gonna put in Q star minus Q hat pi hat star
[2949.02s -> 2953.38s]  plus Q hat pi hat star minus Q pi hat star.
[2954.38s -> 2955.42s]  And then we'll break these up
[2955.42s -> 2956.68s]  using the triangle inequality.
[2957.36s -> 2959.24s]  So this is less than or equal to Q star
[2959.24s -> 2964.24s]  minus Q hat pi hat star plus Q pi hat star
[2965.96s -> 2967.88s]  minus Q hat pi hat star.
[2967.88s -> 2971.16s]  So the second term in this basically is the difference
[2971.16s -> 2972.00s]  between the true Q function
[2972.00s -> 2974.98s]  and the learned Q function for the same policy.
[2974.98s -> 2977.84s]  So we know that that is bounded by epsilon.
[2979.40s -> 2983.40s]  The first term, Q hat pi hat star is just Q hat star.
[2983.40s -> 2984.96s]  That is the Q function corresponding
[2984.96s -> 2987.36s]  to the best policy under our learned model.
[2989.28s -> 2990.80s]  And then in the second term, they're the same policy.
[2990.80s -> 2993.20s]  So that means that we can use the top left result
[2993.20s -> 2995.66s]  for the second term, and we can use the Q star
[2995.66s -> 2998.40s]  minus Q hat star result for the first term.
[2998.40s -> 3000.08s]  And both of them are bounded by epsilon.
[3000.08s -> 3001.04s]  So that means that the whole thing
[3001.04s -> 3004.58s]  is bounded by two times epsilon, okay?
[3004.58s -> 3006.04s]  So that's a pretty straightforward way
[3006.04s -> 3007.48s]  to complete these results.
