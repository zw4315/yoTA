# Detected language: en (p=1.00)

[0.00s -> 7.78s]  In the next part of today's lecture, I want to take things up a level and discuss a little
[7.78s -> 12.26s]  bit of philosophy about different perspectives we can take about what D power L actually
[12.26s -> 20.78s]  is, and I think that's something that has been a little bit implicit over this course,
[20.78s -> 25.64s]  but that I think I should make explicit, is that reinforcement learning really can
[25.64s -> 31.08s]  be thought of as a few very different things, and depending on how you view reinforcement learning,
[31.08s -> 36.16s]  some methods are more relevant to you than others. So the three perspectives I'm going to discuss,
[36.16s -> 39.68s]  although I'm sure there's more than this, are the perspective that reinforcement learning is
[39.68s -> 45.12s]  at its core really an engineering tool, the perspective that reinforcement learning is a
[45.12s -> 51.84s]  way for AI agents to discover behaviors in the real world, and then a third perspective,
[51.84s -> 57.40s]  which is my favorite, but is perhaps a little weird, is that reinforcement learning is truly
[57.40s -> 61.96s]  the most universal and fundamental learning framework that subsumes all others, and of course
[61.96s -> 64.92s]  that's my favorite because I am a reinforcement learning researcher, so I'm going to take the
[64.92s -> 69.60s]  most RL-centric perspective, but I think the other two perspectives are really important to
[69.60s -> 74.44s]  discuss because they might actually be much more relevant perhaps for you if you view
[74.44s -> 78.80s]  reinforcement learning more as a tool to accomplish some goal that you have. So let's
[78.80s -> 84.12s]  start with the first one, reinforcement learning as an engineering tool. In the beginning of this
[84.12s -> 90.72s]  class, I talked a lot about somewhat lofty philosophical ideals about how we want universal
[90.72s -> 95.48s]  learning methods that can acquire the kind of general intelligence that we associate with
[95.48s -> 100.36s]  humans and animals, but let's forget all that for a second and think about engineering problems,
[100.36s -> 106.68s]  like, you know, we think that RL is a model of agents interacting with the world in the
[106.68s -> 110.64s]  same way that animals learn through reward and punishment, perhaps our agents can learn through
[110.64s -> 114.40s]  reward and punishment and have these very naturalistic real-world learning processes,
[114.40s -> 118.16s]  and there's elements of psychology in there, and elements of neuroscience, and elements of CS,
[118.16s -> 125.36s]  all tangled together, and that's cool, but how about some very pragmatic sort of
[125.36s -> 128.72s]  bread-and-butter engineering problems, like let's say you want to fly a rocket, and you
[128.72s -> 134.20s]  want that rocket to fly really well on a really optimal trajectory. Now traditionally this is
[134.20s -> 138.92s]  not the kind of problem that we associate with reinforcement learning. We associate this problem
[138.92s -> 142.48s]  with, you know, difficult calculus, right, like you would write down a bunch of complicated
[142.48s -> 147.52s]  equations that describe the physics of your rocket, and you would solve those equations,
[147.52s -> 150.44s]  you would get some solution for where the rocket is at every point in time, you would
[150.44s -> 153.48s]  linearize it, you would do all sorts of stuff that you learn about in linear systems
[153.48s -> 157.84s]  and control theory, and out comes a feedback controller that you would use to fly that
[157.84s -> 161.80s]  rocket, and that's how we went to the moon, and it's great, and it's unrelated to RL.
[162.80s -> 169.28s]  However, I would posit that one perspective on RL is that it offers us simply another
[169.28s -> 175.56s]  tool for doing this. If we view traditional control problems as simply the problem of
[175.56s -> 181.28s]  inverting physics, RL simply offers us another tool in the toolkit for doing exactly that.
[181.28s -> 184.96s]  Now what do I mean by inverting physics? This process for flying the rocket that
[184.96s -> 191.08s]  I described, the way it works is somebody understood the physical processes of how the
[191.08s -> 196.00s]  system evolves through time, wrote down equations that describe that physical process,
[196.00s -> 201.76s]  and then, using those equations, derived a control law, meaning an equation that describes
[201.76s -> 207.92s]  how the rocket should actuate and throttle its motors to reach a desired configuration.
[207.92s -> 211.56s]  So you write down physics, and then you essentially invert that physical process
[211.56s -> 217.20s]  to determine the controls that lead to the desired boundary condition or outcome.
[217.20s -> 224.20s]  This is extremely similar to what we do when we simulate something, right?
[224.20s -> 230.48s]  So typically when we say equations of motion, we're imagining something relatively more
[230.48s -> 233.12s]  self-contained where we can write down the equations on a piece of paper and maybe
[233.12s -> 236.80s]  even solve them, but a simulator is essentially just a really fancy version of that, where
[236.80s -> 241.08s]  we wrote down a bunch of equations, and we can numerically integrate those equations
[241.08s -> 246.16s]  to figure out how some complex system will evolve in time, how the stress and strain
[246.16s -> 250.04s]  on the airplane wings will affect its flight through the air.
[250.04s -> 255.16s]  So if we run reinforcement learning in a simulator, essentially what we're doing is
[255.16s -> 259.64s]  we are using machine learning to invert our physical model.
[259.64s -> 262.92s]  So instead of interacting with the world, let's say that we're interacting with our
[262.92s -> 269.40s]  own understanding of physics. This provides a very powerful engineering tool that is not
[269.40s -> 274.60s]  that different from the conventional engineering approach. So before, we would characterize
[274.64s -> 281.60s]  our system by writing down equations of motion. We would treat that as a kind of simulator,
[281.60s -> 285.76s]  but then we would derive the equations on paper for how to control the system given
[285.76s -> 292.32s]  those equations. Now, we would characterize the system by implementing code for a simulator,
[292.32s -> 296.84s]  running that simulator, and then using RL to figure out the control law.
[296.84s -> 300.32s]  The RL is just replacing the more manual process we had before where we would do a
[300.36s -> 304.20s]  bunch of calculus to figure out a controller for that rocket.
[304.20s -> 308.48s]  So the main role here that RL plays is essentially a more powerful inversion engine,
[308.48s -> 312.20s]  a way to take a simulator and turn it into a control law, which we would previously
[312.20s -> 316.60s]  do by hand. Of course, the main weakness of this perspective is that someone still
[316.60s -> 320.72s]  needs to characterize a system, so viewed in this way, there's hardly even a learning
[320.72s -> 324.68s]  process. It's really more of an optimization tool. But it's a really, really powerful
[324.68s -> 330.08s]  optimization tool, and this really powerful optimization tool has led to tremendous advances
[330.36s -> 335.68s]  for things like, for example, robotic locomotion, where we have quadrupedal robots that can
[335.68s -> 341.40s]  walk very effectively on rough terrain by leveraging extensive simulation. And
[341.40s -> 347.60s]  I think that, in general, we will see a lot more progress going forward as this
[347.60s -> 353.08s]  tool of reinforcement learning is applied to invert more and more simulators for various
[353.08s -> 358.04s]  control problems. And as RL methods get more reliable and more capable, perhaps in
[358.04s -> 363.44s]  the long run, if we want to control any system, like an airplane, a vehicle, or a
[363.44s -> 367.12s]  walking robot, if it's a system that we understand well enough to characterize
[367.12s -> 371.24s]  and simulate it, RL would be the standard go-to tool to build controllers for it.
[373.64s -> 379.08s]  Now, I should admit that this perspective is in some ways very different from the
[379.40s -> 384.52s]  psychology roots of RL that deal with it more as a model of learning in the real
[384.52s -> 391.96s]  world. This is much more of a computational process. And this, the kind of direction
[391.96s -> 396.68s]  this leads to is developing faster simulators, developing algorithms that can make use of
[396.68s -> 402.36s]  extensive simulation. A very different perspective we could take is reinforcement learning as a
[402.36s -> 410.88s]  tool for learning in the real world, and that leads to some other conclusions. So one of
[410.92s -> 417.28s]  the things that to me is a very motivating perspective for studying RL in the real world
[417.28s -> 423.80s]  is something called Moravik's paradox. So just to set this up a little bit, in 1996,
[423.80s -> 428.12s]  there was a major AI milestone. This was the first time that a computer had defeated
[428.12s -> 432.04s]  the world champion at chess. This was a computer called Deep Blue, and it defeated
[432.04s -> 438.92s]  Gare Gasparov at chess. And then 20 years later, there was another major AI milestone
[438.92s -> 443.24s]  where a computer, for the first time, defeated a human champion at the game of Go. Now, the game
[443.24s -> 446.92s]  of Go turned out to be more complex for AI than the game of chess because the kinds of tree
[446.92s -> 452.36s]  search style methods that could master chess really couldn't handle Go, but RL methods could.
[453.08s -> 458.44s]  So that's great. But something we can notice in both of these pictures from both of these
[458.44s -> 463.00s]  matches separated by 20 years is that in both cases, there's no robot playing the game.
[463.00s -> 466.28s]  So in both cases, there's this other person sitting on the other side of the board
[466.28s -> 473.80s]  from the human champion, and that person seems to be moving the pieces. So where's the computer?
[473.80s -> 477.96s]  Well, the computer is telling them how to move the pieces, so the human is essentially a human
[477.96s -> 482.84s]  robot. Like, usually we would think of a human controlling a robot to tell the operation. Here,
[482.84s -> 488.04s]  it's a computer controlling a human, which means that the human is just used for the
[488.04s -> 492.44s]  ability to move their body. Why is that? Why couldn't we get a computer to actually move the
[492.44s -> 497.56s]  pieces even when the computer could beat the human champion of the game? Well, this epitomizes
[497.56s -> 502.36s]  something called Moravec's paradox. The original statement of Moravec's paradox written here
[502.92s -> 507.40s]  is, we are all prodigious Olympians in perceptual and motor areas, so good that we
[507.40s -> 511.48s]  make the difficult look easy. Abstract thought, though, is a new trick, perhaps less than
[511.48s -> 515.96s]  100,000 years old. We have not yet mastered it. It is not all that intrinsically difficult,
[515.96s -> 521.88s]  it just seems so when we do it. Let's unpack this statement. What Hans Moravec is saying
[521.88s -> 527.08s]  here is that abstract thought of the sort that you need to be a master at chess
[528.28s -> 533.80s]  might not actually be all that hard, we're just not very good at it. On the other hand, we're
[533.80s -> 539.32s]  extremely good at moving our bodies and perceiving the world. Why? Well, because we have to be. If
[539.32s -> 543.40s]  we weren't good at moving our bodies and perceiving the world, we'd be dead, and evolution
[543.40s -> 547.24s]  would get rid of us and replace us with other creatures that are better at moving their
[547.24s -> 552.60s]  bodies and perceiving the world. So perhaps moving our bodies and perceiving the world
[552.60s -> 556.92s]  is much, much harder than beating the world champion at chess, it's just that biology has
[556.92s -> 561.88s]  made us all extremely good at it, so it feels effortless. This was restated more succinctly by
[561.88s -> 566.20s]  Steven Pinker more recently, the main lesson of 35 years of AI research is that the hard
[566.20s -> 570.04s]  problems are easy and the easy problems are hard. The mental abilities of a four-year-old
[570.04s -> 573.40s]  that we take for granted, recognizing a face, lifting a pencil, walking across a room,
[573.40s -> 578.28s]  answering a question, in fact solves some of the hardest engineering problems ever conceived.
[580.52s -> 589.16s]  So what this means is that getting those physical capabilities might actually be the harder end of
[589.16s -> 594.68s]  the AI problem. Now, Morvick's paradox might seem like a statement about AI, in this, like,
[594.68s -> 597.56s]  literally it's saying this is hard for AI to do, this is hard for computers to do,
[598.44s -> 603.40s]  but it's actually a statement about the physical universe. So it's saying that in our physical
[603.40s -> 608.84s]  universe, motor control and perception are difficult. You could imagine other universes
[608.84s -> 612.84s]  where motor control and perception are less difficult. For example, if you're playing chess
[612.84s -> 617.64s]  or flying a rocket, there's really no perception challenge and no motor control challenge. In chess,
[617.64s -> 623.88s]  an action is a command to move a piece to another location. Dropping the piece from the
[623.96s -> 628.28s]  board is not within the rules of the game of chess, that's not part of the chess world.
[628.84s -> 633.16s]  So that is an easy universe, at least with respect to Morvick's paradox. That's not to
[633.16s -> 637.16s]  say that playing chess is easy, it's just that motor control and perception in that universe
[637.16s -> 641.40s]  are easy. Hard universes are the ones that we inhabit, that are messy, that have physics,
[641.40s -> 645.48s]  that have perception, that have lots and lots of real world diversity and variability.
[646.84s -> 650.92s]  And perhaps these are the things that we have to learn in a way that is analogous to how
[650.92s -> 654.52s]  people learn them. Perhaps these are the things that we have to actually learn through experience,
[655.40s -> 660.12s]  because these are difficult things for us to write down and characterize through simple
[660.12s -> 667.48s]  rules and equations. And that's really a motivation for real world learning. To give
[667.48s -> 671.88s]  maybe a more realistic example of this, let's take a hard engineering problem. Let's say
[671.88s -> 677.00s]  the problem of taking an oil tanker and getting it to sail from one end of the world to another.
[677.96s -> 683.24s]  Now the abstract thought part of this problem is like plodding a route through the oceans,
[683.24s -> 686.04s]  and that's actually something that is very easy for computers to do today. You don't even need
[686.04s -> 691.88s]  machine learning for it, like planning a route using GPS and basic search algorithms,
[691.88s -> 696.04s]  quite straightforward. But if something on the oil tanker breaks and someone has to go down to the
[696.04s -> 700.52s]  engine room and fix something, that's something that our current AI systems can't do. So if you
[700.52s -> 705.08s]  have a crew on board the oil tanker, they're vital for solving the unexpected problems.
[705.88s -> 711.80s]  They are not vital for actually navigating the oil tanker. And it's really the variability,
[711.80s -> 716.28s]  the fact that you can encounter very unexpected situations that make the real world so hard.
[717.64s -> 723.00s]  So what does this all have to do with RL? Well, perhaps RL can offer us an answer to
[723.00s -> 727.88s]  the question, how do we engineer a system that can deal with the unexpected? And my claim is,
[727.88s -> 731.64s]  this question is actually at the core of Moravik's paradox, because what makes the real world
[731.64s -> 736.84s]  so hard is the variability and diversity and the fact that unexpected situations outside of
[736.84s -> 743.64s]  your training set can arise all the time. Imagine the story of Robinson Crusoe. This was a person
[743.64s -> 747.40s]  that was stranded on a desert island. And what makes that story so compelling is that
[747.40s -> 752.68s]  they, he had to figure out all of these ingenious ideas to survive on the island,
[752.68s -> 757.24s]  to build shelter, to get food and so on. Imagine an AI agent placed in that situation
[757.24s -> 762.20s]  where it has to improvise and discover solutions to problems using the resources at hand,
[762.20s -> 767.00s]  with minimal external supervision about what to do and unexpected situations that might require
[767.00s -> 772.36s]  adaptation, where it has to discover solutions autonomously. And it has to also stay alive long
[772.36s -> 775.88s]  enough to discover them. So it's not like an Atari game where you have multiple trials.
[777.08s -> 780.52s]  Humans are extremely good at this. And of course, because we have to be,
[780.52s -> 784.68s]  that's what evolution selects for. But our current AI systems are very bad at this.
[784.68s -> 791.48s]  Even the most impressive AI systems that can ingest billions of documents from the web and
[791.48s -> 796.68s]  learn to answer any question, they would not be able to survive on a desert island, right?
[796.68s -> 799.72s]  Because there you really have to deal with the unexpected. Just relying on your training data
[799.72s -> 804.60s]  is not good enough. My claim is that in principle, RL can actually do this.
[805.24s -> 809.80s]  And more or less nothing else can, although mostly by definition, because RL is the
[809.80s -> 814.04s]  framework for solving this kind of problem, right? RL is the framework for learning in the
[814.04s -> 818.28s]  moment, for going to that world, experiencing what happens, getting feedback from things that
[818.28s -> 825.72s]  are going well or going poorly, and adapting. Now, that said, we rarely study the challenges
[825.72s -> 831.48s]  associated with these kinds of situations in RL research. So, easy universes might be
[831.48s -> 835.16s]  universes that look like those control problems from before, where success is
[835.16s -> 839.64s]  equated to getting high reward. There's a closed world with known rules like the game of Go,
[840.20s -> 844.60s]  lots of simulation, and the main question is really can RL optimize really well?
[844.60s -> 848.60s]  The hard universes are the ones where success is basically equivalent to survival,
[848.60s -> 853.24s]  basically doing well enough and you either survive or you don't. It's open world where
[853.24s -> 856.92s]  everything has to come from data, so there's no prior knowledge baked into a simulator necessarily.
[857.80s -> 860.76s]  And the main question is really can you generalize and adapt? Can you handle the
[860.76s -> 865.00s]  variability and unpredictability of the environment? Learning from reward feedback in principle
[865.00s -> 869.08s]  should allow you to do that, but it does introduce lots of challenges that are outside
[869.08s -> 874.36s]  of the kind of RL benchmarks that we typically study. So, RL should be really good in these hard
[874.36s -> 877.80s]  universes, but there are a bunch of questions that come up in the real world that I kind of
[877.80s -> 882.60s]  want to give you a taste of. So, first, how do we tell our RL agents what we want them to
[882.60s -> 886.20s]  do? The real world doesn't necessarily have a score, and if your feedback is like did you
[886.20s -> 891.56s]  survive or not, that feedback is too delayed, so we need more proximate supervision. How do we
[891.56s -> 895.72s]  learn fully autonomously in continual environments? So, the real world is not episodic like an
[895.72s -> 900.92s]  Atari game. It can't just reset the world and try again. How do we remain robust as the environment
[900.92s -> 905.80s]  changes around us, and what's the right way to generalize using experience and prior data?
[905.80s -> 908.68s]  And also, what's the right way to bootstrap exploration of prior experience?
[909.32s -> 913.72s]  So, since this is the last lecture, I'm going to kind of indulge myself a little bit
[913.72s -> 919.88s]  and tell you guys about some research from my lab and from colleagues that I've worked with
[919.88s -> 922.92s]  that maybe touches on some of these problems, just to give you a taste for what these
[922.92s -> 929.32s]  problems really look like. But I will say, I'll give lots of examples that have to do with robots.
[929.88s -> 933.72s]  This challenge is not just about robots, though. Robots are just the most natural for us to think
[933.72s -> 937.64s]  about, because they're embodied like we are, so it's very intuitive for us to imagine what is
[937.64s -> 941.56s]  hard or easy for a robot. But the same kinds of problems I think apply to any system that
[941.56s -> 945.88s]  interacts with the real world, whether it's managing inventory or writing medical prescriptions,
[945.88s -> 949.24s]  or if it's a chatbot on the internet, these are all at some level systems that are
[949.24s -> 954.36s]  interacting with real world settings just through a different medium, rather than necessarily a body
[954.36s -> 958.52s]  like ours. But we'll talk about robots because they do have a body a little bit like ours,
[958.52s -> 964.12s]  and that maybe makes them a little easier for us to empathize with. So, how about other ways
[964.12s -> 969.56s]  to communicate objectives? Well, I mentioned before a number of possibilities. One that I want to
[969.56s -> 973.16s]  zoom in on is the idea of learning from preferences. That's something that has been
[973.24s -> 980.36s]  very significant in recent years. So, this is a framework that was developed originally by Paul
[980.36s -> 985.72s]  Christiano in 2017, where instead of having a ground truth reward function, the agent actually
[985.72s -> 990.36s]  shows trials to a human and asks them to select which one they prefer better. And a human can
[990.36s -> 994.36s]  select different trials and guide the agent in this way towards performing some tasks, like doing
[994.36s -> 998.44s]  a flip in this case, for which it's very difficult to write down a reward function in closed form.
[999.40s -> 1003.80s]  So, this is an example of an algorithm that maybe uses other kinds of supervision
[1003.80s -> 1006.68s]  that are easier to get in the real world than a reward score.
[1008.20s -> 1013.32s]  How do we learn fully autonomously? Here's an example of the story about this. My former
[1013.32s -> 1017.64s]  student Anoush Nagabandi had been doing some interesting work on controlling multi-fingered
[1017.64s -> 1021.24s]  hands, and she could get multi-fingered hands, in this case with model-based RL, to do some
[1021.24s -> 1025.88s]  cool stuff like manipulate objects, write, and so on. But when it came time to do this in the real world,
[1026.68s -> 1030.28s]  in order to get the hand to actually practice the skill, she needed to build an entire
[1030.28s -> 1034.92s]  separate robotic system to reset the environment so that the hand could try again if it dropped
[1034.92s -> 1041.48s]  the objects. We could imagine that maybe there are some ideas that could make this much easier,
[1043.24s -> 1046.20s]  for example, by turning the learning problem into a multitask problem.
[1047.24s -> 1052.84s]  Here's an example. Let's say that the robot needs to make coffee. Well, if it has just one
[1052.84s -> 1056.76s]  task, which is to put the cup in the coffee machine, and it messes up and drops the cup,
[1056.76s -> 1060.12s]  then maybe a person needs to come in and put the cup back so the robot can try again.
[1060.76s -> 1065.08s]  But what if it has a second task, which is to pick up the cup? So now if it fails at task one,
[1065.08s -> 1069.24s]  it can do task two, but essentially that failure is an opportunity to practice something new,
[1069.80s -> 1073.88s]  and if it succeeds at task one, then it can have another task, which is to replace the cup,
[1073.88s -> 1079.24s]  and if it fails at that and spills the coffee, maybe that's an opportunity to clean up the
[1079.24s -> 1083.24s]  spill. So if we're learning multiple tasks at the same time, then each failure can give us
[1083.24s -> 1087.00s]  an opportunity to try to learn something new, and this is something that we could actually try.
[1087.00s -> 1091.80s]  Here's an experiment with a robotic hand that is trying to learn multiple tasks simultaneously.
[1091.80s -> 1095.72s]  Now, the tasks here are a little bit more basic than what I described. The tasks are to
[1095.72s -> 1099.72s]  move this object to the center of the bin, pick it up, reorient it in the palm,
[1099.72s -> 1103.24s]  and if it drops it, then tries picking it back up again. So there's these four tasks,
[1103.24s -> 1110.60s]  re-centering it, in-hand manipulation, lifting and flipping, and the design of the set of these
[1110.60s -> 1114.52s]  tasks is chosen such that whatever failure the robot has, there's some other task that can
[1114.52s -> 1119.16s]  practice from that failed state. And the result of this is that the robot can actually train
[1119.16s -> 1122.76s]  fully automatically, in this case for about 60 hours, to practice this task.
[1124.84s -> 1128.84s]  Now the other challenge, if we think back to that desert island example, is that we might
[1128.84s -> 1133.88s]  really want our agents to learn efficiently enough that they can actually survive, meaning that
[1133.88s -> 1138.04s]  if they experience too many failures, maybe just the structure of the real world will prevent
[1138.04s -> 1142.36s]  them from practicing anymore. So if they fail too badly, if they actually damage themselves,
[1142.36s -> 1148.28s]  maybe that's no good. So it's important to actually use prior knowledge that the agent does
[1148.28s -> 1152.68s]  have available to it. It might not know exactly what it will encounter in a particular environment,
[1152.68s -> 1157.16s]  but it can build up priors that should help it explore new settings. In the same way that
[1157.16s -> 1161.88s]  Robinson Crusoe knew something about making fire and shelter and so on, even though he also
[1161.88s -> 1168.68s]  had to discover a lot of things as he went. So here is a particular setting in which we can
[1169.24s -> 1173.96s]  provide an example of this. Let's say that we want a robot that learns to pick up objects
[1173.96s -> 1178.28s]  and put them on this little block, and the idea is that we're going to need to have it learn
[1178.28s -> 1182.84s]  new tasks, maybe interacting with objects that it's not seen before. If for each task we
[1182.84s -> 1186.68s]  explore entirely from scratch with random actions, then at the beginning the robot will do something
[1186.76s -> 1192.52s]  like this. And the thing is, if you see this video, even if you have no idea what the robot
[1192.52s -> 1196.28s]  is supposed to do, you'll immediately know that whatever it's doing here is not right. Like,
[1196.28s -> 1202.44s]  this is not a solution to any task. So here's an idea. What if the robot had lots of experience
[1202.44s -> 1206.68s]  of other tasks that it has solved before, and it uses that experience to build a kind of
[1206.68s -> 1211.48s]  behavioral prior, essentially a policy that doesn't do any task in particular, but performs
[1211.48s -> 1217.08s]  random tasks that were useful before? Perhaps that could be used to bootstrap exploration,
[1217.08s -> 1221.56s]  so that when it's placed in a new environment, what it will do is it'll essentially attempt
[1221.56s -> 1226.20s]  random tasks that could be useful. So now if you watch this video in the bottom left,
[1226.20s -> 1229.96s]  even though the robot, maybe it's supposed to do a very particular thing, maybe it's supposed
[1229.96s -> 1235.24s]  to put that yellow object on the cube, on many trials it does not do the correct task, but
[1235.24s -> 1238.60s]  it's still doing things that might be useful, and it seems like a clearly better exploration
[1238.60s -> 1242.68s]  strategy. So this notion of building up exploration strategies from prior experience
[1242.68s -> 1245.80s]  is potentially really important if you want to get real-world learning right.
[1247.08s -> 1251.72s]  So this thing works, it does lots of useful stuff, but the main point I want to make is that
[1252.60s -> 1256.12s]  just because we want to do real-world learning doesn't mean that prior knowledge doesn't exist,
[1256.12s -> 1261.40s]  it just means that prior knowledge needs to be of an appropriate type, so it's really priors,
[1261.40s -> 1264.92s]  not a simulation of a thing that you're actually going to be doing, and it needs to be
[1264.92s -> 1270.92s]  acquired perhaps from prior experience. Okay, so all this stuff seems hard, like it seems like
[1270.92s -> 1274.84s]  we're just adding even more challenges after having described other challenges before, so what's
[1274.84s -> 1280.20s]  the point? Why is this interesting? Well, I think it's really exciting to see what solutions
[1280.20s -> 1286.60s]  intelligent agents can come up with, and the most exciting solutions are the ones
[1286.60s -> 1291.56s]  that we didn't really expect, and this requires them to inhabit a world that is rich enough to
[1291.56s -> 1297.16s]  admit novel solutions, so that means that the world has to be complicated enough, and to see
[1297.16s -> 1301.32s]  interesting emergent behavior, we have to train our agents in environments that actually require
[1301.32s -> 1307.40s]  interesting emergent behavior, so perhaps if we're constrained to only use simulation, like in the
[1307.40s -> 1311.88s]  engineering approach I described before, there's sort of no room for the agent to discover things
[1311.88s -> 1314.84s]  that are too different, it'll only discover things that are within the confines of that simulated
[1314.84s -> 1319.16s]  world that we designed, so perhaps if we can build real-world learning systems, they might discover
[1319.16s -> 1323.32s]  really interesting new solutions, and they might have this adaptability and flexibility that would
[1323.32s -> 1327.88s]  address the central challenges in Morvik's paradox, one of the biggest things holding back
[1327.88s -> 1334.20s]  deployment of AI systems in the real world today. All right, there's one more perspective I want
[1334.20s -> 1340.44s]  to tell you about, which maybe is a little bit weird, but I think it's interesting to ponder, and
[1340.44s -> 1344.84s]  maybe we'll give you some ideas, and that's the idea that maybe the real power of reinforcement
[1344.84s -> 1350.84s]  learning is really as a more universal learning framework, perhaps it's not just a way for us to
[1350.84s -> 1355.08s]  learn to interact with the real world, or to learn to solve control problems, but it's a way,
[1355.08s -> 1358.36s]  but it's something that perhaps eventually will subsume all of machine learning.
[1360.44s -> 1367.56s]  So why does deep learning work at some basic, very basic level, supervised deep learning? Well at
[1367.56s -> 1371.24s]  its most basic level, deep learning works because you can take a large model trained with,
[1371.24s -> 1375.32s]  you know, lots of compute and lots of data that you spend lots of money on,
[1375.32s -> 1378.68s]  combine it with a large data set, which is typically a labeled data set,
[1378.68s -> 1383.08s]  and get something that solves interesting problems like being a good chatbot, recognizing
[1383.08s -> 1389.64s]  speech, classifying images, and so on. But if that's the formula for making machine learning
[1389.64s -> 1394.12s]  today work, then everything that we can do that allows us to use more data will be a good thing,
[1395.08s -> 1399.24s]  and increasingly the kind of recipe that we're seeing be more and more successful for
[1399.24s -> 1402.44s]  exactly this reason is one where we have a small amount of data that actually tells
[1402.44s -> 1406.92s]  the model what to do, and then a large amount of unlabeled kind of low-quality garbage data,
[1406.92s -> 1409.56s]  like for example all of the internet. This is how language models work, right?
[1409.56s -> 1412.04s]  Language models are trained on huge amounts of data from the web,
[1412.04s -> 1415.00s]  and then we might define a task by fine-tuning them on a little bit of data,
[1415.00s -> 1419.80s]  or even by few-shot prompting them. But when we're doing this kind of
[1421.40s -> 1425.64s]  unsupervised or weakly supervised training, where does the knowledge that the model
[1425.64s -> 1430.44s]  actually acquires actually come from? Well, classic unsupervised learning
[1430.44s -> 1435.00s]  is basically doing density estimation. It's modeling the distribution in the data. That's,
[1435.00s -> 1438.04s]  for example, what large language models do. When you're doing the next token prediction,
[1438.04s -> 1443.64s]  you're modeling the process that produced the data. And if you're learning, for example,
[1443.64s -> 1448.36s]  self-supervised representations from images, the distribution is the distribution of photographs
[1448.36s -> 1453.24s]  that people took. If you're learning from natural language text on the internet,
[1453.32s -> 1455.88s]  you're learning from the buttons people press on keyboards.
[1457.48s -> 1461.00s]  And as an aside, this is perhaps why prompting large language models is such an art,
[1461.00s -> 1469.16s]  because if you are learning the distribution of the data, and the data is weakly labeled garbage
[1469.16s -> 1473.48s]  data that you just scraped from everything on the web, you're learning a really weird
[1473.48s -> 1479.08s]  distribution. And perhaps a lot of the reason why it's so hard to course, for example,
[1479.08s -> 1482.44s]  language models to do what we want is because you're taking this thing that is fundamentally
[1482.44s -> 1487.56s]  trying to model a low-quality distribution, meaning all the stuff on the web, and you're
[1487.56s -> 1494.68s]  trying to coerce it to do something that is of high quality. So can there be a better way? And
[1494.68s -> 1498.12s]  what I'm going to try to argue is that RL actually gives us a better way to utilize
[1498.12s -> 1502.52s]  this low-quality data. So stepping back a bit, let's ask a very fundamental question.
[1503.16s -> 1507.80s]  Why do we need machine learning? We can step even further back and ask an even more basic
[1507.80s -> 1512.92s]  question. Why do we need a brain? Daniel Walpert, who's a neuroscientist that studies
[1512.92s -> 1517.32s]  motor control, had this to say. He said, we have a brain for one reason and one reason only,
[1517.32s -> 1520.92s]  and that's to produce adaptable and complex movements. Movement is the only way we have
[1520.92s -> 1524.68s]  of affecting the world around us. I believe that to understand movement is to understand
[1524.68s -> 1531.32s]  the whole brain. Now this is perhaps a somewhat reductionist perspective, but I think it's
[1531.32s -> 1536.36s]  a very valid one that fundamentally the value of a computational system is really determined by
[1536.36s -> 1540.84s]  its outputs. And we could apply the same logic to machine learning. We could postulate that we
[1540.84s -> 1545.16s]  need machine learning for one reason and one reason only, and that's to produce adaptable
[1545.16s -> 1550.36s]  and complex decisions. Again, it's a little bit reductionist, but I think there's a good
[1550.36s -> 1555.16s]  argument for this being true. Obviously if you're controlling a robot, the movements of that
[1555.16s -> 1559.56s]  robot are the only outputs that matter if you're steering a car, etc. But the same is actually
[1559.56s -> 1564.28s]  true if you have some other disembodied model, maybe you have an image classifier, it's
[1564.28s -> 1568.44s]  making decisions. The decision is not actually the image label, it has to do with what happens
[1568.44s -> 1573.64s]  afterwards, like do you detect somebody in a security camera? And do you make a decision
[1573.64s -> 1577.56s]  to call the police? Do you make a prediction about traffic? Well, that prediction is going to
[1577.56s -> 1581.80s]  affect where people drive, which in turn has consequences in the real world. If you're a
[1581.80s -> 1585.88s]  language model, what you say has long-term consequences across multiple episodes of
[1585.88s -> 1589.96s]  interaction. So the outputs of machine learning systems really are fundamentally decisions,
[1590.04s -> 1595.40s]  even if they're trained with supervised learning. So if machine learning systems are really all
[1595.40s -> 1600.52s]  about making decisions, perhaps reinforcement learning gives us a better way to use data
[1600.52s -> 1605.56s]  to make better decisions. Because with reinforcement learning we could say that
[1605.56s -> 1610.04s]  this kind of garbage data that we scrape from all available sources
[1610.04s -> 1613.56s]  is really telling us what could be done in the world rather than what should be done,
[1613.56s -> 1617.56s]  and then a limited amount of supervision can specify the task that the agent should perform.
[1618.44s -> 1622.52s]  So then our big data set from past interactions isn't necessarily something that we're going to
[1622.52s -> 1625.72s]  learn to copy, we're not going to learn p of x the way the language model does,
[1625.72s -> 1629.80s]  but instead we'll use it to understand the possibilities available to us, and then among
[1629.80s -> 1633.88s]  those possibilities we'll select the ones that are most optimal at accomplishing the tasks
[1633.88s -> 1639.72s]  that we want to do. So this perspective is perhaps a little bit abstract, but what it's arguing
[1639.72s -> 1644.84s]  for is using data within an RL loop, whether it's a model-based RL loop or an offline RL loop or
[1644.84s -> 1650.76s]  something else, to support a kind of a basis for decisions, and then make the most optimal
[1650.76s -> 1655.40s]  decisions given what your data tells you is possible. And if machine learning is all about
[1655.40s -> 1660.12s]  making decisions, this should be a better framework than one where we run unsupervised
[1660.12s -> 1666.76s]  learning and try to model the entire data distribution. So the trouble is that with
[1666.76s -> 1670.52s]  naive RL this is a costly process because you have to actually interact with the world to collect
[1670.52s -> 1675.64s]  this data, but supervised self-supervised learning is all about using cheap data, so perhaps offline
[1675.64s -> 1679.80s]  RL or model-based RL or something of that sort might be a better building block for this kind
[1679.80s -> 1686.12s]  of philosophy, but at a high level the recipe could be something like take large amounts of
[1686.12s -> 1692.04s]  diverse but low quality data, run some kind of RL-like process on it, whether you learn a model
[1692.04s -> 1695.56s]  or you learn a value function or something, and it should really be self-supervised at that
[1695.56s -> 1699.80s]  point, maybe human-defined skills or goal-conditioned RL or even self-supervised skill discovery,
[1700.52s -> 1706.92s]  and use that to essentially run self-supervised learning of decision making, learn what how to make
[1706.92s -> 1711.40s]  decisions that lead to particular outcomes, and then with a modest amount of supervision figure
[1711.40s -> 1715.96s]  out how to achieve the outcomes that the human actually wants. And at some level I would actually
[1715.96s -> 1719.08s]  posit that a lot of the excitement that we've seen with language models with things like
[1719.08s -> 1725.24s]  ChantGPT is actually because some version of this pipeline is starting to emerge where we're
[1725.24s -> 1732.20s]  seeing that an RL-like process for actually specifying human preferences is providing a more
[1732.20s -> 1736.68s]  useful agent, but I think there's a lot more to unlock from actually using RL as the basic building
[1736.68s -> 1743.48s]  block for the entire training process, actually running unsupervised RL training to discover
[1743.48s -> 1747.24s]  how to achieve any outcome and then specialize that to the outcome that we actually want.
[1749.00s -> 1753.16s]  Now to bring this back down to earth I'll tell you about a few experiments that maybe
[1753.16s -> 1758.44s]  get at some primitive version of this. Here's an experiment that we did at Google a couple years
[1758.44s -> 1763.80s]  back on self-supervised learning of robotic behaviors where we ran goal-conditioned RL,
[1763.80s -> 1767.56s]  so here there's no reward function, the task is defined entirely using a goal image,
[1767.56s -> 1771.96s]  it's trained using offline RL, and it works as a kind of pre-training objective. So the idea is
[1771.96s -> 1776.60s]  that we don't, it's kind of clunky to specify goals for a robot with images, but if we do
[1776.60s -> 1782.60s]  self-supervised goal-reaching RL pre-training, then we can do fine-tuning on top of that
[1782.60s -> 1786.36s]  with a task reward, and that works a lot better and is a lot faster. So this is like a little bit
[1786.36s -> 1790.68s]  like BERT but for robots, so BERT does unsupervised pre-training on text, this does
[1790.68s -> 1796.76s]  self-supervised RL pre-training on goals, and then fine-tunes rapidly with new reward signals.
[1798.52s -> 1804.76s]  We can also apply RL to language models. To my knowledge it hasn't been used as a
[1805.72s -> 1809.00s]  pre-training procedure, but as a fine-tuning procedure can work really well.
[1809.80s -> 1815.40s]  Here's a recent work that was done by Joey Hong, who's actually one of the TAs this year,
[1816.28s -> 1823.00s]  where Joey wanted to get an agent that was much more interactive. Perhaps you want a teacher agent
[1823.00s -> 1827.88s]  and you don't want it to just give you a big long response, what you want is to have it
[1827.88s -> 1832.84s]  actually clarify with you whether you understand certain concepts and then tailor the lesson
[1832.84s -> 1836.76s]  accordingly. So if the user asks can you teach me about the behavior cloning, maybe what the
[1836.76s -> 1840.04s]  agent should do is say of course I'd be happy to explain behavior cloning to start, could you
[1840.04s -> 1843.32s]  tell me if you've ever come across the terms artificial intelligence or machine learning before?
[1843.32s -> 1847.88s]  So you see that question will help the agent tailor its explanation. Of course if you ask
[1847.88s -> 1852.44s]  GPT-4 this question, it doesn't ask you lots of clarifying questions, it just gives you this
[1852.44s -> 1857.88s]  kind of wall of text reply, and that's because that's what it was trained to do. Now why is
[1857.88s -> 1861.56s]  RL a good choice for getting this kind of more interactive agent? Well the reason is that
[1861.56s -> 1867.24s]  a language model trained on lots of internet data might actually be a really good predictive
[1867.24s -> 1871.24s]  model of human behavior. So perhaps if we use something that looks more like model-based RL,
[1871.24s -> 1875.16s]  we can have a language model essentially simulate plausible human responses and then
[1875.16s -> 1879.80s]  optimize for the kinds of questions that might lead to the responses that are useful to us,
[1879.80s -> 1884.76s]  basically treat the whole thing as a POMDP and optimize for a better policy with short
[1884.76s -> 1887.56s]  questions that very quickly get us the necessary information.
[1888.36s -> 1893.32s]  So the way that we could approach this is we could actually prompt a language model to basically
[1893.32s -> 1897.16s]  provide a bunch of dialogues, essentially act as a human simulator to generate data,
[1897.96s -> 1901.00s]  and that data might not be very good, it might not actually teach humans optimally,
[1901.00s -> 1905.48s]  but it might show the RL algorithm the kinds of responses that humans might produce,
[1905.48s -> 1910.28s]  and that huge data set can then be used with RL to figure out more optimal behaviors
[1911.16s -> 1915.88s]  that could actually produce a better teaching agent. So RL with language model is great for
[1916.76s -> 1921.08s]  tasks that are easy for language models to simulate, but hard for them to perform optimally.
[1921.08s -> 1924.68s]  So then with RL, instead of emulating humans, the language models can learn to achieve desired
[1924.68s -> 1929.40s]  outcomes using their understanding of how humans might behave. So again, it's kind of
[1929.40s -> 1933.88s]  using RL as inversion, but not inversion of simulator, instead it's inversion of a language
[1933.88s -> 1941.16s]  model. And these are the kinds of dialogues that you could get. So this is the baseline with
[1941.32s -> 1947.96s]  a version of ChatGPT that was specifically asked to produce lots of clarifying questions,
[1947.96s -> 1951.48s]  so this is kind of giving it the best possible shot. And you can see it has these very verbose
[1951.48s -> 1956.28s]  responses. And this is the agent that was trained with RL, and you can see the RL agent
[1956.28s -> 1960.36s]  actually asks very short, very targeted questions. I'd be happy to explain behavior
[1960.36s -> 1963.64s]  cloning to start, could you tell me if you ever heard the term artificial intelligence or
[1963.64s -> 1966.60s]  machine learning? The person says, yes, I've heard this term, but I'm not exactly sure what
[1966.60s -> 1970.76s]  they mean. The agent says, no problem at all, let's take it step by step, and asks,
[1970.76s -> 1975.80s]  how do you use a computer or smartphone? So the point of this is that once we start
[1975.80s -> 1979.24s]  treating these problems as decision-making problems, we can get much closer to the
[1979.24s -> 1984.84s]  behaviors that we want in a much more natural way. Okay, so that's maybe a little bit of
[1986.04s -> 1993.48s]  research, and hopefully you can humor me on that point since this last lecture in my class,
[1993.48s -> 1997.16s]  but the last thing I want to end on is to bring this back to the bigger picture,
[1997.16s -> 1999.72s]  some of the bigger picture that we had actually in the very first lecture.
[2000.76s -> 2004.04s]  In the very first lecture, we talked about the possibility that learning might be the basis
[2004.04s -> 2009.56s]  of intelligence, and that if that's the case, then perhaps deep RL should be a central part
[2009.56s -> 2012.76s]  of that, because reinforcement learning allows us to reason about decision-making,
[2012.76s -> 2016.44s]  and the deep part allows our algorithms to learn complex input-output mapping,
[2016.44s -> 2020.12s]  so perhaps deep models are what allow reinforcement learning to solve complex problems
[2020.12s -> 2025.72s]  end-to-end. So if that's true, then what is it that's missing? One perspective that I
[2025.72s -> 2029.32s]  think is pretty interesting, I don't think I fully agree with this myself, but I think it's
[2029.32s -> 2033.56s]  worth mentioning, is a perspective that Professor Yann LeCun often brings up,
[2034.36s -> 2040.52s]  that has come to be known as Yann LeCun's Cape, where he talks about how at a very high level,
[2042.12s -> 2046.28s]  you can characterize the effectiveness of different learning problems by how many
[2046.28s -> 2050.04s]  bits of supervision they get from every data point. So the idea is that if you're learning
[2050.04s -> 2056.76s]  an image classifier, and you have let's say a thousand classes, log of a thousand is 10,
[2056.76s -> 2061.32s]  so you're really getting 10 bits of supervision for every data point. If you're doing unsupervised
[2061.32s -> 2064.28s]  learning, like for example predicting the next image in a video, then there are many,
[2064.28s -> 2069.16s]  many pixels, so there's a huge amount of supervision for every data point. And if
[2069.16s -> 2072.44s]  you're doing RL, then perhaps you're learning from fairly sparse rewards, so you're not getting
[2072.44s -> 2079.48s]  as much supervision. So this is an argument for a fundamentally unsupervised learning approach
[2079.48s -> 2083.80s]  to use lots of data, with reinforcement learning as a mechanism to adapt such a model to solve
[2083.80s -> 2090.20s]  a particular task. But this gets us thinking, like what is really the role of RL in this
[2090.20s -> 2095.32s]  bigger picture question of how to build intelligent systems? So Yann LeCun's Cape would
[2095.32s -> 2099.00s]  be an argument that unsupervised or self-supervised learning should be at its core,
[2099.00s -> 2101.96s]  where things like model-based learning or model-based RL should be really important,
[2102.52s -> 2105.96s]  and RL as a mechanism then specializes those systems to accomplish a goal.
[2106.60s -> 2111.80s]  But there are other perspectives. For example, maybe a lot of the learning signal should come
[2111.80s -> 2115.72s]  from imitation understanding other agents. Humans are social animals and we have culture,
[2115.72s -> 2119.32s]  and you could argue that a human that grows up without other humans around without any culture
[2119.32s -> 2124.52s]  might not actually be all that intelligent. So perhaps actually imitation is much more central
[2124.52s -> 2131.56s]  to this general vision of a general purpose AI system. But I think there's also another very
[2131.56s -> 2136.44s]  reasonable perspective that is very defensible, which is actually maybe RL is enough, because
[2136.44s -> 2140.52s]  even though in RL you're learning from a fairly sparse reward signal, that reward signal is
[2140.52s -> 2145.88s]  modulated by a very complex system dynamics. So the actual supervision for something like
[2145.88s -> 2149.88s]  a value function is much more elaborate than just predicting whether the reward is 1 or 0,
[2149.88s -> 2153.72s]  because once the dynamics interacts with that reward signal, it modulates in a very complex
[2153.72s -> 2157.00s]  ways. And you can always run multitask learning and other things that give you a lot more
[2157.00s -> 2162.28s]  supervision, even with standard RL methods. And it could be that the real answer is a
[2162.28s -> 2165.48s]  combination of the above. Maybe we need self-supervised learning methods. Perhaps those
[2165.48s -> 2169.80s]  self-supervised methods can look more like RL methods insofar as they facilitate optimal
[2169.80s -> 2174.44s]  decision-making to achieve desired outcomes. And perhaps we need elements of interaction with
[2174.44s -> 2179.24s]  imitation to get this. I don't know what the answer is. I have my own beliefs about what's
[2179.24s -> 2183.32s]  more likely, but I think the high order bit that I want to get you all thinking about
[2183.32s -> 2187.80s]  is that these questions actually matter. And if we are going to explore reinforcement learning
[2187.80s -> 2191.64s]  as a powerful general purpose tool, then it helps to think about these possibilities.
[2192.68s -> 2196.20s]  So how should we go about answering these questions? Well, we need to pick the right
[2196.20s -> 2200.20s]  problems to work on. And maybe here it's very important to ask yourself the question,
[2201.08s -> 2204.84s]  if you're choosing a research problem, does it have a chance of solving some really important
[2204.84s -> 2208.84s]  problems? So work on research problems where the upper bound on success is really high.
[2208.84s -> 2212.92s]  Take into account the bigger picture philosophy, but then try to reduce it down to concrete,
[2212.92s -> 2217.56s]  actionable things, falsifiable hypotheses where you can measure your success, but that are
[2217.56s -> 2223.16s]  plausibly on the path towards the big questions. Optimism in the face of uncertainty is a great
[2223.16s -> 2228.20s]  strategy, not just for exploring embedded problems, but also for research. Don't be afraid to change
[2228.20s -> 2232.28s]  the problem statement. Many of these challenges will be met by iterating on existing benchmarks.
[2232.28s -> 2236.20s]  Perhaps the standard assumptions of classical RL are not good enough to answer these questions
[2236.20s -> 2240.68s]  and we need to change something and that's okay. And lastly, applications matter. Sometimes
[2240.68s -> 2244.52s]  applying methods to realistic and challenging real world domains can teach us a lot about the
[2244.52s -> 2248.44s]  important things that are missing and we need to take them seriously. So don't be afraid to
[2248.44s -> 2252.28s]  work on more applied things because it might actually reveal which problems are more important
[2252.28s -> 2256.84s]  and which solutions are more promising. RL has a long history of disregarding this fact
[2256.84s -> 2262.84s]  and working on overly simplistic tasks. Don't fall into the same trap. And lastly, think big but
[2262.84s -> 2266.68s]  start small. Think about the big picture questions. Don't be afraid to be ambitious
[2266.68s -> 2271.96s]  and think about what would really allow us to develop truly powerful AI systems that accomplish
[2271.96s -> 2276.20s]  things that nobody thought possible, but at the same time start with clear actionable goals
[2276.20s -> 2278.36s]  where you can measure progress towards success.
