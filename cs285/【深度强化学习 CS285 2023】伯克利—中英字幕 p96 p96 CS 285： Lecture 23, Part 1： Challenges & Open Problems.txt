# Detected language: en (p=1.00)

[0.00s -> 5.36s]  All right, welcome to the final lecture of CS285. Today we're going to talk about
[5.36s -> 11.92s]  challenges and open problems. So first let's have a brief review of the
[11.92s -> 15.32s]  material that we covered in the course. There was a lot of things that we
[15.32s -> 18.80s]  covered and a lot of different methods, so I'm going to try to draw a
[18.80s -> 23.28s]  map to try to illustrate the different principles and how they
[23.28s -> 27.84s]  relate to one another. So at the root of it we have learning-based control.
[27.84s -> 32.24s]  So basically our aim was to cover learning-based control methods
[32.24s -> 35.12s]  interpreted very broadly, and learning-based control methods include
[35.12s -> 39.76s]  imitation learning methods, which are learning from demonstration supervision,
[39.76s -> 44.72s]  and reinforcement learning methods, which are learning from rewards.
[44.72s -> 50.28s]  Reinforcement learning methods include classic model-free RL algorithms, which
[50.28s -> 53.84s]  are things like policy gradients, value-based methods. Value-based
[53.84s -> 58.88s]  methods and policy gradients combined result in actor-critic methods. We covered
[58.88s -> 62.84s]  deep Q-learning as an example of a specific value-based method, Q-function
[62.84s -> 67.92s]  actor-critic methods like SAC, and advanced policy gradient methods like
[67.92s -> 73.76s]  TRPO and PPO. There's also model-based control, and model-based
[73.76s -> 76.88s]  control does not have to be learning-based, so those planning and
[76.88s -> 81.00s]  control methods we discussed like LQR don't by themselves necessarily have
[81.00s -> 84.60s]  anything to do with learning, but they can be combined with learning to produce
[84.60s -> 89.12s]  model-based RL methods. And in their purest form, model-based RL methods
[89.12s -> 92.24s]  that do not use a policy, that simply train a model and then plan through
[92.24s -> 95.72s]  that model, don't actually make use of all these RL concepts we discussed in
[95.72s -> 99.04s]  the model-free portion, but we can of course put them together and use
[99.04s -> 102.60s]  learned models in combination with reinforcement learning algorithms like
[102.60s -> 108.16s]  policy gradients or value-based methods to get more effective model-based
[108.16s -> 112.24s]  RL algorithms. And then there are a bunch of other concepts that kind of
[112.24s -> 116.88s]  apply longitudinally across the range of different RL methods that are sort of
[116.88s -> 120.20s]  orthogonal to the particular choice of algorithm, like for example the
[120.20s -> 125.96s]  choice of exploration strategy, use of unsupervised RL objectives like skill
[125.96s -> 130.68s]  discovery, and so on. There are also other tools that lie outside of the
[130.68s -> 133.76s]  learning-based control framework, but that are very useful, like for example
[133.76s -> 137.44s]  the tools of probabilistic inference and variational inference, which give us
[137.48s -> 141.52s]  the control-as-inference perspective on RL, and that, together with
[141.52s -> 146.28s]  imitation learning, allow us to derive things like inverse RL methods. Now this
[146.28s -> 149.56s]  doesn't fully cover every single thing we discussed, we also discussed
[149.56s -> 153.32s]  things like sequence models, pump DPs, etc., but this hopefully gives you a rough
[153.32s -> 159.96s]  overview of the particular parts of this course. But what I'd like to talk
[159.96s -> 163.60s]  about today are some of the challenges with deep reinforcement learning
[163.60s -> 165.76s]  methods, basically the things that are open problems that we have not yet
[165.76s -> 169.88s]  addressed, and then also some perspectives about how deep reinforcement
[169.88s -> 174.24s]  learning should be used. So let's start with the challenges. Now some of you
[174.24s -> 177.24s]  might already be familiar with quite a few of the challenges of the deep RL
[177.24s -> 180.64s]  from having, for example, done the homeworks in this course and
[180.64s -> 183.28s]  experienced which things are easy and which things are hard in the
[183.28s -> 187.24s]  homeworks, but let's go over them a little bit. So some of the
[187.24s -> 190.92s]  challenges in deep RL are really challenges with the core algorithms.
[190.92s -> 197.00s]  For example, stability. Does your algorithm actually converge? Do you
[197.00s -> 200.56s]  have to tune your hyperparameters very very carefully, or is the same
[200.56s -> 203.84s]  hyperparameter setting going to work across the board for a variety of
[203.84s -> 208.92s]  different problem types? Efficiency. How long does it take to converge,
[208.92s -> 212.56s]  meaning how many samples do you need, how many trials, also potentially how
[212.56s -> 216.84s]  much compute? Generalization. After your algorithm converges, does it actually
[216.84s -> 219.40s]  generalize to new problem settings, and what does that actually mean in your
[219.40s -> 222.72s]  domain? But there are also some challenges with RL methods that
[222.72s -> 226.52s]  really have to do with the assumptions of RL, and these challenges
[226.52s -> 230.36s]  become much more pronounced when we try to apply RL algorithms to
[230.36s -> 233.64s]  real world settings, and we find that certain assumptions that RL
[233.64s -> 237.24s]  algorithms make are a little difficult to satisfy for some real
[237.24s -> 241.48s]  world problems. So is RL actually the right problem formulation?
[241.48s -> 244.40s]  Perhaps you'd like to solve a learning-based control problem, but
[244.40s -> 247.52s]  some of the things that RL assumes don't fit very cleanly. For
[247.52s -> 250.44s]  example, maybe you don't have access to a ground truth reward
[250.44s -> 253.60s]  function. What is the source of supervision for your real world
[253.60s -> 256.84s]  problem? Essentially, somewhere somebody needs to supervise the
[256.84s -> 259.92s]  algorithm. Some forms of supervision have to do with telling the
[259.92s -> 263.20s]  algorithm what you want to do, like the reward. Some of it serve
[263.20s -> 265.76s]  to make the learning process easier, like, for example, access to
[265.76s -> 269.64s]  demonstrations. Some of the things you provide are a combination
[269.64s -> 273.72s]  of both. For example, providing a more well-shaped reward, a reward
[273.72s -> 277.12s]  that is not sparse, might serve to both specify what you want and
[277.12s -> 280.88s]  how you want the method to do it. So the assumptions often
[280.88s -> 284.64s]  represent major challenges. So let's start with the challenges
[284.64s -> 289.40s]  with core algorithms. One big one is stability and hyperparameter
[289.40s -> 293.00s]  tuning. Reinforcement learning algorithms, in a sense, solve a
[293.00s -> 295.20s]  significantly harder problem than supervised learning
[295.20s -> 299.08s]  methods, because they have to get their own data, they don't
[299.08s -> 302.48s]  have to, they don't get to assume the data is IID, they
[302.48s -> 305.36s]  have to optimize an objective rather than being given ground
[305.36s -> 309.60s]  truth optimal actions, and all of these additional challenges
[309.60s -> 312.88s]  mean that these methods are more sensitive to the particular
[312.88s -> 316.24s]  setting of parameters, parameters like exploration rates,
[316.24s -> 320.80s]  learning rates, and so on. Now, devising stable RL algorithms,
[320.80s -> 323.48s]  stable in the sense that small changes in hyperparameters
[323.48s -> 327.44s]  don't lead to large degradation in performance, tends to be very
[327.44s -> 331.56s]  hard. And this shows up in different ways for different
[331.56s -> 335.28s]  classes of methods. So for example, for Q-learning or
[335.28s -> 340.36s]  value-based methods, part of the problem is that hidden value
[340.36s -> 344.48s]  methods with deep network function approximation are
[344.48s -> 347.24s]  generally not contractions, and hence, in the most general
[347.24s -> 350.76s]  case, don't have guarantees of convergence. So we have a lot
[350.76s -> 352.88s]  of tricks that we learned about that can make them
[352.88s -> 356.04s]  convergent practice, but the core theoretical issue is still
[356.04s -> 359.72s]  there, and this theoretical issue shows up in a number of
[359.76s -> 362.52s]  different ways. First, it means that there are lots of
[362.52s -> 366.00s]  parameters that you need to select carefully for
[366.00s -> 367.96s]  stability, like the delay for the target network, the
[367.96s -> 370.28s]  replay buffer size, if you're going to do gradient
[370.28s -> 372.56s]  clipping, how you're going to choose your learning aid,
[372.56s -> 376.32s]  et cetera. And part of the intuition for why these
[376.32s -> 381.00s]  choices are quite sensitive is that the core algorithmic
[381.00s -> 383.60s]  framework in general might not be convergent, and we kind
[383.60s -> 387.04s]  of put these things in as fixes to make it converge.
[387.04s -> 388.80s]  Now, of course, there's quite a bit of research on trying
[388.80s -> 392.56s]  to make these algorithms more stable and easier to use,
[392.56s -> 396.12s]  and one thing I will say here is that a lot of sort of
[396.12s -> 398.68s]  bread and butter deep learning improvements do tend to
[398.68s -> 401.24s]  help. For example, using large networks tends to help
[401.24s -> 403.48s]  if done right. Using the appropriate choice of
[403.48s -> 405.84s]  normalization tends to help. Using data augmentation
[405.84s -> 408.16s]  tends to help. For those of you that are interested in
[408.16s -> 410.40s]  data augmentation, there's some very nice work in a
[410.40s -> 413.80s]  paper called DRQ, which explains how data augmentation
[413.80s -> 417.00s]  can actually greatly facilitate stability of Q-learning.
[417.00s -> 420.20s]  But there might also be some open problems here that are
[420.20s -> 424.92s]  a bit more fundamental. For example, it's still a mystery
[424.92s -> 429.76s]  to us why supervised deep learning works so well.
[429.76s -> 432.88s]  Conventional machine learning theory would hold that
[432.88s -> 436.40s]  supervised deep learning should lead to pretty
[436.40s -> 439.28s]  catastrophic overfitting, because you are using a model
[439.28s -> 442.84s]  with many more parameters than you have data points.
[442.84s -> 445.92s]  Insofar as that catastrophic overfitting does not happen
[445.96s -> 450.08s]  with classical deep learning, there must be some sort of
[450.08s -> 453.84s]  regularizing effect from the use of large neural networks
[453.84s -> 456.16s]  with stochastic gradient descent that makes this
[456.16s -> 459.96s]  problem not so severe. So there's some kind of magic
[459.96s -> 462.20s]  in a sense that makes deep learning work, and there's a
[462.20s -> 464.08s]  lot of active research trying to understand that magic.
[464.08s -> 468.40s]  Well, value-based methods are not gradient descent.
[468.40s -> 470.80s]  They work on slightly different principles, and it's
[470.80s -> 472.84s]  actually a very open question as to whether the same
[472.84s -> 475.36s]  kind of magic that makes supervised deep learning work
[475.40s -> 478.08s]  still applies to value-based methods.
[478.08s -> 480.84s]  Perhaps the regularizing effect of using large models
[480.84s -> 482.80s]  with stochastic gradient descent doesn't work the same
[482.80s -> 485.76s]  way in value-based methods. So this is very much kind
[485.76s -> 488.56s]  of at the frontier of current research and is perhaps
[488.56s -> 492.04s]  the deeper manifestation of some of these challenges.
[492.04s -> 493.08s]  So I don't have an answer here.
[493.08s -> 495.00s]  This is something that is an active area of research,
[495.00s -> 497.36s]  but it is a challenge to keep in mind.
[497.36s -> 499.52s]  Okay, what about policy gradient methods,
[499.52s -> 503.36s]  likelihood ratio reinforced, TRPO, PPO, all that sort of stuff?
[503.36s -> 506.20s]  Well, arguably these methods are somewhat better understood
[506.20s -> 509.52s]  in the sense that we do have convergent policy gradient
[509.52s -> 512.92s]  algorithms. In a sense, the story with policy gradient
[512.92s -> 515.24s]  is that it trades off a lot of the nastiness
[515.24s -> 517.92s]  in value-based methods and model-based RL
[517.92s -> 519.80s]  for much higher variance.
[519.80s -> 524.20s]  So the common theme is that all of the other RL methods
[524.20s -> 526.00s]  have bias from function approximation.
[526.00s -> 527.92s]  Policy gradient methods generally do not have bias,
[527.92s -> 528.76s]  but they do have variance.
[528.76s -> 532.56s]  Of course, once you start using value functions as
[532.56s -> 533.88s]  critics for advantage estimation,
[533.88s -> 536.12s]  you introduce the same bias right back in,
[536.12s -> 538.80s]  but in their purest form, they have high variance,
[538.80s -> 540.96s]  but no bias, which means that they are a bit easier
[540.96s -> 543.32s]  to understand, but the variance is no picnic.
[543.32s -> 544.80s]  It's still a major challenge.
[546.16s -> 548.96s]  And what that variance implies is that you might need
[548.96s -> 550.84s]  lots of samples.
[550.84s -> 553.44s]  And while this might at first seem like kind of a,
[555.04s -> 556.28s]  maybe an esoteric problem, it's like, well,
[556.28s -> 557.16s]  if you need lots of samples,
[557.16s -> 558.76s]  just have a faster simulator.
[558.76s -> 560.32s]  In practice, that increase in variance
[560.32s -> 561.52s]  might be catastrophically large.
[561.52s -> 563.60s]  Maybe that you don't just need 10 times more samples.
[563.60s -> 566.04s]  Maybe you need exponentially more samples.
[566.04s -> 568.92s]  In the worst case, the increase is in fact exponential.
[568.92s -> 571.08s]  Those worst cases are a bit pathological
[571.96s -> 574.00s]  and they can be avoided.
[574.00s -> 575.20s]  But in a general case,
[575.20s -> 577.04s]  it does seem like this can be a challenge.
[577.04s -> 579.24s]  And in particular, it can be an unpredictable challenge
[579.24s -> 581.84s]  in the sense that we might have a hard time predicting
[581.84s -> 582.80s]  for a new problem,
[582.80s -> 585.64s]  whether the catastrophic high variance of that problem
[585.64s -> 588.04s]  might make policy gradients hard to use or not.
[589.04s -> 591.64s]  So the kinds of parameters that we then end up
[591.64s -> 593.56s]  being careful with to address the challenge
[593.56s -> 595.20s]  are things like batch size, learning rate,
[595.20s -> 597.84s]  and the design of the baseline for policy gradients,
[597.84s -> 599.96s]  which is a very crucial choice.
[599.96s -> 602.28s]  Model-based RL algorithms are an interesting one.
[602.28s -> 605.20s]  On the surface, model-based RL might seem like
[605.20s -> 607.96s]  a particularly convenient and stable choice
[607.96s -> 610.16s]  because the model learning process in the end
[610.16s -> 612.04s]  just boils down to supervised learning.
[612.04s -> 613.16s]  And that is true.
[613.16s -> 614.24s]  For a given batch of data,
[614.24s -> 615.40s]  training the model is a regular
[615.40s -> 616.76s]  supervised learning problem.
[616.88s -> 618.76s]  However, model-based RL methods
[618.76s -> 620.52s]  are still iterative procedures,
[620.52s -> 623.24s]  meaning that the model changes over the course of training
[623.24s -> 625.28s]  and the model-based RL method
[625.28s -> 626.92s]  is still collecting its own data.
[628.16s -> 630.52s]  This raises a number of major issues.
[630.52s -> 632.64s]  The model class and the method
[632.64s -> 634.60s]  by which the model is fitted to the data
[634.60s -> 636.08s]  ends up being very sensitive.
[637.24s -> 640.80s]  The trouble is that more accurate models
[640.80s -> 644.68s]  do not necessarily translate directly into better policies.
[644.68s -> 646.12s]  If the model is perfect,
[646.12s -> 648.16s]  of course that will give you the best policy.
[648.16s -> 650.08s]  But if the model simply becomes more accurate,
[650.08s -> 652.12s]  it could be that the model might become more accurate
[652.12s -> 654.56s]  in a way that doesn't actually improve the policy
[654.56s -> 657.16s]  at the cost of slightly lower accuracy somewhere else,
[657.16s -> 659.44s]  which turns out to be catastrophic for the policy.
[659.44s -> 662.52s]  So basically not all errors in the model are made equal.
[662.52s -> 664.44s]  And that should be pretty straightforward, right?
[664.44s -> 665.88s]  If you're flying an airplane,
[665.88s -> 667.44s]  having a slightly incorrect model
[667.44s -> 670.56s]  about how the airplane flies when it's at 30,000 feet
[670.56s -> 672.12s]  is clearly not as catastrophic
[672.12s -> 673.60s]  as having an error in the model
[673.60s -> 675.04s]  when the plane is landing
[675.04s -> 677.04s]  and every inch to the ground counts.
[678.48s -> 681.44s]  So optimizing the policy with respect to the model
[681.44s -> 683.08s]  is generally also non-trivial
[683.08s -> 685.24s]  due to this back propagation through time issue.
[685.24s -> 686.64s]  So we end up using all sorts of other methods,
[686.64s -> 688.76s]  including running those same model-free algorithms
[688.76s -> 690.04s]  through the model, which of course
[690.04s -> 693.36s]  incurs all the challenges associated with model-free RL.
[693.36s -> 695.32s]  And there's this more subtle issue,
[695.32s -> 697.04s]  which is that the policy can exploit the model.
[697.04s -> 698.12s]  Essentially, even if the model
[698.12s -> 699.76s]  is very good in most places,
[699.76s -> 701.88s]  your policy might discover a way
[701.88s -> 703.32s]  to take just that one action
[703.32s -> 705.08s]  where the policy makes a mistake
[705.08s -> 706.84s]  that caused it to erroneous to predict
[706.84s -> 708.32s]  that something good will happen.
[708.32s -> 709.64s]  In a sense, model-based RL
[709.64s -> 711.88s]  is a very kind of adversarial process
[711.88s -> 714.28s]  and that presents major additional challenges.
[714.28s -> 716.92s]  So all these approaches have challenges.
[716.92s -> 719.00s]  Those challenges fundamentally actually stem
[719.00s -> 722.12s]  from the same core issues,
[722.12s -> 723.64s]  the issues having to do with the fact
[723.64s -> 725.72s]  that you have to discover optimal behaviors
[725.72s -> 727.24s]  without ground truth supervision,
[727.24s -> 729.00s]  often by collecting your own data.
[729.00s -> 730.44s]  But the way that these issues manifest
[730.44s -> 732.60s]  for each class of methods is a bit different.
[734.24s -> 737.00s]  Now, in regard to efficiency,
[737.00s -> 740.72s]  we can create a kind of a hierarchy of different methods
[740.72s -> 743.96s]  to try to gauge roughly how efficient they are.
[743.96s -> 746.60s]  This slide is actually pretty old at this point.
[746.60s -> 749.12s]  This was created maybe about five years ago.
[749.12s -> 750.80s]  So some of this is a little bit out of date,
[750.80s -> 753.40s]  but I think that the overall trends still hold.
[753.40s -> 756.52s]  So we're gonna start with the least efficient methods
[756.52s -> 759.64s]  and then progress towards the most efficient methods.
[759.64s -> 762.44s]  And the least efficient,
[762.44s -> 764.96s]  and I'll say at the end why we might actually prefer
[764.96s -> 766.28s]  less efficient methods in practice,
[766.28s -> 767.72s]  but the least efficient methods
[767.72s -> 770.88s]  in the sense that they require the most samples
[770.88s -> 772.68s]  are gradient-free methods,
[772.68s -> 775.24s]  which we didn't actually cover in this course,
[775.24s -> 778.08s]  but these are methods like CMAES
[778.08s -> 781.76s]  or natural evolutionary strategies,
[781.76s -> 784.48s]  which actually don't use gradients through neural nets at all.
[786.36s -> 789.60s]  The next step towards more efficient methods
[789.60s -> 794.32s]  are fully online on policy methods like A2C and A3C.
[794.32s -> 799.00s]  So these are methods that run on policy
[799.00s -> 801.24s]  and use essentially policy gradient updates
[803.68s -> 805.60s]  with fully online updates.
[805.60s -> 807.40s]  Policy gradient methods like TRPO,
[807.40s -> 809.32s]  which are batch mode policy gradient methods
[809.32s -> 810.36s]  tend to be a bit more efficient.
[810.36s -> 811.36s]  So the fully online methods
[811.36s -> 813.40s]  basically don't store any trials.
[813.40s -> 816.00s]  They just update as they collect data.
[816.00s -> 817.72s]  Policy gradient methods collect a batch of data
[817.72s -> 819.28s]  and make batch-wise updates.
[820.44s -> 825.44s]  Then we have a big step up in efficiency
[825.48s -> 827.16s]  with methods that use replay buffers
[827.16s -> 828.08s]  and off-policy learning.
[828.08s -> 829.96s]  So these are Q-learning methods,
[829.96s -> 831.56s]  Q-function, active critic methods, and so on.
[831.56s -> 833.80s]  These are all the methods that have replay buffers.
[833.80s -> 835.76s]  Then we have model-based methods.
[835.76s -> 837.88s]  And then we have shallow model-based methods,
[837.88s -> 839.48s]  which are generally the most efficient,
[839.48s -> 841.60s]  but also often the most limiting.
[841.60s -> 843.08s]  And interestingly enough,
[843.08s -> 844.24s]  the step up in efficiency
[844.24s -> 846.00s]  is about an order of magnitude each time.
[846.00s -> 848.16s]  So here's an example of a classic paper
[848.16s -> 849.64s]  on gradient-free methods for RL
[849.64s -> 851.48s]  using evolutionary strategies.
[851.48s -> 853.56s]  And the reported results in that paper
[853.56s -> 856.88s]  are about 10 times less efficient
[856.88s -> 861.48s]  than fully online updates with an algorithm like A3C.
[861.48s -> 864.92s]  So this is an example of a 2017 paper with A3C
[864.92s -> 867.80s]  showing for a half cheetah style task,
[867.80s -> 869.88s]  about 100 million time steps
[869.88s -> 872.64s]  to learn the task to asymptotic performance,
[872.64s -> 875.24s]  which is the equivalent of about 15 days of real time.
[875.92s -> 880.32s]  If we use a method like TRPO or PPO,
[880.32s -> 881.96s]  then we get something on the order
[881.96s -> 882.96s]  of 10 million transitions,
[882.96s -> 885.84s]  the equivalent of about 1.5 days of real time.
[885.84s -> 889.52s]  If we use off-policy algorithms with replay buffers,
[889.52s -> 890.80s]  then we can learn tasks like this
[890.80s -> 893.76s]  in about 1 million time steps,
[893.76s -> 895.08s]  which is about three hours in real time.
[895.08s -> 895.92s]  And these methods have gotten
[895.92s -> 897.44s]  a lot more efficient in recent years.
[897.44s -> 901.28s]  In recent years, there has been a 10 to 100x improvement
[901.28s -> 902.56s]  actually in the speed of these methods.
[902.56s -> 904.08s]  So this result is actually out of date.
[904.08s -> 906.00s]  These things may be even more efficient,
[906.00s -> 907.60s]  but I think it's still actually
[907.60s -> 910.12s]  a reasonable rough ballpark estimate
[910.12s -> 912.08s]  that for realistic tasks,
[912.08s -> 913.48s]  a few hours of real time
[913.48s -> 915.44s]  is about what it takes to learn policies
[915.44s -> 916.56s]  from low dimensional state.
[916.56s -> 919.12s]  And that actually holds even for real world robotic tasks.
[919.12s -> 921.12s]  So I think if you want like a rule of thumb,
[921.12s -> 922.16s]  a reasonable rule of thumb
[922.16s -> 924.56s]  is that off-policy replay buffer methods
[924.56s -> 926.08s]  can, if done right,
[926.08s -> 928.12s]  learn tasks in something on the order
[928.12s -> 930.12s]  of single digit hours.
[930.12s -> 931.04s]  Of course, that's not accounting
[931.04s -> 933.64s]  for things like perception, learning from pixels and so on.
[934.16s -> 935.28s]  So model-based RL methods can be
[935.28s -> 937.08s]  another order of magnitude faster.
[938.04s -> 940.00s]  So we're talking about less than an hour.
[940.00s -> 942.68s]  And then shallow methods like PILCO can be really fast.
[942.68s -> 944.04s]  They can actually learn in seconds,
[944.04s -> 946.80s]  but they require using non-scalable models
[946.80s -> 948.08s]  like Gaussian processes
[948.08s -> 949.28s]  that might simply be impractical
[949.28s -> 951.64s]  to apply to higher dimensional systems.
[951.64s -> 953.64s]  Now, this is a very rough guide.
[953.64s -> 957.16s]  And of course, an obvious question this might raise is,
[957.16s -> 962.16s]  if this is the kind of hierarchy of sample complexity,
[962.76s -> 965.92s]  why would we ever prefer the less efficient methods?
[965.92s -> 967.92s]  Well, the reason is that actually things like
[967.92s -> 971.20s]  policy grading methods are often more paralyzable,
[971.20s -> 973.32s]  which means that if we can run multiple simulation
[973.32s -> 974.72s]  in parallel, they can actually be faster
[974.72s -> 976.52s]  in terms of wall clock time.
[976.52s -> 978.96s]  And the cost of samples
[978.96s -> 980.20s]  is not the only cost that you pay.
[980.20s -> 983.20s]  So if you have access to lots of simulation
[983.20s -> 986.12s]  and generating interaction with the environment is cheap,
[986.12s -> 987.36s]  and the cost has more to do
[987.36s -> 989.12s]  with the compute for training your models,
[989.12s -> 990.72s]  then maybe you might prefer methods
[990.72s -> 991.80s]  that are less efficient,
[992.32s -> 993.96s]  but that require less compute.
[993.96s -> 995.72s]  And in fact, model-based deep RL methods
[995.72s -> 997.44s]  are often the most compute hungry
[997.44s -> 999.68s]  because you might take many grading updates on the model
[999.68s -> 1002.12s]  for every simulation step.
[1002.12s -> 1002.96s]  So for this reason,
[1002.96s -> 1004.72s]  you might actually prefer methods that are less efficient,
[1004.72s -> 1006.68s]  but have other benefits like better parallelism
[1006.68s -> 1010.28s]  or requiring fewer grading updates on the policy or model.
[1011.84s -> 1014.44s]  Okay, so, but that said,
[1014.44s -> 1016.48s]  why do we care about sample complexity?
[1016.48s -> 1018.32s]  Well, an obvious one is that
[1018.32s -> 1019.92s]  if you have bad sample complexity,
[1019.96s -> 1021.28s]  you have to wait for a long time
[1021.28s -> 1022.72s]  for your homework to finish.
[1023.68s -> 1024.68s]  But the other thing is that
[1024.68s -> 1027.52s]  if you actually want to use deep RL in the real world,
[1027.52s -> 1030.52s]  poor sample complexity means that real world learning
[1030.52s -> 1032.84s]  can become very difficult or even impractical.
[1034.52s -> 1035.72s]  It also precludes the use
[1035.72s -> 1037.56s]  of very expensive high fidelity simulators.
[1037.56s -> 1038.80s]  Maybe you'd like to use
[1038.80s -> 1041.20s]  some sort of finite element analysis
[1041.20s -> 1043.28s]  to simulate a really complex system
[1043.28s -> 1045.80s]  that might even be slower than real time.
[1045.80s -> 1047.36s]  So if your algorithm requires
[1047.36s -> 1048.60s]  hundreds of millions of trials,
[1048.64s -> 1050.52s]  that might simply not be feasible.
[1050.52s -> 1053.32s]  And generally limits applicability to real world problems.
[1053.32s -> 1055.84s]  So developing more efficient RL methods
[1055.84s -> 1057.88s]  is a major open problem.
[1057.88s -> 1059.96s]  I do think that current deep RL methods
[1059.96s -> 1061.64s]  have improved in efficiency tremendously
[1061.64s -> 1063.12s]  to the point where real world training
[1063.12s -> 1065.00s]  often is actually very feasible,
[1065.00s -> 1066.52s]  but in many domains,
[1066.52s -> 1068.96s]  especially as you increase the breadth of the problem,
[1068.96s -> 1070.00s]  meaning that you want systems
[1070.00s -> 1071.12s]  that generalize more broadly
[1071.12s -> 1073.40s]  by training in a greater range of scenarios,
[1073.40s -> 1075.44s]  this becomes an even bigger issue.
[1076.36s -> 1079.32s]  So speaking of breadth and generalization,
[1079.32s -> 1081.32s]  scaling up and generalization
[1081.32s -> 1083.52s]  are major challenges in deep RL.
[1083.52s -> 1085.00s]  When it comes to supervised learning,
[1085.00s -> 1087.72s]  like training on ImageNet or training on Common Crawl
[1087.72s -> 1089.92s]  or large NLP datasets,
[1089.92s -> 1091.72s]  the state of the art in supervised deep learning
[1091.72s -> 1094.12s]  is large scale, emphasizes diversity,
[1094.12s -> 1095.92s]  and it's evaluated on generalization.
[1095.92s -> 1098.08s]  So nobody cares how well your language model
[1098.08s -> 1100.92s]  can memorize a particular piece of text it's trained on.
[1100.92s -> 1102.68s]  Everyone cares how well your language model
[1102.72s -> 1105.92s]  can generalize to some new prompt.
[1105.92s -> 1109.00s]  Whereas in RL, we seem to be often evaluating our methods
[1109.00s -> 1111.64s]  on small scale tasks that emphasize mastery
[1111.64s -> 1113.00s]  and are evaluated on performance,
[1113.00s -> 1115.48s]  meaning that what we really measure is
[1115.48s -> 1118.04s]  how well do we optimize a given objective function
[1118.04s -> 1120.24s]  in the particular environment where it's optimized.
[1120.24s -> 1122.64s]  And that is often a very reasonable thing to measure.
[1122.64s -> 1123.72s]  So if you're trying to improve
[1123.72s -> 1125.88s]  the optimization performance of your method,
[1125.88s -> 1127.32s]  that's the thing you should be measuring.
[1127.32s -> 1130.24s]  But besides optimization performance,
[1130.24s -> 1131.12s]  in the real world,
[1131.12s -> 1133.28s]  we also care about generalization performance,
[1133.28s -> 1134.72s]  diversity, and breadth.
[1134.72s -> 1137.52s]  And that starts implicating a lot of topics
[1137.52s -> 1141.48s]  that go beyond the core questions in basic RL methods
[1141.48s -> 1142.84s]  and have more to do with the ability
[1142.84s -> 1145.16s]  to apply RL methods at larger scale
[1145.16s -> 1147.60s]  to multitask problems and settings
[1147.60s -> 1150.04s]  that require using large amounts of data
[1150.04s -> 1152.88s]  rather than just large amounts of simulation.
[1152.88s -> 1156.00s]  So where's the generalization gonna come from?
[1156.00s -> 1157.72s]  There are a number of issues with this.
[1157.72s -> 1159.72s]  So first we could say off the bat,
[1159.72s -> 1161.52s]  well, what if we just scale up deep RL?
[1161.52s -> 1164.44s]  What if we just run a massive simulation
[1164.44s -> 1166.44s]  with lots and lots of different settings
[1166.44s -> 1170.08s]  and try to get more generalizable and performing policies?
[1170.08s -> 1171.60s]  This was basically the path
[1171.60s -> 1174.80s]  towards game playing systems like AlphaGo,
[1174.80s -> 1176.44s]  but it's quite challenging.
[1176.44s -> 1178.28s]  So with supervised machine learning,
[1178.28s -> 1179.92s]  what we do is we interact with the world
[1179.92s -> 1181.08s]  and collect the data set.
[1181.08s -> 1182.28s]  And this is typically done once
[1182.28s -> 1184.36s]  for supervised learning systems.
[1184.36s -> 1187.44s]  And then we run a learning algorithm on that data set
[1187.44s -> 1189.80s]  for many epochs and get some solution.
[1189.80s -> 1191.44s]  And if we're not happy with that solution,
[1191.44s -> 1192.40s]  we don't recollect the data,
[1192.40s -> 1193.76s]  we just rerun the training
[1193.76s -> 1195.76s]  after changing something about our method.
[1195.76s -> 1196.72s]  In reinforcement learning,
[1196.72s -> 1198.52s]  we typically learn through continuous interaction
[1198.52s -> 1199.96s]  with the world.
[1199.96s -> 1201.28s]  So that means that if we wanna change
[1201.28s -> 1202.40s]  something about our method,
[1202.40s -> 1207.12s]  we would typically rerun the interactive learning process.
[1207.12s -> 1209.68s]  But the reality is that actual reinforcement learning
[1209.68s -> 1212.48s]  has an outer loop and that outer loop is you.
[1212.48s -> 1215.04s]  If you're not happy with how well your method did,
[1216.04s -> 1217.52s]  you would change something about your method
[1217.52s -> 1220.20s]  and then rerun the training process again.
[1220.20s -> 1222.76s]  And that's fine if your training process involves
[1222.76s -> 1225.08s]  training the half cheated or run faster,
[1225.08s -> 1226.76s]  but if your training process involves
[1226.76s -> 1229.08s]  sort of internet scale training
[1229.08s -> 1230.72s]  on a huge range of different settings
[1230.72s -> 1233.40s]  to achieve real world generalization,
[1233.40s -> 1235.60s]  this outer loop quickly becomes impractical.
[1236.72s -> 1238.12s]  So for this reason,
[1238.12s -> 1239.92s]  it's very important to think about
[1239.92s -> 1241.64s]  improvements to RL methods
[1241.64s -> 1244.68s]  that don't just address the core challenges of optimization,
[1245.20s -> 1247.48s]  but also just workflows that are more suitable
[1247.48s -> 1249.64s]  for large scale machine learning research.
[1252.12s -> 1253.88s]  This problem is pretty bad, right?
[1253.88s -> 1257.76s]  So here's a video from TRPO plus GAE.
[1257.76s -> 1259.64s]  This video is quite old at this point,
[1259.64s -> 1261.28s]  but it's still, I think, pretty impressive.
[1261.28s -> 1263.56s]  It shows a humanoid learning to run.
[1263.56s -> 1266.00s]  And while it takes a little while and falls a few times,
[1266.00s -> 1267.60s]  after training is concluded,
[1267.60s -> 1270.76s]  it can run on this infinitely large flat plane,
[1270.76s -> 1272.00s]  essentially perpetually.
[1273.04s -> 1274.24s]  So this is pretty cool.
[1275.56s -> 1278.04s]  It takes about six days of real time
[1278.04s -> 1279.20s]  if this was a real robot,
[1279.20s -> 1280.88s]  of course, and simulation goes a lot faster.
[1280.88s -> 1283.20s]  Now, since then, these algorithms have gotten a lot quicker
[1283.20s -> 1285.16s]  so maybe now it wouldn't take six days.
[1285.16s -> 1287.20s]  It might even be as little as six hours,
[1287.20s -> 1289.96s]  but it's still a pretty non-trivial amount of time.
[1289.96s -> 1291.16s]  The problem though is not just that.
[1291.16s -> 1292.84s]  The problem is like,
[1292.84s -> 1295.76s]  if we could just run a robot for like a few days
[1295.76s -> 1297.60s]  and get a robot that can run anywhere,
[1297.60s -> 1298.92s]  we'd be pretty happy with that.
[1298.92s -> 1300.00s]  But that's not actually what we're getting.
[1300.00s -> 1301.84s]  What we're getting is something that can run
[1301.84s -> 1303.64s]  on an infinitely large flat plane.
[1303.64s -> 1304.96s]  The real world presents a wide range
[1304.96s -> 1306.32s]  of different scenarios.
[1307.40s -> 1308.96s]  The real world is diverse.
[1308.96s -> 1310.60s]  And if you want a practical system
[1310.60s -> 1312.00s]  that does a task like this in the real world,
[1312.00s -> 1313.52s]  it has to handle all sorts of terrains,
[1313.52s -> 1315.04s]  all sorts of situations,
[1315.04s -> 1316.72s]  and maybe even all sorts of behaviors
[1316.72s -> 1318.40s]  in service to locomotion.
[1318.40s -> 1321.16s]  So not just running, but climbing over things and so on.
[1321.16s -> 1324.24s]  And an approach that people have taken with some success
[1324.24s -> 1326.96s]  is to simply simulate a greater range of situations,
[1326.96s -> 1329.72s]  but this does quickly start presenting major challenges.
[1330.36s -> 1335.36s]  Now, those challenges include the challenge
[1335.68s -> 1337.96s]  of figuring out what all those scenarios are.
[1337.96s -> 1339.32s]  So you might want real world data
[1339.32s -> 1341.44s]  to figure out the range of scenarios you have to cover,
[1341.44s -> 1342.96s]  and also actually devising the algorithms
[1342.96s -> 1345.24s]  that can handle such a broad range of scenarios.
[1345.24s -> 1347.12s]  So in terms of utilizing data,
[1347.12s -> 1349.68s]  perhaps off policy or offline RL methods
[1349.68s -> 1350.80s]  can be more effective.
[1350.80s -> 1352.24s]  Maybe we could collect the big data set
[1352.24s -> 1354.28s]  from past interactions to figure out
[1354.28s -> 1358.08s]  that an effective robot needs to run on sand
[1358.36s -> 1361.48s]  in cities and all sorts of other situations,
[1361.48s -> 1363.00s]  and perhaps that we can use this data set
[1363.00s -> 1364.64s]  with an offline RL procedure,
[1364.64s -> 1366.76s]  where if we're not happy with the solution,
[1366.76s -> 1367.92s]  we could go out and get more data
[1367.92s -> 1369.44s]  simply added to our data set
[1369.44s -> 1371.20s]  rather than repeating the process.
[1372.28s -> 1374.20s]  And then if we have to tune something about the method,
[1374.20s -> 1375.12s]  maybe we can do so
[1375.12s -> 1377.40s]  without having to discard all of our data.
[1377.40s -> 1378.76s]  Perhaps we could also approach this
[1378.76s -> 1379.92s]  by building simulations
[1379.92s -> 1381.92s]  and adapting those simulations to the real world,
[1381.92s -> 1383.44s]  if that's the approach you want to take.
[1383.44s -> 1384.60s]  And that's also something
[1384.60s -> 1386.40s]  that perhaps merits more research.
[1387.40s -> 1390.72s]  The multitask setting also presents challenges
[1390.72s -> 1393.16s]  that are often not at the core
[1393.16s -> 1394.68s]  of reinforced learning research,
[1394.68s -> 1396.08s]  but I think are tremendously important.
[1396.08s -> 1397.92s]  So generalization comes from training
[1397.92s -> 1399.32s]  in many different settings.
[1399.32s -> 1400.76s]  We talked about a variety of ways
[1400.76s -> 1402.56s]  to set up multitask learning problems.
[1402.56s -> 1405.04s]  For example, you could say that you have multiple MDPs
[1405.04s -> 1407.44s]  and you model them as one multitask MDP,
[1407.44s -> 1410.68s]  where a different MDP is chosen at the first time step.
[1410.68s -> 1413.20s]  And maybe this doesn't require any new assumption,
[1413.20s -> 1414.80s]  but it might merit additional treatment
[1415.32s -> 1418.04s]  to develop algorithms that are effective in these scenarios.
[1418.04s -> 1419.64s]  So whilst standard RL methods
[1419.64s -> 1421.08s]  can handle multitask learning,
[1421.08s -> 1422.68s]  it does exacerbate certain challenges
[1422.68s -> 1426.52s]  that already are problematic in RL.
[1426.52s -> 1427.76s]  Challenges like variance, right?
[1427.76s -> 1428.84s]  If you have many different MDPs,
[1428.84s -> 1430.12s]  your variance is gonna be even higher
[1430.12s -> 1432.48s]  because now there's more variability in the initial state.
[1432.48s -> 1435.32s]  Challenges like sample complexity.
[1435.32s -> 1436.68s]  If you have more MDPs,
[1436.68s -> 1439.32s]  then you need more samples to train.
[1439.32s -> 1442.52s]  So the existing sample complexity challenges are exacerbated.
[1442.52s -> 1444.12s]  It may be that we can make progress on this problem
[1444.28s -> 1445.96s]  simply by addressing those four challenges,
[1445.96s -> 1447.92s]  or it may be that we can devise better methods
[1447.92s -> 1450.24s]  that target multitask learning in particular.
[1450.24s -> 1452.28s]  This is an important thing to keep in mind.
[1453.44s -> 1455.16s]  But now let's also talk about those assumptions.
[1455.16s -> 1457.56s]  So outside of the capabilities of the core method,
[1459.16s -> 1460.72s]  the assumptions of RL
[1460.72s -> 1462.48s]  that we have access to a reward function,
[1462.48s -> 1464.96s]  that we have interaction with an environment,
[1464.96s -> 1465.80s]  these are all assumptions
[1465.80s -> 1467.64s]  that can be problematic in the real world.
[1467.64s -> 1470.16s]  Where does supervision come from for RL?
[1470.16s -> 1472.28s]  If you want to learn from many different tasks,
[1472.28s -> 1474.04s]  you need to get those tasks somewhere.
[1474.96s -> 1475.80s]  And it might be in some cases very natural
[1475.80s -> 1477.60s]  for humans to specify those tasks.
[1477.60s -> 1480.48s]  So if you want a robot to travel to different locations,
[1480.48s -> 1482.16s]  it might not be that hard
[1482.16s -> 1483.40s]  for a person to simply write down,
[1483.40s -> 1485.08s]  oh, like these are the GPS coordinates
[1485.08s -> 1487.80s]  I want you to travel to, practice doing that.
[1487.80s -> 1488.64s]  But in other cases,
[1488.64s -> 1491.24s]  simply specifying what you want the RL algorithm
[1491.24s -> 1493.32s]  to learn to do can be very hard.
[1493.32s -> 1494.64s]  So if you play a game,
[1494.64s -> 1495.76s]  it's pretty easy to get reward
[1495.76s -> 1496.84s]  because the game has a score
[1496.84s -> 1498.76s]  and winning the game can be the reward.
[1498.76s -> 1501.60s]  But let's say that you want a poor glass of water.
[1501.60s -> 1503.36s]  Now this is something that any child could do,
[1503.40s -> 1504.60s]  but if you want a robot to learn
[1504.60s -> 1505.96s]  how to pour a glass of water,
[1505.96s -> 1508.64s]  simply understanding whether the glass is full of water
[1508.64s -> 1510.88s]  itself requires a complex perception system.
[1511.76s -> 1513.84s]  This problem has actually come to the forefront recently
[1513.84s -> 1517.96s]  because with internet chatbots like chatgpt,
[1517.96s -> 1520.40s]  it's actually a major challenge to figure out
[1520.40s -> 1521.96s]  whether you are interacting with users
[1521.96s -> 1523.48s]  in ways that make those users happy,
[1523.48s -> 1525.00s]  in ways that satisfy those users,
[1525.00s -> 1527.12s]  and traditional ways of specifying reward
[1527.12s -> 1529.52s]  tend to fail in those cases.
[1529.52s -> 1531.24s]  So there's all sorts of other things that could be done.
[1531.24s -> 1533.32s]  For example, we could learn objectives
[1533.32s -> 1534.60s]  or rewards from demonstration,
[1534.60s -> 1536.52s]  which is inverse reinforcement learning.
[1536.52s -> 1538.08s]  We could generate objectives automatically
[1538.08s -> 1539.68s]  with automated skill discovery
[1539.68s -> 1541.56s]  to produce a wide variety of different tasks
[1541.56s -> 1544.24s]  so that we could then generalize to new tasks,
[1544.24s -> 1546.80s]  but we can also explore other sources of supervision.
[1546.80s -> 1549.08s]  So besides demonstrations,
[1549.08s -> 1552.12s]  this is something that's of course been very widely used.
[1552.12s -> 1556.04s]  We can think about methods that leverage language
[1556.04s -> 1558.72s]  to figure out what the robot should be doing
[1558.72s -> 1560.28s]  and perhaps auxiliary supervision
[1560.28s -> 1562.40s]  for models that combine language and perception
[1562.40s -> 1564.92s]  that can offer reward signals through generalization
[1564.92s -> 1566.36s]  from internet-scale training.
[1566.36s -> 1567.28s]  We could also imagine methods
[1567.28s -> 1568.80s]  that learn from human preferences,
[1568.80s -> 1570.84s]  pairwise comparisons of different behaviors.
[1570.84s -> 1574.40s]  This was pioneered for reinforced learning benchmark tasks,
[1574.40s -> 1576.20s]  but has recently gained a lot of attention
[1576.20s -> 1578.84s]  as the preferred method for training language models
[1578.84s -> 1581.04s]  to satisfy user preferences.
[1581.04s -> 1583.76s]  So these are all alternative sources of supervision
[1583.76s -> 1585.00s]  that change the core assumptions
[1585.00s -> 1586.60s]  of reinforced learning algorithms,
[1586.60s -> 1588.04s]  and I think it's important to think about
[1588.04s -> 1591.36s]  the kind of supervision that your particular domain requires.
[1593.40s -> 1596.40s]  And I think there's also a fairly fundamental question
[1596.40s -> 1597.48s]  when it comes to supervision,
[1597.48s -> 1600.28s]  which is, should we be supervising RL agents
[1600.28s -> 1602.08s]  by telling them what we want them to do
[1602.08s -> 1603.84s]  or how we want them to do it?
[1603.84s -> 1606.60s]  So demonstrations provide both the what and the how,
[1606.60s -> 1609.08s]  reward functions in principle provide only the what,
[1609.08s -> 1611.24s]  but if the reward functions are more well-shaped,
[1611.24s -> 1613.28s]  then they're also providing some of the how.
[1613.28s -> 1616.00s]  So there's kind of a balance of these things,
[1616.00s -> 1618.16s]  and we have to strike that balance carefully
[1618.16s -> 1619.36s]  because on the one hand,
[1619.36s -> 1620.76s]  the strength of RL methods
[1620.76s -> 1624.00s]  is that they can discover new and novel solutions,
[1624.00s -> 1626.20s]  so we don't wanna supervise them too closely,
[1626.20s -> 1627.12s]  but at the same time,
[1627.12s -> 1628.56s]  if the supervision is too high level,
[1628.56s -> 1629.48s]  like for example,
[1629.48s -> 1631.68s]  your supervision for your language model chatbot
[1631.68s -> 1633.80s]  is make my company lots of money,
[1633.80s -> 1635.96s]  then it might be just a very difficult learning problem,
[1635.96s -> 1638.96s]  so we have to strike the right balance there.
[1638.96s -> 1640.08s]  And we might wanna rethink
[1640.08s -> 1641.64s]  the problem formulation in other ways,
[1641.64s -> 1643.72s]  like how do we define the control problem?
[1643.72s -> 1644.72s]  What is the data?
[1644.72s -> 1645.56s]  In some cases,
[1645.56s -> 1647.84s]  it's easier to define a control problem with data
[1647.84s -> 1649.68s]  that specifies what might happen in the world
[1649.68s -> 1651.28s]  than to define it with a simulator
[1651.28s -> 1653.36s]  or access to an interactive agent.
[1653.36s -> 1655.28s]  So offline RL supports that kind of setting,
[1655.28s -> 1657.08s]  online RL methods do not.
[1657.08s -> 1657.96s]  What is the goal?
[1657.96s -> 1659.72s]  What is the RL agent trying to achieve?
[1659.72s -> 1661.20s]  Is the goal specified by reward,
[1661.20s -> 1663.40s]  by demonstration or by preferences?
[1663.40s -> 1664.60s]  And what is the supervision?
[1664.60s -> 1665.92s]  Is that the same as the goal?
[1665.92s -> 1668.16s]  Sometimes you might wanna provide the agent with hints
[1668.16s -> 1669.64s]  that help it learn the task
[1669.64s -> 1672.16s]  without biasing the solution that it finds.
[1672.16s -> 1674.88s]  And there's been research on using demonstrations
[1674.88s -> 1677.64s]  as guidance rather than necessarily as goal specification,
[1677.64s -> 1679.40s]  but it's an open area of research.
[1680.56s -> 1682.88s]  In general, there is no one answer here,
[1682.88s -> 1685.56s]  and that's why this is part of the open challenges lecture,
[1685.56s -> 1686.64s]  but I would encourage all of you
[1686.64s -> 1688.40s]  to think about the assumptions
[1688.40s -> 1690.52s]  that fit your problem setting.
[1690.52s -> 1692.52s]  And sometimes the right thing to do
[1692.52s -> 1695.32s]  is to course your problem into the standard RL assumptions,
[1695.32s -> 1696.32s]  but sometimes the right thing to do
[1696.32s -> 1697.64s]  is to invent a new problem.
[1697.64s -> 1700.16s]  Don't assume that the basic RL problem is set in stone
[1700.16s -> 1701.80s]  and think about how it could be adjusted
[1701.80s -> 1703.20s]  to fit your setting.
