# Detected language: en (p=1.00)

[0.00s -> 5.56s]  All right, so in the next portion of the lecture, we'll dive a little deeper into
[5.56s -> 11.72s]  this concept of state coverage, and we'll generalize the notion of
[11.72s -> 15.64s]  maximizing the entropy of your state more generally to matching some target
[15.64s -> 19.64s]  distribution of our states. And in the course of this, I think we'll see some
[19.64s -> 23.50s]  interesting connections between the kind of exploration concepts that we're
[23.50s -> 26.48s]  discussing in this lecture and the kind of concepts that we covered on
[26.48s -> 33.48s]  Monday. All right, so let's start with a little aside. Let's talk about how we can
[33.48s -> 37.56s]  do unsupervised exploration with intrinsic motivation objectives. So
[37.56s -> 41.20s]  intrinsic motivation is another term that's used to refer to these kind of
[41.20s -> 47.64s]  novelty-seeking things that we saw on Monday, like pseudocounts and so on. So
[47.64s -> 51.64s]  the common method of exploration that we saw on Monday is that you somehow
[51.68s -> 56.96s]  incentivize your policy, pi A given S, to explore diverse states, to visit novel
[56.96s -> 61.72s]  states that have not been visited very frequently before. And you could do this
[61.72s -> 65.28s]  before seeing any reward. On Monday, this was because the reward is very
[65.28s -> 68.52s]  delayed or very sparse. It could be done in settings where the reward is
[68.52s -> 72.64s]  absent altogether. But what will you actually get when you do this if
[72.64s -> 77.80s]  there's no reward at all? Well, if your reward visiting novel states, you know,
[77.80s -> 81.96s]  essentially if a state is visited often, then it becomes not novel. So you can add
[81.96s -> 84.80s]  a variety of exploration bonuses that will do this. We learned about things
[84.80s -> 88.52s]  like counts. The one that I'll talk about here is a very simple bonus, which
[88.52s -> 96.52s]  is just negative log P of S. You could use, you know, plus one over N of S,
[96.52s -> 100.28s]  negative log P of S. This is sometimes referred to as intrinsic motivation. It's just
[100.28s -> 104.18s]  another variant on the same idea. But everything I'm going to say is also
[104.34s -> 110.66s]  true for pseudocounts, EX2, hash, you know, hash exploration, all these things.
[110.66s -> 117.26s]  So let's say that we're basically penalizing density under pi. So we
[117.26s -> 120.86s]  want to visit states with low density. So you could imagine the
[120.86s -> 125.50s]  following procedure. Update your policy to maximize this reward with this bonus,
[125.50s -> 129.46s]  and then update your state distribution to fit your current state marginal, and
[129.46s -> 133.06s]  then repeat, right? Very standard procedure, very similar to what we saw on Monday.
[133.06s -> 136.90s]  Well, what you're going to get if you do this when there's no reward at all is
[136.90s -> 141.26s]  that when your policy goes and does something, then the density estimator
[141.26s -> 144.78s]  will fit whatever your policy did, and then the policy will go and do
[144.78s -> 148.78s]  something else. And then the density estimator will fit what that policy
[148.78s -> 153.06s]  did, and then will go and do something else, and so on and so on. So the
[153.06s -> 159.30s]  density estimator, this P pi of S, it will eventually have high coverage. It
[159.30s -> 164.02s]  will assign reasonably high probabilities to all states, but the policy that
[164.02s -> 166.90s]  you'll end up with will kind of be arbitrary. The policy will end up kind
[166.90s -> 171.18s]  of chasing its tail all around the space. You won't actually end up with a
[171.18s -> 175.02s]  policy that gets uniform coverage over states. The final policy will be
[175.02s -> 179.74s]  arbitrary. It'll just go to whatever place the previous policy has been to
[179.74s -> 183.30s]  less often. But the density estimator will get good coverage when you do this.
[183.30s -> 187.14s]  So this is not by itself a great way of solving that original problem I
[187.18s -> 190.82s]  posed, where what you want is a policy that goes to many different places so
[190.82s -> 196.98s]  they can select between them. So what if you actually do want a policy that gets
[196.98s -> 200.74s]  good coverage? So you don't just want a density estimator that has uniform
[200.74s -> 203.78s]  coverage over the state space, but you actually want a policy that will
[203.78s -> 207.26s]  randomly go to different states, and it will have about equal probability of
[207.26s -> 213.26s]  landing in all possible states. So the state marginal matching problem can be
[213.26s -> 218.22s]  posed as learning a policy pi A given S so as to minimize the KL divergence
[218.22s -> 223.06s]  between state marginal, p pi of S, and some target state marginal p star of S.
[223.06s -> 228.50s]  If p star of S is uniform, then you're just maximizing state entropy. But if
[228.50s -> 231.86s]  it's not uniform in general, you might be matching some target state entropy.
[231.86s -> 238.98s]  These are very similar problems. So can you use this intrinsic motivation idea
[238.98s -> 241.90s]  from before? Can you essentially use one of these novelty seeking exploration
[241.90s -> 247.98s]  objectives? So let's construct our reward analogously to before. So we're
[247.98s -> 251.70s]  going to construct it as log p star, which is the desired state
[251.70s -> 257.50s]  distribution, minus log p pi, which is our current state distribution. Now
[257.50s -> 262.30s]  right off the bat, something you might notice if you're up to
[262.30s -> 265.90s]  speed on your information theory definitions is that the RL objective
[265.90s -> 270.34s]  will be the expected value of this under the stationary distribution. So
[270.34s -> 276.06s]  it'll be the expectation under p pi of S of r tilde. And the expectation
[276.06s -> 283.14s]  under p pi of S of r tilde is exactly that KL divergence up there. So that's
[283.14s -> 287.26s]  kind of interesting. It seems like the RL objective is exactly the quantity
[287.26s -> 292.70s]  that we want. So does that imply that RL will optimize this? Well not exactly,
[292.70s -> 297.02s]  because RL is not aware of the fact that the reward itself depends on the
[297.02s -> 301.26s]  policy. So this does not by itself perform state marginal matching for a
[301.26s -> 305.32s]  very subtle reason. Even though the objective is exactly the state
[305.32s -> 309.52s]  marginal matching objective, the algorithm, the RL algorithm, is not aware of the
[309.52s -> 314.10s]  fact that the minus log p pi of S depends on pi. So as a result, you get
[314.10s -> 317.26s]  this tail chasing problem that I had on the previous slide. You get this
[317.26s -> 321.44s]  issue where the policy will keep jumping to different places, and at the
[321.44s -> 328.72s]  end will not actually minimize that KL divergence. But let's try to sketch out
[328.72s -> 331.64s]  what that algorithm would look like anyway, and then we'll see that there's
[331.64s -> 336.96s]  actually a very simple fix that'll fix this problem. So here is how we can
[336.96s -> 341.12s]  sketch out this algorithm, and I'm going to use somewhat tedious notation
[341.12s -> 345.80s]  for a reason. This notation will be important later. So at every iteration
[345.80s -> 351.46s]  we're going to learn a policy pi k, where k here indexes the iteration. So
[351.46s -> 355.56s]  the first iteration is k equals 1, the second iteration is k equals 2, etc. We'll
[355.56s -> 361.68s]  learn a policy pi k to maximize the expected value under pi of r tilde k.
[361.68s -> 367.54s]  And again, I'm using the superscript k on r tilde to denote that this is the
[367.54s -> 372.92s]  r tilde that used the density estimator from iteration k. And then
[372.96s -> 377.48s]  you will update your density estimator, maybe it's a variational autoencoder or
[377.48s -> 380.84s]  some other distribution, to fit the current state marginal, the state
[380.84s -> 389.68s]  marginal of this new policy pi k. And then you'll repeat this process. So at
[389.68s -> 393.84s]  the end, you know, let's say that this orange circle is the density
[393.84s -> 397.64s]  we're trying to match, you know, your policy will keep jumping around, so at
[397.64s -> 401.44s]  the end your final policy, let's say pi 4 here, is going to some arbitrary
[401.64s -> 407.40s]  place, is going to the lower right, but the green circle is the density
[407.40s -> 411.12s]  estimate for all these policies, actually is not too far off from the
[411.12s -> 418.68s]  orange circle. So we need to modify step two a little bit, we need to update
[418.68s -> 423.72s]  p pi k s to fit all the states seen so far, not just states from the latest
[423.72s -> 428.16s]  policy, but states from all the policies, right? So then we get the union of the
[428.16s -> 431.84s]  green circles, not just the last green circle. That's a very easy change to make,
[431.84s -> 437.52s]  essentially, figure density estimator to your replay buffer. But another change
[437.52s -> 441.44s]  that we're going to make is instead of returning this latest policy,
[441.44s -> 445.64s]  we'll actually return a mixture policy. So the final policy that we'll
[445.64s -> 451.88s]  return will be a mixture model that averages together all the policies
[451.88s -> 456.72s]  seen so far. So the end result is not one policy, it's actually many policies,
[456.76s -> 462.60s]  and the way that you're going to actually do this is you're going to run a randomly chosen iterate,
[462.60s -> 470.32s]  a randomly chosen pi k. That might seem like kind of a weird decision, like you'd think that
[470.32s -> 475.12s]  the last policy would be the best one, why are we randomly selecting from among all the
[475.12s -> 481.68s]  policies we saw during learning? Well it turns out that this procedure does perform marginal
[481.68s -> 490.40s]  matching, and proving this requires a little bit of game theory. So the thing is that the last,
[490.40s -> 499.24s]  the state distribution where p pi of s is equal to p star of s is the Nash equilibrium
[499.24s -> 505.80s]  of a two-player game. The players in that game are the state density estimator and the policy.
[505.80s -> 521.48s]  So this is a game between pi k and p pi k. There's a special case here if p star of s
[521.48s -> 525.36s]  is a constant, then you have a uniform target, maybe it's a little easier to think about it
[525.36s -> 535.52s]  that way. In that case the KL diverge is just the entropy. Now it turns out that the way
[536.24s -> 542.12s]  you can recover the Nash equilibrium in a two-player game like this is by just having the players play
[542.12s -> 546.32s]  against each other, meaning that each time each player gives the best response, so the best
[546.32s -> 550.40s]  response for the density matching algorithm is to actually fit the density, the best response for
[550.40s -> 556.52s]  the policy is to maximize our tilde. But simply running this best iterated best response
[556.52s -> 560.88s]  algorithm doesn't actually produce a Nash equilibrium. It turns out that you get a
[560.96s -> 566.72s]  Nash equilibrium if you do what's called self-play, and in self-play what you're supposed to do is
[566.72s -> 571.92s]  you're supposed to return the historical average of all the iterates that you've seen. So the final
[571.92s -> 575.68s]  iterate is not the Nash equilibrium, but the mixture of all the iterates is the Nash
[575.68s -> 581.20s]  equilibrium, and this is a very well-known result in game theory. So essentially you can prove
[581.20s -> 585.64s]  that this pi star that I have in step three, which is a mixture of all the pi k's, is the
[585.64s -> 592.24s]  Nash equilibrium for this two-player game, and that means that it's going to be minimizing the
[592.24s -> 600.68s]  scale divergence between p pi and p star. If you want to learn more about this, if you want to go
[600.68s -> 604.84s]  into more detail about this, check out these two papers linked at the bottom of the slide,
[604.84s -> 608.84s]  Efficient Exploration via State Marginal Matching and Proveably Efficient Maximimetric
[609.24s -> 619.24s]  Okay, so a few experiments. This is a little ant robot that's supposed to run around in this maze with three
[619.24s -> 626.60s]  different wings, and the SAC, this is just a standard RL algorithm, doesn't explore all three
[626.60s -> 630.76s]  wings equally. It kind of gravitates in this case towards the top right one, whereas the state
[630.76s -> 634.36s]  marginal matching does cover all of them equally, so it gets better coverage, and there's some
[634.36s -> 641.40s]  quantitative results as well. So anyway, high-level idea to take away from this is that the individual
[641.40s -> 645.64s]  iterates that you get when you run intrinsic motivation do not get good state coverage, they
[645.64s -> 650.44s]  do not match target distributions, but if you average together those iterates, then you do, and
[650.44s -> 655.88s]  the way you prove that is by using the theory of self-play, which you can show is a Nash
[655.88s -> 665.56s]  equilibrium of this two-player game. Okay, one thing I want to talk about briefly next is,
[667.16s -> 672.04s]  so far we talked about how these unsupervised exploration methods, they aim to get good
[672.04s -> 677.40s]  coverage, either a uniform distribution over goals or matching some state distribution, matching
[677.40s -> 683.16s]  maybe a uniform distribution, but is coverage of valid states actually a good exploration objective?
[683.16s -> 689.56s]  Like, why do we want to cover as many states as possible? So in skew fit, we were covering the
[689.56s -> 694.68s]  space of goals. In state marginal matching, if you have the special case where p star is a constant,
[694.68s -> 699.24s]  then you're maximizing state entropy. They're kind of more or less the same thing.
[699.24s -> 705.48s]  So when is this a good idea? Well, here's a little result of this. It's kind of an obvious result.
[705.48s -> 710.04s]  I'm going to call it somewhat humorously Eisenbach's theorem after Ben Eisenbach, who's the
[710.04s -> 715.96s]  student that actually wrote this out in the paper linked at the bottom, but it's not really a
[715.96s -> 721.16s]  theorem. It's kind of a really trivial result that follows from classic Nuxon entropy modeling.
[722.28s -> 728.84s]  What you can show is that if at test time an adversary chooses the worst possible goal,
[728.84s -> 734.28s]  G, there is actually a single answer to what goals you should practice during training.
[734.92s -> 739.48s]  So if at test time you're going to get the worst possible goal, essentially when you
[739.48s -> 743.00s]  come home in the evening, your robot has done the practicing, you're intentionally going to give the
[743.00s -> 746.68s]  robot the hardest test. Like, you know, you hate the company that made this robot, you want
[746.68s -> 750.36s]  to give them a one-star review, you're going to intentionally give it the hardest task just
[750.36s -> 756.20s]  to watch it fail so that you can complain afterwards. If you're going to give the robot
[756.20s -> 760.44s]  the worst possible goal and the robot knows that you're going to give it the worst possible
[760.44s -> 764.60s]  goal, which goals should it practice during training? How should it construct that training
[764.60s -> 772.68s]  distribution? Take a moment to think about those. So it turns out that we can show that the best
[772.68s -> 776.36s]  distribution to use during training, if you believe that you're going to get an adversarially
[776.36s -> 782.76s]  chosen goal, is actually the uniform distribution. And this is actually a very simple result that
[782.76s -> 788.36s]  follows from classical maximetric modeling results. It's very simple and it's described
[788.36s -> 791.96s]  in this paper linked at the bottom of the slide called unsupervised meta-learning for
[791.96s -> 799.32s]  reinforcement learning. So what this means is that in the absence of any knowledge about what
[799.32s -> 805.16s]  you're going to get, maximizing the entropy of your goals or the entropy of your states
[805.16s -> 808.68s]  is kind of the best thing you can do, because if you don't know what kind of goal you'll get
[808.68s -> 812.76s]  at test time, the only thing you can really assume is that it might be the worst case goal.
[812.76s -> 817.40s]  So if you want to make the worst case as good as possible, go for uniformly distributed
[817.40s -> 822.52s]  goals if you can. And that's kind of the justification for doing this uniform coverage business.
[825.08s -> 831.80s]  And the provably efficient maximetric exploration paper also discusses this point a fair bit.
