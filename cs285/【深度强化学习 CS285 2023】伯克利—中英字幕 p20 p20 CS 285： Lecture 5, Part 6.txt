# Detected language: en (p=1.00)

[0.00s -> 4.88s]  Alright, in the last portion of today's lecture, I'm going to discuss advanced
[4.88s -> 9.32s]  policy gradients. This material will go by a bit faster than the rest of the
[9.32s -> 14.04s]  lecture, so if it's a little hard to follow, don't worry, please ask some
[14.04s -> 17.88s]  questions in the comments and we can discuss it more in class. We will also
[17.88s -> 21.88s]  have an entire other lecture later on in the course on even more advanced
[21.88s -> 26.64s]  policy gradients materials. So the particular issue that I want to discuss
[26.64s -> 30.80s]  is a numerical issue that afflicts policy gradients, particularly in
[30.80s -> 35.92s]  continuous action spaces. To illustrate this issue, let me first describe an
[35.92s -> 41.88s]  example problem. Let's say that you have a one-dimensional state space. So your
[41.88s -> 46.40s]  state is essentially a number line and your goal is to reach the state s
[46.40s -> 50.64s]  equals 0. You also have a one-dimensional action space. So let's
[50.64s -> 53.64s]  say that you're located at the state and your actions can take you left or
[54.64s -> 60.36s]  right. Your reward is negative s squared minus a squared, so you get a penalty
[60.36s -> 64.60s]  based on squared distance from s0 and you also get a penalty for taking
[64.60s -> 70.42s]  large actions. Your policy is going to be univariate and normally
[70.42s -> 75.88s]  distributed with just two parameters. One parameter k multiplies the state, so
[75.88s -> 80.20s]  your mean is linear in the state, and the other parameter determines your
[80.20s -> 86.08s]  variance, sigma. So you have k and sigma as your policy parameters theta. So you
[86.08s -> 89.20s]  can think of the policy as basically a little Gaussian centered at your current
[89.20s -> 94.44s]  location and your action is k times your current state. So you're going to
[94.44s -> 99.96s]  take this kind of noisy walk, hopefully towards the goal at s equals 0. Now the
[99.96s -> 103.40s]  convenient thing with having a two-dimensional parameter space is that
[103.40s -> 107.36s]  we can actually visualize the entire vector field corresponding to the
[107.36s -> 112.12s]  gradient at all locations in the parameter space. This figure is borrowed
[112.12s -> 115.60s]  from an excellent paper by Peters and Schall, which I'm going to cite at the
[115.60s -> 120.20s]  end of this portion of the lecture. The little blue arrow here shows the
[120.20s -> 125.08s]  gradient, normalized to be unit length. The horizontal axis is the first
[125.08s -> 131.20s]  parameter, k, and the vertical axis is the second parameter, sigma. The optimal
[131.20s -> 136.34s]  setting for the parameters is k equals negative 1 and sigma equals 0, so it's in
[136.34s -> 139.70s]  the middle at the bottom of this plot. But one of the things you might notice
[139.70s -> 143.86s]  from looking at this plot is that the arrows don't actually point towards the
[143.86s -> 148.74s]  optimum. The reason for this is that as sigma gets smaller and smaller, the
[148.74s -> 153.66s]  gradient with respect to sigma gets larger and larger. If you look at the
[153.66s -> 157.62s]  form for the for the Gaussian probability, you'll notice this simply
[157.62s -> 161.80s]  because the probability tracks as 1 over sigma squared. So when you take the
[161.80s -> 166.70s]  derivative, you get a 1 over sigma to the fourth term, which means that as sigma
[166.70s -> 171.28s]  gets smaller, the derivative gets larger. The derivative with respect to k is still
[171.28s -> 175.52s]  there, but the derivative with respect to sigma is comparatively much larger, so
[175.52s -> 180.36s]  that when we renormalize the gradient, the sigma portion completely dominates
[180.36s -> 185.44s]  as sigma gets smaller. So that means that if we follow this gradient, it's going
[185.44s -> 188.98s]  to take us a very, very long time to reach the optimal parameter setting
[188.98s -> 194.58s]  because we'll spend all of our time just reducing sigma. Now those of you
[194.58s -> 198.46s]  that are familiar with numerical methods will probably recognize this as an
[198.46s -> 203.38s]  issue of poor conditioning. The intuition is that this is essentially
[203.38s -> 208.50s]  the same problem as optimizing, let's say, a quadratic function where the
[208.50s -> 213.14s]  eigenvalues of the corresponding matrix have a very large ratio. So if
[213.14s -> 216.54s]  you have a quadratic function with some eigenvalues that are very large and
[216.58s -> 220.26s]  some that are very small, then first-order gradient descent methods are really going to
[220.26s -> 223.30s]  struggle in this kind of function. This is essentially the same type of
[223.30s -> 228.98s]  issue. Now again, if you have some background in numerical methods, at this
[228.98s -> 231.50s]  point you might also be thinking, well, if the problem is poor
[231.50s -> 235.10s]  conditioning, can we solve that problem by using a preconditioner? And
[235.10s -> 238.82s]  the answer is yes, and in fact what we're going to describe next could be
[238.82s -> 243.22s]  viewed as a preconditioner, but we're going to actually discuss it from a
[243.22s -> 247.02s]  slightly different perspective, from the perspective of the dependence of your
[247.02s -> 253.14s]  gradient on parameters. So what I'm going to discuss next is how we can arrive at
[253.14s -> 257.78s]  a covariant or natural policy gradient. So here's the picture from the
[257.78s -> 263.30s]  previous slide. When we take a gradient step via policy gradient, when we take a
[263.30s -> 268.54s]  gradient descent step, choosing the step size for this type of gradient can be
[268.54s -> 273.54s]  very delicate, because some parameters affect the policy distribution a lot and
[273.54s -> 278.98s]  some don't affect very much. So it's very hard to pick a single step size
[278.98s -> 282.74s]  that works well both for k and for sigma, because the derivative with respect to
[282.74s -> 286.94s]  sigma is going to get really really really large, whereas the one for k won't.
[286.94s -> 292.46s]  So what's really going on here is that different parameters affect the
[292.46s -> 296.54s]  policy to different degrees. Some parameters change the probabilities a lot,
[296.54s -> 300.22s]  others don't change it very much, but you want all of the parameters to reach
[300.22s -> 303.66s]  their optimal value, so intuitively what you would like to do is to essentially
[303.66s -> 307.10s]  have larger learning rates for those parameters that don't change the policy
[307.10s -> 311.86s]  very much and smaller learning rates for those that change it a lot. If we want
[311.86s -> 314.58s]  to view this a little bit more mathematically, one of the things we
[314.58s -> 319.66s]  can do is look at the constrained optimization view of first-order
[319.66s -> 324.46s]  gradient ascent. So first-order gradient ascent can be viewed as iteratively
[324.50s -> 328.74s]  solving the following constrained optimization problem. Take the argmax
[328.74s -> 332.58s]  with respect to theta prime, that's the new parameter value, where the objective
[332.58s -> 336.58s]  is the first-order Taylor expansion of your original objective, that's given by
[336.58s -> 341.06s]  theta prime minus theta times grad J, and you have a constraint that says
[341.06s -> 346.38s]  that theta prime minus theta squared should be small. So it's like saying
[346.38s -> 349.94s]  within an epsilon ball around your current parameter value, find the
[349.94s -> 355.10s]  parameter value that maximizes the linearization of your objective. That's
[355.10s -> 358.14s]  essentially what first-order gradient descent is doing, and you can think of
[358.14s -> 363.70s]  alpha as basically the Lagrange multiplier for that constraint. So those
[363.70s -> 366.10s]  of you that have studied mirror descent or projected gradient descent
[366.10s -> 370.70s]  would probably recognize this equation. Usually we pick alpha rather than epsilon,
[370.70s -> 374.78s]  but alpha is basically just Lagrange multiplier that corresponds to epsilon.
[374.78s -> 378.46s]  So what this means is that when we do first-order gradient ascent, we're
[378.50s -> 382.74s]  finding the best value for theta prime within an epsilon ball, but that epsilon
[382.74s -> 389.34s]  ball is in theta space. Now our linearized objective is valid in only a
[389.34s -> 393.42s]  small region around our current policy, that's why we can't use very large
[393.42s -> 399.66s]  step sizes. But that region is very awkward to select if you have to
[399.66s -> 402.50s]  select it in parameter space because some parameters will change the policy
[402.50s -> 406.90s]  a lot and some will change it very little. So intuitively what we'd like to do
[406.90s -> 410.30s]  is we would like to somehow re-parameterize this process so that our
[410.30s -> 415.30s]  steps are of equal size in policy space rather than parameter space, which would
[415.30s -> 418.54s]  essentially rescale the gradient so that parameters that change the policy
[418.54s -> 422.08s]  a lot get smaller rates, parameters that change the policy very little get
[422.08s -> 426.62s]  larger rates. So this is basically the problem, this controls how far we go,
[426.62s -> 431.86s]  and it's basically in the wrong space. So can we rescale the gradient so this
[431.86s -> 436.46s]  doesn't happen? What if we instead iteratively solved this problem,
[436.46s -> 440.22s]  maximize the linearized objective, but subject to a constraint that the
[440.22s -> 444.98s]  distributions don't change too much. So here D is some measure of divergence
[444.98s -> 448.74s]  between pi theta prime and pi theta, and we'd like that divergence measure
[448.74s -> 452.72s]  to be less than or equal to epsilon. So we'd like to pick some
[452.72s -> 456.10s]  parametrization-independent divergence measure, a divergence measure that
[456.10s -> 458.96s]  doesn't care about how you're parametrizing your policy, just which
[458.96s -> 463.80s]  distribution it corresponds to. A very good choice for this is the KL divergence.
[463.80s -> 467.40s]  The KL divergence is a standard divergence of Bregman divergence on
[467.40s -> 471.20s]  distributions. I won't go into too much detail about how KL divergences are
[471.20s -> 474.74s]  defined or derived, just that it's a measure of divergence on distributions,
[474.74s -> 477.96s]  and it is parameter-independent, meaning that no matter how you parametrize
[477.96s -> 484.96s]  your distributions, the KL divergence will remain the same. Now the KL
[484.96s -> 488.12s]  divergence is a little complicated to plug into this kind of constrained
[488.12s -> 490.84s]  optimization. One, that constrained optimization should be very simple because we're
[490.84s -> 494.72s]  going to do that at every step of our gradient ascent procedure. But if we
[494.72s -> 498.36s]  take the second-order Taylor expansion of the KL divergence around
[498.36s -> 503.12s]  the point theta prime equals theta, then the KL divergence can be expressed
[503.12s -> 508.16s]  as approximately as a quadratic form for some matrix F, right? That's just
[508.16s -> 513.22s]  what a second-order Taylor expansion is. And it turns out that F is equal to
[513.22s -> 517.00s]  what's called the Fisher information matrix. The Fisher information matrix is
[517.00s -> 522.72s]  the expected value under pi theta, that's your old policy, of grad log pi times
[522.72s -> 527.08s]  grad log pi transpose. So it's the expected value of the outer product of
[527.08s -> 532.36s]  the gradient with itself. Now notice that the Fisher information matrix is
[532.36s -> 535.44s]  an expectation under pi theta, which should immediately suggest that we
[535.44s -> 540.44s]  can approximate it by taking samples from pi theta and actually trying to
[540.44s -> 545.20s]  estimate this expectation, and that's in fact exactly what we're going to do.
[547.12s -> 553.80s]  So now we've arrived at this formulation for our covariant policy gradient. At every
[553.80s -> 556.84s]  single step of our optimization, we maximize the linearized objective,
[556.84s -> 561.24s]  subject to this approximate divergence constraint, which is just the
[561.24s -> 566.72s]  difference between theta prime and theta under the matrix F. So it's just like
[566.72s -> 570.88s]  what we had before, theta prime minus theta, only before it was under the
[570.88s -> 577.24s]  identity matrix, and now it's under the matrix F. And if you actually write down
[577.24s -> 581.72s]  Lagrangian for this and solve for the optimal solution, you'll find that the
[581.72s -> 587.72s]  solution is just to set the new theta to be theta plus alpha, where alpha is
[587.72s -> 593.44s]  the Lagrange multiplier, of F inverse times grad theta j theta. So before we had
[593.44s -> 597.88s]  theta plus alpha grad theta j theta, now we have theta plus alpha F inverse
[597.88s -> 605.36s]  grad theta j theta. So F is basically our preconditioner now. And it turns out
[605.36s -> 610.92s]  that if you apply this F inverse in front of your gradient, then your vector
[610.92s -> 614.72s]  field changes in a very nice way. So the picture on the right shows what
[614.72s -> 618.20s]  you get by using this so-called natural gradient, and now you can see
[618.20s -> 621.88s]  that the red lines actually very nicely point towards the optimum, and that
[621.88s -> 625.40s]  means that you can converge a lot faster, and also you don't have to work
[625.40s -> 631.36s]  nearly as hard at tuning your step size. Now there are a number of
[631.36s -> 635.68s]  algorithms that use this trick. The classical one, natural gradient or
[635.68s -> 640.28s]  natural policy gradient, selects alpha. A more modern variant called
[640.28s -> 644.84s]  trust region policy optimization selects epsilon and then derives alpha.
[644.84s -> 650.08s]  So the way that you derive alpha is by solving for the optimal alpha, while
[650.08s -> 654.28s]  at the same time while solving for F inverse grad theta j theta. We won't go
[654.32s -> 659.12s]  into how to do this, but the high-level idea is that by using conjugate gradient
[659.12s -> 663.64s]  you can actually get both alpha and the natural gradient simultaneously. So for
[663.64s -> 666.48s]  more details on that, you can check out the paper called trust region
[666.48s -> 671.00s]  policy optimization. The takeaway from all of this is that the policy
[671.00s -> 675.08s]  gradient can be numerically very difficult to use because different
[675.08s -> 678.16s]  parameters affect your distribution to very different degrees, and you can
[678.16s -> 681.40s]  address this by using the natural gradient, which simply requires
[681.60s -> 685.72s]  multiplying your gradient by F inverse, where F inverse is an estimate of the
[685.72s -> 689.08s]  Fisher information matrix, and you can do this efficiently by using conjugate
[689.08s -> 695.24s]  gradient. All right, a few notes on advanced policy gradient topics. What
[695.24s -> 699.24s]  more is there? Well, next time we'll talk about actor-critic algorithms,
[699.24s -> 703.52s]  where we'll introduce value functions and Q functions, and talk about how
[703.52s -> 707.72s]  those can further decrease the variance of the policy gradient, and then later
[707.72s -> 710.96s]  in the class we'll talk more about natural gradient, automatic step size
[710.96s -> 717.08s]  adjustment, and trust regions. For now, let me briefly go over some papers
[717.08s -> 721.32s]  that actually use policy gradients in interesting ways. This is a paper
[721.32s -> 726.84s]  actually by myself and Glendon Colton from 2013 that used an off-policy
[726.84s -> 731.24s]  version of policy gradient to incorporate examples. So here example
[731.24s -> 734.36s]  demonstrations were incorporated using importance sampling, but unlike
[734.36s -> 737.60s]  imitation learning, the policy wasn't just trying to copy the examples, it was
[737.60s -> 741.28s]  actually trying to do better than those examples by using policy gradients,
[741.28s -> 745.92s]  and this used neural network policies for some locomotion tasks. Here are some
[745.92s -> 749.88s]  videos from the trust region policy optimization paper. So this paper used a
[749.88s -> 753.92s]  natural gradient with automatic step size adjustment, and with both
[753.92s -> 757.04s]  continuous and discrete actions, and there was some code available for this
[757.04s -> 762.32s]  if you want to check that out from a paper from 2016 by Rocky Dorn. If you
[762.32s -> 764.72s]  want to read more about policy gradients, here are some suggested
[764.72s -> 768.96s]  readings. The classical papers, reinforced, was introduced in this paper
[768.96s -> 774.92s]  by Williams in 1992. This paper by Baxter and Bartlett introduced the, what I
[774.92s -> 778.76s]  called the causality trick in the lecture. They called it GPOMDP. This is
[778.76s -> 781.92s]  actually not the first paper to introduce it. I'll actually mention the first paper
[781.92s -> 786.40s]  when I talk about actor-critic in the next lecture. And this paper by
[786.40s -> 789.40s]  Peters and Schall describes the natural gradient trick with some very
[789.40s -> 795.16s]  nice illustrations. Deep RL papers that use policy gradients, the guided policy
[795.16s -> 799.24s]  search paper that I mentioned before, which used important sampled policy
[799.24s -> 804.12s]  gradients. This is the trust region policy optimization paper, and then the
[804.12s -> 807.40s]  the PPO paper. So these would be ones to check out if you're interested in
[807.40s -> 811.04s]  policy gradients for deep RL.
