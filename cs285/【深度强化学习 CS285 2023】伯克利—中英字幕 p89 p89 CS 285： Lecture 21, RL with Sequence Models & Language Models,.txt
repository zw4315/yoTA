# Detected language: en (p=1.00)

[0.00s -> 5.38s]  All right, so in the previous section, we talked about how we can use sequence models
[5.38s -> 8.18s]  to help handle RL with partial observability.
[8.18s -> 11.30s]  In the next section, we're going to go the other way, and we're going to discuss how
[11.30s -> 16.54s]  RL can help us train better sequence models, specifically for modeling language.
[16.54s -> 19.42s]  And in the third portion of the lecture, we'll actually put these together, and
[19.42s -> 22.58s]  we'll have both partial observability and language models.
[22.58s -> 27.06s]  Okay, so what are language models, and why should we care about them?
[27.06s -> 33.98s]  Well, a language model at its basic level is a model that predicts next tokens.
[33.98s -> 37.78s]  You can roughly think of tokens as words, although in reality they're not words,
[37.78s -> 41.10s]  they're more like combinations of characters.
[41.10s -> 44.34s]  It's actually a little complex as to what a token is, but roughly speaking,
[44.34s -> 50.62s]  it's some granular representation of natural language.
[50.62s -> 54.02s]  Typically, we use transformers for language models, and the way this works
[54.02s -> 61.44s]  is we take our sequence of tokens, x0, x1, x2, x3, at every position we have a little
[61.44s -> 65.98s]  encoder that encodes discrete tokens into a continuous space, along with an encoding
[65.98s -> 71.22s]  of their place in the sequence, basically their place in the sequence as an integer,
[71.22s -> 76.06s]  0, 1, 2, 3, 4, and those are encoded into a continuous representation, which is then
[76.06s -> 80.56s]  passed to what is called a masked self-attention layer, which is essentially a transformer
[80.56s -> 86.68s]  that can produce a representation at each position, conditioned on representations
[86.68s -> 90.88s]  of previous time steps, that's what the masking refers to.
[90.88s -> 97.00s]  Those are then transformed with some per-position nonlinear transformations, and this self-attention
[97.00s -> 101.60s]  block is repeated some number of times, and that's essentially what a transformer is.
[101.60s -> 107.96s]  And at the end, at every position, we read out a distribution over the token to predict,
[107.96s -> 112.96s]  which is basically just a softmax, and then we predict the next token.
[112.96s -> 117.88s]  So at the first time step, we read in x0 and p0, and then we predict, we make a prediction
[117.88s -> 123.56s]  about x1, and if we're decoding, then, for example, we would start off with some token
[123.56s -> 129.90s]  like the word I, we decode some word like, like, that word is then used as the conditioning
[129.90s -> 133.88s]  information for the second time step, you make a prediction about the next one, I
[133.88s -> 139.56s]  like omdp solvers, and there you get a decoding, and at the end, the model outputs an end-of-sequence
[139.56s -> 144.56s]  token to indicate that it's done generating this particular generation.
[144.56s -> 147.04s]  So that's basically a transformer language model.
[147.04s -> 150.24s]  Now, for the purpose of this course, you don't really need to know anything about
[150.24s -> 155.76s]  how the transformer works, so you've got to simplify this diagram to essentially be
[155.76s -> 161.80s]  some kind of box, which we call a transformer, that sequentially reads in tokens and predicts
[161.80s -> 163.22s]  next tokens.
[163.22s -> 170.70s]  So at every time step, it's modeling the distribution p of xt given x1 through t-1,
[170.70s -> 177.10s]  and by repeatedly sampling from that distribution, you end up with a sentence like, I like omdp
[177.10s -> 178.62s]  solvers.
[178.62s -> 185.28s]  Notice that this model is not Markovian, so every token depends on all previous tokens.
[185.28s -> 190.54s]  Of course, the widely known CHA GPT system, Bard, Claude, all these things, are examples
[190.54s -> 196.48s]  of language models, and deep down inside, what all these systems are doing is generating
[196.48s -> 202.70s]  language token by token, where you specify the tokens for the prompt, and then it generates
[202.70s -> 206.58s]  the tokens for the response.
[206.58s -> 210.18s]  Now language models are typically trained with supervised learning, in the sense that
[210.18s -> 215.66s]  you give them lots and lots of English text or text in other languages, and then you
[215.66s -> 220.90s]  have them use all of that data to predict the next token, given all the previous tokens.
[220.90s -> 225.64s]  We can also train them with RL if what we want is not to match the distribution in
[225.64s -> 229.74s]  the data, that is, we don't just want them to output the same kind of text that
[229.74s -> 234.14s]  we saw in the training data, but rather we want them to maximize some reward function,
[234.14s -> 237.54s]  and that can be extremely desirable in many settings.
[237.54s -> 238.54s]  Why?
[239.06s -> 246.98s]  Well, for example, you could use RL to get language models to satisfy human preferences,
[246.98s -> 249.82s]  to produce the kind of text that people like to see.
[249.82s -> 253.46s]  You can also use RL to get language models to learn how to use tools, to learn
[253.46s -> 256.28s]  how to call into databases or calculators.
[256.28s -> 260.08s]  You can also use it to train language models that are better at holding dialogues with
[260.08s -> 264.10s]  humans and achieve dialogue goals, and we'll actually discuss all of these, and these are
[264.10s -> 266.38s]  all different than simply matching the training data.
[266.38s -> 271.06s]  These are all things that require RL, rather than just supervised learning.
[271.06s -> 276.44s]  Okay, but in order to be able to apply RL to language models, we do have to answer
[276.44s -> 278.40s]  some questions.
[278.40s -> 283.58s]  What is the MDP or POMDP that corresponds to the language generation task?
[283.58s -> 288.98s]  An MDP is determined by states, actions, rewards, and transition probabilities, and
[288.98s -> 293.54s]  we have to choose what these things are for our language generation task.
[293.54s -> 299.02s]  Now there's some obvious intuition, like if you're generating language tokens, probably
[299.02s -> 303.52s]  your actions have something to do with language tokens, and if your goal is to maximize user
[303.52s -> 307.18s]  preferences, then your rewards probably have something to do with user preferences,
[307.18s -> 312.06s]  but actually getting those details right has a few interesting design decisions.
[312.06s -> 313.42s]  So what is the reward?
[313.42s -> 315.06s]  And also what algorithms should we use, right?
[315.06s -> 319.42s]  So we learned in the previous section that certain algorithms handle partial observability.
[319.42s -> 323.46s]  Some of them are, in previous lectures we saw, some of them are good for off-policy,
[323.46s -> 327.46s]  some are good for on-policy, so we have to make some choices.
[327.46s -> 333.98s]  So let's talk about some of those choices, and we'll start with RL training of language
[333.98s -> 338.06s]  models for what are sometimes referred to as single-step problems, which is the most
[338.06s -> 340.94s]  widely used application of RL for language models.
[340.94s -> 343.14s]  That's how ChantJPT, for example, is trained.
[343.14s -> 346.86s]  And then in the next section, we'll talk about multi-step problems.
[346.86s -> 349.14s]  So here's a basic formulation.
[349.18s -> 355.98s]  We have some prompt, maybe the prompt is, like, what is the capital of France?
[355.98s -> 359.38s]  And the transformer makes predictions.
[359.38s -> 367.90s]  Now it's not actually predicting the tokens of the prompt, but that is still part of
[367.90s -> 368.90s]  its training data.
[368.90s -> 370.54s]  What it is predicting is the completion.
[370.54s -> 376.02s]  So it's going to predict, like, maybe the word Paris, and during generation that gets
[376.02s -> 380.50s]  fed in as the input at the next time step, and then it predicts end of sequence.
[380.50s -> 384.46s]  So in most applications, the language model is going to complete a sentence rather than
[384.46s -> 389.58s]  generate something from scratch, and the prefix that is provided, that's the prompt,
[389.58s -> 394.38s]  and then the completion is the desired output.
[394.38s -> 397.22s]  So we're going to say that maybe a basic formulation is that the completion is our
[397.22s -> 398.22s]  action.
[398.22s -> 401.82s]  So A is represented by the two tokens Paris and end of sequence, and in general this
[401.82s -> 403.90s]  could be a variable number of tokens.
[403.90s -> 407.94s]  And the prompt or prefix or context is the state S.
[407.94s -> 412.90s]  So our language model is representing P of A given S.
[412.90s -> 416.82s]  Now the way it's representing it is by a product of probabilities at every time step.
[416.82s -> 420.58s]  Since it's not generating x1, x2, and x3, and x4, that's the prompt.
[420.58s -> 424.42s]  It's only generating x5 and x6, so the probability of A given S is given by the probability
[424.42s -> 430.34s]  of x5 given x1 through 4, and the probability of x6 given x1 through 5, and I've separated
[430.34s -> 435.70s]  x1 through 4 from x5 because x5 is really the previous time step of the action, whereas
[435.70s -> 440.10s]  x1 through 4 is the state.
[440.10s -> 445.06s]  So pi of A given S is essentially our policy pi theta.
[445.06s -> 449.18s]  Now something to note here is that there are now two notions of time step, and this
[449.18s -> 451.06s]  is actually super confusing.
[451.06s -> 456.74s]  The time step x1, you know, 1, 2, 3, 4, 5, 6, those are the time steps for the language
[456.74s -> 459.10s]  generation for the transformer.
[459.10s -> 462.06s]  As far as the RL algorithm is concerned, there's only one time step.
[462.06s -> 465.14s]  You observe one state, and you create one action.
[465.14s -> 470.10s]  So this is confusing because now in regular RL, the time step always meant the same thing.
[470.10s -> 472.06s]  Now there's actually two kinds of time steps.
[472.06s -> 475.10s]  There's the language time step, and then there's the RL time step, and they are not
[475.10s -> 476.42s]  necessarily the same.
[476.42s -> 479.34s]  So for RL purposes, there's really only one time step here.
[479.34s -> 480.34s]  It is a bandit problem.
[480.34s -> 482.98s]  It is a one-step MDP.
[482.98s -> 488.38s]  As far as language generation is concerned, there are many time steps.
[488.38s -> 492.82s]  So now we've defined time steps, we've defined actions, we've defined states, and we've
[492.82s -> 497.62s]  defined our policy, our policy probabilities represented by a product of the probabilities
[497.62s -> 502.44s]  of the language time steps for all of the completion steps.
[502.44s -> 506.58s]  Now we can define our objective, which is to maximize the expected reward of the policy,
[506.58s -> 509.34s]  just like in regular RL.
[509.34s -> 513.94s]  And this makes it a basic one-step RL problem that is a bandit.
[514.58s -> 520.34s]  Okay, so let's start with using the simplest RL algorithms, which is policy gradient.
[520.34s -> 521.34s]  So this is our objective.
[521.34s -> 525.86s]  We're going to take its gradient, and we'll use exactly the formulas from the
[525.86s -> 529.70s]  policy gradient lecture, so we know that the gradient of the expected reward is the
[529.70s -> 533.38s]  expectation of grad log pi times R.
[533.38s -> 538.10s]  Now we saw before that pi was just a product of the probability of all the tokens in the
[538.10s -> 542.78s]  completion, so when we take the gradient of the log of pi, that's just the sum of
[542.82s -> 546.74s]  the gradients of the log probabilities of all the completion tokens, okay?
[546.74s -> 549.78s]  So that's pretty straightforward because these are exactly the kinds of gradients that you
[549.78s -> 555.42s]  compute when you do a backward pass from the cross-entropy loss.
[555.42s -> 562.14s]  And of course we can estimate this with samples, so if we use a standard reinforce
[562.14s -> 566.06s]  estimator, then the samples need to come from pi theta, so you would actually sample
[566.06s -> 570.58s]  a completion from your language model, you would actually tell it what is the capital
[570.58s -> 576.42s]  of France, ask it to generate a completion, it would generate Paris end of sequence,
[576.42s -> 579.78s]  and then you would evaluate the reward of that sample and use that as part of your
[579.78s -> 581.82s]  gradient estimator.
[581.82s -> 586.18s]  You can also use an important sampled estimator, where you might generate completions from
[586.18s -> 591.30s]  some other policy, and then use an importance weight to get a gradient for your current
[591.30s -> 592.30s]  policy.
[592.30s -> 595.50s]  And the samples can come from some other policy, pi bar.
[595.50s -> 599.58s]  Pi bar could, for example, be a supervised training model.
[599.58s -> 603.78s]  The first estimator is a reinforced style estimator, the second one is an importance
[603.78s -> 606.66s]  weighted estimator, such as PPO.
[606.66s -> 610.98s]  The second class is a lot more popular for language models.
[610.98s -> 617.62s]  You can take a moment to think about why that is.
[617.62s -> 622.06s]  So the reason why the importance weighted estimators are much more popular for language models
[622.06s -> 626.82s]  is that sampling from a language model takes considerable time, and it would be very
[626.82s -> 633.82s]  desirable not to have to generate a sample every single time you take a gradient step.
[633.82s -> 636.42s]  Especially because evaluating the rewards of those samples can be expensive, and we'll
[636.42s -> 637.70s]  talk about that in a second.
[637.70s -> 642.42s]  So in reality, it's often much preferred to generate samples from your language model,
[642.42s -> 646.22s]  evaluate the rewards of those samples, and then take many gradient steps using importance
[646.22s -> 652.06s]  sampled estimators, and then repeat.
[652.06s -> 656.58s]  So a particular algorithm, let's take this importance sampled estimator, let's call it
[656.58s -> 661.90s]  grad hat as a shorthand, and notice that it's a function of theta, pi bar, and a set
[661.90s -> 664.46s]  of samples, ai.
[664.46s -> 668.06s]  The way that you could do this is you could sample a batch of completions for a particular
[668.06s -> 669.06s]  state.
[669.06s -> 671.98s]  In reality you would have many states, but I've written this out for just a single state.
[671.98s -> 675.74s]  You would sample a batch of completions, you would evaluate the reward for each
[675.74s -> 680.94s]  of them, then you would set pi bar to be your previous policy, the one that generated
[680.94s -> 686.02s]  those samples, and then you would have an inner loop where you would sample a mini-batch,
[686.02s -> 690.30s]  and then on that mini-batch you would take a gradient step using grad hat, and then you
[690.30s -> 691.70s]  would repeat this k times.
[691.70s -> 696.00s]  So your batch might be, let's say, a thousand completions, and then your mini-batch might
[696.00s -> 698.46s]  be 64.
[698.46s -> 701.50s]  And then you would take some number of gradient steps, and then every once in a while you'd
[701.50s -> 707.30s]  go back out and generate more samples from your model, set that to pi bar, and repeat.
[707.30s -> 714.06s]  So this is very much the classic importance sample policy gradient, or PPO, style loop,
[714.10s -> 717.94s]  and this is a very popular way to train language models with our own.
[717.94s -> 720.72s]  But one big question with this loop is the reward.
[720.72s -> 726.90s]  So notice that every time we generate a batch of completions from a language model policy,
[726.90s -> 730.18s]  we have to evaluate the reward of each of those completions.
[730.18s -> 731.70s]  Where do we get that?
[731.70s -> 734.62s]  Because typically if we were to train on, let's say, question answering, questions like
[734.62s -> 738.86s]  what is the capital of France, we might have a ground truth data set of answers.
[738.86s -> 743.06s]  But here the policy might generate answers that are not in that data set.
[743.06s -> 746.70s]  So we need it to have a reward function, and that reward function needs to be able
[746.70s -> 751.50s]  to score any possible completion for a given question.
[751.50s -> 758.02s]  So very often when we do this, we want to represent R itself as a neural network.
[758.02s -> 761.98s]  Because we don't just have to figure out that Paris is the right answer, and we should
[761.98s -> 764.58s]  get a reward, let's say, plus one, we also have to figure out what happens when
[764.58s -> 767.94s]  the language model says, oh, it's a city called Paris.
[767.94s -> 770.26s]  Well that's a pretty good answer, like it's correct.
[770.26s -> 773.42s]  It's maybe not as concise, so maybe we give it a slightly lower reward, maybe we say that,
[773.42s -> 776.06s]  oh, that's a 0.9, not a 1.0.
[776.06s -> 778.02s]  It might also say, I don't know.
[778.02s -> 780.78s]  That might not actually be incorrect, like maybe it really doesn't know, but that's
[780.78s -> 784.74s]  a worse answer, so maybe we give it a negative 0.1 or something.
[784.74s -> 788.46s]  And then if it says London, well that's just bad, that should be a minus one.
[788.46s -> 790.58s]  But it's a language model, so it couldn't really say anything, it might also say
[790.58s -> 792.90s]  like, oh, why are you asking such a stupid question?
[792.90s -> 796.30s]  So that maybe is extremely undesirable, and we give that like a minus 10 to get the
[796.30s -> 797.72s]  network to behave itself.
[797.72s -> 800.60s]  So your reward model doesn't just need to know what the right answer is, it needs to
[800.60s -> 805.32s]  also be able to understand how to assign rewards to answers that are only a little
[805.32s -> 810.24s]  bit off, or answers that are extremely different, kind of out of scope of a question.
[810.24s -> 813.76s]  So this is a very kind of open vocabulary kind of problem, so you need actually a
[813.76s -> 816.88s]  very powerful reward model.
[816.88s -> 819.80s]  So what could we do?
[819.80s -> 825.46s]  Well we could take all these potential answers, we might sample them from some language
[825.46s -> 828.50s]  model, maybe we have a supervised training language model to get started, we sample
[828.50s -> 831.34s]  some answers and we give them to humans, and we get humans to generate these
[831.34s -> 836.58s]  numbers. So maybe humans look at these answers, they assign numbers to all of
[836.58s -> 840.34s]  them, that creates a data set consisting of sentences, like what is the
[840.34s -> 843.82s]  capital of France Paris, what is the capital of France stupid question, where
[843.82s -> 847.62s]  the label is the number, and then we take supervised learning and we train a
[847.62s -> 850.10s]  model that basically looks at the sentence and then outputs this number.
[850.10s -> 853.78s]  And that could be a way to train a reward model R psi, I'm going to use a
[853.86s -> 858.14s]  subscript psi now to denote the parameters of this reward model, but then
[858.14s -> 860.86s]  of course the problem is how do people know these numbers? How can people
[860.86s -> 865.42s]  actually assign the number negative 10 to why is this such a stupid question?
[865.42s -> 869.86s]  Maybe some people can do this, maybe you can actually in some settings have
[869.86s -> 875.46s]  a task where there are clear units of correctness, maybe perhaps it's a
[875.46s -> 879.86s]  teaching application and the reward is how correctly the student answered the
[879.86s -> 884.66s]  test, or maybe it's some salesman application where the reward is how much
[884.66s -> 887.38s]  revenue should you make, so in those cases maybe the rewards are very
[887.38s -> 891.46s]  quantitative and people can actually label that. But in cases where it's
[891.46s -> 895.42s]  very subjective, like saying that why is this such a stupid question to be a
[895.42s -> 898.78s]  minus 10 whereas London should be a minus 1, in those cases maybe it's hard
[898.78s -> 903.86s]  for humans to assign clear numerical values to these things. What might be
[903.86s -> 909.02s]  easier for humans is to compare two answers. So if you tell a person the
[909.02s -> 912.66s]  question was what is the capital of France, and you have A and B, A is Paris, B
[912.66s -> 915.66s]  is why is it such a stupid question, it's pretty easy for a person to say
[915.66s -> 920.22s]  oh I prefer A. So a preference might be in some cases easier to express
[920.22s -> 927.62s]  especially when the utility is very subjective. So here's a thought, can we
[927.62s -> 932.02s]  use these kinds of preferences to design reward functions? Now reward
[932.02s -> 935.74s]  functions have to assign a number to a particular answer. Preferences are a
[935.78s -> 940.30s]  function of two answers, so given S, A1, and A2, how likely is a person to
[940.30s -> 946.02s]  prefer A1 over A2? That is a well-defined probability, so if the
[946.02s -> 949.86s]  state is what is the capital of France, the actions are Paris and why
[949.86s -> 954.62s]  is it such a stupid question, the preference A is the label, we could simply model the
[954.62s -> 959.14s]  probability that A1 is preferred over A2, and we can learn that. But since
[959.14s -> 963.22s]  what we want in the end is a reward function, what we can do is not
[963.22s -> 966.78s]  actually train a neural network that predicts whether A1 is preferred over A2,
[966.78s -> 971.02s]  but we can describe this probability as a function of reward. And there's a
[971.02s -> 973.98s]  choice that we have to make here, so one very popular choice that is
[973.98s -> 977.02s]  actually derived from the same mathematical foundations as an axiom
[977.02s -> 980.70s]  entropy inverse RL that we discussed in the IRL lecture, is to model the
[980.70s -> 985.30s]  probability that A1 is preferred over A2 as the exponential of the reward of A1
[985.30s -> 989.34s]  divided by the exponential of the reward of A1 plus the reward of A2. So
[989.34s -> 992.10s]  roughly speaking this means that the probability that A1 is preferred over A2
[992.10s -> 997.14s]  is proportional to the exponential of its reward, which means that if one
[997.14s -> 1000.06s]  reward is clearly better than the other, then that one will definitely be
[1000.06s -> 1002.78s]  preferred, but if the rewards are about equal, then they're about equally likely
[1002.78s -> 1005.54s]  to be preferred. And the reason for the exponential transformation
[1005.54s -> 1008.62s]  mathematically is very similar to what we saw in the MaxEnt IRL lectures,
[1008.62s -> 1013.42s]  I won't go into the math on that, but that's basically the intuition. So now
[1013.42s -> 1016.78s]  the way that we can actually train this is we just maximize the
[1016.78s -> 1021.42s]  likelihood of the preferences expressed by the human on these S A1 A2
[1021.54s -> 1027.46s]  tuples, but where the predictor for that preference is parameterized by our psi
[1027.46s -> 1031.78s]  using this ratio at the bottom of the slide. And then we just take the
[1031.78s -> 1035.22s]  logarithm of that and maximize the logarithm with respect to psi, and
[1035.22s -> 1039.58s]  that's a well-defined supervised learning problem. So that's a way that we
[1039.58s -> 1044.22s]  can get numerical rewards out of pairwise preferences. And you can, by
[1044.22s -> 1047.10s]  the way, extend this pretty easily to cases where the preference is expressed
[1047.10s -> 1051.18s]  over more than two items, so you can show the person four completions and get
[1051.18s -> 1055.26s]  them to say which one they prefer. In that case, you'll have four values
[1055.26s -> 1058.94s]  in the sum in the denominator. You could also take four-way comparisons
[1058.94s -> 1062.26s]  and turn them into all possible pairwise comparisons, and that's also
[1062.26s -> 1065.42s]  another way that you could express this. So you could say, well, if you
[1065.42s -> 1069.06s]  show someone A1, A2, A3, A4, and they prefer A1, then you say A1 is
[1069.06s -> 1072.34s]  better than A2, A1 is better than A3, A1 is better than A4, and turn
[1072.34s -> 1078.70s]  that into three pairwise comparisons. That's also valid. So here's an overall
[1078.70s -> 1082.38s]  method that we can use with this scheme, and this method was described in two
[1082.38s -> 1085.18s]  papers, fine-tuning language models from human preferences and training
[1085.18s -> 1088.70s]  language models to follow instructions with human feedback. These basically are
[1088.70s -> 1093.10s]  the foundation of InstructGPT, ChatGPT, and so on. The overall method is to
[1093.10s -> 1097.26s]  first run supervised training, or typically fine-tuning, to get your
[1097.26s -> 1101.98s]  initial policy pi-theta, and that's a supervised training of a language
[1101.98s -> 1108.62s]  model. And then for each question in your data set, for each S, you would sample
[1108.62s -> 1114.06s]  k possible answers, a, k, from your policy, and construct a data set consisting of
[1114.06s -> 1121.90s]  tuples with a prompt s, i, and k possible answers a, i, 1 through a, i, k for that prompt.
[1122.78s -> 1128.38s]  Then you would get humans to label each of those points, each of those s, a, 1 through a,
[1128.38s -> 1133.66s]  k tuples to indicate which answer they prefer. And then you would use that label data set to
[1133.66s -> 1139.90s]  train r-psi, and then you would update pi-theta using rl with r-psi as the reward.
[1141.90s -> 1147.18s]  And then you would repeat this process some number of times. Now, typically when you do this,
[1147.90s -> 1153.26s]  in step five, you would actually run many steps of policy optimization. So in step five,
[1153.26s -> 1156.62s]  you wouldn't just optimize against that reward once with important sampling, you would actually
[1156.62s -> 1160.70s]  generate samples from pi-theta, optimize, generate more samples, and repeat. So there's actually
[1161.34s -> 1165.42s]  two nested loops here. There's the outer loop where you're generating more samples and asking
[1165.42s -> 1170.14s]  humans to express preferences. Then there is another loop where you're actually running this
[1170.14s -> 1173.90s]  policy gradient, and inside that there's another loop where you're running important sampled
[1173.90s -> 1183.98s]  updates for multiple steps. So that's the overall method. Now, there are some challenges that we
[1183.98s -> 1189.58s]  have to take care of. First, human preferences are very expensive, because that actually involves,
[1189.58s -> 1193.34s]  like, sending a bunch of data out to human labelers. It might take days or even weeks
[1193.34s -> 1198.54s]  to get responses out. Of course, if you have a really nice crowdsourcing system, perhaps
[1199.18s -> 1203.18s]  you'll actually get answers back within hours, but it's still way slower than taking gradient
[1203.18s -> 1208.46s]  steps on a GPU. So you want to minimize how often you send things out for human labeling.
[1209.50s -> 1213.18s]  So in practice, most preference data typically actually comes from the initial supervised
[1213.18s -> 1217.50s]  train model. So even though I wrote this as though it were a loop where you repeatedly
[1217.50s -> 1222.14s]  query more preferences, in reality, the first time you go to step two, you label lots of
[1222.14s -> 1227.66s]  preferences, and then on subsequent tries you have significantly less. In fact, if you want the
[1227.66s -> 1231.18s]  poor man's version of this, you might actually not even have that outer loop, you might just
[1231.18s -> 1237.74s]  do steps one, two, three, four, five once. So human preferences are expensive. You also want
[1237.74s -> 1242.54s]  to take many iterations of RL, including generating new samples for the policy, per each iteration
[1242.54s -> 1248.54s]  of preference gathering. And that actually makes us very much like a model-based RL method.
[1248.54s -> 1253.02s]  So since this is a one-step problem, there's no dynamics model, but there is a reward model.
[1253.02s -> 1257.18s]  That reward model is trained much less frequently, and then many RL updates are made on the same
[1257.18s -> 1261.66s]  reward model. In fact, if you don't have that outer loop and you just do steps one, two,
[1261.66s -> 1265.10s]  three, four, five only once, it's actually an offline model-based RL method.
[1267.34s -> 1269.58s]  So what's the problem with that? Why should we be worried?
[1270.14s -> 1273.58s]  Well, the problem, of course, is what we saw before in the model-based RL discussion.
[1273.58s -> 1279.42s]  The problem is distributional shift. And in RL per language models, that is sometimes referred
[1279.42s -> 1287.26s]  to as over-optimization, which basically means that you exploit the reward model after a while,
[1287.26s -> 1291.50s]  and while the policy initially gets better, later on it gets worse. The other problem is
[1291.50s -> 1296.22s]  that the reward model needs to be very good. So over-optimization is often tackled
[1297.18s -> 1302.94s]  with a simple modification where we simply add a penalty to our expected reward objective that
[1303.66s -> 1308.06s]  penalizes the policy pi theta from deviating from the original supervised policy.
[1308.62s -> 1313.50s]  And the scale divergence can conveniently be written by just adding log probabilities from
[1313.50s -> 1317.26s]  the original supervised trained model to the reward, and then subtracting the log probabilities
[1317.26s -> 1320.62s]  of the current model, which is just an entropy term. So this just changes the reward function.
[1320.62s -> 1325.10s]  You take your reward model and then you add the log probabilities of your original supervised
[1325.10s -> 1329.10s]  trained model and subtract the log probabilities of your current model, and beta is just a
[1329.10s -> 1336.14s]  coefficient. And typically when you do this, you would use a very large reward model, typically
[1336.14s -> 1340.70s]  a large transformer that is itself pre-trained to be a language model, and then fine-tuned to
[1340.70s -> 1344.22s]  output the reward, because the reward model needs to be very good. It needs to be powerful
[1344.22s -> 1349.66s]  enough that it can resist all that optimization exploitation pressure from reinforcement learning.
[1350.06s -> 1355.18s]  Okay, so to recap an overview, we can train language models with policy gradients.
[1355.18s -> 1359.18s]  We typically use important sampling estimators. It's a bandit problem for now, although we will
[1359.18s -> 1363.82s]  make it multi-step in the next section. We can use a reward model, which can be trained
[1364.70s -> 1369.02s]  from human data, and typically we'd actually train it with preferences rather than
[1369.66s -> 1375.82s]  utility labels, using this equation as the probability that the user prefers A1 over A2.
[1375.82s -> 1380.62s]  And this can be more convenient than direct supervision of reward values.
[1381.90s -> 1385.58s]  Now, all this technically ends up being a model-based RL algorithm, because we
[1386.22s -> 1390.70s]  train the reward as essentially a model, and then we optimize for many RL steps against that
[1390.70s -> 1395.42s]  model. It can potentially be an offline model-based RL algorithm if we actually don't get additional
[1395.42s -> 1400.38s]  samples from a policy and send them out for more labeling. There are details to take care of,
[1400.38s -> 1404.86s]  such as minimizing human labeling and over-optimization, and we should use a large reward
[1404.86s -> 1407.58s]  model that is very powerful so they can handle that same.
[1411.18s -> 1414.86s]  And the way that we address over-optimization is typically by adding this locale divergence
[1414.86s -> 1419.26s]  penalty to ensure that the policy doesn't deviate too much from the supervised train model.
