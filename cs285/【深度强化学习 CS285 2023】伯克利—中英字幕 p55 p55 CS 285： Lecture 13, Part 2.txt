# Detected language: en (p=1.00)

[0.00s -> 5.24s]  Alright, so let's talk about some actual exploration algorithms, and for now we'll
[5.24s -> 8.72s]  still be in the multi-armed bandit setting, and we will be concerned with
[8.72s -> 13.88s]  theoretically principled strategies, strategies that theoretically get good
[13.88s -> 21.16s]  regret, good meaning not too far off from actually solving the POMDP. So how
[21.16s -> 25.44s]  can we beat the bandit? How can we minimize this measure of regret? Well, it
[25.44s -> 29.16s]  turns out there are a variety of relatively simple strategies that
[29.20s -> 34.72s]  provably get regret that is optimal in the big O sense, and we can often provide
[34.72s -> 39.52s]  theoretical guarantees on that regret. Now these algorithms are optimal up to a
[39.52s -> 44.24s]  constant factor, so we're going to do kind of a big O thing, but their actual
[44.24s -> 48.20s]  empirical performance could vary, and not all of them perform the same when
[48.20s -> 54.28s]  you actually use them in numerical simulations. The exploration strategies
[54.28s -> 58.08s]  that we will then learn about from more complex MDP domains will then be
[58.08s -> 64.44s]  inspired by these tractable strategies. Okay, so the first one I'll talk about is
[64.44s -> 70.32s]  optimistic exploration. So in optimistic exploration, here's what we're
[70.32s -> 78.64s]  going to do. Normally, if you are just trying to do pure exploitation, one of
[78.64s -> 82.28s]  the ways you could do it is you could estimate for each of your actions
[82.28s -> 86.28s]  the average reward that that action gets. And if you just want to exploit, if
[86.52s -> 90.20s]  you don't care about exploring very well, you could just pick the action that has
[90.20s -> 94.92s]  the largest current average reward. If you're not going to be allowed to update
[94.92s -> 97.60s]  your strategy later, this is kind of the best you can do. So if you're in
[97.60s -> 101.28s]  pure exploitation mode, the optimal thing to do is just keep picking the
[101.28s -> 107.36s]  action that seemed on average to be the best. You could instead construct an
[107.36s -> 112.20s]  optimistic estimate by taking the mean of the reward for that action
[112.20s -> 117.40s]  and adding some constant C times a standard deviation. So what this will do
[117.40s -> 122.04s]  is it'll select actions that have a very high mean or that have a lower
[122.04s -> 125.84s]  mean with a very high standard deviation, meaning actions for which
[125.84s -> 130.36s]  you're really uncertain about what reward you're going to get. So this
[130.36s -> 133.40s]  sigma is some sort of variance estimate. And if you do this, you're kind of
[133.40s -> 136.76s]  being optimistic. You're saying, I would guess that anything that I haven't
[136.84s -> 141.68s]  learned about thoroughly might be good. So if you think it might be
[141.68s -> 145.28s]  good, just try it. If you're certain that it's bad, then don't do it. But if you
[145.28s -> 150.96s]  think it might be good, then try it. The intuition is you try each arm until
[150.96s -> 154.04s]  you're sure that it's not great. Once you're convinced that an arm is bad,
[154.04s -> 161.44s]  then you stop trying it. So it turns out that there are many very tractable
[161.44s -> 165.88s]  ways to estimate this uncertainty that still work very well, and some of them
[165.92s -> 169.52s]  are really, really simple. So one very, very simple way to estimate this
[169.52s -> 176.16s]  uncertainty is to simply add some quantity to your mean that scales as
[176.16s -> 180.08s]  the inverse of the number of times you've pulled that arm. So this
[180.08s -> 184.92s]  particular bonus is the square root of 2 times the natural log of the
[184.92s -> 188.76s]  number of time steps divided by N of A, where N of A is the number of times
[188.76s -> 194.08s]  you've pulled arm A. So intuitively this bonus will decrease as you pull
[194.08s -> 197.60s]  an arm more often, but for arms that you've pulled very rarely, the bonus
[197.60s -> 203.68s]  is very, very large. And the log of T in the numerator is there to
[203.68s -> 207.40s]  basically ensure that you explore less and less as you've taken more
[207.40s -> 213.52s]  steps. And this kind of very simple bonus that you add to your
[213.52s -> 218.20s]  mean turns out to theoretically get O of log T regret, which is
[218.20s -> 222.76s]  provably as good as any algorithm. So O of log T regret is
[222.76s -> 227.60s]  actually the best that you can do asymptotically for a multi-arm bandit,
[227.60s -> 231.60s]  and this algorithm gets that regret. So that's the same big O regret as
[231.60s -> 238.80s]  actually solving the POMDP in general. So that's very nice. This suggests that
[238.80s -> 243.16s]  a very, very simple strategy that simply adds a bonus to
[243.16s -> 246.16s]  arms that you haven't pulled very much ends up getting asymptotically
[246.16s -> 250.00s]  optimal regret. And many of the practical exploration algorithms that
[250.04s -> 253.40s]  we'll learn about for deep RL build on this intuition of optimistic
[253.40s -> 256.96s]  exploration. This is also sometimes called optimism in the face of
[256.96s -> 264.52s]  uncertainty. Okay, another strategy that we can use to explore for bandit
[264.52s -> 270.32s]  problems is what's called probability matching or posterior sampling. So
[270.32s -> 273.44s]  optimistic exploration, like I described before, is a very model-free
[273.44s -> 277.12s]  approach. It's not trying to explicitly model any uncertainty, it's just
[277.12s -> 281.52s]  counting how many times you've pulled each arm. It's asymptotically optimal,
[281.52s -> 284.12s]  but in practice it's not always actually the best method. In practice
[284.12s -> 288.64s]  there are some empirical differences. So one way that you could do
[288.64s -> 292.28s]  exploration as an alternative is to actually do something that is a little
[292.28s -> 296.84s]  closer to that POMDP. So you could actually maintain a belief state over
[296.84s -> 301.76s]  your thetas. So you could say, well, you have this POMDP with states
[301.76s -> 305.92s]  theta, and you're going to maintain a belief over those thetas in some very
[305.92s -> 310.84s]  approximate way. So this is a kind of model of your bandit. This p-hat of theta
[310.84s -> 315.44s]  is a distribution over possible bandits that you think you might have.
[315.44s -> 320.32s]  The posterior sampling or probability matching strategy says the way that you
[320.32s -> 326.12s]  should explore is you sample a vector of thetas from your belief and then
[326.12s -> 330.60s]  pretend that that's the true MDP and take the optimal action. So if you
[330.60s -> 335.28s]  sample a bunch of thetas and then take the action that is best according to
[335.28s -> 340.56s]  that model, then you will either find that you got the right answer, meaning
[340.56s -> 343.84s]  you'll find the model you sample is pretty accurate and you did in fact get
[343.84s -> 347.72s]  the higher order that you expected, or you'll get a counterexample to that
[347.72s -> 351.72s]  model. So if you sample the model and it says that action one is really good,
[351.72s -> 355.92s]  you pull arm one and you find that arm one is actually terrible, now your belief is
[355.92s -> 359.46s]  going to change and the next time around you won't sample that model
[359.46s -> 364.52s]  anymore because it'll have much lower probability. Now this is not nearly as
[364.52s -> 368.44s]  hard as actually solving the POMDP because this strategy doesn't reason
[368.44s -> 371.88s]  about the information that you will gain from actually pulling that arm. In
[371.88s -> 377.08s]  a sense it kind of acts greedily. But it turns out that acting
[377.08s -> 381.80s]  greedily in this way is pretty good. And then of course you update your
[381.80s -> 387.00s]  model and then repeat. So this is called posterior sampling, probability
[387.00s -> 391.24s]  matching, or sometimes it's called Thompson sampling. So if someone says
[391.24s -> 394.72s]  Thompson sampling, what they really mean is maintain a belief over your model,
[394.72s -> 399.32s]  sample a model, pretend that model is the right one, take the optimal action under
[399.32s -> 405.00s]  that model, and then update the model distribution based on what you observed.
[405.00s -> 409.56s]  Now this is much harder to analyze theoretically, but it can work very well
[409.56s -> 413.56s]  empirically. So to learn more about this, check out this paper by Chappelle
[413.56s -> 417.44s]  and Lee called an empirical evaluation of Thompson sampling. And in
[417.44s -> 421.64s]  general, exploration methods based on Thompson sampling are a very large class
[421.64s -> 425.48s]  of exploration methods very commonly studied, both in bandits and in deep
[425.48s -> 430.36s]  reinforcement learning. All right, the third class of methods that we're
[430.36s -> 434.96s]  going to discuss are methods that use some notion of information gain. So
[434.96s -> 440.44s]  these methods are even more explicitly model-based. The idea here is based
[440.44s -> 446.20s]  on something called Bayesian experiment design. So first I'll illustrate
[446.44s -> 450.00s]  Bayesian experiment design in kind of an abstract way, and then I'll relate it to
[450.00s -> 454.96s]  exploration. So let's say that we want to determine some latent variable z.
[454.96s -> 458.32s]  Let's not worry about what z is, we want to, we just want to know its value
[458.32s -> 463.76s]  as accurately as possible. But we can't look at z directly. So z might be
[463.76s -> 467.02s]  maybe the optimal action or the value of the optimal action, some unknown
[467.02s -> 472.80s]  quantity. But we can take actions, and the question is which action should we
[472.80s -> 481.48s]  take to learn about z. So we're going to use h of p-hat of z to denote the
[481.48s -> 485.88s]  current entropy of our z estimate. So this is how uncertain we are about p-hat
[485.88s -> 492.36s]  of z. And then we can use h of p-hat of z given y to be the entropy of our
[492.36s -> 497.56s]  z estimate after some observation y. So if y is informative about z, then this
[497.56s -> 505.60s]  entropy of z given y will be lower than the entropy of z. So y might be the
[505.60s -> 511.32s]  reward that we actually observed. So the lower this conditional entropy is,
[511.32s -> 515.68s]  the more precisely we know z. So intuitively we would like to do things
[515.68s -> 520.64s]  that result in y's for which the conditional entropy of z given y is as
[520.64s -> 530.58s]  low as possible. So information gain is quantified as the difference between the
[530.58s -> 534.76s]  entropy of p of z now and the entropy we get after observing y. The
[534.76s -> 539.20s]  problem is we don't know which y we're going to observe, right? If we knew
[539.20s -> 541.92s]  which y we're going to observe, we would have already observed it and our belief
[541.92s -> 548.48s]  would have changed. So the information gain about z from y is defined as the
[548.48s -> 553.28s]  expected value under our distribution over y of the difference in the entropy
[553.28s -> 557.12s]  of p of z and the entropy of p of z given y. So it's saying you don't know
[557.12s -> 563.12s]  y, but you have some belief about y and you can measure under your
[563.12s -> 567.84s]  belief about y how your entropy over z will change. So this information gain
[567.84s -> 572.00s]  will allow us to quantify how much we want to observe y. If we can choose to
[572.16s -> 579.24s]  observe y, will that tell us a lot about z? Now typically if we're doing some kind
[579.24s -> 582.56s]  of exploration thing, we want this to depend on the action. So we would have
[582.56s -> 588.36s]  the information gain about z from y given some action a, in which case we
[588.36s -> 593.06s]  would make all these distributions conditional on a. So this is how much we
[593.06s -> 598.40s]  learn about z from action a given our current beliefs. So you would use a
[598.40s -> 605.28s]  conditional expectation. So an example algorithm that uses this idea is
[605.28s -> 609.12s]  described in this paper by Rousseau and van Roy, called Learning to Optimize Via
[609.12s -> 613.36s]  Information-Directed Sampling. And the choice they have to make is what do
[613.36s -> 616.40s]  you gain information about and what is the variable that you're going to use,
[616.40s -> 619.68s]  that you're going to actually observe. So they say that the variable that you
[619.68s -> 624.20s]  observe is the reward for action a, and the variable that you want to learn is
[624.36s -> 629.60s]  theta a, meaning the parameters for the model for action a. So when you observe a
[629.60s -> 632.60s]  reward, you don't actually know what distribution that reward comes from, so
[632.60s -> 637.04s]  what you want to learn about is the parameters for that reward for that
[637.04s -> 642.88s]  action, and what you observe is a sample from that distribution. So they
[642.88s -> 649.32s]  define information gain about theta i from observing r a given the action a.
[649.32s -> 654.56s]  So this is the information gain of some action a, and they define this
[654.56s -> 662.44s]  quantity called delta a, which is the expected sub-optimality of some action
[662.44s -> 667.48s]  a. So this is saying under your current belief about the MDP, what is the
[667.48s -> 671.04s]  difference between the optimal action for what you think the model might
[671.04s -> 674.52s]  be and the action that you're currently considering? So this is called delta a.
[674.52s -> 679.88s]  Now crucially, you don't know a star, so delta a is in expectation
[679.88s -> 685.32s]  over your model distribution. So g a is saying how informative is this action,
[685.32s -> 689.84s]  delta a is saying how sub-optimal do you think this action might be, and the
[689.84s -> 693.00s]  intuition will be that you want to take actions that are informative, but
[693.00s -> 696.56s]  you don't want to take actions that are highly sub-optimal. So the
[696.56s -> 700.36s]  particular decision rule that they analyze in this paper, and they show
[700.36s -> 705.56s]  this to be quite good, is delta a squared divided by g of a, and then you
[705.56s -> 709.20s]  take the min of this. So intuitively, we want to choose the least sub-optimal
[709.20s -> 713.16s]  action, but you divide by the information gain. So if the
[713.16s -> 717.96s]  information gain is very, very large, then because you're dividing, you would
[717.96s -> 721.00s]  have a small value, which means that might be the min, even if its
[721.00s -> 727.28s]  sub-optimality is large. So don't bother taking actions if you know that
[727.28s -> 731.24s]  you won't learn anything. So if g of a is very small, then this will blow up this
[731.24s -> 735.12s]  value, but don't take actions if you're sure that they're sub-optimal, because
[735.12s -> 742.16s]  if delta a is extremely small, then you won't take this action either. Okay, so in
[742.16s -> 744.72s]  the, if you want to learn more about the strategy, check out the paper by
[744.72s -> 748.52s]  Russo and Van Roy called Learning to Optimize by Information-Directed Sampling,
[748.52s -> 752.20s]  but the short versions that they show this strategy is also very, very good,
[752.40s -> 757.76s]  although it's a bit more mathematically involved. Alright, so the general themes
[757.76s -> 761.40s]  that we learned about, we learned about upper confidence bound or optimistic
[761.40s -> 766.08s]  exploration, which is when you take the average expected reward for some action,
[766.08s -> 769.80s]  and you add a bonus to it, which scales as the inverse of the number of
[769.80s -> 772.60s]  times you've taken that action, meaning that actions that haven't been taken
[772.60s -> 776.28s]  very often get a really large bonus, and then you're really incentivized to
[776.28s -> 780.76s]  take them more. We learned about Thompson sampling, where you maintain a belief
[781.00s -> 784.68s]  over theta, you sample from that belief, and then you take the optimal action
[784.68s -> 789.32s]  according to that sample, and we learned about information gain, where you learn,
[789.32s -> 793.16s]  where you try to estimate how much information you gain about some quantity
[793.16s -> 797.64s]  of interest z, based on some observation y, given some action a, and then you
[797.64s -> 801.68s]  might want to, for example, gain information about the model using the
[801.68s -> 808.72s]  rewards as your observations. Now most exploration strategies do require some
[808.76s -> 812.08s]  kind of uncertainty estimation, as we saw. So each of these three requires
[812.08s -> 815.84s]  estimating uncertainty, even if you do it somewhat naively, as in the case of
[815.84s -> 819.44s]  UCB, where your uncertainty estimate is simply the number of times you've
[819.44s -> 823.84s]  taken that action. Usually you assume some kind of value to new
[823.84s -> 826.72s]  information, and this is essential because if you don't know where the
[826.72s -> 830.20s]  reward is, kind of the best thing you can do is just say learning stuff
[830.20s -> 834.64s]  is good, because you can't just say explore the maximized reward, because
[834.64s -> 836.76s]  the whole point is that you don't know where the reward is. So usually
[836.76s -> 842.08s]  you have to assume some kind of value to gaining new information. So in the case
[842.08s -> 846.08s]  of optimus, you assume that unknown things are good. In the case
[846.08s -> 849.72s]  of Thompson sampling, you assume that your sample is kind of the ground
[849.72s -> 853.28s]  truth. In the case of information gain, you assume that information gain is
[853.28s -> 857.72s]  desirable. Now these assumptions might seem a little arbitrary, but the reason that
[857.72s -> 860.24s]  we're comfortable making those assumptions is because in these
[860.24s -> 863.12s]  theoretically tractable bandit settings, we can show that the
[863.16s -> 867.24s]  resulting algorithms are provably optimal. We won't be able to show
[867.24s -> 871.60s]  that same thing in the more complex domains I'll talk about next, but
[871.60s -> 874.72s]  we'll sort of have the intuition to guide us from these more
[874.72s -> 879.24s]  principled algorithms. All right, so why should we care about these
[879.24s -> 882.56s]  multi-armed bandit settings? Well, bandits are much easier to
[882.56s -> 886.52s]  analyze and understand, and you can use them to derive foundations
[886.52s -> 889.76s]  for more practical exploration algorithms. And then you can apply
[889.76s -> 893.28s]  these methods to more complex MDPs where those guarantees don't
[893.28s -> 897.12s]  apply. Now there are many other exploration related topics that we
[897.12s -> 900.32s]  didn't cover here. I'll just mention them for completeness.
[900.32s -> 902.72s]  We didn't really talk about contextual bandits. So these are
[902.72s -> 906.40s]  bandits that have a state, essentially a one-step MDP. We
[906.40s -> 909.52s]  didn't talk about optimal exploration small MDPs, so I didn't go
[909.52s -> 913.28s]  very deep on the theory. There's a lot more theory to this.
[913.28s -> 915.92s]  And we didn't really talk about Bayesian model-based reinforcement
[915.92s -> 920.16s]  learning, which is kind of the logical progression from
[920.16s -> 923.60s]  information gain. So you could go sort of full Bayesian and
[923.60s -> 926.32s]  actually make optimal exploration decisions, which
[926.32s -> 928.64s]  are going to get closer to that POMDP setting, and I
[928.64s -> 934.00s]  didn't really talk about that. We also didn't talk about
[934.00s -> 937.60s]  PAC-based exploration, so you can use PAC theory to develop
[937.60s -> 940.40s]  exploration methods that also have some very appealing
[940.40s -> 942.48s]  guarantees. That goes a little too much in depth into
[942.48s -> 944.80s]  theory, but know that this exists, and if you're
[944.80s -> 946.80s]  interested, you can check that out.
