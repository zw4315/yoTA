# Detected language: en (p=1.00)

[0.00s -> 5.44s]  Alright, the remainder of today's lecture will focus on more practical methods that
[5.44s -> 11.56s]  can make behavioral cloning work, as well as some other algorithms that we could use.
[11.56s -> 15.20s]  So we talked about a little bit of theory, but now we'll talk about how the problem
[15.20s -> 18.98s]  can be addressed in a few ways, by being smart about collecting your data, by using
[18.98s -> 23.46s]  very powerful models that make comparatively fewer mistakes, by using multitask learning,
[23.46s -> 25.34s]  and by changing the algorithm.
[25.34s -> 29.24s]  And I'll go through these pretty fast, so my aim with this portion of the lecture
[29.28s -> 33.48s]  is not really to go in great detail about how to actually implement some of these methods,
[33.48s -> 36.98s]  but just to give you a sense for the types of methodologies that people employ.
[36.98s -> 39.84s]  The one method that you will implement in Homework is Dagger, and I'll go through
[39.84s -> 42.84s]  that somewhat more precisely.
[42.84s -> 47.76s]  Okay, so what makes behavioral cloning easy and what makes it hard?
[47.76s -> 52.48s]  As I mentioned in the previous part of the lecture, if you have very perfect data,
[52.48s -> 55.40s]  then these accumulating errors are a big problem because as soon as you make even
[55.40s -> 58.52s]  a small mistake, you're outside of that distribution of perfect data.
[58.52s -> 62.48s]  But if you actually already have a bunch of mistakes in your data set, and corrections
[62.48s -> 67.18s]  for those mistakes, then when you make a small mistake, you'll be in a state that is somewhat
[67.18s -> 72.00s]  similar to other mistakes that you've seen in the data set, and the labels in that
[72.00s -> 75.44s]  portion of the data set will tell you how to correct that mistake.
[75.44s -> 78.32s]  So there are a few ways that you could leverage this insight.
[78.32s -> 82.70s]  You could actually intentionally add mistakes and corrections during data collection.
[82.70s -> 84.78s]  That's not actually an entirely crazy idea.
[84.78s -> 90.24s]  So the mistakes will hurt, meaning that it will dilute the training set, but the corrections
[90.24s -> 94.74s]  will help, and often the corrections help more than the mistakes hurt.
[94.74s -> 98.34s]  The reason for it is that if the mistakes are somewhat random, they tend to average
[98.34s -> 103.42s]  out, meaning that the most optimal action is still the most probable.
[103.42s -> 109.50s]  However, by making mistakes during data collection, you force the expert to provide examples
[109.50s -> 114.26s]  in more diverse states, and that will teach the policy how to make corrections.
[114.26s -> 117.18s]  The simplest version of this that you can think of is if you force the expert to make
[117.18s -> 122.50s]  a mistake with some probability independent of which state they're in, then the mistakes
[122.50s -> 126.34s]  will be largely uncorrelated with the state, whereas the optimal action will be correlated
[126.34s -> 127.34s]  with the state.
[127.34s -> 130.22s]  So when your neural network learns the action that correlates most of the state, it will
[130.22s -> 133.46s]  actually tend to learn the optimal action and avoid the mistakes, but it will still
[133.46s -> 139.54s]  benefit from seeing the corrections in those worse states.
[139.54s -> 143.42s]  Another thing that we could do is use some form of data augmentation, and that camera
[143.58s -> 146.78s]  trick from before can be thought of as a kind of data augmentation, essentially a method
[146.78s -> 151.54s]  that adds some fake data that illustrates corrections, like those side-facing cameras.
[151.54s -> 155.46s]  And that can be done by leveraging some sort of domain knowledge about the problem
[155.46s -> 158.82s]  you're solving to create some additional fake data.
[158.82s -> 162.04s]  And roughly speaking, the effect of these two tricks is kind of the same.
[162.04s -> 166.86s]  In both cases, the aim is to provide examples in states that the expert is unlikely
[166.86s -> 171.58s]  to visit, but that the policy might end up landing in.
[171.74s -> 174.74s]  Now, there isn't really much more to this methodology than that.
[174.74s -> 179.84s]  So in discussing these tricks, I'm going to just show you two examples of previous papers
[179.84s -> 182.78s]  that use tricks like this to good effect.
[182.78s -> 187.22s]  So the first one I'll mention is a data augmentation-based approach in this paper
[187.22s -> 190.60s]  which focused on flying a drone through the forest.
[190.60s -> 195.46s]  So the output action space is discrete, it's just go left, go straight, or turn right.
[195.46s -> 197.98s]  And here's a video from their work.
[197.98s -> 203.00s]  So they're going to fly these drones through hiking trails in Switzerland.
[203.00s -> 207.02s]  This is from the University of Zurich.
[207.02s -> 211.06s]  And the idea is pretty straightforward.
[211.06s -> 216.26s]  So they have a ConvNet, and the ConvNet looks at the image and predicts one of three
[216.26s -> 219.10s]  discrete labels, left, right, and straight.
[219.10s -> 224.22s]  Now these are examples from the training set.
[224.22s -> 225.22s]  So they're labeled.
[225.22s -> 226.66s]  And where do they get the labels?
[227.54s -> 234.50s]  They get the labels by, of course, using lots of machine learning, using lots of hiking trails.
[234.50s -> 237.66s]  But the data collection procedure is actually very straightforward.
[237.66s -> 240.82s]  They didn't actually have humans fly the quadcopter.
[240.82s -> 246.82s]  What they did instead is they got a person to walk the hiking trails.
[246.82s -> 252.18s]  And the person was, let me fast forward here, wearing a funny hat.
[252.18s -> 256.62s]  Their hat had three cameras on it, a forward-facing camera, a left-facing camera, and a right-facing
[256.62s -> 257.58s]  camera.
[257.58s -> 260.22s]  And their approach was actually even simpler than that driving example.
[260.22s -> 264.38s]  They simply assumed that the person would always go in the correct direction, and they
[264.38s -> 268.38s]  labeled the left-facing camera with the action to go right, and the right-facing
[268.38s -> 271.90s]  camera with the action to go left, and the straight-facing camera with the action
[271.90s -> 273.38s]  to go straight.
[273.38s -> 274.38s]  That's it.
[274.38s -> 276.14s]  That is the entirety of the method.
[276.14s -> 279.10s]  So there's no attempt to record the human's actions.
[279.10s -> 280.10s]  And that actually worked pretty well.
[280.10s -> 285.02s]  And I think this is a really nice illustration of how that data augmentation approach can
[285.02s -> 288.54s]  enable imitation learning to work well.
[288.54s -> 291.98s]  And it wouldn't surprise me in the least if, had they actually flown the quadcopter through
[291.98s -> 295.14s]  the forest and only used a forward-facing camera, if their results would have actually
[295.14s -> 298.66s]  been somewhat worse.
[298.66s -> 301.38s]  So this is a similar thing with a handheld camera.
[301.38s -> 305.38s]  Here is their drone.
[305.38s -> 306.38s]  Here's another interesting example.
[306.38s -> 309.22s]  This is a robotic manipulation example.
[309.22s -> 315.46s]  And here, the authors of this paper are using a very low-cost, very cheap and relatively
[315.46s -> 323.34s]  inaccurate arm, and a very simplistic teleoperation system based on a kind of a hand motion detector.
[323.34s -> 327.86s]  And they're teaching the robot various skills, like using a cloth to wipe down a
[327.86s -> 332.26s]  box of screwdrivers, picking up and pushing objects, things like this, and they're using
[332.26s -> 333.26s]  a game controller here.
[333.26s -> 338.66s]  And one of the things that they do is they illustrate a lot of mistakes in their
[339.10s -> 343.38s]  demonstrations, kind of inevitably just because they have such a low-cost and imperfect teleoperation
[343.38s -> 344.38s]  system.
[344.38s -> 347.62s]  And because they illustrate so many mistakes, they actually end up in a situation where
[347.62s -> 351.10s]  the robot, when it makes mistakes, actually recovers from them.
[351.10s -> 356.30s]  So they have some examples where it picks up objects, sometimes it picks them up incorrectly,
[356.30s -> 360.98s]  sometimes a human actually perturbs it, but the robot is actually pretty good at recovering
[360.98s -> 364.22s]  from perturbations, including ones that are introduced by the person.
[364.22s -> 367.18s]  So here the person is messing with it, but the robot just is unworried about that
[367.70s -> 371.94s]  and keeps trying to do the task.
[371.94s -> 379.30s]  So here it has to slide the wrench into a particular spot.
[379.30s -> 384.86s]  So sometimes imperfect data collection can actually be better than highly perfect data collection.
[384.86s -> 391.46s]  Okay, now that trick for getting behavioral cloning to work is not very reliable and
[391.46s -> 396.70s]  it takes a little bit of domain-specific expertise, although it does provide a kind of
[396.74s -> 400.70s]  a guidance, anytime you're collecting data for imitation learning, keep in mind that
[400.70s -> 405.02s]  having ways to put the system in states where the expert can demonstrate corrections
[405.02s -> 408.98s]  can be a very good thing, and it's also worth thinking about data augmentation tricks.
[408.98s -> 413.38s]  But let's talk about some more technical solutions.
[413.38s -> 415.46s]  Why might you fail to fit the expert behavior?
[415.46s -> 419.86s]  Because if you can minimize the number, that value epsilon, perhaps even epsilon
[419.86s -> 421.88s]  t squared might still be a small number.
[421.88s -> 425.74s]  So if you can understand why you might fail to fit the expert behavior, maybe you can
[425.78s -> 429.30s]  get a model that's so powerful that it's probably that the mistakes are so low that even that
[429.30s -> 433.02s]  quadratic cost doesn't actually worry you too much.
[433.02s -> 434.86s]  So why might you fail to fit the expert?
[434.86s -> 440.22s]  Well, one reason is what I'll refer to as non-Markovian behavior.
[440.22s -> 443.78s]  Non-Markovian behavior means that the expert doesn't necessarily choose the action
[443.78s -> 445.50s]  based only on the current state.
[445.50s -> 447.46s]  A second reason is multimodal behavior.
[447.46s -> 452.06s]  That means that the expert takes actions randomly and their distribution over actions
[452.06s -> 454.90s]  is very complex and might have multiple modes.
[454.94s -> 458.50s]  Okay, let's talk about the non-Markovian behavior first.
[458.50s -> 463.86s]  So when we train a policy that is conditional on the current observation, the policy is
[463.86s -> 466.50s]  Markovian in the sense that it assumes that the current observation is the only
[466.50s -> 467.50s]  thing that matters.
[467.50s -> 470.50s]  It's basically assuming that the observation is the state.
[470.50s -> 475.58s]  That's not necessarily a problem if the expert also chose the action based only on
[475.58s -> 477.40s]  the current observation.
[477.40s -> 478.94s]  But humans very rarely do that.
[478.94s -> 482.90s]  Humans can't really make decisions entirely in the moment, completely forgetting everything
[482.90s -> 484.62s]  they saw before.
[484.62s -> 490.70s]  So if we see the same thing twice, if we were perfectly Markovian agents, we would
[490.70s -> 496.58s]  do the same thing twice, regardless of what happened before, and that's pretty unnatural.
[496.58s -> 501.90s]  Oftentimes, humans will base their decision on all of the past things they saw.
[501.90s -> 506.82s]  For example, if the human driver notices something in their blind spot and then looks
[506.82s -> 510.14s]  back on the road, they still remember what they saw in their blind spot.
[510.14s -> 514.18s]  Or maybe even more problematic, if someone just cut them off and they got a little
[514.22s -> 517.54s]  clustered, maybe they'll be driving a little differently for the next few seconds.
[517.54s -> 521.94s]  So generally, humans are actually very non-Markovian in the sense that human behavior is very
[521.94s -> 524.50s]  strongly affected by the temporal context.
[524.50s -> 527.54s]  So if we're training a policy that only looks at the current image, it's unaware
[527.54s -> 532.14s]  of all that context, and it might simply not be able to learn a single distribution
[532.14s -> 535.70s]  that captures human behavior accurately because human behavior doesn't depend on just
[535.70s -> 539.38s]  the current observation.
[539.38s -> 540.98s]  So how can we use the whole history?
[540.98s -> 543.10s]  Well, it's actually pretty straightforward.
[543.10s -> 549.22s]  We just need a policy representation that can read in the history of observations.
[549.22s -> 553.42s]  So we might have a variable number of frames, and if we just simply combine all the frames
[553.42s -> 557.38s]  into one giant image with like 3,000 channels, that might be difficult because we might
[557.38s -> 558.66s]  have too many weights.
[558.66s -> 561.62s]  So what we would typically do is use some kind of sequence model.
[561.62s -> 564.58s]  So we would have our, let's say if we're using images, we would have our convolutional
[564.58s -> 565.58s]  encoder.
[565.58s -> 567.70s]  If you're not using images, you would have some other kind of encoder, and you would
[567.70s -> 572.42s]  encode all the past frames and then feed them through a sequence model such as an LSTM
[572.46s -> 578.54s]  or a transformer, and then just predict the current action based on the entire sequence.
[578.54s -> 581.58s]  Setting up these models is a little bit involved, but there's actually nothing here that is
[581.58s -> 582.98s]  special for imitation learning.
[582.98s -> 587.82s]  So the same way that you might build a sequence model to process, let's say, videos in supervised
[587.82s -> 592.18s]  learning, exactly the same kinds of approaches can be used here, whether it's LSTMs or
[592.18s -> 595.54s]  transformers or something else entirely like temporal convolutions.
[595.54s -> 599.46s]  Again, I won't talk about those architectural details in detail because they're actually
[599.46s -> 601.06s]  not imitation learning specific.
[601.06s -> 604.86s]  So anything that you learned about before for sequence modeling could just as well be
[604.86s -> 607.54s]  used for imitation learning.
[607.54s -> 611.74s]  There is, however, an important caveat that I want to mention, which is that using histories
[611.74s -> 615.34s]  of observations does not always make things better.
[615.34s -> 620.94s]  And the reason that it might sometimes make things worse is that it might exacerbate correlations
[620.94s -> 623.26s]  that occur in your data.
[623.26s -> 625.30s]  This is a little bit of an aside.
[625.30s -> 630.42s]  I don't necessarily expect all of you to kind of know this in detail, but I do think
[630.46s -> 634.38s]  it's an interesting aside, and it's something that perhaps might inspire some ideas for
[634.38s -> 636.02s]  things like final projects.
[636.02s -> 637.82s]  Why might this work poorly?
[637.82s -> 641.22s]  Well, here's a little scenario.
[641.22s -> 646.46s]  Let's say that you have a strange kind of car where there's a dashboard indicator for
[646.46s -> 648.38s]  whether you're pressing the brakes.
[648.38s -> 652.78s]  So whenever you press the brakes, there's a light bulb that lights up inside the cabin.
[652.78s -> 656.66s]  And the camera that is recording your data for imitation learning is inside the cabin,
[656.66s -> 660.90s]  so the camera can see out of the window, and it can also see the brake indicator.
[660.90s -> 665.66s]  So whenever the person steps on the brake, the light lights up.
[665.66s -> 669.62s]  Now in this case, there's a person standing in front of the car, and the driver stepped
[669.62s -> 672.86s]  on the brakes because there was a person there.
[672.86s -> 677.22s]  But what the policy sees in the training data is it'll see one frame where the person
[677.22s -> 683.58s]  is visible, but the brake indicator is not on, and the brake is pressed.
[683.58s -> 689.54s]  And then we'll see many steps after that where the brake indicator is pressed, is lit,
[689.54s -> 690.54s]  and the brake is pressed.
[690.54s -> 696.46s]  So there's a very strong association between the brake indicator and the brake being pressed.
[696.46s -> 700.74s]  If you have, if you're reading in histories, the situation is a lot worse because now
[700.74s -> 702.86s]  you don't even need the brake indicator.
[702.86s -> 706.82s]  When you're reading in histories, just the fact that the brake was pressed in previous
[706.82s -> 709.82s]  time steps is apparent from looking at the sequential images.
[709.82s -> 712.54s]  You see the car slowing down, you know the brake was pressed.
[712.54s -> 719.98s]  So the point is that the action itself correlates with future instantiations of that action.
[719.98s -> 723.66s]  If the information is somehow hidden, then of course the policy is forced to pay attention
[723.66s -> 726.54s]  to the important cues, which is the fact that there's a person, but if these auxiliary
[726.54s -> 732.26s]  cues are present, even though they are not the real cues that led to the action,
[732.26s -> 737.22s]  they serve to confuse the policy as a spurious correlation, as a kind of causal confounding.
[737.22s -> 741.90s]  So the slowing down that you see when you look at history is caused by braking, but
[741.90s -> 745.78s]  the policy might not realize that, it might think that whenever you see the car slowing
[745.78s -> 748.42s]  down, that's an indication that you should brake.
[748.42s -> 753.42s]  In the same way as the brake indicator is the effect, not the cause of braking,
[753.42s -> 758.54s]  but when you see lots of images with that correlation in there, you might get confused.
[758.54s -> 762.26s]  So you can call this causal confusion as discussed a little bit in this paper.
[762.26s -> 763.58s]  There are a few questions we could ask about this.
[763.58s -> 767.62s]  Does including history mitigate causal confusion, or does it make it worse?
[767.62s -> 770.18s]  And I'll leave this as an exercise for you at home.
[770.22s -> 773.42s]  In other exercises, at the end of this lecture, we'll talk about a method called DAGGER.
[773.42s -> 777.26s]  And after we talk about that, I want you to come back to this point and think about
[777.26s -> 781.14s]  whether the method DAGGER will actually address this problem or make it worse.
[781.14s -> 785.34s]  So I'll leave that as an exercise for you to think about.
[787.34s -> 789.94s]  All right, so that's non-Markovian behavior.
[789.94s -> 791.54s]  You can address it by using histories.
[791.54s -> 796.50s]  Keep in mind that that's not unequivocally always a good thing, but if what you're worried
[796.50s -> 799.66s]  about is non-Markovian behavior, that's the thing to do.
[799.66s -> 801.54s]  Now let's talk about the other one, multimodal behavior.
[801.54s -> 803.94s]  That's kind of a subtle one.
[803.94s -> 807.62s]  Let's say that you wanted to navigate around a tree, and maybe you're flying a quadcopter.
[807.62s -> 811.66s]  You can fly around the tree to the left, or you can fly around the tree to the right.
[811.66s -> 814.22s]  Both are valid solutions.
[814.22s -> 817.66s]  The trouble is that at that point when you're in front of the tree, some expert
[817.66s -> 820.38s]  trajectories might involve going left, and some might involve going right.
[820.38s -> 825.66s]  So in general, in your training data, you'll see very different actions from very similar states.
[825.66s -> 831.22s]  Now, this is not a problem if you're doing something like that Zurich paper where they
[831.22s -> 834.94s]  use a discrete action space, left, right, and straight, because you can easily represent
[834.94s -> 839.10s]  a distribution where there's high probability for left, high probability for right, and
[839.10s -> 842.70s]  low probability for straight, because you're directly outputting three numbers to indicate
[842.70s -> 844.94s]  the probability of each of those actions.
[844.94s -> 849.70s]  However, if you're outputting a continuous action, maybe the mean and variance of a
[849.70s -> 854.46s]  Gaussian distribution, now you have a problem, because a Gaussian has only one node.
[854.46s -> 860.62s]  In fact, if you see examples of left and examples of right, and you average them together,
[860.62s -> 864.50s]  that's very, very bad.
[864.50s -> 865.90s]  So how can we address this?
[865.90s -> 867.78s]  Well, we have a few choices.
[867.78s -> 871.98s]  We can use more expressive continuous distributions, so instead of outputting the mean and variance
[871.98s -> 876.38s]  of a single Gaussian, we can output something more elaborate.
[876.38s -> 881.62s]  Or we can actually use discretization, but make it feasible in high dimensional action
[881.62s -> 885.78s]  spaces, and I'll talk about both solutions a little bit next.
[885.78s -> 890.06s]  So first, let's talk about some examples of continuous distributions we can use.
[890.06s -> 894.70s]  And again, I won't go into great detail about each of these methods, so for details
[894.70s -> 897.54s]  about how to actually implement them, I'll have some pointers and references, and I
[897.54s -> 900.06s]  would encourage you to look that up yourself if you want to actually try it.
[900.06s -> 904.86s]  My aim here is mostly to give you a survey-level coverage of the different techniques so
[904.86s -> 908.50s]  that you kind of know the right keywords and the right ideas.
[908.58s -> 915.54s]  Okay, so what we're going for is some way to set up a neural network so that it can
[915.54s -> 920.58s]  output multiple models, for example high probability of left, high probability of right, and low
[920.58s -> 924.10s]  probability of going straight.
[924.10s -> 926.98s]  So we have a few options.
[926.98s -> 932.30s]  A very simple but maybe less powerful option is to use a mixture of Gaussians, and I'll
[932.30s -> 934.86s]  talk about how to set that up with neural nets.
[934.90s -> 939.42s]  A more sophisticated one is to use linked variable models, and then something that has
[939.42s -> 943.74s]  recently become very popular is to use diffusion models, because diffusion models have
[943.74s -> 949.42s]  gotten a lot more effective and a lot easier to train in recent years.
[949.42s -> 952.38s]  But let's start with a mixture of Gaussians, because that is probably the simplest thing
[952.38s -> 956.30s]  to implement, although it's not quite as powerful as the others.
[956.30s -> 958.10s]  So the idea here is the following.
[958.14s -> 967.18s]  A mixture of Gaussians can be described as a set of means, covariances, and weights.
[967.18s -> 969.58s]  Okay?
[969.58s -> 974.46s]  So let's say you have n different Gaussians that you want to output, let's say maybe
[974.46s -> 979.06s]  it's n equals 10, let's say, you're going to output 10 means, 10 covariances, and
[979.06s -> 984.46s]  a weight on each of those 10 mixture elements to indicate how large each of them is.
[984.46s -> 988.70s]  So you probably learn about mixture of Gaussians in the context of something like clustering,
[988.70s -> 994.26s]  where the means and the variances are just vectors and matrices that you learn.
[994.26s -> 998.18s]  Here, everything is conditional in the observation, so your neural network is actually outputting
[998.18s -> 1001.30s]  the means and variances, so they're not numbers that you store, they're actually
[1001.30s -> 1004.30s]  outputs of your neural net.
[1004.30s -> 1009.10s]  Before our output was just the mean and maybe just one covariance matrix, now it's
[1009.10s -> 1013.78s]  maybe going to be 10 vectors of means and 10 covariance matrices and a scalar weight
[1013.82s -> 1016.70s]  on each of those 10 to indicate how large they are.
[1016.70s -> 1019.26s]  But in terms of implementing it, it's actually pretty straightforward.
[1019.26s -> 1023.34s]  All we have to do is code up our neural network so it has all those outputs, write
[1023.34s -> 1030.04s]  down the equation for a mixture of Gaussians, take its logarithm as our training objective,
[1030.04s -> 1032.62s]  and optimize it the same way that we did before.
[1032.62s -> 1036.58s]  So the way that you would implement this in, let's say, PyTorch, is you would literally
[1036.58s -> 1040.60s]  implement the equation for a mixture of Gaussians, take its log, and use that as
[1040.60s -> 1041.82s]  your training objective.
[1041.86s -> 1045.26s]  Just don't forget to put a minus sign in front if you're minimizing it.
[1045.26s -> 1047.14s]  So that's basically the idea.
[1047.14s -> 1050.38s]  Your neural net outputs means, covariances, and weights.
[1050.38s -> 1055.98s]  And modern autodiff tools like PyTorch actually make this pretty easy to implement.
[1055.98s -> 1058.94s]  Of course the problem with the mixture of Gaussians is that you choose a number
[1058.94s -> 1061.54s]  of mixture elements and that's how many modes you have.
[1061.54s -> 1065.54s]  So if you have 10 mixture elements and you want 10 modes, that's fine, but what if
[1065.54s -> 1066.54s]  you have 100 modes?
[1066.54s -> 1069.82s]  What if you have extremely high-dimensional action spaces?
[1069.86s -> 1073.22s]  Plus, you're not driving a car, but you're controlling a humanoid robot with 50 degrees
[1073.22s -> 1076.38s]  of freedom and you want 1,000 different modes.
[1076.38s -> 1080.06s]  Now you have to do something a little smarter.
[1080.06s -> 1084.82s]  Latent variable models provide us a way to represent a much broader class
[1084.82s -> 1085.62s]  of distributions.
[1085.62s -> 1088.86s]  In fact, you can actually show that latent variable models can represent
[1088.86s -> 1092.30s]  any distribution as long as the neural network is big enough.
[1092.30s -> 1096.38s]  The idea behind a latent variable model is that the output of the neural net is
[1096.38s -> 1102.14s]  still a Gaussian, but in addition to the image, it receives another input which is
[1102.14s -> 1105.54s]  sampled from some prior distribution, like a zero-mean unit variance Gaussian.
[1105.54s -> 1107.66s]  So you can think of it as almost like a random seed.
[1107.66s -> 1110.98s]  The random seed is passed into the network and for different random seeds,
[1110.98s -> 1114.02s]  it'll output different modes.
[1114.02s -> 1119.98s]  So for example, if we have a three-dimensional random vector that we put in,
[1119.98s -> 1122.14s]  if we put in this random vector, we get this mode.
[1122.14s -> 1125.22s]  If we put in this random vector, then we get this other mode.
[1125.22s -> 1127.86s]  We have to train the network so that for different random vectors,
[1127.86s -> 1130.06s]  it outputs different modes.
[1130.06s -> 1135.22s]  Now, unfortunately, you can simply naively take the network and feed in
[1135.22s -> 1138.22s]  random numbers and expect it to do this because if you give it random numbers,
[1138.22s -> 1142.10s]  those random numbers aren't actually correlated with anything in the input or output.
[1142.10s -> 1146.50s]  So if you just do this in the most obvious way, the neural net will actually ignore
[1146.50s -> 1147.86s]  those numbers.
[1147.86s -> 1151.42s]  So the trick in training latent variable models is to make those numbers useful
[1151.42s -> 1155.06s]  during training.
[1155.06s -> 1158.98s]  The most widely used type of method for this is what's called the conditional
[1158.98s -> 1160.82s]  variational autoencoder.
[1160.82s -> 1164.18s]  We'll discuss conditional variational autoencoders in great detail much later
[1164.18s -> 1166.22s]  on in the course in the second half.
[1166.22s -> 1169.26s]  So I won't actually describe how to make this work right now,
[1169.26s -> 1174.58s]  but the high-level intuition is during training, the values of those random
[1174.58s -> 1177.74s]  vectors that go into the network are not actually chosen randomly.
[1177.74s -> 1181.90s]  Instead, they're chosen in a smart way to correlate with which mode you're
[1181.90s -> 1182.94s]  supposed to output.
[1182.94s -> 1185.70s]  So the idea is that during training, you figure out this particular training
[1185.70s -> 1188.50s]  example has the left mode, this training example has the right mode,
[1188.50s -> 1191.82s]  and you assign them different random vectors so that the neural net learns
[1191.82s -> 1194.30s]  that it can pay attention to those random vectors because they tell it
[1194.30s -> 1196.26s]  which mode to output.
[1196.26s -> 1197.42s]  That's the intuition.
[1197.42s -> 1200.66s]  The particular technical way of making this work is a little bit more involved
[1200.66s -> 1203.26s]  and requires more technical background, so we'll talk about that in the
[1203.26s -> 1204.50s]  second half of the course.
[1204.50s -> 1207.14s]  But the high-level idea behind latent variable models is that you have an
[1207.14s -> 1210.82s]  additional input to the model and that additional input tells it which mode
[1210.82s -> 1212.90s]  to output.
[1212.90s -> 1215.14s]  And then, of course, at test time, if you want to actually make this work,
[1215.14s -> 1219.06s]  you can choose that random variable at random, and then you'll randomly choose
[1219.06s -> 1223.66s]  to go around the tree on the left or on the right.
[1223.66s -> 1228.10s]  Okay, the third class of distributions I'll talk about, which has gotten a lot
[1228.10s -> 1230.94s]  of attention in recent years because these kinds of models have started
[1230.94s -> 1233.22s]  working really well, is diffusion models.
[1233.22s -> 1236.18s]  Diffusion models are somewhat similar to latent variable models,
[1236.18s -> 1238.46s]  but there are some differences.
[1238.46s -> 1242.14s]  So some of you might have heard about diffusion models as a way of generating
[1242.14s -> 1246.06s]  images, so things like DALY, stable diffusion, those are all methods that
[1246.06s -> 1248.78s]  use diffusion models for image generation.
[1248.78s -> 1251.26s]  The way that diffusion models work for image generation, and this is a very
[1251.26s -> 1255.90s]  high-level summary, so if this feels vague to you, it's because it is.
[1255.90s -> 1260.06s]  I could teach an entire class on diffusion models in principle,
[1260.06s -> 1264.02s]  but for the purpose of covering it in two slides, I'm going to provide a very
[1264.02s -> 1266.46s]  high-level overview.
[1266.46s -> 1269.14s]  Let's say that we have a particular training image.
[1269.14s -> 1273.38s]  I'm going to denote the training image as x0, and the subscript here doesn't
[1273.38s -> 1276.54s]  denote time, it actually denotes corruptions of the image.
[1276.54s -> 1281.10s]  So x0 is the least corrupted, xt is the most corrupted.
[1281.10s -> 1283.94s]  So x0 is the true image.
[1283.94s -> 1287.34s]  In a diffusion model, you construct a fake training set where you add noise
[1287.34s -> 1292.54s]  to the image, and then you train the model to remove that noise.
[1292.54s -> 1298.06s]  So the image xi plus 1 is going to be the image xi plus noise.
[1298.10s -> 1300.82s]  So x1 is x0, which is the true image, plus noise.
[1300.82s -> 1305.30s]  x2 is x1, which has some noise added, and it adds even more noise to it.
[1305.30s -> 1308.90s]  And if you take an image and you add these different amounts of noise,
[1308.90s -> 1311.94s]  now you have a training set where you can teach your neural network
[1311.94s -> 1313.62s]  to go backwards.
[1313.62s -> 1317.38s]  So the learned network looks at the image xi and it predicts xi minus 1.
[1317.38s -> 1320.30s]  So it's going to look at the slightly noisy image x1 and predicts x0.
[1320.30s -> 1324.02s]  It's going to look at the slightly more noisy image x2 and it's going to
[1324.02s -> 1325.62s]  predict x1 and so on and so on.
[1328.82s -> 1331.78s]  In reality, we actually often train it to go all the way back to x0,
[1331.78s -> 1334.54s]  so there's a choice to be made there, but for simplicity,
[1334.54s -> 1338.26s]  it helps to think about it as just going back one step.
[1338.26s -> 1343.70s]  And in reality, what we actually predict is just the noise itself.
[1343.70s -> 1349.46s]  And that's not actually that different because if you predict xi minus 1,
[1349.46s -> 1351.86s]  well, you can get that by just predicting the noise and just
[1351.86s -> 1353.34s]  subtracting the noise from xi.
[1353.34s -> 1356.26s]  So you can either have f of xi directly output xi minus 1,
[1356.30s -> 1359.62s]  or you can have f of xi output the noise, in which case xi minus 1 is just xi
[1359.62s -> 1361.02s]  minus f of xi.
[1361.02s -> 1363.26s]  And that's a much more common choice to make.
[1363.26s -> 1365.26s]  Okay, now this is image generation.
[1365.26s -> 1367.10s]  What we actually want to do is not generate images,
[1367.10s -> 1369.22s]  we want to, of course, generate actions.
[1369.22s -> 1373.06s]  So what we can do is we can extend this framework to handle actions.
[1373.06s -> 1375.94s]  Now actions, of course, also have a temporal subscript,
[1375.94s -> 1381.02s]  so I'm going to use the subscript t to denote time in the temporal process
[1381.02s -> 1384.90s]  for control, and I'm going to use the second subscript to denote the
[1384.90s -> 1385.86s]  diffusion time step.
[1385.86s -> 1392.02s]  So at comma 0 is the true action in the same way that x0 was the true image.
[1392.02s -> 1397.54s]  At comma i plus 1 is ati plus noise.
[1397.54s -> 1402.22s]  And just like before, we can learn a network that now takes in the current
[1402.22s -> 1407.78s]  observation or state st and ati and then outputs ati minus 1.
[1407.78s -> 1411.54s]  Or just like before, we would actually get it to output the noise,
[1411.54s -> 1416.26s]  so then ati minus 1 is ati minus f of st ati.
[1416.26s -> 1418.90s]  So the training set is produced by taking all the actions,
[1418.90s -> 1423.42s]  adding noise to them, teaching the network to predict what that noise was
[1423.42s -> 1425.90s]  while also looking at the image.
[1425.90s -> 1428.54s]  And then at test time, if we want to actually figure out the action,
[1428.54s -> 1432.10s]  then we feed in completely random noise and run this model for many steps to get
[1432.10s -> 1433.34s]  it to denoise.
[1433.34s -> 1438.42s]  So turning the network over on the side, it gets its input ati,
[1438.46s -> 1442.38s]  it outputs ati minus 1, which is slightly denoised version of that action.
[1442.38s -> 1444.78s]  And it also gets to look at the image.
[1444.78s -> 1449.90s]  We start off with noise at test time, we feed that into this box as the first
[1449.90s -> 1452.22s]  value of ati, and then we repeat this process.
[1452.22s -> 1455.22s]  We denoise it, put it back in, denoise some more, and repeat this process
[1455.22s -> 1456.14s]  many times.
[1456.14s -> 1459.18s]  And then at the end, out comes a clean, good action.
[1459.18s -> 1461.94s]  And during training, we add all this noise to ground truth actions,
[1461.94s -> 1464.38s]  and we teach the network to undo the noise.
[1464.38s -> 1466.78s]  So that's the essential idea of a diffusion model.
[1466.82s -> 1470.34s]  Actually implementing it takes a number of additional design decisions,
[1470.34s -> 1474.46s]  which I won't have time to go into here, but I'll reference some papers
[1474.46s -> 1476.46s]  and you can look at those papers for details.
[1478.10s -> 1482.26s]  The last trick I'm gonna talk about is discretization.
[1482.26s -> 1487.54s]  Discretization is in general a very good way to get complex distributions,
[1487.54s -> 1491.54s]  but it's very difficult to apply discretization naively in high dimensions.
[1491.54s -> 1496.10s]  So remember in that Zurich paper where the actions were to go left,
[1496.14s -> 1499.90s]  or right, and go straight, this multimodality problem basically didn't exist.
[1499.90s -> 1502.18s]  But of course, that was for 1D actions.
[1502.18s -> 1505.70s]  In higher dimensions, if you have, let's say, 10 dimensional actions,
[1505.70s -> 1508.34s]  discretizing the action space is impractical because the number of bins
[1508.34s -> 1512.22s]  you need increases exponentially with the number of dimensions.
[1512.22s -> 1515.54s]  So the solution is to discretize one dimension at a time.
[1515.54s -> 1518.54s]  And that's the idea behind autoregressive discretization.
[1518.54s -> 1520.54s]  So here's how we can do it.
[1520.54s -> 1522.90s]  Let's say our action is a three-dimensional vector.
[1522.90s -> 1525.30s]  I'm gonna use AT zero to denote dimension zero,
[1525.34s -> 1529.18s]  AT one to denote dimension one, and AT two to denote dimension two.
[1529.18s -> 1531.74s]  Don't be confused with the notation from diffusion models before,
[1531.74s -> 1533.10s]  this has nothing to do with that.
[1533.10s -> 1534.86s]  So the second number is just the dimension,
[1534.86s -> 1537.22s]  and it's just a scalar value, right?
[1537.22s -> 1539.22s]  So here's how we're gonna set up our network.
[1539.22s -> 1541.14s]  We take the image and we encode it
[1541.14s -> 1543.14s]  with some kind of encoder like a continent.
[1543.14s -> 1544.66s]  And then we put it into a sequence model,
[1544.66s -> 1547.42s]  which could be a transformer or an LSTM.
[1547.42s -> 1550.14s]  So whatever your favorite sequence model is.
[1550.14s -> 1551.82s]  And at the first step of the sequence,
[1551.86s -> 1555.02s]  we output dimension zero,
[1555.02s -> 1559.26s]  and we can discretize dimension zero just into bins, right?
[1559.26s -> 1561.22s]  So it's just one dimension discretizing a number line
[1561.22s -> 1562.26s]  is pretty easy.
[1562.26s -> 1564.58s]  So we have one bin for every possible value.
[1564.58s -> 1566.62s]  You could have 10 bins, you could have a hundred bins.
[1566.62s -> 1568.90s]  Since it's one dimensional, it's very easy to do.
[1570.50s -> 1573.14s]  And then at the second time step in the sequence,
[1573.14s -> 1576.06s]  we feed in the value AT zero,
[1576.06s -> 1578.86s]  and we output AT one again with a discretization.
[1578.86s -> 1581.66s]  And then the next time step we input AT one,
[1582.38s -> 1583.22s]  and we output AT two.
[1583.22s -> 1584.62s]  So just like in a sequence model
[1584.62s -> 1585.90s]  and something like a language model,
[1585.90s -> 1589.30s]  you would output the next token, the next letter.
[1589.30s -> 1591.02s]  Here, you would output the next dimension
[1591.02s -> 1592.38s]  of the action space.
[1592.38s -> 1594.38s]  And now each dimension is discretized
[1594.38s -> 1596.34s]  and the number of bins is no longer exponential
[1596.34s -> 1597.18s]  in the dimensionality.
[1597.18s -> 1600.06s]  It's actually still linear in the dimensionality.
[1600.06s -> 1602.10s]  And then at test time, if we want to sample,
[1602.10s -> 1603.18s]  then we do exactly what we do
[1603.18s -> 1604.58s]  with any other sequence model is
[1604.58s -> 1606.54s]  instead of feeding in the ground truth value
[1606.54s -> 1608.06s]  of each dimension, which we don't have,
[1608.06s -> 1609.50s]  we would feed in the prediction
[1609.50s -> 1611.06s]  from the previous time step.
[1611.34s -> 1612.22s]  Again, this is exactly the same
[1612.22s -> 1613.86s]  as any other sequence model
[1613.86s -> 1615.22s]  like language models, for example.
[1615.22s -> 1616.94s]  So one way to implement is actually
[1616.94s -> 1619.18s]  with a GPT style decoder only model.
[1621.26s -> 1623.22s]  Now, why does this work?
[1623.22s -> 1625.34s]  Well, the reason that this is a perfectly valid way
[1625.34s -> 1627.34s]  to represent complex distributions
[1627.34s -> 1631.38s]  is that it can be seen by looking at what probabilities
[1631.38s -> 1632.62s]  actually predicted each step.
[1632.62s -> 1636.38s]  So the first time step predicts P of AT zero given ST
[1636.38s -> 1640.38s]  because you get ST as input and your output is AT zero.
[1640.38s -> 1643.54s]  The second step predicts the probability of AT one
[1643.54s -> 1646.38s]  given ST and AT zero.
[1646.38s -> 1648.26s]  So the dependence on ST comes from the fact
[1648.26s -> 1650.78s]  that it's passed in through the sequence model
[1650.78s -> 1652.34s]  and AT zero is fed as input.
[1653.22s -> 1656.58s]  The third time step you predict AT two given ST,
[1656.58s -> 1658.78s]  AT zero and AT one.
[1658.78s -> 1660.50s]  If you multiply all these things together,
[1660.50s -> 1662.26s]  then by the chain rule of probability,
[1662.26s -> 1664.78s]  their product is exactly the probability of AT zero,
[1664.78s -> 1667.34s]  AT one and AT two given ST,
[1667.34s -> 1668.46s]  which is exactly the probability
[1668.46s -> 1669.86s]  of the action given the state.
[1669.86s -> 1671.38s]  So this is the policy.
[1671.38s -> 1672.62s]  If you multiply it together,
[1672.62s -> 1674.78s]  the probability is at all the time steps.
[1674.78s -> 1676.22s]  And that means that if you sample
[1676.22s -> 1680.14s]  the different dimensions in sequence,
[1680.14s -> 1681.82s]  then you will actually get a valid sample
[1681.82s -> 1684.90s]  from the distribution P of AT given ST.
[1684.90s -> 1687.06s]  So autoregressive discretization is actually a great way
[1687.06s -> 1688.70s]  to get complex distributions,
[1688.70s -> 1690.18s]  but it's a little bit more heavyweight
[1690.18s -> 1691.14s]  in the sense that you have to use
[1691.14s -> 1693.50s]  these sequence models for outputting actions.
[1695.90s -> 1699.18s]  All right, next, let me talk about a few case studies
[1699.22s -> 1704.10s]  of papers that use these kinds of formulations.
[1704.10s -> 1706.18s]  The first case study I'll talk about
[1706.18s -> 1710.14s]  is a paper from 2023 from T et al
[1710.14s -> 1711.98s]  that uses the fusion models
[1711.98s -> 1714.50s]  to represent policies for robots.
[1714.50s -> 1717.78s]  So it works more or less exactly as you would expect.
[1717.78s -> 1718.78s]  There are two variants.
[1718.78s -> 1721.02s]  There's a convenient based variant
[1721.02s -> 1722.34s]  and a transform based variant.
[1722.34s -> 1724.74s]  And they differ just in how they read in the image.
[1724.74s -> 1727.78s]  But in both cases, they read in the image
[1727.82s -> 1729.30s]  and they perform this denoising operation.
[1729.30s -> 1731.02s]  So this is actually visualizing the denoising
[1731.02s -> 1734.18s]  and the denoising process yields a short trajectory
[1734.18s -> 1736.58s]  for the end effect to take over the next few steps.
[1736.58s -> 1739.30s]  And then the robot follows that trajectory
[1739.30s -> 1741.30s]  and performs the task.
[1741.30s -> 1743.78s]  And they can use this for imitation learning,
[1743.78s -> 1745.30s]  for learning things like picking up cups
[1745.30s -> 1750.06s]  or putting some sauce on a pizza.
[1750.06s -> 1752.38s]  This is maybe with like a cooperative person
[1752.38s -> 1753.78s]  that kind of helps you make sure
[1753.78s -> 1755.62s]  the sauce doesn't go all over the place.
[1755.78s -> 1759.34s]  But the point is that by using these fairly expressive
[1759.34s -> 1761.66s]  multimodal diffusion model policies,
[1761.66s -> 1763.70s]  they can actually get pretty good behaviors
[1763.70s -> 1765.58s]  out of their imitation learning system.
[1767.26s -> 1770.22s]  Here's an example of a method that uses latent variables.
[1770.22s -> 1772.30s]  Here, the latent variables are actually used
[1772.30s -> 1773.94s]  in conjunction with the transformer,
[1773.94s -> 1776.02s]  but it's not using that action discretization from before.
[1776.02s -> 1780.98s]  It's just outputting the continuous values of the actions.
[1780.98s -> 1782.46s]  So the transformer is just used
[1782.46s -> 1785.18s]  to provide a more elaborate representation.
[1785.22s -> 1787.78s]  The latent variable here is this letter Z
[1787.78s -> 1791.50s]  that you can see at the end of the input sequence.
[1791.50s -> 1794.74s]  And the paper treats it as a kind of style variable
[1794.74s -> 1796.82s]  to account for the fact that human experts
[1796.82s -> 1798.26s]  might perform the task with different styles
[1798.26s -> 1799.22s]  on different trials.
[1799.22s -> 1801.90s]  And that's used to improve the expressivity of the model.
[1801.90s -> 1804.46s]  And imitation learning here uses
[1804.46s -> 1806.72s]  this bimanual manipulation rig,
[1806.72s -> 1808.70s]  which can then be used to provide demonstrations
[1808.70s -> 1809.94s]  to teach the robot to do things
[1809.94s -> 1812.02s]  like put a shoe on a foot.
[1813.02s -> 1816.14s]  So that's not a real foot, it's a mannequin foot.
[1816.14s -> 1821.14s]  And here, the policy is inferring the latent variable
[1821.18s -> 1824.46s]  just by sampling randomly from the prior test time,
[1824.46s -> 1825.58s]  but during training,
[1825.58s -> 1828.16s]  it's using essentially a conditional VAE method
[1828.16s -> 1831.18s]  of the sort that we'll describe later in the course.
[1831.18s -> 1832.78s]  So here's the shoe,
[1832.78s -> 1834.30s]  and it's of course going to buckle the shoe
[1834.30s -> 1836.70s]  because you need to make sure the shoe is buckled.
[1836.70s -> 1841.70s]  And here's another example,
[1842.02s -> 1844.10s]  putting some batteries into a remote
[1844.10s -> 1847.02s]  while an annoying graduate student
[1847.02s -> 1848.82s]  distracts the robot by throwing objects
[1848.82s -> 1850.22s]  into the background.
[1850.22s -> 1851.66s]  I guess this is the kind of thing you want to do
[1851.66s -> 1852.82s]  to stress test your policies
[1852.82s -> 1855.66s]  to make sure that they're pretty good with distractors.
[1860.26s -> 1862.02s]  And here's the last case study that I'll present.
[1862.02s -> 1864.94s]  This is a autoregressive discretization method.
[1864.94s -> 1867.70s]  And in this work called RT1,
[1867.70s -> 1869.98s]  the model is actually a transformer model
[1869.98s -> 1872.68s]  that reads in a history of prior images
[1872.68s -> 1874.30s]  and the language instruction,
[1874.30s -> 1877.32s]  and it actually outputs a per dimension discretization
[1877.32s -> 1880.58s]  of the arm and base motion commands.
[1880.58s -> 1882.50s]  So it's a wheeled robot so it can move the base
[1882.50s -> 1883.78s]  and it can actually move the arm,
[1883.78s -> 1886.10s]  and all of those dimensions are discretized.
[1886.10s -> 1887.94s]  And that makes the entire control problem
[1887.94s -> 1889.30s]  kind of a sequence to sequence problem.
[1889.30s -> 1891.94s]  So it's a sequence of images and text
[1892.02s -> 1896.26s]  converted into a sequence of per dimension actions.
[1898.22s -> 1900.70s]  And because it's language condition,
[1900.70s -> 1902.22s]  it can actually learn to perform a wide variety
[1902.22s -> 1903.42s]  of different tasks when provided
[1903.42s -> 1905.10s]  with a suitably large data set.
[1905.10s -> 1906.70s]  And when it's language condition,
[1906.70s -> 1908.58s]  you can actually do fun things like connect it up
[1908.58s -> 1911.14s]  to a large language model
[1911.14s -> 1912.82s]  that will then parse complex instructions,
[1912.82s -> 1914.90s]  like bring me the right strips from the drawer
[1914.90s -> 1915.86s]  and then say, well, to do that,
[1915.86s -> 1917.30s]  you have to first go to the drawer
[1917.30s -> 1920.12s]  and then things like open the drawer
[1920.12s -> 1922.20s]  and that's then commanded to this RT1 model,
[1922.20s -> 1924.32s]  which then selects the per dimension actions
[1924.32s -> 1927.04s]  using that action discretization truck.
[1927.04s -> 1929.00s]  There's of course, a lot more going on in this paper
[1929.00s -> 1930.84s]  than just auto-arrested discretization,
[1930.84s -> 1932.76s]  but I wanted to show this as just one example
[1932.76s -> 1935.92s]  of that method really working in practice.
[1939.16s -> 1940.64s]  Okay, so I'll pause there
[1940.64s -> 1942.96s]  and I'll resume in the next part.
