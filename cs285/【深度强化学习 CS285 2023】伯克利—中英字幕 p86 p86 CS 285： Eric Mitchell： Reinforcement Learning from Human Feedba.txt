# Detected language: en (p=1.00)

[0.00s -> 7.84s]  All right. Well, this mic is sort of doing something, so I might just end up yelling,
[7.84s -> 17.20s]  but I guess we'll see. Okay, cool. Were we? Are we good? Okay, we're good. Yeah, so thanks
[17.20s -> 23.56s]  a lot for having me. It's fun to be here. Like Kevin said, I'm here from Stanford.
[23.56s -> 29.24s]  I hope you don't hold it against me, but I managed to make it just in time through
[29.28s -> 37.04s]  the Bay Area traffic. Today, I'm going to talk about RLHF, something that people are probably
[37.04s -> 42.70s]  a little bit familiar with, but if not, that's okay. We'll try to cover it more or less,
[42.70s -> 48.36s]  not quite from zero, but give some background. I tried to leave a little bit of time for
[48.36s -> 51.96s]  questions. I do want people to actually learn from this. If you have questions,
[51.96s -> 56.82s]  let me know. I might have to start ignoring them if we get crunched for time, but feel
[56.86s -> 63.78s]  free to raise your hand and I'll do my best as we go without further ado.
[63.78s -> 69.70s]  So, okay, if you're in this class, I guess a lot of people maybe are not in the class,
[69.70s -> 75.46s]  but you're probably familiar with reinforcement learning. If you're a human being these days,
[75.46s -> 78.34s]  you've probably heard something about language models, but why are we talking about
[78.34s -> 84.50s]  reinforcement learning for language models? There are a lot of ways, I think, to do this slide.
[84.50s -> 90.54s]  I thought about doing some long timeline of MLP and RL and showing them converging,
[90.54s -> 95.06s]  but I think really the main idea here is GPT-3 was cool. All of your researcher friends were
[95.06s -> 99.50s]  like, whoa, have you seen this new model? Then Chachi PT came around and it was like,
[99.50s -> 103.74s]  your grandma was like, yo, have you seen this new model? These are just totally different levels
[103.74s -> 111.02s]  of permeation in the public consciousness. The question is, what was between these two things?
[111.10s -> 117.18s]  What was added? And it really is this sort of more or less one weird trick,
[117.18s -> 120.94s]  which is RL from human feedback. And that's what we're going to talk about today.
[122.94s -> 128.62s]  As just a little bit of background, so that we're all a little bit more on the same page
[128.62s -> 136.78s]  if people are not as familiar with language models. What is a language model? For our purposes
[136.86s -> 142.70s]  today, you can just think of a language model as an autoregressive model over tokens. And tokens,
[142.70s -> 147.18s]  you can really just think of as words. Sometimes there'll be sub words or word pieces,
[147.18s -> 151.82s]  but for the most part, it's basically just an autoregressive model over words. We'll have
[151.82s -> 158.70s]  some vocabulary, which is the set of possible tokens. And in sort of modern large language
[158.70s -> 164.06s]  models, this will be something like 50,000, but can vary quite a bit. And basically your
[164.06s -> 169.82s]  language model, your policy here maps token sequences to a distribution over the next token.
[171.18s -> 175.66s]  Pretty simple stuff. And when we started thinking about reinforcement learning, we'll think of
[175.66s -> 180.62s]  states as being sequences of tokens. And actions can be either individual tokens or actually quite
[180.62s -> 185.66s]  commonly, they will also be sequences of tokens. So although you might think of it sort of
[185.66s -> 191.26s]  naturally to think of like each token is an action, in a lot of the work that's out there,
[191.26s -> 195.26s]  people tend to actually think of this as sort of a contextual bandit setting where
[195.26s -> 200.70s]  you get some context, some prompt sequence of tokens, and you emit a single action,
[200.70s -> 206.14s]  which is your sort of response sequence. And so to make this very concrete, if you're using
[206.14s -> 211.26s]  ChatGPT, in this sort of bandit problem, you have like your first state, which is whatever
[211.26s -> 219.98s]  you type in and your action is the model's response. And now if you have sort of multiple
[219.98s -> 227.26s]  turns of dialogue, your state just becomes the first input, first response, and the next input,
[227.26s -> 231.82s]  and your action is just your next response. There's a paper here I noted at the bottom,
[231.82s -> 235.34s]  this contrastive preference learning, learning from human feedback without RL,
[236.22s -> 240.46s]  which is a very recent preprint. But if you're interested in more of this discussion about sort
[240.46s -> 247.10s]  of the contextual bandit versus per token MDP kind of formulations of how we do RL on language
[247.10s -> 251.18s]  models, that paper has some good discussion of this, both in the main text and the appendix.
[251.18s -> 255.34s]  So if you're really hungry for more, I invite you to check it out. But I'm going to stick
[255.34s -> 258.94s]  to this sort of bandit formulation because that's what most of the work so far has used
[258.94s -> 263.02s]  and sort of the basis of the algorithms we're going to talk about. Okay, so with that out
[263.02s -> 267.66s]  of the way, what we're going to cover today is basically three main sections. So I'm going
[267.66s -> 272.38s]  to do kind of a primer on RLHF for people who are not familiar with it. This is sort of
[272.70s -> 276.38s]  the algorithm, at least as far as we know it, because we don't really know,
[276.38s -> 281.66s]  but as far as we know what was done to give us ChatGPT. I'm going to talk about a newer
[281.66s -> 287.18s]  algorithm that simplifies some of the complexity in this original formulation of reinforcement
[287.18s -> 291.90s]  learning from human feedback on language models, which is called direct preference optimization.
[291.90s -> 294.46s]  And then I'm going to talk about some applications, and one in particular,
[295.74s -> 302.06s]  to prove that it was worth coming to listen to this monologue. Hopefully it's not a monologue,
[302.06s -> 305.90s]  hopefully we'll have some good questions. Okay, so part one, reinforcement learning from
[305.90s -> 312.38s]  human feedback. I'm going to just give you the first couple steps of this pipeline. The way
[312.38s -> 316.86s]  that RLHF is done on these big language models basically has four main steps, and the
[316.86s -> 321.18s]  first two are pretty straightforward. So the first one is just unsupervised pre-training,
[321.18s -> 326.62s]  and this is actually just GPT-3. So if you stop after step zero here, you get GPT-3,
[326.62s -> 331.02s]  you take a ton of text from the internet, and we're talking trillions of words of text from
[331.02s -> 335.18s]  the internet. You just do unsupervised generative modeling condition on a sequence,
[335.82s -> 341.98s]  predict the next token, do this a lot on a lot of A100s for a lot of hours, and you get out
[341.98s -> 346.46s]  this unsupervised pre-trained model, and this is this pure autoregressive generative model over
[346.46s -> 353.10s]  text. After we do that, we're actually going to do some supervised time tuning first on
[353.10s -> 358.46s]  human demonstrations. And so this basically gets the behaviors that we're interested in really
[358.46s -> 363.66s]  learning and refining in distribution for the model. We can talk a little bit more later about
[363.66s -> 368.22s]  why this is so important, but basically you can think of these first two steps as kind of
[368.22s -> 372.22s]  learning the background knowledge about the world in step zero, and then step one is doing
[372.22s -> 376.46s]  just a little bit of fine tuning on some human written demonstrations of what would be a good way
[376.46s -> 380.94s]  to respond to this example prompt, like write a poem about jazz or whatever. And so we
[380.94s -> 385.82s]  fine-tune that original unsupervised pre-trained model, and we get this SFT model. And then the
[385.82s -> 393.02s]  question is what do we do next? And I think a question here is also like why do anything
[393.02s -> 398.46s]  next? I mean, why not just stop at step one? Supervised learning works really well.
[400.06s -> 404.70s]  And there are a couple of reasons for this. Two in particular, one is that, and we'll talk
[404.70s -> 408.94s]  about this a little more, it's kind of hard to scale this annotation of human demonstrations.
[409.50s -> 413.74s]  It's really laborious, right? If you want to come up with demonstrations of like,
[413.74s -> 418.62s]  how am I supposed to respond to queries? Write me a sassy poem in the style of Snoop Dogg about
[418.62s -> 423.58s]  quantum mechanics. It's going to be difficult, and it's going to take time to collect those
[423.58s -> 428.30s]  annotations. And also if you want to actually exceed human performance on a lot of these
[429.02s -> 435.26s]  tasks, obviously imitating human behavior is not likely to give you a policy that's
[435.26s -> 438.86s]  actually going to do better than humans. And so these are some of the reasons, both from
[438.86s -> 442.62s]  the perspective of scaling up the annotations and also just getting a really strong model that
[442.62s -> 449.58s]  we might want to do this. Okay, so hopefully you're somewhat convinced we want to try to
[449.58s -> 454.94s]  train these sort of general purpose language agents using RL. And I think, you know, probably
[454.94s -> 458.38s]  the most obvious question then is, well, okay, like what are we going to optimize?
[458.38s -> 464.54s]  What is the reward and what properties should it have? Now, ideally, like we want a reward
[464.54s -> 469.34s]  that's going to assign high reward to stuff humans like and low reward to stuff that humans
[469.34s -> 473.66s]  don't like. Now this is a big oversimplification, as I note in the bottom of the slide,
[473.66s -> 479.02s]  but the point is that knowing what should have high reward really requires knowing
[479.02s -> 485.02s]  something about humans. So it's probably not enough to use something sort of simple
[485.02s -> 489.26s]  and hard-coded or some sort of closed form reward. We probably really are going to need
[489.26s -> 494.62s]  to elicit some sort of, you know, particular annotations or behaviors from humans to learn
[494.62s -> 499.10s]  in sort of like an inverse RL kind of setup, except we might not be taking demonstrations
[499.10s -> 504.54s]  from humans. We might be getting some other kinds of data from them that we can use to
[504.54s -> 508.78s]  infer this sort of reward function that's actually worth optimizing. And so the question
[508.78s -> 515.02s]  is, where do we get this mysterious reward function? Fortunately, I'm gonna tell you.
[515.02s -> 519.50s]  So maybe the first thing we might think of is like, all right, like let's just ask humans
[519.50s -> 523.82s]  for a reward, right? Like I'm going to show you model behaviors and if you really like
[523.82s -> 527.26s]  it, you'll give me a big number, and if you don't like it, you're going to give me a small number.
[527.26s -> 532.94s]  And you could do this, but I'm curious, like what number would you give this response, right?
[536.22s -> 539.90s]  Like can we just like raise hands and people give like one through five, I know I said one
[539.90s -> 545.82s]  through ten, one through five, like what do people give this? We got a five, we got an
[545.82s -> 553.26s]  eight out of five, that's pretty good. Three. Okay, so yeah, I mean this was a silly exercise,
[553.26s -> 558.14s]  but I hope it makes it clear that it's sort of an under-specified task here, and you could have
[558.14s -> 562.46s]  different people that maybe rank different responses the same way, but they might sort of
[562.46s -> 566.70s]  differ in a monotonic transformation of the rewards, and so you might have high disagreement
[566.70s -> 571.90s]  among your labelers that becomes annoying to model. And so maybe we can do a simpler task,
[571.90s -> 576.38s]  right? Maybe instead of just giving rewards directly, it's a lot easier to say which two
[576.38s -> 581.74s]  of these responses is more helpful, right? Like who thinks the response on the left is more helpful?
[583.26s -> 587.18s]  So we're going to see much higher agreement, a lot of the time at least, when we ask for responses
[587.82s -> 592.14s]  this way, and also a lot more people raise their hand, right? Because it was a lot easier
[592.14s -> 595.98s]  to make that judgment than it was to give me a number. And so this sort of gets at this
[595.98s -> 599.98s]  point of, the first point before, it's easier to scale annotation this way, because it's much
[599.98s -> 603.82s]  easier to put two responses in front of a person, say which do you like better than it is to say
[603.82s -> 607.66s]  write me a good demonstration response, or come up with a number that you think captures how
[607.66s -> 612.62s]  good this response is. And so this is exactly how we're going to collect feedback from
[612.70s -> 614.62s]  humans in order to learn this reward function.
[616.94s -> 620.70s]  So this feedback that we're going to get is going to come as preferences over model samples. So
[620.70s -> 624.46s]  you remember we did that supervised fine-tuning step, so our model will roughly do the sort
[624.46s -> 629.34s]  of things that we care about. And what we do then is we have some data set of unlabeled
[629.34s -> 634.22s]  prompts, we collect two model samples from each prompt, and then we put them in front of humans,
[634.22s -> 637.90s]  and the human says that one's better. And then we end up with this data set of these
[638.46s -> 644.30s]  triples x, yw, and yl, where the yw is sort of the preferred or the winning response, and yl is
[644.30s -> 649.26s]  the loser. And then the question is, okay, we have this data, it sort of feels like this is
[649.26s -> 655.02s]  helpful for learning a reward, but how exactly do we do that? And this is where some older
[655.74s -> 659.90s]  models from the economics literature, in particular the one that's widely used is this Bradley
[659.90s -> 665.42s]  Terry model that relates some score function or utility function, maybe a reward function,
[665.98s -> 673.18s]  to preferences. And so, in particular, the Bradley Terry model is built for discrete choice,
[673.18s -> 677.50s]  and in particular binary choice settings, where a human has to decide between two choices,
[677.50s -> 681.34s]  and we're modeling the probability that they're going to prefer choice A to choice B,
[681.34s -> 687.74s]  and we model it as this sort of Boltzmann distribution of the difference in their scores.
[687.74s -> 693.10s]  And this score function is sort of this unobserved implicit sort of latent scoring function that
[693.18s -> 696.46s]  we sort of hypothesize exists, but we only get to observe the human's choices.
[697.90s -> 701.90s]  All right, so now we have a probabilistic model that relates discrete binary choices,
[701.90s -> 706.30s]  which is what we have in our data, to some scoring function, and we can just turn this
[706.30s -> 712.70s]  into a loss function on a reward model and do maximum likelihood. So we just replaced s here
[712.70s -> 718.30s]  with r phi, which is our reward model that's got some parameters, and we're just going to
[718.30s -> 725.90s]  do maximum likelihood here on our data set of preference data, and out pops a reward model
[725.90s -> 731.02s]  that we can then optimize. Okay, so this is sort of the human feedback part of the RLHF.
[732.14s -> 734.86s]  Are we good? Okay.
[737.26s -> 743.10s]  Okay, so we got a little stuck here, but now we figured it out. Good for us. So we got these
[743.10s -> 746.70s]  new prompts, and again we're going to use the human, but this time we're going to use the
[746.70s -> 751.66s]  human to give us these preference pairs over model samples from the SFT model. One of my
[752.38s -> 759.18s]  office mates told me this figure is too overwhelming for a lecture, and I hope he was
[759.18s -> 764.78s]  wrong. So step two, we fit this reward model right with this preference data we have
[764.78s -> 769.82s]  over the SFT samples. All right, so we're almost there. We have this SFT model that
[769.82s -> 774.22s]  kind of does what we want. We now have this reward model, which allegedly assigns high
[774.22s -> 779.26s]  reward to good stuff and low reward to bad stuff. So now we need to actually just fine-tune
[779.26s -> 785.10s]  this policy, and I say allegedly here because it's sort of silly, right? We have a single
[785.10s -> 788.86s]  reward function that gives you an input and an output, and it gives you one number that says
[788.86s -> 793.10s]  how good it is. If you think even a little bit about this, it doesn't make any sense at all,
[793.10s -> 796.94s]  right? People have different opinions on what's good and what's bad, and so
[798.78s -> 803.50s]  in some ways it's almost surprising RLHF works as well as it does because the fact that we
[803.50s -> 807.66s]  can make this unbelievably restrictive assumption that there's a single reward function for all of
[807.66s -> 811.42s]  humanity that's worth optimizing is a little bit silly, and that's an interesting direction for
[811.42s -> 818.30s]  future research. So please figure that out. We want to represent all sorts of value systems.
[819.34s -> 823.02s]  Okay, so we have this reward function, and now we want to learn a policy that achieves
[823.02s -> 829.34s]  high reward. So this is just RL, right, which I've been told you know something about.
[829.74s -> 836.06s]  Now, that's the obvious bit. I mean, do people have any thoughts of what can go wrong if we just
[836.06s -> 840.06s]  optimize this? And I kind of glanced through the lectures that you guys have had, but I'm
[840.06s -> 846.62s]  curious if people have any guesses on this. It's not rhetorical. If you have a thought, you can
[846.62s -> 854.22s]  actually raise your hand and say what you think. You don't have to, but you could. Or not.
[854.22s -> 857.66s]  Oh, yes.
[868.06s -> 872.62s]  Yeah, so that's definitely a problem. So one thing you'll find actually if you fit these reward
[872.62s -> 877.82s]  models on real data is like, this reward model is basically a classifier, right? If I have some
[877.82s -> 883.50s]  data that's like binary preferences over responses, if I fit my reward model and then measure how
[883.50s -> 888.38s]  accurately it actually assigns higher reward to the thing that the human says is better,
[888.38s -> 892.94s]  it'll be like under 70%. So these preferences are super noisy, and that can be a problem.
[892.94s -> 896.38s]  I'm thinking of something else, though. It's a good point, though. Any other guesses?
[896.38s -> 897.26s]  We got one in the back.
[901.18s -> 904.62s]  Yeah, so I mean that's, I don't want to give you full credit for that, because like
[904.62s -> 908.78s]  if any, in any machine learning lecture, if someone's like, okay, so we do this thing,
[908.78s -> 913.50s]  what can go wrong here? You can always say distribution shift and it like will kind of
[913.50s -> 917.02s]  sound right. Unfortunately, it is sort of right here, so I have to give you some credit.
[917.66s -> 921.34s]  But I want to talk about a specific kind of distribution shift, which is that,
[922.14s -> 927.42s]  if you recall, right, we, well, the answer, sorry, is that we want to add some term here
[927.42s -> 932.46s]  to keep our policy that we're fine-tuning from the SFT model close to the SFT model. And the
[932.46s -> 937.66s]  reason for this, right, is our reward model was trained on pairs of trajectories or responses
[937.66s -> 943.10s]  from our SFT model, right. So sort of by definition, our reward model is probably going to be most
[943.10s -> 949.42s]  accurate around things that the SFT model assigns high likelihood to. And so if we optimize this
[949.42s -> 954.22s]  without this constraint to oblivion, we're probably going to end up in some area of the
[954.22s -> 958.06s]  action space where our reward model is giving us totally garbage rewards, and we're going to
[958.06s -> 961.98s]  sort of over-optimize our reward model. So it's going to look like we're getting higher
[961.98s -> 964.46s]  and higher and higher reward, but when you put those responses in front of humans,
[964.46s -> 969.34s]  they're going to say that's garbage. Okay, so that is what I mean. So yes, it is
[969.34s -> 974.94s]  distribution shift. And now that we have this whole objective, we're just going to optimize
[974.94s -> 979.50s]  this whole thing with PPO. I don't know if people are familiar with PPO. It's an RL algorithm.
[979.50s -> 985.90s]  You don't have to use PPO, but that's what John Shulman decided, and therefore that's the
[985.90s -> 994.06s]  world that we live in. It's a PPO world out there. So that basically finishes out
[994.22s -> 998.14s]  our pipeline for RLHF here, right? So we do unsupervised pre-training, we do SFT,
[998.14s -> 1003.90s]  we fit this reward model, and then finally we take our data set of just unlabeled prompts
[1003.90s -> 1008.54s]  again. We might have a new data set of unlabeled prompts, and we do RL with this
[1008.54s -> 1014.06s]  reward model. And we end up with a policy, and that's basically ChatGPT.
[1017.90s -> 1022.78s]  Another view of this process, which I've included here because it's even more complicated than my
[1022.78s -> 1027.34s]  figure, and I think it kind of makes me look better for that reason. So I've included it here.
[1028.14s -> 1035.26s]  But the point here really pedagogically is that this pipeline is really complicated. This is if
[1035.26s -> 1040.46s]  you include all of the actual pieces of PPO, right? We have the sort of the old policy,
[1040.46s -> 1043.82s]  we have the SFT model that our KL constraint is computed from, we have a reward model,
[1043.82s -> 1047.10s]  we also have a value function that we're fitting sort of online as we're training the model,
[1047.10s -> 1051.34s]  we also have our policy, and then we have a replay buffer as well. It's very complicated.
[1052.22s -> 1058.30s]  And this is, you know, non-trivial. Nonetheless, the recipe really does work.
[1058.94s -> 1064.78s]  So this is from the quote-unquote un-instruct GPT paper, the paper that came out when OpenAI
[1064.78s -> 1069.26s]  released DaVinci 003, which was the immediate predecessor to ChatGPT.
[1071.42s -> 1074.22s]  And there are a couple things to point out here. I mean, first thing is we're evaluating
[1074.22s -> 1080.78s]  win rates here, okay? So the y-axis is basically how often a human prefers the response
[1080.86s -> 1088.06s]  of that model to the response of the 175 billion parameter SFT model, okay? Which is why on the SFT
[1088.06s -> 1094.46s]  curve, right, it ends at 0.5. And what's sort of interesting here, right, is humans like the
[1094.46s -> 1101.10s]  1.3B RLHF model better than the 175B SFT model. And like, if you haven't worked with language
[1101.10s -> 1106.30s]  models, these numbers are just numbers flying through the air, but like 1.3B is like, I can
[1106.30s -> 1115.10s]  run it on my, like, 20-70 TI. 175B is like, I pay an engineering team to, like, fine-tune this model.
[1116.22s -> 1121.66s]  And so that's a really interesting kind of difference in scale. And also, importantly,
[1121.66s -> 1126.54s]  like, the gains continue to show up even as the model gets really big, right? So it's not
[1126.54s -> 1129.82s]  like this just helps us maybe do better when we have a really small model. Even when we have
[1129.82s -> 1135.66s]  a really big model, we have a lot of capacity to work with, this is still helpful. And,
[1135.66s -> 1138.38s]  you know, GPT-4 allegedly is, like, out here.
[1141.42s -> 1143.58s]  That could be misinformation, so take that with a grain of salt.
[1146.14s -> 1151.02s]  Okay, so RLHF works. I want to mention just a couple quick variations that I think
[1151.02s -> 1155.26s]  are kind of neat here. So this is a really recent paper that talks about quality diversity
[1155.26s -> 1160.30s]  from human feedback. And here what they do is they use human feedback not to learn
[1160.30s -> 1164.70s]  a reward function but to learn sort of a diversity metric, basically a latent space
[1164.70s -> 1170.30s]  that captures what humans find more or less similar. So here they'll show a human three
[1170.30s -> 1174.46s]  images and they'll say, for image A, let's compare this to image B and C, which one you think is
[1174.46s -> 1179.58s]  more similar to image A? And they use this type of feedback to learn this latent space.
[1179.58s -> 1183.26s]  And what they've shown here then is they've written some prompt, like an astronaut riding
[1183.26s -> 1191.18s]  a horse, and they've used CLIP. Do you know what CLIP is? Oh, some nods. Okay. So CLIP is a
[1191.18s -> 1196.14s]  model that jointly embeds text and images into a joint embedding space, so you can sort of
[1196.14s -> 1200.62s]  write text and find images that are similar or embed images and caption them. So what they've
[1200.62s -> 1204.14s]  done is they've embedded this caption, like an astronaut riding a horse, they found on the
[1204.14s -> 1209.02s]  left, that's just the eight images that are closest to that label, right? They're all
[1209.02s -> 1212.86s]  correct, but they're all really similar. What they've done on the right here is they partition
[1212.86s -> 1217.58s]  the space that they learned from human feedback into 16 cells, and then they found the nearest
[1217.58s -> 1221.98s]  image in each cell. And what is interesting is that you still get good matches to the
[1221.98s -> 1227.10s]  prompt for all of them, but you have clearly higher diversity by constraining your images
[1227.10s -> 1229.90s]  in this way. Okay. So this is another thing that we can learn from human feedback.
[1229.90s -> 1234.70s]  It's not just about learning reward models. Another thing that's needed is RL and AI
[1234.70s -> 1245.26s]  feedback. So this is how CLOD works, which is anthropics sort of offering in a similar
[1245.26s -> 1249.42s]  sort of chatbot space. And basically the deal here is we want to learn dialogue models that
[1249.42s -> 1252.94s]  are not just helpful. It's not just that they will tell you whatever you want, but we also
[1252.94s -> 1256.06s]  want them to have some sort of guardrails, right? We don't want them to tell you how to,
[1256.94s -> 1261.26s]  you know, do bad things. I'm not even going to give examples because everybody disagrees
[1261.26s -> 1264.54s]  on what is a bad thing, but, you know, there are certain queries we want these models to
[1264.54s -> 1268.94s]  refuse, right? And one way to do this is to just ask humans, right? Like, here's a pair
[1268.94s -> 1273.02s]  of responses, which is more harmful. But another thing you can do is you can do the normal
[1273.02s -> 1276.86s]  RLHF thing with just the helpful data. So just where you have data where humans say,
[1276.86s -> 1280.86s]  which is like a better response. And then you actually use the model you get out of that to
[1280.86s -> 1284.70s]  give you the harmlessness annotations. So instead of having to ask humans, then what's
[1284.70s -> 1288.86s]  more harmful, you just show this purely helpful model pairs of responses and it'll
[1288.86s -> 1294.54s]  give you those labels. And this works. Okay. So wrapping up the RLHF section here,
[1296.14s -> 1299.50s]  one of the key points here is humans can provide more scalable feedback with comparison
[1299.50s -> 1303.42s]  than they can through demonstration. And this is a special case of a more general phenomenon
[1303.42s -> 1307.50s]  or general question, which is like, what is the best way to do scalable oversight
[1307.50s -> 1312.70s]  on powerful models? Right. And it seems like, you know, this is one useful way. And I say
[1312.70s -> 1316.70s]  it's scalable both in terms of collecting a lot of the data, but also quality too. It's probably
[1316.70s -> 1319.58s]  more scalable in terms of if we want to get the model better than humans, it's going to
[1319.58s -> 1324.70s]  be easier to do that with comparisons than with demonstrations. The next big thing is we
[1324.70s -> 1329.02s]  use this theoretical preference model or choice model to derive this objective for reward learning.
[1329.02s -> 1333.02s]  And that's really where the meat of the work is here. And once we do that, we just fine tune
[1333.66s -> 1337.98s]  with more or less off the shelf RL with this learning reward and we get a great model. Yay.
[1338.62s -> 1342.06s]  Okay. RLHF is over. Any questions?
[1345.34s -> 1350.78s]  Okay. So now we're going to talk about DPO. So, oh, I'm sorry.
[1350.78s -> 1361.10s]  It's just not an autoregressive model. So it's just a language model. It takes a sequence of
[1361.10s -> 1366.78s]  tokens and gives you a distribution over the next token. And then when you like roll out your
[1366.78s -> 1373.18s]  policy, you get a prompt sequence of tokens. You predict the next one, you sample from that
[1373.18s -> 1378.14s]  distribution. So like an episode is like the whole sequence that you would like take
[1378.14s -> 1382.46s]  an action at every time. Yeah. So it kind of depends on what we call an action here. You
[1382.46s -> 1387.10s]  can call an action an individual token, or you can call an action a whole output sequence until
[1387.10s -> 1393.02s]  your model output sort of a special like end of sequence token basically. So yeah, either a
[1393.02s -> 1397.02s]  trajectory or an episode is like a single action if you take that view or it's a sequence
[1397.02s -> 1403.42s]  of actions if you take the per token view. Yeah. Have they experimented with preference
[1403.42s -> 1406.78s]  models? Like this seems like it's always a dichotomy between two options.
[1410.78s -> 1414.70s]  Yeah. So there are pretty straightforward generalizations of Bradley Terry to rankings
[1414.70s -> 1420.38s]  over many responses, which is a general family called Plackett-Loose models, which looks very
[1420.38s -> 1425.10s]  similar to the Bradley Terry model. Yeah. And there are also a lot of questions about when
[1425.10s -> 1431.18s]  we pick this preference model, what are we actually assuming is the underlying parameter?
[1431.18s -> 1435.18s]  And we sort of assumed that this preference model is relating rewards to preferences.
[1435.18s -> 1438.14s]  It's actually not obvious that should be the case. It could be something else like advantages.
[1438.14s -> 1441.10s]  And there's really recent work looking into that, like in that paper I mentioned earlier,
[1441.10s -> 1444.78s]  the contrast of preference learning where they talk about per token versus per sequence,
[1444.78s -> 1449.98s]  they get into that. Okay. Let's talk about DPO. I love these questions though.
[1450.78s -> 1454.38s]  So what's not ideal about RLHF with PPO as I've described it? Well, there's this
[1454.38s -> 1457.98s]  implementation complexity issue. There are the resource requirements. So again, we have all
[1457.98s -> 1460.70s]  these different models flying around. We have the reward model, the value model,
[1460.78s -> 1466.46s]  the policy, the reference model, and just ability of training. So the rewards actually have this
[1466.46s -> 1472.06s]  extra degree of freedom when we fit them, right? Because this loss function only cares about the
[1472.06s -> 1476.70s]  difference between rewards. So I can shift my rewards for a particular prompt, right? If we
[1476.70s -> 1481.10s]  fix X, I can shift the rewards by an arbitrary constant that doesn't change the loss.
[1481.10s -> 1483.74s]  And the real issue is that the constant could be different for every prompt.
[1484.38s -> 1488.46s]  And so what we could conceptually at least end up with is a reward function that
[1488.46s -> 1492.38s]  for a particular prompt, the relative rewards make sense, but you can't even compare rewards
[1492.38s -> 1496.06s]  across inputs. So this would make it really, really difficult to fit a value function
[1496.06s -> 1502.54s]  when we're doing RL. So let's get rid of all of these issues. I'm going to talk about this
[1502.54s -> 1506.46s]  algorithm direct preference optimization, which basically simplifies the pipeline we've talked
[1506.46s -> 1510.70s]  about. And the punchline here is that if we parameterize the reward model in a special way,
[1510.70s -> 1514.62s]  if we pick sort of a particular architecture for a reward model, we can just extract the
[1514.62s -> 1517.82s]  optimal policy for that reward model in closed form. We don't actually have to do any RL.
[1518.46s -> 1527.82s]  Which is good. And the main idea here is that there is a sort of one-to-one correspondence
[1527.82s -> 1533.02s]  between optimal policies and reward models. So given a particular reward model or reward
[1533.02s -> 1536.78s]  function, there's a closed form expression for the optimal policy, which is intractable,
[1536.78s -> 1540.94s]  but we could actually use that still, at least in our training objective, to make it a lot
[1540.94s -> 1546.70s]  easier to train the policy. And I'm going to show you how. Okay, so we have this RLHF objective.
[1546.94s -> 1552.46s]  We already talked about this. This isn't new. So we have our data set of prompts here, and then
[1552.46s -> 1556.78s]  we have this expectation over responses from our model. We want high reward and we want low KL
[1556.78s -> 1563.58s]  and nothing new. And this is for any reward function. And what you can show, and this is
[1563.58s -> 1567.74s]  like a couple lines of algebra, it's in the DPO paper and many other several other papers,
[1569.02s -> 1572.14s]  is that there's a closed form optimal policy here, which is this.
[1573.10s -> 1578.78s]  And hopefully it's actually somewhat intuitive. Not that you immediately would have pulled this
[1578.78s -> 1584.46s]  out, but basically the probability we're assigning to a particular response is the product of the
[1584.46s -> 1588.70s]  probability that our original SFT model, the reference model, assigns to that response
[1588.70s -> 1594.54s]  and the exponentiated reward. And so we basically are going to end up assigning high
[1594.54s -> 1598.70s]  probability to stuff that both our reference model assigns reasonably high probability and
[1598.86s -> 1603.26s]  gets high reward. Okay, so kind of not crazy, right? Since we're originally our objective
[1603.26s -> 1606.78s]  function is literally high reward, low KL, it kind of makes sense we would see something
[1606.78s -> 1613.90s]  like this. We just have this 1 over z because we need to normalize. So it's a probability distribution.
[1616.22s -> 1620.14s]  Now again, this is intractable because this is going back to this idea that actions are
[1620.14s -> 1625.82s]  entire responses. This is a sum over all possible, not next tokens, all possible sequences.
[1626.46s -> 1627.18s]  Can't compute this.
[1630.54s -> 1634.22s]  But nonetheless, we don't need to. And the only other thing we're going to do is just
[1634.22s -> 1637.26s]  literally algebra. Okay, we're just going to take the second line and we're just going to
[1637.26s -> 1642.14s]  rearrange it so we get the reward function as a function of the optimal policy. And this
[1642.14s -> 1647.66s]  might be like, why are we doing this? But you'll see very soon. Now hopefully this sort of
[1647.66s -> 1652.86s]  makes sense a little bit, right? Like we have basically this idea now that we can parameterize
[1652.86s -> 1657.82s]  a reward function as a log probability ratio between some policy and the reference model.
[1657.82s -> 1660.78s]  Here's the optimal policy, but you can actually do this for any policy.
[1664.78s -> 1668.70s]  And when we put this all together to show sort of what this actually lets us do, because
[1668.70s -> 1672.86s]  so far, right, we've really just kind of done some silly algebra. We took our objective,
[1672.86s -> 1676.86s]  we wrote down an intractable closed form solution for it, and then we did some algebra
[1677.66s -> 1684.70s]  that doesn't seem to really be useful. How is it useful? Well, originally what we're
[1684.70s -> 1688.14s]  starting out with is a loss function on reward functions, right? This is this Bradley Terry loss
[1688.14s -> 1692.38s]  that we used in RLHF to turn our, you know, preference data into a reward function. That
[1692.38s -> 1697.02s]  hasn't changed. What we're going to do is we're going to add basically a transformation
[1697.02s -> 1701.26s]  between reward functions and policies to turn that loss function over reward functions that we
[1701.26s -> 1705.82s]  used before into a loss function directly on policies. So we can skip out the sort of
[1705.82s -> 1709.42s]  stage where we actually learn a reward model and then distill that into a policy.
[1709.42s -> 1713.82s]  We can just directly train on the preference data and optimize our policy directly, which
[1713.82s -> 1716.70s]  is why it's called direct preference optimization. So again, we have this loss
[1716.70s -> 1721.66s]  function on reward functions. Okay, this is the same from RLHF. This is just from
[1721.66s -> 1724.62s]  that Bradley Terry model. But does that, I mean, people remember this, right?
[1726.62s -> 1731.98s]  Okay, so this loss function, we stick in a reward, right, for the chosen thing and
[1731.98s -> 1737.34s]  for the rejected thing. And the probability of this model, right, is just the sigmoid of the
[1737.34s -> 1741.66s]  difference between these two rewards. And then we train our reward function so that it,
[1743.18s -> 1748.14s]  with maximum likelihood. So this is just the same loss function. And what we're going to do
[1748.14s -> 1751.34s]  is we're going to use this transformation I just showed on the last slide, right? So we showed
[1751.34s -> 1758.06s]  that for a particular policy, the reward function for which it is optimal takes this form.
[1758.94s -> 1765.34s]  Okay, so we can stick in any policy here and we get back, and if we sort of compute this
[1765.34s -> 1770.78s]  quantity, again this is intractable, but assuming we could, this is the reward function for,
[1770.78s -> 1776.30s]  the reward for a response y, which is, you know, evaluated by the reward function that
[1776.30s -> 1781.26s]  this is the optimal policy for. Okay, so here we have this transformation between
[1781.26s -> 1787.10s]  policies and reward functions. Goes both directions. And this is just from doing
[1787.42s -> 1793.42s]  algebra on the form of the optimal policy. And the key is if we stick this form, so now we have
[1793.42s -> 1798.94s]  sort of a reward function that is no longer just a general like transformer that you take a
[1798.94s -> 1803.90s]  sequence and it gives you a scalar. Now it has a special form where we have actually an
[1803.90s -> 1808.94s]  autoregressive model, and we compute the log probability of a response, and we subtract the
[1808.94s -> 1813.02s]  log probability of that same response according to the reference model or the SFT model, and the
[1813.02s -> 1819.02s]  difference there is the reward. Okay, so high reward stuff, right, is stuff where your policy
[1819.02s -> 1823.18s]  assigns higher probability than the SFT model. Low reward stuff is stuff where your model assigns
[1823.18s -> 1828.70s]  lower probability. Now if we stick this parametrization of the reward, so again this is
[1828.70s -> 1833.34s]  basically just a specific choice of how to parametrize the reward function, the z's cancel,
[1833.34s -> 1838.70s]  right, because we're assuming we have the same prompt for both the chosen and the rejected
[1838.70s -> 1845.02s]  example. And so when we stick this intractable parametrization of our reward into this just
[1845.02s -> 1849.34s]  normal Bradley Terry loss that we use to learn our reward functions, we actually get out this
[1849.34s -> 1855.42s]  totally tractable objective that we can use to directly train our policy. And that's DPO.
[1856.86s -> 1862.62s]  So this loss function here is really just the reward model loss. It's just that since we have
[1862.62s -> 1869.42s]  this identity that relates a policy to a reward function, we can just turn this loss function on
[1869.42s -> 1874.22s]  reward functions to a loss function on policies directly. Another way of thinking about this is
[1874.22s -> 1878.70s]  instead of training a reward model and then training a policy to get high reward under
[1878.70s -> 1885.58s]  that reward model, we are training a policy pi theta, which is the optimal policy for a reward
[1885.58s -> 1890.54s]  function that satisfies the Bradley Terry model for the preference data that we have. That's sort
[1890.54s -> 1894.78s]  of the other direction to think about this. Okay, so that was a little bit overwhelming,
[1894.78s -> 1900.70s]  but that's pretty much DPO. And so again, substituting in the log z term cancels, and
[1900.70s -> 1906.70s]  so we end up with this simple thing that this is just a classification objective. We don't
[1906.70s -> 1914.38s]  need to do any rollouts during training. We can just compute this. And what's kind of
[1914.38s -> 1918.78s]  interesting is that I mentioned there's this extra degree of freedom earlier. We've lost the
[1918.78s -> 1923.02s]  degree of freedom here because this thing is normalized, but we don't actually lose any
[1923.02s -> 1927.82s]  expressiveness in terms of the set of reward functions we can compute. So we had this
[1927.82s -> 1932.14s]  issue before that, okay, I have some reward, and now let's say I've parameterized my reward
[1932.14s -> 1937.50s]  function this way. So say I assigned some xy pair reward five. Well, we had this issue
[1937.50s -> 1942.70s]  earlier that you can just shift up all the rewards for that prompt by some constant,
[1943.42s -> 1947.82s]  and your optimal policy doesn't change at all. That's not true anymore because
[1947.82s -> 1952.62s]  this is normalized. This is a probability distribution. So in order to shift up the
[1952.62s -> 1957.18s]  reward for all responses by some amount, that means we have to increase the log probability
[1957.18s -> 1960.38s]  for all responses. We can't do that, right? You can't increase the log probability of
[1960.38s -> 1962.78s]  everything. It's not a probability distribution anymore. It's not going to add up to one.
[1963.34s -> 1965.90s]  So this is how we sort of get rid of that extra degree of freedom.
[1967.98s -> 1971.26s]  And you can see in the paper why it doesn't lose expressiveness, but it's not super important.
[1972.14s -> 1976.46s]  Okay, this is the most overwhelming part, so I want to... how do we feel?
[1980.30s -> 1981.02s]  Concerned? Yes.
[1981.90s -> 1987.90s]  So PyRef, is that just the model of the previous batch?
[1987.90s -> 1994.46s]  I'm sorry, this is bad. This is the SFT model. So this is not ever updated. PyRef is fixed
[1994.46s -> 2000.38s]  here. The only thing we're ever changing here is PyTheta. There's no previous batch thing like
[2000.38s -> 2006.30s]  in PPO. You can drain that from your mind forever. We just have one policy that we're
[2006.30s -> 2011.42s]  learning. There's no value function here. There's no sort of trust region from the
[2011.42s -> 2015.58s]  policy we had in the last batch. Yeah.
[2016.46s -> 2021.26s]  So I see that the expectation is over X, YW, YL.
[2021.26s -> 2021.58s]  Yes.
[2021.58s -> 2030.46s]  It seems to mean that you are still doing... you're still training on supervised stuff.
[2030.46s -> 2037.02s]  Does this get rid of the advantage of RL producing better than human responses from before?
[2039.02s -> 2047.90s]  No, it doesn't. So first off, the YW and YL are sampled from your model. So they're not...
[2047.90s -> 2052.62s]  those are not human written, but they're sampled from the SFT model, which is just
[2052.62s -> 2056.22s]  trained with imitation on humans. So they're not necessarily going to be better than humans,
[2056.22s -> 2060.62s]  but the idea is that you do get a little bit of exploration when you sample from that model
[2060.62s -> 2064.86s]  because it's just random. You sample a response, you're sampling token by token,
[2064.86s -> 2068.94s]  there's noise. And so in your data set, there are going to be examples where you did worse
[2068.94s -> 2072.22s]  than humans and some examples where you did a little bit better. And basically by getting
[2072.22s -> 2075.18s]  this preference data over those, you're picking out the ones that were... Oh, that one was
[2075.18s -> 2080.86s]  actually really good. And so you can do some sort of extrapolation in this behavior space.
[2081.82s -> 2085.34s]  And especially what you can do is you can iterate this. So you do one round of this,
[2085.34s -> 2089.02s]  you get a model that's a little better than your SFT model, and then you repeat. You sample
[2089.02s -> 2093.74s]  trajectories from this model, you get preferences on those, you train again, and so on and so
[2093.74s -> 2098.54s]  forth. And so in this way, you can also make a little bit of progress each time and keep
[2098.54s -> 2103.90s]  improving your model. Is that okay? No, it's not okay.
[2104.78s -> 2115.82s]  I mean, I can see why that would certainly help. It's a bit strange that it's no longer seemingly
[2115.82s -> 2122.78s]  on policy without doing that, though I guess on the other hand, figuring out a reward model
[2122.78s -> 2129.34s]  and then doing fun policy rollouts that potentially optimize that can also, as you said,
[2129.34s -> 2134.78s]  get stuck in weird language generation territory. So I do want to correct one thing. This is
[2134.78s -> 2140.94s]  really subtle. I don't want to get bogged down in it, but although we're not doing any
[2140.94s -> 2145.58s]  rollouts, I don't think it's totally right to say that DPO is off policy in the sense that
[2146.86s -> 2152.54s]  we think of off policy or offline learning as we're fitting our policy with some
[2152.54s -> 2156.46s]  offline data that's not from our policy and we're going to hopefully improve upon it.
[2157.18s -> 2159.90s]  Obviously, this is offline in the sense that we're not sampling from our policy,
[2160.46s -> 2166.30s]  but PPO is online in the sense that we're only finding an approximate optimal policy for our
[2166.30s -> 2172.30s]  reward model using policy samples. Here we're guaranteed to find the exact optimal policy
[2172.94s -> 2177.42s]  for the reward model that we fit, right? If we call this term here, this log ratio,
[2177.42s -> 2181.58s]  our implicit reward, we are guaranteed to get the thing that is optimal for that reward under
[2181.58s -> 2186.62s]  the expectation of that policy. So it's not on policy in the sense that we're not sampling from
[2186.62s -> 2189.98s]  it, but it's easy to think of that as maybe a disadvantage when actually it's an advantage
[2189.98s -> 2193.26s]  here. We're not giving anything up by not sampling from the model. It's just that we
[2193.26s -> 2196.14s]  happen to have a closed form solution for the optimal policy. So if anything,
[2196.14s -> 2198.06s]  this should give you something better than what DPO gives you.
[2200.14s -> 2206.14s]  Okay, I'm going to try to move on, but thank you for the questions. So to go back to this
[2206.14s -> 2213.10s]  big picture, what is DPO actually doing? Well, if we kind of look at the four main steps here,
[2214.38s -> 2220.46s]  it's really in this bottom right side where the action's happening. So compared to the PPO-based
[2220.46s -> 2223.98s]  approach, we're basically just skipping these two steps. We don't have to optimize the policy
[2223.98s -> 2229.34s]  anymore, and instead we just fine-tune this particular parametrization of the reward, and
[2229.34s -> 2232.94s]  we do this trivial transformation that doesn't require any training to get out the policy
[2232.94s -> 2243.02s]  that's optimal for that reward, right? Yes? So DPO doesn't use new prompts at all, right?
[2243.02s -> 2253.50s]  Yes. Can you talk a little bit about that? Yeah, so you're totally right. So when you look at
[2253.50s -> 2257.74s]  the PPO graph here, so we have this line going from our new prompts to the policy
[2257.74s -> 2261.74s]  optimization process. We don't do that anymore with DPO because we only use the prompts that
[2261.74s -> 2269.10s]  we use when we fit the reward model, and it's very natural to wonder if PPO is getting anything
[2269.10s -> 2272.78s]  by using these unlabeled prompts. Are you going to have a policy that generalizes better or
[2272.78s -> 2280.38s]  something like this? As far as we can tell, no, because the thing that's a little bit circular
[2280.38s -> 2285.34s]  about it is although you're using new unlabeled prompts, you're still bottlenecked by the
[2285.34s -> 2291.10s]  accuracy of your reward model on responses to those prompts, right? And so if new prompts are
[2291.10s -> 2295.02s]  going to be useful, it's because your reward model gave you accurate rewards. But the thing is,
[2295.02s -> 2299.50s]  DPO is using the exact same prompts that you're using to fit your reward model in PPO. And so
[2299.50s -> 2303.10s]  if that reward model is generalizing well, the DPO policy should also generalize well,
[2303.10s -> 2307.58s]  you might argue. So this is a hand-wavy argument, but that's sort of the thought process.
[2307.58s -> 2311.74s]  And at least when we've done some evaluations of this empirically, it doesn't seem like PPO
[2311.74s -> 2315.90s]  clearly generalizes better than DPO. If anything, the models we've evaluated is the other way
[2315.90s -> 2320.78s]  around. So it's a very interesting question. It's not at all been satisfyingly evaluated,
[2321.18s -> 2326.46s]  but in some anecdotal experiments, I would say, although there's reason to worry, there doesn't seem
[2326.46s -> 2329.10s]  to be like a big generalization issue. Yeah, it's a great question.
[2334.38s -> 2340.86s]  Okay. So yeah, we get rid of this picture, which is great. Get rid of a lot of complexity.
[2340.86s -> 2345.26s]  Cool. One thing I think that is sort of useful to look at is what is the gradient of
[2345.26s -> 2348.94s]  the DPO loss look like, because I think this basically tells you like how it works
[2349.90s -> 2354.22s]  other than this long silly derivation that I did. So what is this gradient when we differentiate
[2354.22s -> 2358.30s]  with respect to the policy parameters? Again, pyref here is fixed. This is just your
[2358.30s -> 2363.98s]  frozen SFT model. I should have done SFT ref is what's used usually. I'm so sorry.
[2365.18s -> 2370.94s]  So the gradient looks like this. And there's a lot going on there, but it's actually not
[2370.94s -> 2375.82s]  that complicated if we go step by step here. So really on the inside here, so again,
[2375.82s -> 2379.26s]  we just have our expectation over our data set that's not different. There are really just two
[2379.26s -> 2383.90s]  things going on here, right? We have one term that is doing maximum likelihood on the chosen stuff.
[2386.06s -> 2389.34s]  We have another term that's doing minimum likelihood or unlikelihood on the
[2389.34s -> 2393.66s]  dispreferred stuff. So all DPO is doing is push up the preferred thing, push down the
[2393.66s -> 2400.70s]  dispreferred thing, but what we also have is this per example weight, where this is this
[2400.70s -> 2404.94s]  per example weight is the sigmoid of the reward differences, but it's the other direction.
[2404.94s -> 2410.86s]  So here it's the reward of the loser minus the reward of the winner. And what this means is that
[2410.86s -> 2415.42s]  we get a higher weight when the reward model is incorrect on a preference pair and we lower
[2415.42s -> 2419.02s]  weight when it's already correct. And so what this does is basically I'm only going to train on
[2419.02s -> 2424.62s]  the stuff that my reward model is assigning sort of the incorrect polarity to which response
[2424.62s -> 2428.46s]  is better. And this is basically implicitly where our KL constraint comes in, right? Because
[2428.46s -> 2433.10s]  once we've gotten the reward on the chosen thing to be a little bit better than the reward
[2433.10s -> 2436.62s]  on the dispreferred thing, we're just going to stop training on that example altogether,
[2436.62s -> 2440.30s]  because this scaling factor is going to go to zero. And in the next slide, I'll show you
[2440.30s -> 2445.42s]  a comparison between just using this thing without that per example weight and using sort
[2445.42s -> 2451.74s]  of the full DPO loss. Does this make sense? Okay. Yeah, I think this is maybe the more
[2451.74s -> 2460.14s]  intuitive way to see what DPO actually does, honestly. And this is great. Archit, who's
[2460.14s -> 2463.18s]  one of the authors of the paper, originally did this analysis, which I think was really great
[2463.18s -> 2468.14s]  and insightful. And yeah, so the beta pops out, which is just the strength of our KL
[2468.14s -> 2472.94s]  constraint, which is essentially a learning rate. Okay, so a quick experiment and then we'll go
[2472.94s -> 2477.74s]  on to an application. So what we're going to do is we want to look at how DPO and other
[2477.74s -> 2481.42s]  methods trade off reward in KL, because again, our objective here, we haven't changed the
[2481.42s -> 2484.78s]  objective. We're still doing reward maximization with the KL constraint. And so the thing
[2484.78s -> 2488.30s]  that we might want to know when we compare algorithms is how efficiently do they make this
[2488.30s -> 2492.14s]  trade off? If you have a particular budget of KL, how much reward can you get from that?
[2492.14s -> 2495.42s]  And so that's what we're going to do in a relatively simple setting. We're going to do this
[2495.42s -> 2499.26s]  kind of stupid task where we want to maximize the sentiment of our generative model. So we're
[2499.26s -> 2502.14s]  going to get like the beginning of a movie review and we just want to complete it in the
[2502.14s -> 2506.86s]  most like positive way possible. So we're going to generate a synthetic data set. We generate
[2506.86s -> 2511.26s]  pairs of movie reviews using GPT-2, prefixed with like the beginning of a movie review,
[2511.26s -> 2514.54s]  so it generates the rest of one. We're going to use a ground truth reward function,
[2514.54s -> 2517.66s]  which we do not usually have in the real world, but so that we can do the study,
[2517.66s -> 2519.74s]  we're going to have it. This is just a sentiment classifier.
[2520.62s -> 2523.50s]  And this is what we're going to use to get our preference data. So we're just going to
[2523.50s -> 2526.54s]  take the pair of reviews, evaluate the sentiment classifier, which everyone has
[2526.54s -> 2530.06s]  more positive sentiment. That's the preferred one. Then we're going to train using DPO,
[2530.06s -> 2533.66s]  RLHF with PPO and some other methods. And then we're going to see, since we have the
[2533.66s -> 2537.74s]  ground truth reward function, we can actually plot this curve where the x-axis is KL and
[2537.74s -> 2541.10s]  the y-axis is reward and we can see what it looks like. Does this make sense?
[2541.10s -> 2548.70s]  OK. So, yeah, some baselines, we'll just see like, I'm actually,
[2549.42s -> 2552.70s]  do we actually have the base model? Good question. So we're going to preferred fine tuning,
[2552.70s -> 2555.58s]  which is just this is just if you fine tune on just the chosen
[2555.58s -> 2560.14s]  completion, just you might think maybe that will do something. We have it in there.
[2560.94s -> 2564.30s]  We have unlikelihood. So this is where you do DPO, but we don't have that per example
[2564.30s -> 2568.14s]  importance weight. So you're just doing maximum likelihood on all the preferred stuff,
[2568.14s -> 2572.86s]  minimum likelihood on all the just preferred stuff. Maybe this will work. Spoiler, it doesn't.
[2572.86s -> 2577.50s]  So then we have DPO, we have PPO using the learn reward. So this is like the normal
[2577.50s -> 2582.22s]  full RLHF pipeline. We also PPO using the ground truth reward function. So we don't even
[2582.22s -> 2586.14s]  worry about the HF part where we learn the reward and we just do PPO on the true
[2586.14s -> 2590.70s]  reward function. What happens? And this is what happens. So what's sort of interesting is
[2590.70s -> 2593.74s]  DPO does provide kind of the strongest reward KL frontier here.
[2594.54s -> 2602.62s]  And PPO doesn't actually end up giving sort of optimal reward, even when it's using the ground
[2602.62s -> 2607.58s]  truth reward function to optimize. And so that's kind of interesting and not obvious. I mean,
[2607.58s -> 2610.94s]  based on the fact that we have this KL constraint to our reference model, it makes
[2610.94s -> 2615.82s]  sense we might not actually achieve the maximum possible reward. But there is some gap here.
[2618.06s -> 2623.50s]  And this preferred FT sort of method really doesn't work so well in case you're worried
[2623.50s -> 2626.38s]  about that. What's also sort of interesting is this unlikelihood thing, right? This is where
[2626.38s -> 2630.06s]  we get rid of that, for example, importance weight in DPO. You can get a couple of good
[2630.06s -> 2632.54s]  policies here. Oh, and what I should say, I'm sorry, I should have explained this in the
[2632.54s -> 2637.50s]  beginning. Each dot here is a model checkpoint. So we did like lots of training runs of each
[2637.50s -> 2641.66s]  of these methods with different hyper parameters, different like KL constraints. I mean,
[2641.66s -> 2645.26s]  we just evaluated the reward and the KL for each one of those checkpoints and just plotted
[2645.26s -> 2649.58s]  them all together. And up until the left is where we want to be here.
[2649.58s -> 2654.30s]  Does it make sense why like this DPO curve is the good one here? That's very important
[2654.30s -> 2659.74s]  or else the slide was a waste of time. Yes. So the ground truth reward function gives
[2659.74s -> 2665.74s]  you a lot more information than the random variables that we started with. So how can
[2665.74s -> 2672.06s]  you use that? Yeah, it's a great question. So it doesn't, it doesn't. I mean, the
[2672.06s -> 2675.98s]  thing about the ground truth reward function in this case is it's sparse. So you either
[2675.98s -> 2680.38s]  get a one or a zero. So if the response is sort of like above some threshold of positivity,
[2680.38s -> 2683.98s]  the classifier is totally uncalibrated. It basically just gives you like one. And if it's
[2683.98s -> 2687.98s]  below some threshold, it gives you a zero. And so there's actually some, on some earlier
[2687.98s -> 2691.98s]  RLHF work, there's some interesting results along these lines where you can actually,
[2691.98s -> 2695.98s]  by doing human feedback, by using human feedback to learn a reward function, you can
[2695.98s -> 2699.98s]  actually end up with a reward function that's better shaped than the true reward function
[2699.98s -> 2703.98s]  and easier to learn from. So in principle, that's sort of how this kind of result could
[2703.98s -> 2707.98s]  end up is you might have the ground truth reward function, but it's actually very difficult to
[2707.98s -> 2711.98s]  learn from. And so the thing that you end up with in human feedback is easier.
[2711.98s -> 2715.98s]  So the noisy preferences are like smoothing over the functionary reward
[2715.98s -> 2721.98s]  Well, here our preferences are not noisy. It's just that you might end up with a reward
[2721.98s -> 2725.98s]  function because your model sort of is like has constrained capacity or it's pre-trained
[2725.98s -> 2729.98s]  so it has some good inductive biases. You might end up with a smoother reward function
[2729.98s -> 2733.98s]  that's easier to learn from than the sparse thing that always gives you one or zero.
[2733.98s -> 2739.98s]  Okay, we're running low on time, so I'm going to speed a little bit.
[2739.98s -> 2745.98s]  But one thing that I think is worth looking at is how does changing that coefficient beta in our original loss
[2745.98s -> 2749.98s]  or original objective, how does this actually change the KL that we end up with?
[2749.98s -> 2753.98s]  And fortunately, it changes it very predictably. So this is like four or five different training runs
[2753.98s -> 2757.98s]  and we're just plotting the log of beta with the log of the KL. And this is really worth
[2757.98s -> 2763.98s]  mentioning because when we do PPO, beta doesn't really fully define what KL we're going to end
[2763.98s -> 2767.98s]  up with. So if you run PPO twice with the same beta, you can get two policies with very
[2767.98s -> 2771.98s]  different KL just because it's very noisy and a little bit unstable.
[2771.98s -> 2775.98s]  And so people use this like dynamic beta controller in practice.
[2775.98s -> 2779.98s]  where they actually measure the policies KL during training and increase beta or decrease beta on the
[2779.98s -> 2783.98s]  fly to try to get the KL into a particular area. And that's just kind of annoying.
[2783.98s -> 2789.98s]  You don't really have to do that. Okay, one other thing. When does DPO fail?
[2789.98s -> 2793.98s]  So let's look at this example. So say we have some input that's like some long Reddit post.
[2793.98s -> 2797.98s]  We have like our chosen and rejected response, which are both summaries, they're TLDRs.
[2797.98s -> 2803.98s]  And then this is what the samples from our example are. And then we have our
[2803.98s -> 2807.98s]  and then this is what the samples from the model look like from our reference model.
[2807.98s -> 2811.98s]  DPO is not going to learn to generate the TLDR token here.
[2811.98s -> 2817.98s]  And this was really annoying and frustrating to us for a while when we were like doing this experiment.
[2817.98s -> 2823.98s]  And I would ask non-rhetorically why, but we don't have time, so I'm going to ask only rhetorically why.
[2823.98s -> 2828.98s]  And if we look at the gradient of the loss, we can see sort of why we can ignore most of it.
[2828.98s -> 2833.98s]  And we just look at these two maximum and minimum likelihood terms in the loss.
[2833.98s -> 2839.98s]  We can just break these up because it's just a sum of log probabilities for each conditional for every time step in the sequence.
[2839.98s -> 2846.98s]  And the key thing here is the first token in the chosen and rejected is TLDR.
[2846.98s -> 2853.98s]  So this term for time step zero is going to be the same for both of these and it's just going to cancel out.
[2853.98s -> 2856.98s]  And so you just never learn anything about the first token.
[2856.98s -> 2859.98s]  Sucks, but you know.
[2859.98s -> 2865.98s]  So that's why it's important to do this SFT stage as well. So right when you do SFT, you're going to learn the TLDR bit.
[2865.98s -> 2870.98s]  And then when you do RLHF, you're just learning the delta between the good and the bad stuff.
[2870.98s -> 2874.98s]  OK, but although this is kind of a silly example, this is how we originally sort of discovered this.
[2874.98s -> 2879.98s]  And it does make you wonder, like, what is the more general version of this problem?
[2879.98s -> 2889.98s]  What other maybe degenerate symmetries or kind of equivalences are there between our chosen and rejected thing that we might not learn, since we're only focusing on the difference between them.
[2889.98s -> 2892.98s]  It's worth thinking about. OK, so takeaways from DPO.
[2892.98s -> 2898.98s]  We can remove the RL training loop from RLHF. Yay. DPO is simple, stable, and it's cheaper to run. Yay.
[2898.98s -> 2903.98s]  It does optimize the same objective. So it's not an approximation that is cheaper.
[2903.98s -> 2908.98s]  You can more or less think of it as kind of the same thing that's cheaper.
[2908.98s -> 2914.98s]  Very small asterisks. And there's a lot of ongoing work still understanding DPO and generally improving RLHF.
[2914.98s -> 2918.98s]  I mean, DPO is not the end story here, right? Like, it's just another idea.
[2918.98s -> 2926.98s]  And there will be new ones and better ones probably in the next, like, six months, knowing how the field moves.
[2926.98s -> 2930.98s]  OK, so for our last five minutes, I'm going to briefly talk about one application of DPO.
[2930.98s -> 2934.98s]  It's going to go kind of quick because we only have five minutes.
[2934.98s -> 2939.98s]  But I want to talk about factuality briefly. So we know that language models just cannot be trusted.
[2939.98s -> 2948.98s]  So there's this infamous Bard demo where after, you know, much fanfare, Bard just, like, said false things about James Webb Space Telescope.
[2948.98s -> 2956.98s]  And it wasn't just Bard. So even in the Bing demo, which was much acclaimed, they did this nice demo where they analyzed GAP's quarterly report.
[2956.98s -> 2961.98s]  This is not my analysis. This is this guy's analysis. I have a link up there, so I don't want to claim credit for this.
[2961.98s -> 2965.98s]  But it's a great analysis. So it's like, OK, the first thing that Bing says, great, it's like, got it right.
[2965.98s -> 2971.98s]  That's so true. Wow, it's so smart. And then the next section, it's like, OK, well, it said they were the adjusted numbers.
[2971.98s -> 2974.98s]  They were actually the unadjusted numbers. That's very subtle. Fine.
[2974.98s -> 2979.98s]  And the next one, it's like, OK, that earnings per share is just not in the doc. You just made that one up.
[2979.98s -> 2984.98s]  And turns out that's true for the last paragraph, too. So the expected growth is not right and the future outlook is also made up.
[2984.98s -> 2986.98s]  So that's really disappointing.
[2986.98s -> 2991.98s]  There's another example with my advisor here where if you ask ChatGPT, where did Chelsea get her PhD?
[2991.98s -> 2996.98s]  It'll say Berkeley. That's true. If you say, where did she get her PhD? Answer in one sentence.
[2996.98s -> 3001.98s]  Berkeley. That's true. If you say, give me just the name of the university, but not a full sentence, it'll say Stanford.
[3001.98s -> 3008.98s]  And this is not a quirk of sampling. It is reproducible. I did it this morning.
[3008.98s -> 3011.98s]  So what the hell, man?
[3011.98s -> 3017.98s]  You know, so this this just tells you, though, how weird and mysterious sort of when the model is going to say something true or false is.
[3017.98s -> 3021.98s]  And this alone is, I think, a starting point for like a research project if you're interested in this problem.
[3021.98s -> 3024.98s]  But nonetheless, it's tempting to use these models anyway, which is problematic.
[3024.98s -> 3033.98s]  CNET got kind of burned by this. And there have also been some lawyers using ChatGPT to come up with some very interesting case law.
[3033.98s -> 3037.98s]  So can we like RLHF our way to better factuality?
[3037.98s -> 3042.98s]  Well, I mean, if you go back to this big picture, this is the DPO picture, like where would we even hope factuality to come from?
[3042.98s -> 3048.98s]  And I'll basically bucket these into parts. We have the pre training where we see trillions of tokens, lots and lots of data.
[3048.98s -> 3051.98s]  Maybe we'll say this is roughly where we learn what's true and what's false.
[3051.98s -> 3056.98s]  And we might hope it in these lighter weight fine tuning phases where we'll see three orders of magnitude less data.
[3056.98s -> 3060.98s]  We don't really have enough data to learn what's true and what's false here. But we can learn.
[3060.98s -> 3065.98s]  OK, I just want to say the true stuff. OK, so so this is sort of roughly how I break it down.
[3065.98s -> 3071.98s]  And there's actually research out there to suggest that this is not a totally imaginary way of thinking about this.
[3071.98s -> 3076.98s]  One other thing that's important is, OK, we already do RLHF. Why do we actually need to do anything special to get factuality?
[3076.98s -> 3080.98s]  Well, the answer is that RLHF encourages behaviors that make humans happy. Right.
[3080.98s -> 3084.98s]  And doing fact checking does not make humans happy. Let me tell you.
[3084.98s -> 3090.98s]  And so deciding, you know, is this actually correct is a lot harder than deciding, do I like it?
[3090.98s -> 3092.98s]  And I'm not just saying this because it sounds good.
[3092.98s -> 3103.98s]  Anthropic did a nice study of this just a few months ago, and they they ranked which attributes of of a response made it most likely to be the preferred thing.
[3103.98s -> 3107.98s]  And being truthful did not really end up on the top of the list.
[3107.98s -> 3114.98s]  In fact, if it agrees with your beliefs, surprise, surprise is the most predictive feature in whether someone is going to prefer something.
[3114.98s -> 3122.98s]  So we have to be really careful when we do RLHF because it's not even sure what it's not even clear what reward we're going to end up optimizing.
[3122.98s -> 3129.98s]  OK, so we basically want preference data that tells us what's more factual than when then each other. Humans are bad at this.
[3129.98s -> 3133.98s]  OK, this is a good opportunity for our with AI feedback. We're really running out of time here.
[3133.98s -> 3143.98s]  But the point is that we can basically do some truthfulness scoring automatically by using existing language models and using some reference like Wikipedia.
[3143.98s -> 3149.98s]  And we can get this preference data set where we have an input and then two responses.
[3149.98s -> 3153.98s]  One is more factual than the other. We can train this with DPO. We can evaluate it.
[3153.98s -> 3159.98s]  We evaluate it on bio generation and medical question answering. And turns out this works.
[3159.98s -> 3166.98s]  So if we compare to the SFT model right here, doing this factuality tuning significantly reduces the number of incorrect facts per response on average.
[3166.98s -> 3169.98s]  And it increases the number of correct facts per response.
[3169.98s -> 3175.98s]  I mean, it's the only method, you know, if we look at RLHF here, RLHF does not give the strict improvement.
[3175.98s -> 3180.98s]  OK, some other things that aren't that important. Takeaways on training on more factual elements.
[3180.98s -> 3185.98s]  This is really important. It's really hard. RLHF alone doesn't really solve the problem because humans are not good at this.
[3185.98s -> 3191.98s]  And we can do RL sort of without human feedback with DPO and sort of AI generated labels to improve factuality.
[3191.98s -> 3198.98s]  So to wrap this up here in my last 30 seconds, RLHF lets these generic LLMs be a lot more useful.
[3198.98s -> 3205.98s]  With just a little bit of RO. Picking this reward function is really important, but it's also really difficult for humans.
[3205.98s -> 3213.98s]  And we can use an implicit reward that we infer from preference labels, these discrete choices to learn this reward.
[3213.98s -> 3217.98s]  Classical RLHF works well, but it's very complex. DPO is a lot simpler without any approximations.
[3217.98s -> 3223.98s]  And we can use DPO to reduce things like hallucinations. And this is with only automated preference generation.
[3223.98s -> 3227.98s]  We don't need any humans to do this. There are a lot of other possible applications and problems.
[3227.98s -> 3231.98s]  But we're often bottlenecked by data. I want to give a big thank you to my collaborators, because this is not just my work.
[3231.98s -> 3242.98s]  This was really incredible work done by a lot of people. And I appreciate you all coming today.
[3242.98s -> 3247.98s]  So we're out of time. I'll hang out afterwards outside if people have questions.
[3247.98s -> 3256.98s]  This is my email and Twitter, if you also want to follow up with me. And Severn has a course on machine learning from human preferences that the slides are online, so you can take a look at those if you're curious.
