# Detected language: en (p=1.00)

[0.00s -> 5.28s]  Okay, in the next section I want to briefly discuss kind of another view that
[5.28s -> 9.24s]  we can take of these Q-learning algorithms that maybe provides a little
[9.24s -> 12.56s]  bit more of a unified perspective, because we covered lots of different
[12.56s -> 17.56s]  variants, we covered fitted Q-duration, online Q-learning, deep Q-learning with
[17.56s -> 22.82s]  replay buffers. We can unify all of these in one kind of conceptual
[22.82s -> 27.96s]  framework, and I just want to highlight this in the next portion. Many of you
[28.00s -> 31.52s]  might probably already suspect what that more general framework is, but let's just
[31.52s -> 36.80s]  make it really explicit. So here is the general Q-learning with replay buffers
[36.80s -> 41.76s]  and target networks that I discussed before. So here we have this outer outer
[41.76s -> 47.62s]  loop where we save the target network parameters, phi prime goes to phi, and we
[47.62s -> 52.08s]  could use this polyac averaging trick or just a standard flip every n steps.
[52.08s -> 56.76s]  In step two we collect some number of data points using some policy and add
[56.76s -> 60.72s]  to the buffer. In step three we sample a batch, and step four we take a gradient step,
[60.72s -> 65.76s]  and then we alternate steps three and four some number of times. Now this is
[65.76s -> 70.84s]  written out here as a kind of a inner loop style algorithm, but it's really
[70.84s -> 76.76s]  not, it's really a bunch of parallel processes. So the fitted Q- iteration
[76.76s -> 81.40s]  algorithm that we had before looks a little bit different. Here I've written
[81.40s -> 85.56s]  it in a way so that it better resembles the version above, but you
[85.60s -> 89.72s]  could also write it with supervised regression. So in that one we have the data
[89.72s -> 93.16s]  collection in the outer outer loop, and then the networks are updated in the
[93.16s -> 98.32s]  middle loop. But they're really kind of the same thing, and if you view them as
[98.32s -> 102.52s]  having the same basic processes running in parallel at different rates,
[102.52s -> 106.48s]  then all of these methods can be unified into one kind of parallel
[106.48s -> 113.00s]  framework. So in the fitted Q- iteration algorithm the inner inner loop is just
[113.00s -> 119.24s]  SGD, the DQN method we had before is a special case where n equals 1 and k
[119.24s -> 123.88s]  equals 1, but all of them are really just special cases of this more general
[123.88s -> 128.04s]  view. So we have our our data set of transitions, our replay buffer, this is
[128.04s -> 132.56s]  the basic object at the center of all of this. We periodically interact
[132.56s -> 136.20s]  with the world, and when we interact with the world what we typically do is
[136.20s -> 141.16s]  we take our latest vector phi, we construct some policy out of phi, for
[141.16s -> 144.76s]  instance using epsilon greedy or Boltzmann exploration, we send it out
[144.76s -> 151.68s]  into the world and it brings back one or more transitions. And you can think of
[151.68s -> 155.72s]  this not as a discrete decision that we make periodically, but as a continuous
[155.72s -> 159.28s]  process that is always running. So let's call it process one, the data collection
[159.28s -> 163.44s]  process. The data collection process takes steps in the environment, and each
[163.44s -> 170.36s]  step it takes it sends back to our replay buffer. Now our replay buffer is
[170.40s -> 175.04s]  a finite size, we can't just keep adding stuff to it forever, so we also have
[175.04s -> 178.84s]  another process, an eviction process, which periodically throws things out of
[178.84s -> 182.96s]  the buffer when it gets too big. There are a lot of decisions about how
[182.96s -> 186.88s]  and when you throw things out, but a very simple and reasonable choice is to
[186.88s -> 192.04s]  simply structure the replay buffer as a ring buffer, where the oldest thing in
[192.04s -> 196.48s]  the buffer gets thrown out when a new thing gets added in. So if your buffer
[196.52s -> 200.32s]  has one million transitions, then as soon as the one millionth and one-th
[200.32s -> 203.96s]  transition gets added in, then the oldest transition gets thrown in the
[203.96s -> 209.44s]  garbage, and that ensures that your buffer doesn't grow unbounded. Then you
[209.44s -> 213.68s]  have your target parameters phi prime, and your target parameters are used to
[213.68s -> 218.00s]  compute those target values. And you have your current parameters phi. Your
[218.00s -> 221.56s]  current parameters are the ones that you're going to give to process one in
[221.56s -> 226.96s]  order to construct that epsilon greedy policy to collect more data. And you
[226.96s -> 232.00s]  have a process two which updates the target values, sorry, the target
[232.00s -> 236.92s]  parameters. So process two will periodically copy phi into phi prime or
[236.92s -> 240.92s]  perform that polyac aberration. And process two is typically a very slow
[240.92s -> 245.88s]  process, so it typically runs very infrequently. And then you have
[245.88s -> 249.64s]  process three, which is kind of the main learning process, and what process
[249.64s -> 254.12s]  three does is it loads a batch of transitions from the replay buffer, so
[254.12s -> 258.24s]  that's step three in the pseudocode above. It loads in the target
[258.24s -> 263.00s]  parameters phi prime. It uses the target parameters to calculate target
[263.00s -> 268.02s]  values for every transition in the batch that was sampled. It uses that to update the
[268.02s -> 271.92s]  current parameters phi, that's step four above, and then saves them back out
[271.92s -> 279.12s]  into the current parameters right there. So this is a kind of graphical
[279.12s -> 284.64s]  depiction of a general Q-learning recipe that encompasses all of the
[284.64s -> 289.44s]  algorithms we've discussed. All of them can essentially be instantiated as
[289.44s -> 294.44s]  special cases of this general three-process, or four-process if you
[294.44s -> 299.64s]  also include eviction, parallel architecture. And in fact you could
[299.64s -> 302.46s]  actually implement this as a parallel architecture, you could actually have these as
[302.46s -> 307.16s]  separate processes in different threads, or you can implement it as a serial
[307.20s -> 310.44s]  sequential process, but this mental model that there are really three
[310.44s -> 313.84s]  different things that can all happen in different rates is still useful for
[313.84s -> 317.36s]  thinking about it. So even though it seems like there are many different Q-learning
[317.36s -> 321.20s]  algorithms, essentially they all just involve different decisions for the
[321.20s -> 329.20s]  rates at which we run process one, process two, and process three. So online
[329.20s -> 333.88s]  Q-learning, the basic basic Watkins online Q-learning that we had in the
[333.92s -> 338.92s]  previous lecture is a special case where you evict immediately, meaning the
[338.92s -> 344.00s]  size of your buffer is one, it's a ring buffer of size one, and then process one,
[344.00s -> 348.60s]  process two, and process three all run at the same speed, and they all take
[348.60s -> 353.02s]  one step sequentially. So process one takes one step which means collect one
[353.02s -> 357.26s]  transition, process two takes one step which means that your target values are
[357.26s -> 360.72s]  always computed using the latest parameters, and then process three takes
[360.72s -> 367.48s]  one step which means that you make one gradient update. The DQN algorithm that
[367.48s -> 373.76s]  we mentioned before is also pretty similar. Process one and process three
[373.76s -> 377.24s]  run at the same speed, which is a slightly arbitrary choice when you
[377.24s -> 379.72s]  think about it because process one and process three are actually fairly
[379.72s -> 383.40s]  decoupled, but they run at the same speed so you to always take one step
[383.40s -> 387.92s]  of data collection and one step of gradient update, and then process two is
[387.92s -> 394.48s]  very slow, and the replay buffer is quite large, so you might store up to a
[394.48s -> 399.76s]  million transitions. Part of why this starts looking so weird is that when
[399.76s -> 403.60s]  your replay buffer is large, process one and process three are pretty
[403.60s -> 407.24s]  heavily decoupled because once it's large enough, the probability you'll
[407.24s -> 412.16s]  sample the transition that you just collected becomes pretty low. It does
[412.16s -> 415.20s]  turn out that it's actually quite important to collect data pretty quickly,
[415.32s -> 419.20s]  so the performance of your Q-learning algorithm can degrade rapidly if you
[419.20s -> 422.88s]  don't collect data fast enough, but nonetheless process one and process
[422.88s -> 428.20s]  three have, you know, quite a bit of buffer space between them, literally. And
[428.20s -> 433.24s]  then the theta-q iteration algorithm that I used for kind of illustrating
[433.24s -> 436.88s]  these concepts can also be viewed as a special case of this. In theta-q
[436.88s -> 441.56s]  iteration, process three is actually in the inner loop of process two, which
[441.56s -> 445.84s]  itself is in the inner loop of process one. So in the theta-q iteration algorithm,
[445.84s -> 449.20s]  you do your regression all the way to convergence, then you update your target
[449.20s -> 451.96s]  number of parameters, and you might alternate these a few times, and then in
[451.96s -> 455.88s]  the outer outer loop you pop all the way back out and collect more data.
[455.88s -> 459.28s]  But these are really not all that different, they're just particular
[459.28s -> 464.80s]  choices about the rates at which we run all these different processes. And
[464.80s -> 469.40s]  there's something, of course, a little bit deeper about this because for each
[469.48s -> 473.36s]  of these processes, they each create non-stationarity for every other
[473.36s -> 478.36s]  process. So if process two and process one are completely halted, then process
[478.36s -> 483.12s]  three is just faced with a standard convergent supervised learning problem.
[483.12s -> 487.20s]  So by varying the rates of these different processes, by making them all
[487.20s -> 491.88s]  run at different rates, we're essentially mitigating the effects of non-stationarity.
[491.88s -> 495.88s]  Because if the rate of process two, for example, is very different from
[495.88s -> 500.16s]  process three, then process three, which is running a lot faster, to it it will
[500.16s -> 504.12s]  kind of look like everything is almost stationary. So that's the kind of deeper
[504.12s -> 507.00s]  reason why having these three different processes running at different
[507.00s -> 513.08s]  rates can help Q-learning algorithms converge more effectively.
