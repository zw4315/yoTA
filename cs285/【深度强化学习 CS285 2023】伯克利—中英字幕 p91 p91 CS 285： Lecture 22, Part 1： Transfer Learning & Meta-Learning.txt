# Detected language: en (p=1.00)

[0.00s -> 4.48s]  Today we're going to talk about transfer learning and meta-learning.
[4.48s -> 9.76s]  This deals with the question of how you can use experience from some source domains,
[9.76s -> 13.76s]  from some source RL problems, to get into a position where you can more efficiently
[13.76s -> 17.94s]  or more effectively solve new downstream tasks.
[17.94s -> 19.78s]  Let's start with a bit of motivation.
[19.78s -> 24.88s]  Let's think back to this example that we discussed in the first exploration lecture.
[24.88s -> 29.32s]  Some of the MDPs that we work with, some of the ATAR games for example, are fairly
[29.32s -> 34.16s]  straightforward to solve, even with relatively simple methods like the ones that you implemented
[34.16s -> 36.46s]  for Homework 3.
[36.46s -> 39.70s]  But some other games, which might not look all that different, seem to be a lot harder
[39.70s -> 40.92s]  to solve.
[40.92s -> 46.92s]  So if you try to learn a policy for Montezuma's Revenge with your Homework 3 Q-Learning algorithm,
[46.92s -> 50.76s]  you'll find that it really struggles to get past even the first level.
[50.76s -> 52.12s]  So why is that?
[52.12s -> 57.84s]  Well, the trouble has to do with the fact that these games look deceptively straightforward
[57.84s -> 62.60s]  to us, but are very difficult for the computer, because the computer doesn't come equipped
[62.60s -> 65.18s]  with prior knowledge that we have.
[65.18s -> 69.24s]  So in this game, the reward structure doesn't provide very good guidance on how to solve
[69.24s -> 70.24s]  the task.
[70.24s -> 73.48s]  You get a little bit of a reward for picking up the key, you get a little bit of a reward
[73.48s -> 76.04s]  for opening the door.
[76.04s -> 78.96s]  Getting killed by a skull is bad, but you don't actually get any negative reward
[78.96s -> 79.96s]  for it.
[79.96s -> 83.28s]  So the only reason that it's bad is because if you get killed by the skull enough, then
[83.28s -> 86.38s]  you lose the game and then you can't keep picking up keys and opening doors.
[86.38s -> 89.62s]  So this reward structure doesn't really provide you with a great deal of guidance about how
[89.62s -> 91.22s]  to make progress in the game.
[91.22s -> 95.46s]  And besides that, picking up the key and opening doors isn't necessarily the right
[95.46s -> 98.86s]  thing to do to win the game, because in later levels you can go to different places
[98.86s -> 103.22s]  and so on, and maybe you don't need to do all the steps that are rewarding to
[103.22s -> 106.40s]  get to the finish.
[106.40s -> 108.66s]  But for us, all these things don't pose a huge challenge.
[108.66s -> 111.46s]  In fact, when we're playing the game, we might not even be paying attention to the
[111.46s -> 112.46s]  score.
[112.54s -> 118.14s]  So we're actually using our prior knowledge, which informs us about how we should be making
[118.14s -> 119.22s]  progress in a game like this.
[119.22s -> 120.66s]  There's a visual metaphor at play here.
[120.66s -> 126.02s]  This is some kind of explorer exploring an ancient cursed temple, and we kind of have
[126.02s -> 129.66s]  this belief, both from our knowledge of video games and from our general physical
[129.66s -> 135.82s]  common sense, that skulls denote bad stuff, keys are good for opening doors, ladders
[135.82s -> 140.46s]  are things that can be climbed, and also we know from basic knowledge of video games
[140.46s -> 143.86s]  that progressing through different levels, entering new rooms and so forth is a good
[143.86s -> 145.54s]  way to make progress.
[145.54s -> 149.30s]  So our prior understanding of the problem structure can help us solve complex tasks
[149.30s -> 154.38s]  very quickly when something about these tasks accords with our expectations, which
[154.38s -> 156.50s]  is to say that it accords with our prior knowledge.
[156.50s -> 161.82s]  What we're doing when we learn to play Montezuma's Revenge is we're essentially doing
[161.82s -> 163.38s]  a kind of transfer learning.
[163.38s -> 166.60s]  We're transferring what we know about the world into this new MDP.
[166.60s -> 170.00s]  So while before we talked about Montezuma's Revenge as an example of a domain in which
[170.00s -> 177.04s]  to study exploration, now we're going to talk about these things as transfer learning
[177.04s -> 181.68s]  problems, as problems where maybe the right thing to do is not to devise a good de novo
[181.68s -> 185.82s]  exploration algorithm, but rather an algorithm that can transfer what you know from other
[185.82s -> 190.96s]  tasks that you've solved to solve this new task more effectively.
[190.96s -> 196.40s]  So can reinforcement learning use this prior knowledge the way that we do perhaps?
[196.40s -> 201.90s]  So could we, for example, build an algorithm that can watch lots of Indiana Jones movies
[201.90s -> 205.34s]  and use that to figure out how to solve Montezuma's Revenge?
[205.34s -> 211.36s]  Well, that's perhaps a very lofty goal, so we're realistically not quite there yet,
[211.36s -> 215.14s]  but we can start thinking about the transfer learning problem and we can actually devise
[215.14s -> 218.44s]  some pretty clever algorithms to address it.
[218.44s -> 222.76s]  So the idea is that if we've solved prior tasks, prior MDPs, we might acquire useful
[222.76s -> 226.30s]  knowledge for solving a new task.
[226.30s -> 229.70s]  How can this knowledge be encapsulated, how can it be represented?
[229.70s -> 233.10s]  Well, there are actually quite a few choices to be made here.
[233.10s -> 235.98s]  We could say that, well, the Q function is good to transfer because the Q function
[235.98s -> 240.08s]  tells us which actions or states are good, so if you somehow had an Indiana Jones Q
[240.08s -> 244.44s]  function for, you know, stealing the treasure, maybe that would be a good Q function
[244.44s -> 247.34s]  to initialize Montezuma's Revenge.
[247.34s -> 251.32s]  But maybe not, like, you know, in the video game you don't move around by moving
[251.32s -> 254.68s]  your legs and arms, you move around by pushing buttons, so perhaps the Q function doesn't
[254.70s -> 256.80s]  transfer that well in this case.
[256.80s -> 260.42s]  The policy tells us which actions are potentially useful, that can transfer really well.
[260.42s -> 263.90s]  Some actions are never useful, so as long as you can rule those out, then maybe you
[263.90s -> 266.02s]  can make more progress.
[266.02s -> 269.48s]  Or maybe you could transfer models, maybe the laws of physics, the laws that govern
[269.48s -> 272.82s]  how the world works are the same in both domains, even though some other factors
[272.82s -> 278.24s]  of the task are different, and transferring models can also be a very effective strategy.
[278.24s -> 281.62s]  It could also be that in more abstract settings, transferring some kind of feature
[281.64s -> 285.18s]  or some hidden state might provide you with a good representation.
[285.18s -> 288.24s]  So perhaps you don't have good actions for playing Montezuma's Revenge, you don't
[288.24s -> 291.42s]  even have good models because things don't quite line up, but the visual features might
[291.42s -> 292.42s]  help.
[292.42s -> 296.48s]  Maybe from watching some videos you figured out that skulls and ladders and keys are
[296.48s -> 300.00s]  important things in the world, and as long as you just start off the game with a good
[300.00s -> 305.84s]  skull, ladder, or key detector, that might already give you some mileage.
[305.84s -> 309.46s]  So for this last one, don't underestimate it, and it may actually be that a little
[309.46s -> 314.40s]  bit of visual pre-training can actually go a long way.
[314.40s -> 318.66s]  But before we go into the particular techniques, and I won't cover techniques for doing all
[318.66s -> 324.04s]  of these things, I'll cover just a sampling of influential ideas in this area.
[324.04s -> 326.98s]  Before we get into that, let's nail down some definitions.
[326.98s -> 330.78s]  So formally, transfer learning deals with the problem of using experience from one
[330.78s -> 336.50s]  set of tasks for faster learning and better performance on a new task.
[336.54s -> 340.86s]  Now, when we talk about transfer learning in the context of RL, a task is of course
[340.86s -> 341.86s]  an MDP.
[341.86s -> 346.38s]  So you can also read this as saying, use experience from one MDP, or from a set of
[346.38s -> 353.26s]  MDPs, for faster learning and better performance on a new MDP.
[353.26s -> 360.94s]  So the MDP that you train on, the one where you're getting the initial experience,
[360.94s -> 364.18s]  that's called the source domain, or source domains, because that's the source of your
[364.18s -> 365.58s]  knowledge.
[365.58s -> 369.34s]  And then the MDP that you want to solve, the new task, that's the target domain.
[369.34s -> 371.86s]  So that's the one where you want to get good results.
[371.86s -> 374.58s]  Classically, transfer learning is concerned with the question of how well you can do
[374.58s -> 378.94s]  in the target domain, although it's closely related to things like lifelong learning
[378.94s -> 382.26s]  or continual learning, where the aim is to do well on both the source and target
[382.26s -> 383.42s]  domains.
[383.42s -> 388.42s]  Of course, typically, it's pretty unusual to do well on a target domain without also
[388.42s -> 391.14s]  doing decently well on the source domains.
[391.14s -> 394.42s]  But there's a little bit of terminology where people will refer to things like backward
[394.46s -> 398.54s]  transfer as the question of whether after training on the target domain are you
[398.54s -> 400.30s]  still good on the source domain.
[400.30s -> 401.74s]  But for now, we won't concern ourselves with that.
[401.74s -> 404.54s]  We'll just say our goal is to do well on the target domain, regardless of what
[404.54s -> 408.86s]  happens in the source domains.
[408.86s -> 411.82s]  People will use a little bit of terminology to refer to how quickly you
[411.82s -> 413.66s]  learn in the target domain.
[413.66s -> 417.38s]  And the term shot is sometimes used to refer to this, like how many shots do you
[417.38s -> 419.86s]  take at trying the target domain?
[419.86s -> 423.26s]  So you could say, well, maybe my algorithm transfers in zero shots.
[423.26s -> 427.14s]  Zero shot means that without even trying anything in the target domain, right away,
[427.14s -> 430.22s]  right after the training on the source domains, you immediately get good
[430.22s -> 431.86s]  performance in the target domain.
[431.86s -> 436.26s]  So zero shot would be that if you interacted with some MDPs that involve
[436.26s -> 440.26s]  Indiana Jones stealing the treasure, right away, you'd get a policy that you
[440.26s -> 442.90s]  could simply deploy on the Montezuma's Revenge game, and it would immediately
[442.90s -> 443.94s]  play the game well.
[443.94s -> 446.98s]  So that would be called zero shot transfer.
[446.98s -> 449.62s]  One shot transfer means you tried the task once.
[449.62s -> 451.14s]  What that means is somewhat domain dependent.
[451.14s -> 454.10s]  So maybe for Montezuma's Revenge, it would mean that you play it for one
[454.10s -> 458.02s]  episode, basically until you run out of lives, or maybe if it's a robot
[458.02s -> 461.22s]  interacting with the world, maybe the robot will make one attempt.
[461.22s -> 464.94s]  Few shot means you try the task a few times, and many shot means you
[464.94s -> 466.58s]  try the task many times.
[466.58s -> 472.22s]  So these are not necessarily very precise terms, but it can provide good
[472.22s -> 475.14s]  indication if you read a paper and you see one shot or zero shot to get
[475.14s -> 478.50s]  a sense for what's going on.
[478.50s -> 481.58s]  So how can we frame these transfer learning problems?
[481.58s -> 485.58s]  Well, I'll start off by saying that there isn't really any one single
[485.58s -> 486.50s]  problem statement here.
[486.50s -> 490.18s]  So a lot of what we discussed so far in the course has been fairly
[490.18s -> 494.18s]  foundational material with fairly well understood theory and guidelines
[494.18s -> 495.18s]  on how to do this.
[495.18s -> 498.58s]  A lot of transfer learning is a little bit ad hoc, because it's so dependent on
[498.58s -> 501.18s]  the nature of the domain that you're transferring from and the one that
[501.18s -> 502.50s]  you're transferring to.
[502.50s -> 505.02s]  So if you want to pre-train on Indiana Jones videos and play
[505.02s -> 507.82s]  Montezuma's Revenge, you might use a very different algorithm
[507.82s -> 510.70s]  than if, for example, you wanted to train a robot to grasp lots of
[510.70s -> 514.30s]  objects and then deploy it to grasp a new object.
[514.30s -> 517.86s]  But there are still a few ideas, a few common ideas that people use,
[517.86s -> 520.50s]  and that's what I'll try to cover in today's lecture.
[520.50s -> 523.90s]  So keep in mind that I'll only cover a smattering of the ideas, not
[523.90s -> 532.38s]  everything, and I'll try to cover the main central hub ideas that can be
[532.38s -> 535.78s]  used in a variety of settings.
[535.82s -> 537.70s]  So I'll talk about forward transfer.
[537.70s -> 540.58s]  So forward transfer deals with learning policies that transfer
[540.58s -> 544.98s]  effectively, where you might train on a source task, and maybe you
[544.98s -> 547.54s]  just run something on a target task, or maybe you fine-tune on a
[547.54s -> 549.94s]  target task.
[549.94s -> 552.70s]  Conventionally, this relies on the task being quite similar, and a
[552.70s -> 556.38s]  lot of work on forward transfer deals with finding ways to make the
[556.38s -> 559.42s]  source domain either look like the target domain or make it so that
[559.42s -> 563.22s]  the policy you recover from the source domain is more likely to
[563.22s -> 566.82s]  transfer to the target domain.
[566.82s -> 569.50s]  The other big area is multitask transfer, where you train on many
[569.50s -> 572.10s]  different tasks and then transfer to a new task.
[572.10s -> 573.78s]  And that could work really well, because now instead of
[573.78s -> 577.02s]  relying on a single source domain being close to the target
[577.02s -> 580.46s]  domain, you could get many source domains so that the target
[580.46s -> 582.98s]  domain is sort of within their convex hull intuitively.
[582.98s -> 587.50s]  So if you want a robot to grasp a new object, and you've
[587.50s -> 590.90s]  only ever trained it on one other object before, it might be
[590.90s -> 592.02s]  very hard for that to transfer.
[592.02s -> 594.26s]  But if you train on many different objects, then this new
[594.26s -> 596.98s]  object might look kind of similar to the range of things
[596.98s -> 597.86s]  you've seen.
[597.86s -> 601.34s]  So multitask transfer tends to be easier.
[601.34s -> 602.98s]  There are a variety of ways to do it, sharing
[602.98s -> 606.90s]  representations on layers across tasks, or maybe simply
[606.90s -> 610.02s]  just training a policy that is conditioned on some
[610.02s -> 612.18s]  representation of task and generalizes to the new one
[612.18s -> 613.38s]  immediately.
[613.38s -> 615.50s]  It does typically require the new task to be similar to the
[615.50s -> 618.22s]  distribution of training tasks, but that's often easier to
[618.22s -> 621.30s]  achieve than ensuring that your single source task is
[621.30s -> 625.22s]  similar to your single target task.
[625.22s -> 629.62s]  And then for a large chunk of today's lecture, we're going
[629.62s -> 631.50s]  to actually talk about something called meta-learning.
[631.50s -> 633.38s]  And meta-learning you can think of as the logical
[633.38s -> 636.42s]  extension of transfer learning, where instead of trying
[636.42s -> 641.34s]  to simply train on some source domain or domains and
[641.34s -> 643.54s]  succeed in the target domain, either in zero shot or
[643.54s -> 646.06s]  with naive fine-tuning, in meta-learning, we're actually
[646.06s -> 649.70s]  going to train a special way in our source domains in a
[649.74s -> 651.34s]  way that is aware of the fact that we're going to be
[651.34s -> 653.46s]  adapting to a new target domain later.
[653.46s -> 655.46s]  So meta-learning is often framed as a problem of
[655.46s -> 656.78s]  learning to learn.
[656.78s -> 658.70s]  Essentially, you're going to try to solve those
[658.70s -> 661.02s]  source domains, not necessarily in a way that gets you a
[661.02s -> 662.50s]  really great solution on all of them, but in a way
[662.50s -> 665.66s]  that prepares you to solve new domains.
[665.66s -> 667.78s]  So it accounts for the fact that we'll be adapting
[667.78s -> 670.70s]  to a new task during training.
[670.70s -> 673.78s]  So we'll talk about each of these things.
[673.78s -> 675.86s]  And I'll actually spend most of the time today on
[675.86s -> 678.30s]  meta-learning, but I will briefly go over forward
[678.30s -> 680.26s]  about meta-learning transfer, multitask transfer.
[680.26s -> 682.14s]  Part of the reason why I like to spend more time on
[682.14s -> 684.50s]  meta-learning in these lectures is that in some ways
[684.50s -> 687.02s]  that's the area where there's a little bit more in
[687.02s -> 689.78s]  the way of principles and common themes, things that we
[689.78s -> 692.34s]  can learn that we can use in many different settings.
[692.34s -> 696.98s]  A lot of the non-meta-learning transfer work,
[696.98s -> 699.06s]  it's very deep, there's a lot of interesting work there,
[699.06s -> 700.50s]  but it tends to be a little scattered, so it's a little
[700.50s -> 703.74s]  hard to nail down a small set of principles.
[703.74s -> 705.66s]  But I'll do my best to nail down those principles
[705.70s -> 708.78s]  that appear to be broadly applicable, and hopefully
[708.78s -> 710.34s]  that'll get you some idea for where to go.
[710.34s -> 711.94s]  And then I'll also have lots of references that you
[711.94s -> 714.30s]  could read if you want to dive deeper into this.
[714.30s -> 716.50s]  But keep in mind that these things really are
[716.50s -> 719.58s]  the frontier of research, and there isn't sort of
[719.58s -> 723.06s]  one set of algorithms that you could just take
[723.06s -> 724.62s]  and use for whatever transfer learning problem
[724.62s -> 725.62s]  you might encounter.
[727.54s -> 729.98s]  Okay, so let's talk a little bit about pre-training
[729.98s -> 732.14s]  and fine-tuning in RL.
[732.14s -> 735.18s]  So outside of reinforcement learning,
[735.74s -> 739.10s]  if you were to think about addressing just kind of
[739.10s -> 742.22s]  general transfer learning problems, let's say,
[742.22s -> 745.38s]  in computer vision, a very popular approach
[745.38s -> 748.18s]  is to train some kind of representation
[748.18s -> 750.34s]  on a large data set, like maybe you train a ConvNet
[750.34s -> 752.14s]  on a large data set of images, or you train
[752.14s -> 754.78s]  a language model on a large data set of text,
[754.78s -> 756.62s]  like something like a BERT model.
[756.62s -> 759.50s]  And you use that to basically extract representations.
[759.50s -> 760.94s]  These could be representations of images,
[760.94s -> 763.38s]  representations of text, something like that.
[763.42s -> 765.94s]  And then you would train a few additional layers
[765.94s -> 768.02s]  on top of those, maybe just a few fully connected layers,
[768.02s -> 769.94s]  or maybe you would fine-tune the entire network
[769.94s -> 771.82s]  to solve a particular task that you want
[771.82s -> 772.94s]  for which you have a comparatively more
[772.94s -> 774.26s]  limited amount of data.
[774.26s -> 775.94s]  And this is a pretty standard workflow
[775.94s -> 778.38s]  across a range of supervised learning domains.
[778.38s -> 780.18s]  And you could imagine employing something like this
[780.18s -> 782.30s]  in reinforcement learning too.
[782.30s -> 784.06s]  In some cases, this will actually work out of the box.
[784.06s -> 785.38s]  So you could learn representations
[785.38s -> 786.94s]  with reinforcement learning,
[786.94s -> 788.94s]  and then take those representations
[788.94s -> 791.26s]  and use a reinforcement learning algorithm
[791.26s -> 794.98s]  to fine-tune an additional few layers on top of those
[794.98s -> 796.22s]  for solving your task.
[796.22s -> 797.46s]  You could also learn representations
[797.46s -> 798.62s]  with supervised learning,
[798.62s -> 801.14s]  and then fine-tune a few layers on top of those
[801.14s -> 801.98s]  with reinforcement learning
[801.98s -> 803.74s]  to solve your reinforcement learning task.
[803.74s -> 806.26s]  So these things are all fairly straightforward.
[806.26s -> 807.74s]  I won't go into them into too much detail
[807.74s -> 810.58s]  because these are kind of the standard techniques
[810.58s -> 813.78s]  that we would imagine inheriting from supervised learning.
[813.78s -> 815.54s]  So you can learn about that
[815.54s -> 817.66s]  in your favorite deep learning class,
[817.66s -> 819.66s]  and it's not really all that different.
[819.66s -> 821.74s]  But I will talk about a few peculiarities
[821.74s -> 825.22s]  and a few tools that, to me at least,
[825.22s -> 827.26s]  have proven to be especially useful
[827.26s -> 829.42s]  when applying this kind of pre-training
[829.42s -> 831.74s]  and fine-tuning schema in RL.
[833.78s -> 834.62s]  So before we do that,
[834.62s -> 836.34s]  let's talk about what issues we're likely to face
[836.34s -> 838.02s]  when we do this.
[838.02s -> 839.46s]  And these are not necessarily issues
[839.46s -> 841.02s]  that are exclusive to reinforcement learning,
[841.02s -> 842.42s]  but they tend to come up in reinforcement learning
[842.42s -> 844.78s]  pretty often from my experience.
[844.78s -> 846.30s]  One issue is domain shift.
[846.30s -> 847.86s]  And this is a fairly obvious one.
[847.90s -> 850.58s]  Basically, representations learned in the source domain
[850.58s -> 853.30s]  might not work well in the target domain.
[853.30s -> 855.42s]  This happens very often if you imagine,
[855.42s -> 859.46s]  for example, learning a task in some kind of simulator
[859.46s -> 861.78s]  that simulates visual observations
[861.78s -> 863.46s]  or other kinds of high-dimensional observations
[863.46s -> 864.82s]  like sounds and so on,
[864.82s -> 867.62s]  and then transferring the resulting policy
[867.62s -> 870.22s]  to some maybe real-world environment
[870.22s -> 872.10s]  where the observations are structurally similar,
[872.10s -> 873.46s]  but not exactly the same.
[873.46s -> 875.74s]  So if you trained a driving policy
[875.74s -> 877.62s]  to drive a car in a video game,
[878.22s -> 879.50s]  and then you wanted to drive a real car,
[879.50s -> 880.66s]  things are not that different.
[880.66s -> 881.50s]  Things basically line up.
[881.50s -> 883.06s]  The physics are similar.
[883.06s -> 884.98s]  The mechanics of the environment are similar.
[884.98s -> 886.86s]  But things don't look exactly the same.
[886.86s -> 888.46s]  So there's a little bit of a gap.
[891.34s -> 892.74s]  Now, it could be that the gap is even larger.
[892.74s -> 894.70s]  It could be that the gap is not merely perceptual.
[894.70s -> 896.06s]  It could be that there's actually some things
[896.06s -> 897.18s]  that you can do in the source domain
[897.18s -> 900.50s]  that are not possible in the target domain at all.
[900.50s -> 901.98s]  And this is actually a big difference.
[901.98s -> 903.74s]  So the first category just deals
[903.74s -> 905.86s]  with things being different visually,
[905.86s -> 907.14s]  but not necessarily different mechanically.
[907.50s -> 908.82s]  The second category deals with things
[908.82s -> 912.14s]  actually being different in a physical sense,
[912.14s -> 913.62s]  but still structurally similar enough
[913.62s -> 914.74s]  that you feel like there is something
[914.74s -> 916.78s]  from the source domain that you can inherit.
[917.82s -> 919.54s]  This is much harder, but there are also tools
[919.54s -> 921.38s]  that we could use to deal with this.
[922.74s -> 924.70s]  And then there's also some issues that we get
[924.70s -> 927.42s]  with just applying the notion of fine-tuning
[927.42s -> 928.78s]  in general to RL.
[929.78s -> 933.90s]  For example, the fine-tuning process may need to explore
[933.90s -> 935.46s]  in the new target domain.
[935.46s -> 937.54s]  But remember that the optimal policy
[937.54s -> 940.14s]  in any fully-observed MDP can be deterministic.
[940.14s -> 941.82s]  So you might end up with a deterministic policy
[941.82s -> 943.14s]  after running, let's say, a policy gradient
[943.14s -> 944.06s]  in your source domain.
[944.06s -> 945.46s]  You deploy it in the target domain,
[945.46s -> 946.46s]  and it doesn't explore anymore
[946.46s -> 948.22s]  because it has become fully deterministic
[948.22s -> 950.70s]  because that was the optimal solution in the target domain.
[950.70s -> 952.34s]  So there are a few of these kind of
[952.34s -> 953.70s]  low-level technical issues
[953.70s -> 955.22s]  that you might need to deal with.
[957.30s -> 958.58s]  Let's talk about the first issue,
[958.58s -> 959.94s]  the domain shift problem.
[960.90s -> 963.58s]  There are a number of tools that have been developed
[963.62s -> 965.58s]  principally in the computer vision community
[965.58s -> 966.86s]  for dealing with these kinds of problems.
[966.86s -> 969.62s]  And I should say that I'm going to discuss these issues
[970.66s -> 973.02s]  as they relate to visual perception,
[973.02s -> 973.86s]  but these are not things
[973.86s -> 975.26s]  that are exclusive to visual perception,
[975.26s -> 977.10s]  and they're kind of general issues
[977.10s -> 980.10s]  with high-dimensional observations, of course.
[980.10s -> 981.26s]  Those high-dimensional observations
[981.26s -> 983.02s]  often do tend to be visual
[983.02s -> 985.82s]  because that's where we often want to use simulation,
[985.82s -> 988.98s]  for example, and we encounter the most challenges.
[989.82s -> 993.58s]  So let's think about this example
[993.58s -> 996.02s]  of learning to drive in simulation.
[996.02s -> 999.22s]  So we want to train the images in the simulator,
[999.22s -> 1002.14s]  and we want to do well when the policy
[1002.14s -> 1005.02s]  is presented with images in the real world.
[1005.02s -> 1006.74s]  So we're going to, of course,
[1006.74s -> 1008.22s]  be training our network in the simulator,
[1008.22s -> 1011.10s]  and then we'll use that network in the real world.
[1011.10s -> 1012.78s]  And let's imagine that we have
[1012.78s -> 1015.18s]  some small number of real-world images.
[1015.18s -> 1017.74s]  We might not even have real-world experience.
[1017.74s -> 1019.74s]  We might simply have a few images
[1019.74s -> 1022.82s]  that kind of anchor us to the real world.
[1022.82s -> 1024.90s]  So we might not even know anything about the actions.
[1024.90s -> 1027.30s]  They're just examples of real-world photographs.
[1030.26s -> 1034.18s]  We can supervise the simulated experience
[1034.18s -> 1035.98s]  with the correct answer.
[1035.98s -> 1037.30s]  This could be supervised learning,
[1037.30s -> 1038.34s]  or it could be reinforced learning,
[1038.34s -> 1039.38s]  so it could be the correct answer here
[1039.38s -> 1040.58s]  means a target Q value.
[1040.58s -> 1041.42s]  That kind of doesn't matter.
[1041.42s -> 1042.70s]  It's just some kind of loss
[1042.70s -> 1044.58s]  that you put on top of your network.
[1045.42s -> 1048.58s]  But of course, when we then evaluate that model
[1048.58s -> 1051.30s]  on the real-world image, we might get an incorrect answer
[1052.46s -> 1054.70s]  because the real-world image looks different.
[1056.22s -> 1058.66s]  So one assumption that allows us to deal with this
[1058.66s -> 1060.62s]  is something called the invariance assumption.
[1060.62s -> 1062.30s]  The invariance assumption says that
[1062.30s -> 1063.26s]  everything that is different
[1063.26s -> 1065.54s]  between the two domains is irrelevant.
[1067.30s -> 1069.70s]  Let's pause for a minute to think about what this means.
[1069.70s -> 1070.70s]  Everything that is different
[1070.70s -> 1072.86s]  between the domains is irrelevant.
[1075.34s -> 1079.22s]  So maybe the simulator doesn't simulate rain,
[1079.22s -> 1081.78s]  but the real-world images might have rain in them.
[1081.78s -> 1083.70s]  The invariance assumption would imply
[1083.70s -> 1085.02s]  that whether or not it's raining
[1085.02s -> 1087.06s]  is irrelevant for how you should drive.
[1087.94s -> 1091.42s]  On the other hand, the positions of the cars on the road
[1091.42s -> 1093.86s]  would match in the simulation in the real world,
[1093.86s -> 1095.46s]  well, statistically, of course.
[1096.74s -> 1099.66s]  So that is not irrelevant, that is irrelevant to you.
[1099.66s -> 1101.42s]  Now, is this assumption reasonable?
[1102.42s -> 1103.98s]  Well, sort of.
[1103.98s -> 1105.98s]  So in reality, the rain might actually affect
[1105.98s -> 1108.46s]  how you drive, but it's not a bad assumption
[1108.46s -> 1109.54s]  to make in many cases
[1109.54s -> 1111.22s]  if you believe that most of the discrepancies
[1111.22s -> 1113.10s]  don't have to do with the physics or the dynamics,
[1113.10s -> 1116.02s]  but they really have to do with how things look.
[1116.02s -> 1119.22s]  So if you subscribe to the invariance assumption,
[1119.22s -> 1120.90s]  you can write it out formally.
[1120.90s -> 1124.42s]  So let's say that the images are denoted with x.
[1124.42s -> 1127.10s]  Formally, what this means is that p of x is different.
[1127.10s -> 1128.66s]  So you have a different distribution of images
[1128.66s -> 1131.06s]  in the source domain and in the target domain.
[1131.66s -> 1133.62s]  But there exists some representation,
[1133.62s -> 1135.58s]  let's call it z equals f of x.
[1135.58s -> 1137.38s]  Basically, there's some way to featurize x
[1137.38s -> 1138.42s]  with a featurizer f,
[1140.50s -> 1143.94s]  so that the probability of the output y given z
[1143.94s -> 1147.18s]  is the same as the probability of the output given x.
[1147.18s -> 1149.62s]  So that means that essentially if you featurize x
[1149.62s -> 1153.46s]  using f of x, you retain all the information needed
[1153.46s -> 1155.46s]  to predict the label or to predict the Q value
[1155.46s -> 1156.78s]  or to predict the action.
[1156.78s -> 1159.02s]  So that just means the representation is not lossy.
[1159.50s -> 1163.70s]  But p of z is the same in the source and the target domain.
[1165.26s -> 1167.22s]  So just to unpack this statement again,
[1168.10s -> 1169.66s]  p of x, the distribution of inputs
[1169.66s -> 1171.62s]  is different in the two domains,
[1171.62s -> 1174.58s]  but there exists some featurization, z equals f of x,
[1174.58s -> 1177.66s]  so that p of z is the same in the two domains
[1177.66s -> 1182.22s]  and p of y given z is equal to p of y given x,
[1182.22s -> 1185.90s]  meaning that z contains everything you need to predict y.
[1185.90s -> 1188.26s]  If you can find such a representation
[1188.30s -> 1190.34s]  and the invariance assumption holds,
[1190.34s -> 1192.62s]  then you will be able to transfer perfectly.
[1196.18s -> 1198.10s]  Okay, we can pause for a minute
[1198.10s -> 1201.10s]  and think about the implications of this more.
[1201.10s -> 1206.10s]  For example, if this assumption doesn't hold perfectly
[1206.10s -> 1207.78s]  but it holds mostly,
[1207.78s -> 1209.94s]  you could imagine that you would be in a situation
[1209.94s -> 1213.50s]  where you find a z,
[1214.42s -> 1218.02s]  where p of z is the same in the source and target domain,
[1218.02s -> 1219.62s]  but perhaps p of y given z
[1219.62s -> 1221.58s]  is not exactly the same as p of y given x,
[1221.58s -> 1223.62s]  perhaps something was lost.
[1223.62s -> 1225.22s]  And that might happen in the example with the rain.
[1225.22s -> 1227.94s]  So if you neglect the rain,
[1227.94s -> 1229.98s]  you can still solve the task mostly,
[1229.98s -> 1232.14s]  but maybe your solution will not be as good
[1232.14s -> 1235.46s]  as what you would have gotten if you had held on to that.
[1235.46s -> 1236.74s]  But of course you can't hold on to it
[1236.74s -> 1238.66s]  because there's no rain in the simulator.
[1239.62s -> 1241.30s]  So there are a number of ways
[1241.30s -> 1244.38s]  to acquire these kinds of invariant representations.
[1244.38s -> 1247.26s]  One of the most commonly used techniques
[1248.18s -> 1249.98s]  it actually goes under a variety of different names,
[1249.98s -> 1251.02s]  domain confusion,
[1251.02s -> 1254.30s]  domain adversarial neural networks, and so on.
[1254.30s -> 1256.34s]  But the idea is basically the following.
[1256.34s -> 1258.14s]  We're going to take some middle layer
[1258.14s -> 1261.06s]  in these neural networks.
[1261.06s -> 1264.06s]  People often take the layer right after the convolutions.
[1264.06s -> 1266.30s]  And then we add an additional loss
[1266.30s -> 1268.74s]  to force that layer to be invariant.
[1269.62s -> 1271.18s]  Invariant means that
[1271.18s -> 1273.54s]  if the activation in that layer are denoted with z,
[1273.54s -> 1274.70s]  it means that p of z
[1274.70s -> 1276.94s]  is the same in the source and target domain.
[1276.94s -> 1277.86s]  And that's where we need a few
[1277.86s -> 1279.94s]  of those target domain images.
[1281.42s -> 1282.62s]  So what we're going to do
[1282.62s -> 1284.98s]  is we're going to train a binary classifier
[1284.98s -> 1288.14s]  that gets to look at the activations at that layer.
[1288.14s -> 1291.54s]  So the classifier d5 of z.
[1291.54s -> 1293.34s]  And it's going to predict true
[1293.34s -> 1295.26s]  if it is in the target domain
[1295.26s -> 1298.18s]  and false if it's in the source domain.
[1298.18s -> 1301.30s]  And then we'll compute the gradient of that classifier
[1301.30s -> 1305.06s]  with respect to z and we'll reverse the gradient
[1305.06s -> 1306.42s]  and back that up into the network.
[1306.42s -> 1307.90s]  So we'll basically teach the network
[1307.90s -> 1311.46s]  to produce such a z, such that a classifier can't tell
[1311.46s -> 1314.10s]  whether it came from the source or the target domain.
[1315.22s -> 1316.74s]  Now, of course, there are a variety of details
[1316.74s -> 1317.90s]  on how to do this right,
[1317.90s -> 1319.42s]  whether you reverse the gradient
[1319.42s -> 1322.38s]  or whether you train the classifier
[1322.38s -> 1323.66s]  to output the opposite label
[1323.66s -> 1326.62s]  or you train the classifier to output a probability of 0.5.
[1326.62s -> 1327.58s]  There's a bunch of these details
[1327.58s -> 1329.46s]  that do actually matter in practice,
[1329.46s -> 1331.54s]  but this is the high level idea.
[1331.54s -> 1333.22s]  And that's why in order to do this,
[1333.26s -> 1336.54s]  you do need some examples from the target domain,
[1336.54s -> 1340.02s]  but you don't necessarily need to run RL
[1340.02s -> 1340.86s]  in the target domain.
[1340.86s -> 1342.74s]  You just need some example images.
[1344.82s -> 1347.42s]  You do want to be a little careful with this idea.
[1347.42s -> 1349.50s]  There are some ways in which you could go wrong.
[1349.50s -> 1353.02s]  For example, if you have bad data from the target domain,
[1353.02s -> 1355.22s]  data of let's say very bad human drivers,
[1355.22s -> 1356.82s]  and then you run reinforcement learning
[1356.82s -> 1358.06s]  in the source domain
[1358.06s -> 1360.74s]  and you end up with very good behavior,
[1360.74s -> 1362.74s]  very good driving,
[1363.10s -> 1365.58s]  then your representation will not only be invariant
[1365.58s -> 1368.18s]  to whether you're in the simulator or in the real world,
[1368.18s -> 1369.26s]  it'll also try to be invariant
[1369.26s -> 1370.78s]  to whether you're good or bad.
[1370.78s -> 1372.98s]  And you really don't want that, right?
[1372.98s -> 1375.90s]  Because that will really mess up your Q-learning algorithm.
[1375.90s -> 1378.62s]  So you have to be a little careful about this
[1378.62s -> 1379.50s]  and the nature of the data
[1379.50s -> 1380.42s]  that you get in the target domain
[1380.42s -> 1382.94s]  actually really does affect how well this trick works,
[1382.94s -> 1385.34s]  but it can be a very effective trick in practice.
[1388.34s -> 1389.38s]  Now I mentioned that sometimes
[1389.38s -> 1390.54s]  you could also get into a situation
[1390.54s -> 1392.54s]  where it's not just the images that differ,
[1393.18s -> 1396.66s]  it's actually the dynamics themselves that are different,
[1396.66s -> 1399.50s]  in which case simply forcing invariance like this
[1399.50s -> 1401.06s]  might not be a good idea.
[1401.06s -> 1403.90s]  So can we do some kind of domain adaptation
[1403.90s -> 1405.30s]  if the dynamics change?
[1407.74s -> 1409.18s]  So invariance is not good enough
[1409.18s -> 1410.46s]  if the dynamics don't match,
[1410.46s -> 1413.18s]  because you don't actually want to ignore
[1413.18s -> 1415.94s]  the functionally relevant differences.
[1415.94s -> 1417.30s]  But what you could do
[1417.30s -> 1421.22s]  is you could actually change your reward function
[1421.22s -> 1424.34s]  to punish the agent from doing things in the source domain
[1424.34s -> 1427.18s]  that would not be possible in the target domain.
[1427.18s -> 1428.98s]  So here's a little illustration of this.
[1428.98s -> 1432.10s]  Let's say that in the target domain in the real world,
[1432.10s -> 1433.62s]  you want to get from the start to the goal,
[1433.62s -> 1434.62s]  and there's a wall in between,
[1434.62s -> 1436.42s]  so you have to go around the wall.
[1436.42s -> 1439.46s]  But in your source domain in your simulator,
[1439.46s -> 1441.34s]  this wall is not present.
[1441.34s -> 1443.30s]  So if you train in the source domain,
[1443.30s -> 1444.98s]  you'll get this behavior that goes straight to the goal,
[1444.98s -> 1448.18s]  which of course doesn't work in the target domain.
[1448.18s -> 1449.54s]  So what we can do
[1450.14s -> 1453.14s]  if we have a little bit of experience from the target domain
[1453.14s -> 1456.06s]  is we can change our reward function
[1456.06s -> 1459.26s]  to provide a really big negative reward
[1459.26s -> 1461.38s]  for doing things in the source domain
[1461.38s -> 1463.82s]  that would not be possible in the target domain.
[1465.78s -> 1468.66s]  This is a very similar idea to domain adaptation,
[1468.66s -> 1472.18s]  except that instead of changing your representation
[1472.18s -> 1473.74s]  of the input to make it seem invariant,
[1473.74s -> 1475.22s]  you're actually changing your behavior
[1475.22s -> 1477.54s]  to make your behavior seem invariant.
[1477.54s -> 1479.06s]  Intuitively, what such a technique would do
[1479.58s -> 1481.50s]  is it would punish the agent for doing those things
[1481.50s -> 1483.30s]  that sort of violate the illusion,
[1483.30s -> 1485.54s]  that make it apparent to the agent
[1485.54s -> 1486.58s]  that it's in the source domain
[1486.58s -> 1488.62s]  and not in the target domain.
[1488.62s -> 1491.54s]  So this is a clip from a film
[1491.54s -> 1493.62s]  that somebody might recognize.
[1493.62s -> 1496.38s]  So this man thinks that he's sailing on a boat,
[1496.38s -> 1498.42s]  but then he gets to the edge of the green screen
[1498.42s -> 1500.62s]  and goes through the green screen
[1500.62s -> 1503.42s]  and realizes that it's not actually a clear sky,
[1503.42s -> 1505.94s]  it's actually the wall of a film studio.
[1505.94s -> 1508.14s]  So you don't want to violate the illusion
[1508.14s -> 1509.62s]  and what this additional reward term will do
[1509.62s -> 1512.38s]  is it'll prevent you from violating the illusion.
[1512.38s -> 1514.38s]  Or for example, if you want to train the ant
[1514.38s -> 1517.26s]  to run on an infinitely large flat plane
[1517.26s -> 1519.62s]  and it has a limited arena in which to practice,
[1519.62s -> 1521.22s]  then when it gets to the edge of the arena,
[1521.22s -> 1522.46s]  it will violate the illusion
[1522.46s -> 1524.06s]  and incur some negative reward.
[1525.54s -> 1527.90s]  It turns out that concepts very similar
[1527.90s -> 1531.74s]  to that invariance technique from before
[1531.74s -> 1534.34s]  can actually be used to compute this reward function,
[1534.34s -> 1535.94s]  essentially the reward function
[1537.06s -> 1541.38s]  that optimally leads to the desired behavior here
[1541.38s -> 1543.86s]  is going to be the difference between the log probability
[1543.86s -> 1546.14s]  of a transition happening in the target domain
[1546.14s -> 1548.54s]  minus its log probability in the source domain.
[1549.94s -> 1551.54s]  And this is very intuitive,
[1551.54s -> 1553.42s]  it's just saying, take those transitions
[1553.42s -> 1555.42s]  that are likely in the target domain,
[1556.62s -> 1558.54s]  that are no less likely in the target domain
[1558.54s -> 1560.06s]  than they are in the source domain.
[1562.06s -> 1564.06s]  There are a variety of ways to approximate this quantity
[1564.06s -> 1565.34s]  without training a dynamics model.
[1565.34s -> 1566.46s]  One of the ways to do it
[1566.46s -> 1571.46s]  is to train a discriminator for it.
[1571.82s -> 1574.18s]  Now, since you're estimating a conditional probability,
[1574.18s -> 1575.78s]  you actually need two discriminators.
[1575.78s -> 1580.06s]  So you have a discriminator for a joint S A S prime
[1580.06s -> 1582.86s]  and a discriminator for a joint S A
[1582.86s -> 1585.06s]  and you actually take the difference of the two.
[1585.06s -> 1588.06s]  I won't go into the details for why this works.
[1588.06s -> 1589.90s]  For the details of the method,
[1589.90s -> 1591.66s]  I would encourage you to read the paper,
[1591.66s -> 1593.18s]  but the high level idea I want you to take away
[1593.22s -> 1597.62s]  from this slide is just that you can use
[1597.62s -> 1600.66s]  invariance-based ideas to handle changes in dynamics.
[1600.66s -> 1603.74s]  What that leads to is agents that will try to behave
[1603.74s -> 1605.10s]  in the source domain in such a way
[1605.10s -> 1605.94s]  that they don't do anything
[1605.94s -> 1607.86s]  that will be impossible in the target domain.
[1607.86s -> 1609.38s]  And it can be instantiated by adding a term
[1609.38s -> 1612.98s]  to the reward function based on a discriminator,
[1612.98s -> 1614.22s]  but it's a little more complex
[1614.22s -> 1616.66s]  than just the standard image-based setting from before
[1616.66s -> 1617.98s]  and you need actually two discriminators
[1617.98s -> 1619.30s]  and you take the difference of them.
[1619.30s -> 1621.38s]  So to learn more about the technical approach,
[1621.38s -> 1623.14s]  I'd encourage you to read the paper.
[1623.18s -> 1628.02s]  Now, you could also consider, well,
[1628.02s -> 1630.78s]  when might this not work, right?
[1630.78s -> 1633.14s]  So a technique like this would prevent you
[1633.14s -> 1634.94s]  from doing things in the source domain
[1634.94s -> 1636.78s]  that would not be possible in the target domain,
[1636.78s -> 1638.14s]  but it might also be that the source domain
[1638.14s -> 1639.74s]  doesn't permit you to do certain things
[1639.74s -> 1641.70s]  that are needed in the target domain.
[1641.70s -> 1643.82s]  And this wouldn't do anything just to fix that.
[1643.82s -> 1646.62s]  So in a sense, you would be learning intuitively
[1646.62s -> 1648.18s]  like the intersection of the two domains.
[1648.18s -> 1649.30s]  And if the intersection is big
[1649.30s -> 1650.90s]  and you have good behavior there,
[1650.90s -> 1652.46s]  then you're in good shape,
[1652.50s -> 1653.78s]  but maybe it isn't.
[1658.54s -> 1662.90s]  Now, if you further fine-tune in the target domain,
[1662.90s -> 1665.18s]  just basically by writing more RL,
[1665.18s -> 1667.10s]  that can also make the transfer learning process
[1667.10s -> 1668.46s]  work a lot better.
[1668.46s -> 1670.62s]  But there are a few issues that make fine-tuning in RL
[1670.62s -> 1671.98s]  a little bit harder than fine-tuning
[1671.98s -> 1673.50s]  with supervised learning.
[1673.50s -> 1675.92s]  First, RL tasks are generally less diverse.
[1675.92s -> 1678.78s]  So pre-training and fine-tuning, for example,
[1678.78s -> 1681.00s]  in computer vision or natural language processing,
[1681.00s -> 1683.96s]  typically relies on scenarios where your pre-training
[1683.96s -> 1686.44s]  is done in extremely broad settings.
[1686.44s -> 1688.24s]  Maybe you pre-train in a setting
[1688.24s -> 1690.90s]  where you have millions of ImageNet images,
[1690.90s -> 1692.52s]  or you pre-train in a setting
[1692.52s -> 1696.24s]  where you have billions of lines of text for BERT,
[1697.12s -> 1700.00s]  and then you fine-tune on a much more narrow domain.
[1700.00s -> 1701.48s]  In RL, that's usually not how it works.
[1701.48s -> 1704.24s]  In RL, you might have much more narrow tasks
[1704.24s -> 1705.08s]  to pre-train on.
[1705.08s -> 1706.04s]  Although, of course, if you're lucky enough
[1706.04s -> 1707.84s]  to have a very broad task distribution,
[1707.84s -> 1710.00s]  then things will go better.
[1710.04s -> 1711.76s]  But if you pre-train on more narrow tasks,
[1711.76s -> 1714.16s]  then the features are generally less general,
[1714.16s -> 1715.84s]  and the policies and value functions
[1715.84s -> 1717.62s]  can become overly specialized.
[1719.52s -> 1722.56s]  The optimal policies and fully observed MDPs
[1722.56s -> 1723.56s]  tend to be deterministic.
[1723.56s -> 1724.40s]  The longer you train,
[1724.40s -> 1726.50s]  the more deterministic your policy will become.
[1726.50s -> 1728.24s]  And combined with problem number one,
[1728.24s -> 1729.12s]  this can be a big issue
[1729.12s -> 1731.92s]  because as your policy becomes more deterministic,
[1731.92s -> 1733.00s]  it explores less and less.
[1733.00s -> 1736.20s]  So you have loss of exploration and convergence.
[1736.20s -> 1738.24s]  And low entry policies will adapt very slowly
[1738.24s -> 1740.68s]  to new settings because they're not exploring enough.
[1740.68s -> 1743.56s]  This combined with having features that are less general
[1743.56s -> 1746.04s]  and overly specialized policies and value functions
[1746.04s -> 1747.80s]  can make fine-tuning extremely slow
[1747.80s -> 1749.44s]  in reinforcement learning.
[1749.44s -> 1750.60s]  So for that reason,
[1750.60s -> 1754.48s]  simply fine-tuning naively often is not very effective.
[1754.48s -> 1757.60s]  And we often have to do a variety of things
[1757.60s -> 1759.72s]  to ensure that our pre-training process
[1759.72s -> 1763.24s]  results in solutions that are more diverse
[1763.24s -> 1764.54s]  than what we ordinarily would have gotten
[1764.54s -> 1766.24s]  with regular RL pre-training.
[1767.24s -> 1770.32s]  Now, there isn't any one technique for doing this,
[1770.32s -> 1773.32s]  but we actually discussed techniques
[1773.32s -> 1775.16s]  that would provide some degree of this
[1775.16s -> 1776.52s]  in the Exploration II lecture
[1776.52s -> 1778.48s]  when we talked about unsupervised skill discovery,
[1778.48s -> 1780.36s]  and in the Control as Inference lecture
[1780.36s -> 1781.24s]  when we talked about
[1781.24s -> 1783.48s]  maximum entropy reinforcement learning methods.
[1783.48s -> 1784.90s]  And both of those classes of techniques
[1784.90s -> 1787.28s]  can be effective as pre-training methods
[1787.28s -> 1789.52s]  because they can get you more diverse solutions.
[1789.52s -> 1791.58s]  So in Exploration II,
[1791.58s -> 1793.24s]  we talked about how you could run,
[1793.24s -> 1795.28s]  for example, the diversity is all you need algorithm
[1795.32s -> 1797.16s]  or other information theoretic methods
[1797.16s -> 1801.08s]  that would discover a variety of different behaviors,
[1801.08s -> 1802.96s]  a variety of different skills.
[1802.96s -> 1804.74s]  You could either use those techniques directly,
[1804.74s -> 1806.18s]  or you could use them in combination
[1806.18s -> 1808.06s]  with a conventional reward function
[1808.06s -> 1810.82s]  to learn to solve a task in a variety of ways.
[1810.82s -> 1812.76s]  You could also use the maximum entropy RL methods
[1812.76s -> 1815.02s]  that we talked about in the Control as Inference lecture
[1815.02s -> 1817.10s]  to avoid this loss of exploration and convergence.
[1817.10s -> 1818.42s]  And both classes of techniques
[1818.42s -> 1820.00s]  can be more effective for transfer
[1820.00s -> 1822.68s]  than just naively pre-training and fine-tuning.
[1822.68s -> 1824.52s]  I won't go into this in more detail though,
[1824.56s -> 1827.60s]  because unfortunately to date,
[1827.60s -> 1829.24s]  there aren't really very general
[1829.24s -> 1830.88s]  and broadly applicable principles here.
[1830.88s -> 1832.60s]  There's a lot of research,
[1832.60s -> 1834.40s]  but so far the best I can tell you is,
[1834.40s -> 1835.88s]  well, maybe borrow some of the ideas
[1835.88s -> 1838.08s]  in Exploration II and Control as Inference.
[1841.12s -> 1844.72s]  Another thing that is often a very useful tool
[1844.72s -> 1847.00s]  in transfer learning
[1847.00s -> 1849.24s]  is to manipulate the source domain a little bit.
[1849.24s -> 1850.40s]  Now you can't always do this.
[1850.40s -> 1851.56s]  Like maybe the source domain
[1851.56s -> 1854.08s]  is just a particular simulator that you're training in
[1854.60s -> 1855.72s]  so that you don't get to change.
[1855.72s -> 1856.98s]  But if you do get to have some control
[1856.98s -> 1857.96s]  over the source domain,
[1857.96s -> 1858.92s]  there are a few things you can do
[1858.92s -> 1862.26s]  that will maximize the probability of successful transfer.
[1863.52s -> 1865.78s]  The basic intuition behind a lot of these things
[1865.78s -> 1868.16s]  is that the more varied the training domain is,
[1868.16s -> 1870.80s]  the more likely we are to generalize in zero shot
[1870.80s -> 1872.40s]  to a slightly different domain.
[1873.30s -> 1875.52s]  Basically, if you train a video game
[1875.52s -> 1876.48s]  where you're driving a car
[1876.48s -> 1878.48s]  and it's always sunny and bright out
[1878.48s -> 1879.88s]  and it's always in a city,
[1880.80s -> 1883.04s]  that is less likely to transfer to the real world
[1883.80s -> 1886.20s]  than if the game simulates different times of day
[1886.20s -> 1888.10s]  and maybe different urban environments.
[1888.10s -> 1889.92s]  The more variety you have in the source domain,
[1889.92s -> 1892.24s]  the more things will transfer.
[1892.24s -> 1894.36s]  The way that this often shows up in practice
[1894.36s -> 1895.88s]  is through randomization
[1895.88s -> 1899.48s]  where people will intentionally add more randomness
[1899.48s -> 1900.32s]  to the source domain,
[1900.32s -> 1901.72s]  perhaps a lot more than they would expect
[1901.72s -> 1902.64s]  in the real world
[1903.74s -> 1906.24s]  to increase the robustness of the policy
[1906.24s -> 1910.44s]  to variability in various physical parameters.
[1910.44s -> 1911.28s]  So for example,
[1911.28s -> 1913.64s]  if you'd like to train the Hopper to hop
[1913.64s -> 1915.76s]  and it has some physical parameters,
[1915.76s -> 1917.80s]  maybe it has friction and mass,
[1917.80s -> 1919.48s]  so these are two parameters
[1919.48s -> 1922.90s]  and the real world system is over here,
[1922.90s -> 1925.40s]  so it has these values of parameter one and two.
[1925.40s -> 1927.60s]  If you train on a narrow range of parameters,
[1927.60s -> 1930.60s]  you might not generalize to the kind of parameters
[1930.60s -> 1932.10s]  that you will see in the target domain.
[1932.10s -> 1934.12s]  But if you train on a broader range of parameters,
[1934.12s -> 1935.68s]  then it's kind of more likely
[1935.68s -> 1937.28s]  that the target domain will sort of be
[1937.28s -> 1940.36s]  within the set of parameters that you've trained on.
[1940.36s -> 1941.92s]  So that's the basic idea.
[1941.92s -> 1943.96s]  And this idea has shown up in the literature
[1943.96s -> 1945.00s]  in various ways,
[1945.00s -> 1947.64s]  both for randomizing physical behavior
[1947.64s -> 1950.20s]  and for randomizing visual appearance.
[1950.20s -> 1953.28s]  One of the earliest papers to apply this in Deep RL
[1953.28s -> 1956.00s]  was this paper by Rajeswaran et al called EP-OPT,
[1956.00s -> 1959.04s]  which studied randomization of physical parameters.
[1959.04s -> 1961.60s]  So the idea is that maybe you want to train
[1961.60s -> 1964.08s]  on one kind of Hopper and test on a different Hopper,
[1964.08s -> 1966.32s]  maybe a Hopper with different mass parameters.
[1967.32s -> 1972.32s]  But if you do that with a single setting, that might not work.
[1973.32s -> 1975.48s]  So you train on a variety of different parameter settings
[1975.48s -> 1977.92s]  and maybe then transfer will be more effective.
[1977.92s -> 1980.16s]  So in this paper, there is some analysis
[1980.16s -> 1983.00s]  of how this works or doesn't work.
[1983.00s -> 1984.38s]  This is some very basic analysis.
[1984.38s -> 1988.00s]  So here, the mass of the torso of the Hopper
[1988.00s -> 1989.24s]  is being varied.
[1989.24s -> 1993.80s]  On the x-axis, you can see the mass at test time,
[1993.80s -> 1996.16s]  the y-axis performance.
[1996.88s -> 1998.04s]  And the three different plots correspond
[1998.04s -> 1999.72s]  to three different training masses.
[1999.72s -> 2001.36s]  So the left one is trained with a mass of three,
[2001.36s -> 2002.72s]  the middle one a mass of six,
[2002.72s -> 2004.44s]  and the right one with a mass of nine.
[2004.44s -> 2006.00s]  And you can see the training with a mass of three,
[2006.00s -> 2007.68s]  of course, performs best with a mass of three
[2007.68s -> 2010.08s]  and then rapidly falls off as the mass gets bigger
[2010.08s -> 2011.16s]  and training with a mass of six
[2011.16s -> 2013.00s]  performs best with a mass of six.
[2014.48s -> 2017.94s]  If the policy is trained on a range of masses,
[2017.94s -> 2019.08s]  then this is the performance.
[2019.08s -> 2019.92s]  So you can see the performance
[2019.92s -> 2022.08s]  is very high across all masses.
[2022.08s -> 2023.30s]  And for your reference,
[2023.30s -> 2024.46s]  the distribution of training masses
[2024.46s -> 2025.82s]  looks roughly like this.
[2027.14s -> 2029.26s]  So it's kind of a normal distribution centered
[2029.26s -> 2030.60s]  at a mass of six.
[2031.70s -> 2034.66s]  It's not too surprising that training
[2034.66s -> 2036.50s]  on a variety of physical parameters
[2036.50s -> 2038.42s]  results in a policy that is more robust.
[2038.42s -> 2040.84s]  What was perhaps a little bit surprising about this paper
[2040.84s -> 2044.74s]  is that it worked very well
[2044.74s -> 2048.02s]  with comparatively little degradation overall performance.
[2048.02s -> 2049.70s]  So you would think that there would be a trade off
[2049.70s -> 2052.42s]  between robustness and optimality,
[2052.46s -> 2054.02s]  meaning that if you want to be more robust,
[2054.02s -> 2055.06s]  you'll get lower reward.
[2055.06s -> 2057.90s]  But in fact, that didn't seem to be the case in many cases.
[2057.90s -> 2059.30s]  Of course, it's not a universal conclusion,
[2059.30s -> 2061.78s]  but part of the reason why you might expect there
[2061.78s -> 2063.38s]  to be a little bit of a free lunch
[2063.38s -> 2064.58s]  is that well, deep neural networks
[2064.58s -> 2065.78s]  are actually very powerful.
[2065.78s -> 2068.82s]  So it's not actually unreasonable to imagine
[2068.82s -> 2070.06s]  that you can train one network
[2070.06s -> 2072.30s]  that is just as good across a variety of masses
[2072.30s -> 2074.14s]  as any single network for that mass.
[2076.06s -> 2077.42s]  Another kind of interesting observation,
[2077.42s -> 2078.54s]  and this is very important
[2078.54s -> 2080.30s]  when we talk about transfer learning,
[2080.30s -> 2083.58s]  is that randomizing enough parameters
[2083.58s -> 2085.38s]  actually creates robustness to other parameters
[2085.38s -> 2087.14s]  that were not randomized.
[2087.14s -> 2090.30s]  So the particular experiment that Rajeswaran et al. did here
[2090.30s -> 2093.78s]  was they had four different physical parameters
[2093.78s -> 2095.90s]  that were being varied in the simulator,
[2095.90s -> 2098.98s]  and they excluded one of them from the randomization.
[2098.98s -> 2100.26s]  So the mass was always the same,
[2100.26s -> 2103.08s]  but friction, joint damping, and armature were varied,
[2103.08s -> 2107.82s]  and found that with this kind of solution,
[2107.82s -> 2109.32s]  that's the blue line here,
[2109.32s -> 2111.56s]  it was actually decently robust to mass.
[2111.56s -> 2113.00s]  And intuitively, you can kind of see
[2113.00s -> 2113.84s]  why that would be the case,
[2113.84s -> 2114.96s]  because many of these physical parameters,
[2114.96s -> 2116.38s]  they have kind of redundant effects.
[2116.38s -> 2118.44s]  So if you decrease mass,
[2118.44s -> 2119.74s]  that's a lot like decreasing friction,
[2119.74s -> 2123.32s]  because your ground reaction forces won't be as large,
[2123.32s -> 2126.08s]  and therefore your friction force will not be as large.
[2126.08s -> 2129.28s]  So while changing friction is not exactly the same
[2129.28s -> 2131.24s]  as changing mass, if you randomize friction,
[2131.24s -> 2133.44s]  you will be a little bit more robust to mass.
[2133.44s -> 2134.40s]  And that's actually very important,
[2134.40s -> 2136.94s]  because in reality, when we're doing transfer learning,
[2136.94s -> 2138.84s]  we almost always have unmodeled effects.
[2139.32s -> 2142.44s]  We almost always can't actually vary all the parameters
[2142.44s -> 2145.12s]  that distinguish the source domain from the target domain.
[2145.12s -> 2146.00s]  So it's very good to know
[2146.00s -> 2147.64s]  that if you vary enough parameters,
[2147.64s -> 2149.40s]  perhaps even those parameters that you didn't vary,
[2149.40s -> 2151.40s]  you might still be a little bit robust too.
[2152.52s -> 2153.76s]  And then of course, the other thing you could do
[2153.76s -> 2155.16s]  is if you get a little bit of experience
[2155.16s -> 2157.28s]  in the target domain, you can adapt a little bit.
[2157.28s -> 2160.66s]  You can change your distribution of parameters
[2160.66s -> 2163.12s]  to be closer to the target domain,
[2163.12s -> 2164.92s]  and then things would work better.
[2164.92s -> 2169.92s]  Now, the idea of randomization has been used very widely
[2170.48s -> 2173.24s]  in transfer learning for RL.
[2173.24s -> 2176.00s]  This was the first paper that applied it for deep RL
[2176.00s -> 2179.00s]  with visual observations for flying a drone.
[2179.00s -> 2181.52s]  It's been used with recurrent neural networks
[2181.52s -> 2183.38s]  that can be robust to physical variation,
[2183.38s -> 2185.76s]  and then also actually adaptive.
[2185.76s -> 2188.62s]  More recently, this technique has been extremely popular
[2188.62s -> 2190.12s]  for learning locomotion policies.
[2190.12s -> 2192.38s]  This is a paper from ETH Zurich
[2192.42s -> 2196.14s]  that shows a fairly extreme degree of robustness
[2196.14s -> 2198.14s]  for a policy trained in a physical simulator
[2198.14s -> 2199.90s]  with a high degree of randomization.
[2199.90s -> 2203.94s]  So this has been a very influential idea across the board
[2203.94s -> 2206.20s]  for transfer learning, especially in robotics.
[2206.20s -> 2207.72s]  Now, the idea is not exclusive to robotics.
[2207.72s -> 2209.54s]  You could imagine randomizing simulators
[2209.54s -> 2210.60s]  for all sorts of other domains,
[2210.60s -> 2212.12s]  but robotics especially seems to be one
[2212.12s -> 2213.80s]  where this is really taken off.
[2215.82s -> 2217.26s]  So if you want to read more
[2217.26s -> 2218.62s]  about the techniques that I covered,
[2218.62s -> 2220.52s]  here are some references for domain adaptation,
[2220.52s -> 2223.20s]  fine tuning, and randomization.
[2226.48s -> 2228.84s]  In the last part of this section,
[2228.84s -> 2231.96s]  I'm gonna also talk a little bit about multitask transfer.
[2231.96s -> 2234.52s]  This discussion will be quite a bit shorter
[2234.52s -> 2237.48s]  because multitask transfer is a very powerful tool,
[2237.48s -> 2238.92s]  but we're gonna talk about it a lot more
[2238.92s -> 2240.70s]  when we discuss metal learning later.
[2240.70s -> 2243.32s]  So this will just be like kind of a quick primer
[2243.32s -> 2245.16s]  to the multitask idea.
[2246.92s -> 2249.24s]  The basic idea is that you can learn faster
[2249.24s -> 2252.28s]  and perhaps transfer better by learning multiple tasks.
[2252.28s -> 2253.60s]  So if you have a variety of tasks,
[2253.60s -> 2255.00s]  maybe you have this robot that needs to do
[2255.00s -> 2256.80s]  a whole range of household chores,
[2256.80s -> 2259.00s]  you could learn each of these tasks individually,
[2259.00s -> 2261.00s]  but it's very likely that the tasks
[2261.00s -> 2262.92s]  share some common structure.
[2262.92s -> 2264.92s]  Not all of their structure will be shared,
[2264.92s -> 2266.26s]  but perhaps they share some structure
[2266.26s -> 2269.68s]  in the sense that the way the robot moves its arms
[2269.68s -> 2270.52s]  is kind of similar,
[2270.52s -> 2272.92s]  the way it touches objects might be similar.
[2272.92s -> 2275.16s]  So perhaps if you don't train them individually,
[2275.16s -> 2276.96s]  but you instead train them all together,
[2276.96s -> 2278.86s]  the fact that you can share those representations
[2279.42s -> 2280.54s]  will make all the tasks learn quicker.
[2280.54s -> 2282.98s]  So if one of the tasks figures out how to pick up an object,
[2282.98s -> 2285.48s]  that kind of capability can sort of immediately be used
[2285.48s -> 2286.50s]  by the other tasks,
[2286.50s -> 2290.32s]  and maybe they'll explore better or they'll learn faster.
[2290.32s -> 2292.94s]  And furthermore, if you learn multiple different tasks
[2292.94s -> 2296.30s]  and then you are provided with a new task at test time,
[2296.30s -> 2298.98s]  it seems more likely that you will have something
[2298.98s -> 2300.66s]  to transfer to that target task
[2301.80s -> 2303.38s]  if you have more source tasks,
[2303.38s -> 2305.62s]  kind of a similar intuition as the randomization.
[2305.62s -> 2308.10s]  If you have a greater variety of training situations,
[2308.10s -> 2309.60s]  then it's more likely the test situation
[2309.60s -> 2311.86s]  will look somewhat familiar to you.
[2311.86s -> 2313.50s]  So multitask learning can accelerate learning
[2313.50s -> 2315.14s]  of all the tasks that are learned together,
[2315.14s -> 2316.56s]  and it can provide better pre-training
[2316.56s -> 2317.92s]  for downstream tasks.
[2318.90s -> 2322.38s]  Now, there's not a heck of a lot more to say about this.
[2322.38s -> 2326.18s]  The basic version of this really hinges on the ability
[2326.18s -> 2327.34s]  to have a sufficiently effective
[2327.34s -> 2328.60s]  reinforcement learning algorithm
[2328.60s -> 2330.82s]  that you can do this multitask training.
[2330.82s -> 2333.42s]  And that kind of amounts of scaling things up appropriately,
[2333.42s -> 2335.26s]  choosing the right type of parameters and so on.
[2335.26s -> 2337.68s]  There are various techniques that people have proposed
[2338.24s -> 2339.16s]  that are specifically designed
[2339.16s -> 2341.32s]  to make multitask training better,
[2341.32s -> 2343.40s]  but there isn't really kind of one killer technique
[2343.40s -> 2344.78s]  that works across the board.
[2344.78s -> 2346.46s]  So what I would say there is,
[2346.46s -> 2347.56s]  if you're really interested in this,
[2347.56s -> 2348.98s]  you can do a literature search
[2348.98s -> 2350.76s]  and read up on multitask learning in RL,
[2350.76s -> 2352.06s]  but the basic starting point
[2352.06s -> 2353.64s]  is just to take the same kinds of algorithms
[2353.64s -> 2355.12s]  that you would use in single task RL
[2355.12s -> 2356.52s]  and try to scale them up.
[2359.82s -> 2361.88s]  I do want to say a few words about
[2361.88s -> 2363.34s]  kind of the way that this kind of stuff
[2363.34s -> 2365.24s]  can be framed as an MDP.
[2365.24s -> 2368.56s]  So one, you know, somewhat straightforward,
[2368.56s -> 2371.08s]  but very important idea is that multitask RL
[2371.08s -> 2374.84s]  really does correspond to single task RL in a joint MDP.
[2374.84s -> 2376.84s]  So if you're wondering how to represent
[2376.84s -> 2379.62s]  multitask reinforcement learning as an RL problem,
[2379.62s -> 2382.44s]  keep in mind that it's really the same RL problem.
[2382.44s -> 2385.32s]  So in a standard RL setting,
[2385.32s -> 2388.12s]  you first sample the initial state from P of S zero,
[2388.12s -> 2390.44s]  and then you roll out your policy.
[2390.44s -> 2393.96s]  If you want to embed a multitask problem into the setting,
[2393.96s -> 2395.00s]  all you have to do
[2395.00s -> 2397.60s]  is just change the initial state distribution.
[2397.60s -> 2399.04s]  So you could think about it like this.
[2399.04s -> 2401.12s]  If you're learning to play multiple different Atari games
[2401.12s -> 2402.68s]  with the same policy,
[2402.68s -> 2403.64s]  in a regular Atari game,
[2403.64s -> 2404.60s]  the initial state distribution
[2404.60s -> 2406.20s]  is just the start of that game.
[2406.20s -> 2408.58s]  In this multitask Atari MDP,
[2408.58s -> 2410.00s]  the initial state distribution
[2410.00s -> 2411.28s]  is the distribution over games.
[2411.28s -> 2412.28s]  So on the first time step,
[2412.28s -> 2413.32s]  you sample a game
[2413.32s -> 2414.96s]  and then you play that game thereafter.
[2414.96s -> 2417.32s]  That is still a single MDP.
[2417.32s -> 2419.00s]  So in principle,
[2419.00s -> 2420.88s]  the algorithm doesn't actually need to change at all
[2420.88s -> 2422.56s]  in order to do this.
[2422.56s -> 2424.96s]  You just pick the MDP randomly in the first state,
[2424.96s -> 2426.96s]  and that's part of the MDP definition.
[2429.68s -> 2431.80s]  Now, there can be a little bit of a nuance
[2431.80s -> 2434.44s]  because maybe the policy can do multiple things
[2434.44s -> 2435.68s]  in the same environment.
[2435.68s -> 2437.08s]  If this was an Atari game,
[2437.08s -> 2437.92s]  this wouldn't be an issue
[2437.92s -> 2439.40s]  because you could tell which Atari game you're playing
[2439.40s -> 2440.48s]  by looking at the screen.
[2440.48s -> 2442.24s]  Then maybe you have a robot in your home
[2442.24s -> 2444.40s]  and the robot can, in the same initial state,
[2444.40s -> 2446.96s]  go and do the laundry or go and do the distress.
[2446.96s -> 2447.96s]  So in that case,
[2447.96s -> 2450.20s]  if you want to instantiate multitask learning
[2450.20s -> 2452.06s]  as a standard RL problem,
[2452.42s -> 2453.26s]  you can do a little bit more
[2453.26s -> 2454.42s]  to indicate to the agent
[2454.42s -> 2457.14s]  which task it's supposed to be doing.
[2457.14s -> 2458.18s]  So the way that we do this
[2458.18s -> 2460.98s]  is by assigning some sort of context to each task.
[2460.98s -> 2462.66s]  The context can be a variety of different things.
[2462.66s -> 2465.06s]  It could be as simple as just a one-hot vector
[2465.06s -> 2466.66s]  indicating whether we're doing the dishes
[2466.66s -> 2467.70s]  or doing the laundry,
[2467.70s -> 2469.54s]  or it could be some kind of descriptor of a task,
[2469.54s -> 2472.54s]  maybe a gold image or even a textual description.
[2472.54s -> 2474.54s]  And all of those are reasonable choices.
[2475.94s -> 2477.70s]  So we call these contextual policies.
[2477.70s -> 2480.10s]  A standard policy is just pi of A given S.
[2480.14s -> 2484.38s]  A contextual policy is pi of A given S, context, omega.
[2484.38s -> 2485.94s]  And if you learn it with an active critic method
[2485.94s -> 2487.06s]  or a Q-learning method,
[2487.06s -> 2489.50s]  then your Q function would also be
[2489.50s -> 2491.62s]  taking omega in as input.
[2491.62s -> 2493.08s]  So this could be a one-hot vector
[2493.08s -> 2493.92s]  indicating whether you're doing
[2493.92s -> 2495.10s]  the dishes or the laundry.
[2495.10s -> 2496.62s]  It could be a textual description
[2496.62s -> 2498.18s]  or it could be something else.
[2499.16s -> 2500.58s]  This is very, very simple.
[2500.58s -> 2503.34s]  All you have to do is simply augment the state space
[2503.34s -> 2505.86s]  to add the context to the state.
[2505.86s -> 2509.58s]  And now the problem of training this multitask policy
[2510.02s -> 2511.42s]  to do all these tasks basically amounts
[2511.42s -> 2513.66s]  to the same multitask MDP from before
[2513.66s -> 2516.82s]  where omega is chosen randomly at the initial time step.
[2516.82s -> 2519.84s]  And then it doesn't change for the rest of the episode.
[2519.84s -> 2522.86s]  So people have trained contextual policies
[2522.86s -> 2524.06s]  for all sorts of settings.
[2524.06s -> 2526.02s]  Maybe you have a robot stacking Lego blocks
[2526.02s -> 2528.20s]  and omega is the location where it stacks something.
[2528.20s -> 2529.68s]  You have a virtual character walking
[2529.68s -> 2531.58s]  and omega is the desired walking direction.
[2531.58s -> 2532.90s]  Maybe you have a hockey robot
[2532.90s -> 2535.50s]  and omega is where it should hit the puck.
[2535.50s -> 2538.60s]  So this is all pretty straightforward to do.
[2538.60s -> 2540.32s]  You don't actually have to change your algorithm
[2540.32s -> 2541.36s]  for the most basic version.
[2541.36s -> 2543.20s]  You just change your MDP definition
[2543.20s -> 2545.40s]  to add this additional variable to the state.
[2546.48s -> 2549.02s]  A particularly common choice of contextual policies
[2549.02s -> 2551.36s]  is what's called a goal condition policy.
[2551.36s -> 2552.48s]  So in a goal condition policy,
[2552.48s -> 2554.52s]  the context is just another state
[2554.52s -> 2556.76s]  and your reward rewards you for reaching that state,
[2556.76s -> 2559.12s]  either just one if you reach the state correctly
[2559.12s -> 2561.08s]  or a little ball around the state to say,
[2561.08s -> 2562.80s]  well, if you're close to the desired state,
[2562.80s -> 2564.86s]  then that's considered a positive reward.
[2566.32s -> 2568.52s]  Goal condition policies can be especially convenient
[2569.40s -> 2570.22s]  because you don't need to manually define
[2570.22s -> 2571.06s]  the reward for each task.
[2571.06s -> 2572.64s]  You can just sample a bunch of random states
[2572.64s -> 2575.60s]  to be your goals and you can transfer a zero shot
[2575.60s -> 2577.66s]  to a new task if it's another goal.
[2577.66s -> 2579.40s]  So if you're lucky enough that your new task
[2579.40s -> 2581.56s]  is defined by a goal,
[2581.56s -> 2584.20s]  then zero shot transfer is entirely possible.
[2585.80s -> 2587.12s]  This has some disadvantages though,
[2587.12s -> 2589.50s]  because training such goal condition policies
[2589.50s -> 2591.64s]  is often actually difficult in practice.
[2591.64s -> 2593.32s]  It just represents a fairly difficult
[2593.32s -> 2594.96s]  reinforcement learning problem
[2594.96s -> 2597.28s]  and not all tasks are goal reaching tasks.
[2597.28s -> 2600.76s]  So this example from the Exploration II lecture
[2600.76s -> 2602.20s]  shows an instance of a task
[2602.20s -> 2603.56s]  that cannot be represented as a goal,
[2603.56s -> 2605.14s]  where you have to reach the green location
[2605.14s -> 2607.02s]  while avoiding the red circle.
[2607.02s -> 2609.28s]  So there's no single goal that explains this.
[2611.04s -> 2613.32s]  If you want to learn more about goal condition policies,
[2613.32s -> 2615.18s]  I encourage you to read a few of the related papers
[2615.18s -> 2616.24s]  in this area,
[2616.24s -> 2619.96s]  because while setting up goal condition RL
[2619.96s -> 2622.16s]  in the most straightforward way is actually very simple,
[2622.16s -> 2623.96s]  you just define a particular MDP.
[2625.12s -> 2626.84s]  If you want to make these methods work really well,
[2627.36s -> 2628.88s]  there's a variety of tricks that could be really useful.
[2628.88s -> 2631.26s]  And these tricks include clever ways of selecting
[2631.26s -> 2633.14s]  which goals to train for,
[2633.14s -> 2636.90s]  clever ways of representing value functions or Q functions,
[2636.90s -> 2640.60s]  and clever ways of formulating rewards
[2640.60s -> 2643.04s]  and reinforcement learning loss functions
[2643.04s -> 2644.56s]  that are especially effective
[2644.56s -> 2646.04s]  when you're trying to reach goals.
[2646.04s -> 2648.32s]  So the basic version of this is straightforward.
[2648.32s -> 2650.28s]  However, making it work really well is more complex
[2650.28s -> 2651.64s]  outside of the scope of this lecture.
[2651.64s -> 2652.84s]  But if you're interested in this,
[2652.84s -> 2655.34s]  I would encourage you to read these papers.
[2656.84s -> 2658.32s]  Thank you.
