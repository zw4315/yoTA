# Detected language: en (p=1.00)

[0.00s -> 4.84s]  All right, in the next two portions of the lecture,
[4.84s -> 6.78s]  we're going to talk about
[6.78s -> 10.72s]  classic offline reinforcement learning methods
[10.72s -> 15.68s]  that kind of predate the deep RL techniques.
[15.68s -> 17.52s]  And these are, you know,
[17.52s -> 18.80s]  generally these are not the methods
[18.80s -> 21.12s]  that you would start with
[21.12s -> 24.06s]  if you wanted to use offline RL today.
[24.06s -> 25.20s]  You would start with methods
[25.20s -> 27.16s]  that we would cover a bit later.
[27.16s -> 29.80s]  But I think it helps to discuss these
[29.80s -> 31.44s]  to give everybody kind of the perspective
[31.44s -> 33.48s]  of where a lot of these ideas come from
[33.48s -> 35.92s]  and how people have thought about offline RL
[35.92s -> 37.40s]  and batch RL in the past.
[38.84s -> 41.56s]  And by the way, in terms of terminology,
[41.56s -> 43.16s]  the term batch reinforcement learning
[43.16s -> 46.36s]  really kind of was popularized
[46.36s -> 47.88s]  in around the early 2000s.
[47.88s -> 50.78s]  And somewhere in the last few years,
[50.78s -> 52.78s]  the term offline RL became a little bit more prevalent
[52.78s -> 54.96s]  because it's a bit more self-explanatory.
[54.96s -> 56.50s]  It better captures what's really going on.
[56.50s -> 58.12s]  And the term batch is kind of overloaded
[58.36s -> 59.88s]  in current machine learning parlance,
[59.88s -> 62.52s]  but they mean exactly the same thing.
[62.52s -> 64.88s]  So if you see a paper that says batch RL,
[64.88s -> 67.52s]  it means exactly the same thing as offline RL.
[67.52s -> 69.72s]  So the topic that I'll discuss
[69.72s -> 71.92s]  in the next portion of the lecture
[71.92s -> 76.92s]  is batch RL or offline RL by importance sampling.
[76.94s -> 78.60s]  Most of the methods that we'll talk about
[78.60s -> 81.52s]  in this course for offline RL are value-based methods,
[81.52s -> 83.24s]  dynamic programming-based methods,
[83.24s -> 84.16s]  but we will discuss
[84.16s -> 85.60s]  importance sampling-based methods a bit
[85.64s -> 88.28s]  because this does occupy a significant portion
[88.28s -> 91.60s]  of the classic literature on these kinds of techniques.
[93.20s -> 94.88s]  Now, a lot of this,
[94.88s -> 96.32s]  you guys will already be familiar with
[96.32s -> 99.08s]  from our discussion of importance sample policy gradients.
[99.08s -> 101.08s]  And that forms the basic idea
[101.08s -> 105.24s]  for importance sampling techniques for offline and batch RL.
[105.24s -> 107.84s]  So we have our RL objective,
[107.84s -> 109.06s]  we have our policy gradient,
[109.06s -> 110.70s]  just like what we covered before,
[110.70s -> 114.10s]  grad log Q, sorry, grad log pi times Q.
[115.06s -> 117.58s]  And the problem that you're all hopefully familiar with
[117.58s -> 119.58s]  by this point is that estimating the policy gradient
[119.58s -> 121.76s]  requires samples from pi theta.
[121.76s -> 123.78s]  So if you only have samples from pi beta,
[123.78s -> 126.30s]  what you would do is importance sampling.
[126.30s -> 129.34s]  And we learned all about importance sampling so far.
[130.54s -> 135.54s]  We multiply our policy gradient by an importance weight,
[136.18s -> 138.26s]  which is the ratio of the probabilities of a trajectory
[138.26s -> 139.72s]  under pi theta and pi beta.
[139.72s -> 140.58s]  And as we saw before,
[140.58s -> 142.34s]  when we write out those probabilities,
[142.34s -> 143.94s]  all the terms that don't depend on the policies
[144.78s -> 145.62s]  can't sound, right?
[145.62s -> 148.86s]  So this is just a recap of the policy gradient lecture.
[148.86s -> 151.90s]  So the ratio of the trajectory probabilities
[151.90s -> 154.14s]  under the two policies consists of a product
[154.14s -> 156.54s]  of initial states, transitions, and policies.
[156.54s -> 159.02s]  But because the initial states and transition probabilities
[159.02s -> 162.64s]  are exactly the same for both pi theta and pi beta,
[162.64s -> 164.98s]  it's only the ratio of the action probabilities
[164.98s -> 165.82s]  that shows up.
[167.38s -> 168.78s]  And as we discussed before,
[168.78s -> 171.90s]  this is a perfectly reasonable, unbiased way
[172.22s -> 174.58s]  to construct an estimator for the policy gradient
[174.58s -> 176.48s]  using only samples from pi beta.
[176.48s -> 177.88s]  But it has a big problem
[177.88s -> 179.70s]  because you are multiplying together
[179.70s -> 181.14s]  these action probabilities.
[181.14s -> 182.98s]  And the number of probabilities
[182.98s -> 186.74s]  that you're multiplying in general is O of capital T,
[186.74s -> 190.60s]  which means that the weights are exponential in capital T,
[190.60s -> 191.78s]  which means that the weights are likely
[191.78s -> 194.06s]  to become degenerate as capital T becomes large.
[194.06s -> 195.84s]  By degenerate, I mean that one weight
[195.84s -> 197.30s]  will become very big
[197.30s -> 200.14s]  and all the other weights will become vanishingly small,
[200.14s -> 202.66s]  which means effectively as T increases,
[202.66s -> 204.26s]  you're estimating your policy gradient
[204.26s -> 206.32s]  using one fairly arbitrary sample.
[207.22s -> 209.42s]  Mathematically, what this means is that
[209.42s -> 210.78s]  although the policy gradient
[210.78s -> 213.34s]  with these importance weights is unbiased,
[213.34s -> 217.26s]  meaning that if you were to generate
[217.26s -> 219.46s]  many, many different samples,
[219.46s -> 221.38s]  if you were to run the estimator many, many times
[221.38s -> 222.78s]  with independent samples,
[222.78s -> 226.24s]  on average, it would in fact give you the right answer.
[226.24s -> 228.22s]  But the variance of the estimator is very large.
[228.22s -> 230.18s]  In fact, it is exponentially large,
[231.26s -> 233.14s]  which of course means that you need exponentially
[233.14s -> 235.18s]  many samples to get an accurate estimate.
[236.54s -> 237.56s]  Can we fix this?
[239.92s -> 242.96s]  Well, before I actually break down this equation,
[242.96s -> 245.36s]  one comment I would make here is that
[245.36s -> 248.34s]  we did see in our discussion of advanced policy gradients
[248.34s -> 250.74s]  that a common way to use important sampling
[250.74s -> 253.34s]  in practical modern policy gradient methods
[253.34s -> 257.54s]  is to simply drop the P of A given S terms
[257.82s -> 261.06s]  in those weights for all the time steps prior to T.
[261.06s -> 262.92s]  And in the advanced policy gradients lecture,
[262.92s -> 265.94s]  we learned how this is reasonable to do
[265.94s -> 269.10s]  if pi theta and the policy that generated the data,
[269.10s -> 271.60s]  in which in our case is pi beta, are similar.
[273.10s -> 275.02s]  That doesn't really apply to offline RL
[275.02s -> 277.08s]  because in offline RL,
[277.08s -> 279.54s]  the whole point is to get a much better policy.
[281.68s -> 286.58s]  So the short answer to can we fix this is no, we can't,
[286.58s -> 288.86s]  but we can sort of ruminate on this point
[288.86s -> 291.18s]  a little bit more and to ruminate on it a little bit more,
[291.18s -> 295.02s]  we can separate those weights into two parts.
[295.02s -> 298.02s]  So you can think of it as a product
[298.02s -> 301.60s]  of all the action probabilities for the whole trajectory,
[301.60s -> 303.50s]  the product from zero to little t
[303.50s -> 306.34s]  and then the product from little t to capital T,
[306.34s -> 307.82s]  and you can break that into two halves
[307.82s -> 309.54s]  and you can put those halves,
[309.54s -> 312.02s]  one of them in front of grad log pi and one of them after.
[312.02s -> 313.06s]  Now, I didn't actually change anything.
[313.06s -> 316.46s]  This is just the, because multiplication commutes.
[317.30s -> 319.78s]  So this is just exploiting the commutative property
[319.78s -> 321.42s]  to just write the same exact importance weight
[321.42s -> 322.58s]  in a different way.
[322.58s -> 323.42s]  But writing it in this way
[323.42s -> 324.82s]  makes it a little bit more apparent
[324.82s -> 327.90s]  that there are really two parts to the importance weight.
[327.90s -> 329.42s]  The part that multiplies all the actions
[329.42s -> 330.38s]  prior to little t,
[330.38s -> 331.90s]  which you can think of as essentially accounting
[331.90s -> 334.28s]  for the fact that pi theta has a different probability
[334.28s -> 337.50s]  of reaching the state st than pi beta does.
[337.50s -> 339.42s]  And then all this stuff afterwards,
[339.42s -> 342.92s]  which basically accounts for the fact that your value,
[342.92s -> 344.34s]  the value Q had that you estimate
[344.34s -> 346.82s]  by summing up the rewards in pi beta
[346.82s -> 347.94s]  might be different from the value
[347.94s -> 349.74s]  that you would get from pi theta.
[349.74s -> 351.14s]  So the second part of the weight
[351.14s -> 354.50s]  accounts for the difference in reward to go.
[354.50s -> 355.90s]  So the first part accounts for the difference
[355.90s -> 357.66s]  in probability of landing in st,
[358.50s -> 360.62s]  because we have states sampled from d pi beta
[360.62s -> 362.74s]  and we want states from d pi theta.
[362.74s -> 365.30s]  And the second part accounts for having the incorrect Q hat
[365.30s -> 366.62s]  because Q hat here
[366.62s -> 368.74s]  in the classic Monte Carlo policy gradient
[368.74s -> 370.78s]  is formed by just summing up the rewards
[370.78s -> 372.14s]  that you saw from pi beta.
[372.14s -> 374.78s]  And instead, what you want is the rewards from pi theta.
[376.38s -> 377.94s]  So you could disregard the first term.
[377.94s -> 382.94s]  That's what classic on policy techniques
[386.00s -> 388.42s]  with multiple gradient steps do.
[388.42s -> 390.08s]  So that's what, for example, PPO does.
[390.08s -> 393.64s]  These are methods that do collect active samples,
[393.64s -> 395.26s]  but then they take many gradient steps
[395.26s -> 397.02s]  with an important sampled estimator
[397.02s -> 398.86s]  and then collect some more samples.
[398.86s -> 401.10s]  And the justification for dropping that is basically
[401.10s -> 403.02s]  if the policy that collected the data
[403.02s -> 404.94s]  is close enough to your latest policy,
[404.94s -> 406.06s]  then it's okay to disregard this
[406.06s -> 407.94s]  because you have a bound as we discussed
[407.94s -> 409.98s]  in the advanced policy gradient selection.
[411.70s -> 414.14s]  So that's why that could be a reasonable approximation,
[414.14s -> 417.14s]  but only if you are willing to not have pi theta
[417.14s -> 418.74s]  deviate too much from pi beta.
[422.22s -> 424.38s]  So we could talk about just the other term.
[425.38s -> 430.38s]  So the other term, naively, you would estimate Q hat
[431.90s -> 434.26s]  by just summing up the rewards from pi beta.
[434.26s -> 435.88s]  But what you want is you wanna sum up the rewards
[435.88s -> 436.72s]  from pi theta.
[439.22s -> 443.02s]  So you could think about breaking up
[443.02s -> 446.54s]  these importance weights even further.
[446.54s -> 451.30s]  You could say that the sum Q hat is really a sum
[451.30s -> 453.62s]  from T prime equals T to capital T
[453.62s -> 456.66s]  of the reward that you actually saw at T prime,
[456.66s -> 458.34s]  multiplied by the whole importance weight.
[458.34s -> 459.42s]  So I didn't change anything here.
[459.42s -> 461.38s]  This is just the distributive property.
[462.72s -> 464.86s]  But you know that actions in the future
[464.86s -> 466.64s]  don't affect rewards in the past.
[466.64s -> 468.54s]  So one of the things you could do is
[468.54s -> 470.66s]  for any time step T prime,
[470.66s -> 475.18s]  you could sum up only the actual probabilities
[475.18s -> 478.14s]  from T to T prime, not from T to capital T.
[478.14s -> 480.84s]  Right, so essentially I have the reward
[480.84s -> 482.70s]  at the current time step.
[482.70s -> 486.10s]  That thing is not affected by the action
[486.10s -> 487.28s]  two time steps from now.
[487.28s -> 489.78s]  So I can exclude that from the importance weights.
[491.08s -> 494.02s]  The importance weights are still going to be multiplying
[494.02s -> 496.10s]  O of capital T terms.
[496.10s -> 499.46s]  So in terms of big O, this didn't actually get any better
[499.46s -> 502.20s]  but it does mean that we have lower variance
[502.20s -> 504.18s]  for time steps closer to the current one.
[504.18s -> 506.98s]  So it's still exponential, but it's a little bit better.
[509.50s -> 511.42s]  In fact, it actually turns out that to avoid
[511.46s -> 513.62s]  exponentially exploding importance weights,
[513.62s -> 516.54s]  we must use value function estimation.
[516.54s -> 518.90s]  There's actually no way to avoid
[518.90s -> 520.38s]  the exponential problem altogether
[520.38s -> 522.38s]  without using value function estimation.
[523.46s -> 526.86s]  But that hasn't stopped various techniques in the literature
[526.86s -> 530.14s]  from trying to still make this not as bad.
[530.14s -> 532.70s]  None of them avoid the exponential problem altogether
[532.70s -> 537.44s]  but there are many ways to still reduce the variance
[537.44s -> 538.50s]  to make it more manageable.
[538.50s -> 540.00s]  So one of them is the one on the slide
[540.00s -> 544.32s]  which is to only multiply together
[544.32s -> 546.88s]  the action probabilities from T to T prime
[546.88s -> 548.20s]  rather than from T to capital T
[548.20s -> 550.46s]  for the reward of time step T prime.
[554.76s -> 556.84s]  But there are better ways to do it.
[556.84s -> 559.56s]  So later on, we'll talk about how this would work
[559.56s -> 563.60s]  if you knew Q pi theta or if you had to learn Q pi theta
[563.60s -> 564.92s]  but first let's conclude our discussion
[564.92s -> 567.92s]  of importance sampling with a few other ideas
[567.92s -> 569.96s]  that have been explored in the literature.
[571.00s -> 573.44s]  So one idea that is worth discussing
[573.44s -> 576.48s]  because it has served to inspire
[576.48s -> 578.76s]  quite a few more recent techniques
[578.76s -> 581.68s]  is the idea of the doubly robust estimator.
[581.68s -> 583.94s]  You can think of the doubly robust estimator
[583.94s -> 585.96s]  as a little bit like a baseline
[585.96s -> 587.36s]  but for importance sampling.
[590.04s -> 594.88s]  So this is just the importance sampled value estimator
[594.88s -> 596.36s]  from the previous slide
[596.36s -> 599.36s]  and it's still exponential in the time horizon.
[599.60s -> 601.28s]  So notice that it's multiplying together
[601.28s -> 604.92s]  the action probabilities from little t to little t prime
[604.92s -> 609.36s]  and little t prime goes all the way up to capital T.
[609.36s -> 610.84s]  So at the very last time step,
[610.84s -> 612.44s]  you're still multiplying together
[612.44s -> 614.66s]  O of capital T probabilities.
[618.12s -> 622.52s]  For simplicity, I'll just drop the indices
[622.52s -> 625.36s]  and I will turn my T primes into T
[625.36s -> 626.20s]  just to keep it simple.
[626.20s -> 629.36s]  So before I noticed that I was writing V of ST.
[629.36s -> 631.96s]  Now I'll just write it as V of S zero.
[631.96s -> 635.16s]  I'll just write it out for the initials time step.
[635.16s -> 637.82s]  Mostly this is just to declutter my notation
[637.82s -> 640.92s]  but you can basically substitute and replace the zero
[640.92s -> 644.98s]  with T and you would get all the stuff from before.
[644.98s -> 647.08s]  And what I'm going to try to do
[647.08s -> 648.34s]  is I'm gonna try to reduce the variance
[648.34s -> 649.92s]  of these importance weights further.
[649.92s -> 652.12s]  So I'm gonna introduce a little bit of notation.
[652.12s -> 654.20s]  I'm gonna introduce a row T prime
[654.20s -> 657.12s]  and row T prime will denote P theta AT prime
[657.12s -> 661.36s]  given ST prime divided by pi beta AT prime given ST prime.
[661.36s -> 664.54s]  So I'll just condense that ratio into row T prime.
[665.72s -> 666.84s]  So now we can see that
[666.84s -> 668.48s]  this important sample value estimator
[668.48s -> 670.68s]  is sum over all the time steps
[670.68s -> 673.72s]  of a product of all the rows up until that time step
[673.72s -> 675.72s]  times gamma TRT.
[677.08s -> 680.80s]  And it's that product of rows that we're concerned about.
[680.80s -> 682.98s]  So if we're to actually write this out,
[683.82s -> 686.10s]  just like actually expand the sum,
[686.10s -> 688.98s]  you would get the first term which is row zero times R zero
[688.98s -> 691.62s]  then you get the second term which is row zero
[691.62s -> 694.60s]  times gamma times row one times R one.
[695.70s -> 698.74s]  And then you would get row zero times gamma,
[698.74s -> 701.54s]  row one, gamma, row two, gamma, et cetera, R two.
[702.74s -> 705.90s]  And I wrote it in kind of a slightly
[705.90s -> 707.04s]  counterintuitive way intentionally
[707.04s -> 710.02s]  where I actually interleave the gammas with the rows.
[710.02s -> 711.76s]  So I could also just collect all the gammas
[711.80s -> 713.88s]  and just write gamma to the T.
[713.88s -> 715.48s]  But I intentionally wrote it this way
[715.48s -> 717.92s]  so that you get this alternating pattern of row,
[717.92s -> 719.80s]  gamma, row, gamma, row, gamma.
[719.80s -> 721.64s]  And this is gonna be important later.
[722.48s -> 725.78s]  So what we can then do is
[725.78s -> 727.08s]  we can put some parentheses around.
[727.08s -> 729.40s]  You'll notice that every term in the sum
[729.40s -> 730.80s]  starts with row zero.
[730.80s -> 733.50s]  So you can just take all the row zero out
[733.50s -> 735.04s]  and collect all the other terms in parentheses.
[735.04s -> 738.72s]  So now you have row zero times in parentheses a big sum
[738.72s -> 739.84s]  which consists of all the other stuff.
[739.84s -> 742.70s]  R zero plus gamma plus all the future stuff.
[743.76s -> 744.92s]  And then you can repeat the process.
[744.92s -> 747.40s]  You can collect all the terms that have row zero
[747.40s -> 751.36s]  and row one, and that's the second set of parentheses.
[751.36s -> 753.60s]  So that's why you have gamma, row one,
[753.60s -> 756.44s]  and then in parentheses, R one plus all the other stuff.
[756.44s -> 757.64s]  And you can just keep going like this
[757.64s -> 760.48s]  and you can get all these nested summations.
[760.48s -> 761.52s]  We're not actually changing anything.
[761.52s -> 764.04s]  We're just basically using distributive
[764.04s -> 765.72s]  and commutative properties of multiplication
[765.72s -> 768.68s]  and addition to group these terms together.
[768.72s -> 771.08s]  So just a little bit of arithmetic,
[772.76s -> 774.56s]  a little bit of algebra.
[774.56s -> 779.56s]  And let's call this V bar superscript T.
[781.44s -> 784.22s]  V bar superscript T is an important sampling estimator
[784.22s -> 785.62s]  of V pi theta S zero.
[791.16s -> 795.28s]  And now you can notice that there's a recursion here.
[795.28s -> 800.28s]  So V bar T plus one minus T is equal to row T
[801.12s -> 806.12s]  times RT plus gamma V bar capital T minus little t.
[808.28s -> 813.28s]  So essentially V bar to the capital T minus little t,
[815.08s -> 817.68s]  that's the stuff in parentheses after the gamma.
[820.52s -> 822.84s]  So if this is not completely obvious,
[822.84s -> 824.78s]  you may wanna pause the video here.
[825.60s -> 827.60s]  You could consider even getting out a little sheet of paper
[827.60s -> 829.68s]  and just working this out to convince yourself
[829.68s -> 830.64s]  that this is true.
[832.20s -> 833.66s]  This is a little subtle, right?
[833.66s -> 835.48s]  We're introducing a little bit of notation
[835.48s -> 840.44s]  to induce a recursion that allows us to describe
[840.44s -> 843.94s]  this important sampling estimator in a recursive way.
[843.94s -> 848.94s]  Okay, so our goal is to ultimately get V bar capital T.
[855.60s -> 857.46s]  And this recursion describes a way
[857.46s -> 860.80s]  to essentially bootstrap our way to V bar capital T.
[863.08s -> 865.58s]  So now let's talk about doubly robust estimation.
[866.48s -> 870.14s]  Doubly robust estimation is a little bit easier
[870.14s -> 872.74s]  to derive first in the case of a bandit problem.
[873.62s -> 877.60s]  So in the bandit case, there's only one time step.
[877.60s -> 878.66s]  And all you're trying to do
[878.66s -> 881.46s]  is you're trying to estimate the reward.
[881.46s -> 883.42s]  Now you can still do this with important sampling.
[883.42s -> 885.24s]  Doing a bandit with important sampling is a little weird,
[885.24s -> 888.68s]  but it works and it gives us the intuition
[888.68s -> 889.94s]  for the multi-step case.
[891.42s -> 894.94s]  So a regular important sampled bandit
[894.94s -> 897.78s]  would just be row SA times RSA, right?
[897.78s -> 901.82s]  So we have rewards from some other distribution
[901.82s -> 903.46s]  and we're gonna multiply them by an importance weight,
[903.46s -> 905.90s]  and that'll give us the value of our bandit.
[905.90s -> 907.30s]  And this is a contextual bandit, right?
[907.30s -> 909.10s]  So this is a bandit that has a state.
[910.66s -> 914.10s]  But now let's say that we have some guess
[914.10s -> 915.62s]  as to the value function.
[915.62s -> 918.82s]  This guess doesn't have to be very accurate, right?
[918.82s -> 923.82s]  So we have some guess V hat S and some guess Q hat SA.
[924.06s -> 925.74s]  And how do we get this guess?
[925.74s -> 927.48s]  Well, maybe we just train a neural network
[927.48s -> 929.94s]  to regress onto the values, right?
[931.98s -> 936.04s]  One thing we need here is we need V hat S
[936.04s -> 938.44s]  to actually be the expected value of Q hat SA
[938.44s -> 939.82s]  with respect to the actions.
[939.82s -> 941.70s]  But that's pretty easy to get.
[941.70s -> 944.38s]  You could just estimate Q hat SA
[944.38s -> 946.34s]  and then just estimate V hat S
[946.34s -> 948.58s]  with samples from your policy, okay?
[950.74s -> 952.50s]  Then the doubly robust estimator
[952.50s -> 957.24s]  basically takes this importance weighted rewards,
[957.24s -> 961.50s]  subtracts your estimated function approximation
[961.50s -> 963.62s]  and then adds back in its expected value.
[963.62s -> 965.96s]  So this is a lot like the control variants
[965.96s -> 968.06s]  or baselines that we learned about before.
[969.44s -> 972.18s]  And the doubly robust estimator
[972.18s -> 975.70s]  is going to be unbiased in expectation,
[975.70s -> 978.22s]  just like the baseline regardless of the choice of Q hat,
[978.22s -> 981.82s]  so long as V hat is in fact the expected value of Q hat.
[981.82s -> 984.22s]  But of course, the closer Q hat is
[984.22s -> 988.46s]  to the true Q values,
[988.46s -> 989.80s]  the lower the variance of this will be
[989.80s -> 992.72s]  because in the best case,
[992.72s -> 995.48s]  if Q hat perfectly cancels off RSA here,
[995.48s -> 997.56s]  then that second term, the high variance,
[997.56s -> 999.66s]  important sample term goes to zero
[999.66s -> 1002.24s]  and the first term, which has very low variance,
[1002.24s -> 1003.64s]  dominates.
[1003.64s -> 1007.46s]  So just like the baseline allowed us to reduce variance,
[1007.46s -> 1009.70s]  this function approximator allows us to reduce
[1009.70s -> 1011.92s]  the variance of this important sample estimator.
[1011.92s -> 1013.34s]  Now this is the bandit case.
[1013.34s -> 1015.64s]  The real trick with the doubly robust estimator
[1015.64s -> 1018.72s]  is to extend it to the multi-step case.
[1018.72s -> 1020.12s]  And what we're going to do is we're going to take this
[1020.12s -> 1021.28s]  thing in the blue box,
[1021.28s -> 1023.36s]  and we're going to try to apply the same idea
[1023.36s -> 1024.56s]  to these V bars.
[1027.12s -> 1028.44s]  So let's do that.
[1028.44s -> 1029.92s]  So we're going to define,
[1029.92s -> 1033.48s]  in the same way that we define V bar capital T plus one
[1033.48s -> 1034.68s]  minus little t,
[1034.68s -> 1037.68s]  we're going to define a doubly robust version,
[1037.68s -> 1040.80s]  V bar capital T plus one minus little t.
[1040.80s -> 1043.00s]  And the way we're going to do that is we'll directly
[1043.00s -> 1046.08s]  substitute it in the bandit case into this.
[1046.08s -> 1047.08s]  So in the bandit case,
[1047.08s -> 1049.50s]  we're doing an important sample estimate of RSA.
[1049.50s -> 1051.72s]  Now we're doing an important sample estimate of RT
[1051.72s -> 1055.60s]  plus gamma V bar capital T minus little t.
[1055.60s -> 1057.24s]  So essentially this equation that I have,
[1057.24s -> 1060.88s]  the V bar DR is exactly the bandit case in the blue box,
[1060.88s -> 1065.26s]  but with R replaced by R plus gamma V bar capital T
[1065.26s -> 1066.10s]  minus little t.
[1067.52s -> 1070.20s]  So it's essentially a recursive version of the bandit case
[1070.20s -> 1072.08s]  for the multi-step problem.
[1072.08s -> 1073.30s]  So in order to do this,
[1073.30s -> 1076.76s]  you need to construct an estimate Q hat STAT,
[1077.28s -> 1079.26s]  Q hat STAT could be some neural net,
[1079.26s -> 1081.06s]  your favorite function approximator,
[1081.06s -> 1082.52s]  and you need to get its expected value
[1082.52s -> 1084.72s]  with respect to the actions distributed according
[1084.72s -> 1086.20s]  to your policy pi theta,
[1086.20s -> 1087.78s]  and that gives you V hat.
[1090.00s -> 1094.14s]  And then just like the recursion V bar capital T
[1094.14s -> 1095.88s]  plus one minus little t,
[1095.88s -> 1100.44s]  can be used to obtain the important sample estimate,
[1100.44s -> 1103.76s]  the doubly robust recursion can be used
[1103.76s -> 1105.64s]  to obtain a doubly robust version
[1105.64s -> 1107.32s]  of the important sample estimate.
[1110.80s -> 1115.30s]  So that's the idea behind the doubly robust
[1115.30s -> 1117.62s]  off-policy value evaluation.
[1117.62s -> 1119.64s]  Now, this is an off-policy evaluation method,
[1119.64s -> 1120.64s]  this is an OPE method,
[1120.64s -> 1122.76s]  it is not a reinforcement learning method.
[1122.76s -> 1124.76s]  So this will give you estimates of values,
[1124.76s -> 1126.78s]  and you could use those values just to evaluate
[1126.78s -> 1127.84s]  which policy is better,
[1127.84s -> 1130.04s]  or you can plug them into an important sample
[1130.04s -> 1131.44s]  policy gradient estimator.
[1131.44s -> 1136.44s]  Okay, there is one more topic
[1137.96s -> 1139.84s]  that I want to very briefly cover.
[1139.84s -> 1141.78s]  I'm not gonna go into the technical details for this,
[1141.78s -> 1143.54s]  but I just wanna describe this
[1143.54s -> 1145.32s]  so that all of you are aware it exists,
[1145.32s -> 1147.74s]  which is marginalized important sampling.
[1147.74s -> 1150.00s]  So, so far when we talked about important sampling,
[1150.00s -> 1152.72s]  we always talked about important sampling
[1152.72s -> 1156.60s]  for the case where you're computing importance weights
[1156.60s -> 1159.16s]  as ratios of action probabilities.
[1159.16s -> 1161.54s]  But it is actually possible to do important sampling
[1161.54s -> 1162.88s]  with state probabilities.
[1164.04s -> 1165.70s]  So the main idea in what is called
[1165.70s -> 1167.74s]  marginalized important sampling,
[1167.74s -> 1170.22s]  is that instead of using a product of action probabilities
[1170.22s -> 1171.56s]  like we did before,
[1171.56s -> 1173.22s]  we're gonna estimate importance weights
[1173.22s -> 1175.24s]  that are ratios of state probabilities
[1175.24s -> 1177.96s]  or state action probabilities.
[1177.96s -> 1180.20s]  Now, the difference between states and state actions
[1180.20s -> 1181.08s]  is not actually that different,
[1181.08s -> 1183.60s]  because once you have a ratio of state probabilities,
[1183.60s -> 1185.22s]  it's very easy to turn them into ratios
[1185.22s -> 1186.44s]  of state action probabilities,
[1186.44s -> 1188.34s]  because you know A given S.
[1190.16s -> 1195.16s]  But if you can recover these state action importance weights
[1197.44s -> 1200.72s]  then it's very easy to estimate the value of some policy
[1200.72s -> 1203.32s]  just by summing over all of your samples
[1203.32s -> 1207.48s]  and averaging together the weighted rewards.
[1209.20s -> 1211.40s]  So doing off-policy evaluation is trivial
[1211.40s -> 1214.00s]  if you can recover these WSAs.
[1214.00s -> 1216.38s]  And of course, if you can do off-policy evaluation,
[1216.38s -> 1218.48s]  then you could also plug those value estimates
[1218.52s -> 1220.68s]  into policy gradient as well, if you prefer.
[1222.60s -> 1224.50s]  But typically marginalized importance sampling
[1224.50s -> 1226.28s]  in the literature has been used
[1226.28s -> 1227.86s]  just for off-policy evaluation.
[1227.86s -> 1230.28s]  I haven't seen it used very much for policy learning,
[1230.28s -> 1232.96s]  although I think that should be possible.
[1234.64s -> 1236.84s]  So the biggest challenge with this
[1236.84s -> 1239.72s]  is of course how to determine WSA.
[1239.72s -> 1243.60s]  And typically, since we don't know the state marginals
[1243.60s -> 1246.32s]  of either our policy or the behavior policy,
[1246.32s -> 1247.88s]  what we would typically do
[1247.88s -> 1250.12s]  is we would write down some kind of consistency condition
[1250.12s -> 1253.20s]  on W and then solve that consistency condition.
[1253.20s -> 1255.30s]  You can think of this consistency condition
[1255.30s -> 1258.12s]  as kind of the equivalent of the Bellman equation,
[1258.12s -> 1260.00s]  but for importance weights.
[1260.00s -> 1262.00s]  So the Bellman equation describes
[1262.00s -> 1264.24s]  the consistency condition on value functions
[1264.24s -> 1266.72s]  that like for example, QSA should be equal to RSA
[1266.72s -> 1269.36s]  plus gamma QS prime A prime,
[1269.36s -> 1270.54s]  that's a consistency condition.
[1270.54s -> 1273.24s]  And if you can make that equality hold true everywhere,
[1273.24s -> 1275.50s]  then you will recover a valid Q function.
[1275.50s -> 1276.56s]  In the same way,
[1276.56s -> 1279.60s]  you can write down a consistency condition for W,
[1279.60s -> 1282.04s]  and if you can make that condition hold true everywhere,
[1282.04s -> 1284.86s]  then you will have recovered the true state
[1284.86s -> 1286.68s]  or state action importance weights.
[1287.72s -> 1289.78s]  So here's one such consistency condition.
[1289.78s -> 1292.92s]  This is from a paper by Zhang et al called GenDICE.
[1292.92s -> 1295.48s]  I won't go through this in detail
[1295.48s -> 1297.28s]  because the derivation for this is kind of involved
[1297.28s -> 1298.82s]  and I've already spent quite a bit of time
[1298.82s -> 1300.58s]  on importance sampling,
[1300.58s -> 1301.84s]  but I wanna just give you a taste
[1301.84s -> 1305.64s]  for what the general gist of this is.
[1305.64s -> 1309.12s]  So if you look at this consistency condition,
[1309.12s -> 1311.16s]  you can see that on the left hand side,
[1311.16s -> 1314.18s]  you have the probability of seeing a state
[1314.18s -> 1315.98s]  and an action S prime A prime
[1315.98s -> 1319.58s]  under the behavior policy pi beta times the weight.
[1320.64s -> 1322.64s]  Now, what's going on here?
[1322.64s -> 1325.04s]  Well, if you multiply D pi beta by the weight,
[1325.04s -> 1326.88s]  you get D pi theta, right?
[1326.88s -> 1330.16s]  Because the weight is D pi theta over D pi beta.
[1330.16s -> 1333.80s]  So what this is really describing is a condition
[1333.80s -> 1336.58s]  that state action marginals need to obey.
[1338.00s -> 1340.06s]  When it comes time to actually optimize this in practice,
[1340.06s -> 1343.22s]  of course, we're gonna subtract the right-hand side
[1343.22s -> 1344.44s]  from the left-hand side.
[1344.44s -> 1346.64s]  And as long as we get a multiplier of D pi beta
[1346.64s -> 1347.74s]  in front of everything,
[1349.38s -> 1351.40s]  then we can approximate that with samples.
[1351.40s -> 1354.00s]  So in reality, we never actually estimate
[1354.00s -> 1355.22s]  these D pi beta terms directly.
[1355.22s -> 1357.20s]  We always use samples from our dataset
[1357.20s -> 1358.70s]  as samples from D pi beta.
[1361.32s -> 1362.68s]  So the left-hand side of this
[1362.68s -> 1365.48s]  is just the probability of seeing S prime A prime
[1365.48s -> 1367.72s]  under the policy pi theta.
[1367.72s -> 1370.42s]  And the probability of seeing S prime A prime
[1370.42s -> 1371.84s]  is basically equal to the probability
[1371.84s -> 1374.92s]  that you start in S prime A prime,
[1374.92s -> 1376.60s]  and that's what the first term captures.
[1376.60s -> 1378.48s]  So it's the probability you start in S prime
[1378.48s -> 1381.14s]  times the probability that you take the action A prime
[1381.14s -> 1384.20s]  plus the probability that you transition
[1384.20s -> 1386.60s]  into S prime A prime from another state.
[1387.72s -> 1388.98s]  The probability that you transition
[1388.98s -> 1391.84s]  to S prime A prime from another state,
[1391.84s -> 1396.52s]  in this case sA, is given by the probability that you are in that state, which is d pi
[1396.52s -> 1400.68s]  theta sA, and that's exactly what the product of those last two terms gives you, times
[1400.68s -> 1404.36s]  the probability that you make that transition, which is what you get by multiplying it
[1404.36s -> 1409.64s]  by p of s prime given sA times pi theta A prime given s prime. So this is the probability
[1409.64s -> 1414.36s]  of starting in s prime A prime, and this is the probability of transitioning into it
[1414.36s -> 1418.32s]  from another state multiplied by the probability that you are actually in that state, which
[1418.32s -> 1424.24s]  is what the last two terms account for. So solving for w sA typically involves some kind
[1424.24s -> 1429.00s]  of fixed point problem, so it involves subtracting the right-hand side from the left-hand side
[1429.00s -> 1433.96s]  and minimizing the squared difference, and the trick in deriving these algorithms is
[1433.96s -> 1438.68s]  to basically turn that difference into an expected value under d pi beta, because
[1438.68s -> 1442.28s]  once you can express it as an expected value under d pi beta, then you can use samples
[1442.28s -> 1447.60s]  from d pi beta to estimate it, and that means that you never have to explicitly approximate,
[1447.60s -> 1451.48s]  you never need to have a neural net that approximates d pi beta or d pi theta, you
[1451.48s -> 1458.72s]  only need a neural net that approximates w. So this is the basic idea of marginalized
[1458.72s -> 1465.72s]  importance sampling. Write down a relationship between the w's at future states and current
[1465.72s -> 1470.80s]  states, turn that relationship into an error, and then estimate that error using only
[1470.80s -> 1474.80s]  the samples in your dataset, and then typically you would represent the w's with some kind
[1474.80s -> 1481.36s]  of neural net. Okay, so that was kind of a quick whirlwind tour of importance sampling
[1481.36s -> 1487.64s]  for off-policy evaluation and Batch-RL. If you want to learn more about this, classic
[1487.64s -> 1492.24s]  work on importance sampled policy gradients and return estimation by doing a pre-cup as
[1492.24s -> 1496.84s]  well as by Peschkin and Shelton, doubly robust estimators, very interesting if you want
[1496.84s -> 1503.56s]  to learn about OPE with importance weights, so certainly for bandits and for small MDPs,
[1503.60s -> 1507.80s]  robust estimators are typically the methods of choice if you're doing things like add placement,
[1507.80s -> 1513.40s]  stuff like that, but there are better techniques for actually learning policies these days.
[1513.40s -> 1518.12s]  Some analysis and theory, so this paper by Philip Thomas, High Confidence Off Policy
[1518.12s -> 1522.32s]  Evaluation, provides a lot of analysis of these types of estimators, and if you want
[1522.32s -> 1526.80s]  to learn about marginalized importance sampling, consider checking out these two papers as
[1526.80s -> 1529.88s]  well as the Zhang et al. paper that I referenced on the previous slide.
