# Detected language: en (p=1.00)

[0.00s -> 3.66s]  Okay, let's talk about what we'll cover in the class.
[5.06s -> 9.08s]  So this course goes through a variety
[9.08s -> 10.70s]  of deep reinforcement learning methods,
[10.70s -> 12.22s]  construed very broadly.
[12.22s -> 13.46s]  We'll start with some basics.
[13.46s -> 17.04s]  We'll start by talking about how we can take a journey
[17.04s -> 19.54s]  from supervised learning methods
[19.54s -> 22.50s]  to decision-making methods, provide some definitions,
[22.50s -> 23.98s]  and generally come to understand
[23.98s -> 25.78s]  the reinforcement learning problem.
[25.78s -> 26.60s]  Then we'll have a unit
[26.60s -> 28.74s]  on model-free reinforcement learning algorithms,
[28.74s -> 29.82s]  where we'll cover Q-learning,
[29.82s -> 31.90s]  policy gradient, and actor-critic methods.
[31.90s -> 33.14s]  And you'll have some homeworks
[33.14s -> 34.92s]  where you'll implement each of these.
[34.92s -> 37.02s]  Then we'll have another unit on model-based algorithms.
[37.02s -> 39.38s]  We'll talk about planning, optimal control,
[39.38s -> 42.18s]  sequence models, images, and things like that.
[42.18s -> 44.38s]  And then we'll have a variety of more advanced topics.
[44.38s -> 46.74s]  We're going to cover exploration algorithms.
[46.74s -> 47.66s]  We're going to cover algorithms
[47.66s -> 49.10s]  for offline reinforcement learning,
[49.10s -> 51.50s]  which are methods that can use both data
[51.50s -> 53.78s]  and reinforcement learning methods.
[53.78s -> 55.90s]  We'll talk about inverse reinforcement learning,
[55.90s -> 57.96s]  which deals with inferring objective functions
[57.96s -> 62.00s]  from behavior, and have some discussion in there
[62.00s -> 64.40s]  about the relationship between reinforcement learning
[64.40s -> 66.36s]  methods and things like probabilistic inference.
[66.36s -> 68.12s]  And then we'll have a few advanced topics
[68.12s -> 69.64s]  like meta-learning and transfer learning,
[69.64s -> 71.02s]  maybe hierarchical RL,
[71.02s -> 73.84s]  and a set of research talks and invited lectures.
[73.84s -> 76.28s]  So that's the overall overview of the class.
[76.28s -> 78.08s]  You're going to have five assignments.
[78.08s -> 80.10s]  There will be an assignment on annotation learning,
[80.10s -> 82.80s]  policy gradients, Q-learning and actor-critic algorithms,
[82.80s -> 85.76s]  model-based RL, and the last one will be on offline RL.
[85.76s -> 87.24s]  And there will be a final project.
[87.28s -> 89.80s]  The final project is a research-level project
[89.80s -> 90.80s]  of your choice.
[90.80s -> 92.72s]  You can form a group of up to two to three students,
[92.72s -> 93.72s]  and you're more than welcome
[93.72s -> 95.32s]  to start on this project early.
[96.26s -> 98.32s]  The students every year have some questions
[98.32s -> 101.44s]  about our expectations for the scope of this project.
[101.44s -> 103.36s]  Roughly speaking, you should think about it
[103.36s -> 105.82s]  as roughly at the level of a paper
[105.82s -> 107.96s]  that you might submit, for example, to a workshop.
[107.96s -> 109.96s]  If you're not sure about the scope of your project,
[109.96s -> 111.72s]  definitely come into office hours
[111.72s -> 113.78s]  to talk to the TAs or to myself.
[114.62s -> 117.74s]  We will have multiple rounds of feedback
[117.74s -> 118.58s]  for you for your project.
[118.58s -> 120.62s]  So we'll have a project proposal deadline
[120.62s -> 122.50s]  and our project milestone report.
[122.50s -> 123.94s]  These are really meant for you.
[123.94s -> 128.42s]  We strongly encourage you to write up your plan
[128.42s -> 130.46s]  to describe potential concerns you have about your plan
[130.46s -> 131.90s]  and so on, and we'll give you feedback on those.
[131.90s -> 134.22s]  So the proposal and the milestone
[134.22s -> 135.50s]  are not graded very strictly.
[135.50s -> 136.38s]  They're really meant much more
[136.38s -> 138.46s]  as a way for you to get feedback on your project plan
[138.46s -> 141.42s]  before the final report at the end of the semester.
[141.42s -> 143.18s]  You'll be graded 50% on the homeworks,
[143.62s -> 147.62s]  40% on the project, and 10% on quizzes after every lecture.
[147.62s -> 151.40s]  And you'll have a total of five late days
[151.40s -> 152.24s]  for your homeworks.
[152.24s -> 153.36s]  Don't exceed those five late days.
[153.36s -> 155.34s]  That's five late days total.
[155.34s -> 157.62s]  If you exceed them, then we unfortunately
[157.62s -> 159.62s]  cannot give you credit for that homework.
[160.78s -> 163.46s]  You also have a little bit of homework for today.
[163.46s -> 165.86s]  Make sure that you're signed up for ED,
[165.86s -> 167.94s]  UC Berkeley CS285.
[167.94s -> 169.94s]  All of you who have been signed up
[169.94s -> 171.26s]  for the course officially
[171.26s -> 173.90s]  would have received an invitation for this.
[173.90s -> 176.34s]  We strongly encourage you to start forming project groups,
[176.34s -> 178.30s]  unless you want to work alone, which is fine.
[178.30s -> 179.70s]  And take the Lecture 1 Quiz.
[179.70s -> 181.98s]  So the Lecture 1 Quiz is posted on Gradescope.
[181.98s -> 184.82s]  The Lecture 1 Quiz is very much a practice quiz.
[184.82s -> 187.58s]  It's not, there's not a real quiz there,
[187.58s -> 188.82s]  and it's really to get you to be familiar
[188.82s -> 190.42s]  with the Gradescope interface.
[192.78s -> 195.10s]  However, what I want to focus on mainly
[195.10s -> 198.06s]  in today's lecture is discussing why we should study
[198.06s -> 199.62s]  reinforced learning, what it is,
[199.66s -> 201.34s]  and a little bit of context for why
[201.34s -> 203.78s]  I myself like to teach this class.
[203.78s -> 204.86s]  But let's start with some basics.
[204.86s -> 206.50s]  What is reinforcement learning?
[208.02s -> 209.66s]  Well, reinforcement learning is really two things.
[209.66s -> 210.82s]  It's a mathematical formalism
[210.82s -> 212.42s]  for learning-based decision-making,
[212.42s -> 214.50s]  and it's also an approach for learning decision-making
[214.50s -> 216.54s]  and control from experience.
[216.54s -> 218.34s]  And it's important to keep in mind
[218.34s -> 219.38s]  these are somewhat separate,
[219.38s -> 221.62s]  because we could imagine taking the formalism
[221.62s -> 223.44s]  and then applying all sorts of different methods to it.
[223.44s -> 224.66s]  So it's important not to confuse
[224.66s -> 226.10s]  reinforcement learning the problem
[226.10s -> 228.14s]  from reinforcement learning the solution.
[228.78s -> 230.18s]  Okay, how is this different
[230.18s -> 232.30s]  from other machine learning topics?
[232.30s -> 233.74s]  Well, the kind of machine learning
[233.74s -> 235.58s]  that most of you are probably familiar with
[235.58s -> 237.10s]  is supervised learning.
[237.10s -> 239.90s]  Supervised learning is fairly straightforward to define.
[239.90s -> 241.82s]  You have a data set of inputs and outputs.
[241.82s -> 244.26s]  We refer to them typically as X and Y,
[244.26s -> 246.54s]  and you want to learn to predict Y from X.
[246.54s -> 249.22s]  So you want to learn some kind of function, f of X,
[249.22s -> 252.06s]  which outputs values Y that are close
[252.06s -> 253.62s]  to the Y labels in the data set.
[253.62s -> 255.62s]  So for example, f might be represented
[255.62s -> 257.94s]  by a deep neural network that you would train
[258.78s -> 262.06s]  via classification or regression to match the labels Y.
[262.06s -> 264.62s]  And while the basic formulation
[264.62s -> 267.86s]  of supervised machine learning is very straightforward,
[267.86s -> 269.40s]  supervised machine learning methods
[269.40s -> 270.90s]  make a number of assumptions
[270.90s -> 272.46s]  that we often don't even think about
[272.46s -> 273.86s]  because they're so natural,
[273.86s -> 275.58s]  but that are important to bring up
[275.58s -> 277.46s]  if we're gonna discuss how this differs
[277.46s -> 279.30s]  from reinforcement learning.
[279.30s -> 281.50s]  Supervised learning typically assumes what is called
[281.50s -> 284.18s]  independent and identically distributed data.
[284.18s -> 286.82s]  This is such an obvious assumption in some ways,
[286.82s -> 289.06s]  especially for someone who studied machine learning,
[289.06s -> 291.14s]  that we often don't make it explicit.
[291.14s -> 294.06s]  But what it means is that all of these X, Y pairs
[294.06s -> 297.84s]  in your data set are independent of one another
[297.84s -> 299.70s]  in the sense that the label for one X
[299.70s -> 302.14s]  doesn't influence the label for another X.
[302.14s -> 304.26s]  And they're distributed identically in the sense
[304.26s -> 308.18s]  that the true function that produced the label Y from X
[308.18s -> 310.72s]  is the same for all the samples.
[310.72s -> 313.10s]  It's almost an obvious statement,
[313.10s -> 315.82s]  but it's something that is important to keep in mind.
[317.02s -> 318.76s]  Supervised learning also assumes
[318.76s -> 319.86s]  that our data set is labeled
[319.86s -> 322.02s]  in the sense that every X we've seen in D
[322.02s -> 323.78s]  also has an accompanying Y
[323.78s -> 326.22s]  and that Y is the true label for that X.
[327.88s -> 329.26s]  This is very natural if you're doing things
[329.26s -> 332.38s]  like image classification with labels obtained from humans.
[332.38s -> 334.50s]  But remember how we discussed in the grasping example,
[334.50s -> 336.22s]  this can actually be pretty unnatural.
[336.22s -> 338.50s]  If you want a robot to learn how to grasp objects,
[338.50s -> 340.16s]  it's actually a very strong assumption
[340.16s -> 342.02s]  to assume that you're given a set of images
[342.02s -> 344.80s]  with ground truth optimal grasp locations.
[345.72s -> 347.84s]  Reinforcement learning does not assume
[347.84s -> 350.48s]  that the data is independent identically distributed
[350.48s -> 352.32s]  in the sense that previous outputs
[352.32s -> 353.68s]  influence future inputs.
[353.68s -> 356.06s]  Things are arranged in a temporal sequence
[356.06s -> 358.04s]  and the past influences the future.
[358.92s -> 360.94s]  Typically, the ground truth answer is not known.
[360.94s -> 363.80s]  It's only known how good a particular outcome was,
[363.80s -> 365.68s]  whether it was a failure or success
[365.68s -> 368.72s]  or more generally what its reward value was.
[368.72s -> 372.50s]  So in reinforcement learning, you might collect data,
[372.50s -> 374.12s]  but you can't simply copy that data.
[374.40s -> 376.04s]  That doesn't actually lead to success.
[376.04s -> 378.36s]  The data might tell you which things were successful
[378.36s -> 379.84s]  and which things failed.
[379.84s -> 381.28s]  Although even those labels
[381.28s -> 382.76s]  are difficult to interpret properly
[382.76s -> 384.76s]  because if you have a sequence of events
[384.76s -> 386.92s]  that led to a failure, you don't know which event,
[386.92s -> 388.88s]  which particular choice in that sequence
[388.88s -> 390.92s]  was the one that precipitated the failure.
[390.92s -> 394.12s]  This is not unlike human decision-making.
[394.12s -> 395.80s]  Perhaps you got a really bad grade
[395.80s -> 397.00s]  at the end of a course.
[397.00s -> 400.78s]  Well, it wasn't the fact that you looked up your grade
[400.78s -> 403.18s]  on Cal Central that caused you to get the bad grade.
[403.22s -> 404.66s]  It was something you did earlier in the class,
[404.66s -> 407.66s]  perhaps the fact that you did poorly on an exam.
[407.66s -> 409.14s]  At the time, perhaps you didn't realize
[409.14s -> 410.82s]  that this would lead you to fail the course.
[410.82s -> 412.52s]  So this is very much an issue
[412.52s -> 413.62s]  that we have in reinforcement learning
[413.62s -> 415.06s]  referred to as credit assignment,
[415.06s -> 417.54s]  where the decision that actually results in a bad outcome
[417.54s -> 419.30s]  or a good outcome might not itself
[419.30s -> 420.62s]  be labeled with a high or low reward.
[420.62s -> 423.22s]  The reward might only happen later.
[423.22s -> 424.76s]  So we need to take this data,
[424.76s -> 427.58s]  which is not labeled with ground truth optimal outputs
[427.58s -> 429.62s]  and might involve these delayed rewards,
[429.62s -> 431.18s]  run reinforcement learning on it,
[431.18s -> 433.66s]  and hopefully get a behavior that is better
[433.66s -> 435.54s]  than the behavior we saw before.
[435.54s -> 437.86s]  So that's really the challenge in reinforcement learning.
[439.18s -> 441.74s]  So let's try to make this a little bit more precise.
[441.74s -> 442.58s]  In supervised learning,
[442.58s -> 444.72s]  you have an input x and you have an output y,
[444.72s -> 447.94s]  and you have a data set that consists of x-y pairs.
[447.94s -> 449.96s]  The goal is to learn a function
[449.96s -> 452.08s]  that takes an x and approximates y.
[452.08s -> 454.30s]  And typically this function has some parameters,
[454.30s -> 455.82s]  which we refer to as theta.
[455.82s -> 457.02s]  These might be, for example,
[457.02s -> 458.72s]  the weights in a neural network.
[459.72s -> 461.52s]  In reinforcement learning,
[461.52s -> 465.00s]  we have a kind of a cyclical online learning procedure
[465.00s -> 466.96s]  where an agent interacts with the world.
[466.96s -> 470.44s]  The agent chooses actions, a team at every point in time,
[470.44s -> 473.68s]  and the world responds with the resulting state,
[473.68s -> 476.24s]  st plus one, and reward signal.
[476.24s -> 478.24s]  And the reward signal simply indicates
[478.24s -> 479.28s]  how good that state is,
[479.28s -> 480.36s]  but it doesn't necessarily tell you
[480.36s -> 481.64s]  if the action that you just took
[481.64s -> 482.88s]  was a good or bad action.
[482.88s -> 485.00s]  Perhaps you got lucky and landed in a good state,
[485.00s -> 487.08s]  or perhaps you did something really good earlier
[487.08s -> 489.32s]  that caused you to get into a good state now.
[490.96s -> 493.00s]  The input to our agent
[493.00s -> 495.62s]  is going to be the state st at each time step.
[495.62s -> 497.84s]  So this is kind of the analog of x.
[497.84s -> 500.72s]  The output is at each time step.
[500.72s -> 502.60s]  The data, which is collected
[502.60s -> 504.16s]  by the agent itself classically,
[504.16s -> 506.68s]  consists of sequences of states, actions, and rewards.
[506.68s -> 508.56s]  Rewards are numbers, scalar values.
[509.80s -> 513.70s]  And whereas in supervised learning, the data is given to you.
[513.70s -> 515.92s]  You don't have to worry about who gave you the data.
[515.92s -> 518.88s]  It's just provided to you as a set of x, y tuples
[518.88s -> 520.14s]  in reinforcement learning.
[520.14s -> 521.24s]  You have to pick your own actions
[521.24s -> 522.08s]  and collect your own data.
[522.08s -> 523.98s]  So not only do you have to worry about the fact
[523.98s -> 526.08s]  that the actions in your data set
[526.08s -> 528.26s]  might not be the optimal actions,
[528.26s -> 529.82s]  you have to also actually decide
[529.82s -> 531.92s]  how that data will be collected.
[531.92s -> 534.64s]  And your goal is to learn a policy, pi theta,
[534.64s -> 537.12s]  which maps states s to actions a.
[537.12s -> 539.80s]  And just like f, pi has parameters theta,
[539.80s -> 542.56s]  so those might again be the weights in a neural network.
[543.68s -> 545.80s]  And a good policy is one that maximizes
[546.62s -> 547.46s]  the cumulative total reward.
[547.46s -> 548.88s]  So not just the reward at your point in time,
[548.88s -> 551.22s]  but the total reward the agent receives.
[551.22s -> 552.84s]  So that involves strategic reasoning.
[552.84s -> 555.92s]  Maybe you might do something that might seem unrewarding
[555.92s -> 557.88s]  now to attain higher rewards later.
[559.52s -> 563.84s]  So let's talk about some examples of how problems
[563.84s -> 567.06s]  could be cast in the terminology of reinforcement learning.
[568.04s -> 569.80s]  Let's say that you'd like to train a dog
[569.80s -> 572.04s]  to perform some trick, right?
[572.04s -> 574.52s]  So in this case, the actions might be
[574.52s -> 577.32s]  the muscle contractions of the dog's muscles.
[577.32s -> 579.16s]  The observations might be what the dog perceives
[579.16s -> 580.76s]  through its sense of sight and smell.
[580.76s -> 582.88s]  The reward might be the treat that it gets.
[582.88s -> 585.12s]  And the dog will then learn to do
[585.12s -> 586.82s]  whatever maximizes that reward,
[586.82s -> 588.52s]  which might be the trick that you want to perform
[588.52s -> 589.96s]  because you reward it with food
[589.96s -> 593.96s]  when the dog performs the trick successfully, okay?
[595.10s -> 596.12s]  Here's another example.
[596.12s -> 597.48s]  Maybe you have a robot.
[597.48s -> 599.76s]  Its actions might be the motor current or pork,
[599.76s -> 602.02s]  some kind of actuation command sent to its motors.
[602.02s -> 604.04s]  Its observations might be the readings from its sensors
[604.44s -> 605.56s]  like camera images.
[605.56s -> 607.54s]  And its reward might be some measure of task success.
[607.54s -> 610.00s]  Maybe this robot needs to run as fast as possible
[610.00s -> 611.16s]  to reach a destination.
[611.16s -> 613.78s]  So its reward function might be the running speed,
[613.78s -> 616.76s]  or it might be whether it reached the destination
[616.76s -> 618.64s]  or not, just maybe received a plus one
[618.64s -> 620.56s]  when it reaches the destination successfully
[620.56s -> 622.04s]  and a minus one otherwise.
[623.68s -> 624.52s]  Here's another problem.
[624.52s -> 626.20s]  Let's say you want to manage inventory,
[626.20s -> 628.28s]  route goods between different warehouses
[628.28s -> 630.72s]  in order to maintain stock levels.
[630.72s -> 633.92s]  Perhaps the actions are which inventory to purchase.
[634.68s -> 636.16s]  The observations are the current inventory levels
[636.16s -> 637.68s]  and the reward might be the profit you make.
[637.68s -> 638.82s]  Perhaps you have to pay
[638.82s -> 640.88s]  if you want to store inventory for a long time,
[640.88s -> 642.88s]  so your profits will be low.
[642.88s -> 645.50s]  So you can see that this formulation is very general.
[645.50s -> 646.88s]  Many different problems can be cast
[646.88s -> 649.20s]  into the framework of reinforcement learning.
[649.20s -> 650.36s]  We'll of course make all of this
[650.36s -> 651.34s]  a lot more precise later.
[651.34s -> 653.60s]  So this is a very high level introduction.
[653.60s -> 656.36s]  Don't worry yet if the particular details
[656.36s -> 657.48s]  aren't very clear.
[657.48s -> 660.18s]  We'll make this a lot more precise in later lectures.
[661.18s -> 663.54s]  But for now, let me just give you some examples
[664.14s -> 664.98s]  of the kinds of things
[664.98s -> 667.02s]  that reinforcement learning methods could do.
[667.98s -> 669.98s]  One of the things that reinforcement learning
[669.98s -> 672.38s]  is very good at is learning policies
[672.38s -> 674.10s]  for physically complex tasks.
[674.10s -> 676.62s]  Tasks where it might be very difficult for a person
[676.62s -> 680.18s]  to describe precisely how the task should be performed,
[680.18s -> 681.66s]  but much easier to find the reward.
[681.66s -> 682.50s]  Like in this case,
[682.50s -> 684.50s]  the reward is that the nail should be hammered in
[684.50s -> 686.62s]  and the reinforcement learning algorithm figures out
[686.62s -> 689.18s]  how to control this robotic hand
[689.18s -> 691.34s]  to move the hammer to hammer in the nail.
[692.34s -> 694.78s]  Here's another complex physical task.
[694.78s -> 698.10s]  Here, this quadrupedal robot needs to be able to jump
[698.10s -> 699.54s]  over different obstacles.
[699.54s -> 702.06s]  Now coding up manually a skill for jumping like this
[702.06s -> 704.34s]  is very tough, but reinforcement learning can learn
[704.34s -> 707.26s]  the actuations that will allow the robot to jump
[707.26s -> 710.46s]  in different locations to various distances and so on.
[710.46s -> 712.58s]  It can even perform more physically complex tasks.
[712.58s -> 714.30s]  So here in the next clip,
[714.30s -> 717.80s]  the quadrupedal robot, this is a baseline method,
[717.80s -> 719.10s]  so don't worry about this.
[719.14s -> 721.78s]  Here, this quadrupedal robot needs to figure out
[721.78s -> 723.42s]  how to stand on its hind legs and balance.
[723.42s -> 725.74s]  And that's also very difficult to do manually,
[725.74s -> 727.82s]  but with an appropriate reinforcement learning method,
[727.82s -> 729.98s]  that's actually possible.
[729.98s -> 732.14s]  In fact, reinforcement learning has been applied
[732.14s -> 734.74s]  very, very widely to robotic problems.
[734.74s -> 736.74s]  Here's an even more recent work.
[736.74s -> 738.12s]  This is from ETH Zurich,
[738.12s -> 739.78s]  showing a robot using reinforcement learning
[739.78s -> 741.94s]  with a combination of simulated optimization
[741.94s -> 744.02s]  to learn various agile skills.
[744.02s -> 746.26s]  And you can see that it can clamber onto obstacles
[746.26s -> 747.46s]  and things like that.
[748.34s -> 750.30s]  The other thing that reinforcement learning is great at
[750.30s -> 751.86s]  is coming up with unexpected solutions.
[751.86s -> 754.34s]  I alluded to this before with the AlphaGo example.
[754.34s -> 755.20s]  Here's another example,
[755.20s -> 757.26s]  which you'll actually implement in your homeworks,
[757.26s -> 759.70s]  where a Q-learning algorithm has learned to play Atari
[759.70s -> 761.78s]  and discovered the strategy that if you bounce the ball
[761.78s -> 763.66s]  up over the bricks, then it will bounce around
[763.66s -> 765.26s]  and you'll get lots of points.
[767.26s -> 768.64s]  Reinforcement learning can also be applied
[768.64s -> 770.36s]  at larger scale in the real world.
[770.36s -> 772.58s]  This is a project that was done
[772.58s -> 774.82s]  at a company called Everyday Robots,
[774.82s -> 776.20s]  which is an alphabet company,
[776.24s -> 778.92s]  where the robots learn to sort trash.
[778.92s -> 782.00s]  So the idea is that if people put trash into recyclables
[782.00s -> 783.88s]  that should actually go into the compost,
[783.88s -> 785.64s]  then the robot can come and sort it.
[785.64s -> 788.60s]  The robots here learn in the real world,
[789.96s -> 792.12s]  both in these classroom environments where they can practice
[792.12s -> 794.88s]  and in actual office buildings,
[794.88s -> 796.24s]  and these vision-based skills
[796.24s -> 798.20s]  that are kind of similar in spirit
[798.20s -> 800.64s]  to the ones that I mentioned in the beginning,
[802.78s -> 804.74s]  can then pick up and move objects
[804.74s -> 806.30s]  in real world office buildings.
[806.30s -> 807.14s]  So that's pretty neat
[807.14s -> 808.58s]  that you can actually practice these things.
[808.58s -> 809.62s]  You can practice them on the job,
[809.62s -> 811.42s]  you can practice them in the real world.
[812.42s -> 814.86s]  And it's of course not just for games and robots.
[815.86s -> 817.54s]  I really like this next example.
[817.54s -> 819.66s]  This is work that was done by Cathy Wu,
[819.66s -> 821.18s]  who's now a professor at MIT
[821.18s -> 823.70s]  and was previously a PhD student here at UC Berkeley.
[823.70s -> 825.02s]  And what Cathy was working on
[825.02s -> 828.32s]  is reinforcement learning algorithms for controlling traffic.
[829.90s -> 832.32s]  This is a kind of a toy example
[832.32s -> 834.80s]  where these cars drive in a circle
[834.80s -> 836.52s]  and what tends to happen
[836.52s -> 838.52s]  even in a simple circular environment like this,
[838.52s -> 842.48s]  if you have a very accurate model
[842.48s -> 843.96s]  of how human drivers behave,
[843.96s -> 846.84s]  you'll actually get traffic jams forming spontaneously.
[846.84s -> 848.12s]  So cars will kind of bunch up
[848.12s -> 849.64s]  and when they bunch up like this,
[849.64s -> 851.48s]  they'll actually spontaneously form traffic jams
[851.48s -> 853.32s]  even though they drive in a circle.
[853.32s -> 854.36s]  So what Cathy then did
[854.36s -> 857.68s]  is she optimized the reinforcement learning policy,
[857.68s -> 860.40s]  which will be shown next for the car shown in red,
[860.68s -> 862.90s]  not to optimize its own speed,
[862.90s -> 866.24s]  but to optimize the speed of the entire circle.
[866.24s -> 868.16s]  And you can see that what this car in red is gonna do
[868.16s -> 869.68s]  is gonna actually slow down
[869.68s -> 873.44s]  and wait for everybody to resolve the traffic jam.
[873.44s -> 874.72s]  And by going a little bit slower,
[874.72s -> 876.88s]  it'll actually avoid the formation of traffic jams
[876.88s -> 877.98s]  in the entire circle.
[879.04s -> 880.96s]  Cathy also experimented with this in other settings.
[880.96s -> 884.18s]  This is a figure eight kind of intersection.
[884.18s -> 885.20s]  And as you might expect,
[885.20s -> 889.00s]  cars will bunch up at this intersection and cause delays.
[889.00s -> 890.96s]  So if there's an autonomous car
[890.96s -> 892.48s]  that is trying to optimize
[892.48s -> 896.36s]  the driving speed of all the cars,
[896.36s -> 899.88s]  the autonomous car will actually slow down a little bit
[899.88s -> 901.16s]  and regulate the traffic
[901.16s -> 902.84s]  so that everyone passes through the intersection
[902.84s -> 904.04s]  at exactly the perfect time.
[904.04s -> 906.04s]  Now, this example maybe is a little bit synthetic,
[906.04s -> 908.12s]  but there's a considerable follow-up work to this
[908.12s -> 909.56s]  showing that in fact,
[909.56s -> 912.12s]  autonomous regulation of traffic with reinforcement learning
[912.12s -> 913.62s]  can be quite a powerful tool.
[915.74s -> 917.52s]  Reinforcement learning has also been used very widely
[917.52s -> 919.48s]  with language models.
[919.48s -> 921.02s]  Many of you are probably familiar
[921.02s -> 922.76s]  with the advances in recent language models
[922.76s -> 924.76s]  with things like ChatGPT
[924.76s -> 927.68s]  and many other systems like a Therapex Cloud
[927.68s -> 929.38s]  or Google's BARD,
[929.38s -> 931.94s]  which use large amounts of data to train models
[931.94s -> 934.78s]  that will fulfill user requests.
[934.78s -> 936.24s]  And this is an example on the right
[936.24s -> 938.08s]  where someone asked ChatGPT to explain
[938.08s -> 940.32s]  how RL with human feedback works for language models
[940.32s -> 942.42s]  and it produces some kind of explanation.
[943.50s -> 946.28s]  Now, by themselves,
[946.32s -> 949.56s]  large language models train on lots of internet data
[949.56s -> 951.56s]  can solve very sophisticated problems,
[951.56s -> 953.36s]  but it's quite difficult to persuade them to do this
[953.36s -> 956.16s]  because these models are basically trying to complete text
[956.16s -> 958.04s]  based on what they learn from internet data.
[958.04s -> 959.10s]  So you have to prompt them in a way
[959.10s -> 962.18s]  that kind of indexes into the right context.
[962.18s -> 963.76s]  Reinforcement learning can be used to make this
[963.76s -> 966.16s]  a lot easier by essentially training these models
[966.16s -> 968.44s]  based on human scores.
[968.44s -> 970.24s]  So instead of just asking them to provide
[970.24s -> 971.40s]  the kind of completions
[971.40s -> 973.00s]  that are most likely from internet data,
[973.00s -> 975.76s]  they can actually be trained to respond to queries
[976.20s -> 979.24s]  in ways that human raters find to be desirable.
[979.24s -> 980.08s]  And reinforcement learning
[980.08s -> 982.10s]  is actually a very important part of this.
[983.04s -> 984.30s]  Reinforcement learning has also been used
[984.30s -> 986.20s]  with image generation.
[986.20s -> 988.84s]  Here's an example with a stable diffusion 1.4.
[988.84s -> 990.00s]  If you ask it to generate a picture
[990.00s -> 991.52s]  of a dolphin riding a bike,
[991.52s -> 992.64s]  it actually generates a picture
[992.64s -> 994.62s]  that is not very good for this.
[994.62s -> 996.48s]  What you can do is you can take this image
[996.48s -> 998.48s]  and you can give it to a captioning model,
[998.48s -> 999.60s]  in this case lava,
[999.60s -> 1001.60s]  to produce a description of the image
[1001.60s -> 1004.04s]  and then use RL where the reward function
[1004.04s -> 1006.56s]  is given by the similarity between the description
[1006.56s -> 1008.38s]  from lava and the original prompt.
[1008.38s -> 1009.72s]  So when lava looks at this picture,
[1009.72s -> 1011.64s]  it might say, oh, this is a picture of a dolphin
[1011.64s -> 1012.80s]  above the water,
[1012.80s -> 1014.60s]  which is not very similar to a dolphin riding a bike.
[1014.60s -> 1016.70s]  So it receives a bad reward for that.
[1018.44s -> 1022.16s]  If we then optimize the image generation model with RL
[1022.16s -> 1023.52s]  to maximize this reward,
[1023.52s -> 1025.04s]  it'll gradually make the image
[1025.04s -> 1026.38s]  more appropriate to the prompt.
[1026.38s -> 1028.36s]  So now there's both a dolphin and a bicycle,
[1028.36s -> 1030.98s]  although the dolphin's not riding the bicycle just yet.
[1030.98s -> 1032.04s]  With a few more iterations,
[1032.04s -> 1034.38s]  now there's a dolphin-like creature
[1034.38s -> 1036.08s]  that is in fact on a bicycle.
[1036.08s -> 1037.42s]  And with some more iterations,
[1037.42s -> 1039.52s]  the creature becomes much more clearly a dolphin,
[1039.52s -> 1041.12s]  apparently putting some waves in the background
[1041.12s -> 1042.64s]  makes an extra dolphin-like,
[1042.64s -> 1044.60s]  and then eventually there's a full-fledged picture
[1044.60s -> 1046.04s]  of a dolphin riding a bicycle.
[1046.04s -> 1047.64s]  So reinforcement learning can be used
[1047.64s -> 1049.44s]  to optimize image generation models.
[1050.84s -> 1052.40s]  Reinforcement learning can also be used for other things.
[1052.40s -> 1054.48s]  This is an example on chip design,
[1054.48s -> 1056.48s]  where the actions correspond to placement
[1056.48s -> 1058.74s]  of chip parts for layout,
[1058.74s -> 1059.78s]  and the reward has to do
[1059.78s -> 1062.04s]  with various chip design parameters,
[1062.04s -> 1064.84s]  like the cost or the congestion or latency of the chip.
[1066.22s -> 1067.06s]  So reinforcement learning
[1067.06s -> 1068.66s]  can actually be applied by thought.
[1069.70s -> 1070.98s]  So I'll pause here,
[1070.98s -> 1071.82s]  and in the next section,
[1071.82s -> 1074.16s]  I'll discuss why we should study deep RL today.
