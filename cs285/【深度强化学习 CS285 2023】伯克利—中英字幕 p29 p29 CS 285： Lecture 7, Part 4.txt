# Detected language: en (p=1.00)

[0.00s -> 5.06s]  The last topic I'll cover in this lecture is a little bit of theory in
[5.06s -> 8.62s]  regard to value-based methods and a little bit more explanation for what I
[8.62s -> 11.50s]  meant before when I said that value-based methods with neural
[11.50s -> 18.38s]  networks don't in general converge to the optimal solution. So to get started,
[18.38s -> 22.46s]  let's start with the value iteration algorithm that we covered before. It's
[22.46s -> 25.22s]  a pretty simple algorithm and it's a little easier for us to think about,
[25.22s -> 28.30s]  but we'll get back to the Q-iteration methods a little bit later.
[28.56s -> 33.10s]  So to remind everybody in value iteration, we can think of it as having two steps.
[33.10s -> 38.64s]  Step one, construct your table of Q-values as the reward plus gamma times
[38.64s -> 42.48s]  the expected value at the next state. And then step two, set your value
[42.48s -> 49.12s]  function to be the max over the rows of that table. So you can think of it as
[49.12s -> 54.76s]  constructing this table of values and then iterating this procedure. So the
[54.76s -> 58.54s]  question we could ask is, does this algorithm converge? And if it does
[58.54s -> 63.82s]  converge, what does it converge to? So one of the ways that we can get started with
[63.82s -> 67.72s]  this analysis is we can define an operator, which I'm going to write as
[67.72s -> 74.34s]  script B, and this operator is called the Bellman operator. The Bellman operator,
[74.34s -> 77.66s]  when applied to a value function, and remember the value function here is a
[77.66s -> 81.12s]  table, so you can think of it as just a vector of numbers, when applied to this
[81.12s -> 85.98s]  vector of numbers, it performs the following operation. First, it takes v
[85.98s -> 93.30s]  and applies the operator T subscript a. T subscript a is a matrix with
[93.30s -> 97.22s]  dimensionality s by s, where every entry in that matrix is the
[97.22s -> 104.62s]  probability of s prime given s comma a, where a is chosen according to that
[104.62s -> 108.98s]  max. So this is basically computing that expectation. The expectation is a
[108.98s -> 115.72s]  linear operation. We multiply it by gamma and we add this vector rA. The vector
[115.72s -> 122.06s]  rA is a vector of rewards where for every state you pick the reward for the
[122.06s -> 127.20s]  corresponding action a. And then outside of this you perform a max over a.
[127.20s -> 134.18s]  And crucially, this max is per element, so for every state we take a max. So
[134.20s -> 140.78s]  this funny way of writing the Bellman backup basically just captures the
[140.78s -> 143.36s]  value iteration algorithm. So the value iteration algorithm consists of
[143.36s -> 149.08s]  repeatedly applying the operator b to the vector v. The max comes from step 2
[149.08s -> 156.10s]  and the stuff inside the max comes from step 1. So the reward is a stacked
[156.10s -> 161.04s]  vector of rewards at all states fraction a, and T a is a matrix of
[161.04s -> 166.22s]  transitions for action a such that T aij is the probability that s prime equals
[166.22s -> 174.66s]  i given that s equals j and we took the action a. Now one interesting
[174.66s -> 179.94s]  property that we can show is that v star is a fixed point of b. What is v
[179.94s -> 185.28s]  star? V star is the value function for the optimal policy. So if we can get v
[185.28s -> 190.86s]  star then we will recover the optimal policy. V star is equal to the max
[190.86s -> 198.12s]  over a of RSA plus gamma times the expected value of v star s prime, right?
[198.12s -> 202.32s]  So if we find a value function, if we find a vector that satisfies this
[202.32s -> 206.68s]  equation, we found the optimal value function and if we use the arg max
[206.68s -> 210.08s]  policy with respect to that, we'll get the optimal policy, the policy that
[210.08s -> 216.60s]  maximizes total rewards. So that means that v star is equal to b times v star.
[216.62s -> 221.58s]  So v star is a fixed point of b. So that's very nice. If we find a fixed
[221.58s -> 227.74s]  point of b, then we'll have found the optimal value function. And furthermore,
[227.74s -> 232.02s]  it's actually possible to show that v star always exists, this fixed point
[232.02s -> 235.98s]  always exists, it's always unique, and it always corresponds to the optimal
[235.98s -> 241.06s]  policy. So the only question that we're left with is, does repeatedly applying
[241.06s -> 245.86s]  b to v actually find this fixed point? So it's a fixed point
[245.88s -> 249.48s]  iteration algorithm. Does the fixed point iteration algorithm converge? If it does
[249.48s -> 252.92s]  converge, it will converge to the optimal policy and it has a unique
[252.92s -> 259.08s]  solution. So will we reach it? So I won't go through the proof in detail in this
[259.08s -> 263.68s]  lecture, but the high-level sketch behind how we argue that value
[263.68s -> 270.92s]  iteration converges is by arguing that it's a contraction. So we can prove that
[270.92s -> 275.64s]  value iteration reaches b star because b is a contraction. What does it mean to
[275.66s -> 281.40s]  be a contraction? It means that if you have any two vectors, v and v bar, then
[281.40s -> 285.78s]  applying b to both v and v bar will bring those vectors closer together,
[285.78s -> 291.22s]  meaning that bv minus bv bar, their norm is less than or equal to
[291.22s -> 297.10s]  the norm of v minus v bar. In fact, it's a contraction by some coefficient.
[297.10s -> 302.52s]  That coefficient happens to be gamma. So not only is bv minus bv bar norm less
[302.52s -> 306.18s]  than or equal to v minus v bar norm, it's actually less than or equal to v
[306.18s -> 310.02s]  minus v bar norm times gamma. So you will contract and you'll actually
[310.02s -> 315.18s]  contract by some non-trivial amount, which means that v and v bar
[315.18s -> 320.16s]  will always get closer together as you apply b to them. Now the proof
[320.16s -> 323.88s]  that b is a contraction is not actually all that complicated, I just don't want to
[323.88s -> 328.34s]  go through it on this slide, but you can look it up as a standard kind of
[328.34s -> 332.40s]  textbook result. But just to very briefly explain why showing that it's a
[332.40s -> 339.86s]  contraction implies that value iteration converges, if you choose v
[339.86s -> 344.88s]  star as your v bar, you know that v star is a fixed point of b, so if you
[344.88s -> 349.20s]  substitute in v star for v bar, then you get the equation bv minus v star
[349.20s -> 353.94s]  norm is less than or equal to gamma times v minus v star norm, which means
[353.94s -> 361.74s]  that each time you apply b to v, you get closer to v star. So each time
[361.76s -> 365.66s]  you change your value function by applying the nonlinear
[365.66s -> 371.00s]  operator b, you get closer to your optimum v star. It's important to note
[371.00s -> 376.46s]  here that the norm under which the operator b is a contraction is the
[376.46s -> 382.34s]  infinity norm. So the infinity norm is basically the difference for the
[382.34s -> 386.40s]  largest entry. So the infinity normal vector is the value of the largest
[386.44s -> 393.60s]  entry in that vector. So the state at which v and v star disagree the
[393.60s -> 400.00s]  most will be, they will disagree less after you apply b. So infinity norm, and
[400.00s -> 405.20s]  this is important, this will come up shortly. All right, so regular value
[405.20s -> 408.84s]  iteration can be written extremely concisely as just repeatedly applying
[408.84s -> 416.60s]  this one step, v goes to bv. Now let's go to the fitted value iteration
[416.60s -> 420.84s]  algorithm. The fitted value iteration algorithm has another operation, it has
[420.84s -> 424.00s]  a step two where you have to perform the arg min with respect to phi.
[424.00s -> 429.28s]  How can we mathematically understand that second step? So the first step is
[429.28s -> 433.36s]  basically the Bellman backup, the second step trains a neural network.
[433.36s -> 439.40s]  What does this step actually do abstractly? Well, one of the ways you can think of
[439.40s -> 444.60s]  supervised learning is that you have some set of value functions that you
[444.60s -> 448.84s]  can represent. That set, if your value function is a neural network, it's actually a
[448.84s -> 453.34s]  continuous set that consists of all possible neural nets with your
[453.34s -> 457.48s]  particular architecture but with different weight values. So we'll denote
[457.48s -> 461.64s]  that set as a set omega. In supervised learning we sometimes refer to this as
[461.68s -> 467.36s]  the hypothesis set or the hypothesis space. Supervised learning consists of
[467.36s -> 472.00s]  finding an element in your hypothesis space that optimizes your objective, and
[472.00s -> 478.52s]  our objective is the squared difference between v5s and the target
[478.52s -> 484.48s]  value. Now what is our target value? Our target value is basically bv, right,
[484.48s -> 487.72s]  that's what we did in step one. Step one is basically doing bv, that's
[487.76s -> 492.80s]  literally the equation for bv. So you can think of the entire fitted value
[492.80s -> 496.48s]  iteration algorithm as repeatedly finding a new value function v'
[496.48s -> 503.88s]  which is the arg min inside the set omega of the squared difference between
[503.88s -> 513.84s]  v' and bv, where bv is your previous value function. Now this
[513.88s -> 519.56s]  procedure is itself actually also a contraction, right, so when you
[519.56s -> 524.36s]  perform this supervised learning you can think of it as a projection in the L2
[524.36s -> 529.28s]  norm. So you have your old v, you have your set of possible neural nets
[529.28s -> 533.56s]  represented by this line, so omega is basically all the points on that line,
[533.56s -> 537.68s]  the whole space is all possible value functions. Omega doesn't contain all
[537.68s -> 542.40s]  possible value functions, so omega restricts us to this line. When we
[542.40s -> 548.24s]  construct bv we might step off this line, so the point bv doesn't line the set
[548.24s -> 552.32s]  omega. When we perform supervised learning, when we perform step two of
[552.32s -> 555.96s]  fitted value iteration, what we're really doing is we're finding a point
[555.96s -> 561.04s]  in the set omega that is as close as possible to bv, and as close as
[561.04s -> 564.36s]  possible means that it's going to be at a right angle, so we'll project down
[564.36s -> 568.28s]  onto the set omega and it'll be a right angle projection, so that'll get
[568.28s -> 574.56s]  us v prime. So we can define this as a new operator, we can call this operator
[574.56s -> 580.84s]  pi for projection, and we're going to say that pi v is just the arg min
[580.84s -> 586.72s]  within the set omega of the subjective, and the subjective is just
[586.72s -> 596.12s]  the L2 norm. Now pi is a projection onto omega in terms of the L2 norm,
[596.32s -> 601.96s]  and pi is also a contraction, because if you project something on the L2 norm,
[601.96s -> 606.96s]  it gets closer. So the complete fitted value iteration can be written also in
[606.96s -> 612.64s]  one line as just v becomes pi bv. So first you take a Bellman backup on v,
[612.64s -> 617.84s]  then you project it, and then you get your new v. So that's our fitted value
[617.84s -> 622.44s]  iteration algorithm. B is a contraction with respect to the infinity norm, the
[622.48s -> 628.04s]  so-called max norm, so that's what we saw before. Pi is a contraction with
[628.04s -> 634.40s]  respect to the L2 norm with respect to Euclidean distance, so pi v minus pi v
[634.40s -> 638.36s]  bar squared is less than or equal to v minus v bar squared. So so far so
[638.36s -> 642.96s]  good, both of these operators are contractions. The reason, by the way, the
[642.96s -> 646.40s]  intuition behind why pi is a contraction is that if you have any two points in Euclidean
[646.40s -> 649.36s]  space and you project them on a line, they can only get closer if you show
[649.36s -> 653.84s]  that they can never get further. So that's why pi is a contraction.
[653.84s -> 660.44s]  Unfortunately, pi times b is not actually a contraction of any kind. This might at
[660.44s -> 663.60s]  first seem surprising, because they're both contractions individually, but
[663.60s -> 667.44s]  remember that they're contractions for different norms. B is a contraction in
[667.44s -> 672.00s]  the infinity norm, pi is a contraction in the L2 norm. It turns out if you put
[672.00s -> 674.52s]  those two together, you might actually end up with something that is not a
[674.52s -> 678.40s]  contraction under any norm. And this is not just a theoretical
[678.44s -> 683.04s]  idiosyncrasy. This actually happens in practice. So if you imagine that this is
[683.04s -> 688.08s]  your starting point, the yellow star is the optimal value function, and you take
[688.08s -> 694.32s]  a step, so your regular value iteration will gradually get closer and
[694.32s -> 700.00s]  closer to the star. If you have a projected value iteration algorithm, a fitted
[700.00s -> 702.92s]  value iteration algorithm, then you're going to restrict your value function
[702.92s -> 709.52s]  to this line each step of the way. So your Bellman backup, BV, will get you
[709.52s -> 714.00s]  closer to the star in terms of infinity norm, and then your projection
[714.00s -> 718.60s]  will move you back onto the line. And while both of those operations are
[718.60s -> 723.52s]  contractions, notice that V prime is now actually further from the star than
[723.52s -> 728.64s]  V is. And you can get these situations where each step of the way actually
[728.64s -> 732.32s]  gets you further and further from V star. And this is not just a
[732.72s -> 737.68s]  theoretical idiosyncrasy. This can actually happen in practice. So the sad
[737.68s -> 741.68s]  conclusions from all this are that value iteration does converge in a
[741.68s -> 746.88s]  tabular case. Fitted value iteration does not converge in general, and it
[746.88s -> 752.68s]  doesn't converge in general, and it often doesn't converge in practice. Now
[752.68s -> 756.80s]  what about fitted Q iteration? So far all of our talk has been about value
[756.80s -> 760.84s]  iteration. What about fitted Q iteration? It's actually exactly the same thing. So
[760.88s -> 764.56s]  in fitted Q iteration, you can also define operator B. It looks a little bit
[764.56s -> 771.56s]  different. Now it's R plus gamma T times max Q. So the max is now, you
[771.56s -> 776.48s]  know, at the target value, but same basic principle. So now the max is after
[776.48s -> 779.92s]  transition operator. That's the only difference. B is still a contraction in the
[779.92s -> 784.88s]  infinity norm. You can define an operator pi exactly the same way as the
[784.88s -> 789.32s]  operator that finds the arg min in your hypothesis class that minimizes
[789.36s -> 795.04s]  square difference. You can define fitted Q iteration as Q becomes pi b Q, just
[795.04s -> 799.16s]  like with value iteration. And just like before, B is a contraction in the
[799.16s -> 804.60s]  infinity norm, pi is a contraction in the L2 norm, and pi b is not a
[804.60s -> 808.24s]  contraction of any kind. This also applies to online Q learning and
[808.24s -> 812.36s]  basically any algorithm of this sort.
[813.48s -> 818.04s]  Now at this point, some of you might be looking at this thing and thinking
[818.08s -> 821.72s]  something is very contradictory here. Like we just talked about how
[821.72s -> 825.60s]  this algorithm doesn't converge, but at the core of this algorithm is something
[825.60s -> 829.08s]  that looks suspiciously like gradient descent. Like isn't this whole
[829.08s -> 831.60s]  process just doing regression on the target values? Don't we know that
[831.60s -> 838.00s]  regression converges? Isn't this just gradient descent? Well, the subtlety here
[838.00s -> 842.44s]  is that Q learning is not actually gradient descent. So Q learning is not
[842.48s -> 849.00s]  taking gradient steps on a well-defined objective. It's because the target values
[849.00s -> 853.12s]  in Q learning themselves depend on the Q values, and this is also true for Q
[853.12s -> 856.84s]  iteration, but you're not considering the gradient through those
[856.84s -> 860.72s]  target values. So the gradient that you're actually using is not the true
[860.72s -> 866.76s]  gradient of a well-defined function, and that's why it might not converge. Now
[866.76s -> 870.48s]  it's probably worth mentioning that you could turn this algorithm into a
[870.80s -> 873.12s]  gradient descent algorithm by actually computing the gradient through those
[873.12s -> 877.44s]  target values. They're non-differential because of the max, but there's some
[877.44s -> 880.92s]  technical ways to deal with that. The bigger problem is that the resulting
[880.92s -> 884.36s]  algorithm, which is called a residual algorithm, has very very poor numerical
[884.36s -> 890.00s]  properties and doesn't work very well in practice. In fact, even though this
[890.00s -> 893.00s]  kind of Q learning procedure that I described is not guaranteed to converge,
[893.00s -> 897.00s]  in practice it actually tends to work much much better than residual gradient,
[897.04s -> 901.64s]  which, as though guaranteed to converge, has extremely poor numerical properties.
[901.64s -> 906.84s]  Okay, so short version, Q learning and fitted Q iteration are not actually
[906.84s -> 910.20s]  doing gradient descent, and the update is not the gradient of any
[910.20s -> 914.76s]  well-defined function. There's also unfortunately another sad
[914.76s -> 917.88s]  corollary to all this, which is that our actual criticality that we
[917.88s -> 922.08s]  discussed before also is not guaranteed to converge under function approximation
[922.12s -> 927.52s]  for the same reason. So there we also do a Bellman backup when we use a
[927.52s -> 931.04s]  bootstrap update, and we do a projection when we update our value
[931.04s -> 937.32s]  function, and the concatenation of those is not a convergent operator. So
[937.32s -> 942.76s]  fitted bootstrap policy evaluation also doesn't converge. And by the way,
[942.76s -> 947.16s]  one aside about terminology, most of you probably already noticed this, but
[947.16s -> 951.24s]  when I use the term V pi, I'm referring to the value function for some policy
[951.24s -> 955.32s]  pi. This is what the critic does. When I use V star, this is the value
[955.32s -> 959.00s]  function for the optimal policy pi star, and this is what we're trying to find
[959.00s -> 964.88s]  in value iteration. Okay, so to review, we talked about some value
[964.88s -> 970.36s]  iteration theory. We discussed the operator for the backup, the operator
[970.36s -> 974.28s]  for the projection. This is a typo on the slide, they're not actually linear
[974.28s -> 978.12s]  operators, but they are operators. We talked about how the backup is a
[978.12s -> 984.08s]  contraction, and how tabular value iteration converges. We talked about
[984.08s -> 988.32s]  some convergence properties of function approximation, where the projection is
[988.32s -> 993.24s]  also a contraction, but because it's a contraction in a different norm, backup
[993.24s -> 997.48s]  followed by projection is not actually a contraction, and therefore fitted value
[997.48s -> 1001.60s]  iteration does not in general converge, and its implications for Q-learning are
[1001.60s -> 1005.66s]  that Q-learning fitted to Q-iteration etc. also do not converge when we use
[1005.66s -> 1010.42s]  neural nets when we have a projection operator. This might seem
[1010.42s -> 1013.94s]  somewhat somber and depressing. We will find out in the next lecture that in
[1013.94s -> 1017.30s]  practice we can actually make all of these algorithms work very well, but
[1017.30s -> 1021.94s]  their theoretical properties leave us with a lot to be desired.
