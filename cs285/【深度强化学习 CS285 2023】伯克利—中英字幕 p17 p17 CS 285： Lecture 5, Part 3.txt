# Detected language: en (p=1.00)

[0.00s -> 4.68s]  So in the next portion of today's lecture, we're going to talk about how we can
[4.68s -> 10.46s]  modify the policy gradient calculation to reduce its variance, and in this way
[10.46s -> 14.30s]  actually obtain a version of the policy gradient that can be used as a
[14.30s -> 18.82s]  practical reinforcement learning algorithm. The first trick that we'll
[18.82s -> 24.06s]  start with is going to exploit a property that is always true in our
[24.06s -> 30.50s]  universe, which is causality. Causality says that the policy at time t prime
[30.50s -> 36.30s]  can't affect the reward at another time step t if t is less than t prime. This is
[36.30s -> 39.58s]  another way of saying that what you do now is not going to change the reward
[39.58s -> 43.42s]  that you got in the past. Now it's important to note here that this is
[43.42s -> 48.94s]  not the same as the Markov property. The Markov property says that the state
[48.94s -> 53.34s]  in the future is independent of the state in the past given the present. The
[53.38s -> 57.34s]  Markov property is sometimes true, sometimes not true, depending on your
[57.34s -> 61.94s]  particular temporal process. Causality is always true. Causality just says that
[61.94s -> 66.10s]  rewards in the past are independent of decisions in the present. So this is
[66.10s -> 70.18s]  not really an assumption. This is always true for any process where time
[70.18s -> 74.26s]  flows forward. The only way this would not be true is if you had time travel
[74.26s -> 76.86s]  and you could take an action or travel back into the past and change
[76.86s -> 81.30s]  your action. But we're not allowed to do that. All right, so I'm going to
[81.34s -> 85.34s]  claim that the policy gradient that I've derived so far does not actually make
[85.34s -> 90.18s]  use of this assumption and that it can be modified to utilize this assumption and
[90.18s -> 96.14s]  thereby reduce variance. You can take a moment to think about where this
[96.14s -> 101.86s]  assumption might be introduced. The way that we're going to see this is we're
[101.86s -> 107.14s]  going to rewrite the policy gradient equation. I've not changed it
[107.14s -> 110.52s]  anyway, I've simply rewritten it, and what I've done here is I use the
[110.52s -> 115.84s]  distributive property to distribute the sum of rewards into the sum over grad
[115.84s -> 121.30s]  log pi's. So you can think of it as taking that first set of parentheses
[121.30s -> 125.94s]  over the sum of grad log pi's and taking the outer parentheses and wrapping
[125.94s -> 129.96s]  it around the rewards. So this gives me the sum over all of my samples from
[129.96s -> 135.44s]  i equals 1 to n times the sum over time steps from 1 to capital T of grad
[135.44s -> 141.32s]  log pi at that time step multiplied by another sum over another
[141.32s -> 146.72s]  variable t prime from 1 to capital T of the rewards. So that means that at
[146.72s -> 151.68s]  every time step I multiply the grad log probability of the action at that
[151.68s -> 158.04s]  time step t by the sum of rewards over all time steps in the past, present,
[158.04s -> 164.80s]  and future. Now at this point you might start imagining how causality fits
[164.80s -> 169.68s]  into this. We're going to change the log probability of the action at every time
[169.68s -> 176.04s]  step based on whether that action corresponded to larger rewards in the
[176.04s -> 180.96s]  present and in the future, but also in the past. And yet we know that the action
[180.96s -> 185.48s]  time step t can't affect the rewards in the past. So that means that those
[185.48s -> 189.82s]  other rewards will necessarily have to cancel out an expectation, meaning that
[189.82s -> 194.48s]  if we generate enough samples, eventually we should see that all the
[194.48s -> 200.62s]  rewards at time steps t prime less than t will average out to a multiplier of
[200.62s -> 205.98s]  zero, and they will not affect the log probability at this time step. In fact,
[205.98s -> 208.90s]  we can prove this is true. The proof is somewhat involved, so I won't go
[208.90s -> 213.66s]  through it here, but once we show that this is true, then we can simply
[213.66s -> 219.14s]  change the summation of rewards, and instead of summing from t prime equals
[219.14s -> 223.62s]  one to capital T, simply sum from t prime equals t to capital T. Basically
[223.62s -> 227.22s]  discard all the rewards in the past because we know that the current policy
[227.22s -> 232.78s]  can't affect them. Now we know that they'll all cancel out an expectation, but for a
[232.78s -> 236.38s]  finite sample size, they wouldn't actually cancel out. So for a finite
[236.38s -> 240.18s]  sample size, removing all those rewards from the past will actually change your
[240.18s -> 244.86s]  estimator, but it will still be unbiased. So this is the only change that we
[244.86s -> 250.86s]  made. Now, having made that change, we actually end up with an estimator that
[250.86s -> 255.18s]  has lower variance. The reason it has lower variance is very simple. We've
[255.18s -> 258.62s]  removed some of the terms from the sum, which means that the total sum is a
[258.62s -> 265.62s]  smaller number, and expectations of smaller numbers have smaller variances.
[266.62s -> 271.66s]  Now, one aside that I might mention here is that this quantity is sometimes
[271.70s -> 276.06s]  referred to as the reward-to-go. You can kind of guess why that is. It's the rewards from
[276.06s -> 279.50s]  now until the end of time, which means that it refers to the rewards that
[279.50s -> 282.58s]  you have yet to collect, basically all the rewards except for the ones in the
[282.58s -> 288.66s]  past, or the reward-to-go. And we sometimes use the symbol q hat i comma t
[288.66s -> 293.50s]  to denote the reward-to-go. Now take a moment to think back to the
[293.50s -> 301.58s]  previous lecture, where we also used the symbol q. The reward-to-go q hat here
[301.58s -> 305.66s]  actually refers to an estimate of the same quantity as the q function that we
[305.66s -> 310.02s]  saw in the previous lecture. We will get much more into this in the next lecture
[310.02s -> 314.18s]  when we talk about extra critical algorithms, but for now we'll just use a
[314.18s -> 320.62s]  similar symbol with a hat on top to denote that it's a single sample estimate.
[321.50s -> 326.62s]  All right. Now, the causality trick that I described before, you can always
[326.62s -> 330.62s]  use it. You'll use it in Homework 2. It reduces your variance. There's
[330.66s -> 333.90s]  another slightly more involved trick that we can use that also turns out to be
[333.90s -> 337.66s]  very important to make policy gradients practical, and it's something called a
[337.66s -> 344.62s]  baseline. So let's think back to this cartoon that we had where we collect
[344.62s -> 348.74s]  some trajectories and we evaluate the rewards, and then we try to make the
[348.74s -> 352.86s]  good ones more likely and the bad ones less likely. That seemed like a
[352.86s -> 357.98s]  very straightforward, elegant way to formalize trial and error learning as a
[358.02s -> 364.54s]  gradient ascent procedure. But is this actually what policy gradients do? Well,
[364.54s -> 370.38s]  intuitively, policy gradients will do this if the rewards are centered,
[370.38s -> 373.70s]  meaning that the good trajectories have positive rewards and the bad
[373.70s -> 378.06s]  trajectories have negative rewards. But this might not necessarily be true. What
[378.06s -> 381.92s]  if all of your rewards are positive? Then the green checkmark will be
[381.92s -> 385.50s]  increased, its probability will be increased, the yellow checkmark will
[385.54s -> 390.86s]  be increased a little bit, and the red X will be also increased but a tiny bit. So
[390.86s -> 394.18s]  intuitively, it kind of seems like what we want to do is we want to center
[394.18s -> 397.82s]  our rewards so the things that are better than average get increased and
[397.82s -> 401.78s]  the things that are worse than average get decreased. For example, maybe
[401.78s -> 406.34s]  we want to subtract a quantity from our reward, which is the average reward. So
[406.34s -> 412.50s]  instead of multiplying grad log P by R of tau, we multiply by R of tau minus B,
[412.50s -> 417.54s]  where B is the average reward. This would cause policy gradients to align with our
[417.54s -> 420.74s]  intuition. This would make policy gradients increase the probability of
[420.74s -> 424.66s]  trajectories that are better than average and decrease the probabilities
[424.66s -> 428.90s]  of trajectories that are worse than average. And then this would be true
[428.90s -> 431.82s]  regardless of what the reward function actually is, even if the rewards
[431.82s -> 438.02s]  are always positive. That seems very intuitive, but are we allowed to do
[438.02s -> 442.08s]  that? It seems like we just arbitrarily subtract our constant from all of our
[442.16s -> 449.80s]  rewards. Is this even correct still? Well, it turns out that you can show that
[449.80s -> 454.28s]  subtracting a constant B from your rewards in policy gradient will not
[454.28s -> 458.32s]  actually change the gradient in expectation, although it will change its
[458.32s -> 464.32s]  variance, meaning that for any B, doing this trick will keep your gradient
[464.32s -> 470.16s]  estimator unbiased. Here's how we can derive this. So we're going to use the
[470.16s -> 474.64s]  same convenient identity from before, which is that P of tau times grad log P
[474.64s -> 479.32s]  of tau is equal to grad P of tau. And now we're going to substitute this
[479.32s -> 483.92s]  identity in the opposite direction. So what we're going to do is we're
[483.92s -> 489.60s]  going to analyze grad log P of tau times B. So if I take the difference R
[489.60s -> 494.52s]  of tau minus B, and I distribute grad log P into it, then I get a grad log P
[494.52s -> 498.72s]  times R term, which is my original policy gradient, minus a grad log P
[498.72s -> 504.16s]  times B term, which is the new term that I'm adding. So let's analyze just that term.
[504.16s -> 508.60s]  It's the expected value of grad log P times B, which means that it's the
[508.60s -> 513.92s]  integral of P of tau times grad log P of tau times B. And now I'm going to
[513.92s -> 518.20s]  substitute my identity back in. So using the convenient identity in the blue
[518.20s -> 524.84s]  box over there, I know this is equal to the integral of grad P of tau times B.
[524.84s -> 529.20s]  Now by linearity of the gradient operator, I can take both the gradient
[529.20s -> 533.96s]  operator and B outside the integral. So this is equal to B times the gradient of
[533.96s -> 539.56s]  the integral over tau of P of tau. But P of tau is a probability distribution, and we
[539.56s -> 544.60s]  know that probability distributions integrate to 1, which means that this is equal to B
[544.60s -> 549.48s]  times the gradient with respect to theta of 1. But the gradient with
[549.48s -> 554.26s]  respect to theta of 1 is 0, because 1 doesn't depend on theta. Therefore we
[554.30s -> 559.70s]  know that this expected value comes out equal to 0 in expectation. But for a
[559.70s -> 563.50s]  finite number of samples, it's not equal to 0. So what this means is that
[563.50s -> 568.34s]  subtracting B will remain, will keep our policy gradient unbiased, but it will
[568.34s -> 573.38s]  actually alter its variance. So subtracting a baseline is unbiased in
[573.38s -> 579.42s]  expectation. The average reward, which is what I'm using here, turns out to not
[579.42s -> 583.50s]  actually be the best baseline, but it's actually pretty good. And in many cases
[583.62s -> 587.10s]  when we just need a quick and dirty baseline, we'll use average reward.
[587.10s -> 592.26s]  However, we can actually derive the optimal baseline. The optimal baseline
[592.26s -> 596.38s]  is not used very much in practical policy gradient algorithms, but it's
[596.38s -> 599.58s]  perhaps instructive to derive it just to understand some of the mathematical
[599.58s -> 604.54s]  tools that go into studying variance. So that's what we're going to do in the
[604.54s -> 607.46s]  next portion. In the next portion, we'll go through a mathematical calculation
[607.46s -> 612.54s]  where we'll actually derive the expression for the optimal baseline to
[612.58s -> 617.54s]  optimally minimize variance. So to start with, we're going to write down variance.
[617.54s -> 621.26s]  So if you have the variance of some random variable X, it's equal to the
[621.26s -> 629.02s]  expected value of X squared minus the expected value of X squared. So we can
[629.02s -> 633.98s]  use the same equation to write down the variance of our policy gradient. So
[633.98s -> 639.02s]  here's our policy gradient. The variance of the policy gradient is equal to
[639.02s -> 644.30s]  the expected value of the quantity inside the bracket squared minus the
[644.30s -> 652.78s]  whole expected value squared. Now the second term here is just the
[652.78s -> 658.34s]  policy gradient itself, right, because we know that R of tau minus B in
[658.34s -> 662.78s]  expectation ends up not making a difference. So basically the actual
[662.78s -> 666.50s]  expected value of grad log P times R minus B is the same as the expected
[666.50s -> 671.74s]  value of grad log P times R. So we can just forget about the second term,
[671.74s -> 677.90s]  changing R is not going to change its value in expectation. So it's really
[677.90s -> 684.30s]  only the first term that we care about. All right, I'm going to change my
[684.30s -> 688.02s]  notation a little bit just to declutter it, so I'll just use g of
[688.02s -> 694.26s]  tau in place of grad log P of tau. So if you see g at the bottom,
[694.26s -> 700.10s]  that's just grad log P. I just wanted to write a shorter value. So I
[700.10s -> 703.78s]  know that the second term in the variance doesn't depend on B, but the
[703.78s -> 708.46s]  first term does. So then in order to find the optimal B, I'm going to write
[708.46s -> 713.42s]  down the derivative d-var d-B and solve for the best B. So the derivative of the
[713.42s -> 716.94s]  second part is 0 because it doesn't depend on B. So I just use the first
[716.94s -> 725.14s]  part, d-db of the expected value of g squared times R minus B squared. Now I
[725.14s -> 730.90s]  can expand out the quadratic form and I get d-db of the expected value of g
[730.90s -> 736.78s]  squared R squared minus 2 times the expected value of g squared RB plus B
[736.78s -> 740.14s]  squared times the expected value of g squared. So all I've done here is I've
[740.14s -> 745.98s]  just expanded out the quadratic form R minus B squared, distributed the g
[746.02s -> 751.50s]  squared into it, and then pulled constants out of expectations. Now looking
[751.50s -> 755.42s]  at this equation, we can see the first term doesn't depend on B, but the
[755.42s -> 760.62s]  second two terms do. So we can eliminate this part. And the second two
[760.62s -> 766.86s]  terms, if we take their derivative with respect to B, the minus 2 term is linear in B, and
[766.86s -> 771.50s]  the plus term is quadratic in it, so we get the derivative is equal to
[771.50s -> 777.26s]  negative 2 times the expected value of g squared R plus 2B times the expected
[777.26s -> 783.14s]  value of g squared. Now we can push the constant term on the right-hand side
[783.14s -> 787.34s]  and solve for B, and we get this equation. B is equal to the expected
[787.34s -> 792.98s]  value of g squared R divided by the expected value of g squared. Right, so
[792.98s -> 797.22s]  I've just solved for B when the derivative is equal to 0. So this is
[797.22s -> 803.38s]  the optimal value of B. Now looking at this thing, you could try to imagine
[803.38s -> 809.46s]  what is the optimal baseline, really, intuitively. Well, perhaps one thing that
[809.46s -> 812.30s]  might jump out at you is that the baseline now actually depends on the
[812.30s -> 815.64s]  gradient, which means that if the gradient is a vector with multiple
[815.64s -> 818.74s]  dimensions, if you have multiple parameters, you'll actually have a different
[818.74s -> 823.42s]  baseline for every entry in the gradient. So if you have a hundred
[823.42s -> 826.62s]  different policy parameters, you'll have one value of the baseline for
[826.62s -> 831.14s]  parameter 1, a different value of the baseline for parameter 2. And intuitively,
[831.14s -> 835.82s]  looking at this equation, the baseline for each parameter value is basically
[835.82s -> 840.94s]  the expected value of the reward weighted by the magnitude of the gradient
[840.94s -> 846.78s]  for that parameter value. So it's a kind of re-weighted version of the
[846.78s -> 849.86s]  expected reward. It's not the average reward anymore, it's a re-weighted
[849.86s -> 854.74s]  version of it. It's re-weighted by gradient magnitudes. So this is the
[854.78s -> 858.70s]  baseline that minimizes the variance. Now again, in practice, we often don't use the
[858.70s -> 862.92s]  optimal variance, we just, sorry, we often don't use the optimal baseline, we
[862.92s -> 866.62s]  typically just use the expected reward. But if you wanted the optimal
[866.62s -> 871.24s]  baseline, this is how you would get it. All right, so to review what we've covered so
[871.24s -> 875.90s]  far, we talked about the high variance of policy gradient algorithms, we talked
[875.90s -> 879.90s]  about how we can lower that variance by exploiting the fact that present
[879.90s -> 883.98s]  actions don't affect past rewards, and we talked about how we can use
[883.98s -> 888.62s]  baselines, which are also unbiased, and we can analyze variance to solve for
[888.62s -> 891.50s]  the optimal baseline.
