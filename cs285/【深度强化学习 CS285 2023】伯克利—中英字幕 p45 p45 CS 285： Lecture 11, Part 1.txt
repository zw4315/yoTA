# Detected language: en (p=1.00)

[0.88s -> 6.32s]  Welcome to lecture 11 for CS285. In today's lecture, we're going to talk about model-based
[6.32s -> 13.76s]  reinforcement learning algorithms. So first, we'll discuss the basics of model-based RL,
[13.76s -> 18.00s]  where we learn a model and then use this model for control. We'll talk about a naive
[18.00s -> 22.00s]  way of approaching this problem, discuss a few candidate algorithms, and then talk about some
[22.00s -> 26.48s]  problems with these algorithms. We'll talk about the effect of distributional shift in model-based
[26.48s -> 32.08s]  RL, and then we'll discuss uncertainty in model-based RL and how being aware of uncertainty
[32.08s -> 34.72s]  can make a really big difference in the performance of algorithms.
[35.92s -> 39.76s]  Then we'll conclude with a discussion of model-based RL with complex observations.
[41.20s -> 46.08s]  And then next time, we'll discuss how we can use model-based RL to learn policies.
[46.08s -> 52.40s]  So all the algorithms we'll discuss in today's lecture learn only a model, and then they
[52.40s -> 57.28s]  use algorithms such as the ones in the previous lecture to plan through that model, whereas
[57.28s -> 60.16s]  next time we'll talk about how we could also use models to learn policies.
[61.20s -> 65.28s]  So the goals for today's lecture will be to understand how to build model-based RL
[65.28s -> 71.36s]  algorithms, understand the important considerations for model-based RL, and understand the trade-offs
[71.36s -> 77.76s]  between different model classes. So why should we learn the model? Well, if we had some
[77.76s -> 83.52s]  estimate of the dynamics, for example, a function f of s t comma a t that returns s t plus one,
[83.52s -> 88.56s]  then we could simply use all those tools from last week to control our system instead
[88.56s -> 94.08s]  of having to deal with model-free RL algorithms. In the stochastic case, we would learn a
[94.08s -> 99.68s]  stochastic model of the form p of s t plus one given s t a t. For most of the algorithms
[99.68s -> 104.00s]  that I'll discuss in today's lecture, I'll present them as deterministic model algorithms
[104.00s -> 110.08s]  of the form f of s t comma a t equals s t plus one, but almost all those ideas can just as well
[110.08s -> 115.04s]  be used with probabilistic models to learn a distribution over the next state, and when
[115.04s -> 120.40s]  this distinction is salient, I'll make it explicit. So let's think about how we can learn
[120.40s -> 126.48s]  f of s t comma a t from data and then plan through it to select our actions. We could imagine
[126.48s -> 133.60s]  a very simple model-based RL algorithm prototype. I'm going to call it version 0.5. It's not
[133.60s -> 137.84s]  quite version 1.0. It's not quite the thing you'd want to use, but it's perhaps the most
[137.84s -> 143.60s]  obvious thing we can start thinking about. In this model-based RL version 0.5 algorithm,
[143.60s -> 148.00s]  step one would be to run some basic exploration policy to collect some data,
[148.00s -> 151.76s]  and this exploration policy could just be a completely random policy, so it's not a neural
[151.76s -> 156.56s]  network or anything like that. It just selects actions uniformly at random and collects a data
[156.56s -> 163.28s]  set of transitions. So here a transition is a tuple s comma a comma s prime. We saw state s,
[163.28s -> 167.92s]  we took action a, and we arrived at state s prime, and that got us our transition, and we'll
[167.92s -> 173.52s]  have a data set of these transitions. And we can use this data set to then train our model
[173.52s -> 180.40s]  with supervised learning. So we'll learn a dynamics model f of s comma a that minimizes the average
[180.40s -> 187.20s]  over all the points in our data set of the difference between f s i a i and s i prime.
[188.16s -> 191.60s]  And if your states are discrete, then maybe you'd use something like cross-entropy loss.
[191.60s -> 195.92s]  If your states are continuous, you could use something like a squared error loss. Most
[195.92s -> 201.28s]  generally you would use a negative log-likelihood loss, of which squared error is a particular
[201.28s -> 207.60s]  special case for our Gaussian likelihood. And then once you've trained your dynamics model,
[207.68s -> 212.72s]  you would use your model to go and select actions using, for example, any of the algorithms
[212.72s -> 222.00s]  that we covered last week. So does this basic recipe work? Well, in a sense yes. So in some
[222.00s -> 226.88s]  cases this basic recipe will work very well. In fact, there are many previously proposed methods
[226.88s -> 231.68s]  that have utilized this recipe. This is essentially how system identification works in
[231.68s -> 236.56s]  classical robotics. So if you have a robotics background, if you've heard the term system
[236.56s -> 243.12s]  identification, system identification basically refers to the problem of taking some data and
[243.12s -> 248.72s]  using that data to identify the unknown parameters in a dynamics model. Now typically the parameters
[248.72s -> 253.20s]  that are being identified in this way are not something like the weights in a neural net. They
[253.20s -> 258.16s]  might be the unknown parameters of a known physics model. So maybe you have the equations
[258.16s -> 262.56s]  of motion of your robot, but you don't know the masses or the friction coefficients of
[262.56s -> 267.52s]  different parts and you would identify those. This is why this is referred to a system identification
[267.52s -> 271.84s]  instead of system learning. So you really know a lot already about your system by just
[271.84s -> 279.36s]  identifying a few unknowns. When you use this kind of procedure, you do need to take some care
[279.36s -> 285.28s]  in designing a good base policy, because that good base policy needs to explore the different
[285.28s -> 289.12s]  modes of your system. So if your system can react in different ways to different states,
[289.12s -> 294.64s]  you really need to see representative examples of all the states that elicit different reactions.
[294.64s -> 298.80s]  So if there's some large region that you haven't visited, perhaps you will get an erroneous
[298.80s -> 301.76s]  identification of your parameters that will not model that region one.
[303.84s -> 308.00s]  But this kind of approach can be particularly effective if you can hand engineer a dynamics
[308.00s -> 312.08s]  representation, maybe using your knowledge of physics or using your knowledge of the system,
[313.28s -> 315.44s]  and then fit a relatively modest number of parameters.
[316.40s -> 323.28s]  In general, however, this approach doesn't really work with large high-capacity models
[323.28s -> 328.80s]  like deep neural networks. And to understand why that's the case, let's consider a simple example.
[328.80s -> 333.36s]  Let's say that I'm trying to walk around on this mountain and I'd like to get to the top of
[333.36s -> 338.72s]  the mountain. So my procedure will be to first run a base policy pi zero, like a random policy,
[338.72s -> 342.72s]  to collect my data set. So maybe I do essentially a random walk on the mountain.
[342.96s -> 348.96s]  And I'm going to use the result of this random walk to learn my dynamics model. So this is my
[348.96s -> 354.08s]  policy pi zero, maybe just a random policy, that produced some data, and I'm going to use it to
[354.08s -> 358.96s]  learn f. And I want to get to the highest point on the mountain, so I'm going to ask f to predict
[358.96s -> 362.64s]  how high will I be if I take certain actions, and then I'll plan through that.
[363.68s -> 368.64s]  Now for this part of the mountain that I walked on, it seems like going further to the right
[369.44s -> 374.64s]  gets me higher up. So from this random data that I got from this pi zero policy, my model will
[374.64s -> 380.56s]  probably figure out that the more right you go, the higher your altitude will be, which is a very
[380.56s -> 385.04s]  reasonable inference based on that data. So when I then plan through that model to choose my
[385.04s -> 391.36s]  actions, well what do you think is going to happen? Right, so I'm going to be in for a bad time.
[392.32s -> 397.04s]  Right, so I'm going to be in for a bad time. I'm going to be in for a bad time
[398.00s -> 405.20s]  for a reason that we've actually seen before. So the data that I use to train my model
[406.00s -> 411.12s]  comes from the distribution over states induced by the policy pi zero. We can call this p pi zero
[411.12s -> 420.96s]  st. Take a moment to think about why using a model trained on p pi zero st can lead to
[420.96s -> 428.48s]  some really bad outcomes when we then use that model to plan. As a hint, the answer to this
[428.48s -> 431.92s]  is something that we've seen before in several of our previous lectures.
[435.92s -> 439.60s]  So the issue is basically the following. The issue is that when we
[439.60s -> 444.24s]  plan through our model, you can think of that planning as executing another policy.
[444.96s -> 450.64s]  We can call it pi f, because f is the model and pi f is the policy induced by that model.
[451.04s -> 455.44s]  Pi f is not a neural net, pi f is just a planning algorithm run on top of the model f,
[455.44s -> 461.84s]  so pi f is fully determined by f. And it has its own state distribution. Its state distribution
[461.84s -> 468.08s]  is p pi f of st, and in this case that distribution involves going very far to the
[468.08s -> 475.68s]  right and falling off the mountain. The reason that the problem happens is because p pi f of st
[475.68s -> 482.08s]  is not equal to p pi zero of st. So we are experiencing distributional shift.
[483.44s -> 488.64s]  And the way the problems of the distribution shift manifest themselves is that our model is valid
[489.52s -> 495.04s]  for estimating the outcomes of actions in the region that was visited during data collection,
[495.60s -> 501.20s]  meaning for states with high probability p pi zero of st. But when we plan under that model,
[501.20s -> 505.36s]  when we select the actions for which the model produces states of the highest reward,
[505.36s -> 512.96s]  the ones that go the most up, we will end up going to states during our planning process virtually
[513.52s -> 518.80s]  that have very low probability under p pi zero of st, and our model will make erroneous
[518.80s -> 522.88s]  predictions at those states. And when it makes erroneous predictions at those states,
[523.68s -> 528.16s]  then it will choose the best action in that erroneously predicted state,
[529.44s -> 533.76s]  feed that back into itself, and then make an even more erroneous prediction about the
[533.76s -> 541.44s]  following state. This is basically exactly the phenomena that we saw before when we talked about
[541.44s -> 547.36s]  imitation learning. And in fact, the two have a very close duality because you can think of
[547.36s -> 552.16s]  the full trajectory distribution as just the product of policy times dynamics times policy
[552.16s -> 558.32s]  times dynamics times policy, etc., etc. So if you can experience distributional shift
[558.32s -> 562.48s]  by messing with the policy, you can of course also experience distributional shift
[562.48s -> 563.52s]  by messing with the model.
[566.64s -> 572.40s]  So distribution mismatch becomes exacerbated as we use more expressive model classes,
[572.40s -> 578.08s]  because more expressive model classes will fit more tightly to the particular distribution
[578.08s -> 582.32s]  seen in the training data. In the system identification example in the previous slide,
[582.96s -> 588.64s]  if we have, let's say, an airplane and we're fitting, you know, three numbers like some
[588.64s -> 594.64s]  drag coefficient, lift coefficient, and so on, yeah, we can technically overfit to a narrow
[594.64s -> 597.84s]  region of training data, but there's only three numbers and there's only so many ways
[597.84s -> 603.28s]  that those numbers can be chosen to fit the training data. So if our model, if the true model
[603.28s -> 606.64s]  is in the class of our learned models, then we'll probably get those three parameters right.
[606.64s -> 611.60s]  And that's why system identification basically works in robotics. But when we use high capacity
[611.60s -> 616.32s]  model classes like deep neural networks, then this distributional shift becomes a really major
[616.32s -> 620.48s]  problem and we have to do something to address it, otherwise we fall off the mountain.
[622.32s -> 627.84s]  So can we do better? Well, take a moment to imagine how we could modify
[627.84s -> 633.44s]  the model-based RL algorithm version 0.5 to mitigate this distributional shift problem.
[637.12s -> 643.12s]  So one way we can do this is we can borrow a very similar idea to what we had before
[643.12s -> 648.16s]  when we talked about Dagger. So in Dagger we also posed this question,
[648.16s -> 652.88s]  can we make the state distribution of one policy equal to the state distribution of another policy?
[653.68s -> 658.56s]  And the way we answered that question for Dagger was by collecting additional data
[658.56s -> 664.00s]  and requesting ground truth labels for that data. Now with Dagger this was quite difficult
[664.00s -> 668.80s]  because getting ground truth labels required asking a human expert to tell us what the
[668.80s -> 674.48s]  optimal action was. In model-based RL, this turns out to actually be a lot easier.
[674.48s -> 679.20s]  In model-based RL, you don't need to ask any human expert what the right next state is,
[679.20s -> 682.40s]  you can simply take a particular action, a particular state,
[682.40s -> 686.16s]  and observe the next state in nature, which means collect more data.
[688.80s -> 694.88s]  So from this we can get kind of model-based RL version 1.0. The reason I call this 1.0 is
[694.88s -> 702.24s]  because arguably this is kind of the simplest model-based RL method, which does work in general,
[702.24s -> 706.32s]  at least conceptually, although there are a lot of asterisks attached to that statement
[706.32s -> 710.40s]  in terms of how to implement it so that it works well. So the procedure goes like this.
[711.04s -> 716.80s]  Number one, run your base policy to collect data just like before. Number two,
[717.44s -> 723.92s]  learn your dynamics model from data just like before. Number three, plan through your dynamics
[723.92s -> 731.04s]  model, choose actions. And number four, execute those actions and add the resulting data to your
[731.04s -> 737.36s]  data set, and then go to step two again. So the main loop consists of training the dynamics
[737.36s -> 743.44s]  model on all the data so far, planning through it to collect more data, appending that data to
[743.44s -> 750.80s]  your data set, and retraining. It's essentially just like Dagger only for models, although
[750.80s -> 755.44s]  the statement is a little bit of an anachronism because this procedure actually
[755.44s -> 760.00s]  existed in the literature long before Dagger did, but we presented it in the opposite order in
[760.00s -> 766.16s]  this class, so you can think of it as Dagger for model-based RL. So this recipe does in principle
[766.16s -> 770.88s]  work. It actually does mitigate distributional shift, and in principle you should get a model that
[770.88s -> 776.96s]  works well. So where have we seen that before? Well, this is just Dagger for models.
[777.92s -> 782.24s]  Okay, so at this point you have version 1.0, which is a viable algorithm, and you could actually use
[782.24s -> 788.80s]  that algorithm, but we can still do better. So first let's ask this question. What if we made a
[788.80s -> 795.60s]  mistake? Right, so with falling off a cliff, it's rather sudden, so if you fall off a cliff,
[795.60s -> 798.72s]  you realize you've made a mistake, but at that point it's too late, and there's nothing more
[798.72s -> 804.32s]  that you could do. But many real problems are not like that. Let's say that you're driving a
[804.80s -> 811.36s]  car, and your model is a little bit erroneous. So your model predicts that you'll go straight,
[811.36s -> 815.12s]  not if your steering wheel is pointed straight, but your steering wheel is a little bit to the
[815.12s -> 818.72s]  left. So the model is just a tiny bit off. It says, well, if you steer like two degrees to
[818.72s -> 823.60s]  the left, then you'll go straight. A fairly innocent mistake in a complex dynamical system.
[824.40s -> 828.24s]  So if you do that, then when you actually execute your model, you'll go a little bit to
[828.24s -> 831.68s]  the left instead of going straight, and then you'll go a little bit to the left again and
[831.68s -> 836.72s]  again and again. Now as you collect more data, that iterative data collection should fix this
[836.72s -> 843.76s]  problem. So asymptotically, this method will still do the right thing. However, we can do
[843.76s -> 848.72s]  better and we can get it to learn faster by fixing the mistakes immediately as they happen,
[848.72s -> 853.28s]  instead of waiting for the whole model to get updated. So what we can do is we can
[854.08s -> 859.28s]  replant our actions at exactly the moment when you've made the mistake and perhaps correct it.
[859.84s -> 864.72s]  So the way that you can do better is look at the state that actually resulted from taking that
[864.72s -> 869.20s]  action and then ask your model what action you should take in this new state instead of
[869.20s -> 874.48s]  continuing to execute your plan. And this is what we're going to call model-based reinforcement
[874.48s -> 879.84s]  learning version 1.5, which is also in the literature often called model predictive control
[879.84s -> 886.24s]  or MPC, which I alluded to briefly at the end of the previous lecture. So the idea here
[887.12s -> 892.56s]  is just like before, run your base policy, train your dynamics model, plan through your dynamics
[892.56s -> 897.44s]  model to choose a sequence of actions, but then execute only the first planned action,
[898.16s -> 903.92s]  observe the actual real estate that results from taking that action, and then plan again.
[905.04s -> 908.08s]  So you append that transition to your data set and then,
[908.88s -> 912.16s]  instead of waiting for that whole sequence of actions to finish, you immediately replan.
[912.16s -> 917.04s]  Now this is much more computationally expensive because you have to repeat the planning every
[917.04s -> 924.24s]  single time step, but with this procedure you can do much better with a much worse model,
[924.24s -> 929.44s]  because your model maybe doesn't realize that, you know, steering a little to the left won't cause
[929.44s -> 933.76s]  it to go straight. Once it's actually gone to the left, at that point the mistake is so big
[933.76s -> 937.20s]  that it can probably figure out that, okay, now I really need to steer to the right to get out
[937.20s -> 942.16s]  of here. And this kind of model predictive control procedure can be much more robust
[942.72s -> 947.76s]  to mistakes in the models than the naive 1.0 procedure that I showed on the previous slide.
[949.36s -> 953.92s]  And then of course every n outer loop steps you repeat this whole process
[953.92s -> 959.44s]  and retrain your model. And n here might be some multiple of the length of your trajectory.
[960.32s -> 967.12s]  So in practice this version 1.5 procedure pretty much always works better with the
[967.12s -> 971.36s]  main consideration being that it's considerably more computationally expensive.
[973.60s -> 976.16s]  So replying basically helps avoid model errors.
[979.28s -> 983.52s]  And your homework 4 assignment will essentially involve implementing an algorithm
[983.52s -> 987.76s]  that basically does this. So if this procedure is not clear to you,
[988.08s -> 992.08s]  please do make sure to ask a question in the comments and come to the class discussion
[992.08s -> 997.12s]  and ask ask about clarifications because it's very important to get this particular procedure
[997.12s -> 1004.64s]  correct. Okay, so now one question we could ask is, well, how should we replan?
[1005.76s -> 1012.24s]  So how do we plan through f of a set of choose actions? The intuition is that the more you
[1012.24s -> 1017.92s]  replan the less perfect each individual plan needs to be. So while the computational cost of
[1017.92s -> 1022.80s]  replanning might be very high, in practice since you're going to be replanning and fixing
[1022.80s -> 1028.16s]  your mistakes you can kind of afford to make more mistakes during this replanning process.
[1028.16s -> 1032.48s]  So a very common thing that people do when they actually implement this procedure is they
[1032.48s -> 1037.36s]  use much shorter horizons for step three than they would if they were making a standard open
[1037.36s -> 1044.64s]  loop plan and they just rely on the replanning to fix those mistakes. So even things like random
[1044.64s -> 1048.88s]  sampling can often work well here, random shooting, whereas they might not work well for
[1048.88s -> 1053.84s]  constructing a long open loop plan. And if you remember the demonstration that was shown
[1053.84s -> 1059.04s]  at the end of class last week, this was illustrating MPC with replanning using actually
[1059.04s -> 1063.92s]  a fairly short horizon and really relying almost entirely on that replanning to fix up the mistakes.
