# Detected language: en (p=1.00)

[0.00s -> 5.24s]  Alright, now that we've talked about policy evaluation and how value functions
[5.24s -> 8.76s]  can be incorporated into the policy gradient, let's put these pieces together
[8.76s -> 14.24s]  and construct an actor-critic reinforcement learning algorithm. So a
[14.24s -> 19.20s]  basic batch actor-critic algorithm can look something like this. This is a
[19.20s -> 24.32s]  kind of based on the reinforce procedure before with some additional
[24.32s -> 29.04s]  steps added. So step one, just like before, is going to be to generate
[29.08s -> 32.76s]  samples by running rollouts through our policy. That's basically the orange box,
[32.76s -> 38.96s]  and that remains essentially unchanged. Step two is to fit our approximate value
[38.96s -> 43.74s]  function to those sampled rewards, and that's what's going to replace the green
[43.74s -> 47.38s]  box. So instead of just naively summing up all the rewards, we're not going to
[47.38s -> 52.24s]  fit a neural network, as we discussed in the previous section. Step three, for
[52.24s -> 55.72s]  every state action tuple that we sampled, evaluate the approximate
[55.72s -> 60.12s]  advantage as the reward plus the approximate value at the next state
[60.12s -> 65.92s]  minus the value at the current state. Step four, use these advantage values to
[65.92s -> 70.20s]  construct a policy gradient estimator by taking grad log pi at every time
[70.20s -> 74.40s]  step and multiplying it by the approximate advantage. And then step five,
[74.40s -> 83.28s]  like before, is to take a gradient descent step. So the part that we
[83.32s -> 86.56s]  talked about when we discussed policy evaluation is mostly the step to how do
[86.56s -> 90.28s]  we actually fit the value function, and we talked about how we can make a
[90.28s -> 94.68s]  number of different choices. We could fit it to single sample Monte Carlo
[94.68s -> 97.72s]  estimates, meaning that we actually sum up the rewards that we got along that
[97.72s -> 102.76s]  trajectory, and that gives us our target values. We also talked about how
[102.76s -> 106.40s]  we could use bootstrap estimates where we use the actual observed reward
[106.40s -> 111.44s]  plus the estimated value at the next state by using our previous value
[111.44s -> 116.00s]  function estimator, and these give us a few different options for how to fit the
[116.00s -> 122.48s]  critic. Now at this point I want to make a little aside to discuss what
[122.48s -> 128.52s]  happens when we fit value functions with this bootstrap rule in
[128.52s -> 134.96s]  infinite horizon settings. So the trouble that we might get into is if the
[134.96s -> 139.84s]  episode length is infinite, then each time we apply this bootstrap rule, our
[139.88s -> 145.72s]  value function will increase. So if we have, for example, an episodic task that
[145.72s -> 148.48s]  ends at a fixed time, maybe this is not such a big issue, perhaps we could
[148.48s -> 152.08s]  have a different value function for every time step, and everything is
[152.08s -> 155.68s]  finite horizon. Episodic tasks are fairly common in some settings, like
[155.68s -> 159.92s]  this robotics task here, but we could have an infinite horizon, a continuous or
[159.92s -> 164.28s]  cyclic task, like this running task, or we might simply want to use the
[164.28s -> 169.32s]  same value function for all time steps. In these cases, using a bootstrap rule
[169.32s -> 173.36s]  the way that I discussed is liable to lead to some problems. For example, if
[173.36s -> 177.08s]  the rewards are always positive, each time you bootstrap in this way, your
[177.08s -> 180.52s]  value function increases, and eventually your value function might become
[180.52s -> 186.44s]  infinite. So how can we modify this rule to ensure that we can always have
[186.44s -> 192.44s]  finite values and that we can handle infinite horizon settings? Well, one very
[192.44s -> 196.92s]  simple trick is to assume that we want a larger reward sooner rather than
[196.92s -> 202.08s]  later. This is very natural. If you imagine that I were to tell you that
[202.08s -> 206.40s]  I'll give you $100, you might be quite pleased about that. If I tell you that
[206.40s -> 210.08s]  I'll give you $100 next year, you'll probably still be somewhat pleased, but
[210.08s -> 214.86s]  less so. If I tell you that I'll give you $100 in a million years, you
[214.86s -> 220.28s]  probably won't take me very seriously. Why? Well, because it matters a lot less
[220.28s -> 223.72s]  to you what will happen in one year, and significantly less what will happen in a
[223.76s -> 227.48s]  million years, simply because there's so much uncertainty about what will happen
[227.48s -> 231.30s]  to everybody, including you, in that amount of time that those delayed
[231.30s -> 236.08s]  rewards just don't have as much value. Another way of thinking about it
[236.08s -> 241.12s]  is that you'd prefer a reward sooner rather than later for a very basic
[241.12s -> 246.52s]  biological reason, which is that someday you're going to die, and you'd
[246.52s -> 250.00s]  like to receive the reward before you die. And if I tell you get the reward in
[250.00s -> 253.66s]  a million years, then it's very unlikely that you'll get the reward before you
[253.66s -> 259.54s]  die. That might sound kind of grim, but we can use this cute metaphor to
[259.54s -> 263.38s]  actually construct a solution to this infinite reward problem. We can favor
[263.38s -> 267.62s]  rewards sooner rather than later by actually modeling the fact that the
[267.62s -> 274.10s]  agent might quote-unquote die. So the way that we're going to do this is we
[274.10s -> 280.02s]  will introduce a little multiplier in front of the value. So instead of
[280.02s -> 284.34s]  setting the target value to be R plus the next V, we'll set it to be R plus
[284.34s -> 289.82s]  the next V times gamma, where gamma is what we call a discount factor. It's a
[289.82s -> 295.14s]  number between 0 and 1. 0.99 works really well if you want an example of
[295.14s -> 298.82s]  a discount factor. Generally we choose them to be somewhere between like 0.9
[298.82s -> 306.82s]  and 0.999. And one way that you can interpret the role of gamma is that
[306.82s -> 312.02s]  gamma kind of changes your MDP. So let's say that you have this MDP, where you
[312.02s -> 314.54s]  have four states and you can transition between those four states,
[314.54s -> 319.30s]  and those transitions are governed by some probability distribution P of S
[319.30s -> 324.38s]  prime given SA. When we add gamma, one way we can think about this is that
[324.38s -> 329.50s]  we're adding a fifth state, a death state, and we have a probability of 1
[329.50s -> 334.62s]  minus gamma of transitioning to that death state at every time step. Once we
[334.66s -> 338.22s]  enter the death state, we never leave, so there's no resurrection in this MDP, and
[338.22s -> 343.02s]  our reward is always 0. So that means that the expected value for the next
[343.02s -> 348.02s]  time step will always be expressed as gamma times its expected value in the
[348.02s -> 354.10s]  original MDP plus 1 minus gamma times 0. And that's where we get this
[354.10s -> 359.14s]  gamma factor. So the probability of entering the death state is 1 minus
[359.14s -> 363.90s]  gamma, which means that the modified dynamics now are just gamma times the
[363.90s -> 368.06s]  original probabilities, and that 1 minus gamma remaining slice accounts for
[368.06s -> 373.42s]  entering the death state. So mechanically, the modification that we
[373.42s -> 377.22s]  have with the discount factors just multiply our values by gamma for every
[377.22s -> 381.82s]  time step that we back them up. What this does is it makes us prefer
[381.82s -> 386.10s]  rewards that happen sooner rather than later, and mathematically, one way that
[386.10s -> 390.02s]  we can interpret this is that we're actually modifying our MDP to introduce
[390.06s -> 395.62s]  the probability of death with probability of 1 minus gamma. All right,
[395.62s -> 400.62s]  let's dig into discount factors a little bit more. First, could we introduce
[400.62s -> 405.62s]  discount factors into regular Monte Carlo policy gradients? Well, the answer is
[405.62s -> 411.06s]  we most definitely can. So, for example, there's one option for how to do this
[411.06s -> 414.62s]  is we can just take that single sample reward-to-go calculation and
[414.62s -> 418.82s]  we can put gamma into it. So the equation I have here is exactly the
[418.82s -> 422.54s]  reward-to-go calculation I had before, except that now I've added this gamma
[422.54s -> 427.78s]  raised to the power t prime minus t in front of my reward. So that means that
[427.78s -> 432.06s]  the first reward, the one that happens at time step t, has a multiplier
[432.06s -> 436.86s]  of 1, but the next one at t plus 1 gets a multiplier of gamma, the next one
[436.86s -> 440.82s]  at t plus 2 gets a multiplier of gamma squared, and so on. So we're much more
[440.82s -> 446.90s]  affected by rewards that happen closer to us in time. This type of estimator is
[446.94s -> 450.86s]  essentially the single sample version of what you would get if you were to use
[450.86s -> 455.82s]  the value function with a discount bootstrap. There is another way that we
[455.82s -> 460.82s]  can introduce a discount into the Monte Carlo policy gradient, which seems like
[460.82s -> 468.70s]  it's very similar but has a somewhat important difference. What if we take
[468.70s -> 474.78s]  the original Monte Carlo policy gradient that we had before we did that
[474.78s -> 478.74s]  causality trick, where we just sum together the grad log pi's and then
[478.74s -> 483.02s]  multiply them together with the sum of the rewards. And then we're going to put a
[483.02s -> 487.62s]  discount into that. We'll put a gamma to the t minus 1 multiplier in front of
[487.62s -> 491.82s]  the reward, so that the reward of the first time step is multiplied by 1, the
[491.82s -> 495.66s]  reward of the second time step is multiplied by gamma, the reward of the third
[495.66s -> 502.22s]  time step is multiplied by gamma squared, and so on. Take a moment to think about
[502.26s -> 507.22s]  how these two options compare. Consider if these two options are actually
[507.22s -> 513.94s]  identical mathematically or not. So if we were to apply the causality trick to
[513.94s -> 519.54s]  option two, meaning that we remove all of the rewards from the past, will we
[519.54s -> 522.78s]  end up with option one or not?
[523.78s -> 532.82s]  So we'll come back to this question shortly, but to help us think about how
[532.82s -> 536.98s]  these options compare, let's write out just for completeness what we would get
[536.98s -> 540.74s]  if we had a critic. So with a critic, this is the gradient that we would get.
[540.74s -> 544.22s]  We have the current reward plus gamma times the next value minus the
[544.22s -> 551.26s]  current value, and that's our approximate advantage. So option one and option two
[551.30s -> 557.90s]  are not the same. In fact, option one matches the critic version, with the
[557.90s -> 564.34s]  exception that we have a single sample estimator. Option two does not. In fact,
[564.34s -> 568.42s]  if we were to rewrite option two by using the causality trick where we
[568.42s -> 572.34s]  distribute the rewards inside the sum over grad log pi's and then eliminate
[572.34s -> 576.06s]  all of the rewards that happened before the current time step, we'll end up
[576.06s -> 581.06s]  with this expression. We'll end up with grad log pi times step t times the sum from
[581.06s -> 586.30s]  t prime equals t to capital T of gamma to the t prime minus 1, whereas before
[586.30s -> 593.10s]  we had gamma to the t prime minus t. So what's going on here? Why do we have
[593.10s -> 597.38s]  this difference? Well, one way that we can understand this difference is if we
[597.38s -> 601.90s]  take the gamma to the t minus 1 factor and distribute it out of the
[601.90s -> 607.26s]  sum. So the last line I have here is exactly equal to a preceding line.
[607.50s -> 613.10s]  I've just distributed out a gamma to the t minus 1 factor, so now the reward-to-go
[613.10s -> 616.42s]  calculation is exactly the same as option one, but I have this additional
[616.42s -> 621.50s]  multiplier of gamma to the t minus 1 in front of grad log pi. So what is
[621.50s -> 625.58s]  that doing? Well, what that's doing is actually quite natural. It's saying
[625.58s -> 629.94s]  that because you have this discount, not only do you care less about rewards
[629.94s -> 633.98s]  further in the future, you also care less about decisions further in the
[633.98s -> 639.94s]  future. So if you're starting at time step one, rewards in the future matter
[639.94s -> 643.18s]  less, but also your decisions matter less because your decisions further in
[643.18s -> 647.70s]  the future will only influence future rewards. So as a result you
[647.70s -> 651.26s]  actually discount your gradient at every time step by gamma to the t
[651.26s -> 655.22s]  minus 1. Essentially it means that making the right decision at the first
[655.22s -> 658.70s]  time step is more important than making the right decision at the second
[658.70s -> 661.90s]  time step because the second time step will not influence the first time
[661.98s -> 666.22s]  step's reward, and that's what that gamma to the t minus 1 factor out front
[666.22s -> 671.54s]  represents. This is in fact the right thing to do if you truly want to solve
[671.54s -> 675.02s]  a discounted problem. If you are really in a setting where you have a
[675.02s -> 679.06s]  discount factor and that discount factor represents your preference for
[679.06s -> 682.90s]  near-term rewards or equivalently the probability of entering the death state,
[682.90s -> 689.14s]  then in fact your policy gradient should discount future gradients because in a
[689.18s -> 692.38s]  truly discounted setting, making the right decision now is more important than
[692.38s -> 698.46s]  making the right decision later. Coming back to my analogy about the $100, if I
[698.46s -> 703.90s]  tell you that I will give you $100 if you pass my math exam and I tell you
[703.90s -> 707.78s]  the same thing that I can give you the exam in today or I can give you
[707.78s -> 711.30s]  the exam next year or I can give you the exam in a million years, well
[711.30s -> 713.54s]  chances are if you know that I'm going to give you the exam in a million
[713.54s -> 717.86s]  years, you're probably not going to study for it. So your policy gradient for
[717.86s -> 721.98s]  that math exam will have a very small multiplier because you'd rather deal
[721.98s -> 726.94s]  with things that will give you rewards much nearer to the present. So it
[726.94s -> 730.62s]  makes sense to have this gamma to the t minus 1 term out front if we're
[730.62s -> 737.02s]  really solving a discounted problem. But in reality this is often not quite what
[737.02s -> 744.46s]  we want. So saying that later time steps matter less might not actually give us
[744.46s -> 750.34s]  the solution that we're after. So this is the the death version. Later steps
[750.34s -> 754.22s]  don't matter if you're dead, it's all mathematically consistent. The version
[754.22s -> 760.86s]  that we actually usually use is option one. Why is that? Well take a
[760.86s -> 764.06s]  moment to think about that. Why would we prefer to use option one instead of
[764.06s -> 772.34s]  option two? So if we think about this cyclic continuous RL task that I
[772.34s -> 777.02s]  presented before, where the goal is to make this character run as far as
[777.02s -> 783.42s]  possible, while we can model this as a task of discounted reward, in reality
[783.42s -> 788.66s]  we really do want this guy to run as far as possible, ideally
[788.66s -> 795.14s]  infinitely far. So we don't really want a discounted problem. What we want to do
[795.14s -> 799.78s]  is we want to use the discount to help get us finite values so that we can actually do
[799.78s -> 804.34s]  the URL, but then what we'd really like to do is get a solution that works for
[804.34s -> 811.54s]  running for arbitrarily long periods of time. So option one in
[811.54s -> 815.78s]  some ways is closer to what we want. Maybe what we really want is more like
[815.78s -> 819.90s]  average reward. So we want to put a 1 over capital T and remove the
[819.90s -> 823.94s]  discount altogether. Average reward is computationally and algorithmically very
[823.94s -> 828.62s]  very difficult to use, so we would use discount in practice because it's so
[828.66s -> 833.18s]  mathematically convenient, but omit the gamma to the T minus 1 multiplier that
[833.18s -> 836.74s]  shows up in option two because we really do want a policy that does the
[836.74s -> 841.38s]  right thing at every time step, not just in the early time steps. Another
[841.38s -> 845.22s]  way to think about the role that the discount factor plays that provides an
[845.22s -> 850.98s]  alternative perspective to this death state, and you can read about this in
[850.98s -> 856.34s]  this paper by Philip Thomas called Bias and Natural Ectocritic Algorithms, is that
[856.34s -> 864.62s]  the discount factor serves to reduce the variance of your policy gradient. So
[864.62s -> 869.38s]  if you have infinitely large rewards, you also have infinitely large
[869.38s -> 873.68s]  variances, right, because infinitely large values have infinite variances. By
[873.68s -> 876.98s]  ensuring that your reward sums are finite by putting a discount in front of them,
[876.98s -> 881.18s]  you're also reducing variance at the cost of introducing bias by not
[881.18s -> 888.46s]  accounting for all those rewards in the future. All right, so what happens when we
[888.46s -> 891.82s]  introduce the discount into our active critic algorithm? Well, the only thing
[891.82s -> 896.06s]  that changes is step three. So in step three you can see that we've added a
[896.06s -> 901.10s]  gamma in front of v pi phi s prime. Everything else stays exactly the same.
[901.10s -> 907.58s]  One of the things we can do with active critic algorithms, once we take them
[907.58s -> 910.66s]  into the infinite horizon setting, is we can actually derive a fully online
[911.14s -> 918.02s]  active critic method. So so far when we talked about policy gradients, we always
[918.02s -> 923.02s]  used policy gradients in a kind of episodic batch mode setting where we
[923.02s -> 927.18s]  collect a batch of trajectories, each trajectory runs all the way to the
[927.18s -> 930.70s]  end, and then we use that batch to evaluate our gradient and update our policy.
[930.70s -> 934.34s]  But we could also have an online version when we use active critic where
[934.34s -> 939.34s]  every single time step, every time we step the simulator or we step the
[939.34s -> 943.86s]  real world, we also update our policy. And here's what an online active critic
[943.86s -> 951.06s]  algorithm would look like. We would take an action, a sample from pi theta a
[951.06s -> 956.74s]  given s, and get a transition, s comma a comma s prime comma r. So we take one
[956.74s -> 960.54s]  time step. And at this point I'm not putting t subscripts on anything because
[960.54s -> 968.68s]  this can go on in a single infinitely long non-episodic process. Step two, we
[968.72s -> 974.40s]  update our value function by using the reward plus the value at the next state
[974.40s -> 978.08s]  as our target. Because we're using a bootstrapped update, we don't actually
[978.08s -> 981.44s]  need to know what state we'll get at the following types of or the one
[981.44s -> 984.34s]  after that or the one after that. We just need the one next time step s
[984.34s -> 988.04s]  prime. So we don't need s double prime, s triple prime, etc. because we're
[988.04s -> 991.32s]  using the bootstrap. So that's enough for us to update our value function.
[991.32s -> 996.96s]  Step three, we evaluate the advantage as the reward plus the value function
[996.96s -> 1000.08s]  at the next state minus the value function of the current state. Again,
[1000.08s -> 1005.44s]  this only uses things that we already know. It uses s, a, s prime, r, and our
[1005.44s -> 1010.40s]  learned value function. And then using this we can construct an estimate for
[1010.40s -> 1015.00s]  the policy gradient by simply taking the grad log pi for this action that we
[1015.00s -> 1019.16s]  just took, multiplied by the advantage that we just calculated. And then we can
[1019.16s -> 1024.16s]  update the policy parameters with policy gradient. And then we repeat this
[1024.16s -> 1028.64s]  process. And we do this every single time step. Now there are a few problems
[1028.64s -> 1033.04s]  with this recipe when we try to do deep RL. And maybe each of you could
[1033.04s -> 1036.04s]  take a moment to think about what might go wrong with this algorithm if we
[1036.04s -> 1039.72s]  implement it in practice. This is kind of the textbook online action-critic algorithm,
[1039.72s -> 1043.64s]  but for deep RL, it's a bit problematic.
[1044.36s -> 1050.48s]  All right, let's continue this in the next section.
