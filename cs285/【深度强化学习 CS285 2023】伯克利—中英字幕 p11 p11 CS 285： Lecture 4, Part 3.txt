# Detected language: en (p=1.00)

[0.00s -> 6.36s]  Alright, in the next portion of this lecture, I'm going to introduce the notion of value
[6.36s -> 11.84s]  functions, which are a very useful mathematical object both for designing reinforcement learning
[11.84s -> 18.48s]  algorithms and for conceptually thinking about the reinforcement learning objective.
[18.48s -> 25.48s]  So as I mentioned earlier, the reinforcement learning objective can be defined as an expectation.
[25.48s -> 31.40s]  It's an expectation of a sum of rewards with respect to the trajectory distribution or equivalently a
[31.40s -> 40.16s]  sum over time of the expected reward for every state action marginal. Now, one of the things we
[40.16s -> 47.20s]  could do with this expectation is we can actually write it out recursively. So you know how we can
[47.20s -> 51.56s]  apply the chain rule of probability to factorize the trajectory distribution as a product of many
[51.60s -> 57.56s]  distributions. In the same way, we can apply the chain rule and write out an expected value with
[57.56s -> 62.96s]  respect to that distribution as a series of nested expectations. So the outermost expectation
[62.96s -> 70.00s]  here would be over p of s1. Inside of it, we have an expected value with respect to a1,
[70.00s -> 75.52s]  distributed according to pi of a1 given s1. And now, since we have an expectation for both s1
[75.52s -> 82.08s]  and a1, we can put in the first reward, R of s1 comma a1. And notice that this inner expectation,
[82.08s -> 87.44s]  the one over a1, is conditioned on s1. I have a bunch of blank space here because I'm going to
[87.44s -> 93.84s]  need to put in all the other rewards. But we already have R of s1 a1. Now, we add to that
[93.84s -> 99.68s]  all the other rewards, but those require putting in another expectation now over s2, distributed
[99.68s -> 104.48s]  according to p of s2 given s1 a1. So this expectation is conditioned on s1 and a1. And
[104.48s -> 110.40s]  inside of that, we have another expectation over a2, distributed according to pi of a2 given s2. And
[110.40s -> 117.72s]  now, since we have both s2 and a2, we can put in R of s2 a2. And then we add to that the expected
[117.72s -> 123.76s]  value over s3, inside of which is the expected value over a3, inside of it is R of s3 a3,
[123.76s -> 129.68s]  and so on and so on. And we have these nested expectations. Now, at first it kind of seems
[129.68s -> 135.04s]  like we just wrote a very concise expected value over trajectories as a really, really messy set
[135.04s -> 143.28s]  of nested expectations. But one thing that we could think about is, well, what if we had some
[143.28s -> 150.48s]  function that told us the stuff that goes inside of the second expectation? What if we had some
[150.48s -> 156.16s]  function that told us R of s1 comma a1 plus the expected value over s2 plus et cetera, et cetera,
[156.16s -> 165.84s]  et cetera. So what if we knew this part? So let's define a symbol for this. Let's say that Q of s1
[165.84s -> 172.88s]  comma a1 is equal to R of s1 comma a1 plus the expectation over s2 of the expectation over a2
[172.88s -> 176.88s]  of R of s2 a2, et cetera. So basically just this middle part, the part that goes inside
[176.88s -> 181.36s]  the second set of square brackets, I'm just going to call that Q of s1 comma a1.
[181.36s -> 190.64s]  Then we can write our original RL objective as simply the expected value over s1 of the expected
[190.64s -> 197.04s]  value over a1 of Q of s1 comma a1. So it's just a little bit of symbolic manipulation, a little
[197.04s -> 205.84s]  bit of definition. But the important point about this definition is that if you knew Q of s1 comma
[205.84s -> 213.52s]  a1, then optimizing the policy at the first time step would be very easy. So if you had access to
[213.52s -> 219.92s]  Q of s1 comma a1 and you needed to select the policy pi of a1 given s1, you would just select
[219.92s -> 227.04s]  the policy for which this expected value is largest. You could simply test every action and
[227.04s -> 234.24s]  just assign 100% probability to the best one, the one with the largest value for Q. So this
[234.24s -> 240.80s]  basic idea can be extended to a more general concept. So this is the simple rule that I
[240.80s -> 245.36s]  said, you know, a simple way to get pi here is just assign a probability of 1 to the arc max.
[247.12s -> 255.20s]  So the more general principle is what we're going to call the Q function. So the Q function
[255.20s -> 259.60s]  can be defined at other time steps, not just time step one, and the definition is this.
[260.40s -> 269.84s]  Q pi of s t comma a t, and I say Q pi because it depends on pi, Q pi of s t comma a t is equal
[269.84s -> 279.04s]  to the sum over all time steps from t until the end, capital T, of the expected value of
[279.76s -> 286.16s]  the reward at that future time step conditioned on starting in s t comma a t.
[289.60s -> 294.24s]  So what that means is basically if you start in s t comma a t and then roll out your policy
[294.24s -> 300.16s]  for the rest of time, what will be the expected sum of rewards? A closely related quantity that
[300.16s -> 305.76s]  we can also define is something called the value function. The value function is defined in much
[305.76s -> 310.08s]  the same way, only it can show only a state rather than a state and action. So the value
[310.08s -> 314.88s]  function says if you start in state s t and then roll out your policy, what will be your expected
[314.88s -> 322.32s]  total value? And the value function can also be written as the expected value over actions
[322.32s -> 327.68s]  of the Q function, right, because if the Q function tells you the expected total reward
[327.68s -> 332.88s]  if you start in s t comma a t, then taking the expectation of that with respect to a t
[332.88s -> 336.24s]  will give you the expected total reward if you start in s t.
[338.56s -> 344.24s]  So now one observation we could make is the expectation of the value function at state s 1
[344.24s -> 349.12s]  is the entirety of the reinforcement learning objective for the same reason that the expected
[349.12s -> 355.04s]  value with respect to s 1 a 1 of Q s 1 a 1 was the RL objective on the previous slide.
[357.04s -> 361.60s]  Okay, so at this point I would like everyone to pause for a minute and think about these
[361.60s -> 366.80s]  definitions of Q functions and value functions. You might want to flip back to the previous slide
[366.80s -> 371.92s]  if something here is unclear. Take a moment to think about that, and if something about
[371.92s -> 375.44s]  these definitions is unclear, please make sure to write a question in the comments.
[378.32s -> 384.56s]  All right, let's continue. So what are Q functions and value functions good for?
[385.28s -> 389.52s]  Well, I provided some intuition for this a couple slides ago when I talked about how once you have
[389.52s -> 394.48s]  a Q function, for at least the first time step, you can recover a better policy for the first
[394.48s -> 402.80s]  time step. So one idea is that if we have a policy pi, and if we can figure out its full
[402.80s -> 411.52s]  Q function Q pi s comma a, then we can improve pi. For example, we can pick a new policy pi
[411.52s -> 418.40s]  prime that assigns a probability of 1 to a given action if that action is the r max of Q pi s a,
[418.40s -> 422.72s]  and we can do this on not just the first time step but on all of the time steps,
[422.72s -> 428.56s]  and in fact we can show that this policy is at least as good as pi and probably better. Don't
[428.56s -> 432.56s]  worry if this is not obvious to you right now why this is true, we'll cover this in much
[432.56s -> 437.68s]  more detail later, but this is the basis of a class of methods called policy iteration algorithms
[437.68s -> 440.64s]  which themselves can be used to derive Q-learning algorithms.
[442.96s -> 445.92s]  And crucially, it doesn't matter what pi is, you can always improve it in this way.
[447.84s -> 452.64s]  Another idea which we will use in the next lecture when we talk about policy gradients
[453.20s -> 458.40s]  is you can use this to compute a gradient to increase the probability of a good action a.
[460.00s -> 468.72s]  So the intuition is that if Q pi s a is larger than V of s, then a is better than average.
[472.40s -> 477.60s]  Because remember that the that V pi of s is just the expected value of Q pi s a under pi of a
[477.60s -> 485.12s]  given s. By this definition, V pi of s is how you will do on average when you use your policy
[485.12s -> 490.40s]  from state s. So if you can do better than average, if you can choose an action a so that
[490.40s -> 494.88s]  Q pi s a is larger than V pi of s, then you will do better, you will do better than average
[494.88s -> 500.16s]  under your old policy. So one thing you could do is you could modify pi of a given s
[500.16s -> 506.16s]  to increase the probability of actions whose value under the Q function is larger than the value
[506.16s -> 510.96s]  at that state. And you can actually use this to get a gradient-based update rule on pi.
[513.12s -> 516.80s]  These ideas are very important in RL and we'll revisit them again and again
[516.80s -> 520.56s]  in the next few lectures when we talk about model-free reinforcement learning algorithms.
[523.44s -> 530.08s]  All right, so in the anatomy of the reinforcement learning algorithm, the green box
[530.08s -> 537.52s]  is typically where you would use or where you would learn Q functions or value functions. So Q
[537.52s -> 541.44s]  functions and value functions fundamentally are objects that evaluate how good your policy
[541.44s -> 545.92s]  currently is. So you would typically fit them or learn them in the green box
[545.92s -> 550.64s]  and then use them in the blue box to improve the policy.
