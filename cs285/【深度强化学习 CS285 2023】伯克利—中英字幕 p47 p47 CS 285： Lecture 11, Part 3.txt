# Detected language: en (p=1.00)

[0.00s -> 5.08s]  All right, let's talk about how we can train uncertainty-aware neural network
[5.08s -> 11.90s]  models to serve as our uncertainty-aware dynamics models for model-based RL. So
[11.90s -> 17.12s]  how can we have uncertainty-aware models? Well, one very simple idea is to
[17.12s -> 21.52s]  use the entropy of the output distribution. I'll tell you right now
[21.52s -> 25.24s]  this is a bad idea, this does not work, but I'm going to explain it just
[25.24s -> 31.20s]  to make it clear why it doesn't work. So let's say that you have your neural
[31.20s -> 36.88s]  network dynamics model that takes an s and a as input and it produces p of s t
[36.88s -> 41.44s]  plus 1 given s t a t, which could be represented by a softmax distribution if
[41.44s -> 45.76s]  you're in the discrete action setting, or it can be represented by a
[45.76s -> 49.64s]  multivariate Gaussian distribution in the continuous setting. So in the
[49.64s -> 53.92s]  multivariate Gaussian case you output a mean and a variance, in the softmax
[53.92s -> 61.60s]  case you just output the logit for every possible next state. Why is this not
[61.60s -> 72.12s]  enough? Well, we talked about how the the problem we're having is this
[72.12s -> 78.12s]  erroneous extrapolation, so for the setting where we have limited data we
[78.12s -> 82.02s]  might overfit and make erroneous predictions, and the particular kind of errors
[82.02s -> 85.86s]  that we're especially concerned with are ones where the optimizer can exploit
[85.86s -> 91.18s]  those errors by optimizing against our model. When the optimizer optimizes
[91.18s -> 93.90s]  against our model, what it's really going to be doing is going to be
[93.90s -> 98.34s]  finding out-of-distribution actions that lead to out-of-distribution states
[98.34s -> 102.22s]  that then lead to more out-of-distribution states, which
[102.22s -> 106.14s]  means that our model is going to be asked to make predictions for states
[106.14s -> 112.18s]  and actions that it was not trained on. The problem is that if the model is
[112.18s -> 117.58s]  outputting the uncertainty and it's trained with a regular maximum
[117.58s -> 121.90s]  likelihood, the uncertainty itself will also not be accurate for
[121.90s -> 125.86s]  out-of-distribution inputs. So out-of-distribution inputs will result in erroneous
[125.86s -> 129.86s]  predictions like an erroneous mean, but they'll also result in an erroneous
[129.86s -> 135.66s]  variance for the same exact reason. And this is because the uncertainty of
[135.70s -> 143.02s]  the neural net output is the wrong kind of uncertainty. So if you imagine this
[143.02s -> 147.22s]  highly overfitted model, you could say, well, what variance is this model going
[147.22s -> 150.26s]  to be predicting? Let's say that the blue curve represents the
[150.26s -> 153.40s]  predictions from the model, the model outputs a mean, and the variance
[153.40s -> 158.14s]  overrides every point. Well, if it looks at the training points, the training
[158.14s -> 162.78s]  means are basically exactly the same as the actual values. So the optimal
[162.78s -> 166.90s]  variance for it to output is actually zero. This model will be extremely
[166.90s -> 172.66s]  confident, but of course it's completely wrong. And we'll see exactly the same
[172.66s -> 176.70s]  thing from deep nets. We'll see very confident predictions that are very good
[176.70s -> 180.82s]  on the training points, but are both incorrect and overconfident on the
[180.82s -> 185.02s]  test points. And this is not something special about neural nets. It's not
[185.02s -> 188.70s]  about neural nets being bad at estimating uncertainty. It's just because
[188.74s -> 194.66s]  this is the wrong kind of uncertainty to be predicting. This measure of entropy
[194.66s -> 201.74s]  is not trying to predict the uncertainty about the model. It's trying to predict
[201.74s -> 207.86s]  how noisy the dynamics are. See, there are two types of uncertainty, and there
[207.86s -> 210.82s]  are a variety of names that people have used for them, but we can call them
[210.82s -> 216.54s]  aleatoric or statistical uncertainty, which is essentially the case where you
[216.58s -> 223.58s]  have a function that is itself noisy, and then we have epistemic or model
[223.58s -> 229.34s]  uncertainty, which happens not because the true function itself is noisy or
[229.34s -> 234.42s]  not, but because you don't know what the right function is. And these are
[234.42s -> 238.66s]  fundamentally different kinds of uncertainty. Aleatoric uncertainty
[238.66s -> 242.58s]  doesn't go down necessarily as you collect more data. If the true function
[242.62s -> 247.58s]  is noisy, no matter how much data you collect, you will have high entropy
[247.58s -> 251.90s]  outputs just because the true function has high entropy. Like, for example, if
[251.90s -> 256.26s]  you're learning the dynamics model for a game of chance, for a game where you
[256.26s -> 265.86s]  roll two dice, the correct answer for the model that models the numerical
[265.86s -> 269.78s]  value of the sum of those two dice is going to be random. It's never going to
[269.78s -> 272.86s]  become deterministic as you collect more data. Seeing the dice roll more and
[272.86s -> 276.86s]  more doesn't allow you to turn that stochastic system
[276.86s -> 281.14s]  into a deterministic one. That's aleatoric uncertainty. That's when the
[281.14s -> 286.18s]  world itself is actually random. Epistemic uncertainty comes from the
[286.18s -> 290.74s]  fact that you don't know what the model is. So epistemic uncertainty
[290.74s -> 294.86s]  would be like the setting we had when approaching the cliff or walking
[294.86s -> 297.66s]  around on the top of the mountain. Once you collect enough data, that
[297.66s -> 300.70s]  uncertainty goes away. But in the limited data regime, you have to
[300.70s -> 302.70s]  maintain that uncertainty because you don't know what the model
[302.70s -> 305.94s]  actually is. This is essentially a setting where the model is certain
[305.94s -> 309.10s]  about the data, but we are not certain about the model, and that's
[309.10s -> 312.78s]  what we want. Maximum likelihood training doesn't give you this. So
[312.78s -> 316.42s]  just outputting a distribution over the next state or a Gaussian
[316.42s -> 319.90s]  distribution with a mean and variance will not get you this
[319.90s -> 320.54s]  capability.
[320.54s -> 327.70s]  So how can we get it? Well, we can try to estimate model
[327.70s -> 329.74s]  uncertainty, and there are a number of different techniques
[329.74s -> 332.58s]  for doing this. So this is basically the setting where the
[332.58s -> 334.30s]  model is certain about the data, but we are not certain
[334.30s -> 337.30s]  about the model. In order to not be certain about the model,
[337.50s -> 341.70s]  we need to represent a distribution over models. So before we
[341.70s -> 344.66s]  have one neural net that outputted a distribution over st
[344.66s -> 348.46s]  plus one, and it has some parameters theta. So being
[348.46s -> 351.82s]  uncertain about the model really means being uncertain
[351.82s -> 358.34s]  about theta. So usually we would estimate theta as the arg
[358.34s -> 362.14s]  max of the log probability of theta given our data set,
[362.22s -> 364.74s]  which when we're doing maximum likelihood estimation, we
[364.74s -> 367.62s]  take to also be the arg max of the log probability of the
[367.62s -> 371.42s]  data given theta, and that presumes having a uniform
[371.54s -> 377.02s]  prior. But can we instead estimate the full distribution
[377.02s -> 379.34s]  p theta given d? So instead of just finding the most
[379.34s -> 381.66s]  likely theta, what if we actually try to estimate the
[381.66s -> 384.94s]  full distribution of theta given d and then use that to
[384.94s -> 387.90s]  get our uncertainty? That is the right kind of uncertainty
[387.90s -> 391.18s]  to get in this situation. So the entropy of this
[391.18s -> 394.38s]  distribution will tell us the model uncertainty, and we
[394.38s -> 398.46s]  can average out the parameters and get a posterior
[398.46s -> 402.62s]  distribution over the next state. So when we then have
[402.62s -> 404.82s]  to predict, we would actually integrate out our
[404.82s -> 407.94s]  parameters. So instead of taking the most likely theta and
[407.94s -> 411.70s]  outputting the probability of ST plus one given STAT and
[411.70s -> 415.58s]  that most likely theta, we'll output our parameters by
[415.58s -> 418.34s]  integrating out theta, by taking the integral p of ST
[418.34s -> 422.10s]  plus one given STAT comma theta times p of theta
[422.10s -> 427.14s]  given d, d theta. Now, of course, for large high
[427.14s -> 429.30s]  dimensional parameter spaces of the sort that we would
[429.30s -> 431.66s]  have with neural nets, performing this operation
[431.66s -> 434.06s]  exactly is completely intractable. So we have to
[434.06s -> 437.42s]  resort to a variety of different approximations, and
[437.42s -> 440.94s]  that's what we're going to talk about in this lecture.
[440.94s -> 444.62s]  So intuitively, you could imagine this is producing
[444.62s -> 447.82s]  some distribution over next states, which is going to
[447.82s -> 453.42s]  integrate out all the uncertainty in your model.
[453.42s -> 456.06s]  So one choice that we could use is something called a
[456.06s -> 459.66s]  Bayesian neural network. I'm not going to go into great
[459.66s -> 462.46s]  detail about Bayesian neural networks in this lecture
[462.46s -> 464.38s]  because it requires a little bit of machinery, a little
[464.38s -> 467.10s]  bit of variational inference machinery, which we're
[467.10s -> 470.94s]  actually going to cover next week. But I do want to
[470.94s -> 473.50s]  explain the high-level idea behind Bayesian neural
[473.50s -> 477.58s]  nets. So in a standard neural net of the sort shown on
[477.58s -> 483.50s]  the left, you have inputs x and outputs y, and every
[483.50s -> 486.30s]  weight, every connection between the hidden units, the
[486.30s -> 489.90s]  inputs, and the outputs is just a number. So all the
[489.90s -> 492.14s]  neural nets that you've trained so far in this class
[492.14s -> 497.58s]  basically work on this principle. In Bayesian neural
[497.58s -> 502.06s]  networks, there's a distribution over every weight.
[502.06s -> 503.74s]  In the most general case, there's actually a joint
[503.74s -> 506.78s]  distribution over all the weights. If you want to
[506.78s -> 509.58s]  make a prediction, what you can do is you can sample
[509.58s -> 512.38s]  from this distribution, essentially sample a neural net
[512.38s -> 515.50s]  from the distribution over neural nets, and ask it for
[515.50s -> 518.86s]  its prediction. And if you want to get a posterior
[518.86s -> 521.10s]  distribution over predictions, if you want to sample from
[521.10s -> 523.34s]  the posterior distribution, you would sample a neural net
[523.34s -> 525.34s]  and then sample a y given that neural net, and you
[525.34s -> 527.50s]  could repeat this process multiple times if you want
[527.50s -> 530.46s]  to get many samples to get a general impression of the
[530.46s -> 533.82s]  true posterior distribution y given x with theta having
[533.82s -> 538.46s]  been integrated out. Now modeling full joint
[538.46s -> 541.02s]  distributions over the parameters is very difficult
[541.02s -> 543.42s]  because the parameters are very high-dimensional, so
[543.42s -> 545.18s]  there are a number of common approximations that
[545.26s -> 549.02s]  could be made. One approximation is to estimate the
[549.02s -> 552.70s]  parameter posterior, this p of theta given d, as a
[552.70s -> 556.14s]  product of independent marginals. This basically
[556.14s -> 559.50s]  means that every weight is distributed randomly but
[559.50s -> 562.86s]  independently of all the other weights. This is of
[562.86s -> 565.58s]  course not a very good approximation because in reality
[565.58s -> 569.90s]  the weights have very tightly interacting effects. So
[569.90s -> 571.66s]  you know if you want to vary one weight and you
[571.66s -> 573.18s]  vary the other one in the opposite direction, maybe
[573.18s -> 574.94s]  your function doesn't change very much, but if
[574.94s -> 576.54s]  you vary them independently it could change quite a
[576.54s -> 579.98s]  lot. So using a product of independent marginals to
[579.98s -> 582.78s]  estimate the parameter posterior is a very crude
[582.78s -> 585.66s]  approximation, but it's a very simple and tractable
[585.66s -> 587.66s]  one, and for that reason it is used quite often.
[589.42s -> 592.46s]  A common choice for the independent marginals is to
[592.46s -> 595.58s]  represent each marginal with a Gaussian distribution,
[596.14s -> 598.38s]  and that means that for every weight, instead of
[598.38s -> 600.70s]  learning its numerical value, you learn its mean
[600.70s -> 604.54s]  value and its variance. So for each weight you have
[604.54s -> 607.74s]  not one number but two numbers now. You have the
[607.74s -> 610.06s]  expected weight value and the uncertainty about the
[610.06s -> 612.06s]  weight, and that is a very nice intuitive
[612.06s -> 614.46s]  interpretation because you've gone from learning
[614.46s -> 617.18s]  just a single weight vector to learning a mean
[617.18s -> 619.34s]  weight vector, and for every dimension you have a
[619.34s -> 624.38s]  variance. For more details about these kinds of
[624.38s -> 626.94s]  methods, here are a few relatively simple papers on
[626.94s -> 629.18s]  this topic, Weight Uncertainty in Neural Networks by
[629.18s -> 631.74s]  Blundell et al. and Concrete Dropout by Gall et al.,
[631.74s -> 634.46s]  although there are many more recent substantially
[634.46s -> 636.22s]  better methods that you could actually use if you
[636.22s -> 639.26s]  want to do this in practice. So Bayesian neural
[639.26s -> 641.50s]  networks are actually a reasonable choice to get an
[641.50s -> 644.86s]  uncertainty-aware model. To learn more about how
[644.86s -> 648.30s]  to train them, check out these papers or hang on
[648.30s -> 650.46s]  until we cover the variational inference material
[650.46s -> 650.94s]  next week.
[655.74s -> 657.74s]  Today we're instead going to talk about a simpler
[657.74s -> 660.14s]  method that, from my experience, actually works a
[660.14s -> 662.30s]  little bit better in model-based reinforcement
[662.30s -> 664.86s]  learning, and that's to use bootstrap ensembles.
[666.30s -> 668.62s]  Here is the basic idea behind bootstrap ensembles.
[668.62s -> 670.86s]  I'll present it first intuitively and then discuss
[670.86s -> 673.10s]  a little bit more mathematically what it's doing.
[674.94s -> 677.50s]  What if instead of training one neural network to
[677.50s -> 679.90s]  give us the distribution over the next state given
[679.90s -> 683.42s]  the current state in action, we instead train many
[683.42s -> 687.18s]  different neural networks and we somehow diversified
[687.18s -> 689.34s]  them so that each of those neural networks learns
[689.34s -> 692.38s]  a slightly different function? Ideally, they would
[692.38s -> 695.02s]  all do similar and accurate things on the training
[695.02s -> 697.58s]  data, but they would all make different mistakes
[697.58s -> 698.86s]  outside of the training data.
[701.66s -> 704.38s]  If we can train this kind of ensemble of models,
[704.38s -> 706.14s]  then we can get them to essentially vote on what
[706.14s -> 709.10s]  they think the next state will be, and the
[709.98s -> 711.74s]  dispersion in their votes will give us an
[711.74s -> 713.10s]  estimate of uncertainty.
[716.62s -> 718.86s]  Mathematically, this amounts to estimating your
[718.86s -> 722.54s]  parameter posterior p of theta given d as a mixture
[722.54s -> 724.06s]  of Dirac delta distributions.
[725.50s -> 727.58s]  So you've probably learned about mixtures of
[727.58s -> 730.30s]  Gaussians. A mixture of Dirac deltas is like a
[730.30s -> 732.86s]  mixture of Gaussians, only instead of Gaussians, you
[732.86s -> 735.10s]  have very narrow spikes, so each element has no
[735.10s -> 737.26s]  variance. It's just a mixture of delta functions
[739.26s -> 743.34s]  where each delta function is centered at the
[743.34s -> 745.82s]  parameter vector for the corresponding network in
[745.82s -> 746.54s]  the ensemble.
[749.42s -> 752.62s]  So, intuitively, you can train multiple models and see
[752.62s -> 754.86s]  if they agree as your measure of uncertainty.
[754.86s -> 757.58s]  Formally, you get a parameter posterior p of theta
[757.58s -> 761.34s]  given d represented as this mixture of Dirac deltas,
[761.34s -> 762.94s]  which means that if you want to integrate out your
[762.94s -> 766.70s]  parameters, you simply average over your models.
[766.70s -> 769.34s]  So you construct a mixture distribution where each
[769.34s -> 771.02s]  mixture element is the prediction of the
[771.02s -> 771.98s]  corresponding model.
[773.26s -> 776.06s]  Now, very importantly, in continuous state spaces,
[776.06s -> 778.94s]  it doesn't mean that we average together the actual
[778.94s -> 783.02s]  mean state. We're averaging together the probabilities,
[783.02s -> 784.70s]  which means that each of these models, if each of
[784.70s -> 786.86s]  these models is Gaussian and their means are in
[786.86s -> 789.74s]  different places, our output is not one Gaussian with
[789.74s -> 792.06s]  the average of those means, it's actually multiple
[792.06s -> 793.90s]  different Gaussians. It's actually a mixture of
[793.90s -> 798.06s]  Gaussians. So we're mixing the probabilities, not
[798.06s -> 800.70s]  the means. So when you implement this in homework
[800.70s -> 803.18s]  4, don't just average together the next states
[803.18s -> 806.22s]  that your models predict, actually treat it as a
[806.22s -> 807.50s]  mixture of Gaussians.
[809.98s -> 813.02s]  Okay, how can we train this bootstrap ensemble to
[813.02s -> 815.34s]  actually get it to represent this parameter posterior?
[816.38s -> 819.90s]  Well, one mathematical tool we can use is something
[819.90s -> 823.26s]  called the bootstrap. The main idea in the bootstrap
[823.26s -> 825.82s]  is that we take our single training set and we
[825.82s -> 828.70s]  generate multiple independent data sets from the
[828.70s -> 831.90s]  single training set to get independent models.
[831.90s -> 834.94s]  So each model needs to be trained on a data set that
[834.94s -> 836.78s]  is independent from the data set for every other
[836.78s -> 839.18s]  model, but still comes from the same distribution.
[841.34s -> 843.74s]  Now, if we had a very large amount of data, one
[843.74s -> 846.06s]  very simple way we could do this is we could take
[846.06s -> 849.10s]  our training set and just chop it up into n
[849.10s -> 851.82s]  non-overlapping pieces and train a different model
[851.82s -> 855.26s]  on each piece. But that's very wasteful because we're
[855.26s -> 857.90s]  essentially decimating our data set and therefore
[857.90s -> 861.02s]  we can't have too many bootstraps, we can't have
[861.02s -> 863.10s]  too many models. So in the bootstrap ensemble, each
[863.10s -> 866.06s]  of these models is called a bootstrap. So there's a
[866.06s -> 868.78s]  cool trick that we can do, which is going to
[868.78s -> 871.18s]  maintain our data efficiency and give us as many
[871.18s -> 875.58s]  models as we want. The idea is to train each
[875.58s -> 879.50s]  theta i on a data set D i which is sampled with
[879.50s -> 883.74s]  replacement for D. So if D contains n data points,
[883.74s -> 886.94s]  D i will also contain n data points, but they
[886.94s -> 890.22s]  will be resampled from D with replacement, which
[890.22s -> 894.06s]  means that for every entry in D i, let's say you
[894.06s -> 897.10s]  have n data points in D, you select an integer
[897.10s -> 899.82s]  uniformly at random between 1 and D and pick the
[899.82s -> 903.34s]  corresponding element from D. So you select a
[903.34s -> 906.06s]  random integer from 1 to n, pick the element from D,
[906.06s -> 908.14s]  write that over the first entry of D i. For the
[908.14s -> 910.70s]  second entry in D i, pick a random integer between 1
[910.70s -> 914.14s]  and D, grab it from D, put it in entry 2. For
[914.14s -> 917.26s]  entry 3, random integer from 1 to n, take it from
[917.26s -> 921.58s]  D, put entry 3, and so on and so on and so on.
[921.58s -> 923.90s]  In expectation, you get a data set that comes from the
[923.90s -> 927.74s]  same distribution as D, but every individual D i is
[927.74s -> 930.46s]  going to look a little bit different. Intuitively, you
[930.46s -> 932.54s]  can think of this as putting integer counts on
[932.54s -> 935.42s]  every data point, and those counts can range from
[935.42s -> 938.70s]  0 to n, although n is very unlikely. So every
[938.70s -> 941.10s]  model trained on every D i is going to see a
[941.10s -> 945.10s]  slightly different data set, although statistically the
[945.10s -> 947.74s]  data sets will be similar. And it turns out this is
[947.74s -> 951.50s]  enough to give you a parameter posterior.
[953.10s -> 956.46s]  So that's the theory. Now, in the world of deep
[956.46s -> 958.70s]  learning, it turns out that training in Bootstrap
[958.70s -> 962.06s]  Ensemble is actually even easier. So the basic
[962.06s -> 964.46s]  recipe I outlined on the previous slide
[964.46s -> 969.50s]  essentially works. It's a fairly crude approximation
[969.50s -> 971.02s]  because the number of models we would have is
[971.02s -> 973.66s]  usually small, right? So if the cost of training
[973.66s -> 976.70s]  one neural net is three hours, and we have to train 10 of them,
[976.70s -> 980.22s]  that will take 30 hours of compute. Now you can parallelize it,
[980.22s -> 983.34s]  but it's still expensive, so usually we'd use a smaller number of models,
[983.34s -> 987.50s]  typically less than 10. So our uncertainty will be a fairly crude
[987.50s -> 991.90s]  approximation to the true parameter posterior.
[992.86s -> 996.94s]  Conveniently though, it appears experimentally that
[996.94s -> 1000.46s]  if you're training deep neural network models, resampling with
[1000.46s -> 1003.58s]  replacement is actually usually unnecessary,
[1003.58s -> 1006.22s]  because just the fact that you train your model with stochastic gradient
[1006.22s -> 1009.50s]  descent and random initialization usually makes
[1009.50s -> 1012.46s]  the model sufficiently independent, even if they are trained on exactly
[1012.46s -> 1015.90s]  the same data set. So when implementing this in practice, we
[1015.90s -> 1019.34s]  can usually actually forego the resampling with replacement. So
[1019.34s -> 1022.38s]  that makes things a little easier. It's important for theoretical
[1022.38s -> 1027.18s]  results, but practically you can skip it.
