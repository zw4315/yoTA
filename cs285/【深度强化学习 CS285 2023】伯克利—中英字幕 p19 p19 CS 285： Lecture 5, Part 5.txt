# Detected language: en (p=1.00)

[0.00s -> 5.36s]  In the next portion of today's lecture, we're going to talk about implementing policy gradients
[5.36s -> 10.28s]  in practice, in deep RL algorithms.
[10.28s -> 14.00s]  One of the main challenges with implementing policy gradients is that we would like to
[14.00s -> 18.44s]  implement them in such a way that automatic differentiation tools like TensorFlow or
[18.44s -> 23.68s]  PyTorch can calculate the policy gradient for us with reasonable computational and
[23.68s -> 25.64s]  memory requirements.
[25.64s -> 31.12s]  If we wanted to implement policy gradients naively, we could simply calculate grad log
[31.12s -> 37.48s]  pi AIT given SIT for every single state action tuple that we sampled.
[37.48s -> 42.84s]  However, typically this is very inefficient because neural networks can have a very large
[42.84s -> 43.84s]  number of parameters.
[43.84s -> 48.72s]  In fact, the number of parameters is usually much larger than the number of samples that
[48.72s -> 49.72s]  we've produced.
[49.72s -> 53.84s]  So let's say that we have n parameters, where n might be on the order of a million,
[53.84s -> 58.44s]  and we have 100 trajectories, each with 100 time steps.
[58.44s -> 62.80s]  So we have 10,000 total state action pairs, which means that we're going to need to calculate
[62.80s -> 67.04s]  10,000 of these 1 million length vectors.
[67.04s -> 72.28s]  That's going to be very, very expensive in terms of memory storage and also computation.
[72.28s -> 77.22s]  Typically, when we want to calculate derivatives through neural networks efficiently, we want
[77.22s -> 79.76s]  to utilize the backpropagation algorithm.
[79.76s -> 83.12s]  So instead of calculating the derivative of the neural net's output with respect to
[83.12s -> 87.32s]  its input and then multiplying that by the derivative of the loss, we do the opposite.
[87.32s -> 91.08s]  We first calculate the derivative of the loss and then backpropagate it through the
[91.08s -> 95.28s]  neural network using the backpropagation algorithm, which is what our automatic differentiation
[95.28s -> 97.80s]  tools will do for us.
[97.80s -> 100.80s]  In order to do that, we need to set up a graph such that the derivative of that
[100.80s -> 103.60s]  graph gives us the policy gradient.
[103.60s -> 106.28s]  All right.
[106.28s -> 109.32s]  So how do we compute policy gradients with automatic differentiation?
[109.32s -> 115.48s]  Well, we need a graph such that its gradient is the policy gradient.
[115.48s -> 119.52s]  The way that we're going to figure this out is by starting with the gradients that
[119.52s -> 122.96s]  we already know how to compute, which are maximum likelihood gradients.
[122.96s -> 126.92s]  So if we want to compute maximum likelihood gradients, then what we would do is we
[126.92s -> 131.56s]  would implement the maximum likelihood objective using something like a cross-entropy loss
[131.56s -> 136.20s]  and then call .backward or .gradients on it, depending on your automatic differentiation
[136.20s -> 139.76s]  package, and obtain your gradients.
[139.76s -> 143.72s]  So the way that we're going to implement policy gradients to get our auditive package
[143.72s -> 148.44s]  to calculate them efficiently is by implementing a kind of pseudo-loss as a weighted maximum
[148.44s -> 150.48s]  likelihood.
[150.48s -> 156.28s]  So instead of implementing J maximum likelihood, we'll implement this thing called J tilde,
[156.28s -> 162.16s]  which will just be the sum of the log probabilities of all of our sampled actions, multiplied
[162.16s -> 165.48s]  by the rewards to go, Q hat.
[165.48s -> 170.60s]  Now critically, this equation is not the reinforcement learning objective.
[170.60s -> 173.20s]  In fact, this equation is not anything.
[173.20s -> 179.36s]  It's just a quantity chosen such that its derivatives come out to be the policy gradient.
[179.36s -> 184.16s]  Of course, a critical portion of this is that our automatic differentiation package
[184.16s -> 188.36s]  doesn't realize that those Q hat numbers are themselves affected by our policy.
[188.36s -> 191.92s]  So it's just dealing with the graph that we provided it.
[191.92s -> 195.80s]  So in a sense, we're almost trying to trick our auditive package into giving us
[195.80s -> 199.64s]  the gradient that we want.
[199.64s -> 205.36s]  So here, log pi would be, for example, our cross-entropy loss if we have discrete actions
[205.36s -> 209.84s]  or squared error if we have normally distributed continuous actions.
[210.72s -> 211.72s]  All right.
[211.72s -> 214.32s]  So I have some pseudocode here.
[214.32s -> 219.20s]  This pseudocode is actually in TensorFlow because I taught the class in TensorFlow in past years.
[219.20s -> 224.08s]  You're going to be doing the policy gradient assignment in PyTorch.
[224.08s -> 225.92s]  The basic idea is very much the same.
[225.92s -> 230.00s]  It's just the particular terminology is going to be a little different.
[230.00s -> 234.28s]  But hopefully, the pseudocode is still straightforward for everyone to parse.
[234.28s -> 239.36s]  So the pseudocode that I have here is the pseudocode for maximum likelihood learning.
[239.36s -> 241.16s]  This is supervised learning.
[241.16s -> 247.12s]  Here, actions is a tensor with dimensionality n times t along the first dimension,
[247.12s -> 249.68s]  so number of samples times the number of time steps,
[249.68s -> 252.68s]  and the dimensionality of the action along the second dimension.
[252.68s -> 257.80s]  And states is a tensor n times t times the number of state dimensions.
[257.80s -> 262.28s]  So the first line, logits equals policy.predictions states,
[262.28s -> 266.56s]  that simply asks the policy network to make predictions for those states.
[266.56s -> 269.68s]  Basically, output the logits over the actions.
[269.68s -> 272.04s]  This is a discrete action example.
[272.04s -> 278.96s]  Then the second line, negative likelihoods, basically uses the softmax cross-entropy function
[278.96s -> 282.08s]  to produce likelihoods for all the actions.
[282.08s -> 286.12s]  And then we do a mean reduce on those and calculate their gradients.
[286.12s -> 289.52s]  So this will give you the gradient of the likelihood.
[289.52s -> 293.16s]  This is what you do for supervised learning.
[293.20s -> 299.24s]  To implement policy gradients, you just have to put in weights to get a weighted likelihood,
[299.24s -> 302.24s]  and those weights correspond to those reward-to-go values.
[302.24s -> 307.88s]  So I'm going to assume that the reward-to-go values are all packed into a tensor called q underscore values,
[307.88s -> 310.92s]  which is an n times t by one tensor.
[310.92s -> 313.60s]  And then after I calculate my likelihoods,
[313.60s -> 319.40s]  I'll turn them into weighted likelihoods by point-wise multiplying them by the q values.
[319.40s -> 321.56s]  And that's the only change that I make.
[321.60s -> 325.00s]  Then I mean reduce those, and then I call their gradients.
[325.00s -> 329.92s]  So this will essentially trick your additive package into calculating a policy gradient.
[332.32s -> 335.92s]  So in math, what we've implemented is this.
[335.92s -> 342.92s]  We've basically turned our maximum likelihood loss into this modified pseudo-loss, j tilde,
[342.92s -> 345.24s]  where we weight our likelihoods by q-hats.
[347.24s -> 351.48s]  And of course, it's up to you to actually implement some code to compute those q values
[351.48s -> 353.88s]  which you could do simply in NumPy.
[353.88s -> 356.88s]  You don't really need to use your autodev package to compute those.
[359.68s -> 363.28s]  All right, a few general tips about using policy gradients in practice.
[364.48s -> 368.36s]  First, remember that the policy gradient has high variance.
[368.36s -> 372.28s]  So even though the implementation looks a lot like supervised learning,
[372.28s -> 375.00s]  it's going to behave very differently from supervised learning.
[375.00s -> 379.00s]  The high variance of a policy gradient will make some things quite a bit harder.
[379.00s -> 381.36s]  It means your gradients will be very noisy.
[381.68s -> 384.32s]  Which means that you probably need to use larger batches,
[384.32s -> 386.84s]  probably much larger than what you're used to for supervised learning.
[386.84s -> 390.44s]  So batch sizes in the thousands or tens of thousands are fairly typical.
[391.56s -> 394.32s]  Tweaking the learning rate is going to be substantially harder.
[396.16s -> 399.04s]  Adaptive step size rules like Adam can be okay-ish,
[399.04s -> 403.04s]  but just regular SGD with momentum can be extremely hard to use.
[403.04s -> 407.76s]  We'll learn about policy gradient-specific learning rate adjustment methods later
[407.76s -> 409.48s]  when we talk about things like natural gradient.
[409.48s -> 412.12s]  But for now, using Adam is a good starting point.
[413.60s -> 416.80s]  And in general, just expect you to have to do more hyperparameter tuning
[416.80s -> 418.92s]  than you've usually had to do for supervised learning.
[420.40s -> 425.16s]  So just to review, we talked about how the policy gradient is on policy,
[425.16s -> 428.20s]  how we can derive an off-policy variant using importance sampling,
[428.20s -> 431.12s]  which unfortunately has exponential scaling in the time horizon.
[431.12s -> 435.00s]  But we can ignore the state portion, which gives us an approximation.
[435.00s -> 438.84s]  We talked about how we can implement policy gradients with automatic differentiation,
[439.08s -> 444.16s]  and the key to doing that is setting it up so that AutoDiff backpropagates things for us properly
[444.16s -> 445.96s]  by using the pseudo-loss.
[445.96s -> 450.48s]  And we talked about some practical considerations, batch signs, learning rates, and optimizers.
