# Detected language: en (p=1.00)

[0.00s -> 6.12s]  So why do we have so many different RL algorithms? Why is it that we can't just
[6.12s -> 10.84s]  teach you one RL algorithm in a couple lectures and be done with it? Why do we
[10.84s -> 15.28s]  need an entire course? Well, these RL algorithms have a number of trade-offs
[15.28s -> 19.32s]  that will determine which one works best for you in your particular
[19.32s -> 25.64s]  situation. So one important trade-off between different algorithms, and we'll
[25.64s -> 29.12s]  touch on this as we go through the next few lectures, is sample efficiency.
[29.12s -> 34.84s]  Meaning, when you execute the stuff in this orange box, when you
[34.84s -> 38.72s]  generate samples in the environment, how many samples will you need before you
[38.72s -> 46.16s]  can get a good policy? Another trade-off is stability and ease of use.
[46.16s -> 49.64s]  Reinforcing learning algorithms can be quite complex. They require trading off
[49.64s -> 53.38s]  a number of different parameters, how you collect samples, how you explore, how
[53.38s -> 56.48s]  you fit your model, how you fit your value function, how you update your
[56.48s -> 61.16s]  policy. Each of these trade-offs and each of these choices often introduce
[61.16s -> 64.80s]  additional hyperparameters, which can sometimes be difficult to select for
[64.80s -> 69.40s]  your particular problem. Different methods will also have different
[69.40s -> 74.40s]  assumptions. For example, do they handle stochastic environments or can they
[74.40s -> 78.28s]  only handle deterministic environments? Do they handle continuous states and
[78.28s -> 81.88s]  actions? Can they only handle discrete actions or can they only handle
[81.88s -> 88.32s]  discrete states? Do they handle episodic problems, meaning problems with a fixed
[88.32s -> 92.52s]  capital T horizon, or do they handle infinite horizon problems where T goes
[92.52s -> 97.78s]  to infinity, or both? And different things are easier hard in different
[97.78s -> 101.96s]  settings. For example, in some settings it might be easier to represent a
[101.96s -> 105.84s]  policy, even if the physics of the environment are very very complex, while
[105.84s -> 109.48s]  in other settings it might be easier to learn a model than it is to learn the
[109.48s -> 116.00s]  policy directly. Each of these trade-offs will involve making some set of design
[116.00s -> 120.56s]  choices. For instance, you might opt for an algorithm that is not very sample
[120.56s -> 123.76s]  efficient for the sake of having something that is easier to use, or
[123.76s -> 126.52s]  maybe for the sake of having something that can handle stochastic and
[126.52s -> 130.72s]  partially observed problems. Or you might opt for a very efficient algorithm
[130.72s -> 134.76s]  because your samples are very expensive, but then be willing to
[134.76s -> 138.76s]  accommodate some other limitations like, for example, only allowing for discrete
[138.76s -> 142.80s]  actions. So typically we have to make these trade-offs depending on the
[142.80s -> 147.32s]  particular problem that we're facing. Let's talk about sample efficiency
[147.32s -> 151.44s]  first, because that's a pretty big one. So sample efficiency refers to how
[151.44s -> 155.92s]  many samples we need to obtain a good policy. Basically how many times do we
[155.92s -> 160.40s]  have to sample from our policy until we can make it perform well? That's the
[160.40s -> 165.36s]  orange box. One of the most important questions in determining the sample
[165.36s -> 168.72s]  efficiency of an algorithm is whether the algorithm is what's called an
[168.72s -> 173.64s]  off-policy algorithm or not. An off-policy algorithm is an algorithm
[173.64s -> 178.84s]  that can improve the policy by using previously collected samples. An
[178.84s -> 182.88s]  on-policy algorithm has to throw out all of its samples each time the policy
[182.88s -> 187.64s]  changes, even a little bit, and generate new samples. For this reason
[187.64s -> 193.04s]  on-policy algorithms can be a lot less efficient. For instance, a policy
[193.04s -> 196.48s]  gradient algorithm, which is an on-policy algorithm, must collect new
[196.48s -> 200.08s]  samples each time it takes a gradient step on the policy, because each time
[200.08s -> 206.56s]  the policy changes even a little bit, new samples must be collected. So in
[206.56s -> 212.96s]  general, if we want to look at a kind of spectrum of with more efficient
[212.96s -> 216.24s]  algorithms on the left and less efficient algorithms on the right, a
[216.24s -> 219.68s]  major dividing line on the spectrum is whether it's an on-policy or an
[219.68s -> 224.12s]  off-policy algorithm, where on the extreme end of less efficient algorithms
[224.12s -> 228.52s]  will be things like evolutionary or gradient-free methods, then on-policy
[228.52s -> 232.16s]  policy gradient algorithms, then actor-critic style methods, which can be
[232.16s -> 236.60s]  either on-policy or off-policy, then purely off-policy methods like Q
[236.60s -> 241.32s]  learning, then maybe model-based deep RL methods, model-based shallow RL methods,
[241.32s -> 246.44s]  and so on. But then we could say, well, why would we ever want to use a less
[246.44s -> 250.20s]  efficient algorithm? So it seems like we should just go with the stuff on the
[250.20s -> 255.44s]  left end of the spectrum. Well, it's because the other trade-offs might not
[255.44s -> 260.28s]  be in our favor as we move to the left. For example, wall clock time, the
[260.28s -> 263.72s]  amount of computation the algorithm needs, is not the same as sample
[263.72s -> 267.96s]  efficiency. So maybe generating samples for your application is actually very
[267.96s -> 271.48s]  cheap. Maybe you're using a very very fast simulator. For example, if you're
[271.48s -> 275.28s]  learning how to play a game like chess, simulating chess is very very fast.
[275.28s -> 279.68s]  So most of your computation time will go into updating your value
[279.68s -> 284.24s]  functions, models, and policies. In that case, you probably don't care nearly as
[284.24s -> 288.88s]  much about sample efficiency. And interestingly enough, the wall clock time
[288.88s -> 293.12s]  for these algorithms is often flipped. So if your simulation is very cheap, you
[293.12s -> 295.72s]  might actually find the stuff on the right end of the spectrum to be
[295.72s -> 298.70s]  computationally less expensive, and the stuff on the left side of the
[298.70s -> 305.84s]  spectrum to be computationally much more expensive. Stability and ease of use.
[305.88s -> 310.52s]  When it comes to stability and ease of use, we might ask questions like, does our
[310.52s -> 314.32s]  algorithm converge? Meaning if we run it long enough, is it guaranteed to
[314.32s -> 318.08s]  eventually converge to a fixed solution, or will it keep oscillating or
[318.08s -> 322.76s]  diverging? And if it does converge, what does it converge to? Does it converge to
[322.76s -> 326.80s]  a local optimum of the RL objective, or a local optimum of any other well-defined
[326.80s -> 334.36s]  objective? And does it converge every time? Coming from an optimization or
[334.40s -> 338.08s]  supervised learning background, you might wonder at this point, why is any of
[338.08s -> 341.68s]  this even a question? Because typically when we deal with supervised learning
[341.68s -> 346.68s]  or kind of well-defined, especially convex optimization methods, essentially we
[346.68s -> 350.60s]  only care about methods that converge. In reinforcement learning, convergent
[350.60s -> 353.62s]  algorithms are actually a rare luxury, and many methods that we use in
[353.62s -> 360.22s]  practice are not guaranteed to converge in general. So the reason for this is
[360.22s -> 363.60s]  that reinforcement learning often is not pure gradient descent or pure
[363.64s -> 367.24s]  gradient ascent. Many reinforcement learning algorithms are actually fixed
[367.24s -> 370.76s]  point algorithms that only carry convergence guarantees under very
[370.76s -> 374.54s]  simplified, tabular, discrete state assumptions, which often do not hold in
[374.54s -> 379.64s]  practice. And in theory, the convergence of many of the most popular RL
[379.64s -> 385.36s]  algorithms, such as Q-learning algorithms, is actually an open problem. So Q
[385.36s -> 389.72s]  learning is a fixed point iteration. Model-based reinforcement learning is
[389.72s -> 392.86s]  a kind of a peculiar case because the model is not actually optimized with
[392.90s -> 397.34s]  respect to the RL objective. The model is optimized to be an accurate model. The
[397.34s -> 400.66s]  model training itself is convergent, but there's no guarantee that getting a
[400.66s -> 406.74s]  better model will actually result in a better reward value. Policy gradient
[406.74s -> 410.70s]  is gradient descent or technically gradient ascent, but also the least
[410.70s -> 418.50s]  efficient of the bunch. Value function fitting is a fixed-point iteration and
[418.50s -> 421.66s]  at best it minimizes error of fit. It minimizes what's called Bellman error,
[421.66s -> 424.74s]  meaning is your value function predicting values accurately, but that's
[424.74s -> 428.14s]  not the same as saying does your value function produce a policy with good
[428.14s -> 431.74s]  rewards. And at worst, value function fitting doesn't even minimize the
[431.74s -> 435.82s]  Bellman error. At worst, it actually might even diverge. Many popular deep RL
[435.82s -> 438.90s]  value fitting algorithms are not guaranteed to converge to anything in
[438.90s -> 444.10s]  the nonlinear case, in the case where you use neural networks. Model-based RL,
[444.10s -> 448.94s]  the model minimizes error of fit, which will definitely converge to a good
[448.94s -> 452.42s]  model, but there's no guarantee that a good model will lead to a better policy.
[452.42s -> 456.02s]  Policy gradient is the only one that actually performs gradient ascent on
[456.02s -> 460.34s]  the true objective, but as I said it's the least efficient of the bunch.
[460.34s -> 466.56s]  Assumptions. One common assumption that many RL algorithms will make is full
[466.56s -> 469.90s]  observability, meaning that you have access to states rather than
[469.90s -> 474.18s]  observations, or, but another way, the thing that you're observing satisfies
[474.18s -> 480.30s]  the Markov property. So no car is driving in front of cheetahs. This is
[480.30s -> 484.14s]  generally assumed by most value function fitting methods. It can be
[484.14s -> 487.50s]  mitigated by adding things like recurrence and memory, but in general
[487.50s -> 492.38s]  can be a challenge. Another common assumption, this one is common with
[492.38s -> 496.94s]  policy gradient methods, is episodic learning. So here's a robot performing
[496.94s -> 501.14s]  episodic learning. You can see that it makes a trial, then resets, and then
[501.14s -> 506.28s]  makes another trial. So this ability to reset and try again repeatedly is often
[506.28s -> 509.90s]  assumed by pure policy gradient methods, and although it's not technically
[509.90s -> 513.14s]  assumed by most value-based methods, they tend to work best when this
[513.14s -> 518.86s]  assumption is satisfied. It's also assumed by some model-based RL algorithms.
[518.86s -> 522.74s]  Another common assumption, very common in model-based methods especially, is
[522.74s -> 528.14s]  continuity or smoothness. This is assumed by some continuous value
[528.14s -> 531.70s]  functional learning methods, and it's often assumed by model-based RL
[531.70s -> 534.98s]  methods derived from optimal control, which really require continuity or
[534.98s -> 540.42s]  smoothness to work well. So as we cover various RL algorithms over the
[540.42s -> 543.74s]  next few weeks, I'll point out some of these assumptions as we go, but keep in
[543.74s -> 546.46s]  mind that many of these methods will differ in the kinds of assumptions
[546.46s -> 550.46s]  they make, and also how rigidly these assumptions must be satisfied in order
[550.46s -> 553.94s]  for those methods to work well in practice.
