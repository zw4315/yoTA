# Detected language: en (p=1.00)

[0.00s -> 6.78s]  All right, so the first algorithm that I'm going to kind of dive more deeply into is
[6.78s -> 12.14s]  going to tackle this question, how do we learn without a reward function by proposing
[12.14s -> 14.22s]  and reaching goals?
[14.22s -> 19.26s]  And as I mentioned in the beginning, this lecture is really intended more to discuss
[19.26s -> 23.60s]  sort of cutting-edge research topics and maybe provide a slightly different perspective
[23.60s -> 25.62s]  for thinking about exploration.
[25.62s -> 30.68s]  So I won't actually discuss the algorithm in sort of enough detail to implement it, but
[30.68s -> 34.44s]  hopefully enough detail for you to kind of understand the main ideas.
[34.44s -> 39.12s]  But I will have references to papers at the bottom, and if you want to get all the details
[39.12s -> 41.98s]  then I would encourage you to read those papers.
[41.98s -> 47.64s]  But you know, think of this more as a way to get a perspective on how we can approach
[47.64s -> 53.60s]  this unsupervised exploration problem mathematically, less as a specific tutorial about a particular
[53.60s -> 56.44s]  method that you actually should be using.
[56.44s -> 61.92s]  All right, so the example scenario again that we're dealing with is this setting where
[61.92s -> 66.64s]  you have a robot, you put it in your kitchen, it's supposed to spend the day practicing
[66.64s -> 70.94s]  various skills, and then in the evening when you come home you're going to give
[70.94s -> 75.50s]  it a task, and perhaps you will ask it to do the dishes and should somehow utilize
[75.50s -> 80.50s]  the experience that it acquired to perform that task.
[80.50s -> 84.88s]  Now one fairly mundane thing that we have to figure out for this before we can even
[84.88s -> 90.78s]  get started is how we're going to actually propose, how we're actually going to command
[90.78s -> 94.40s]  goals to the robot once the learning is finished.
[94.40s -> 99.94s]  So if you want sort of a real world analogy, maybe you can think of it like this.
[99.94s -> 104.92s]  Maybe you're going to show the agent an image of the situation that you would like
[104.92s -> 106.86s]  it to reach.
[106.86s -> 112.38s]  In RL parlance, this would amount to giving it the observation or the state that constitutes
[112.38s -> 114.82s]  the goal for the task.
[114.82s -> 118.70s]  So what you would like is you would like to somehow have the agent learn something
[118.70s -> 123.38s]  that enables it to accomplish whatever goal you give it, and the goal will be specified
[123.38s -> 124.38s]  by a state.
[124.38s -> 128.68s]  If we're talking about images, maybe it's an image of the desired outcome.
[128.68s -> 133.26s]  This is not necessarily the best way to communicate with autonomous agents, but it just allows
[133.26s -> 135.68s]  us to nail down something very concrete.
[135.68s -> 140.10s]  The problem will be given a state, the agent should reach that state.
[140.10s -> 144.92s]  And then the unsupervised learning phase should train up a policy that would allow
[144.92s -> 151.24s]  the agent to reach whatever state you would care to command it.
[151.24s -> 158.22s]  Now as a technical detail, we need some mechanism for comparing different states.
[158.22s -> 163.96s]  If those states are very complex, like images, just like we saw in the exploration lecture
[163.96s -> 168.24s]  on Monday, we need some notion of similarity between those states, because in general in
[168.24s -> 172.64s]  high-dimensional or continuous spaces, every state will be unique.
[172.64s -> 176.20s]  So there are many ways to deal with this problem, but the way that we'll deal with
[176.20s -> 180.84s]  it for now is we'll say, well, let's just train some kind of generative model.
[180.84s -> 183.70s]  The particular generative model I'll use as a working example is something called
[183.70s -> 187.38s]  a variational autoencoder, which we'll cover a few weeks from now.
[187.38s -> 189.00s]  But there are many other choices.
[189.00s -> 194.56s]  And we'll just assume that this generative model has some latent variable representation
[194.56s -> 195.68s]  of your image.
[195.68s -> 200.84s]  So if your image is x, you can also think of it as a state s, then your latent variable
[200.84s -> 205.60s]  model will construct some latent variable representation of that state, which I'm going
[205.60s -> 207.44s]  to note as z.
[207.44s -> 212.48s]  So z would be sort of a compact vector that describes what's going on in the scene, and
[212.48s -> 218.76s]  we'll assume that that vector is at least somewhat well-behaved, meaning that similar,
[218.76s -> 223.46s]  functionally similar states will lie close together in that latent space.
[223.46s -> 226.14s]  But there are many ways to get this effect.
[226.14s -> 228.90s]  All right.
[228.90s -> 233.12s]  And then of course the main thing that we're concerned with is we would like our agent
[233.12s -> 238.74s]  to basically have this unsupervised training phase where before we even specify any goals
[238.74s -> 244.08s]  that it should accomplish, it can sort of imagine its own goals, propose those goals
[244.08s -> 249.56s]  to itself, attempt to reach them, and as a result acquire a goal-reaching policy without
[249.56s -> 253.48s]  any manual supervision, without any reward supervision.
[253.48s -> 257.40s]  So intuitively what it's going to be doing is it's going to be using this latent space
[257.40s -> 262.64s]  to propose potential z-vectors that it could try to treat as goals, attempt to reach
[262.64s -> 266.64s]  those goals, and as a result improve its policy.
[267.54s -> 272.14s]  Okay, so let's try to sketch out what such an algorithm might look like.
[272.14s -> 277.78s]  We're going to have our variational autoencoder as our generative model, so that has a distribution
[277.78s -> 281.60s]  x given z, which is a distribution over images given latent codes.
[281.60s -> 286.70s]  You can also think of it as s given z, so I'm going to use x here, but s means
[286.70s -> 288.54s]  the same thing.
[288.54s -> 292.22s]  And then we have our latent variable distribution p of z.
[292.22s -> 295.58s]  And when you train a variational autoencoder, as we'll learn a few weeks from now, you
[295.60s -> 300.60s]  also need an inference network that maps back to z's from states.
[300.60s -> 305.60s]  So if you have a generative model like this, one of the ways you could propose a goal
[305.60s -> 307.66s]  is you could just sample it from the model.
[307.66s -> 312.40s]  So you could sample your latent variable from the latent variable prior, so sample
[312.40s -> 320.40s]  zg from p of z, and then reconstruct the corresponding image by sampling xg from d-theta
[320.40s -> 321.84s]  xg given zg.
[321.84s -> 323.96s]  So that'll give you an imagined image.
[323.96s -> 327.70s]  And again, you don't have to do this with VAEs, any kind of generative model would work,
[327.70s -> 331.28s]  something that can propose a goal.
[331.28s -> 334.86s]  And then you could attempt to reach that goal using a policy.
[334.86s -> 341.56s]  So your policy now would be a conditional policy, so it would be a distribution over
[341.56s -> 347.68s]  actions given the current image x and given the goal xg.
[347.68s -> 352.02s]  And when you attempt to reach the goal using this policy, the policy may or may
[352.02s -> 353.90s]  not succeed.
[353.90s -> 358.06s]  So let's say that it reaches some state, and we'll call that state x-bar.
[358.06s -> 362.22s]  Ideally we'd like x-bar to be equal to xg, but in general it might not be.
[362.22s -> 365.88s]  In fact, xg might not even be a real image, it may be impossible to reach.
[365.88s -> 370.50s]  So you'll get some other image x-bar.
[370.50s -> 374.02s]  And in the process of running that policy, you'll of course collect data which you can
[374.02s -> 378.06s]  use to update your policy, maybe using something like a Q-learning algorithm, like what
[378.06s -> 381.08s]  you're doing for homework 3.
[381.08s -> 383.78s]  And you can also use that data to update your generative model.
[383.78s -> 387.22s]  So if in the course of attempting to reach that goal, you saw some other images that
[387.22s -> 391.86s]  you hadn't seen before, incorporating that data to update your generative model might
[391.86s -> 396.22s]  give you a better generative model that can propose more interesting goals.
[396.22s -> 398.72s]  And then you can repeat this process.
[398.72s -> 404.34s]  So this is a basic sketch of an algorithm that utilizes a goal proposal mechanism, an
[404.34s -> 410.10s]  unsupervised goal proposal mechanism, and a goal condition policy, and the interaction
[410.10s -> 415.18s]  of these two things leads it to propose goals and then attempt to reach them.
[415.18s -> 420.94s]  Okay, but there's a little bit of a problem with this recipe, because the generative
[420.94s -> 424.40s]  model is being trained on the data that you've seen.
[424.40s -> 428.34s]  So it's going to generate data that looks very much like the data that you've seen,
[428.34s -> 433.12s]  which means that if your agent figures out how to do one very specific thing, maybe
[433.12s -> 437.14s]  it figures out how to pick up a mug, now it has lots of data of picking up that
[437.18s -> 441.42s]  mug, and when it generates additional images, additional goals, it'll generate lots more
[441.42s -> 445.94s]  data of picking up that same mug and might not bother with other things.
[445.94s -> 449.38s]  So this is where we can bring in some ideas related to what we covered in the lecture
[449.38s -> 453.26s]  on Monday, some of these exploration ideas.
[453.26s -> 456.22s]  Let's imagine that we have this 2D navigation scenario.
[456.22s -> 461.10s]  So the little circles represent states that you visited.
[461.10s -> 465.70s]  Intuitively, what you would like to do is you would like to take this data set and
[466.26s -> 472.82s]  modify it, skew it some way, to up-weight the rarely seen states, very much like a novelty-seeking
[472.82s -> 474.66s]  exploration that we discussed on Monday.
[474.66s -> 479.34s]  And if you can do this, if you can up-weight the rarely seen states before fitting your
[479.34s -> 483.34s]  generative model, then when you fit your generative model, it should assign higher
[483.34s -> 487.70s]  probability to the tails of this distribution, so that when you propose new goals, it'll
[487.70s -> 492.94s]  sort of broaden it out and visit more states there on the fringes, and hopefully
[492.94s -> 497.42s]  expand its repertoire of states that it can reach.
[497.42s -> 504.26s]  So this is the intuition behind what we want to make such an algorithm really work.
[504.26s -> 506.02s]  So how do we do this?
[506.02s -> 509.94s]  Well, the idea is that we're going to modify step 4.
[509.94s -> 514.86s]  Instead of blindly using all the data we've collected to fit our generative model, we're
[514.86s -> 518.78s]  going to actually weight that data.
[518.78s -> 521.82s]  So that's basically what this step will be.
[521.82s -> 526.18s]  So the standard way to fit our generative model is basically maximum likelihood estimation.
[526.18s -> 530.94s]  Find the generative model that maximizes the expected log probability of the states
[530.94s -> 535.58s]  that you actually reached, which I'm denoting here with x-bar.
[535.58s -> 540.22s]  Instead you could imagine having a weighted maximum likelihood learning procedure, where
[540.22s -> 543.66s]  you train your generative model to assign high likelihood to the states that you've
[543.66s -> 549.42s]  seen x-bar, but weighted by some weighting function w of x-bar.
[549.42s -> 554.58s]  And intuitively, you would like that weighting function to up-weight those states or those
[554.58s -> 559.54s]  images that have been seen rarely.
[559.54s -> 565.34s]  What do we have at our disposal that can tell us how rarely something has been seen?
[565.34s -> 570.98s]  Well, we're using a generative model to propose these goals, and a generative model
[570.98s -> 576.58s]  should be able to give us a density score, just like when we learned about counts
[576.58s -> 579.02s]  and pseudocounts.
[579.02s -> 584.14s]  So what we can do is we can assign a weight based on the probability density that our
[584.14s -> 588.02s]  current model p-theta assigns to that state x.
[588.02s -> 593.66s]  So we'll set the weight to be p-theta of x-bar raised to some power alpha, where alpha
[593.66s -> 596.66s]  is a negative number.
[596.66s -> 603.42s]  So this will essentially be 1 over p-theta of x-bar to some positive power, or equivalently
[603.42s -> 607.12s]  p-theta of x-bar to some negative power.
[607.12s -> 610.96s]  And one of the things we can prove, I'm not going to go through the proof for this, but
[610.96s -> 616.16s]  the proof is in these papers, it's possible to prove that if you use a negative exponent,
[616.16s -> 622.12s]  then the entropy of p-theta of x will increase, meaning that each time you go around
[622.12s -> 626.08s]  this loop, you'll be proposing broader and broader goals.
[626.08s -> 629.92s]  And if your entropy always increases, that means that you eventually converge to the
[629.92s -> 634.72s]  maximum entropy distribution, which would be a uniform distribution over possible valid
[634.72s -> 635.72s]  states.
[636.72s -> 640.76s]  Now, a uniform distribution over valid states is not the same as a uniform distribution
[640.76s -> 641.76s]  over x.
[641.76s -> 646.40s]  So x might represent an image, totally random images, just kind of static, might not actually
[646.40s -> 647.76s]  be valid states.
[647.76s -> 652.76s]  So what you should be looking for is a uniform distribution over valid states, a uniform distribution
[652.76s -> 654.76s]  over valid images.
[656.76s -> 661.56s]  Okay, now looking at this equation, one thing that might jump out at you is that this
[661.60s -> 667.12s]  looks an awful lot like what we saw when we had pseudocounts and count-based exploration.
[667.12s -> 671.80s]  So if you remember, at count-based exploration, our bonuses had this form like 1 over n
[671.80s -> 674.64s]  of s, or the square root of 1 over n of s.
[674.64s -> 679.16s]  In general, they were of the form n of s raised to some negative power, negative
[679.16s -> 684.16s]  one-half if you have 1 over square root, or negative 1 if you have 1 over n.
[684.16s -> 687.12s]  So this looks an awful lot like that.
[687.68s -> 691.60s]  By raising the p-theta of x-bar to some negative power, we're actually doing something that
[691.60s -> 696.20s]  greatly resembles this count-based exploration, except instead of using it as a reward, we're
[696.20s -> 701.20s]  using it to train our goal-proposal mechanism to propose diverse goals.
[704.20s -> 710.40s]  Alright, so the main change we're going to make is we're going to fit our generative
[710.40s -> 715.00s]  model with this weighting scheme, where the weight is the previous density for that
[715.08s -> 717.08s]  state raised to some negative exponent.
[718.08s -> 723.08s]  Now, one question we could ask is, well, what's the overall objective of this entire procedure?
[723.08s -> 726.68s]  It seems like we laid out a recipe, but in machine learning, we like to think of
[726.68s -> 729.68s]  algorithms as optimizing objectives.
[729.68s -> 732.68s]  So what is the objective for this algorithm?
[734.68s -> 739.68s]  Well, I mentioned that the entropy of the goal distribution will increase every step
[739.68s -> 744.68s]  around this loop, which means that one of the things we're doing is we're maximizing
[745.36s -> 747.36s]  the entropy of the goal distribution.
[747.36s -> 749.36s]  That's good because we want good coverage.
[749.36s -> 751.36s]  We want to cover many, many different goals.
[752.36s -> 756.36s]  So the goals get higher entropy due to this skew-fit procedure.
[757.36s -> 759.36s]  What does the RL part do?
[760.36s -> 765.36s]  Well, your policy, which you can also write as pi of a given s comma g, so it's
[765.36s -> 769.36s]  probability of action given current state and given goal, your policy is trying to
[769.36s -> 773.36s]  reach the goal g, which means that as the policy gets better,
[774.04s -> 779.04s]  the final state, which I'll denote as s here, is going to get closer and closer to g.
[782.04s -> 787.04s]  So that means that the probability of g given your final state
[787.04s -> 789.04s]  becomes more and more deterministic.
[789.04s -> 792.04s]  Essentially, if your policy is very good, you could pose this question.
[792.04s -> 797.04s]  Given the final state s that the policy reached, what is the goal g that it was trying to reach?
[797.04s -> 801.04s]  If the policy is very good, you could just say, well, the goal was probably the thing
[801.72s -> 804.72s]  that it actually reached, because it's a good policy, it's going to reach its goal.
[804.72s -> 809.72s]  So that means that the better the policy is, the easier it is to predict g from s,
[809.72s -> 814.72s]  which means that the entropy of p of g given s is lower.
[816.72s -> 822.72s]  So that means that you're also minimizing the conditional entropy of g given s.
[824.72s -> 828.72s]  And now when we look at this equation, something should jump out at us.
[829.40s -> 834.40s]  If we are maximizing h of g minus h of g given s,
[834.40s -> 839.40s]  that means that we are maximizing the mutual information between s and g.
[840.40s -> 845.40s]  And maximizing the mutual information between s and g leads to good exploration,
[845.40s -> 848.40s]  because we're maximizing the entropy of our goals,
[848.40s -> 852.40s]  so we have coverage of all possible goals, and effective goal reaching,
[852.40s -> 856.40s]  because we're minimizing the entropy of the goal given the state.
[857.08s -> 860.08s]  So that's another way that this concept of mutual information
[860.08s -> 864.08s]  leads to an elegant and very simple objective that quantifies exploration performance.
[864.08s -> 868.08s]  Essentially in this case, the mutual information between states and goals
[868.08s -> 873.08s]  quantifies how effectively we can reach the most diverse possible set of goals.
[876.08s -> 879.08s]  All right, now for a quick robot video.
[879.08s -> 882.08s]  This was an actual research paper that we did a few years back,
[882.08s -> 886.08s]  and what we did with this kind of objective is we put the robot in front of a door,
[886.76s -> 890.76s]  so that hook-shaped thing, that's the gripper for the robot,
[890.76s -> 893.76s]  but we didn't tell it that it needs to open the door,
[893.76s -> 895.76s]  it was just supposed to figure this out on its own.
[895.76s -> 898.76s]  In the top row, you can see the goals that it's suggesting to itself,
[898.76s -> 900.76s]  the actual images that it's generating,
[900.76s -> 902.76s]  and in the bottom row, you can see the behavior.
[902.76s -> 905.76s]  And at zero hours, it's not really doing very much,
[905.76s -> 907.76s]  it's kind of wiggling around in front of the door.
[907.76s -> 910.76s]  Ten hours in, it tends to touch the door handle
[910.76s -> 912.76s]  and occasionally gets the door open,
[913.44s -> 916.44s]  and for 25 hours, it pretty reliably messes with the door
[916.44s -> 918.44s]  and opens it to all different angles.
[918.44s -> 920.44s]  And when the system is fully trained,
[920.44s -> 922.44s]  then you could give it an image of the door
[922.44s -> 925.44s]  open to a different angle, and it will successfully open it to that angle.
