# Detected language: en (p=1.00)

[0.00s -> 5.44s]  All right, let's talk about how we can actually solve this inference problem. So
[5.44s -> 9.72s]  as we discussed in the previous part, there are really three inference
[9.72s -> 13.20s]  problems that we're interested in in this control-as-inference graphical
[13.20s -> 17.04s]  model. The first is to compute backward messages, which are the
[17.04s -> 23.04s]  probability of optimality from little t to capital T given S, D and A, T. The
[23.04s -> 26.92s]  second one is to compute a policy, the probability of an action given a
[26.92s -> 31.16s]  state and given that the entire trajectory is optimal, and that's what
[31.16s -> 35.04s]  we're interested in if we want to solve the forward RL problem. And we'll see how
[35.04s -> 38.40s]  this can be recovered more or less directly from the backward messages. And
[38.40s -> 42.88s]  then the third one is to compute the forward messages, the probability of S, T
[42.88s -> 47.68s]  given optimality at 1 through T minus 1. And this will be important
[47.68s -> 51.72s]  later for inverse RL. So let's start with the backward messages. These are
[51.72s -> 55.64s]  really the most important ones, because if you can calculate these, then you can
[55.68s -> 61.24s]  recover near-optimal policies. And the way that we will derive the backward
[61.24s -> 64.60s]  message is just with a bit of recursion and probability theory and a
[64.60s -> 69.08s]  little bit of algebra. So first we can take this equation for the backward
[69.08s -> 74.36s]  message, and we can insert the next state S, T plus 1 and integrate it out. So
[74.36s -> 77.96s]  the backward message is equal to the integral over all values of S, T plus 1
[78.04s -> 85.96s]  of P of O T through big T comma S, T plus 1 given S, T, A, T. And now what we'll do is
[85.96s -> 90.40s]  we'll factorize this distribution using the CPDs we have in our model. And our
[90.40s -> 94.92s]  goal in doing this factorization is to recover a recursive expression where
[94.92s -> 102.64s]  we can represent beta T S, T, A, T as some function of beta T plus 1 S, T plus 1
[102.80s -> 108.88s]  A, T plus 1. So in order to factorize this expression, the thing that we have
[108.88s -> 114.94s]  to note is that future optimality variables, namely O T plus 1 through
[114.94s -> 120.76s]  capital T, are independent of everything in the past when conditioned
[120.76s -> 124.64s]  on S, T plus 1. And we can see this just from inspection of the graphical
[124.64s -> 128.56s]  model. And that means that we can factorize this expression into three
[128.56s -> 133.48s]  parts. The first part is the probability of all of the optimality
[133.48s -> 140.28s]  variables from T plus 1 until the end given S, T plus 1. And we know that we
[140.28s -> 145.00s]  don't have to condition this on S, T and A, T because given S, T plus 1, all the
[145.00s -> 150.68s]  future optimality variables are independent of S, T and A, T. Then we have the
[150.68s -> 155.84s]  probability of S, T plus 1 given S, T, A, T. That's just the transition
[155.84s -> 159.10s]  probability that we already know. And then we have the probability of the
[159.10s -> 164.64s]  remaining optimality variable O T given S, T, A, T. And that just corresponds to
[164.64s -> 167.60s]  the exponentiated reward because that's just one of the CBDs in our
[167.60s -> 173.32s]  graphical model. So we know this thing. We know this thing, that's our
[173.32s -> 177.32s]  transition probability. Although when we're doing RL, we might want to compute
[177.32s -> 180.68s]  backward messages without functional knowledge of this probability. For now
[180.68s -> 186.80s]  we'll assume that we know it. And that just leaves this remaining term. So the
[186.80s -> 192.30s]  probability of optimality from T plus 1 to capital T given S, T plus 1 can
[192.30s -> 196.20s]  itself be written as the probability of optimality from T plus 1 to capital
[196.20s -> 202.08s]  T given S, T plus 1 and A, T plus 1 times the probability of A, T plus 1
[202.08s -> 209.36s]  given S, T plus 1. Now this part is simply the backward message of time step
[209.36s -> 215.60s]  T plus 1. So we've already gotten a little bit better. We've figured out
[215.60s -> 219.80s]  something that's mostly a recursive expression, except it has this kind of
[219.80s -> 223.60s]  weird term that we haven't defined yet. What's the probability of an action
[223.60s -> 229.84s]  given a state? Now crucially this is not a policy. This is saying which
[229.84s -> 235.00s]  actions are likely a priori, meaning if you don't know whether you're optimal
[235.00s -> 242.04s]  or not, how likely are you to take a particular action? In general, if you
[242.04s -> 245.00s]  don't know whether you're being optimal or not, you probably wouldn't
[245.00s -> 249.76s]  know anything about which actions are more likely or less likely. So we
[249.76s -> 254.36s]  could define this term, we could define an action prior, A p of A given S,
[254.36s -> 259.72s]  but we'll assume that it's uniform for now. And this is a reasonable
[259.72s -> 264.84s]  assumption for a couple of reasons. First, if you don't know anything about what
[264.84s -> 268.24s]  the monkey is trying to do, then you probably can't say much about which
[268.24s -> 272.44s]  action is more or less likely to perform. Second, and perhaps more
[272.44s -> 277.20s]  mathematically, if you want to impose an action prior, it turns out that you
[277.20s -> 281.40s]  can equivalently modify the reward function and keep a uniform action
[281.40s -> 285.16s]  prior and get exactly the same solution. This result is a little more
[285.16s -> 288.40s]  subtle and I would leave it as an exercise to you guys to work this out
[288.40s -> 293.44s]  on paper. But long story short, for now we'll assume that the action prior p of
[293.48s -> 298.28s]  A t plus 1 given S t plus 1 is uniform, which means that it's a constant, which
[298.28s -> 303.96s]  means that we can disregard it. Now this does not mean that that policy is
[303.96s -> 308.80s]  uniform, because remember our policy is the posterior distribution A t given S t
[308.80s -> 313.84s]  comma O 1 through capital T. The action prior is just the probability of the
[313.84s -> 320.64s]  actions a priori before you know whether you're optimal or not. All right, so now
[320.72s -> 324.16s]  that we've gotten rid of the action prior and we've expressed everything in
[324.16s -> 328.80s]  terms of probabilities of transitions, probabilities of optimality, and
[328.80s -> 334.80s]  recursively in terms of future backward messages, we can write down a
[334.80s -> 339.32s]  recursive algorithm for computing the backward messages. It's a for loop that
[339.32s -> 343.92s]  starts at the end of the trajectory and steps backward until the beginning,
[343.92s -> 348.96s]  and at every time step from the end until the beginning, we calculate the
[348.96s -> 354.60s]  backward message at time step t as the probability of optimality now given S t
[354.60s -> 361.60s]  A t multiplied by the expected value over S t plus 1 of a quantity that I'm
[361.60s -> 367.60s]  going to call beta t plus 1 S t plus 1. Beta t plus 1 of S t plus 1 is a
[367.60s -> 373.88s]  state backward message. So that's this equation basically. And the state
[373.88s -> 378.44s]  backward message is just the expected value of the state action backward
[378.44s -> 385.20s]  message in expectation over the actions distributed according to the uniform
[385.20s -> 391.12s]  action prior. So that's just this thing. And if we alternate these two steps,
[391.12s -> 395.32s]  then we can recursively calculate the backward message all the way from the
[395.32s -> 400.76s]  end to the beginning. The very last backward message, beta capital T, is just
[400.76s -> 406.52s]  the last reward because the probability of O capital T given S capital T comma A
[406.52s -> 412.12s]  capital T is actually a CPD that is already present in your model. All right,
[412.12s -> 416.24s]  now let's take a closer look at this backward pass. So here's the
[416.24s -> 422.76s]  recursive algorithm that I derived on the previous slide, and we're going to
[422.76s -> 426.08s]  make some definitions with some very suggestive names that will help us
[426.08s -> 432.20s]  understand this algorithm. First, we're going to define V t of S t as the
[432.24s -> 440.32s]  logarithm of beta t S t. And then we'll define Q t of S t A t as the logarithm of
[440.32s -> 449.92s]  beta t S t A t. And now we can write these equations in log space. So if we
[449.92s -> 453.92s]  write the equation for the state backward message in log space, then we
[453.92s -> 458.36s]  get V t of S t is equal to the logarithm of the integral of the
[458.36s -> 464.44s]  exponentiated Q t S t A t. So that's kind of interesting. What a funny
[464.44s -> 469.12s]  equation. At first it doesn't seem like it's doing something particularly
[469.12s -> 477.56s]  intuitive, but imagine that the values of Q t are quite large. If you take the
[477.56s -> 484.32s]  sum of the exponentials of a bunch of large values, then the largest of
[484.32s -> 488.00s]  those values will dominate that sum, which means when you then take the
[488.04s -> 494.44s]  logarithm, you will recover a number that is close to the largest among the
[494.44s -> 500.84s]  values whose exponentials you summed up. In the extreme case, you could imagine
[500.84s -> 506.00s]  that as the Q t's become closer and closer to infinity, then the log of the
[506.00s -> 511.64s]  sum of the exponentials becomes closer and closer to a max. So in fact we
[511.64s -> 517.84s]  can call this log of sum of exponentials a kind of a soft max. This is not the
[517.88s -> 522.28s]  same as the soft max that we use as a loss function in deep learning, it is a soft
[522.28s -> 530.80s]  relaxation of the max operator. So this expression for V t of S t approaches the
[530.80s -> 536.68s]  max over A of Q t as Q t gets bigger and bigger. So that's pretty
[536.68s -> 540.24s]  interesting. We saw in reinforcement learning that the optimal value
[540.24s -> 544.16s]  function is the max of the optimal Q function. Now we see that in the
[544.20s -> 549.60s]  inference perspective, the value function is the soft max of the Q
[549.60s -> 554.48s]  function, which kind of makes sense. We want to soften our notion of
[554.48s -> 558.72s]  optimality so that slightly suboptimal things are still possible.
[558.72s -> 563.40s]  Let's talk about the other expression. So if we write the other expression in
[563.40s -> 570.60s]  log space, then we get the following equation. Q t of S t A t is equal to R
[570.68s -> 576.96s]  of S t A t plus the log of the expected value of the exponential of V
[576.96s -> 582.28s]  t plus 1 at S t plus 1. So that looks a lot like a Bellman
[582.28s -> 587.60s]  backup because it has a reward and an expectation term, except that now you
[587.60s -> 591.92s]  have this log around the expectation. Okay, so let's try to understand this
[591.92s -> 596.08s]  equation a little bit better. There is a particular special case where it is
[596.08s -> 600.28s]  exactly the same as the Bellman equation. Take a moment to think about
[600.28s -> 604.44s]  what that special case would be. In which setting is the equation that I have on
[604.44s -> 611.00s]  the right side of the slide exactly the same as the Bellman backup? Well, it
[611.00s -> 614.68s]  turns out to be the case in the setting where the next state is a
[614.68s -> 618.76s]  deterministic function of the current state in action, because if the next
[618.76s -> 622.26s]  state is a deterministic function of the current state in action, then the
[622.26s -> 626.96s]  expected value only has one non-zero element in the sum, which means that
[626.96s -> 633.64s]  the log and the exponent cancel out. So in the deterministic setting it
[633.64s -> 638.76s]  is exactly the Bellman equation. So we saw before how the value
[638.76s -> 643.76s]  iteration algorithm alternates between computing the Q function using the
[643.76s -> 648.56s]  Bellman equation and computing the value function by taking a max. In the
[648.56s -> 652.04s]  case of a deterministic transition, this expression for Q is basically the
[652.04s -> 658.32s]  same as the Bellman backup. In the case of a stochastic transition, you actually
[658.32s -> 662.92s]  get a kind of optimistic transition. Notice that this is also a log sum x,
[662.92s -> 667.28s]  which means that these target values will be dominated by the next state
[667.28s -> 670.44s]  that has the largest value. So if you have the potential to get lucky, you will
[670.44s -> 676.08s]  think that that's more likely. That's not actually a good idea, and we can
[676.08s -> 681.00s]  actually improve our inference-based process by fixing this issue. But
[681.04s -> 684.80s]  intuitively, the reason that this problem happens is because when we ask
[684.80s -> 691.00s]  the question, how likely are you to be optimal given a particular state of
[691.00s -> 695.32s]  action, that doesn't distinguish between being optimal because you got lucky
[695.32s -> 699.48s]  versus being optimal because you took the right action. We'll come back to
[699.48s -> 703.60s]  this later and discuss the stochastic case in more detail on how to fix
[703.60s -> 707.32s]  this problem. But for now, the thing I would note is that at least the
[707.32s -> 712.08s]  deterministic case perfectly matches the classic notion of value iteration we
[712.08s -> 717.12s]  saw before, with the exception that we use a softmax instead of a hardmax.
[717.12s -> 722.88s]  All right, so just to summarize the backward pass, we have these backward
[722.88s -> 727.92s]  messages beta t of s t a t, which are equal to the probability of O little t
[727.92s -> 732.36s]  through capital T, given s t a t. That's the probability that we can be
[732.36s -> 735.88s]  optimal at steps little t through capital T, given that we take action
[735.92s -> 746.04s]  a t in state s t. And we can calculate this via recursion by repeating these
[746.04s -> 751.48s]  two steps, computing the state action backward message as the expected value
[751.48s -> 756.20s]  of the next state message times p of O t given s t a t, and computing the
[756.20s -> 760.04s]  state backward message as the expected value of the state action backward
[760.04s -> 767.48s]  message under the action distribution, which is uniform. So the log of beta t
[767.48s -> 773.76s]  is kind of a Q-function-like object, and we can make this more apparent by
[773.76s -> 780.84s]  setting v t to be log of beta t s t and Q t to be log of beta t s t a t.
[780.84s -> 784.92s]  All right, let's discuss the action prior a little bit more, because I kind
[784.92s -> 787.20s]  of brushed this under the rug a little bit, and this is something where
[787.40s -> 792.60s]  I often get a lot of questions. So what if the action prior is not uniform? What
[792.60s -> 795.96s]  if you believe the monkey is, maybe it's a lazy monkey, that even if it's not
[795.96s -> 801.40s]  being optimal, it's more likely to take small actions than big actions? Well, if
[801.40s -> 806.04s]  the action prior is not uniform, then our expression for v of s t becomes a
[806.04s -> 809.76s]  little bit more complex. Instead of being the log integral of the
[809.76s -> 814.96s]  exponentiated Q, now it's the log integral of the exponentiated Q plus the
[815.00s -> 822.72s]  log probability of the action given the state, where our Q is defined like this.
[822.72s -> 828.44s]  So if we redefine Q to be a different quantity Q tilde, which is given
[828.44s -> 835.72s]  by R plus log P of a t given s t plus the log expectation of the next
[835.72s -> 843.56s]  value, then we get this expression that the corresponding value function is just
[843.64s -> 849.04s]  the log of the integral of the exponentiated Q tilde. So this makes it
[849.04s -> 856.16s]  apparent that if we simply add log P of a t given s t to the reward, and then
[856.16s -> 859.88s]  do everything the same way we did before, as though the action prior was
[859.88s -> 864.00s]  uniform, we'll require, we'll recover exactly the right answer as we would
[864.00s -> 868.52s]  have if we had properly accounted for a non-uniform action prior. And this is
[868.52s -> 871.52s]  basically why we don't worry about the action prior, because we can always
[871.68s -> 876.32s]  construct a different reward function that accounts for the action prior, and
[876.32s -> 881.12s]  then treat it from there on out as though it was uniform. So that's why we basically
[881.12s -> 885.24s]  don't worry about that term. We can always fold the action prior to the
[885.24s -> 889.44s]  reward, and a uniform action prior can therefore be assumed without any loss of
[889.44s -> 896.36s]  generality. All right, next let's talk about how we can recover the
[896.36s -> 901.04s]  policy from the backward messages. So the policy is the probability of a t,
[901.04s -> 909.84s]  given s t, and given that the entire trajectory is optimal. So this is
[909.84s -> 916.80s]  basically our near-optimal policy. Now, as before, we can note that past
[916.80s -> 922.28s]  optimality variables are conditionally independent given the state, which means
[922.28s -> 926.84s]  that we can equivalently write this query as P of a t given s t comma o t
[926.84s -> 930.68s]  through capital T. So basically all of those optimality variables 1 through t
[930.72s -> 938.28s]  minus 1 do not affect a t because they are de-separated by s t. And this makes
[938.28s -> 941.84s]  it maybe a little more obvious why we can recover the policy using only the
[941.84s -> 945.96s]  backward messages, because all of the variables that appear in this expression
[945.96s -> 952.64s]  now were all present in the backward messages. So what we're going to do is
[952.64s -> 956.44s]  we're going to write this conditional probability using the definition of
[956.44s -> 961.64s]  conditional probability as P of a t comma s t given o t through capital T
[961.64s -> 968.16s]  divided by P of s t given o t through capital T. And then what we'll do is we
[968.16s -> 973.04s]  will apply Bayes' rule to both the top and bottom. So it's a little weird
[973.04s -> 975.52s]  because usually you think of applying Bayes' rule to one conditional
[975.52s -> 979.00s]  distribution, but we're going to apply Bayes' rule at the same time to both the
[979.00s -> 985.06s]  numerator and the denominator. So Bayes' rule will allow us to flip the order, so
[985.06s -> 989.62s]  our backward messages are o of t through capital T given a t s t, and we want P
[989.62s -> 993.42s]  of a t s t given o t through capital T, so Bayes' rule allows us to flip those.
[993.42s -> 997.80s]  And that's what we see on the next slide. We apply Bayes' rule to both the
[997.80s -> 1001.18s]  numerator and the denominator, which means that we flip the order of things
[1001.18s -> 1004.34s]  on the left and right and right side of the conditioning bar. We've
[1004.34s -> 1009.38s]  introduced P of a t s t divided by P of o t through capital T in the
[1009.38s -> 1013.86s]  numerator and P of s divided by P of o t through capital T in the
[1013.86s -> 1018.70s]  denominator. And now we're going to eliminate and group some of these terms.
[1018.70s -> 1024.38s]  So first the denominator and Bayes' rule appears both on the top and bottom of
[1024.38s -> 1028.78s]  the fraction, so that goes away, that simplifies very nicely, and that leaves
[1028.78s -> 1034.34s]  us with these two expressions. Now remember that our backward message is
[1034.34s -> 1040.22s]  o t through capital T given s t a t, and our state backward message is o t
[1040.22s -> 1045.26s]  through capital T given s t. So the first fraction in this expression is just the
[1045.26s -> 1050.54s]  ratio of the state action backward message and the state backward message.
[1050.54s -> 1056.10s]  The second ratio is just P of a t given s t, which is our action prior,
[1056.10s -> 1060.78s]  which we've assumed to be uniform, so that the second fraction goes away
[1060.78s -> 1064.58s]  because it's uniform, it's a constant, leaving us with just the first
[1064.58s -> 1071.38s]  fraction. So that means that the optimal policy, P of a t given s t, can simply be
[1071.38s -> 1075.22s]  recovered as the ratio between the state action message and the state
[1075.22s -> 1086.02s]  message. All right, now let's bring back this idea of expressing things
[1086.02s -> 1090.50s]  in log space. So in log space we saw that the logarithm of the state
[1090.50s -> 1094.46s]  action backward message is kind of Q-like, and the logarithm of the state
[1094.46s -> 1101.22s]  backward message is V-like, so what if we plug those into the policy? If we plug
[1101.22s -> 1107.22s]  these into the policy, then we get that pi of a t given s t is equal to the
[1107.22s -> 1110.94s]  exponential of Q divided by the exponential of V, which is just the
[1110.94s -> 1118.10s]  exponential of Q minus V, and Q minus V is an advantage-like function. So
[1118.14s -> 1122.62s]  that's quite appealing. We're now able to express the policy pi of a t given s t
[1122.62s -> 1128.26s]  as the exponential of the advantage s t a t, which means your probability of
[1128.26s -> 1134.38s]  taking an action is the Boltzmann distribution in this soft advantage-like
[1134.38s -> 1139.70s]  function. Actions with higher advantage are more likely, the action with the
[1139.70s -> 1143.14s]  largest advantage is the most likely, and actions become exponentially less
[1143.14s -> 1147.70s]  likely as their advantage decreases. That seems like a fairly intuitive
[1147.70s -> 1154.42s]  notion. All right, so to summarize policy computation, it's the exponential of the
[1154.42s -> 1158.96s]  advantage. You could add in a temperature, so if you put in a 1 over
[1158.96s -> 1165.54s]  alpha in front of the exponent, then you can smoothly interpolate
[1165.54s -> 1170.70s]  between hard optimality and soft optimality. So as alpha goes to 0,
[1170.70s -> 1175.62s]  then the policy becomes deterministic on the optimal action. As the
[1175.62s -> 1178.90s]  alpha goes to 1, then you recover the classic inference framework, so that
[1178.90s -> 1182.10s]  gives you interpolation between standard hard optimality and the soft
[1182.10s -> 1187.30s]  optimality that we learned about. Better actions are more probable, you get
[1187.30s -> 1190.62s]  random tie-breaking, so if two actions have exactly the same advantage, you'll
[1190.62s -> 1193.82s]  take them with equal value, and it's kind of analogous to Boltzmann
[1193.82s -> 1199.06s]  exploration. And of course this approaches the greedy policy as the temperature
[1199.06s -> 1203.06s]  decreases, so if you take alpha down to 0, then you recover the standard greedy
[1203.10s -> 1211.86s]  optimal policy. All right, so lastly let's talk about forward messages. So the
[1211.86s -> 1217.22s]  forward message is the probability of the state given optimality from 1
[1217.22s -> 1221.94s]  through t minus 1. The way that we're going to calculate forward messages
[1221.94s -> 1226.02s]  will be similar to how we calculated backward messages. We'll put in the
[1226.02s -> 1230.34s]  previous time step variables, integrate them out, and recover a recursive
[1230.34s -> 1235.22s]  procedure. So the two variables that I'm going to put in are s t minus 1 and a t
[1235.22s -> 1241.98s]  minus 1, so I get p of s t comma s t minus 1 comma a t minus 1 given o 1
[1241.98s -> 1247.94s]  through t minus 1. Again I'm going to factorize this expression into terms
[1247.94s -> 1253.38s]  that I can figure out. So I'm going to factorize it into s t given s t minus 1
[1253.38s -> 1257.82s]  comma a t minus 1 comma o 1 through t minus 1, so that's just the first
[1257.86s -> 1264.90s]  term. Then the third term p of a t minus 1 given s t minus 1 comma o 1 through t
[1264.90s -> 1270.82s]  minus 1, and then the third term p of s t minus 1 given o 1 through t minus 1, so
[1270.82s -> 1274.34s]  everything is conditional o 1 through t minus 1, and the rest is all chain
[1274.34s -> 1280.32s]  rule. Now the first thing to note is the probability of s t given s t minus 1
[1280.32s -> 1284.46s]  a t minus 1 doesn't depend on o 1 through t minus 1, that's just our
[1284.46s -> 1288.14s]  transition probability, we already know that quantity, so this thing can be
[1288.14s -> 1293.94s]  deleted. So we've collected it like this, we have three terms, the first one we
[1293.94s -> 1298.70s]  know, the second and third are a little harder. So let's write the
[1298.70s -> 1303.22s]  product of the second and third terms. The product of the second and
[1303.22s -> 1309.78s]  third terms in this integral can be written by applying Bayes' rule to both
[1309.86s -> 1318.54s]  expressions, and I'm going to apply Bayes' rule in a slightly funny way. So for the
[1318.54s -> 1324.10s]  first expression I'll do Bayes' rule to flip o t minus 1 and a t minus 1, so
[1324.10s -> 1327.98s]  I'll throw o t minus 1 on the left side and a t minus 1 on the right side, so
[1327.98s -> 1333.26s]  that's the first fraction. When I do that I get p of o t minus 1 given s t
[1333.26s -> 1337.42s]  minus 1 a t minus 1, I know what that is, that's just exponential reward, then I
[1337.42s -> 1342.46s]  get a t minus 1 given s t minus 1, that's my action prior, and then I get my
[1342.46s -> 1350.18s]  denominator o t minus 1 given s t minus 1. When I apply Bayes' rule to the
[1350.18s -> 1354.80s]  second quantity p of s t minus 1 given o 1 through t minus 1, what I'm
[1354.80s -> 1361.34s]  going to do is I'm actually going to flip just the o t minus 1 part and
[1361.34s -> 1365.66s]  flip that on the left side of the conditioning bar, put s t minus 1 on the
[1365.66s -> 1370.38s]  right side, but then I'll keep o 1 through t minus 2 on the right side of
[1370.38s -> 1375.70s]  the conditioning bar. And when I do that then I get p of o t minus 1 given s t
[1375.70s -> 1380.98s]  minus 1, p of s t minus 1 given o 1 through t minus 2, and then a denominator
[1380.98s -> 1387.54s]  which is p of o t minus 1 given o 1 through t minus 2. So this is, the
[1387.54s -> 1390.02s]  reason I'm doing this trick is because now the denominator of the first
[1390.02s -> 1394.18s]  fraction cancels with the o t minus 1 given s t minus 1 term in the
[1394.22s -> 1399.30s]  numerator of the second fraction. The action prior also goes away because
[1399.30s -> 1403.84s]  that's a constant. This term in the numerator is just a backward message
[1403.84s -> 1409.30s]  at the previous time step, and the backward message of the first time step
[1409.30s -> 1415.50s]  is usually known. So now I'm left with only expressions that I know how to
[1415.50s -> 1421.78s]  calculate up to a normalization constant. So that's how we can calculate the
[1421.78s -> 1426.50s]  forward messages. Once we've calculated the forward messages, the next question
[1426.50s -> 1431.50s]  we can ask is, well, what if we want to recover the state marginal? What if
[1431.50s -> 1434.54s]  we want to recover the probability of a state given optimality everywhere
[1434.54s -> 1440.34s]  from 1 through capital T? So in that case, now that we have both backward
[1440.34s -> 1443.82s]  and forward messages, we can actually derive this equation pretty easily. So p
[1443.82s -> 1447.78s]  of s t given o 1 through capital T, using the definition of
[1447.78s -> 1453.54s]  conditional probability, is p of s t comma o 1 through capital T divided by p of
[1453.54s -> 1463.74s]  1 o 1 through capital T. Now we can factorize the numerator, and the way
[1463.74s -> 1468.26s]  that we factorize the numerator is we're going to write it as p of o little
[1468.26s -> 1473.18s]  t through capital T given s t times the probability of s t and all the other
[1473.18s -> 1476.86s]  o's. And the reason we can do that is because we know that o t through
[1476.94s -> 1484.02s]  capital T is conditionally independent of past o's given s t. Now here the first term in the
[1484.02s -> 1488.98s]  numerator is just the state backward message of s t, so we can write
[1488.98s -> 1493.18s]  this expression as following. We could write it as being proportional, if we
[1493.18s -> 1500.54s]  disregard the denominator, to beta t s t of p of s t o 1 through t minus 1
[1500.54s -> 1508.58s]  times p of o 1 through t minus 1. The second term is just the forward message,
[1508.58s -> 1513.82s]  and the last term doesn't depend on s t, so if we're willing to accept it in an
[1513.82s -> 1517.78s]  unknown normalizing constant, we can simply disregard that. So then we're
[1517.78s -> 1522.34s]  left with the probability of a state given o 1 through t is proportional to
[1522.34s -> 1526.38s]  the backward message multiplied by the forward message. Very simple, very
[1526.38s -> 1529.70s]  elegant. So that's how you can recover state marginals for the soft
[1529.74s -> 1533.46s]  optimal policy, and this will become very important later when we talk about
[1533.46s -> 1537.86s]  inverse reinforcement learning. So there's quite a lot of math here, but all of it is
[1537.86s -> 1542.38s]  fairly straightforward algebra with a combination of probability theory. So if
[1542.38s -> 1545.66s]  something here is unclear, what I would recommend you do is to download
[1545.66s -> 1548.74s]  the slides from the course website and just go over the math on your own.
[1548.74s -> 1552.58s]  Something that helps me when faced with equations like this is to write
[1552.58s -> 1554.90s]  them down in a piece of paper and really work through them, because then
[1554.98s -> 1560.82s]  everything becomes usually pretty clear. Alright, let's think a little bit about
[1560.82s -> 1565.34s]  the intuition behind this expression. One of the ways we can think about this
[1565.34s -> 1569.54s]  way of computing state marginals, let's think back to the
[1569.54s -> 1572.46s]  monkey case where you just want to go in a straight line from the start to
[1572.46s -> 1578.02s]  the goal, is that the backward messages represents sort of a cone radiating
[1578.02s -> 1584.22s]  outward from the end. So this cone kind of radiates like this, and the
[1584.22s -> 1587.58s]  cone represents states from which you'll be able to reach the goal. So the
[1587.58s -> 1590.82s]  further back you go, the more states there are from which you'll be able to
[1590.82s -> 1597.90s]  reach the goal. So that's what's represented by this yellow thing. The
[1597.90s -> 1605.86s]  forward messages represent a cone radiating forward, forward, from the
[1605.86s -> 1611.58s]  initial state, and that represents states with high probability of being reached
[1611.58s -> 1614.46s]  from the initial state with high rewards. These are things you can
[1614.46s -> 1619.34s]  reach if you start at the green circle and then maintain high reward. And then
[1619.34s -> 1622.46s]  your state marginals are basically the intersections of these things, because
[1622.46s -> 1624.54s]  you're going to be reaching the goal and you're going to be starting at the
[1624.54s -> 1629.58s]  beginning. So the blue cone represents the forward messages, which are states you can
[1629.58s -> 1633.10s]  reach with high reward if you start at the green circle. The yellow cone
[1633.10s -> 1637.06s]  represents states from which you'll be able to reach the goal, and near
[1637.06s -> 1642.26s]  optimal behavior will basically align the intersection of these two things. And
[1642.26s -> 1648.26s]  one interesting tidbit is in experiments involving human motor
[1648.26s -> 1652.06s]  control, scientists have actually observed that real human reaching
[1652.06s -> 1656.42s]  behaviors exhibit this sort of distribution. So if you ask a person to
[1656.42s -> 1660.46s]  hold a little tool and move that tool so that it touches a particular
[1660.46s -> 1664.26s]  location, in this case the person was asked to touch their elbow, and you
[1664.26s -> 1668.98s]  actually plot the distribution of positions that the tool tip will travel
[1668.98s -> 1672.58s]  through in space, you'll find that it's of course very precise at the beginning
[1672.58s -> 1675.58s]  because it starts at the beginning, it's very precise at the end because
[1675.58s -> 1682.38s]  it reaches the destination, and it has this outward growing cigar-shaped
[1682.38s -> 1686.06s]  distribution of states where you have the widest state marginals right in
[1686.06s -> 1692.38s]  the middle. All right, so to summarize, we talked about how we can derive a
[1692.38s -> 1696.06s]  probabilistic graphical model for optimal control, which is this graphical
[1696.06s -> 1700.02s]  model. We talked about how control can be framed as inference, very similar to
[1700.02s -> 1704.62s]  hidden Markov models, Kalman filters, and so on, and how the resulting
[1704.62s -> 1708.18s]  inference procedure is very similar to dynamic programming or value iteration,
[1708.18s -> 1713.44s]  but using the softmax instead of a hardmax. In the next portion of the
[1713.44s -> 1719.54s]  lecture, I'll talk about how we can use variational inference to improve on this framework.
