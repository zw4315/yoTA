# Detected language: en (p=1.00)

[0.00s -> 6.88s]  All right, welcome to Lecture 7 of CS285. Today we're going to talk about value function methods.
[7.60s -> 14.08s]  So we first saw algorithms that use value functions when we discussed actor-critic algorithms.
[14.08s -> 19.28s]  And just to recap, the basic batch mode actor-critic algorithm that we discussed
[19.28s -> 23.36s]  extends the policy gradient algorithm to introduce a value function.
[23.36s -> 26.56s]  So in the actor-critic algorithm that we covered in the last lecture,
[26.56s -> 32.00s]  we would generate some samples from our current policy by running that policy on the robot.
[32.00s -> 34.72s]  We would fit a value function to those samples,
[35.28s -> 41.52s]  which is a neural network in the previous lecture that mapped states to scalar-valued values.
[42.16s -> 46.16s]  Then we would use that value function to estimate the advantage
[46.16s -> 49.68s]  for each state action tuple s-i-a-i that we sampled.
[49.68s -> 53.52s]  And we generated these advantage estimates by taking the current reward
[53.52s -> 57.60s]  plus the next value minus the current value. And we could also optionally
[57.60s -> 61.44s]  insert a discount factor in front of the next value, so that's line 3.
[62.16s -> 67.60s]  And then we would use these estimated advantages to estimate a policy gradient on line 4
[67.60s -> 72.16s]  using the same policy gradient formula that we learned about in the preceding lecture.
[72.16s -> 74.72s]  And then we would do gradient descent on the policy parameters.
[75.36s -> 80.80s]  So in this scheme, it again follows the usual recipe for a reinforcement learning
[80.80s -> 84.88s]  algorithm that we discussed. So the orange box consists of generating samples,
[84.88s -> 88.40s]  the green box consists of fitting our value function,
[88.40s -> 93.12s]  and the blue box consists of taking a gradient descent step on the policy parameters.
[95.36s -> 101.52s]  So can we maybe omit the policy gradient entirely? What if we just learn a value function
[101.52s -> 105.92s]  and then try to use that value function to figure out how to act?
[105.92s -> 110.40s]  The intuition for why this should be possible is that the value function tells us
[110.40s -> 116.08s]  which states are better than which other states. So if we simply select actions that go into the
[116.08s -> 120.56s]  better states, maybe we don't need an explicit policy neural network anymore.
[122.72s -> 126.40s]  So here is the way to make this intuition a bit more formal.
[127.20s -> 133.04s]  A pi S-T-A-T is our advantage. That's the difference between our Q-value and our value.
[133.04s -> 138.56s]  And intuitively, the advantage says how much better is A-T than the average action
[138.56s -> 143.36s]  according to the policy pi, where pi is the policy for which we calculated this advantage.
[145.60s -> 152.96s]  So then argmax with respect to A-T of A pi S-T-A-T is the best action
[152.96s -> 156.72s]  that we could take from S-T if we follow pi thereafter.
[160.08s -> 166.88s]  Which means that the argmax with respect to A-T of the advantage is going to be at least as good
[166.88s -> 169.36s]  as an action that we would have sampled from our current policy.
[170.16s -> 174.16s]  We know it's at least as good because it's actually the best. So if it's the best action
[174.16s -> 179.44s]  from S-T if we then follow pi thereafter, then it's at least as good as whatever action pi would
[179.44s -> 187.04s]  have chosen. And the interesting thing is that this is true regardless of what pi actually is.
[188.32s -> 193.76s]  So this means that this argmax should immediately suggest to us that regardless of which
[193.76s -> 199.04s]  policy we had before, even if it was a very bad random policy, we ought to be able to improve it
[199.04s -> 202.88s]  by selecting the action according to the argmax of the advantage.
[205.92s -> 210.24s]  So maybe we could forget about representing policies explicitly and we could just use this
[210.24s -> 215.52s]  argmax to select our actions. And that's the basis for value-based methods.
[217.20s -> 223.36s]  So we'll construct new policies implicitly, so at every iteration we can construct a new policy
[223.36s -> 229.60s]  pi prime that assigns a probability of one to the action A-T if it is the argmax
[229.60s -> 235.76s]  of the advantage A pi, S-T A-T, where A pi is the advantage for the previous implicit policy.
[237.28s -> 240.48s]  Crucially, we don't need another neural network to represent this policy. The policy
[240.48s -> 245.28s]  is represented implicitly as this argmax, so the only thing that we need to actually learn
[245.28s -> 250.64s]  is the advantage. And then we will of course re-estimate the advantage function for pi prime
[250.64s -> 254.16s]  and then construct a new policy that's the argmax with respect to that.
[256.40s -> 260.72s]  So each time we create this implicit pi prime, we know that that is
[260.72s -> 264.08s]  at least as good as pi and in most cases better.
[267.60s -> 274.56s]  So we still have an algorithm with the usual three boxes, where in the orange box we generated
[274.56s -> 280.88s]  samples, in the green box we're going to fit some kind of value function, either q pi or v pi,
[280.88s -> 286.08s]  which we will use to estimate the advantage. And in the blue box, instead of taking a gradient
[286.08s -> 292.08s]  ascent step on an explicit policy, we will construct this implicit policy as the argmax.
[292.08s -> 295.12s]  So there's no actual learning that happens in the blue box anymore,
[295.12s -> 299.04s]  it's just setting the policy to be this argmax policy.
[299.04s -> 308.64s]  So this is the high-level idea behind what is called policy iteration. So in policy iteration,
[308.64s -> 315.36s]  step one is to evaluate the advantage of your current policy pi, and then step two is to
[315.36s -> 321.76s]  construct a new policy that's going to be this pi prime, where pi prime takes an action with
[321.76s -> 326.80s]  probability one if it is the argmax of the advantage. And then we alternate these two steps.
[327.60s -> 332.80s]  It's called policy iteration because we iterate between evaluating the policy in step one
[332.80s -> 339.44s]  and updating the policy in step two. So step two is pretty straightforward, especially if
[339.44s -> 344.00s]  we have a discrete action space. Computing an argmax is something that is not hard to do
[344.00s -> 349.20s]  by simply checking the advantage value of every possible action. If you have continuous valued
[349.20s -> 353.92s]  actions, things get a little more complex, and we'll cover that case in the subsequent lecture.
[353.92s -> 358.64s]  But for now let's say that we have discrete actions. The big puzzle is really how to do
[358.64s -> 365.20s]  step one. How do you evaluate the advantage a pi for a particular state action tuple for a given
[365.20s -> 370.40s]  previous policy pi, which will also be an implicit policy, but we don't care so much about that
[370.40s -> 379.12s]  right now. So like before, we can express the advantage a pi s, a as the reward s, a plus
[379.12s -> 389.44s]  gamma times the expected value of v pi at s prime minus v pi at s. So let's try to evaluate v pi of s.
[392.00s -> 397.04s]  One way to evaluate v pi of s in order to then estimate these advantages for policy iteration
[397.04s -> 404.24s]  is to use dynamic programming. So for now, let's assume that we know p of s prime given s, a.
[404.24s -> 409.36s]  Let's assume that we know the transition probabilities, and furthermore, let's assume that
[409.36s -> 415.52s]  both s and a are small and discrete. So this is kind of the known dynamic setting. This is not the
[415.52s -> 420.24s]  setting we usually operate in in model-free RL, but we'll assume that that's our setting for now,
[420.24s -> 424.32s]  just so that we can derive the simple dynamic programming algorithm and then turn it into a
[424.32s -> 431.36s]  model-free algorithm. So if we have a small discrete s and a, we can imagine that we can
[431.36s -> 435.52s]  essentially enumerate our entire state and action space. We can represent it with a table.
[436.08s -> 441.04s]  For instance, you might have this grid world. In this grid world, your actions correspond to steps
[441.04s -> 446.00s]  that move left, right, up, and down. So here we have 16 states,
[446.00s -> 450.00s]  and you have four actions per state, actions from moving left, right, up, and down.
[451.92s -> 457.84s]  So in this kind of small state space, you can actually store the full value function v pi
[458.64s -> 463.76s]  in a table, right. You can actually construct a table with 16 numbers and just write down the
[463.76s -> 466.88s]  v pi for every one of those 16 numbers. You don't need a neural network for that.
[467.92s -> 475.76s]  So here's a potential table of 16 numbers. And your transition probabilities t
[476.96s -> 483.76s]  are represented by a 16 by 16 by 4 tensor. So when we say we're doing tabular reinforcement
[483.76s -> 488.48s]  learning or tabular dynamic programming, what we're really referring to is a setting kind of
[488.48s -> 496.08s]  like this. And now we can write down the bootstrapped update for the value function
[496.08s -> 502.88s]  that we saw in lecture 6 in terms of these explicit known probabilities. So if we want to
[502.88s -> 508.56s]  update v pi of s, we can set it to be the expected value with respect to the actions,
[508.56s -> 515.92s]  a sample from our policy pi, of the reward s a plus gamma times the expected value over s prime
[515.92s -> 524.08s]  sampled from p of s prime given s a of v pi s prime. And if you have a tabular MDP, meaning
[524.08s -> 529.84s]  you have a small discrete state space and you know the transition probabilities, this backup can
[529.84s -> 535.60s]  be calculated exactly. So each of the expected values can be computed by summing over all
[535.60s -> 541.28s]  values of that random variable and multiplying the value inside the parentheses by its probability.
[544.32s -> 548.16s]  And then of course we need to know v pi s prime, so we're just going to use our current
[548.16s -> 552.80s]  estimate of the value function for that value. We're going to basically take that number from the table.
[555.04s -> 560.96s]  And then once we calculate a value function v pi this way, then we can construct a better
[561.84s -> 566.56s]  policy pi prime, as I mentioned before, by assigning probability 1
[566.56s -> 571.28s]  to the action that is the argmax of the advantage that we obtain from this value function.
[574.24s -> 579.52s]  Now this also means that our policy will be deterministic, so expected values with respect
[579.52s -> 586.56s]  to this pi will be pretty easy to compute. So we can simplify our bootstrap update by removing
[586.56s -> 591.20s]  the expectation with respect to pi and just directly plugging in the only action that has
[591.20s -> 596.72s]  non-zero probability. So then we get the simplified backup where v prime of s is set
[596.72s -> 604.96s]  to r of s comma pi of s plus gamma times the expectation under p s prime given s pi of s
[604.96s -> 615.20s]  of v pi s prime. Okay, so now we can plug this procedure into our policy iteration algorithm.
[615.20s -> 620.00s]  So as a reminder, our policy iteration algorithm. Step one is evaluate our advantage, which we're
[620.00s -> 624.16s]  going to obtain from the value function. So it's really step one is evaluate the value function.
[625.04s -> 631.60s]  And then step two, set your new policy to be this pi prime policy obtained via the argmax,
[631.60s -> 635.12s]  and then repeat. So this is exactly the policy iteration algorithm we had before,
[635.76s -> 640.40s]  and the thing that we're going to learn now is the value function, which for now we'll
[640.40s -> 647.36s]  represent in this tabular form as a table of 16 numbers if you have 16 states. So policy
[647.36s -> 653.12s]  evaluation is what goes into step one, and the way that we can do policy evaluation is by
[653.12s -> 659.20s]  repeatedly applying this recursion, by repeatedly setting the value for every state for every entry
[659.20s -> 666.24s]  in our table to be the reward at that state plus the expected value of the value at the next
[666.24s -> 672.88s]  state. And we just repeat this multiple times. You can prove that repeating this recursion
[672.88s -> 678.08s]  eventually converges to a fixed point, and this fixed point is the true value function v pi.
[679.44s -> 683.44s]  For those of you that are a bit more mathematically inclined, I will also point out
[684.00s -> 690.32s]  that if you write v pi of s equals r of s pi s plus this expectation,
[691.20s -> 697.92s]  that actually represents a system of linear equations that describe the value function v pi,
[698.96s -> 703.68s]  and the system of linear equations can then be solved with any linear equation solver.
[704.72s -> 710.00s]  So something that you could do as a homework assignment to understand this a little bit better
[710.00s -> 714.40s]  is to actually write down the system of linear equations and work out its solution.
[714.40s -> 718.08s]  It's fairly straightforward to do, but it's a good exercise to make sure that you really
[718.08s -> 720.88s]  understand dynamic programming and policy evaluation.
[723.76s -> 731.92s]  Okay, so we have our tabular MDP, 16 states, four actions per state. We can store the full
[731.92s -> 736.48s]  value function in a table. We can compute the value function using policy evaluation
[736.48s -> 742.24s]  by repeatedly using this recursion, and we perform this in the inner loop of our policy
[742.24s -> 746.40s]  iteration procedure where it simply alternates between policy evaluation
[746.40s -> 749.84s]  and updating the policy to be this argmax policy,
[749.84s -> 753.76s]  where the advantage is obtained from the value function that we found in step one.
[756.56s -> 759.68s]  Now there is an even simpler dynamic programming process
[760.32s -> 765.12s]  that you can design that kind of short circuits this policy iteration procedure.
[768.08s -> 772.96s]  So to see this, let's hear the following steps that we need. So first notice that we're
[772.96s -> 776.80s]  taking the argmax of the advantage function when you compute the policy,
[777.52s -> 782.40s]  and the advantage is the reward plus the expected next value minus the current value.
[784.56s -> 791.04s]  Now if you remove the minus v pi s, you just get the q function. Since you're taking the
[791.04s -> 796.24s]  argmax with respect to a, any term that doesn't depend on a actually doesn't influence the argmax,
[796.24s -> 800.64s]  so the argmax of the advantage is actually equal to the argmax of the q function.
[800.64s -> 805.92s]  So we can equivalently write the new policy as the argmax of q, which is a little simpler
[805.92s -> 812.32s]  because we removed one of the terms, and the way that we can think about this graphically
[812.32s -> 816.08s]  is that the q function is a table with one entry for every state and every action,
[816.08s -> 819.60s]  so here different rows are different states and different columns are different actions,
[820.40s -> 824.24s]  and when we compute the argmax, we're basically finding the entry in each row
[824.24s -> 829.44s]  that has the largest value, and we're selecting the corresponding index as our policy.
[830.80s -> 837.20s]  When we then later on go on to actually evaluate that policy, we're going to plug that index back
[837.20s -> 846.32s]  into a q function to get its value. So the argmax gives us the policy, but the max actually gives
[846.32s -> 850.88s]  us the new value of that policy. So what we can do is we can short circuit this. We can
[850.88s -> 854.88s]  actually skip the step where we recover the indices and just directly take the values.
[854.88s -> 862.00s]  So we can skip the policy and compute the values directly, and this gives us a new algorithm,
[862.00s -> 869.76s]  which is called value iteration, where in step one we set the q values, basically the entries
[869.76s -> 875.76s]  in this s by a table, to be the reward plus the expected value of the value function at the
[875.76s -> 883.20s]  next time step, and then in step two we set the value function to be the max over a in this
[883.20s -> 887.60s]  q function table. So we basically take each row in the q function table and pick the entry
[887.60s -> 896.00s]  with the largest number in it and store that as the value for that state. So here explicit
[896.00s -> 900.88s]  policy computation is skipped. We don't have to actually represent the policy explicitly,
[900.88s -> 905.84s]  but you can think of it as showing up implicitly in step two, because setting the value to be the
[905.84s -> 913.52s]  max over the actions in the q value table is analogous to taking the argmax and then plugging
[913.52s -> 918.40s]  the index of the argmax into the table to recover the value. But since taking the argmax
[918.40s -> 922.48s]  and then plugging into the table is the same as just taking the max, we can basically short
[922.48s -> 929.36s]  circuit that step and get this procedure. So step one, construct your q value table by setting
[929.36s -> 934.72s]  it to be the reward plus the expected value of the next time step. Step two, set the value
[934.72s -> 944.72s]  to be the max. So this yields a slightly modified and simpler procedure, where in the green box
[944.72s -> 949.36s]  you construct your table of q values, and in the blue box you construct the value function
[949.36s -> 956.96s]  by taking the max. Now this procedure can be simplified even further if you actually take
[956.96s -> 961.84s]  step two and plug it into step one. So you notice that v of s only shows up in one place,
[961.84s -> 967.52s]  which is inside that expectation step one. So if you simply replace that with a max over a of q s a,
[968.08s -> 973.52s]  you don't even need to represent the value function, you only need to represent the q function.
