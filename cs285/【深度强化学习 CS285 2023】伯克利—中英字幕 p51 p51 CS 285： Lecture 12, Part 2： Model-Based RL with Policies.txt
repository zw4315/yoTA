# Detected language: en (p=1.00)

[0.00s -> 5.28s]  All right, let's talk about model-free reinforcement learning with a learned model,
[5.28s -> 8.40s]  so essentially model-based RL by using model-free RL methods.
[8.40s -> 10.40s]  Before we get into algorithms,
[10.40s -> 14.12s]  uh, let's make a little bit more precise some of the things that we discussed in
[14.12s -> 19.18s]  the previous portion of the lecture on what these backpropagation gradients
[19.18s -> 23.88s]  actually look like and why they might not be as good as using model-free methods.
[23.88s -> 27.50s]  So this is the familiar policy gradient expression that we had before.
[27.50s -> 30.62s]  This is basically just directly taken from previous lectures.
[30.62s -> 35.40s]  Now, we talked about the policy gradient as a model-free reinforcement learning algorithm,
[35.40s -> 38.94s]  but you could just as well think of it as a gradient estimator that could be used
[38.94s -> 44.08s]  to estimate the gradient of the reward with respect to the policy parameters.
[44.08s -> 46.14s]  Used in this way, it's sometimes referred to as
[46.14s -> 50.94s]  a likelihood ratio gradient estimator or colloquially as the reinforced gradient estimator.
[50.94s -> 53.38s]  But it's important to remember that as a gradient estimator,
[53.38s -> 56.02s]  it doesn't necessarily have to have anything to do with reinforcement learning.
[56.02s -> 59.78s]  Anytime that you have these kinds of stochastic computation graphs,
[59.78s -> 62.02s]  you could use this type of estimator.
[62.02s -> 66.70s]  Now before, the really convenient thing for us about this kind of gradient estimator
[66.70s -> 71.18s]  is that it doesn't contain the transition probabilities.
[71.18s -> 73.26s]  That's a little bit of a lie, of course,
[73.26s -> 77.02s]  because in reality, you do need the transition probabilities to calculate
[77.02s -> 79.98s]  the policy gradient because you need a sample and
[79.98s -> 84.06s]  those samples come from the policy and from the transition probabilities.
[84.06s -> 87.66s]  But the transition probabilities themselves do not show up in the expression,
[87.66s -> 89.82s]  except insofar as they generate the samples,
[89.82s -> 92.86s]  and in particular, we don't need to know their derivatives.
[92.86s -> 96.86s]  But there's nothing stopping you from using this gradient estimator with a learned model.
[96.86s -> 99.02s]  So just the same way that you would sample from
[99.02s -> 103.10s]  the real MDP before and now you could sample from the learned model.
[103.10s -> 106.22s]  The alternative, the backpropagation gradient,
[106.22s -> 108.42s]  it's also sometimes called a pathwise gradient,
[108.42s -> 109.98s]  can be written out like this.
[109.98s -> 112.90s]  Now this might seem like a very daunting mathematical expression,
[112.94s -> 115.26s]  but all I did here was I just applied
[115.26s -> 118.50s]  the chain rule of calculus to compute the derivatives with
[118.50s -> 120.86s]  respect to the policy parameters for
[120.86s -> 123.50s]  those computation graphs that I showed in the previous section.
[123.50s -> 127.10s]  So there's an outer sum over all time steps and every time step,
[127.10s -> 129.86s]  there's the derivative of the action at that time step with respect to
[129.86s -> 134.14s]  the policy parameters times the derivative of the next state with respect to the action.
[134.14s -> 137.42s]  And then that expression in parentheses is just the derivative
[137.42s -> 142.10s]  of the reward for all future states with respect to the next state.
[142.62s -> 145.54s]  And the, you know,
[145.54s -> 148.26s]  the particularly problematic part is, of course,
[148.26s -> 151.22s]  that giant product in the second set of parentheses,
[151.22s -> 157.58s]  which is a product of all the Jacobians between time step t prime and t plus 1.
[157.58s -> 163.70s]  So that's a little bit of a problem because in there you have these ds,
[163.70s -> 165.62s]  da and ds, ds terms,
[165.62s -> 167.50s]  which are basically the derivative of the next state with respect to
[167.50s -> 170.22s]  the previous action and the next state with respect to the previous state.
[170.42s -> 172.34s]  And they all get multiplied together.
[172.34s -> 176.14s]  So if you imagine that your states are n-dimensional,
[176.14s -> 181.30s]  just those ds terms that the very last term in the expression,
[181.30s -> 182.94s]  that's going to be an n by n matrix.
[182.94s -> 186.06s]  And there's going to be a lot of those matrices getting multiplied together.
[186.06s -> 189.02s]  And if those matrices have eigenvalues that are larger than one,
[189.02s -> 191.86s]  then if you multiply enough of them together, eventually they explode.
[191.86s -> 193.42s]  And if they have eigenvalues less than one,
[193.42s -> 196.02s]  you multiply enough of them together, eventually they vanish.
[196.02s -> 199.86s]  So that's what makes this pathwise gradient so difficult to deal with.
[200.22s -> 204.66s]  Just as a detail, I do want to note here that the likelihood ratio gradient at the top
[204.66s -> 210.38s]  is technically only valid for stochastic policies and stochastic transitions.
[210.38s -> 213.78s]  The pathwise gradient at the bottom is technically only valid for
[213.78s -> 216.62s]  deterministic policies and transitions.
[216.62s -> 217.98s]  But this is a solvable problem.
[217.98s -> 221.42s]  And you could in fact extend the pathwise gradient to some types of
[221.42s -> 224.58s]  stochastic transitions by using something called the reparameterization trick,
[224.58s -> 226.42s]  which we will learn about in a later lecture.
[226.42s -> 229.06s]  And you can even make the policy gradient nearly deterministic
[229.10s -> 233.06s]  by taking the limit as the variance of, let's say, a Gaussian policy or
[233.06s -> 236.26s]  transition probability goes to zero, and you can still get an expression for it,
[236.26s -> 237.58s]  although it'll be a little bit different.
[237.58s -> 241.46s]  So it's easier to write the policy gradient for stochastic systems
[241.46s -> 244.18s]  and the pathwise gradient for deterministic systems,
[244.18s -> 245.98s]  but that's not a fundamental limitation.
[245.98s -> 249.54s]  The fundamental difference is that the pathwise gradient involves that
[249.54s -> 252.74s]  product of all those Jacobians, whereas the policy gradient does not.
[254.34s -> 257.02s]  Now, some of you might be wondering at this point, well,
[257.02s -> 258.82s]  it seems like there's kind of a free lunch going on here,
[258.82s -> 262.98s]  like how is it that you can just get rid of a giant product of Jacobians?
[262.98s -> 263.98s]  But there is a trade-off, of course,
[263.98s -> 266.26s]  which is that the policy gradient requires sampling.
[266.26s -> 269.10s]  So that's kind of where the difference comes in.
[269.10s -> 271.90s]  And in fact, if you were to really dig down to the optimization details of
[271.90s -> 274.86s]  these procedures, it actually turns out that the policy gradient
[274.86s -> 280.78s]  has some fairly deep connections with things like finite differencing methods.
[280.78s -> 284.82s]  So there is no free lunch, the policy gradient does pay a price for
[284.82s -> 286.70s]  getting rid of the product of Jacobians.
[287.34s -> 291.70s]  But if you're multiplying enough Jacobians together, paying the price of
[291.70s -> 293.02s]  switching over to sampling can be worth it.
[294.58s -> 299.50s]  So policy gradient might in fact be more stable if you generate enough samples,
[299.50s -> 302.66s]  because it doesn't require multiplying many Jacobians.
[302.66s -> 305.30s]  Now before, generating lots of samples was a problem,
[305.30s -> 307.54s]  because when we were talking about model-free RL,
[307.54s -> 310.94s]  those samples required actually running a real physical system.
[310.94s -> 312.90s]  But if we're talking about model-based RL,
[312.90s -> 316.74s]  then generating those samples could involve simply running your model,
[316.74s -> 320.46s]  which costs compute, but it doesn't cost any kind of like physical interaction
[320.46s -> 321.50s]  with your MDP.
[321.50s -> 323.54s]  So now that trade-off might be well worth it for us,
[323.54s -> 326.54s]  because generating more samples is just a matter of,
[326.54s -> 328.50s]  sort of sticking more GPUs in the data center.
[331.14s -> 334.34s]  If you want to learn more about the numerical stability issues,
[334.34s -> 336.38s]  specifically in regard to policy gradients,
[336.38s -> 341.22s]  you can check out this 2018 paper that talks about some of the stability issues.
[341.22s -> 344.34s]  But the short version is that the model-free gradient can actually be better.
[348.18s -> 351.70s]  Now from this, we could write down, you know, what again,
[351.70s -> 354.18s]  I might call them, I might make up a name for,
[354.18s -> 357.42s]  the name I'm gonna make up is model-based RL version 2.5.
[357.42s -> 360.70s]  Model-based RL version 2.5 is gonna be very similar to 2.0,
[360.70s -> 364.50s]  except instead of using back propagation, it's going to use the policy gradient.
[364.50s -> 367.38s]  So step one, run some policy to collect the data set.
[367.38s -> 369.66s]  Step two, learn a dynamics model.
[369.70s -> 374.50s]  Step three, use that dynamics model to sample a whole lot of trajectories
[374.50s -> 376.10s]  with your current policy.
[376.10s -> 379.94s]  Step four, use those trajectories to improve the policy via policy gradient.
[379.94s -> 383.46s]  And you can, of course, use all the actor-critic tricks, all that stuff here.
[383.46s -> 385.46s]  And then repeat step three a few more times.
[385.46s -> 389.42s]  So you can take many policy gradient steps, resampling trajectories each time,
[389.42s -> 393.70s]  but not generating any more real data, nor retraining your model.
[393.70s -> 396.66s]  And once you've improved your policy enough that you're happy with it,
[396.66s -> 399.62s]  then you would run your policy to collect more data,
[399.62s -> 406.02s]  append those to your data set, and use that larger data set to now train a better model, okay?
[406.02s -> 411.50s]  So this algorithm would get rid of the issue with back propagation that we discussed before,
[411.50s -> 412.70s]  but it still has some problems.
[412.70s -> 415.82s]  And in the end, this is not actually the model-based RL method
[415.82s -> 417.46s]  that most people would want to use.
[418.70s -> 421.06s]  So what might be the problem with this procedure?
[423.06s -> 424.26s]  Take a moment to think about this.
[424.26s -> 425.78s]  And again, you could pause the video
[425.78s -> 427.66s]  and ponder this on your own time.
[427.66s -> 430.82s]  And when you're ready to continue, then continue,
[430.82s -> 432.94s]  and I will tell you what's wrong with this.
[434.86s -> 436.54s]  Okay, so the issue really has to do
[436.54s -> 438.58s]  with making long model-based rollouts.
[439.74s -> 442.22s]  To understand this issue, let's actually think back
[442.22s -> 444.38s]  to something we discussed earlier in the course
[444.38s -> 446.82s]  when we talked about imitation learning.
[446.82s -> 448.62s]  When we talked about imitation learning,
[448.62s -> 451.94s]  we learned that if you train a policy
[451.94s -> 456.42s]  with supervised learning and you try to run that policy,
[456.42s -> 457.74s]  it might make a small mistake,
[457.74s -> 459.26s]  because every learned model will make
[459.26s -> 461.02s]  at least a small mistake.
[461.02s -> 462.82s]  But the problem is that when your learned policy
[462.82s -> 465.46s]  makes a small mistake, it'll deviate a little bit
[465.46s -> 467.22s]  from what was seen in the data.
[467.22s -> 468.34s]  And when it deviates a little bit
[468.34s -> 469.26s]  from what was seen in the data,
[469.26s -> 471.66s]  it'll find itself in an unfamiliar situation
[471.66s -> 473.42s]  where it'll make a bigger mistake.
[473.42s -> 475.06s]  And these mistakes will compound.
[476.02s -> 480.14s]  And we learned when we discussed imitation learning
[480.14s -> 481.34s]  that this issue really comes down
[481.34s -> 482.78s]  to something called distributional shift.
[482.78s -> 485.34s]  It comes down to the problem that the distribution
[485.34s -> 487.26s]  over states under which your policy
[487.26s -> 488.82s]  was trained with supervised learning
[488.82s -> 490.58s]  differs from the distribution of states
[490.58s -> 491.74s]  that it received as input
[491.74s -> 494.66s]  when it's actually executed in the environment.
[494.66s -> 497.86s]  Now, the same exact challenge applies to learned models.
[498.78s -> 500.46s]  So, and we talked about this, of course,
[500.46s -> 502.86s]  before when we discussed learned models,
[502.86s -> 506.98s]  that if the black curve now represents
[506.98s -> 508.90s]  running pi theta with the true dynamics
[508.90s -> 512.02s]  and the red curve represents running it with a learned model
[512.02s -> 513.90s]  then when you run it with a learned model,
[513.90s -> 515.54s]  the learned model will make some mistakes.
[515.54s -> 517.62s]  It'll put itself into slightly different states
[517.62s -> 518.70s]  and in those slightly different states
[518.70s -> 519.90s]  it'll make bigger mistakes.
[519.90s -> 521.90s]  So if your model-based rollout is long enough,
[521.90s -> 524.30s]  eventually it'll differ significantly from the real world
[524.30s -> 527.14s]  because the mistakes get bigger and bigger.
[527.14s -> 528.22s]  Now, this is all for the case
[528.22s -> 530.06s]  where you're running the same policy
[530.06s -> 532.62s]  as the one that you used to collect the data.
[532.62s -> 535.74s]  But of course, in model-based RL version 2.5,
[535.74s -> 536.90s]  you're going to change the policy.
[536.90s -> 537.78s]  You're going to make it better
[537.78s -> 539.02s]  with respect to your model,
[539.02s -> 542.26s]  which means that the issue is even further exacerbated
[542.30s -> 545.18s]  because now you'll be running with a learned dynamics model
[545.18s -> 547.06s]  that is different from the real dynamics model
[547.06s -> 548.38s]  and with a modified policy
[548.38s -> 550.42s]  that is different from the policy that collected the data.
[550.42s -> 553.62s]  So the distributional shift will be even worse.
[553.62s -> 555.06s]  So how quickly does the error accumulate?
[555.06s -> 556.06s]  Well, even in the best case,
[556.06s -> 557.98s]  when you run the same exact policy,
[557.98s -> 560.18s]  just like in the behavior cloning discussion,
[560.18s -> 562.50s]  it'll accumulate as epsilon t squared.
[562.50s -> 564.14s]  And I would leave it as an exercise to you guys
[564.14s -> 565.22s]  to show that in fact,
[565.22s -> 566.82s]  that there's a bound of epsilon t squared
[566.82s -> 568.62s]  and the bound is tight.
[568.62s -> 570.54s]  The logic for that is extremely similar
[570.54s -> 572.42s]  to what we had in behavior cloning,
[572.42s -> 574.34s]  but the takeaway for us for now
[574.34s -> 578.54s]  is that errors build up very quickly
[578.54s -> 581.42s]  as the horizon of your model-based rollout increases,
[581.42s -> 584.22s]  which means that making long model-based rollouts
[584.22s -> 587.38s]  is very costly to you in terms of accumulated error.
[587.38s -> 588.34s]  This is another way of saying
[588.34s -> 590.18s]  that the longer your rollout is,
[590.18s -> 591.14s]  the more likely it is
[591.14s -> 592.90s]  that the conclusion you draw from that rollout,
[592.90s -> 594.30s]  meaning that it's reward,
[594.30s -> 596.10s]  will differ from what you would actually get
[596.10s -> 598.14s]  if you were to roll out in the real world.
[598.66s -> 603.66s]  So perhaps what we want to do is avoid long rollouts.
[604.02s -> 606.58s]  Perhaps we want to devise model-based RL methods
[606.58s -> 609.54s]  that can get away with only ever using short rollouts.
[610.74s -> 612.90s]  Can we do something like this?
[612.90s -> 614.46s]  So long rollouts are bad
[614.46s -> 616.62s]  because they have huge accumulating error.
[616.62s -> 618.30s]  Well, what if we just reduce the horizon?
[618.30s -> 620.78s]  Like, you know, our task has a horizon of 1,000
[620.78s -> 623.82s]  and we'll limit our rollouts to only 50 steps.
[623.82s -> 626.04s]  This will have much lower error.
[626.04s -> 627.96s]  The problem is that, of course,
[628.80s -> 631.12s]  an MDP with a horizon of 1,000
[631.12s -> 633.16s]  doesn't look the same as the same MDP
[633.16s -> 634.92s]  with a horizon of 50.
[634.92s -> 636.08s]  There may be something that happens
[636.08s -> 637.28s]  in those later time steps
[637.28s -> 639.64s]  that you would never see in the earlier time steps.
[639.64s -> 641.46s]  If you're, for example, controlling a robot
[641.46s -> 644.08s]  that's supposed to cook a meal in the kitchen,
[644.08s -> 646.52s]  well, maybe it'll take 30 minutes to cook the meal,
[646.52s -> 648.00s]  and if you only make model-based rollouts
[648.00s -> 648.96s]  that are five minutes in length,
[648.96s -> 650.48s]  that's hardly enough time for the robot
[650.48s -> 652.44s]  to, like, put the pot on the stove.
[653.60s -> 654.92s]  So this isn't really good enough
[654.92s -> 657.72s]  because you're essentially just changing the problem.
[657.72s -> 660.30s]  So here's a little trick that we can use.
[660.30s -> 663.56s]  What if we only ever make short model-based rollouts,
[663.56s -> 665.52s]  but we'll still use a smaller number
[665.52s -> 667.82s]  of long real-world rollouts?
[667.82s -> 669.68s]  So let's say that these black trajectories
[669.68s -> 671.40s]  actually represent real-world rollouts
[671.40s -> 673.76s]  that are to the full length of the MDP,
[674.92s -> 677.12s]  and we'll collect these relatively infrequently,
[677.12s -> 678.96s]  and then when we make our model-based rollouts,
[678.96s -> 680.48s]  we won't start them at the beginning.
[680.48s -> 682.28s]  We'll actually sample some states
[682.28s -> 683.92s]  from these real-world rollouts.
[683.92s -> 685.60s]  We'll sample them maybe uniformly at random
[685.60s -> 688.12s]  over the whole trajectory, and then from each one,
[688.12s -> 690.38s]  we'll make a little short model-based rollout.
[692.76s -> 695.04s]  So this has some interesting trade-offs.
[695.04s -> 696.40s]  We do have the much lower error
[696.40s -> 699.24s]  because our model-based rollouts are very short now,
[699.24s -> 700.72s]  and we get to see all the time steps.
[700.72s -> 703.32s]  So you will sample some of these states
[703.32s -> 705.92s]  from very late in the trajectory
[705.92s -> 707.34s]  and make your model-based rollout from there,
[707.34s -> 710.32s]  so you will see later time steps as well.
[710.32s -> 711.52s]  But here's the problem.
[712.52s -> 715.36s]  What kind of policy does the state distribution
[715.36s -> 717.52s]  of these model-based rollouts correspond to?
[718.92s -> 722.28s]  Well, the answer is it's complicated.
[722.28s -> 724.32s]  In fact, if your policy is changing
[724.32s -> 727.20s]  as you're making these short model-based rollouts
[727.20s -> 731.10s]  that are branched from the real-world rollouts,
[732.08s -> 735.00s]  you use the different policy to roll into that state
[735.00s -> 736.72s]  than you're using to roll out of it.
[736.72s -> 738.04s]  So you got to those orange dots
[738.04s -> 739.88s]  using the policy that was collecting the data,
[739.88s -> 741.92s]  and then when you run your model from there,
[741.92s -> 743.32s]  now you're switching to the new policy
[743.32s -> 744.52s]  that you are improving.
[745.56s -> 746.98s]  And that's actually a little bit problematic
[746.98s -> 749.78s]  because the state distribution that you get from this
[749.78s -> 752.94s]  is not the state distribution of your latest policy.
[752.94s -> 754.32s]  It's not the state distribution of the policy
[754.32s -> 755.32s]  that collected the data either.
[755.32s -> 757.52s]  It's actually kind of a mix of the two.
[758.84s -> 760.12s]  That's not necessarily fatal,
[760.12s -> 763.24s]  and certainly if you make small changes to your policy,
[763.24s -> 765.26s]  then all that same logic that we talked about
[765.26s -> 767.18s]  when we discussed advanced policy gradient methods
[767.18s -> 768.68s]  would still apply.
[768.68s -> 771.72s]  But usually the point of using model-based RL
[771.72s -> 773.54s]  is to improve your policy more
[773.54s -> 775.30s]  between data collection episodes
[775.30s -> 777.92s]  so that you can be more data efficient.
[777.92s -> 778.76s]  And in that case,
[778.76s -> 780.08s]  this gets to be a little bit of a problem
[780.08s -> 782.40s]  because if the whole point is to change the policy a lot
[782.40s -> 785.10s]  now that state distribution mismatch is gonna hurt us
[785.10s -> 786.80s]  if we use on-policy methods
[786.80s -> 790.02s]  like policy gradient algorithms.
[790.02s -> 792.28s]  So we can do this,
[792.28s -> 794.96s]  but it turns out that to make this work really well,
[794.96s -> 797.54s]  typically it's better to use off-policy algorithms
[797.54s -> 800.66s]  like Q-learning or Q-function active-critic methods.
[801.66s -> 802.62s]  Although it is possible
[802.62s -> 805.18s]  and people have devised policy gradient-based strategies
[805.18s -> 806.62s]  that employ this kind of idea,
[806.62s -> 808.58s]  you just can't change the policy as much
[808.58s -> 810.28s]  in between data collection rounds.
[812.02s -> 815.94s]  Okay, so model-based RL with short rollouts
[815.94s -> 816.80s]  is something that we could call
[816.80s -> 819.46s]  model-based RL version 3.0.
[819.46s -> 821.06s]  And this is actually getting much closer
[821.06s -> 821.90s]  to the kinds of methods
[821.90s -> 823.68s]  that people actually use in practice.
[823.68s -> 826.16s]  So the way these methods work is just like before,
[826.16s -> 827.40s]  they would collect some data,
[828.22s -> 829.48s]  use that data to train a model,
[829.48s -> 832.40s]  then they would pick some states
[832.40s -> 835.32s]  from the data set they collected in the real world,
[835.32s -> 836.24s]  and then use the model
[836.24s -> 838.08s]  to make short rollouts from those states.
[838.08s -> 839.22s]  And these can be very short.
[839.22s -> 841.36s]  They can be as short as one time step
[841.36s -> 842.40s]  in practical algorithms,
[842.40s -> 843.40s]  even when they're longer,
[843.40s -> 844.80s]  they're on the order of 10 time steps.
[844.80s -> 845.64s]  So very short,
[845.64s -> 848.08s]  much shorter than the full horizon of the problem.
[849.72s -> 852.62s]  And then typically these methods would use both real data
[852.62s -> 855.62s]  and the model-based data to improve the policy
[855.62s -> 857.40s]  with some kind of off-policy RL method.
[857.40s -> 859.06s]  So this might involve Q-learning,
[859.06s -> 860.98s]  or it might involve actor-critic methods.
[860.98s -> 862.16s]  And what I have written here
[862.16s -> 863.38s]  is that they improve the policy,
[863.38s -> 864.20s]  but in reality,
[864.20s -> 866.70s]  they typically have both a policy and a Q function.
[867.66s -> 869.60s]  And typically they would generate a lot more data
[869.60s -> 872.70s]  from the model than they would have from the real MDP.
[874.02s -> 875.74s]  So they would do this a few times,
[875.74s -> 878.90s]  and then they would run the policy in the real MDP
[878.90s -> 881.58s]  to collect more data to append it to the data set,
[881.58s -> 884.42s]  and then retrain the model and repeat the process.
[884.42s -> 886.50s]  And there's often a lot of delicate design decisions
[886.50s -> 887.90s]  that go into these methods in terms of
[887.90s -> 889.48s]  how much they improve the policy
[889.48s -> 890.74s]  in between data collection,
[890.74s -> 891.78s]  how much data they collect,
[891.78s -> 894.42s]  how much data they collect from the model and so on.
[894.42s -> 896.46s]  So in the next portion of the lecture,
[896.46s -> 899.06s]  we will talk about specific designs for these algorithms
[899.06s -> 901.22s]  and get a sense for the overall system architecture
[901.22s -> 902.62s]  for these kinds of methods.
