# Detected language: en (p=1.00)

[0.00s -> 7.68s]  The last group of offline RL algorithms that I'm going to discuss today are model-based
[7.68s -> 13.72s]  offline RL methods. So, so far all of the offline RL methods we talked about are model-free,
[13.72s -> 18.28s]  but actually model-based methods are a pretty nice fit for offline RL, because you can
[18.28s -> 21.88s]  take all of your data, you can train a model on that data as long as you want,
[21.88s -> 26.22s]  get a really good model, and then use that model to obtain a good policy, or even
[26.22s -> 32.22s]  just plan with that model directly. So how does model-based RL work? Well, we already
[32.22s -> 36.78s]  know this, we train our model using our data, we would use that model either to get a policy
[36.78s -> 41.96s]  or just to plan directly, and ordinarily we would then collect more data, right? But
[41.96s -> 47.64s]  if we don't collect more data, then we have a little bit of a problem. It's a similar
[47.64s -> 51.26s]  kind of problem as the one that we had with the Q function, so just like with the
[51.26s -> 56.70s]  Q function, the model essentially is trying to answer what-if questions. What if we were
[56.70s -> 62.42s]  to go back to one of those states in our dataset and take different actions, what
[62.42s -> 66.14s]  kind of states would we end up in, and what would happen then? So if you remember
[66.14s -> 71.02s]  back in our discussion of model-based RL, we talked about dynastyle methods. Dynastyle
[71.02s -> 77.94s]  methods are ones that use the states and actions collected in the real world as starting
[78.02s -> 83.38s]  points for short model-based rollouts, and the first two model-based offline RL methods
[83.38s -> 89.26s]  we'll discuss are basically dynastyle methods adapted to the offline setting. So what goes
[89.26s -> 93.38s]  wrong in the offline setting when we don't collect any more data? Well, analogously
[93.38s -> 98.02s]  to how for the Q function we could discover these adversarial actions that cause the
[98.02s -> 103.52s]  Q function to erroneously overestimate, for a model-based method, the policy could
[103.52s -> 108.20s]  learn to take actions that trick the model into going into states with very high reward,
[108.20s -> 112.84s]  because those states are out of distribution. So we still have the out of distribution actions
[112.84s -> 116.32s]  problem, and now we also have an out of distribution states problem for the model,
[116.32s -> 121.80s]  because the policy can take some crazy action, fool the model into going into an out of
[121.80s -> 125.84s]  distribution state, and from that out of distribution state, the model would then go to
[125.84s -> 130.48s]  an even more crazy state, and the policy can learn to exploit this to trick the model
[130.72s -> 134.08s]  into going into states with a high reward, even though that would probably not actually
[134.08s -> 139.24s]  happen in reality. So intuitively, in order to repair this problem, what we need to do
[139.24s -> 144.04s]  is somehow modify these model-based methods, so that if the policy starts tricking
[144.04s -> 151.84s]  the model to going into crazy states, it gets some kind of penalty, and the
[151.84s -> 155.60s]  policy is then incentivized to change its behavior to come back to regions that are
[155.60s -> 165.16s]  closer to the data. So first I'll discuss methods that essentially modify the reward
[165.16s -> 169.24s]  function to impose a little penalty, and the particular method I'll cover is called MOPO,
[169.24s -> 175.64s]  model-based offline policy optimization. There's kind of two methods that are very
[175.64s -> 180.00s]  similar, conceptually MOPO and MORAL. I'll talk about the MOPO version, but they're
[180.00s -> 183.04s]  pretty similar, and I would encourage you to actually check out both papers if you're
[183.04s -> 186.00s]  interested in this topic in more detail. They have, you know, slightly different
[186.00s -> 191.76s]  analysis and slightly different forms for the penalty. But the basic idea in both cases
[191.76s -> 198.54s]  is to essentially punish the policy for exploiting the model. So the way that we've
[198.54s -> 203.32s]  punished the policies is by actually changing the reward function in some way, or in general
[203.32s -> 208.76s]  changing the model-based MDP. So one way that we can change it is to have the reward
[208.76s -> 215.04s]  function assign a little penalty, which I'm denoting here with the letter U, for going
[215.04s -> 222.72s]  into states where the model might be incorrect. So U is essentially an uncertainty penalty,
[222.72s -> 226.92s]  and all we're going to do is we're going to add this uncertainty penalty and then
[226.92s -> 232.24s]  use any existing model-based RL algorithm. I'll explain the particular choice of uncertainty
[232.24s -> 237.12s]  penalty that we can use in a second, but first I want to describe the kind of the
[237.12s -> 243.72s]  intuition behind what this is going to do. So if your policy ends up choosing some action
[243.72s -> 248.80s]  that fools the model into going into a state with a higher reward, but the transition
[248.80s -> 255.84s]  is erroneous, then we want the uncertainty penalty to be large in those situations.
[255.84s -> 259.76s]  So the uncertainty penalty basically has to quantify how wrong the model is. If the model
[259.76s -> 263.56s]  goes into an erroneously good-looking state, then the model must have done something
[263.56s -> 267.80s]  wrong, and if we have a good uncertainty penalty, it should be able to catch that.
[267.80s -> 273.20s]  So essentially the uncertainty penalty is supposed to punish the policy enough that
[273.20s -> 277.24s]  exploiting is not worth it, and that's why we have to choose the multiplier lambda
[277.24s -> 282.36s]  carefully, but if we choose it carefully so that the penalty always costs the policy
[282.36s -> 287.16s]  more than it gains by cheating, then the policy will change its behavior and avoid
[287.16s -> 293.44s]  those crazy out-of-distribution states. Technically the condition on U that we need
[293.44s -> 300.36s]  to satisfy is that U should be at least as large as the error in the model measured
[300.36s -> 305.64s]  according to some kind of divergence. Now this is not an easy quantity to quantify
[305.64s -> 310.24s]  because in general you don't actually know how right or wrong your model is. So the
[310.24s -> 314.44s]  way that we would do this in practice is we would use one of those model uncertainty
[314.44s -> 318.76s]  techniques that we discussed in the model-based RL lecture, something like an ensemble.
[318.76s -> 322.80s]  So one common way to do this is to train an ensemble of models and measure the degree
[322.84s -> 328.28s]  of disagreement among the different models in the ensemble as a proxy for this error
[328.28s -> 334.40s]  metric. But in general getting good error metrics, getting good estimates of U that are
[334.40s -> 338.40s]  actually greater than or equal to the true model error is an open problem, so there isn't
[338.40s -> 342.08s]  a great way to do it that is guaranteed to work every time, but ensemble disagreement
[342.08s -> 346.88s]  is one common choice. Let's talk a little bit about the theory
[346.96s -> 353.96s]  behind this. So let's say that we can find a way to estimate U that does in fact provide
[354.68s -> 359.68s]  an error metric that is always at least as large as the true error in the model.
[359.68s -> 364.48s]  Here's an interesting result that we can show. So there are two assumptions here.
[364.48s -> 369.84s]  The first assumption basically is that our value function is expressive enough that
[369.84s -> 374.16s]  we can represent the value accurately. So if we're using large neural nets we can
[374.16s -> 377.52s]  more or less assume that this is true. The second assumption is that assumption
[377.52s -> 381.28s]  on U, and this is a very strong assumption. It says that the model error, the true model
[381.28s -> 385.44s]  error, is bounded above by U, meaning that U is at least as large as the true
[385.44s -> 390.44s]  error of the model as measured by some divergence metric like total variation divergence.
[394.12s -> 399.12s]  A to M here is the true return of the policy that was trained under the model,
[399.96s -> 406.96s]  the model represented by M. Epsilon U is the expected value under the model of this error metric of U.
[420.68s -> 426.80s]  So what the above equation is showing is that if we train a policy pi hat using
[426.80s -> 433.80s]  our model, then the true reward of that policy will be at least as large as the best policy
[436.40s -> 443.40s]  we can find minus its expected error. So we can find the best possible policy but
[450.64s -> 455.68s]  optimized against a different objective, against the reward minus the error. And we can guarantee
[455.68s -> 462.16s]  that our learned policy will be at least as good as that. Another way to interpret this is to
[463.04s -> 470.72s]  introduce this symbol pi delta, and pi delta is the best policy that exists according to the true
[470.72s -> 475.44s]  return, not the model according to the true return. So pi delta is the best policy under
[475.44s -> 481.92s]  the true return for which the expected value of the error, the expected value of U,
[481.92s -> 487.92s]  is bounded by delta. So it's basically the best policy that doesn't visit states where the model
[487.92s -> 496.16s]  might be incorrect, where the model, where that latter phrase is quantified as the average error
[496.16s -> 501.44s]  of the model is less than or equal to delta. What this equation 12 is saying is that the
[501.44s -> 505.84s]  model that we learn, so the policy that we learn under our model will be at least as good
[505.84s -> 513.44s]  in terms of its true return as the best policy whose average error is bounded by delta minus
[513.44s -> 518.00s]  an error term that depends on delta. So if we choose delta to be very small then we will
[518.00s -> 523.68s]  actually improve on this. Okay this might seem a little cryptic but this theorem has a few
[523.68s -> 532.64s]  interesting implications. One implication is if we substitute it in pi beta, the behavior policy,
[532.64s -> 536.80s]  well we would expect that the error of the model under the states visited by the behavior policy
[536.80s -> 540.72s]  will be very low, close to zero, because those are the states that are actually used to train
[540.72s -> 548.24s]  them all. So if delta for that is very low, it's close to zero, then we would expect that this
[548.24s -> 552.40s]  equation 12 would essentially guarantee that the learned policy is at least as good as the
[552.40s -> 556.96s]  behavior policy, which means that we very likely improve over the behavior policy, or at least
[556.96s -> 564.08s]  we don't do worse. The other thing that this can quantify is the optimality gap. So if we plug in
[564.08s -> 570.88s]  the optimal policy pi star, then the policy that we learn is at least as good as pi star
[570.88s -> 578.00s]  minus a penalty for how wrong the model is on the states that pi star visits. So if the model
[578.00s -> 582.72s]  is very accurate for the states and actions visited by the optimal policy, then this would
[582.72s -> 588.16s]  guarantee that we'll recover something close to the optimal policy. So these are interesting
[588.16s -> 593.76s]  results to show. Essentially these results tell us that this method will improve the behavior
[593.76s -> 598.00s]  policy, and it can get something close to the optimal policy if your model is accurate for the
[598.00s -> 601.76s]  states and actions visited by the optimal policy. Now whether the model is accurate
[601.76s -> 609.12s]  in that case or not really depends on the data that you have. So this is a little bit of
[609.12s -> 612.80s]  analysis and hopefully that kind of gives you a taste for the types of results
[612.80s -> 615.76s]  that in general we can try to show for offline RL methods.
[619.12s -> 627.76s]  There's a kind of a more evolved version of this idea where we can actually apply a CQL-like
[627.76s -> 634.72s]  principle to model-based offline RL, and that's a newer algorithm called COMBO. So the basic
[634.72s -> 640.88s]  idea behind COMBO is just like CQL minimizes Q values of policy actions, we can minimize Q
[640.88s -> 648.00s]  values of model state action tuples. So here's our dyna picture again, and we're going to have a
[648.00s -> 655.28s]  loss function for our critic that looks very similar to the CQL loss from before, where
[655.28s -> 659.76s]  we're minimizing Bellman error, but now we're going to be using data from the model to do
[659.76s -> 665.04s]  this, so it's a dyna-style method. We're going to be maximizing the Q values in the data set,
[665.60s -> 670.96s]  and then we'll be pushing down on state and action Q values from the model.
[673.12s -> 678.24s]  So we're trying to make Q values of the model be worse and Q values under the data set
[679.20s -> 683.52s]  be better. And the intuition here is if the model produces something that looks clearly
[683.52s -> 687.52s]  different from real data, it's very easy for the Q function to make that look bad.
[688.16s -> 691.20s]  But if the model produces very realistic states and actions, the ones that are
[691.20s -> 695.28s]  indistinguishable from the ones in the data, then these two terms should really balance out.
[696.80s -> 702.56s]  So it's a somewhat GAN-like idea in a sense. And this is nice because we're not actually
[702.56s -> 706.56s]  changing the reward function, we're just imposing this additional regularizer on the Q
[706.56s -> 710.80s]  function that says Q function, try to make the model-based states and actions look worse.
[711.36s -> 713.92s]  And if you can't make them look worse than the data, that means that they're
[713.92s -> 717.68s]  indistinguishable from the data, which means that the model is actually being correct,
[717.68s -> 720.32s]  the model is not producing states and actions that look unrealistic.
[722.08s -> 725.20s]  And this actually ends up working a little bit better than MOPO and MORAL.
[727.04s -> 732.96s]  Now, both of the algorithms I described so far are dyna-style algorithms. We can't actually
[732.96s -> 737.28s]  do offline RL in a non-dina-style way, we can do offline RL where we actually
[737.28s -> 740.00s]  don't learn a policy at all and we just try to plan under the model,
[740.00s -> 743.52s]  but we still need some mechanism to compensate for out-of-distribution actions.
[744.48s -> 748.56s]  So there are a few ways to do it. One recent paper that I want to tell you about is something
[748.56s -> 755.52s]  called the trajectory transformer. So the basic idea here is that we would train a joint state
[755.52s -> 758.96s]  action model. So we're not just going to train a model that predicts future states,
[758.96s -> 763.52s]  conditional current states and actions, we'll train a model over entire trajectories.
[763.52s -> 767.36s]  So this model will provide us with probabilities of state action sequences
[768.40s -> 771.68s]  by just doing density estimation on the trajectories in the data set.
[771.68s -> 774.72s]  And I'm going to use a subscript beta to denote that this distribution,
[774.72s -> 776.64s]  of course, depends on the behavior policy.
[779.12s -> 782.96s]  And intuitively, what we're going to do with this distribution is we're going to
[782.96s -> 789.44s]  optimize for a plan for a sequence of actions that has high probability under this distribution,
[789.44s -> 794.48s]  which means that we will avoid actions that are very unlikely in the data, which we'll avoid
[794.48s -> 799.60s]  out-of-distribution actions. And then the other thing we can do is, once we're doing
[800.24s -> 804.16s]  offline model-based RL, we can use a very big and expressive model, such as a transformer.
[805.20s -> 808.64s]  That design decision is largely orthogonal to the first point, so you could do the
[808.64s -> 812.56s]  number one with any model class. But if you're doing offline RL, it's actually very convenient
[812.56s -> 817.04s]  to use a very large, very expressive model class, because you don't have to do active
[817.04s -> 820.72s]  data collection, you don't have to update your model between trials, which means that it's
[820.72s -> 824.00s]  okay for the model to be really big and computationally expensive. So you want,
[824.00s -> 827.36s]  basically, the most powerful density estimation model you can get your hands on.
[827.36s -> 831.36s]  And these days, if you want a really powerful sequence density estimation model,
[831.36s -> 835.20s]  a transformer is a good choice, although the same thing could be done with other kinds of density
[835.20s -> 841.36s]  estimators. So I have a reference at the bottom here to a paper that actually describes
[842.16s -> 848.32s]  an approach of this sort, and the model that's used there actually is a transformer model.
[849.12s -> 855.52s]  And in order to model multimodal distributions, it actually discretizes the entire trajectory.
[855.52s -> 859.92s]  Now you can't discretize the trajectory as a whole, because there will be exponentially many
[859.92s -> 864.88s]  discrete states, so instead the discretization is done per dimension of every state and action.
[864.88s -> 868.48s]  So it's not a sequence model over time steps, it's actually a sequence model over
[868.48s -> 873.68s]  dimensions of states and actions. So transformers and other sequence models
[873.68s -> 878.96s]  operate at the level of tokens. So the first token is the first dimension of the state of the
[878.96s -> 883.84s]  first time step. And based on that token, the model predicts the second dimension of the
[883.84s -> 890.40s]  state of the first time step. Then it predicts the third dimension, the fourth, and so on,
[890.40s -> 894.56s]  until you get to the last dimension of the state of the first time step, and from that it
[894.56s -> 899.12s]  predicts the first dimension of the action of the first time step. Based on that it predicts
[899.12s -> 902.88s]  the second dimension, and so on and so on and so on, until we get to the last dimension of
[902.88s -> 907.44s]  the first action, and then you predict the first dimension of the state of the second time step,
[907.44s -> 910.32s]  and so on until you get to the very end of the trajectory, until you get to the last
[910.32s -> 916.32s]  dimension of the last action. Now, I drew this model here as a autoregressive
[916.32s -> 920.64s]  sequence model, which you could do with something like an LSTM, but you can also
[920.64s -> 925.96s]  do this with a transformer. You need a causal mask for the transformer, so
[925.96s -> 929.08s]  this would be something they would do in something like a GPT-style model. If
[929.08s -> 931.60s]  you're not too familiar with transformers, don't worry about this
[931.60s -> 934.76s]  too much. This is basically the main idea is that any kind of sequence model
[934.76s -> 939.92s]  could be used. Now, one nice thing about this is because you're modeling
[939.92s -> 944.16s]  state and action probabilities, you can make very accurate predictions out to
[944.16s -> 948.96s]  much, much longer horizons. So this problem of accumulating errors is a big
[948.96s -> 951.48s]  problem if you're selecting new actions different from those seen in the
[951.48s -> 955.60s]  data set. But if you're keeping, if you're restricting yourself to actions
[955.60s -> 958.12s]  similar to those seen in the data set, then you can make very accurate
[958.12s -> 961.92s]  predictions very far into the future. So this this animation is actually
[961.92s -> 965.30s]  showing the trajectory transformer making predictions for the humanoid
[965.30s -> 968.36s]  hundreds of steps into the future. So this is not a simulation, this is
[968.40s -> 972.48s]  actually just a prediction from the model. And then you can use this
[972.48s -> 977.64s]  model to do planning. Now, you can use many of the same techniques for
[977.64s -> 981.36s]  planning as what we discussed in the model-based control lecture a few
[981.36s -> 985.20s]  weeks ago, but it's important to take the action probabilities into account
[985.20s -> 988.32s]  because you don't want the plan to produce actions that have a low
[988.32s -> 993.04s]  probability under the data. One approach to this would be to use beam
[993.04s -> 996.44s]  search, which already works quite well with these sequence models, but instead
[996.44s -> 999.60s]  of using beam search to maximize probability, use beam search to
[999.60s -> 1004.12s]  maximize the reward. So given the current subsequence, so let's say
[1004.12s -> 1007.48s]  you've decoded up to time step three, select the next token from
[1007.48s -> 1010.56s]  the model by just sampling it from the probability distribution,
[1011.32s -> 1014.52s]  sample many tokens, let's say you sample k tokens, and then you
[1014.52s -> 1019.26s]  store the top k with the highest cumulative reward. So that's
[1019.26s -> 1022.56s]  basically beam search. So you have k prefixes for each one, you
[1022.60s -> 1027.56s]  sample k tokens, sort them, and take the top k. And top here
[1027.56s -> 1030.00s]  means top in terms of total reward, and you do this one time
[1030.00s -> 1033.32s]  step at a time. That's the basic high level idea. There are some
[1033.32s -> 1036.00s]  details to get this to actually work, but that's the principle.
[1036.44s -> 1038.36s]  Now, of course, you could use any other planning method. You
[1038.36s -> 1042.64s]  could use MCTS with this, you could use even differentiable
[1042.64s -> 1046.60s]  things, although you would have to be careful to turn those
[1046.60s -> 1050.16s]  discodes into continuous values. The important thing is that you
[1050.16s -> 1053.76s]  need to make sure that you're maximizing reward and also
[1053.76s -> 1056.40s]  taking the probabilities into account. So the beam search
[1056.40s -> 1058.44s]  approach takes probabilities into account because when it
[1058.44s -> 1064.00s]  samples those k tokens, they're sampled from P beta. So
[1064.08s -> 1066.40s]  they're sampled from a distribution of tokens that have
[1066.40s -> 1068.56s]  a high probability under the data set, and then you select
[1068.56s -> 1073.40s]  the best one among those, and that's okay. So why does this
[1073.40s -> 1077.16s]  work? Well, generating high probability trajectories avoids
[1077.20s -> 1079.04s]  out of distribution states and actions because you're
[1079.04s -> 1081.56s]  actually using P beta both to select the states and the
[1081.56s -> 1085.24s]  actions. And using really big models works well for offline
[1085.24s -> 1087.36s]  RL because you can use lots of compute and capture
[1087.36s -> 1089.84s]  complex behavior policies. So you can capture complex
[1089.84s -> 1092.08s]  dynamics, you can also capture complex behavior
[1092.08s -> 1092.80s]  policies.
