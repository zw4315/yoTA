# Detected language: en (p=1.00)

[0.00s -> 4.92s]  Alright, in the last portion of today's lecture, I'll go through some tips and
[4.92s -> 8.18s]  tricks for implementing Q-learning algorithms, which might be useful for
[8.18s -> 12.58s]  homework 3, and then I'll give a few examples of papers that have used
[12.58s -> 18.06s]  variants of the methods that I described in this lecture. So first, a
[18.06s -> 22.62s]  few practical tips. Q-learning methods are generally quite a bit more
[22.62s -> 25.92s]  finicky to use than policy gradient methods, so they tend to require a
[25.96s -> 30.76s]  little bit more care to use correctly. It takes some care to stabilize Q-learning
[30.76s -> 35.56s]  algorithms, and what I would recommend is to start off by testing your algorithms
[35.56s -> 39.54s]  on some easy, reliable problems where you know that your algorithm should work,
[39.54s -> 43.88s]  just to make sure your implementation is correct, because essentially you have
[43.88s -> 47.24s]  to go through several different phases of troubleshooting. You first have to
[47.24s -> 49.48s]  make sure that you have no bugs, then you have to make sure that you
[49.48s -> 53.32s]  tune your hyperparameters, and then get it to work on your real problems. So
[53.32s -> 56.60s]  you want to do the debugging before the hyperparameter tuning, which means that
[56.60s -> 59.20s]  you want to do it on really easy problems, where basically any correct
[59.20s -> 64.12s]  implementation should really work. Q-learning performs very differently on
[64.12s -> 70.76s]  different problems. So these are some plots of DQN-type experiments on a
[70.76s -> 74.48s]  variety of different Atari games, and something you might notice is that
[74.48s -> 78.12s]  there's a huge difference in the stability of these methods. So for Pong,
[78.32s -> 84.24s]  your reward basically steadily goes up, and then flatlines. For Breakout, it kind of
[84.24s -> 87.00s]  goes up and then wiggles a whole bunch, and then for some of the harder
[87.00s -> 90.28s]  games like Video Pinball and Venture, it's just completely all over the place.
[90.28s -> 94.04s]  And the different colored lines here simply represent different runs of the
[94.04s -> 97.56s]  same exact algorithm with different random seeds. You can see the
[97.56s -> 100.92s]  different random seeds for Pong are basically identical. For Breakout, they're
[100.92s -> 103.72s]  kind of qualitatively the same but have different noise, whereas for
[103.72s -> 109.52s]  something like Venture, some of the runs work and some fail completely. Large
[109.52s -> 113.08s]  replay buffers do tend to help to improve stability quite a lot, so using
[113.08s -> 117.88s]  a replay buffer of a size about 1 million can be a pretty good choice. And
[117.88s -> 120.36s]  at that point the algorithm really starts looking a lot more like fit-a-
[120.36s -> 123.88s]  queue iteration, which is perhaps part of the explanation for its improved
[123.88s -> 130.12s]  stability. And lastly, Q-learning takes a lot of time, so be patient. It might be
[130.12s -> 134.00s]  no better than random for a long time while that random exploration finds the
[134.00s -> 137.00s]  good transitions, and then it might take off once those good transitions are
[137.00s -> 140.92s]  found. And many of you will probably experience this in Homework 3 when
[140.92s -> 145.12s]  you train on the Pong video game. Start with high exploration, start with
[145.12s -> 148.44s]  large values of epsilon, and then gradually reduce exploration as you go,
[148.44s -> 151.92s]  because initially your Q-function is garbage anyway, so it's mostly the
[151.92s -> 154.72s]  random exploration that will be doing most of the heavy lifting. And then
[154.72s -> 158.40s]  later on, once your Q-function gets better, then you can decrease epsilon. So
[158.40s -> 164.16s]  it often helps to put it on a schedule. A few more advanced tips for Q-learning.
[164.16s -> 170.24s]  The errors of the Bellman error, the gradients of the Bellman error, can be
[170.24s -> 174.96s]  very big. So it's kind of a least squares regression, so these squared
[174.96s -> 178.32s]  error quantities can be large quantities, which means their gradients can be very
[178.32s -> 181.80s]  large. And something that's a little troublesome is that if you have a
[181.80s -> 185.72s]  really bad action, you don't really care about the value of that action, but
[185.80s -> 190.24s]  your squared error objective really cares about figuring out exactly how bad
[190.24s -> 193.60s]  it is. So if you have some good actions that are like plus 10, plus 9,
[193.60s -> 196.84s]  plus 8, and you have some bad actions that are minus 1 million, that
[196.84s -> 199.76s]  minus 1 million will create a huge gradient, even though you don't really
[199.76s -> 203.80s]  care that's minus 1 million. Like if you were to guess minus 900,000,
[203.80s -> 207.52s]  it would result in the same policy. But your Q-function objective
[207.52s -> 211.44s]  really cares about that, and that will result in big gradients. So what you
[211.44s -> 215.00s]  can do is you can either clip your gradients, or you can use what's
[215.00s -> 219.20s]  called a Huber loss. A Huber loss you can think of as kind of interpolating
[219.20s -> 222.64s]  between a squared error loss and an absolute value loss. So far away from
[222.64s -> 227.00s]  the from the minimum, the Huber loss looks like absolute value, and close to
[227.00s -> 230.92s]  the minimum, because absolute value is a non-differentiable cusp, the Huber loss
[230.92s -> 237.32s]  actually flattens it out with a quadratic. So the green curve here on the
[237.32s -> 241.28s]  right shows a Huber loss, whereas the blue curve shows a quadratic loss. So the
[241.28s -> 245.44s]  Huber loss action mechanically behaves very similarly to clipping gradients,
[245.44s -> 250.56s]  but it can be a little easier to implement. Double Q-learning helps a lot
[250.56s -> 254.36s]  in practice. It's very simple to implement, and it basically has no
[254.36s -> 260.44s]  downsides. So probably a good idea to use double Q-learning. N-step returns can
[260.44s -> 263.84s]  help a lot, especially in the early stages of training, but they do have
[263.84s -> 267.80s]  some downsides, because n-step returns will systematically bias your objective,
[267.96s -> 272.00s]  especially for larger values of n. So be careful with n-step returns, but do
[272.00s -> 276.20s]  keep in mind that they can improve things in the early stages of training.
[276.20s -> 280.60s]  Schedule exploration and schedule learning rates. Adaptive optimization
[280.60s -> 284.84s]  rules like Adam can also help a lot. So some of the older work use things
[284.84s -> 289.04s]  like our RMSprop, which doesn't work quite as well as the most recent
[289.04s -> 294.72s]  adaptive optimizers like Adam. So good idea to use Adam. And also when
[294.72s -> 297.76s]  debugging your algorithm, make sure to run multiple random seeds, because you'll
[297.76s -> 301.48s]  see a lot of variation between random seeds. You'll see that the algorithm is
[301.48s -> 304.72s]  very inconsistent between runs, so you should run a few different random seeds
[304.72s -> 307.56s]  to make sure that things are really working the way you expect and that
[307.56s -> 310.92s]  you didn't get a fluke. And the fluke can either be unusually bad or
[310.92s -> 316.16s]  unusually good, so keep that in mind. Okay, so in the last portion of the
[316.16s -> 318.72s]  lecture, what I'm going to do is I'm going to go through a few examples of
[318.72s -> 322.36s]  previous papers that have used algorithms that relate to the ones
[322.36s -> 326.04s]  that I covered in this lecture. The first paper that I want to briefly
[326.04s -> 329.48s]  talk about is this paper called Autonomous Reinforcement Learning from
[329.48s -> 335.28s]  Raw Visual Data by Lange and Riedmiller, or Lange and Riedmiller rather. So this is a
[335.28s -> 339.56s]  paper from 2012. It's quite an old paper, and it's one of the actually
[339.56s -> 344.00s]  earliest papers that used deep learning with fit accuteration methods.
[344.00s -> 348.28s]  The particular procedure this paper used though is a little different from
[348.28s -> 351.28s]  the methods that I covered in this lecture. It's actually more similar to some
[351.28s -> 354.56s]  of the model-based algorithms that I'll discuss later. So in this paper, what
[354.60s -> 359.04s]  the authors did is they actually learn a kind of a latent space representation of
[359.04s -> 363.92s]  images by using an autoencoder, and then they actually run fit accuteration
[363.92s -> 368.04s]  on the latent space of this autoencoder, on the feature space, but the
[368.04s -> 371.48s]  particular fit accuteration they use actually doesn't use neural networks. It
[371.48s -> 377.00s]  uses something called random trees. So they use a non-deep but still fit
[377.00s -> 380.80s]  accuteration procedure on the representation learned by a deep neural
[380.84s -> 384.80s]  network. So it's Q-learning on top of a latent space learned with an autoencoder,
[384.80s -> 389.80s]  using fit accuteration, and something called extra random trees for function
[389.80s -> 393.72s]  approximation. You can think of extra random trees as basically very similar
[393.72s -> 397.72s]  to random forces. And the demonstration that they had in this paper, which was
[397.72s -> 402.16s]  pretty cool, is to use an overhead camera to look at this little slot car
[402.16s -> 405.88s]  racetrack and then learn to control the slot car to drive around the
[405.88s -> 412.84s]  racetrack. Here is a paper that uses convolutional neural networks with
[412.84s -> 416.28s]  Q-learning. This is deep Q-learning. So this is a paper called human-level
[416.28s -> 419.68s]  control through deep RL, and this paper uses Q-learning with
[419.68s -> 424.20s]  ConvNets with replay buffers and target networks and this kind of
[424.20s -> 429.08s]  simple one-step backup that I mentioned and one gradient step to play
[429.08s -> 432.84s]  Atari games. And this can be improved a lot, so it can be improved a lot with
[432.84s -> 435.80s]  double Q-learning. The original method in this paper can actually also be
[435.80s -> 439.04s]  improved a lot just by using Adam, so that alone actually gets you much much
[439.04s -> 444.40s]  better performance. But for homework 3 you'll be implementing something
[444.40s -> 449.72s]  fairly similar to this paper. Here is a paper on Q-learning with continuous
[449.72s -> 454.88s]  actions for robotic control application, or kind of a simulated control
[454.88s -> 459.28s]  application. So this is the the GDPG paper called continuous control with
[459.28s -> 463.64s]  deep reinforcement learning. It uses continuous actions with a maximizer
[463.68s -> 468.48s]  network and uses a replay buffer and target networks of poly-cabraging with
[468.48s -> 472.72s]  a one-step backup and one gradient step per simulation step. And they
[472.72s -> 479.84s]  evaluated on some kind of simple toy low-dimensional simulator robotics tasks.
[479.84s -> 484.88s]  Here's a paper that actually uses a deep Q-learning algorithm with continuous
[484.88s -> 488.64s]  actions for real-world robotic control, and this actually kind of exploits
[488.64s -> 491.88s]  some of the parallelism ideas that I discussed before. So here you have
[491.92s -> 496.36s]  multiple robots learning in parallel to open doors. It's a paper called robotic
[496.36s -> 500.00s]  manipulation with deep reinforcement learning and asynchronous off-poly updates.
[500.00s -> 504.24s]  And this uses that NAF representation, so this is a Q-function that is
[504.24s -> 508.64s]  quadratic in the actions, making maximization easier. You use a replay
[508.64s -> 512.36s]  buffer and target network, a one-step backup, and this one actually uses four
[512.36s -> 516.72s]  gradient steps per simulation step to improve the efficiency. Because
[516.72s -> 520.08s]  collecting data from the robots, I guess it's not even simulation, it's actually
[520.08s -> 523.68s]  real-world, collecting data from the robots is expensive, so you'd like to do
[523.68s -> 527.16s]  as much computation with as little data as possible. And it's further
[527.16s -> 533.56s]  paralyzed across multiple robots for better efficiency. This method, which I
[533.56s -> 537.72s]  showed actually in lecture one, is also a deep Q-learning algorithm that
[537.72s -> 544.24s]  takes this paralyzed interpretation of fitted Q-iteration to the extreme. So here
[544.24s -> 547.24s]  there are multiple robots that are learning grasping, all in parallel, and
[547.36s -> 552.40s]  there are actually multiple workers that are all computing target values, multiple
[552.40s -> 556.48s]  workers that are all performing regression, and a separate worker that
[556.48s -> 560.32s]  is managing the replay buffer. So this is literally instantiating that system
[560.32s -> 565.04s]  that I showed before with process one, process two, and process three, and in
[565.04s -> 568.28s]  this case each of those processes are themselves forked off into multiple
[568.28s -> 574.24s]  different workers on a large server farm. All right, if you want to learn
[574.28s -> 578.64s]  more about Q-learning, some suggested readings, classical papers, this is the
[578.64s -> 581.84s]  Watkins Q-learning paper that introduced the Q-learning algorithm in
[581.84s -> 587.88s]  1989. This paper, called neural fitted Q-iteration, introduces batch mode
[587.88s -> 592.56s]  Q-learning with neural networks. Some deep RL papers for Q-learning, the Wang and
[592.56s -> 597.88s]  Riedmiller paper that I mentioned before, the DQN paper, this is the paper
[597.88s -> 601.20s]  that introduced double Q-learning, this is the paper that introduced that
[601.24s -> 607.04s]  approximate maximization with mu theta, this is the paper that introduced NAF.
[607.04s -> 610.16s]  This is a paper that introduces something called dueling network
[610.16s -> 615.24s]  architectures, which is very very similar to the NAF architecture, but
[615.24s -> 619.16s]  adapted for discrete action spaces. I didn't cover this in the lecture, but
[619.16s -> 623.60s]  it's also a pretty useful trick for making Q-learning work better. All
[623.60s -> 625.96s]  right, so these are the suggested readings, and you can find them in the
[625.96s -> 628.48s]  slides if you want to learn more. I highly encourage you to check it out.
[628.72s -> 633.40s]  And just to recap what we've covered in today's lecture, we talked about Q-learning
[633.40s -> 636.56s]  in practice, how we can use replay buffers and target networks to
[636.56s -> 640.80s]  stabilize it, we talked about a generalized view of fitted Q-iteration
[640.80s -> 645.32s]  in terms of three processes, we talked about how double Q-learning can make
[645.32s -> 649.16s]  Q-learning algorithms work a lot better, how we can do multi-step Q-learning,
[649.16s -> 653.92s]  how we can do Q-learning with continuous actions, including with random
[653.92s -> 658.64s]  sampling, analytic optimization, and a second actor network, and that's it.
