# Detected language: en (p=1.00)

[0.00s -> 4.74s]  All right, so in the next part of today's lecture we're going to extend the
[4.74s -> 9.26s]  discussion of LQR that we had in the previous part to stochastic dynamics
[9.26s -> 14.58s]  and also to nonlinear systems. So let's start with stochastic dynamics because
[14.58s -> 19.44s]  that's actually a pretty easy one. Before we had linear dynamics that are
[19.44s -> 25.30s]  deterministic, if we have stochastic dynamics, in the special case of the
[25.30s -> 30.50s]  stochastic dynamics are Gaussian, meaning that p of x t plus 1 given x t u t is a
[30.50s -> 34.56s]  multivariate normal distribution with a mean given by the linear dynamics and a
[34.56s -> 41.06s]  constant covariance, then it turns out that exactly the same control law that
[41.06s -> 45.54s]  we had before turns out to still be optimal. I'm not going to derive this,
[45.54s -> 51.46s]  although you can derive it as a homework exercise on your own, but the
[51.46s -> 54.34s]  intuition for why that's true is that a Gaussian is symmetric, which means
[54.34s -> 59.30s]  that if your mean is a particular value of x t, if you go a little to the left
[59.30s -> 63.50s]  and a little to the right, those differences will actually cancel out in your
[63.50s -> 67.06s]  quadratic cost and you'll end up with the same value. So adding Gaussian
[67.06s -> 72.64s]  noise turns out to not change the solution for u t. However, there is a
[72.64s -> 75.98s]  little bit of a nuance to this, which is that adding Gaussian noise does
[75.98s -> 80.58s]  change the states that you end up visiting. So remember in LQR we had this
[80.58s -> 83.42s]  backward recursion and the forward recursion computes the states that you
[83.42s -> 87.70s]  visit. Now the states that you visit are actually stochastic, which means you
[87.70s -> 92.10s]  can't produce a single open-loop sequence of actions, but you can treat
[92.10s -> 96.70s]  that expression for the optimal action k x plus little k as a controller,
[96.70s -> 100.88s]  as a policy, and it turns out that if you use that policy, that turns out to
[100.88s -> 105.02s]  be the optimal closed-loop policy in the linear quadratic Gaussian case.
[105.02s -> 109.90s]  That's quite an interesting result. So there's no change to the algorithm.
[109.90s -> 115.30s]  You can ignore the sigma due to symmetry, and if you want to check this
[115.30s -> 117.38s]  on your own, the hint is that the expectation of a quadratic function
[117.38s -> 120.86s]  under a Gaussian actually has an analytic solution. You can look this up.
[120.86s -> 125.10s]  And once you can express the expected value of your quadratic functions under
[125.10s -> 129.50s]  these Gaussians, you can calculate their derivatives, set the derivative to 0, and you
[129.50s -> 132.86s]  will find that the control law is the same. But the important
[132.86s -> 136.38s]  difference here is that now you are not getting a single sequence of states
[136.38s -> 141.02s]  and actions, you're really getting a closed-form control law out of LQR.
[141.02s -> 143.82s]  So that's kind of interesting because it turns out that LQR actually does
[143.82s -> 151.34s]  produce closed-loop plans, not just open-loop plans. So xc is now sampled
[151.34s -> 154.42s]  from some distribution, and it's no longer deterministic. It turns out to
[154.42s -> 162.50s]  actually still be Gaussian, which is very convenient. Okay, so this is
[162.54s -> 167.34s]  basically the stochastic closed-loop case, and the particular form of pi that
[167.34s -> 172.70s]  we ended up with after using LQR is a time-varying linear controller. So our
[172.70s -> 178.14s]  action is now linear in the state as capital K times the state plus little k,
[178.14s -> 181.90s]  but it's potentially a different capital and lowercase k at every time
[181.90s -> 187.06s]  step. So this maybe gives us some idea of an alternative to global neural net
[187.06s -> 193.02s]  policies. All right, so that's kind of the easy part. Now the main thing
[193.02s -> 195.06s]  I'm going to talk about in this portion of the lecture is actually what
[195.06s -> 200.70s]  happens in the nonlinear case, and in the nonlinear case we can extend LQR to
[200.70s -> 204.14s]  get something that is sometimes called differential dynamic programming or DDP
[204.14s -> 209.86s]  and also sometimes called iterative LQR or ILQR, also sometimes called ILPG if
[209.86s -> 214.74s]  you have the linear Gaussian setting. So before we had these linear quadratic
[214.74s -> 218.42s]  functions, which were the assumption that our dynamics is a linear function
[218.42s -> 223.90s]  of x and u, and our cost is a quadratic function of x and u. So can we
[223.90s -> 230.84s]  approximate some nonlinear system as a linear quadratic system locally? Take a
[230.84s -> 234.46s]  moment to think about this question. What are some mathematical tools that
[234.46s -> 241.10s]  you know of that can allow us to do this? Well, one thing we can do is we
[241.10s -> 245.10s]  can employ a Taylor expansion. So if you have some nonlinear function and you
[245.10s -> 248.16s]  want to get, let's say, a first-order function or a second-order function that
[248.16s -> 251.58s]  approximates it in some neighborhood, what you can do is that you can
[251.58s -> 254.94s]  compute its first and second derivatives and then employ the Taylor
[254.94s -> 260.44s]  expansion. So if we have some current sequence of states and actions, maybe the
[260.44s -> 264.46s]  best states and actions we've found so far, which I'm denoting with x-hat, so
[264.46s -> 269.78s]  you have x-hat 1, x-hat 2, x-hat 3, u-hat 1, u-hat 2, u-hat 3, then you can
[269.82s -> 278.70s]  express the dynamics approximately as the f evaluated at x-hat and u-hat, which is
[278.70s -> 283.86s]  just x-hat t plus 1, plus the gradient of the dynamics with respect to the
[283.86s -> 288.22s]  state and action. And similarly you can express the cost with a linear term
[288.22s -> 290.82s]  depending on the gradient and a quadratic term depending on the
[290.82s -> 296.66s]  hessian. So now we've approximated our dynamics and our cost as linear and
[296.66s -> 300.90s]  quadratic in the neighborhood of some sequence of states and actions denoted
[300.90s -> 307.90s]  by x-hat and u-hat. And if you do this then, you know, you can express this
[307.90s -> 311.94s]  linear quadratic system, I'm going to call it f-bar and c-bar, in terms of
[311.94s -> 317.46s]  the deviations of x from x-hat. So this delta x and delta u represents x
[317.46s -> 322.62s]  minus x-hat and u minus u-hat. So these are deviations, these are
[322.62s -> 326.78s]  differences from x-hat and u-hat. And now we're back in the linear quadratic
[326.78s -> 332.98s]  regime, which means that we can simply plug in this thing into the regular LQR
[332.98s -> 338.42s]  algorithm and solve for the optimal delta x and delta u. So delta x and
[338.42s -> 344.22s]  delta u are the deviation from x-hat and u-hat. You can use LQR to solve for
[344.22s -> 348.10s]  the optimal delta x and delta u and then add them to the old x-hats and
[348.10s -> 357.02s]  u-hats to find new x's and u's. So here is the iterative LQR algorithm based on
[357.02s -> 360.30s]  this idea. We're going to repeat the following process until convergence.
[360.30s -> 365.54s]  For all the time steps, we're going to calculate the dynamics matrix as the
[365.54s -> 369.54s]  gradient of the dynamics around x-hat and u-hat, and we're going to calculate a
[369.54s -> 374.02s]  linear quadratic term for the cost. Then we're going to run the LQR
[374.02s -> 379.90s]  backward pass using delta x and delta u as our state. And then we're going to
[379.90s -> 384.10s]  run the forward pass, but for the forward pass we're actually going to
[384.10s -> 389.14s]  use the original nonlinear dynamics. So we're not going to actually use the
[389.14s -> 392.42s]  linearized dynamics to get xt plus 1, we're going to use the original
[392.42s -> 396.10s]  nonlinear dynamics. And the reason that we do this is because we want to get
[396.10s -> 400.70s]  the xt plus 1 that will actually result from taking that actual
[400.78s -> 405.06s]  u-t, not just some approximation of that. So we'll do the forward pass with
[405.06s -> 411.74s]  linear dynamics and u given by capital K times delta x plus lowercase k plus u-
[411.74s -> 414.90s]  hat, which we just get from substituting in the delta x and delta
[414.90s -> 420.74s]  u equations. And then we will update x-hat and u-hat by simply setting them
[420.74s -> 424.46s]  to be the x's and u's that we got from our forward pass, and then repeat this
[424.46s -> 429.86s]  process. So essentially the backward pass computes a controller expressed in
[429.90s -> 434.66s]  terms of delta x and delta u that will give you better costs than we had so
[434.66s -> 440.42s]  far. The forward pass checks how good that controller actually is and checks
[440.42s -> 443.26s]  which states you will get from running it, and then updates x-hat and
[443.26s -> 450.14s]  u-hat to be those new states and actions. All right, so why does this
[450.14s -> 455.34s]  work? Well, let's compare this procedure to a well-known optimization algorithm
[455.34s -> 458.62s]  which is Newton's method. So Newton's method is a procedure that you might
[458.66s -> 464.10s]  use for minimizing some function g of x. In Newton's method you repeat the
[464.10s -> 468.82s]  following process until convergence. Compute the gradient at some initial x-
[468.82s -> 475.18s]  hat, compute the Hessian, and then set x-hat to be the arg min of the
[475.18s -> 478.46s]  quadratic approximation of the function formed by that gradient in
[478.46s -> 484.70s]  Hessian, and then repeat. This is very much like what iterative LQR does.
[484.70s -> 488.38s]  So iterative LQR is basically the same idea. You locally approximate a
[488.38s -> 492.90s]  complex nonlinear function via its Taylor expansion which leads to a very
[492.90s -> 497.90s]  simple optimization problem in the LQR, a linear quadratic problem, and then you
[497.90s -> 503.18s]  repeat this process multiple times until you arrive at a local optimum. In
[503.18s -> 507.90s]  fact, ILQR can be viewed as an approximation of Newton's method for
[507.90s -> 511.94s]  solving that original optimization problem we posed. The main way in which
[511.94s -> 516.90s]  ILQR differs from Newton's method is that it doesn't consider the second
[516.94s -> 521.90s]  derivative of the dynamics. It's not too hard to derive a version of LQR that
[521.90s -> 525.70s]  does consider quadratic dynamics, and if you do that you get exactly Newton's
[525.70s -> 530.18s]  method and you still have an elegant recursive formulation for it, and that is
[530.18s -> 536.54s]  what differential dynamic programming, or DDP, is doing. So Newton's method needs
[536.54s -> 539.78s]  to use second-order dynamics approximations, which is reasonable
[539.78s -> 543.50s]  although it requires a tensor product because the second derivative of the
[543.50s -> 547.74s]  dynamics now is a 3D tensor, and that's what differential dynamic programming
[547.74s -> 552.82s]  does. So if you really want to consider a full Newton's method, check out DDP,
[552.82s -> 558.30s]  although in practice just linearizing dynamics tends to be pretty good.
[558.78s -> 564.38s]  Now the connection to Newton's method allows us to derive a little
[564.38s -> 568.50s]  improvement to the iterative LQR procedure that I presented so far that
[568.50s -> 572.98s]  ends up being very important for good practical performance. So let's go
[572.98s -> 576.66s]  back to regular Newton's method to gain some intuition. Here is the
[576.66s -> 581.26s]  optimization that Newton's method does in the inner loop, and we could ask why
[581.26s -> 585.46s]  is this a really bad idea? Think about this for a minute. If you actually
[585.46s -> 590.78s]  do this repeatedly, I would posit that for many real functions you will
[590.78s -> 594.22s]  really struggle to find a local optimum. Think about why that might be
[594.22s -> 600.18s]  the case. So here's a picture that illustrates that point. Let's say that
[600.26s -> 603.82s]  the blue line represents your function and you're currently located at this
[603.82s -> 609.06s]  point. Now Newton's method approximates your complicated function with a
[609.06s -> 612.18s]  quadratic function, and let's say that the first and second derivative of
[612.18s -> 618.66s]  your blue function results in this quadratic approximation. If you
[618.66s -> 623.02s]  actually go to the optimum of this quadratic, you'll end up at this point, and this
[623.02s -> 627.90s]  point is actually worse than the point that you started at. So what you want is
[627.90s -> 632.26s]  you want to kind of backtrack and find a point that is close to your starting
[632.26s -> 637.58s]  point where this quadratic approximation is still trustworthy. This notion of
[637.58s -> 641.66s]  trustworthiness is very related to the trust regions that we discussed in the
[641.66s -> 650.26s]  advanced policy gradients lecture last time. So using this intuition, let's go
[650.26s -> 654.86s]  back to our iterative LQR algorithm. Where in the iterative LQR algorithm can we
[654.86s -> 658.90s]  perform this backtracking? So essentially what we want to do is we want to compute our
[658.90s -> 662.58s]  solution, and then we want to check if the solution is actually better than
[662.58s -> 666.14s]  what we had before, and if it's not better, then we want to somehow move closer
[666.14s -> 673.06s]  to where we were before. It turns out that the forward pass is a very
[673.06s -> 677.58s]  convenient place to do this, and a very simple way to modify the forward
[677.58s -> 683.70s]  pass to perform this line search is to modify the constant term, the little k, by
[683.70s -> 687.62s]  some constant alpha between 0 and 1. So this is the only change that we've made.
[687.62s -> 692.38s]  This constant alpha allows us to control how much we deviate from our
[692.38s -> 697.34s]  starting point. So imagine what would happen if I set alpha to 0. If I set
[697.34s -> 704.54s]  alpha to 0, then at the first time step my action would be x1 minus x1 hat, but
[704.54s -> 709.42s]  x1 is always the same, so that's just 0, alpha is 0, so that means that my
[709.46s -> 714.50s]  first action is just u hat 1, and because my first action is u hat 1, my
[714.50s -> 722.78s]  second state is x hat 2, which means that x2 minus x hat 2 is also 0, alpha
[722.78s -> 727.02s]  is 0, which means my second control is also u hat 2, so if alpha goes to 0, I
[727.02s -> 731.54s]  will execute exactly the same actions that I executed before. And in general,
[731.54s -> 738.96s]  as I reduce alpha, I will come closer and closer to the action
[738.96s -> 744.12s]  sequence that I had before. So you can search over alpha until you get
[744.12s -> 749.24s]  improvement. Essentially you can run the backward pass and then
[749.24s -> 753.08s]  run the forward pass repeatedly with different values of alpha until you
[753.08s -> 757.36s]  find one that you're happy with. Now in practice you actually want to be a
[757.36s -> 761.28s]  little more ambitious, so a very simple way to do it is to just reduce alpha
[761.28s -> 765.32s]  until the new cost is lower than the old cost, but the other thing you
[765.32s -> 768.56s]  could do is you could calculate how much improvement in cost you would
[768.56s -> 771.56s]  anticipate from your quadratic approximation, and you could actually
[771.56s -> 774.60s]  reduce alpha until you get some fraction of that anticipated improvement.
[774.60s -> 778.92s]  You could also do a bracketing line search, where you basically assume that
[778.92s -> 784.64s]  the true function looks roughly quadratic in between the blue
[784.64s -> 788.72s]  circle and the red cross, and a bracketing line search will
[788.72s -> 793.00s]  basically find the optimum of that function. So there are many other ways
[793.00s -> 796.40s]  to do line searches, but if you want to implement this in practice, look up
[796.40s -> 799.44s]  something like a bracketing line search, and that can be a pretty good choice.
