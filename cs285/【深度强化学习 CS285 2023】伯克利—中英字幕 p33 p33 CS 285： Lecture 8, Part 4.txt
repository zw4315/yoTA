# Detected language: en (p=1.00)

[0.00s -> 5.94s]  In the next section of today's lecture I'm going to discuss some practical
[5.94s -> 10.50s]  considerations that we need to take into account when actually implementing Q
[10.50s -> 13.98s]  learning algorithms, and then some improvements that can make them work a
[13.98s -> 19.74s]  bit better. So one question that we can start with, are our Q-values
[19.74s -> 24.24s]  actually accurate? We can think of Q-values as this kind of abstract
[24.24s -> 28.26s]  object that we use inside reinforcement learning to help us improve our
[28.30s -> 33.26s]  policy and get that arc max, but a Q-function is also a prediction. It's a
[33.26s -> 37.14s]  prediction about the total reward that you will get in the future if you start
[37.14s -> 41.78s]  in a particular state of action and then follow your policy. So it makes
[41.78s -> 45.34s]  sense to ask whether these predictions are actually accurate predictions. Do
[45.34s -> 47.58s]  they match up with the reality? Do they match up with what you actually
[47.58s -> 55.54s]  get when you run the policy? So if we look at the kind of a basic
[55.54s -> 60.42s]  learning curve where the x-axis is the number of iterations of Q learning that
[60.42s -> 65.78s]  we've taken and the y-axis is the average reward per episode, and we look
[65.78s -> 70.78s]  at it on a bunch of let's say Atari games, we'll see that for all of these
[70.78s -> 75.62s]  Atari games our average reward per episode is going up, so things are
[75.62s -> 81.14s]  getting better. If we look at the average Q-values that are being
[81.14s -> 84.50s]  predicted, and that's the two plots on the right, we'll see that the Q
[84.50s -> 88.98s]  function is predicting larger and larger Q-values as training progresses. And
[88.98s -> 93.26s]  that intuitively makes sense. As training progresses, our policy gets
[93.26s -> 97.40s]  better, it's getting higher rewards, so our Q-function should also predict
[97.40s -> 103.98s]  that it's getting higher Q-values. So as predicted, Q increases and so does the
[103.98s -> 110.50s]  return. We can also look at whether the actual Q-values or value function
[110.50s -> 117.52s]  values occur in places that essentially anticipate future rewards. So this is the
[117.52s -> 121.10s]  game of Breakout. For those of you that are not familiar with Breakout, the
[121.10s -> 125.10s]  goal was to use the little orange paddle at the bottom to hit a ball, and
[125.10s -> 129.74s]  the ball is reflected by the paddle, bounces up, and hits these colored
[129.74s -> 134.34s]  rainbow colored blocks. And every block you break gets your point. A
[134.34s -> 140.06s]  particularly cool thing you can do in Breakout is if you break through all
[140.10s -> 144.62s]  of the blocks on one side, which is happening there in panel number 3, then
[144.62s -> 147.70s]  you can get the ball to bounce all the way up, and it will actually bounce off the
[147.70s -> 151.78s]  ceiling and ricochet off the top blocks, and they'll get you lots of
[151.78s -> 155.18s]  points, because it's just bouncing back and forth there, breaking all the blocks
[155.18s -> 161.16s]  from the top. So it's quite a cool strategy if you can break through to
[161.16s -> 166.10s]  the top like that and have it bounce around. And the graph at the bottom
[166.14s -> 170.62s]  shows the value function value, which is essentially the Q value for the best
[170.62s -> 176.02s]  action, at different points in time with the particular frames 1, 2, 3, and 4
[176.02s -> 181.46s]  labeled on the graph. And what you can see here is that some of these
[181.46s -> 184.78s]  values actually make a lot of sense. So number one, you're about to break a
[184.78s -> 188.90s]  block and you have the highest value. After you break that block, you bounce
[188.90s -> 191.62s]  back down and your value dips because you know that you're not going to get
[191.62s -> 195.30s]  any points for a little while while your ball flies down and needs to get
[195.30s -> 201.98s]  bounced from the paddle. In step 3, you're about to break through the top, so your
[201.98s -> 205.66s]  value becomes quite large. But in step 3, you actually don't quite make it. So that
[205.66s -> 208.46s]  last red block that you break, you break it, but then you bounce back down,
[208.46s -> 212.90s]  so your value goes down for a while, and then it rises right back up. And in
[212.90s -> 216.54s]  step 4, your value is actually at its largest, even though you actually
[216.54s -> 220.52s]  haven't broken any blocks for a while. So in step 4, you just bounced off the
[220.52s -> 223.82s]  paddle, you haven't broken any blocks, but you're about to ricochet off the
[223.86s -> 227.38s]  ceiling and you're about to get those mad points, so that's why your value
[227.38s -> 231.14s]  function is actually at the largest value. And it actually only goes down from there,
[231.14s -> 234.14s]  because once you actually get the points, the value function is going to
[234.14s -> 236.38s]  drop because it knows you've received your points, you're going to get fewer
[236.38s -> 242.10s]  points left over. So that all makes sense, that all seems reasonable. And we can
[242.10s -> 247.42s]  also look at the relative value, the Q values, for different actions. So these
[247.42s -> 253.46s]  are frames from the game Pong. So in Pong, you need to use your paddle, which is
[253.50s -> 257.94s]  green on the right side, to hit the ball so that it ricochets and goes to the
[257.94s -> 261.34s]  other side, and your opponent with the orange paddle needs to hit it back, and
[261.34s -> 265.26s]  your goal, kind of like in tennis, is to hit the ball back so your opponent
[265.26s -> 269.06s]  can't return it. If your opponent can't return it, then you get a point. If
[269.06s -> 272.78s]  your opponent can return it, then they might get a point on you, because you
[272.78s -> 278.34s]  might fail to hit it back. So what we're seeing in frame 1 is that all
[278.34s -> 282.70s]  actions have about the same Q value. Take a moment to think about why this
[282.70s -> 289.58s]  might be. Why does this make sense? Well, the reason it makes sense is
[289.58s -> 295.06s]  because when the ball is quite far away, many different actions will still allow
[295.06s -> 297.78s]  you to catch the ball later. So even though it might seem like moving the
[297.78s -> 301.82s]  paddle up is the right thing to do, in reality the Q function here is very
[301.82s -> 306.62s]  good and understands that even if it fails to move the paddle up, it'll be
[306.62s -> 310.10s]  able to move it up at the next time step, which actually means that the Q
[310.10s -> 314.34s]  values of different actions at this time step are about equal. It's a little
[314.34s -> 319.74s]  counterintuitive at first, but it really makes sense. At time step 2, now
[319.74s -> 322.78s]  the ball is getting pretty close to the zone where you have to
[322.78s -> 327.42s]  return it, and here now the up action has a much larger Q value. So the Q
[327.42s -> 332.42s]  function understands that it still has a split-second chance to return the
[332.42s -> 336.54s]  ball, but only if it moves up right now. So now the Q value for moving up is
[336.54s -> 341.10s]  very large, the Q value for moving down or staying still is very very negative.
[341.10s -> 346.98s]  And of course in step 4, once you've actually returned the ball, again it's
[346.98s -> 351.06s]  saying that the values for different actions don't really matter. So again,
[351.06s -> 354.98s]  this basically agrees with our intuition. In terms of the relative
[354.98s -> 359.18s]  values, the Q values make sense with respect to actions and they make sense
[359.18s -> 366.18s]  with respect to states. But there's a little bit of a problem. While the
[366.18s -> 370.14s]  relative Q values of different states and actions seem to make sense on a
[370.14s -> 375.62s]  successful training run, their actual absolute values will in practice
[375.62s -> 379.06s]  actually not be very predictive of real values. And you can verify this
[379.06s -> 383.30s]  yourself in homework 3 when you implement Q-learning. You can measure the
[383.30s -> 386.70s]  numerical value of the Q value that you're getting and then measure the
[386.70s -> 389.98s]  actual return that you get and compare those two numbers, you'll find they
[389.98s -> 394.02s]  don't agree very well. There are a few details that you have to get right. One
[394.02s -> 396.82s]  thing that you have to get right is that you have to make sure that when
[396.82s -> 400.98s]  you calculate the true value, you use a discounted estimator. So you calculate
[400.98s -> 403.58s]  the true value by taking the trajectories that you actually executed
[403.58s -> 408.02s]  and taking the reward times step 1 plus gamma times the reward at 2 plus
[408.02s -> 412.96s]  gamma squared times the reward at 3 etc etc etc and then compare that to the Q
[413.20s -> 417.84s]  value at step 1, because the Q value at step 1 is trying to predict the
[417.84s -> 422.40s]  expected sum of discounted rewards. So if you compare that to the discounted
[422.40s -> 427.36s]  sum of rewards that you actually got, if your Q value is a good predictor, you
[427.36s -> 429.96s]  should see that those are similar. And what you'll actually see is that
[429.96s -> 434.52s]  they're not very similar. So what these graphs are showing is basically
[434.52s -> 440.86s]  exactly this. What you should look at is the red lines. The blue lines,
[440.86s -> 443.98s]  don't worry about those, we'll talk about those later, but the red lines
[443.98s -> 449.18s]  represent the following. The kind of spiky red line, the one that's usually
[449.18s -> 453.26s]  higher, represents the estimate of your Q function. So this is basically
[453.26s -> 457.86s]  how, so this is what your Q function thinks the total discount reward that
[457.86s -> 464.14s]  you'll get will be. The solid flat line represents the actual sum of
[464.14s -> 468.26s]  discounted rewards that you're actually getting when you run that policy. And
[468.34s -> 472.22s]  what you're seeing here is that the Q function estimates are always much much
[472.22s -> 476.26s]  larger than the actual sums of discounted rewards that you're getting.
[476.26s -> 482.22s]  And that seems kind of strange. Like why is it that the Q function seems to
[482.22s -> 486.02s]  systematically think it's going to get larger rewards than it actually
[486.02s -> 490.90s]  gets? This is not a fluke. It's not just that the Q function is wrong and it
[490.90s -> 495.38s]  can be above or below the true reward, it's actually systematically larger. And
[495.42s -> 498.62s]  this is a very consistent pattern, and you try this in Homework 3, you'll also
[498.62s -> 504.90s]  see this pattern. So why is that? This problem is sometimes referred to as
[504.90s -> 509.74s]  overestimation in Q learning, and it has actually a fairly straightforward
[509.74s -> 514.42s]  and intuitive reason. Let's look at how we're computing our target values.
[514.42s -> 519.78s]  When you compute your target value, you take your current target Q function, Q5
[519.78s -> 523.98s]  prime, and you take the max of that Q function with respect to the action
[523.98s -> 531.50s]  aj prime. And it's really this max that's the problem. So here's how we can
[531.50s -> 535.46s]  think about why max would cause overestimation. Let's forget about Q
[535.46s -> 538.78s]  values for a minute, and let's just imagine that you have two random
[538.78s -> 545.60s]  variables, x1 and x2. You could think that maybe x1 and x2 are normally
[545.60s -> 548.82s]  distributed random variables, so maybe they have some true value plus
[548.82s -> 556.14s]  some norms. You can prove that the expected value of the max of x1 and x2
[556.14s -> 561.06s]  is always greater than or equal to the max of the expected value of x1 and
[561.06s -> 566.10s]  the expected value of x2. The intuition for why this is true is
[566.10s -> 570.66s]  that when you take the max of x1 and x2, you're essentially picking the value
[570.66s -> 574.62s]  that has the larger noise. And even though the noise for x1 and x2 might
[574.62s -> 578.66s]  be zero mean, maybe they're both univariate Gaussians, the max of two
[578.66s -> 582.38s]  zero mean noises is not in general zero mean. So you can imagine that one
[582.38s -> 585.74s]  noise is positive or negative with 50%, the other is positive or negative
[585.74s -> 589.74s]  with 50%, but when you take the max of the two, if either of them is
[589.74s -> 594.70s]  positive, you'll get a positive answer. So, of course, the probability that one
[594.70s -> 600.14s]  of the two noises is positive is going to be pretty high, right? So in order for
[600.14s -> 605.54s]  them to both be negative, that has only 25% probability. So the one of them being
[605.54s -> 610.18s]  positive, that's 75% probability. So with 75% probability, when you take the
[610.18s -> 614.02s]  expected value of their max, you'll get a positive number. When you take the max
[614.02s -> 618.46s]  of their expected values, you'll get, you'll get zero because their expected
[618.46s -> 621.70s]  values are zero. But when you take the expected value of the max, you'll get a
[621.70s -> 627.94s]  positive value. Now, what does this have to do with Q-learning? Well, if you
[627.94s -> 632.54s]  imagine that your Q function is not perfect, if you imagine that your Q
[632.54s -> 637.94s]  function kind of looks like the true Q function plus some noise, then when you
[637.94s -> 642.46s]  take this max and the target value, you're doing exactly this. So imagine
[642.46s -> 647.22s]  that your Q5' for different actions represents the true Q value of
[647.22s -> 651.90s]  that action plus some noise, so it might be up and down, and those errors are
[651.90s -> 656.86s]  not biased, so those errors are just as likely to be positive as negative. But
[656.90s -> 660.70s]  when you take the max in the target value, then you're actually selecting the
[660.70s -> 665.14s]  positive errors. And for the same reason that the expected value of the max of
[665.14s -> 669.82s]  x1 and x2 is greater than or equal to the max of their expected values, the
[669.82s -> 674.02s]  max over the actions will systematically select the errors in the positive
[674.02s -> 678.34s]  direction, which means that it will systematically overestimate the true Q
[678.34s -> 683.36s]  values, even if your Q function initially does not systematically have errors
[683.36s -> 690.72s]  that are positive or negative. So for this reason, the max over A' of Q5'
[690.72s -> 696.52s]  S' A' systematically overestimates the next value. It
[696.52s -> 703.16s]  basically preferentially selects errors in the positive direction. So how
[703.16s -> 707.68s]  can we fix this? Well, one way that we can think about fixing this is to note,
[707.68s -> 712.80s]  if we think back to the Q iteration, the way that we got this max was by
[712.84s -> 717.04s]  basically modifying the policy iteration procedure. So we had our greedy policy,
[717.04s -> 723.52s]  which is the argmax over A', and then we we then send that argmax back
[723.52s -> 729.20s]  into our Q function to get its value. So this is just another way of saying
[729.20s -> 735.76s]  the max over A' of Q5' is just Q5' evaluated at the argmax. And
[735.76s -> 739.84s]  this is actually the observation that we're going to use to try to mitigate
[740.08s -> 746.40s]  this problem. See, the trouble is that we select our action according to Q5'.
[746.40s -> 751.32s]  So if Q5' erroneously thinks that some action is a little bit
[751.32s -> 755.24s]  better because of some noise, then that's the action we'll select, and then the
[755.24s -> 759.04s]  value that we'll use for our target value is the value for that same action,
[759.04s -> 765.00s]  which has that same noise. But if we can somehow decorrelate the noise in
[765.08s -> 769.92s]  the action selection mechanism from the noise in the value evaluation mechanism,
[769.92s -> 775.80s]  then maybe this problem can go away. So the problem is that the value also
[775.80s -> 780.64s]  comes from the same Q5' which has the same noise as the rule that we
[780.64s -> 787.96s]  used to select our action. All right, so one way to mitigate this
[787.96s -> 794.60s]  problem is to use something called double Q learning. If the function that
[794.60s -> 799.00s]  gives us the value is decorrelated from the function that selects the action,
[799.00s -> 803.88s]  then in principle this problem should go away. So the idea is to just not use
[803.88s -> 807.84s]  the same network to choose the action as the network that we used to evaluate
[807.84s -> 814.12s]  the value. So double Q learning uses two networks, one network which we're
[814.12s -> 817.48s]  going to call phi A and another network which we're going to call phi B.
[817.80s -> 824.04s]  And phi A uses the values from phi B to evaluate the target values,
[824.04s -> 828.84s]  but selects the action according to phi A. So if you assume that phi B and phi A are
[828.84s -> 833.76s]  decorrelated, then the action that phi A selects for the arc max will be corrupted by
[833.76s -> 838.36s]  some noise, but that noise will be different from the noise that phi B has, which means
[838.36s -> 842.16s]  that when phi B evaluates that action, if the action was selected because it had a positive
[842.16s -> 846.28s]  noise, then phi B will actually give it a lower value. So the system will be kind of
[846.28s -> 852.40s]  self-correcting. And then analogously phi B is updated by using phi A as its target
[852.40s -> 858.52s]  network, but using phi B as the action selection rule. So this is the essence of
[858.52s -> 862.84s]  double Q learning, and its purpose is to decorate the way that you select the
[862.84s -> 870.12s]  action from the way that you evaluate the value of that action. So if the two Q networks
[870.12s -> 873.12s]  are noisy in different ways, then in principle the problem should go away.
[873.44s -> 880.80s]  Now in practice, the way that we can implement double Q learning is without actually adding
[880.80s -> 885.48s]  another Q function, but actually using the two Q functions we already have. So we already have
[885.48s -> 890.70s]  a phi and a phi prime, and they are different networks, so we'll just use those in place of phi
[890.70s -> 896.70s]  A and phi B. So in standard Q learning, if we write it out in this arc max way,
[896.70s -> 902.44s]  which is exactly equivalent, our target value is Q phi prime, evaluated at the arc max from Q
[902.44s -> 909.36s]  phi prime. In double Q learning, we select the action using Q phi, but evaluate it using Q phi
[909.36s -> 916.08s]  prime. So now as long as phi prime and phi are not too similar, then these will be
[916.08s -> 921.00s]  de-correlated. So this is the only difference, we're using phi to select the action instead
[921.00s -> 928.96s]  of phi prime. And we still use the target network to evaluate our value to avoid this
[929.00s -> 933.24s]  kind of moving targets problem. Now you could say that we do still have a little bit of the
[933.24s -> 939.80s]  moving targets problem, because as our phi changes, so does our action, but presumably the change in
[939.80s -> 943.88s]  the arc max is a very sudden discrete change, and it doesn't happen all the time. So if you
[943.88s -> 949.60s]  have, you know, three different actions, the arc max isn't going to change as often. Now
[949.60s -> 953.80s]  something I might mention here is that, and many of you might already be thinking about this,
[953.80s -> 959.16s]  phi prime and phi are of course not totally separate from each other, because periodically
[959.16s -> 964.40s]  you do set phi prime to be equal to phi. So the solution is far from perfect, it doesn't totally
[964.40s -> 969.04s]  de-correlate phi prime and phi. But in practice, it actually tends to work pretty well,
[969.04s -> 974.80s]  and it actually mitigates a large fraction of the problems with overestimation. But of course
[974.80s -> 980.76s]  not all of them. All right, there's another trick that I should mention that we can use
[980.76s -> 985.56s]  to improve Q-learning algorithms, and it's similar to something that we saw in the actor-critic lecture,
[985.56s -> 993.72s]  and that's the use of multi-step returns. So our Q-learning target is, and here I intentionally
[993.72s -> 1000.82s]  am writing it out with time steps, is rj t plus the max at t plus one, and where does the
[1000.82s -> 1006.70s]  signal in this learning process come from? Well, if your initial Q function is very bad,
[1006.70s -> 1015.78s]  it's essentially random, then almost all of your learning has to come from the R. So if
[1015.78s -> 1021.42s]  your Q phi prime is good, then the target values do most of the heavy lifting. If your Q phi primes
[1021.42s -> 1025.62s]  are bad, then the only thing that really matters is the reward, and that second term is
[1025.62s -> 1030.22s]  essentially just contributing norms. And early on in training, your Q function is pretty bad,
[1030.22s -> 1034.90s]  so almost all of your learning signal really comes from the reward. Later on in training,
[1034.90s -> 1040.26s]  your reward gets, you know, your Q function becomes better, and the Q values are much larger
[1040.26s -> 1044.90s]  in magnitude than the rewards, so later on in training the Q values dominate. But your take-off,
[1044.90s -> 1051.30s]  your initial learning period, can be very slow if your Q function is bad, because this target
[1051.30s -> 1057.34s]  value is mostly dominated by the Q value. So this is quite similar to what we saw in actor-critic,
[1057.34s -> 1063.94s]  when we talked about how the actor-critic style update that uses the reward plus the next
[1063.98s -> 1070.18s]  value has lower variance, but it's not unbiased, because if the value function is wrong, then your
[1070.18s -> 1074.38s]  advantage values are completely messed up. And Q learning is the same way. If the Q function is
[1074.38s -> 1077.82s]  wrong, then your target values are really messed up, and you're not going to be making
[1077.82s -> 1082.94s]  much learning progress. The alternative that we had in the actor-critic lecture is to use
[1082.94s -> 1087.58s]  a Monte Carlo sum of rewards, because the rewards are always the truth, they're just higher
[1087.58s -> 1093.82s]  variance because they represent a single sample estimate. We can use the same basic idea in Q
[1094.70s -> 1101.66s]  learning. So Q learning by default does this kind of one-step backup, which has maximum
[1101.66s -> 1107.18s]  bias and minimum variance, but you could construct a multi-step target just like in actor-critic.
[1107.18s -> 1112.94s]  Take a moment to imagine what this multi-step target would look like. If you have a piece
[1112.94s -> 1116.86s]  of paper in front of you, consider writing it down, and then you can check what you wrote
[1116.86s -> 1122.54s]  down against what I'm going to tell you on the next slide. Sorry, it's actually on this
[1122.54s -> 1131.38s]  slide. So the way that you can construct a multi-step target is basically exactly analogous to what we
[1131.38s -> 1137.10s]  saw in the actor-critic lecture. So the way that you construct your multi-step target is by
[1137.10s -> 1143.62s]  not just using one reward, but making a little sum from t prime equals t to t plus n minus 1,
[1143.62s -> 1150.70s]  and for each of those you take rj t prime multiplied by gamma to t minus t prime. And you
[1150.70s -> 1156.94s]  can verify that if n equals 1, then you recover exactly the standard rule that we had for Q
[1156.94s -> 1162.38s]  learning. But for n larger than 1, you sum together multiple reward values, and then you
[1162.38s -> 1170.06s]  use your target network for the t plus n step multiplied by gamma to the n. So this is
[1170.06s -> 1175.62s]  sometimes called an n-step return estimator because instead of summing the reward for one step,
[1175.62s -> 1182.94s]  you sum it for n steps. So this is the n-step return estimator. And just like with actor-critic,
[1182.94s -> 1189.18s]  the trade-off of the n-step return estimator is that it gives you a higher variance because
[1189.18s -> 1194.26s]  of that single sample estimate for r, but lower bias because even if your Q function is incorrect,
[1194.26s -> 1199.34s]  now it's being multiplied by gamma to the n, and for large values of n, gamma to the n might
[1199.34s -> 1207.50s]  be a very small number. Okay, so let's talk about Q learning with these n-step returns. It's less
[1207.50s -> 1213.34s]  biased because the target, because the Q values are multiplied by a small number, and it's typically
[1213.34s -> 1218.74s]  faster early on because when the target values are bad, those sums of rewards really give you
[1218.74s -> 1225.26s]  a lot of useful learning signal. Unfortunately, once you use n-step returns, this is actually
[1225.26s -> 1231.06s]  only a correct estimate of the Q value when you have an on-policy sample. So the reason for this
[1231.06s -> 1236.62s]  is that if you have a sample collected with a different policy, then that second step, t plus
[1236.62s -> 1242.22s]  1, might actually be different for your new policy, right? Because if on the second step you take a
[1242.22s -> 1246.94s]  different action, that won't match what you're getting from your n-step return. So n-step
[1246.94s -> 1252.86s]  returns technically are not correct with off-policy data anymore. With off-policy data, technically
[1252.86s -> 1258.10s]  you're only allowed to use n equals 1. With n equals 1, everything is pretty straightforward because
[1258.10s -> 1263.26s]  you're not actually assuming anywhere that your transition came from your policy. Your Q functions
[1263.26s -> 1267.78s]  condition on action, so that'll be valid for any policy, and your second time step, where this
[1267.78s -> 1272.62s]  would matter, in the second times if you actually take the max with respect to action, you don't use the
[1272.62s -> 1278.62s]  action that was actually sampled. So for n equals 1, it's valid to do off-policy, but for n greater than 1,
[1278.86s -> 1284.94s]  it's no longer valid. Basically, your new policy might never have landed in the state sj t plus n.
[1284.94s -> 1292.90s]  So why? Because you actually end up using the action from the sample for those intermediate
[1292.90s -> 1298.62s]  steps, which is not the action that your new policy would have taken. As an interesting
[1298.62s -> 1302.34s]  thought exercise, and this is something you can think about at home after the lecture, you could
[1302.34s -> 1307.98s]  imagine how to utilize the same trick that we used to make Q-learning off-policy to try to make
[1308.02s -> 1314.54s]  this n-step version off-policy. As a hint, to make the n-step version off-policy, you can't learn a Q
[1314.54s -> 1318.86s]  function anymore. You have to learn some other object, condition on some other information, and if
[1318.86s -> 1322.66s]  you think a little bit about how you could do this, that might shed some light on kind of
[1322.66s -> 1326.82s]  giving you a better intuitive understanding for how it is that Q-learning can be off-policy.
[1326.82s -> 1331.94s]  So as a homework exercise after the lecture, maybe take a moment to think about how to make n-step
[1331.94s -> 1336.74s]  returns off-policy, and what kind of object you would need to learn to make that possible.
[1336.90s -> 1348.26s]  So the estimate that we get from regular n-step returns is an estimate of Q pi for pi, but for
[1348.26s -> 1353.46s]  that you need transitions from pi for all the intermediate steps. And this is not an issue
[1353.46s -> 1360.34s]  when n equals 1. So how can we fix it? Well, we can ignore the problem, which often works
[1360.34s -> 1366.58s]  very well. The other thing we can do is we can dynamically cut the trace. So we can dynamically
[1366.58s -> 1372.22s]  choose n to only get on-policy data. Essentially, we can look at what our deterministic greedy policy
[1372.22s -> 1376.46s]  would do, we could look at what we actually did in the sample, and we can choose n to be the
[1376.46s -> 1380.50s]  largest value such that all of the actions exactly match what our policy would have done,
[1380.50s -> 1386.26s]  and then we'll also remove the bias. So this works well when data is mostly on-policy,
[1386.26s -> 1392.22s]  and the action space is pretty small. Another thing we can do is importance sampling,
[1392.26s -> 1396.98s]  so we can construct a stochastic policy and importance weight these n-step return estimators.
[1396.98s -> 1401.58s]  I won't talk about this in detail, but if you want to learn more about this, check out this paper
[1401.58s -> 1406.46s]  called Safe and Efficient Off-Policy Reinforcement Learning by Munos et al. And then there's this
[1406.46s -> 1410.90s]  mystery solution that I haven't told you about where you don't do any of this stuff, but you
[1410.90s -> 1414.54s]  condition the Q function on some other additional information that allows you to make it
[1414.54s -> 1418.50s]  off-policy. And that's a solution you can think about in your own time after the lecture.
