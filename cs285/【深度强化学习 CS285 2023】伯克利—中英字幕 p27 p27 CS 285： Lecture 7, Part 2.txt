# Detected language: en (p=1.00)

[0.00s -> 4.80s]  All right, so far we talked about how we can learn value functions represented in
[4.80s -> 9.50s]  a tabular form. So there's no neural net, no deep RL, no function approximation,
[9.50s -> 14.32s]  just a small discrete state space where we can enumerate the value in every
[14.32s -> 18.66s]  possible state. Now let's talk about how we can introduce neural networks
[18.66s -> 26.96s]  and function approximation. So first, how do we represent V of s? Well, so far we
[27.00s -> 30.36s]  talked about how we can represent it as a big table with one entry for each
[30.36s -> 37.04s]  discrete state. So in state 0, you say V of s is 0.2, in state s, s equals 1 is
[37.04s -> 43.00s]  0.3, and so on. Why is this not enough? Well, take a moment to think
[43.00s -> 47.68s]  about this. Why might we not want to represent the value function as a table?
[47.68s -> 53.08s]  Let's say that you're playing a video game from images, right? Now in this
[53.08s -> 57.20s]  video game, the number of possible states if you have a 200 by 200 pixel
[57.20s -> 62.80s]  image is 255, which is the number of values that each pixel can take on, raised
[62.80s -> 65.88s]  to the third power because there are three color channels, raised to the
[65.88s -> 69.44s]  power of 200 times 200. So these are, this is the number of possible images
[69.44s -> 74.36s]  you can see. Maintaining a table over this many entries is impossible, this is
[74.36s -> 77.44s]  more than the number of atoms in the universe, and that's for a discrete
[77.44s -> 81.72s]  state space. For a continuous state space, it's actually just infinite, and this
[81.72s -> 86.48s]  would never be possible. This is also, by the way, sometimes referred to as the
[86.48s -> 89.76s]  curse of dimensionality. If someone says curse of dimensionality in the
[89.76s -> 93.96s]  context of reinforcement learning, what that refers to is the simple fact that
[93.96s -> 97.72s]  if you have a state space with many dimensions, the number of entries that
[97.72s -> 101.72s]  you need in a table for tabular reinforcement learning is exponential in
[101.72s -> 107.80s]  the number of dimensions. So we'll use a function approximator. Let's say just
[107.80s -> 111.72s]  like in lecture 6, we're going to have a neural net value function that maps
[111.72s -> 116.92s]  from states to scalar-valued values. So we're going to have a neural net that
[116.92s -> 122.68s]  looks like this, and it has some parameters fine. So what we can do is
[122.68s -> 127.52s]  we can fit our neural net value function in much the same way as we
[127.52s -> 133.00s]  did in lecture 6, by doing least squares regression onto target values. And if we
[133.04s -> 138.16s]  use the value iteration procedure from the previous section, then our
[138.16s -> 148.36s]  target values are just the max over a of q pi s a. So then our fitted value
[148.36s -> 156.76s]  iteration algorithm would look like this. We would take our data set of
[156.76s -> 162.36s]  states and actions. For every sample state, we would evaluate every possible
[162.36s -> 166.36s]  action you could take in that state, and we would take a max over those
[166.36s -> 169.68s]  actions of our q values. So what I have in the parentheses here is I've just
[169.68s -> 174.44s]  substituted in the q value. So the q value is the reward plus gamma times
[174.44s -> 178.08s]  the expected value of the next state. So I've substituted that into the
[178.08s -> 182.08s]  parameters. We don't represent the q function explicitly here, we just
[182.08s -> 187.00s]  compute it as we go to evaluate the max, and that gives us our target values
[187.00s -> 193.96s]  y, and then we solve for phi by doing least squares regression so that v phi of
[193.96s -> 200.64s]  s i is close to y. So this is fitted value iteration. Step one, compute your target
[200.64s -> 206.00s]  values by constructing the q function for every possible action at each
[206.00s -> 210.36s]  sampled state. So you have a finite sampling of states, and we still assume
[210.36s -> 213.88s]  that we have discrete action space, so we can perform this enumeration exactly.
[213.88s -> 218.76s]  For every action, we valued its reward plus gamma times the expected value of
[218.76s -> 224.16s]  the value at the next state. Do the max over that, that gives us our target
[224.16s -> 231.60s]  value, and then in step two, regress onto those target values. All right, so this
[231.60s -> 235.20s]  is a reasonable algorithm that we could use, but it still requires us to know
[235.20s -> 239.28s]  the transition dynamics. Where do we need to know the transition dynamics?
[239.28s -> 245.00s]  Which part of this algorithm requires knowing the transition dynamics? Well, it's
[245.00s -> 249.40s]  basically this part. So there are two ways in which this requires knowledge
[249.40s -> 253.08s]  of the transition dynamics. It requires being able to compute that expected
[253.08s -> 257.80s]  value, and perhaps more importantly, it requires us to be able to try multiple
[257.80s -> 261.80s]  different actions from the same state, which we can't do in general if we can
[261.80s -> 265.32s]  only run policies in the environment instead of teleporting to a state
[265.32s -> 269.40s]  multiple times and trying multiple actions from the same exact state. So if
[269.40s -> 275.08s]  we don't know the transition dynamics, generally we can't do this. So let's go
[275.08s -> 278.84s]  back to policy iteration. In policy iteration, we alternated between
[278.84s -> 284.36s]  evaluating q pi, or a pi, but if you have a pi, or if you have q pi, you can
[284.36s -> 288.48s]  recover a pi, and then step two, setting our policy to be this greedy
[288.68s -> 296.28s]  arcmax policy. So that was policy iteration. And step one in policy iteration involved
[296.28s -> 300.76s]  policy evaluation, which involved repeatedly applying this value function
[300.76s -> 307.20s]  recurrence that we saw before. So what if instead of applying the value
[307.20s -> 311.68s]  function recurrence to learn the value function, we instead directly constructed
[311.68s -> 318.04s]  a q function recurrence in an analogous way. So if I wanted to construct the q
[318.08s -> 322.00s]  function at a particular state action tuple, I can write exactly the same
[322.00s -> 327.24s]  recurrence, except that now, since the q function is a function of a state and
[327.24s -> 333.64s]  an action, I don't need to evaluate the next state given s and pi of s. I
[333.64s -> 338.48s]  just evaluate the next state given the SA tuple that I'm training my q
[338.48s -> 343.24s]  function on. And this might at first seem like a very subtle difference, but it's
[343.24s -> 350.48s]  a very very important one, because now as my policy pi changes, the action for
[350.48s -> 354.56s]  which I need to sample s prime, basically the a that's on the right of the
[354.56s -> 360.28s]  conditioning bar and p of s prime given SA, doesn't actually change, which
[360.28s -> 365.32s]  means that if I have a bunch of samples, s comma a comma s prime, I can
[365.32s -> 371.56s]  use those samples to fit my q function regardless of what policy I have. The
[371.56s -> 376.04s]  only place where the policy shows up is as an argument to the q function at the
[376.04s -> 381.08s]  state s prime inside of the expectation. And it turns out that this very
[381.08s -> 385.72s]  seemingly very simple change allows us to perform policy iteration style
[385.72s -> 390.76s]  algorithms without actually knowing the transition dynamics, just by sampling
[390.76s -> 399.24s]  some SA s prime tuples, which we can get by running any policy we want. So
[399.32s -> 405.44s]  this second recurrence that I've written here doesn't require knowing the
[405.44s -> 410.00s]  transition probabilities, it just requires samples of the form s comma a
[410.00s -> 416.00s]  comma s prime. So if we do this for step one in policy iteration, we would no
[416.00s -> 420.32s]  longer require knowing the transition probabilities. And this is very important.
[420.32s -> 431.24s]  This is the basis of most value-based model-free RL algorithms. Alright, now we
[431.24s -> 435.24s]  seemingly took a step back because before we derived policy iteration and
[435.24s -> 439.36s]  then we simplified it to get value iteration. And the way that we got value
[439.36s -> 443.82s]  iteration is by using this max trick. In value iteration, we saw that when we
[443.82s -> 448.20s]  construct the policy, we take the argmax, but then we simply take the value of
[448.20s -> 452.64s]  that argmax action, so evaluating the value of the argmax is just like taking
[452.64s -> 458.60s]  the max. So we can forgo policy construction, we can forgo that step
[458.60s -> 464.00s]  two and directly perform value iteration. Can we do the same max trick
[464.00s -> 467.84s]  with Q functions? So can we essentially do something like value
[467.84s -> 473.18s]  iteration but without knowing the transition probabilities? So what we did
[473.18s -> 476.80s]  before is we took policy iteration, which alternates between evaluating the
[476.80s -> 480.80s]  value function in step two and setting the policy to be the greedy policy in
[480.80s -> 484.68s]  step, sorry, evaluating the value function in step one and setting the policy to be the
[484.68s -> 489.00s]  greedy policy in step two, and we transformed this other algorithm where
[489.00s -> 493.60s]  step one constructs target values by taking a max over the Q values and
[493.60s -> 499.74s]  step two fits a new value function to those target values. So here we
[499.74s -> 503.20s]  compute, forgot the policy, we just compute the values directly. So can we
[503.20s -> 507.32s]  do this with Q values also but still retain this benefit of not needing to
[507.32s -> 513.08s]  know the transitions? So the way that we construct a fitted Q iteration
[513.08s -> 518.40s]  algorithm is very much analogous to fitted value iteration. We construct our
[518.40s -> 524.40s]  target value yi as the reward of a sampled state action tuple si and ai plus
[524.40s -> 529.96s]  gamma times the expected value of the value function at state s prime, and
[530.08s -> 534.96s]  then in step two we simply regress our Q function Q phi onto those target
[534.96s -> 539.76s]  values. The trick of course is that we have to evaluate step one without
[539.76s -> 544.60s]  knowing the transition probabilities, so we're going to do two things.
[544.60s -> 553.00s]  First, we're going to replace V of si prime with the max over a at Q phi si
[553.00s -> 556.88s]  prime ai prime, because we're only approximating Q phi, we're not
[556.92s -> 562.24s]  approximating V phi. And second, instead of taking a full expectation over all
[562.24s -> 567.24s]  possible next states, we're going to use the sampled state si prime that we
[567.24s -> 571.88s]  got when we generated that sample. And now all we need to run this fitted Q
[571.88s -> 577.72s]  iteration algorithm is samples si ai si prime, which can be constructed by
[577.72s -> 582.72s]  rolling out our policy. So this is fitted Q iteration. It alternates between two
[582.76s -> 587.68s]  steps. Step one, estimate target values, which you can do using only samples and
[587.68s -> 593.88s]  your previous Q function Q phi. Step two, fit a new phi with regression onto
[593.88s -> 597.96s]  your target values using the same exact samples that you used to compute
[597.96s -> 603.96s]  your target values. And this doesn't require simulation of different actions,
[603.96s -> 607.76s]  it only requires the actions that you actually sampled last time when you
[607.76s -> 614.92s]  ran your policy. So this works even for off-policy samples. So this algorithm does
[614.92s -> 618.26s]  not make any assumptions that the actions were actually sampled from the
[618.26s -> 622.56s]  latest policy. The actions could have been sampled from anything. So you can
[622.56s -> 625.68s]  store all the data you've collected so far, it doesn't need to come from your
[625.68s -> 629.64s]  latest policy, unlike actor-critic where we had a non-policy algorithm.
[629.64s -> 633.76s]  There's only one network, there's no policy gradient at all, there's no actor,
[633.76s -> 637.20s]  there's only a Q function estimator, which is a neural network that takes
[637.64s -> 643.56s]  a state of n in action and outputs a scalar-valued Q-value. Unfortunately it
[643.56s -> 647.84s]  turns out that this procedure does not have any convergence guarantees for
[647.84s -> 651.82s]  nonlinear function approximation. So if you do this with a neural net, it may
[651.82s -> 655.44s]  not converge to the true solution, and we'll discuss this a lot more later in the
[655.44s -> 658.58s]  lecture. If you use a tabular representation, it is actually
[658.58s -> 661.88s]  guaranteed to converge, but for a neural network it's in general not
[661.88s -> 668.80s]  guaranteed to converge. Alright, so just to put the pieces together, here's
[668.80s -> 673.52s]  the full fit-a-q-duration algorithm, and for each step of the algorithm
[673.52s -> 678.40s]  there's some free parameters that I'm going to mention. Step one, collect a
[678.40s -> 687.56s]  data set consisting of tuples s-i, a-i, s-i prime, and r-i using some policy. The
[687.56s -> 690.92s]  algorithm works for a wide range of different policies. Now not all policy
[690.96s -> 695.72s]  choices are equally good, but the principles will apply to any policy and
[695.72s -> 699.20s]  it certainly doesn't have to be the latest policy. And one of the
[699.20s -> 703.20s]  parameters you have to choose is the number of such transitions you're going to
[703.20s -> 706.80s]  collect. So typically you would draw your policy for some number of steps
[706.80s -> 710.88s]  or some number of trajectories, but you get to choose how many, and of course
[710.88s -> 714.16s]  you also choose the policy that you're going to be rolling out. What policy
[714.16s -> 718.20s]  do you use to collect this data? A very common choice is in fact to use the
[718.20s -> 722.28s]  latest policy, but there are a few nuances about that choice that I'll
[722.28s -> 729.00s]  discuss shortly. Step two, for every transition that you sampled, calculate a
[729.00s -> 733.76s]  target value. So you calculate the target value y-i by taking the reward
[733.76s -> 739.60s]  from the transition plus gamma times the max over the next action a-i prime
[739.60s -> 746.96s]  of the q-value q phi s-i prime a-i prime using your previous q function
[746.96s -> 754.84s]  estimator q phi. Step three, train a new q function, which means find a new
[754.84s -> 759.72s]  parameter vector phi by minimizing the difference between the values of q phi
[759.72s -> 766.32s]  s-i a-i and the corresponding target value y-i. So you have a q function
[766.32s -> 771.40s]  which takes as input s and a, it outputs a scalar value, and it has
[771.40s -> 776.40s]  parameters phi. I should mention, by the way, that a very common design for
[776.64s -> 780.80s]  a neural network architecture for a q function with
[780.80s -> 785.04s]  discrete actions is actually to have the actions, the outputs rather than inputs.
[785.04s -> 789.92s]  So an alternative design is to input the state s and then output a different q
[789.92s -> 794.88s]  value for every possible action a. You can think of that as a special case of
[794.88s -> 799.92s]  this design, and I'll discuss in class a little bit how those relate, but
[799.92s -> 802.88s]  conceptually it's probably easiest to think about as a neural network that
[802.88s -> 806.96s]  takes s and a as input and outputs a value, but you could also think of it as
[806.96s -> 810.08s]  a network that takes s as input and outputs a different value for every
[810.08s -> 816.48s]  possible a. So in step three, one parameter you have to choose is the number of
[816.48s -> 819.88s]  gradient steps, capital S, that you will make in performing this
[819.88s -> 823.20s]  optimization. You could run this optimization all the way to convergence,
[823.20s -> 830.00s]  or you can run it for just a few gradient steps. Now doing step three
[830.08s -> 836.48s]  once doesn't actually get you the best possible q function. You could
[836.48s -> 839.28s]  alternate step two and step three some number of times,
[839.28s -> 844.08s]  let's say capital K times, before going out and collecting more data,
[844.08s -> 846.80s]  and the number of times you alternate step two and step three we're going to
[846.80s -> 849.84s]  refer to as k. That's the number of iterations, the
[849.84s -> 852.48s]  fit of q iteration that you take in the inner loop,
[852.48s -> 855.44s]  and then once you've taken those k iterations, maybe you could take your
[855.44s -> 858.40s]  latest policy, modify it with some exploration rules,
[858.40s -> 862.48s]  which I'll discuss shortly, and use it to collect some more data. So
[862.48s -> 865.12s]  this is the general design of fit of q iteration.
[865.12s -> 869.12s]  Many different algorithms can actually be interpreted as variants of fit of q
[869.12s -> 872.16s]  iteration, including algorithms like Q-learning,
[872.16s -> 876.88s]  which I will cover shortly. All right, so to review this portion
[876.88s -> 880.40s]  of the lecture, we discussed value-based methods.
[880.40s -> 883.60s]  Value-based methods do not learn a policy explicitly,
[883.60s -> 887.60s]  they just learn a value function or a q function represented as
[887.60s -> 891.52s]  a table or a neural network. If we have a value function,
[891.52s -> 895.76s]  we can recover a policy by using the argmax policy.
[895.76s -> 899.36s]  We talked about how fit of q iteration removes the need for us
[899.36s -> 902.88s]  to need to know the transition probabilities, and we discussed
[902.88s -> 907.68s]  this kind of generic form of the fit of q iteration algorithm.
