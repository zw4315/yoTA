# Detected language: en (p=1.00)

[0.00s -> 4.80s]  In the last portion of today's lecture, I'm going to just briefly go through a
[4.80s -> 9.24s]  few examples of actual deep RL algorithms just to kind of show you
[9.24s -> 13.36s]  some of the things that they do. This will be the least technical portion of
[13.36s -> 16.56s]  the lecture and these algorithms will be covered in much more detail in the
[16.56s -> 19.92s]  subsequent lectures, whereas this part is mainly just to kind of round out
[19.92s -> 24.96s]  today's lecture with some nice interesting examples and videos. So some
[24.96s -> 27.88s]  examples of specific algorithms, and don't worry if you haven't heard these
[27.88s -> 32.32s]  names, we'll cover these more later. Value function fitting methods, so these are
[32.32s -> 36.60s]  things like Q-learning, DQN, temporal difference learning, these are all value
[36.60s -> 40.66s]  function methods, fitted value iteration. Policy gradient methods, these are
[40.66s -> 45.16s]  methods like reinforce, natural gradient, transfusion policy optimization or TRPO,
[45.16s -> 50.20s]  PPO, etc. Actor-critic algorithms, either things like asynchronous advantage
[50.20s -> 55.84s]  actor-critic or A3C, soft actor-critic, DDPG, and so on. Model-based RL
[55.84s -> 62.16s]  algorithms, these are things like DINA, guided policy search, MPPO, SVG, etc.
[62.16s -> 67.78s]  And we'll learn about most of these in the next few weeks, but first let's go
[67.78s -> 75.80s]  through a few examples. So here's a video of the Q-learning result for
[75.80s -> 81.06s]  playing Atari games. This is an algorithm that learns policies for
[81.06s -> 85.04s]  playing video games directly from pixels. This is from a paper by me et
[85.04s -> 91.28s]  al. from 2013, and this particular algorithm uses Q-learning with
[91.28s -> 94.16s]  convolutional neural networks. So Q-learning is a value-based method, it
[94.16s -> 99.88s]  actually learns an estimate of Q-SA by using a neural network. Atari games are
[99.88s -> 103.00s]  discrete action environments, which means that you just have to produce a
[103.00s -> 106.82s]  different Q-value for each of a small discrete set of actions, and then you
[106.82s -> 110.20s]  take the argmax over those Q-values to select the best action when playing
[110.20s -> 119.32s]  the game. Here is a robotics example. This is from the paper end-to-end training
[119.32s -> 123.10s]  of deep visuomotor policies, and this is a model-based RL algorithm called
[123.10s -> 127.60s]  guided policy search, which uses a combination of dynamics models and
[127.60s -> 132.84s]  image-based convolutional networks to perform a variety of robotic skills.
[132.84s -> 137.76s]  Here's a policy gradients example. This is from a paper high-dimensional
[137.76s -> 141.76s]  continuous control with generalized advantage estimation. It uses a variant of
[141.76s -> 144.88s]  an algorithm called trust region policy optimization, which is a policy
[144.88s -> 149.48s]  gradient method that combines, in this case, a trust region with value
[149.48s -> 152.24s]  function approximation. So this is technically an actor-critic algorithm
[152.24s -> 156.88s]  derived from policy gradient algorithm, and here you can see it training this
[156.88s -> 162.64s]  little humanoid robot how to walk. And here is the video that I actually
[162.84s -> 167.64s]  showed in the first lecture for the grasping robot, and this particular
[167.64s -> 172.00s]  result was actually also produced by a Q-learning algorithm, not that
[172.00s -> 176.44s]  different from the Atari example that I showed a few slides ago, but in this
[176.44s -> 181.64s]  case with a particular modification to handle continuous actions.
