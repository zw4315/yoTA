# Detected language: en (p=1.00)

[0.00s -> 4.60s]  All right, in the last portion of today's lecture I'm going to go over a
[4.60s -> 10.00s]  little case study that demonstrates the power of optimal control algorithms in
[10.00s -> 13.88s]  the case where we know the true dynamics and the point that I want to
[13.88s -> 18.04s]  make with this is just kind of partly to motivate why I want to study
[18.04s -> 21.24s]  model-based RL and partly to show that these things really do work and
[21.24s -> 24.44s]  they really do things that are pretty impressive compared to even the best
[24.44s -> 28.68s]  model-free RL methods. So the case study that I'm going to talk about is this
[28.72s -> 32.92s]  paper called Synthesis and Stabilization of Complex Behaviors through Online
[32.92s -> 38.72s]  Trajecture Optimization by Yuval Tasa, Tom Perez and Emmanuel Todorov. What this
[38.72s -> 44.80s]  paper describes is a fairly simple, almost textbook algorithm, but implemented
[44.80s -> 49.84s]  quite well, that uses iterative LQR as an inner loop in something called
[49.84s -> 54.84s]  model predictive control. So model predictive control is a way to use a
[54.88s -> 59.08s]  model-based planner in settings where your state might be unpredictable. And
[59.08s -> 63.40s]  the main idea of model predictive control is very simple. Every time step
[63.40s -> 67.88s]  you observe your current state x t, then you use your favorite planning or
[67.88s -> 72.52s]  control method to figure out a plan, a sequence of actions u t, u t plus one,
[72.52s -> 77.92s]  etc., all the way out to u capital T, and then you execute only the first
[77.92s -> 81.94s]  action of that plan, discard the other actions, observe the next state that
[81.98s -> 87.26s]  occurs, and replan all over again. So essentially model predictive control is a
[87.26s -> 91.74s]  fancy way of saying replan on every single time step. And that's what this
[91.74s -> 95.02s]  paper does. Most of the contributions in this paper are actually in the
[95.02s -> 98.66s]  particular implementation of iterative LQR, and if you want to kind of know
[98.66s -> 102.46s]  all the bells and whistles, all the tips and tricks for implementing
[102.46s -> 106.34s]  iterative LQR effectively, I would encourage you to check it out. What I
[106.34s -> 110.54s]  want to show you today is the video of the result from that paper. So
[111.06s -> 115.26s]  I'm going to play the video and narrate a little bit. So here, what they're
[115.26s -> 119.98s]  showing is a simple acrobot system using iterative LQR. They're going to
[119.98s -> 124.50s]  show a swimmer and a little hopper, as well as a more complex humanoid.
[127.10s -> 130.70s]  So here's the acrobot, two degrees of freedom and only one control
[130.70s -> 135.06s]  dimension, a very simple cost. And first they just run it passively, so
[135.06s -> 138.86s]  no controls at all, and then they turn on the controller. And you can
[138.86s -> 141.42s]  see that in real time, the controller actually discovers how to swing
[141.42s -> 144.70s]  up the acrobot. So there's no learning at all, although you do
[144.70s -> 149.26s]  have to know the dynamics. But the impressive thing is that this
[149.26s -> 153.30s]  behavior is actually discovered completely automatically and completely
[153.30s -> 153.94s]  in real time.
[161.14s -> 164.54s]  And because they're using model predictive control, when they
[164.54s -> 168.54s]  apply perturbations to the system, the robot successfully recovers
[168.54s -> 169.66s]  from those perturbations.
[177.14s -> 180.46s]  Here they have a little swimming snake, and his goal is to get
[180.46s -> 183.70s]  to the green dot while avoiding the red dot. And again, kind of
[183.70s -> 186.98s]  the interesting thing here is that this undulating swimming
[186.98s -> 189.70s]  gait is actually discovered by the controller automatically just
[189.70s -> 193.10s]  through optimization, without needing to know or learn
[193.10s -> 196.30s]  anything in advance, except of course for the system dynamics.
[198.54s -> 212.46s]  Here's the Hopper system. So here what they're going to do is
[212.46s -> 216.82s]  they're going to first apply some perturbation forces to it
[216.82s -> 219.58s]  just to show off their physics engine. And then having
[219.58s -> 222.02s]  applied those perturbation forces, they'll show what
[222.02s -> 227.62s]  happens when you actually ask the hopper to stand up. So it
[227.62s -> 231.94s]  figures out somehow to jump up and stand. And then when they
[231.94s -> 235.02s]  apply perturbations to it, it reacts to those perturbations
[235.14s -> 237.02s]  and manages to stay upright.
[249.42s -> 252.54s]  And here they show that it can react to even very extreme
[252.54s -> 253.42s]  perturbations.
[258.62s -> 261.06s]  Here they're showing what happens if they give it the wrong
[261.06s -> 263.74s]  dynamics. So because they're reclining every step, they can
[263.74s -> 265.90s]  actually get somewhat sensible results, even when the
[265.90s -> 268.82s]  dynamics are misspecified. So here, the true robot has
[268.82s -> 271.90s]  half the mass that the controller thinks it does, and
[271.90s -> 273.74s]  here it has double the mass that the controller thinks it
[273.74s -> 275.86s]  does. So you can see with double the mass, it kind of
[275.86s -> 278.86s]  struggles a little bit, but still does something seemingly
[278.86s -> 279.46s]  reasonable.
[282.82s -> 285.38s]  Here, they're going to be controlling a 3d humanoid. The
[285.38s -> 287.46s]  cost function here is, I should say, pretty heavily
[287.46s -> 290.18s]  engineered. So it's still a fairly short horizon
[290.18s -> 293.10s]  controller. It's not planning far into the future. And the
[293.10s -> 295.66s]  cost function therefore needs to be quite detailed. So here
[295.66s -> 298.58s]  they turn it on, and it figures out how to stand up.
[299.02s -> 301.14s]  It's a little bit slower than real time. So they had
[301.14s -> 305.62s]  to speed up this video to play it back. But it manages to
[305.62s -> 308.98s]  do some rudimentary stepping, balancing, and pretty
[308.98s -> 311.70s]  intelligent reactions, even in the face of fairly extreme
[311.70s -> 312.62s]  perturbations.
[315.38s -> 323.58s]  Okay, if you're interested in additional readings on these
[323.58s -> 327.78s]  topics, here's what I would recommend. This monograph by
[327.78s -> 329.46s]  Main and Jacobson called Differential Dynamic
[329.46s -> 332.50s]  Programming is the original description of the DDP
[332.50s -> 337.62s]  algorithm from which ILQR is inspired. This is the MPC
[337.62s -> 340.70s]  paper for which I just presented the video. This is a
[340.70s -> 343.30s]  paper that provides a probabilistic formulation and
[343.38s -> 346.22s]  trust-fusion alternatives to the deterministic line search
[346.22s -> 348.62s]  for ILQR. So if you want to know how to handle these
[348.62s -> 350.98s]  kind of ILQR things in stochastic settings, this
[350.98s -> 356.10s]  could be something to check out. And in next week's
[356.10s -> 358.74s]  lectures, we will extend this to the case where the
[358.74s -> 361.50s]  dynamics are perhaps not known. So what's wrong with
[361.50s -> 364.50s]  known dynamics? Well, known dynamics are great if you're
[364.50s -> 366.66s]  controlling some system that is easy to model, like the
[366.66s -> 368.86s]  kinematics of a car. But if you're trying to get a
[368.86s -> 372.42s]  robot to fold a towel or sort some objects in a factory,
[372.42s -> 375.30s]  maybe modeling all those exactly is very difficult or
[375.30s -> 378.02s]  even impossible. And in those cases, maybe we can learn
[378.02s -> 380.06s]  our models. So that's what we'll talk about next.
