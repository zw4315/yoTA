# Detected language: en (p=1.00)

[0.00s -> 4.68s]  For the last part of today's lecture, I want to briefly review the material that we've
[4.68s -> 9.24s]  covered and then discuss some examples of actor-critic algorithms in the literature.
[9.24s -> 11.52s]  So this will be a pretty brief section.
[11.52s -> 16.12s]  To summarize what we've covered, we discussed how an actor-critic algorithm consists of
[16.12s -> 20.84s]  several parts, an actor, which is the policy, and the critic, which is the value
[20.84s -> 21.84s]  function.
[21.84s -> 25.92s]  The actor-critic algorithm can be viewed as a version of policy gradient with substantially
[25.92s -> 30.96s]  reduced variance, and like the policy gradient and all other RL algorithms, it consists of
[30.96s -> 35.84s]  three parts, the orange box where we generate samples, the green box where we estimate
[35.84s -> 40.24s]  our return, which now corresponds to fitting the value function, and a blue box where
[40.24s -> 45.40s]  we use gradient descent to update our policy, just like in policy gradients.
[45.40s -> 51.00s]  Policy evaluation refers to the process of fitting the value function, and discount
[51.00s -> 56.24s]  factors are something that we can use to make it feasible to do policy evaluation with infinite
[56.24s -> 57.56s]  horizons.
[57.56s -> 59.52s]  It has several different interpretations.
[59.52s -> 63.88s]  You can interpret it as the fear of death, meaning that you would like to receive rewards
[63.88s -> 68.28s]  sooner rather than later before you die, but you can also interpret it as a kind
[68.28s -> 71.04s]  of variance reduction trick.
[71.04s -> 74.64s]  We talked about the design of actor-critic algorithms, how you could have one network
[74.64s -> 80.96s]  with two heads or two separate networks, and how you could have batch mode or online
[80.96s -> 85.20s]  actor-critic algorithms, and you could use parallelism to get mini-batch sizes that
[85.20s -> 87.20s]  are larger than one.
[87.20s -> 91.16s]  We also talked about state-dependent baselines and even action-dependent control variants
[91.16s -> 95.44s]  as another way to use the critic while remaining unbiased, and we talked about how
[95.44s -> 100.76s]  we can combine these with n-step returns or even the generalized advantage estimator,
[100.76s -> 105.24s]  which averages together many different n-step return estimators.
[105.24s -> 108.96s]  Here are some examples of actor-critic algorithms in the literature.
[108.96s -> 112.60s]  This video, which I've showed several times already, is actually from a paper called
[112.60s -> 116.72s]  High-Dimensional Continuous Control with Generalized Advantage Estimators, which introduced
[116.72s -> 122.88s]  the GAE estimator, which is basically a kind of weighted sum of different n-step estimators.
[122.88s -> 127.36s]  This uses a kind of batch mode actor-critic, which combines Monte Carlo and function
[127.36s -> 131.92s]  approximator estimators via this GAE trick.
[131.92s -> 136.92s]  And in this paper, the experiments focus on continuous control tasks like this running
[136.92s -> 139.16s]  humanoid robot.
[139.16s -> 140.52s]  Here's another example.
[140.52s -> 144.36s]  This is from a paper called Asynchronous Methods for Deep Reinforcement Learning, and
[144.36s -> 151.60s]  this paper focuses on online actor-critic algorithms using paralyzed asynchronous systems.
[151.60s -> 155.80s]  So in this particular video, they actually have an image-based actor-critic algorithm
[155.80s -> 160.28s]  using a CondeNet and a recurrent neural network to navigate a maze.
[160.28s -> 164.68s]  They also use n-step returns with n equals 4, and they have a single network for the
[164.68s -> 168.44s]  actor and critic with multiple heads.
[168.44s -> 172.52s]  If you want to read more about actor-critic algorithms, here are a few recommendations,
[172.52s -> 174.44s]  some classic papers.
[174.44s -> 178.56s]  This paper, called Policy Gradient Methods for Reinforcement Learning with Function Approximation,
[178.56s -> 179.96s]  is actually a very nice paper to read.
[179.96s -> 185.56s]  It describes the theoretical foundations of policy gradients and also discusses what
[185.56s -> 190.52s]  I refer to as the causality trick before, where you can disregard past rewards at
[190.52s -> 194.32s]  the current time step, and it actually describes how to turn all of this into an
[194.32s -> 195.32s]  actor-critic method.
[195.32s -> 200.36s]  So, a lot of the material in today's lecture is based on the ideas presented in this paper.
[200.36s -> 203.20s]  Some more recent deep reinforcement learning papers.
[203.20s -> 207.64s]  This is the Asynchronous Methods paper that I showed on the previous slide.
[207.64s -> 212.12s]  This is the GAE paper, and then this is the paper that used action-dependent control
[212.12s -> 213.88s]  variants called QPROP.
