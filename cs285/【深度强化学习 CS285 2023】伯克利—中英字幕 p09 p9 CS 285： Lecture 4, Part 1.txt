# Detected language: en (p=1.00)

[0.88s -> 8.48s]  Hello and welcome to the fourth lecture of CS285. In today's lecture we're going to go over a
[8.48s -> 13.20s]  comprehensive introduction to reinforcement learning algorithms, definitions, and basic concepts.
[14.24s -> 19.04s]  So let's start with some definitions. First let's go over some of the terminology that we
[19.04s -> 23.92s]  covered in the previous lecture. When we talked about imitation learning, we learned
[23.92s -> 29.84s]  that we can represent a policy as a distribution over actions a, t, conditional observations o, t.
[30.64s -> 36.24s]  We call this policy pi, and we often use a subscript theta to denote the policy depends
[36.24s -> 41.60s]  on a factor of parameters that we're going to denote theta. When we're doing deep reinforcement
[41.60s -> 46.56s]  learning, oftentimes we will represent the policy with a deep neural network, although as we will
[46.56s -> 51.20s]  learn in the next few lectures in the course, depending on the type of reinforcement learning
[51.20s -> 56.32s]  algorithm, we might choose to represent the policy directly or implicitly through some other object,
[56.32s -> 62.88s]  such as a value function. Important definitions to know are the state which we denote s, t,
[62.88s -> 68.56s]  the observation o, t, and the action a, t. As we learned in the imitation learning lecture,
[68.56s -> 73.12s]  the observation and state can be related to one another via the following graphical model,
[73.68s -> 78.72s]  where the edge between observations and actions is the policy, the edge between current
[78.72s -> 82.96s]  states and actions and future states is the transition probability or the dynamics,
[83.68s -> 90.80s]  and the state satisfies the Markov property, which means that the state at time t plus one
[90.80s -> 95.84s]  is independent of the state at time t minus one when conditional on the current state s, t.
[96.56s -> 100.88s]  The Markov property is the main thing that distinguishes the state from the observation.
[100.88s -> 105.52s]  The state has to satisfy the Markov property, whereas the observation does not. And we
[105.60s -> 110.56s]  learn in the imitation learning lecture that the observation is some stochastic function of the
[110.56s -> 115.92s]  state which may or may not contain all the information necessary to infer the full state.
[115.92s -> 122.16s]  So that's kind of the primary difference. We will discuss algorithms for both fully observed
[122.72s -> 126.00s]  reinforcement learning, where we have access to the state, and partially
[126.00s -> 129.28s]  observed reinforcement learning, where you only have access to an observation.
[129.28s -> 138.48s]  All right, so that's the Markov property. And typically you'll see me write the policy as pi
[138.48s -> 143.60s]  theta a t given o t or pi theta a t given s t, depending on whether I'm talking about
[143.60s -> 148.32s]  the partially observed or the fully observed case. I will sometimes get a little sloppy
[148.32s -> 153.76s]  and use s t, when in fact you could also use o t, but in cases where this distinction
[153.76s -> 155.92s]  is important, I'll make a remark in the lectures.
[158.24s -> 162.24s]  So in imitation learning, we saw that we could collect the data set,
[162.24s -> 167.36s]  let's say of humans driving a vehicle, consisting of observation action tuples,
[167.36s -> 172.16s]  and then use supervised learning algorithms to figure out how to train a policy to take
[172.16s -> 176.40s]  actions that resemble those of the expert. In today's lecture, we'll introduce the
[176.40s -> 181.44s]  formalism of reinforcement learning, which allows us to train these policies without having
[181.44s -> 188.16s]  access to expert data. So to do that, of course, we need to define what it is that
[188.16s -> 193.20s]  we want the policy to do, and we define the objective by means of something called
[193.20s -> 198.00s]  a reward function. So we could say, well, which action is better or worse if you're
[198.00s -> 201.52s]  driving this car? If you don't have any data, how can you say what is a good action,
[201.52s -> 206.24s]  what is a bad action? So the reward function essentially tells you that the reward function
[206.24s -> 210.72s]  is a scalar-valued function of the state and the action, although sometimes it can depend
[210.72s -> 214.24s]  on only the state. Most generally it can depend on both the state and the action.
[215.52s -> 220.56s]  And it tells us which states and actions are better. So for example, if you're trying to
[220.56s -> 226.40s]  drive a car, you could say, well, a state where the car is driving quickly on the road
[226.40s -> 230.96s]  is a high-reward state, whereas a state where the car is collided with another car
[230.96s -> 238.24s]  is a low-reward state. But crucially, the objective in reinforcement learning is not just
[238.24s -> 243.36s]  to take actions that have high rewards right now, but rather to take actions that will lead
[243.36s -> 249.12s]  to high rewards later. So if you're driving on the road a little too fast, you might be getting
[249.12s -> 253.36s]  a high reward, but that might lead to an inevitable collision later that will lead to
[253.36s -> 258.48s]  low reward. So you have to consider the future rewards when choosing the current actions, and
[258.48s -> 262.32s]  that's really at the heart of the decision-making problem, that's at the heart of the
[262.32s -> 266.72s]  reinforcement learning problem, is how do you choose the right actions now to receive high
[266.72s -> 268.16s]  rewards later?
[270.56s -> 277.44s]  Okay, so together, the state, the action, the reward, and the transition probabilities define
[277.44s -> 282.80s]  what we call a Markov decision process. It is a decision process on a Markovian state.
[284.56s -> 290.56s]  So let's build up towards a full formal definition of Markov decision processes. We'll
[290.56s -> 297.20s]  start with something called a Markov chain. The Markov chain is named after Andrey Markov,
[297.20s -> 302.16s]  who was a mathematician who pioneered the study of stochastic processes, including Markov chains.
[303.04s -> 306.32s]  And the Markov chain has a very simple definition. It consists of just two things,
[307.12s -> 316.40s]  a set of states s and a transition function t. The state space is simply a set which could be
[316.40s -> 321.52s]  either discrete or continuous, so you can have a discrete state, in which case each state is a
[321.52s -> 325.60s]  discrete element in a finite size set, or you could have a continuous state, in which case
[325.60s -> 334.16s]  perhaps your states correspond to real value vectors in Rn. t is a transition operator.
[334.16s -> 337.92s]  It can also be referred to as a transition probability or a dynamics function.
[338.88s -> 346.00s]  It specifies a conditional probability distribution. So in a Markov chain, t denotes the probability
[346.00s -> 352.64s]  of the state at time t plus one conditioned on the state at time t. And the reason that it's
[352.64s -> 359.52s]  called an operator is because if we represent the probabilities of each state at time step t
[359.52s -> 364.48s]  as a vector, so let's say we have n states, this becomes a vector with n elements, and we can
[364.48s -> 370.96s]  call it mu t comma i for the probability of the ith state. The whole vector would be called
[370.96s -> 381.36s]  mu t. Then we can write the transition probabilities as a matrix, where the ijth entry is the
[381.36s -> 386.64s]  probability of going into state i if you're currently in a state j. And if we do this,
[386.64s -> 391.44s]  then we can express the vector of state probabilities at the next time step mu t plus one
[392.00s -> 399.36s]  as simply a matrix vector product between the matrix of probabilities t and the vector of
[399.36s -> 408.56s]  state probabilities mu t. This is simply a way of writing the chain rule of probability with a
[408.56s -> 416.56s]  little bit of linear algebra. But here you can see that t acts on mu t as a linear operator,
[416.56s -> 420.08s]  which is why we call it the transition operator. It's an operator that when applied
[420.08s -> 425.12s]  to the current vector of state probabilities produces the next vector of state probabilities.
[425.12s -> 430.80s]  So here's the graphical model corresponding to the Markov chain,
[432.16s -> 435.52s]  and here is the edge denoting transition probabilities.
[438.40s -> 443.60s]  And of course the states in the Markov chain denote, the states in the Markov chain satisfy
[443.60s -> 447.60s]  the Markov property, which means that the state at time t plus one is conditionally
[447.60s -> 451.52s]  independent of the state at time t minus one given the state at time t.
[451.52s -> 456.64s]  All right, now the Markov chain by itself doesn't allow us to specify a decision-making problem
[456.64s -> 462.64s]  because there's no notion of actions. So in order to go towards a notion of actions,
[462.64s -> 468.80s]  we need to turn the Markov chain into a Markov decision process. And this was really a much
[468.80s -> 476.24s]  more recent invention pioneered in the 1950s. So the Markov decision process adds a few additional
[476.24s -> 482.32s]  objects to the Markov chain. It adds an action space and a reward function. So now we have a state
[482.32s -> 487.60s]  space, which is a discrete or continuous set of states. We have an action space, which is also
[487.60s -> 492.96s]  a discrete or continuous set. So the graphical model now contains both states and actions,
[494.00s -> 498.56s]  and our transition probabilities are now conditioned on both states and actions. So we have p of
[498.56s -> 509.52s]  of s t plus one given s t comma a t. t is still called the transition operator, but it can no longer be expressed as a matrix.
[509.52s -> 514.64s]  Now it's actually a tensor because it has three dimensions, the next state, the current state,
[514.64s -> 522.08s]  and the current action. But we can do the same kind of linear algebra trick. So if we let mu
[522.16s -> 529.84s]  t comma j denote the probability of being in state j at time t, and we can have another
[529.84s -> 536.32s]  vector that will denote the probability of taking some action, and now we can write t as
[536.32s -> 541.60s]  a tensor. So t i j k is the probability of entering state i if you're in state j and taking
[541.60s -> 549.04s]  action k. Then you can write a linear form that describes the state probability mu t plus one
[549.44s -> 556.24s]  comma i at the next time step as a linear function of the current state probabilities,
[556.24s -> 560.96s]  the current action probabilities, and the transition probabilities. So that means that
[560.96s -> 565.60s]  this transition operator, although it is now a tensor, is still a linear operator
[565.60s -> 570.96s]  that transforms current action state probabilities into next time step state probabilities.
[570.96s -> 577.04s]  Now we also have this reward function, and the reward function is a mapping from the
[577.04s -> 581.60s]  Cartesian product of the state and action space into real value numbers. And this is
[581.60s -> 587.76s]  what allows us to define an objective for reinforcement learning. So we call r of
[587.76s -> 593.44s]  s t comma a t the reward, and our objective, which I will define in a few slides from now,
[593.44s -> 598.88s]  will be to maximize total rewards. But before I do that, I just want to extend this
[599.12s -> 605.04s]  Markov decision process definition to also define the partially observed Markov decision process,
[605.04s -> 610.00s]  and this is what will allow us to bring in the notion of observations. So a partially
[610.00s -> 615.04s]  observed Markov decision process further augments the definition with two additional objects,
[615.04s -> 621.60s]  an observation space O and an emission probability or an observation probability
[621.60s -> 628.72s]  or an observation probability E. So again, S is the state space, A is an action space,
[628.72s -> 634.88s]  and O is now an observation space. The graphical model now looks the same as it did for the MDP
[634.88s -> 638.80s]  with the addition that we have these observations O that depend on the state.
[640.00s -> 644.24s]  So we have a transition operator just like before, and now we have an emission probability,
[644.24s -> 648.96s]  a p of O t given s t. And of course we also have the reward function.
[649.84s -> 653.68s]  The reward function is still mapping from states and actions to real numbers,
[653.68s -> 657.28s]  so the reward function convention is the final states, not on observations.
[658.00s -> 662.48s]  But typically in a partially observed Markov decision process, or POMDP,
[662.48s -> 666.48s]  we would be making decisions based on observations without access to the true states.
[668.64s -> 673.44s]  All right, now that we've defined the mathematical objects of the Markov chain,
[673.44s -> 677.52s]  the Markov decision process, and the partially observed Markov decision process,
[678.32s -> 680.96s]  let's define an objective for reinforcement learning.
[682.72s -> 688.32s]  So in reinforcement learning, we're going to be learning some object that defines a policy. So
[688.32s -> 692.32s]  for now let's just assume that we learned the policy directly, and we'll see later on how
[692.32s -> 696.00s]  there are some other methods that might represent a policy implicitly. But for now we'll be
[696.00s -> 702.08s]  explicitly learning pi theta A given s. We'll come back to the partially observed case later,
[702.08s -> 706.80s]  for now let's just say that our policies condition on s. And theta corresponds to the
[706.80s -> 712.56s]  parameters of the policy. So if the policy is a deep neural net, then the theta denotes the
[712.56s -> 718.32s]  parameters of that deep neural net. The state goes into the policy, the action comes out,
[718.96s -> 723.20s]  and then the state and action go into the transition probability, basically the physics
[723.20s -> 728.16s]  that govern the world, which produces the next state, right? So that's the process that we are
[728.16s -> 736.56s]  controlling. Now in this process we can write down a probability distribution over trajectories.
[736.56s -> 742.72s]  So trajectories are sequences of states and actions, s1 a1, s2 a2, etc, etc, until you get to
[742.72s -> 748.88s]  s t a t. For now we'll assume that our control problem is finite horizon, which means that
[748.88s -> 754.08s]  the decision-making task lasts for a fixed number of time steps capital T and then ends.
[754.96s -> 759.52s]  We will extend this to the infinite horizon setting shortly, but for now we'll write down
[759.52s -> 766.48s]  the finite horizon version because it's quite a bit easier to start with. So if we write
[766.48s -> 772.96s]  down the joint distribution over states and actions, and here I'm putting the subscript
[772.96s -> 776.56s]  theta on this joint distribution to indicate that it depends on the policy pi theta,
[777.20s -> 781.52s]  we can factorize it by using the chain rule in terms of probability distributions that we've
[781.52s -> 786.80s]  already defined. So we have an initial state distribution p of s1. I sort of brushed this
[786.80s -> 791.04s]  under the rug when I defined the Markov chain, the MDP, and the POMDP, but all of these also
[791.04s -> 796.80s]  have an initial state distribution p of s1. And then we have a product over all time steps
[796.80s -> 802.00s]  of the probability of an action a t given s t and the probability of the transition to the
[802.00s -> 808.48s]  next time step s t plus 1 given s t a t. Now I said this is derived from the chain rule of
[808.48s -> 813.28s]  probability, but of course in the chain rule of probability you need to condition on all past
[813.28s -> 819.04s]  variables, but here we are exploiting the Markov property to drop the dependence on s t minus
[819.04s -> 825.20s]  1, s t minus 2, etc., etc., because we know that s t plus 1 is conditionally independent
[825.20s -> 830.40s]  of s t minus 1 given s t. So this is how we can define the trajectory distribution.
[832.96s -> 840.08s]  And for notational brevity, I will sometimes write p of tau to denote p of s1 through
[840.08s -> 846.48s]  s t a t. So tau is just a shorthand for trajectory, and all it means is a sequence
[846.48s -> 847.84s]  of states and actions.
[850.40s -> 855.44s]  Okay, so having defined the trajectory distribution, we can actually define an objective for reinforcement
[855.44s -> 860.08s]  learning, and we can define that objective as an expected value under the trajectory
[860.08s -> 865.52s]  distribution. So the goal in reinforcement learning is to find the parameters theta
[865.52s -> 872.48s]  that define our policy so as to maximize the expected value of the sum of rewards over the
[872.48s -> 879.28s]  trajectory. So we would like a policy that produces trajectories that have the highest
[879.28s -> 885.20s]  possible rewards in expectation. And the expectation, of course, accounts for the
[886.32s -> 891.76s]  stochasticity of the policy, the transition probabilities, and the initial state distribution.
[892.64s -> 896.96s]  So this is the definition of the reinforcement learning objective that we're going to work
[896.96s -> 901.36s]  with. There are of course a few variants on this, and we will derive them over the course
[901.36s -> 906.88s]  of the next few lectures, but this is the most basic version. So at this point, I would
[906.88s -> 910.80s]  like all of you to pause and look carefully at this objective and really make sure that
[910.80s -> 914.32s]  you understand what this means, that you understand what it means to have a sum of
[914.32s -> 918.40s]  rewards, what it means to take their expectation under a trajectory distribution,
[919.36s -> 924.24s]  what a trajectory distribution is, and how it is influenced by our choice of policy parameters
[924.24s -> 929.20s]  theta, which in turn influence the policy pi theta. Because if this part is unclear,
[929.20s -> 933.36s]  then what follows in the remainder of this lecture will be quite hard to follow. So
[933.36s -> 936.64s]  please take a moment to think about this, and if you have any questions
[936.64s -> 941.20s]  about the trajectory distribution, please be sure to write a comment on the video.
[944.08s -> 950.64s]  All right, let's proceed. So one of the things that we might notice
[951.20s -> 954.96s]  about this factorization of the trajectory distribution
[955.84s -> 961.04s]  is that it actually, although it's defined in terms of the objects that we had in the
[961.04s -> 967.04s]  Markov decision process, it can also be interpreted as a Markov chain. And to interpret
[967.04s -> 973.28s]  this as a Markov chain, we need to define a kind of augmented state space. So our original
[973.28s -> 977.44s]  state space is S, but we also have these actions, and the actions make this a Markov decision
[977.44s -> 984.00s]  process. But we know that the action depends on the state based on the policy, so we
[984.96s -> 989.36s]  So pi theta a t given S t allows us to get a distribution of our actions conditioned on states.
[990.16s -> 995.20s]  So what we can do is we can group this state and action together into a kind of augmented state,
[996.00s -> 1000.40s]  and now the augmented states actually form a Markov chain.
[1002.40s -> 1010.24s]  So P of S t plus one comma a t plus one given S t comma a t, the transition operator in this
[1010.24s -> 1016.72s]  augmented Markov chain is simply the product of the transition operator in the MDP and the policy.
[1021.12s -> 1025.84s]  So this can allow us to define the objective in a slightly different way
[1025.84s -> 1028.96s]  that will be convenient to use in some of our later derivations.
[1028.96s -> 1034.24s]  So, so far I've defined the objective as an expected value under the trajectory distribution
[1034.24s -> 1041.36s]  of the sum of rewards. But remember that our distribution actually follows a Markov chain
[1041.36s -> 1048.24s]  with this augmented space, and this transition operator is the product of the MDP transitions
[1048.24s -> 1054.08s]  and the policy. So we could also write the objective by linearity of expectation
[1054.08s -> 1061.20s]  as the sum over time of the expected values under the state action marginal in this Markov chain
[1061.20s -> 1067.12s]  of the reward of that time step. So this is just using linearity of expectation to take the sum out
[1067.12s -> 1072.72s]  of the expectation, so that you have a sum over t of the expectation over tau of r s t a t,
[1073.36s -> 1078.32s]  and then since the thing inside the expectation not only depends on S t a t, we can marginalize
[1079.12s -> 1085.68s]  all the other variables out, and we are left with a sum over the expectation under
[1085.68s -> 1094.08s]  p theta s t comma a t of r s t a t. Now this might seem like kind of a useless little
[1094.08s -> 1099.52s]  mathematical, you know, kind of rewriting of the original objective,
[1099.52s -> 1103.52s]  but it turns out to be quite useful if we want to extend this to the infinite horizon case.
[1105.20s -> 1112.40s]  So this marginal p theta s t given a t in a finite time Markov chain can be obtained
[1112.40s -> 1114.64s]  just by marginalizing out all the other time steps,
[1116.72s -> 1122.64s]  but we can also use this objective to get the infinite horizon case. So what if t equals
[1122.64s -> 1128.16s]  infinity? Well, okay, the first thing that happens if t equals infinity is your objective
[1128.16s -> 1134.48s]  might become ill-defined. For example, if your reward is always positive, then you have a sum
[1134.48s -> 1138.88s]  of an infinite number of positive numbers, which is going to be infinity. So we need some way to
[1138.88s -> 1143.44s]  make the objective finite, and there are a few ways of doing this. One way of doing this,
[1143.44s -> 1149.68s]  which I'll use now for convenience, but it's actually not the most common way, is to use
[1150.32s -> 1156.32s]  what's called the average reward formulation. So you basically take this sum of expected
[1156.32s -> 1161.76s]  rewards and you divide it by capital T. So basically the average reward over all time steps.
[1162.32s -> 1167.84s]  Dividing by capital T is a constant, so in general this doesn't change the maximum,
[1167.84s -> 1171.76s]  but then you can take t to infinity and get a well-defined quantity. Later on we'll learn
[1171.76s -> 1175.76s]  about something called discounts, which is another way to get a finite number for the infinite
[1175.76s -> 1183.36s]  horizon case. But, so making this finite is pretty easy, but let's talk about how we can
[1183.36s -> 1188.00s]  actually define an infinite horizon objective. So we have our Markov chain from before,
[1188.88s -> 1195.12s]  and our augmented Markov chain has this transition operator, so that means that we can write
[1195.12s -> 1201.76s]  the vector s t plus one comma a t plus one as some linear operator t applied to s t comma a t,
[1202.96s -> 1208.88s]  and this is the state action transition operator. And more generally we can skip k
[1208.88s -> 1215.12s]  time steps ahead, and we can say that s t plus k a t plus k is equal to t to the power k
[1215.12s -> 1222.40s]  times s d a t. So one question we could ask is, does the state action marginal
[1222.40s -> 1226.88s]  p of s t comma a t converge to a stationary distribution,
[1226.88s -> 1231.52s]  basically converge to a single distribution, as little k goes to infinity?
[1234.00s -> 1238.56s]  If this is true, that means that we should be able to write the stationary distribution mu
[1239.20s -> 1246.24s]  as being equal to t times mu. And under a few technical assumptions, namely ergodicity and
[1246.96s -> 1251.84s]  the chain being aperiodic, we can actually show the stationary distribution exists.
[1251.92s -> 1256.64s]  Intuitively being aperiodic simply means exactly what it sounds like, that the Markov chain is not
[1256.64s -> 1262.24s]  periodic, and being ergodic means that, roughly speaking, every state can be reached from every
[1262.24s -> 1267.44s]  other state with non-zero probability. The ergodic assumption is important because it prevents a
[1267.44s -> 1271.76s]  situation where if you start in one part of the MDP you might never reach another one.
[1271.76s -> 1276.56s]  So if this is true, if starting in one part may result in you never reaching another part,
[1277.28s -> 1281.44s]  then where you start always matters and the stationary distribution doesn't exist. But if this
[1281.44s -> 1285.20s]  is not the case, if there's even a slight chance of getting to any state from any other
[1285.20s -> 1289.60s]  state eventually, then you will have a stationary distribution provided that it's aperiodic.
[1291.92s -> 1296.32s]  So the stationary distribution must obey this equation mu equals t times mu,
[1296.32s -> 1298.32s]  because otherwise it's not a stationary distribution.
[1300.32s -> 1304.48s]  So stationary means it's the same before and after the transition, and if it's the same before
[1304.48s -> 1309.20s]  and after the transition, then applying t enough times will eventually allow you to reach it.
[1310.16s -> 1315.52s]  You can solve for the stationary distribution simply by rearranging this equation to see that
[1315.52s -> 1322.00s]  it is equal to tau minus i times, that can be written as tau minus i times mu equals zero.
[1322.00s -> 1327.36s]  And remember that mu is a distribution, so it's a vector of numbers that are all positive in
[1327.36s -> 1334.96s]  sum to one. So one way you can find mu is by finding the eigenvector with eigenvalue one
[1335.92s -> 1341.60s]  for the matrix defined by t. So mu is eigenvector of t with eigenvalue one,
[1343.44s -> 1346.96s]  and it always exists under the ergodicity and aperiodicity assumptions.
[1349.12s -> 1353.76s]  So if we know that if we run this Markov chain forward enough times,
[1353.76s -> 1360.24s]  eventually it'll settle into mu. That means that as t goes to infinity, this sum
[1360.24s -> 1365.44s]  of the expectations of the marginals becomes dominated by the stationary distribution terms.
[1365.44s -> 1368.56s]  So you have some finite number of terms initially that are not in the stationary
[1368.56s -> 1372.24s]  distribution, mu one, mu two, mu three, etc., but then you have infinitely many
[1372.24s -> 1374.72s]  terms that are very very close to the stationary distribution,
[1376.72s -> 1381.28s]  which means that once you put in the average reward case, so you're going to find put a one
[1381.28s -> 1386.56s]  over t and then take the limit as t goes to infinity, the limit is basically going to be
[1386.56s -> 1389.84s]  the expected value of the reward under the stationary distribution.
[1390.56s -> 1393.52s]  And that allows us to define an objective for reinforcement learning
[1393.52s -> 1396.48s]  in the infinite horizon case as t goes to infinity.
[1398.64s -> 1402.64s]  Okay, this is perhaps a lot to take in, so this would be a good place to pause,
[1403.20s -> 1406.56s]  think about the derivation on this slide, and if something is unclear
[1406.56s -> 1409.52s]  or you have any questions, please be sure to write them in the comments.
[1413.52s -> 1420.08s]  All right, now one last bit that I want to describe in this section, which is very important
[1420.08s -> 1424.40s]  for understanding the basic principle behind a lot of reinforcement learning methods,
[1425.12s -> 1429.44s]  is that reinforcement learning is really about optimizing expectations.
[1430.16s -> 1434.72s]  So although we talk about reinforcement learning in terms of choosing actions that
[1434.72s -> 1439.60s]  lead to high rewards, we're always really concerned about expected values of rewards.
[1440.56s -> 1446.56s]  And the interesting thing about expected values is that expected values can be continuous
[1446.56s -> 1450.24s]  in the parameters of the corresponding distributions, even when the function
[1450.24s -> 1453.92s]  that we're taking the expectation of is itself highly discontinuous.
[1454.64s -> 1460.00s]  And this is a really important fact for understanding why reinforcement learning
[1460.00s -> 1465.84s]  algorithms can use smooth optimization methods like gradient descent to optimize objectives
[1465.84s -> 1470.72s]  that are seemingly non-differentiable, like binary rewards for winning or losing a game.
[1471.60s -> 1473.52s]  Let me explain this with a little toy example.
[1474.80s -> 1481.04s]  Let's imagine that you're driving down a mountain road and your reward is plus one if you stay on
[1481.04s -> 1485.60s]  the road and zero if you fall, or negative one if you fall off the road.
[1486.48s -> 1489.36s]  So the reward function here appears to be discontinuous.
[1489.36s -> 1493.04s]  There is a discontinuity between staying on the road and falling off the road,
[1493.04s -> 1497.44s]  and if you try to optimize the reward function with respect to, for example,
[1498.40s -> 1503.92s]  the position of the car, that optimization problem can't really be solved with gradient-based methods
[1503.92s -> 1509.12s]  because the reward is not a continuous or much less a differentiable function of the car's
[1509.12s -> 1518.48s]  position. However, if you have a probability distribution over some action, let's say that
[1518.48s -> 1522.72s]  abstractly that you just get to choose like fall or don't fall, so you have a binary action,
[1522.72s -> 1527.92s]  you either fall or you don't fall, and it's a Bernoulli random variable with parameter theta,
[1527.92s -> 1532.24s]  so with probability theta you fall off, with probability one minus theta you don't fall off.
[1534.56s -> 1540.24s]  Now the interesting thing is that the expected value of the reward with respect to pi theta is
[1540.24s -> 1545.12s]  actually smooth in theta because you have a probability of theta falling off, which has
[1545.12s -> 1549.04s]  a reward of minus one, and a probability of one minus theta of staying on the road,
[1549.68s -> 1557.52s]  so the reward is one minus theta plus one minus theta minus theta, and that's perfectly smooth
[1557.52s -> 1564.00s]  and perfectly differentiable in theta. So this is a very important property that will come up
[1564.00s -> 1568.08s]  again and again, and that it really explains why reinforcement learning algorithms can optimize
[1569.04s -> 1574.00s]  seemingly non-smooth and even sparse reward functions, which is that expected values
[1574.00s -> 1578.96s]  of non-smooth and non-differentiable functions under differentiable and smooth probability
[1578.96s -> 1588.08s]  distributions are themselves smooth and differentiable. Okay, let's pause there.
