# Detected language: en (p=1.00)

[0.00s -> 9.26s]  Okay, so let's talk about the conditions under which we can use p theta s t in place
[9.26s -> 15.10s]  of p theta prime of s t and still have a reasonable objective that accurately approximates
[15.10s -> 23.28s]  the return of the new policy. So to recap, what we want to do is we want to ignore
[23.28s -> 28.78s]  the distribution mismatch. We want to use p theta of s t instead of p theta prime
[28.78s -> 33.70s]  of s t, such that the only dependence on theta prime in this whole equation is in
[33.70s -> 37.46s]  the importance weight, because we know from our previous policy gradient
[37.46s -> 41.30s]  lecture that if we differentiate the thing on the right hand side, we get
[41.30s -> 47.14s]  exactly the policy gradient. So we want this to be true so that j theta
[47.14s -> 50.94s]  prime minus j theta is well approximated by a bar theta prime, this
[50.94s -> 56.02s]  thing on the right hand side, so that we could just maximize a bar and get a
[56.30s -> 63.34s]  better new policy. And what we're going to try to show is that p theta of s t is
[63.34s -> 67.90s]  close to p theta prime of s t when pi theta is close to pi theta prime, and
[67.90s -> 71.70s]  when that's the case, then the right hand side approximates the left hand
[71.70s -> 75.90s]  side, meaning the difference between them can be bounded by some quantity
[75.90s -> 82.78s]  which is small if the difference between pi theta and pi theta prime is small.
[82.78s -> 87.86s]  Okay, so this is what we're interested in. We want to show that pi theta s t is
[87.86s -> 92.98s]  close to pi theta prime s t, sorry, p theta s t is close to p theta prime s t
[92.98s -> 99.06s]  when pi theta is close to pi theta prime. So let's start with a simple case.
[99.06s -> 105.10s]  Let's first assume that pi theta is a deterministic policy, which means that
[105.10s -> 109.50s]  we can express it as a t equals pi theta of s t. We'll generalize the
[109.54s -> 113.70s]  stochastic policies later, but I think the deterministic derivation provides a
[113.70s -> 117.14s]  little bit of intuition, which then makes the stochastic case easier to
[117.14s -> 125.90s]  understand. So in this case what we're going to try to show is that the
[125.90s -> 132.46s]  state marginals for theta and theta prime are close if pi theta prime is
[132.46s -> 137.14s]  close to pi theta. Now pi theta prime is not necessarily deterministic, and the
[137.14s -> 141.86s]  way that we define close is that the probability that pi theta prime assigns
[141.86s -> 147.64s]  to any action that is not the action that pi theta would have taken is less
[147.64s -> 151.30s]  than or equal to epsilon. Essentially there's a bounded
[151.30s -> 159.34s]  probability that pi theta prime does something different. So if this is the
[159.34s -> 164.74s]  case, then we can write the state marginal at time step t for theta prime
[165.06s -> 171.82s]  as the sum of two terms. The first term describes the case where every
[171.82s -> 176.90s]  single time step up to t, the new policy pi theta prime did exactly the
[176.90s -> 181.26s]  same thing as pi theta. And since the probability of doing exactly the same
[181.26s -> 186.14s]  thing as 1 minus epsilon, this term has a multiplier of 1 minus epsilon to
[186.14s -> 190.70s]  the power t in front of it, and the state marginal is exactly p theta s t,
[190.70s -> 194.22s]  because if you did all the same things as pi theta you have the same state
[194.22s -> 200.58s]  distribution. The other term is simply everything else. It's 1 minus 1 minus
[200.58s -> 205.78s]  epsilon to the t, and it multiplies some other state distribution which
[205.78s -> 208.86s]  we're going to assume that we don't know anything about. So we're going to
[208.86s -> 213.02s]  call it p-mistake. This is the distribution over states you get if you
[213.02s -> 216.10s]  made at least one mistake, and we're going to assume that we don't know
[216.10s -> 219.74s]  anything about it. So it could be like that tightrope walker from the
[219.74s -> 223.06s]  imitation learning lecture at the beginning of class, where if you fall off
[223.10s -> 226.86s]  the tightrope you can never recover, and you're in some completely different place.
[226.86s -> 231.42s]  So of course most of you hopefully recognize this equation as being
[231.42s -> 235.94s]  exactly the same as the equation we had before when we analyzed behavioral
[235.94s -> 239.30s]  cloning. And in fact our assumption is very similar to the assumption we had
[239.30s -> 244.90s]  before, which is that the new policy has a bounded probability of deviating
[244.90s -> 251.06s]  from the old policy. So this is the probability we made no mistake, and
[251.10s -> 254.86s]  this is some other distribution on which we're going to make no assumption
[254.86s -> 263.50s]  whatsoever. So hopefully this seems familiar to all of you. And just like before, this
[263.50s -> 269.02s]  equation implies that we can write the total variation divergence between p
[269.02s -> 275.70s]  theta prime and p theta as essentially just the part that's different, right? So
[275.70s -> 280.46s]  that 1 minus epsilon to the t times p theta st part has zero total
[280.46s -> 285.46s]  variation divergence against p theta of st, so it's only that second part, and
[285.46s -> 290.78s]  that second part has a multiplier of 1 minus 1 minus epsilon to the t in front
[290.78s -> 295.66s]  of it, and that multiplies the TV divergence between p mistake and p
[295.66s -> 299.98s]  theta. Now we can't bound this TV divergence the p mistake minus p theta
[299.98s -> 304.10s]  because we're making no assumption on p mistake, so the only bound we have is
[304.10s -> 308.38s]  the trivial bound, which is that any total variation divergence always has to
[308.38s -> 312.90s]  be less than or equal to 2. So that means that the total variation
[312.90s -> 319.58s]  divergence between the state marginals is bounded by 2 times 1 minus 1 minus
[319.58s -> 323.58s]  epsilon to the t. This is all exactly the same as in the
[323.58s -> 330.26s]  imitation learning analysis that we had in the second lecture. And we use
[330.26s -> 334.46s]  the same useful identity, which is that 1 minus epsilon to the t is greater
[334.50s -> 339.02s]  than or equal to 1 minus epsilon times t for any epsilon between 0 and 1, and
[339.02s -> 343.38s]  that allows us to express this bound as a quantity that is linear in epsilon
[343.38s -> 347.10s]  and linear in t. So the difference between the state marginals is less than
[347.10s -> 353.62s]  or equal to 2 times epsilon times t. Okay, so it's not a great bound, but it
[353.62s -> 357.04s]  is a bound because it shows that as epsilon decreases, the state
[357.04s -> 362.10s]  marginals become similar to one another. But of course this was all for
[362.14s -> 366.46s]  the case of the deterministic policy pi-theta. In general, pi-theta will not be
[366.46s -> 373.46s]  deterministic. So what we're going to do next is we're going to analyze the
[373.46s -> 377.62s]  general case where pi-theta is an arbitrary distribution, and this proof
[377.62s -> 381.18s]  follows the proof in the trust region policy optimization paper, which is
[381.18s -> 387.86s]  referenced at the bottom of this slide. Okay, so here we're going to say that
[387.98s -> 393.06s]  pi-theta prime is close to pi-theta if their total variation divergence is
[393.06s -> 400.38s]  bounded by epsilon for all states s, t. Now it turns out that we actually don't
[400.38s -> 403.14s]  need the bound to be point-wise. It turns out we can actually use a bound
[403.14s -> 407.06s]  that's in expectation, but let's go with a point-wise bound for now
[407.06s -> 411.18s]  because it's a little easier to explain. But keep in mind this will also hold true
[411.18s -> 416.62s]  if the bound is in expectation, meaning the expected value of
[416.74s -> 422.98s]  the total variation divergence is less than or equal to epsilon. Okay, so a useful
[422.98s -> 428.96s]  lemma that we're going to use for this analysis is this one. This lemma
[428.96s -> 432.50s]  will take a little bit of unpacking, and I'm not going to prove it. This is
[432.50s -> 436.18s]  in prior work, but it's referenced in the trust region policy optimization
[436.18s -> 440.02s]  paper at the bottom. So this lemma says that if you have two
[440.02s -> 445.58s]  distributions, which I'm going to call px and py, and the total variation
[445.60s -> 448.42s]  divergence between these two distributions, meaning the sum over all
[448.42s -> 453.18s]  values of x of the absolute value of the difference between px of x and py of x,
[453.18s -> 459.86s]  is equal to epsilon, so the total variation divergence between them is epsilon, then
[459.86s -> 467.58s]  there exists a joint distribution px comma y so that its marginal p of x
[467.58s -> 473.42s]  is px of x, and its marginal p of y is py of y, and the probability of x
[473.46s -> 480.90s]  equals y is one minus epsilon. So to unpack this lemma a little bit, what this says is that if you have two
[480.90s -> 486.26s]  distributions over the same variable, px of x and py of x, and their total variation
[486.26s -> 493.42s]  divergence is epsilon, then you can construct a joint distribution over two variables, such that the
[493.42s -> 498.22s]  marginals of that joint distribution with respect to its first and second argument are the two
[498.22s -> 503.34s]  distributions you started with, and the probability of its two arguments being equal
[503.34s -> 509.90s]  is one minus epsilon. And intuitively, the reason that this lemma is useful to us is that we essentially
[509.90s -> 514.54s]  want a generalization of the assumption from the previous slide. So in the previous slide, our
[514.54s -> 519.58s]  assumption was that there's a one minus epsilon probability that pi theta prime takes the same
[519.58s -> 524.86s]  action as pi theta. So if we could express a probability that pi theta prime and pi theta will
[524.86s -> 530.38s]  take the same action when both of them are stochastic, then we can use that to generalize
[530.38s -> 534.38s]  the result from the previous slide to the case where the policies are stochastic.
[536.54s -> 542.30s]  So essentially this says that px of x agrees with py of y with probability epsilon.
[544.70s -> 549.66s]  And that means that pi theta prime takes a different action from pi theta
[549.66s -> 557.02s]  with probability at most epsilon. In retrospect, this is actually kind of obvious because if their
[557.02s -> 561.90s]  total variation divergence is epsilon and total variation diverges is the difference in probabilities,
[561.90s -> 565.90s]  it kind of makes sense that the sliver of probability mass that is different between them
[565.90s -> 569.90s]  would have a volume of epsilon. So if you kind of have a more geometric way of thinking
[569.90s -> 575.50s]  about it, just imagine the two distributions as bar graphs, overlay them on top of each other,
[575.50s -> 579.98s]  and look at the differences in the bars. The volume of those differences will be equal to epsilon,
[579.98s -> 583.58s]  which means that your probability of doing something different is epsilon. So that's kind of the
[583.58s -> 587.98s]  geometric intuition, but if you prefer to think about things symbolically, then hopefully this
[587.98s -> 594.94s]  lemma kind of puts your mind at ease, that in fact if the total variation divergence is epsilon,
[594.94s -> 602.38s]  then the probability of the mistake is at most epsilon. So what this lemma allows us to do is
[602.38s -> 606.38s]  it allows us to state the same result that we had on the previous slide, which is the total
[606.38s -> 613.10s]  variation divergence between the state marginals is 1 minus 1 minus epsilon to the t
[613.10s -> 619.98s]  times p mistake minus p theta, only now we can say this even when pi theta is stochastic,
[619.98s -> 625.18s]  provided the total variation divergence between pi theta prime and pi theta is bounded by epsilon.
[626.86s -> 630.38s]  So from here everything is exactly the same, we can write the same bound,
[630.38s -> 635.18s]  and we can say that the state marginals differ by at most 2 times epsilon t.
[637.18s -> 641.34s]  Okay, so essentially the trick that we used on this slide was to use this lemma
[641.90s -> 645.18s]  to express a probability that two policies will take different actions
[646.94s -> 648.86s]  in terms of their total variation divergence.
[652.06s -> 656.06s]  Okay, so this is the result that we have on the state marginals.
[657.34s -> 662.06s]  So what does this tell us about the actual objective value, right? What we want is we
[662.06s -> 665.66s]  actually want to relate the two objectives expressed in terms of advantage values.
[666.78s -> 671.58s]  Well, so for this I'm going to derive another little calculation which describes the expected
[671.58s -> 675.98s]  values of functions under distributions when the total variation divergence between those
[675.98s -> 681.42s]  distributions is bounded. So we're going to have some function f of st, which in our case
[681.42s -> 686.14s]  is this complicated thing that involves expectations over actions and advantage values,
[686.14s -> 689.66s]  but it doesn't really matter what it is, whatever it is, let's call it f of st,
[690.30s -> 693.34s]  we can bound its expected value between the two distributions.
[694.30s -> 698.22s]  So we can write the expected value under p theta prime of st
[698.22s -> 703.02s]  of f of st as the sum over all possible states of p theta prime times f,
[704.94s -> 712.94s]  and we know that this quantity is greater than or equal to the sum over all the states of p
[712.94s -> 720.46s]  theta times f minus the total variation divergence between p theta and p theta prime times the
[720.46s -> 726.06s]  maximum value that f could possibly take on. So the way that I arrive at this calculation is I
[726.06s -> 733.26s]  write p theta prime as being equal to p theta prime plus p theta minus p theta,
[733.26s -> 740.86s]  I group the terms to get a p theta times f minus a p theta minus p theta prime times f,
[741.98s -> 746.62s]  and while I don't know the absolute value of the difference in the probabilities, I only
[746.62s -> 750.54s]  bound the total variation divergence. If I multiply them by the largest value that f takes,
[750.54s -> 754.94s]  I'm being as pessimistic as possible, so that's what allows me to write this inequality.
[755.82s -> 759.74s]  If this is not clear to you, then I would recommend pausing the lecture now,
[759.74s -> 763.74s]  getting out a piece of paper, and actually deriving this inequality yourself. It's a good
[763.74s -> 768.06s]  exercise to do to make sure that you understand how to manipulate total variation divergences,
[768.06s -> 771.26s]  so if this inequality doesn't make sense, please get out a piece of paper,
[771.26s -> 775.02s]  try to work through it now, so pause the lecture and do that.
[777.50s -> 780.94s]  And if you're having trouble doing that, then please ask a question in the comments,
[780.94s -> 782.86s]  and we'll go over this in more detail in class.
[785.34s -> 791.66s]  Okay, so then all we do is we take that first term, and we notice that it's just the
[791.66s -> 796.46s]  definition of the expected value under p theta, and we take the second term and we replace it
[796.46s -> 802.14s]  by our bound on the total variation divergence, which means that the expected value under p
[802.22s -> 809.10s]  theta prime of f is just, is bounded below by its expected value under p theta,
[809.10s -> 814.86s]  minus an error term, which is two times epsilon times t, times the largest value that f can
[814.86s -> 820.30s]  take on. Now this error term might seem pretty big because, you know, the largest value of f
[820.30s -> 824.70s]  might be huge, but remember that everything is multiplied by epsilon, so as the two policies
[824.70s -> 829.10s]  get closer together, as epsilon gets small, that second term can always be made arbitrarily
[829.10s -> 833.82s]  small, so long as f is bounded, basically, so long as f doesn't go off to infinity for any state.
[836.94s -> 840.22s]  So this was the equation that we were originally concerned about. We were concerned
[840.22s -> 845.98s]  about the expected value under p theta prime of s t of the expected value over the actions
[845.98s -> 848.38s]  of the importance sampled estimator for the advantage.
[851.18s -> 857.50s]  So by taking everything inside the brackets to be f, then we can bound this quantity below
[858.14s -> 864.62s]  by the expected value under p theta of s t of the same quantity, and this thing now just
[864.62s -> 867.90s]  looks exactly like the thing that we differentiate to get the policy gradient,
[868.78s -> 876.46s]  minus an error term, and the error term is two times epsilon times t, times a constant c,
[876.46s -> 881.34s]  and the constant c is the largest value that the thing inside the brackets, the thing inside
[881.34s -> 889.26s]  the state expectation, can take on. Take a moment to think about what the largest possible
[889.26s -> 892.86s]  value for that quantity inside the brackets could be. Basically what should we use for c?
[895.26s -> 899.18s]  The thing inside the brackets is a really complicated equation, but notice that
[899.18s -> 903.26s]  most of the terms in that equation are probabilities, and we know that probabilities
[903.26s -> 910.86s]  have to sum to one. So in fact what we can notice is that the quantity inside the
[910.86s -> 918.06s]  bracket is basically it's some expected value of an advantage, and what is an advantage?
[918.06s -> 923.34s]  Well an advantage is the sum of rewards over time. So that means that the largest value
[923.34s -> 929.66s]  that c could take on is the largest possible reward times the number of time steps, because
[929.66s -> 933.90s]  all those importance weights and all those expectations, they have to sum up to one.
[933.90s -> 938.86s]  So that means that basically the expected value of any function can't possibly be larger
[938.86s -> 943.26s]  than the largest value that function takes on, and the largest value that an advantage can take
[943.26s -> 948.46s]  on is the number of time steps times the largest reward, because ultimately an advantage
[948.46s -> 953.42s]  is the sum of rewards over time. So that means that the c is on the order of the number
[953.42s -> 959.26s]  of time steps times r max, and if you have infinite time steps but you use a discount,
[959.82s -> 963.90s]  then you know that the discount values, they form a geometric series, so they have to sum to
[963.90s -> 970.14s]  one over one minus gamma. So in a finite horizon case c is capital T times r max,
[970.14s -> 975.90s]  in an infinite horizon case it's r max over one minus gamma. By the way as an aside here,
[976.70s -> 981.10s]  if you're doing reinforcement learning theory and you ever see a term that looks like one over
[981.10s -> 985.42s]  one minus gamma, just mentally substitute the time horizon for that, because one over
[985.42s -> 990.62s]  one minus gamma is essentially the equivalent of a horizon in the infinite horizon case.
[990.62s -> 995.66s]  It's basically the number of time steps, the effective number of time steps that you're
[995.66s -> 1005.50s]  summing rewards over. Okay, so essentially all this says is that maximizing this equation
[1005.50s -> 1010.30s]  at the bottom maximizes a bound on the thing that we really want, which is the expectation
[1010.30s -> 1015.50s]  under p theta prime, which we've proven to be the same as maximizing the RL objective.
[1016.38s -> 1020.30s]  And the thing that we have to watch out for is that we get this error term that scales us
[1020.30s -> 1026.38s]  two epsilon t times a constant. Everything in that error term is a constant except for
[1026.38s -> 1031.02s]  epsilon, and epsilon is the total variation divergence between your new policy and your old
[1031.02s -> 1037.66s]  policy. So take a moment at this point to think about what kind of RL algorithm we should use
[1038.54s -> 1041.98s]  informed by this derivation. Basically this derivation suggests that a certain
[1041.98s -> 1046.86s]  very tractable objective is a good approximation to the true RL objective under certain
[1046.86s -> 1050.86s]  circumstances, and this implies something about the sort of reinforcement learning
[1050.86s -> 1053.50s]  algorithm that we should be using if we want to get good performance.
[1057.66s -> 1065.50s]  All right, so what we have so far is that maximizing this objective, basically the expected
[1065.50s -> 1070.30s]  value under p theta, the expected value under pi theta of the importance weighted advantage,
[1071.90s -> 1078.46s]  is a good way to maximize the RL objective so long as pi theta prime is close to pi theta
[1078.46s -> 1084.22s]  in terms of total variation divergence. Essentially if you restrict theta prime to not go too far
[1084.22s -> 1089.82s]  from theta so this constraint is satisfied, then maximizing this tractable objective is the
[1089.82s -> 1095.74s]  same as maximizing the true RL objective. How do we maximize this objective? Well,
[1095.74s -> 1099.98s]  we take its derivative with respect to theta prime, and theta prime only appears in the
[1099.98s -> 1103.82s]  importance weight, so when we take its derivative we get exactly the policy gradient.
[1104.94s -> 1111.26s]  So our derivation, what it has shown so far, is that this is a good thing to do if theta
[1111.26s -> 1117.98s]  prime stays close to theta. So for small enough epsilon this is guaranteed to improve
[1117.98s -> 1123.02s]  j theta prime minus j theta, which means that it's guaranteed to improve the RL objective.
[1124.54s -> 1127.82s]  Okay, so the next part of the lecture we'll talk about how to actually do this in practice.
