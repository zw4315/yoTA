# Detected language: en (p=1.00)

[0.00s -> 4.96s]  Hi, welcome to lecture 12 of CS285.
[4.96s -> 6.92s]  Today, we're going to talk about
[6.92s -> 10.56s]  model-based reinforcement learning for learning policies.
[10.56s -> 13.72s]  So at the conclusion of the previous lecture,
[13.72s -> 16.72s]  the final model-based RL algorithm that we concluded on
[16.72s -> 20.20s]  was a model-based RL method with model predictive control.
[20.20s -> 23.88s]  So I dubbed this model-based RL version 1.5.
[23.88s -> 25.48s]  That's, of course, not what it's actually called,
[25.48s -> 27.96s]  but that's just the name that I was using for it.
[28.36s -> 30.40s]  The way it works is you first collect some data
[30.40s -> 32.96s]  using some initial policy like random policy,
[32.96s -> 35.20s]  then you train a dynamics model,
[35.20s -> 37.56s]  then you plan through this model to choose actions
[37.56s -> 39.96s]  using one of the variety of planning methods
[39.96s -> 41.00s]  that we discussed,
[41.00s -> 43.56s]  execute the first action that was planned,
[43.56s -> 46.28s]  then observe the resulting state,
[46.28s -> 48.92s]  append that transition to the buffer,
[48.92s -> 49.84s]  and then plan again.
[49.84s -> 51.00s]  So every single time step,
[51.00s -> 52.12s]  you're replanning through the model,
[52.12s -> 53.52s]  which of course, as we discussed,
[53.52s -> 55.56s]  helps compensate for model errors.
[55.56s -> 57.32s]  And then every so many steps,
[57.32s -> 58.36s]  you would retrain the model
[58.36s -> 60.36s]  with the additional data that you've collected.
[60.36s -> 64.60s]  Oftentimes, this N is some multiple of the horizon,
[64.60s -> 65.88s]  so you would retrain the model
[65.88s -> 67.08s]  after every few trajectories,
[67.08s -> 68.96s]  but you could also do it continuously.
[70.20s -> 72.28s]  So the convenience with methods like this
[72.28s -> 73.80s]  is that they can essentially use
[73.80s -> 76.00s]  many of those optimal control planning
[76.00s -> 77.92s]  or trajectory optimization procedures
[77.92s -> 79.56s]  as a kind of subroutine,
[79.56s -> 81.72s]  but they have some shortcomings.
[81.72s -> 83.64s]  So the biggest shortcoming, of course,
[83.64s -> 84.96s]  is that most of the planning methods
[84.96s -> 86.68s]  that we talked about,
[86.68s -> 89.08s]  especially the simplest ones like random shooting
[89.08s -> 90.32s]  and cross-entropy method,
[90.32s -> 91.84s]  are open loop methods,
[91.84s -> 94.24s]  meaning that they are going to optimize
[94.24s -> 98.12s]  for the expected reward given a sequence of actions.
[100.56s -> 102.32s]  As we discussed before,
[102.32s -> 104.84s]  open loop control is suboptimal.
[104.84s -> 108.96s]  So if you want to think about an example
[108.96s -> 110.68s]  that illustrates why it's suboptimal,
[110.68s -> 112.36s]  the example that I gave last week
[112.36s -> 114.28s]  was this example of the math test.
[114.28s -> 116.40s]  So if I tell you that I'm going to give you a math test,
[117.00s -> 120.08s]  it's very simple and just add two one-digit numbers,
[120.08s -> 122.12s]  and you get a choice.
[122.12s -> 123.00s]  You have two time steps.
[123.00s -> 123.84s]  On the first time step,
[123.84s -> 126.16s]  you can either accept the test or go home.
[126.16s -> 127.60s]  And on the second time step,
[127.60s -> 129.92s]  you have to look at the test and produce an answer.
[129.92s -> 132.24s]  If you produce the right answer, you get $2,000.
[132.24s -> 134.72s]  If you produce the wrong answer, you lose $1,000.
[134.72s -> 137.36s]  And if you go home, then you get nothing.
[137.36s -> 139.16s]  If you have an open loop strategy,
[139.16s -> 140.80s]  that means that on that very first time step,
[140.80s -> 142.28s]  before even seeing the test,
[142.28s -> 143.44s]  you have to commit to an answer.
[143.44s -> 145.60s]  You have to actually commit to both actions.
[145.64s -> 147.56s]  So any optimal planner in this case
[147.56s -> 149.32s]  would actually choose not to take the test
[149.32s -> 151.92s]  because the probability of getting the answer randomly
[151.92s -> 154.40s]  before seeing the exam is very low.
[155.32s -> 157.32s]  Model predictive control doesn't actually solve this
[157.32s -> 159.28s]  because although model predictive control
[159.28s -> 161.60s]  replans every time step,
[161.60s -> 164.72s]  each time step, it is still planning an open loop.
[164.72s -> 166.40s]  So if it ever enters the state
[166.40s -> 168.00s]  where it's presented with a math test,
[168.00s -> 169.56s]  it'll produce the right answer,
[169.56s -> 171.52s]  but it will not choose to enter that state
[171.52s -> 172.56s]  because it doesn't realize
[172.56s -> 174.92s]  that it'll get the opportunity to replan.
[175.40s -> 176.52s]  This is actually very important.
[176.52s -> 179.88s]  This is part of what makes these control algorithms
[179.88s -> 181.32s]  relatively simple,
[181.32s -> 184.24s]  but it's also one of their biggest downsides.
[184.24s -> 185.40s]  So to overcome this,
[185.40s -> 187.88s]  we really need to move towards the closed loop case,
[187.88s -> 190.36s]  where instead of committing to a sequence of actions,
[190.36s -> 191.84s]  we commit to a policy.
[191.84s -> 193.28s]  So in the closed loop case,
[193.28s -> 196.32s]  the agent observes a state and then produces a policy
[196.32s -> 198.08s]  that will be followed thereafter.
[198.08s -> 199.96s]  And the objective for that is much closer
[199.96s -> 202.20s]  to the original reinforcement learning problem.
[202.20s -> 203.16s]  The difference, of course,
[203.16s -> 204.80s]  with the model-based RL setting
[205.56s -> 208.64s]  is that we will model P of ST plus one,
[208.64s -> 210.48s]  given STAT explicitly,
[210.48s -> 212.16s]  which might potentially give us some other ways
[212.16s -> 213.04s]  of addressing this problem.
[213.04s -> 214.68s]  But the objective is exactly the same
[214.68s -> 215.60s]  as in regular RL.
[215.60s -> 219.12s]  It's to produce a policy that will choose actions
[219.12s -> 221.60s]  for whatever state you might find yourself in.
[221.60s -> 223.68s]  And this kind of approach,
[223.68s -> 224.84s]  if we're able to develop
[224.84s -> 227.52s]  an effective closed loop control algorithm
[227.52s -> 229.32s]  would address the math test problem
[229.32s -> 232.12s]  because it would be able to optimize a policy
[232.12s -> 233.52s]  that gives the right answer
[233.56s -> 237.24s]  for every possible observed state of the math test.
[237.24s -> 238.36s]  And therefore it will realize
[238.36s -> 239.88s]  that it can safely take the test
[239.88s -> 240.88s]  and then produce the right answer,
[240.88s -> 242.48s]  even though it doesn't know what the answer is
[242.48s -> 244.92s]  until the test has been observed.
[244.92s -> 246.36s]  Now, of course, in reality,
[246.36s -> 247.36s]  there's a choice to be made,
[247.36s -> 249.00s]  which is the form of pi.
[249.92s -> 253.04s]  So when we're talking about model-free RL before,
[253.04s -> 255.72s]  we generally discussed pi represented
[255.72s -> 258.08s]  by highly expressive function approximators
[258.08s -> 259.28s]  like neural networks.
[259.28s -> 261.24s]  And these give us global policies
[261.24s -> 263.36s]  that will produce decent actions everywhere
[264.08s -> 266.24s]  or at least everywhere that they were trained.
[266.24s -> 269.36s]  When we discussed optimal control methods last week,
[269.36s -> 271.40s]  we also talked about some methods
[271.40s -> 273.44s]  that can give closed loop controllers,
[273.44s -> 275.04s]  but that are only locally good.
[275.04s -> 276.84s]  For example, iterative LQR
[276.84s -> 279.04s]  doesn't just produce a sequence of actions.
[279.04s -> 281.44s]  It will actually produce a linear feedback controller
[281.44s -> 283.16s]  and that linear feedback controller
[283.16s -> 285.84s]  will give you good actions in a local neighborhood
[285.84s -> 288.52s]  around the open loop plan that you've produced.
[288.52s -> 290.32s]  So this is a kind of an in-between
[290.32s -> 291.72s]  where it is technically closed loop,
[291.72s -> 294.52s]  but it's not a globally effective closed loop strategy.
[294.52s -> 295.68s]  So it's only gonna be effective
[295.68s -> 298.28s]  if you deviate a little bit from your initial plan.
[299.60s -> 300.44s]  So in today's lecture,
[300.44s -> 302.00s]  we're really going to focus on methods
[302.00s -> 303.88s]  that we can use to train global policies
[303.88s -> 305.00s]  like neural networks,
[305.00s -> 307.36s]  but leveraging learned models
[307.36s -> 310.80s]  and thereby getting us full policy-based,
[310.80s -> 312.20s]  model-based RL methods.
[314.16s -> 316.72s]  So let's start with a little straw man example.
[316.72s -> 319.48s]  If we didn't know anything about reinforcement learning,
[319.56s -> 323.36s]  we just missed the entire first part of this course.
[323.36s -> 325.44s]  And we tried to set up this problem
[325.44s -> 327.16s]  where we have learned models
[327.16s -> 331.16s]  and we want to learn a policy that maximizes reward.
[331.16s -> 333.20s]  And we just wanted to bring to bear
[333.20s -> 335.20s]  the tools that we know and love from deep learning,
[335.20s -> 337.20s]  essentially the tools of back propagation
[337.20s -> 338.76s]  and continuous optimization.
[338.76s -> 340.72s]  How might we do this?
[340.72s -> 341.92s]  Well, what we might do
[341.92s -> 343.88s]  is we might set up a computation graph, right?
[343.88s -> 345.24s]  We have an optimization problem,
[345.24s -> 347.92s]  which is to maximize the total reward.
[347.92s -> 348.76s]  In deep learning,
[348.76s -> 350.32s]  we know how to solve optimization problems.
[350.32s -> 351.68s]  We implement them
[351.68s -> 355.16s]  with automatic differentiation software, compute gradients,
[355.16s -> 357.92s]  and then do gradient descent or gradient descent.
[357.92s -> 360.32s]  So our objective is the sum of rewards.
[360.32s -> 362.40s]  So let's set up a computation graph
[362.40s -> 364.08s]  that allows us to compute the sum of rewards
[364.08s -> 365.88s]  for a given policy.
[365.88s -> 367.44s]  So in this computation graph,
[367.44s -> 369.36s]  there are basically three types of functions.
[369.36s -> 370.48s]  There are policies,
[370.48s -> 372.68s]  which look at states and produce actions.
[372.68s -> 373.88s]  There are dynamics,
[373.88s -> 375.00s]  which look at states and actions
[375.00s -> 376.32s]  that produce the next state.
[376.32s -> 378.24s]  And there are rewards that look at states and actions
[378.24s -> 379.56s]  that produce a scalar value.
[379.56s -> 382.08s]  So this is basically the computation graph.
[382.08s -> 383.88s]  If you assume that the reward is a known function,
[383.88s -> 387.00s]  maybe something that you can also compute derivatives for,
[387.00s -> 388.84s]  and f is a learned model,
[388.84s -> 389.88s]  so that's a neural net,
[389.88s -> 391.20s]  and pi is a learned model,
[391.20s -> 393.16s]  that's also a neural net.
[393.16s -> 395.96s]  This is a perfectly valid computation graph
[395.96s -> 396.80s]  that you could set up.
[396.80s -> 399.20s]  You could actually implement this in PyTorch.
[399.20s -> 400.52s]  You could implement a loss,
[400.52s -> 402.64s]  which is the negative of the sum of all the rewards,
[402.64s -> 405.16s]  and you can call dot gradients on it.
[405.28s -> 406.44s]  I'm kind of sweeping under the rug,
[406.44s -> 407.80s]  all the details about stochasticity.
[407.80s -> 409.92s]  So this is only for deterministic systems,
[409.92s -> 411.80s]  but even for stochastic systems,
[411.80s -> 415.20s]  for certain kinds of stochastic models,
[415.20s -> 416.04s]  you can still do this.
[416.04s -> 417.28s]  For example, if you have Gaussian models,
[417.28s -> 419.44s]  you can use something called the reparameterization trick,
[419.44s -> 421.80s]  which we'll talk about later in this course
[421.80s -> 423.72s]  to set up computation graphs like this
[423.72s -> 425.56s]  that are continuously differentiable,
[425.56s -> 427.56s]  even if you have Gaussian noise
[427.56s -> 429.48s]  being added to both the policy and the model.
[429.48s -> 430.32s]  But for now,
[430.32s -> 431.36s]  let's just think about the deterministic case.
[431.36s -> 433.40s]  So everything here can be implemented.
[433.40s -> 435.60s]  You can call dot gradients on it,
[435.60s -> 437.72s]  and it will actually give you the gradient
[437.72s -> 439.16s]  of the reinforcement learning objective,
[439.16s -> 440.80s]  at least for the stochastic case.
[442.08s -> 443.76s]  So the question is, will this work?
[443.76s -> 444.96s]  If you actually set this up,
[444.96s -> 446.92s]  will you be able to optimize pi theta?
[450.36s -> 452.76s]  So will you be able to actually run back propagation
[452.76s -> 454.24s]  from each of those reward nodes
[454.24s -> 455.92s]  into all of the thetas,
[455.92s -> 457.92s]  the parameters of those pi theta nodes,
[457.92s -> 459.24s]  and actually optimize your policy
[459.24s -> 461.08s]  to yield a larger and larger reward?
[464.40s -> 467.36s]  In general, the answer is no.
[467.36s -> 469.92s]  So let me just walk through this.
[469.92s -> 472.08s]  I'm gonna call this model-based RL version 2.0.
[472.08s -> 473.64s]  Let me just make it very explicit
[473.64s -> 474.72s]  what this algorithm is,
[474.72s -> 476.68s]  and then I'll explain why, in general,
[476.68s -> 477.92s]  this doesn't really work.
[479.48s -> 480.40s]  So just like before,
[480.40s -> 482.68s]  you would start off by running some base policy
[482.68s -> 483.88s]  to collect the dataset,
[483.88s -> 485.56s]  train a dynamics model on it,
[485.56s -> 487.88s]  and then you would build this computation graph
[487.88s -> 489.88s]  that I've drawn here or something like it,
[489.88s -> 492.48s]  back propagate through your dynamics into the policy
[492.48s -> 494.40s]  to optimize the policy parameters,
[494.40s -> 495.84s]  and then run the policy,
[495.84s -> 497.56s]  appending the visited data to the dataset,
[497.56s -> 499.20s]  and then retrain, okay?
[499.20s -> 501.40s]  So this is a very reasonable way
[501.40s -> 505.04s]  to extend the model-based RL methods we've seen before
[505.04s -> 507.60s]  to now have learned policies instead of planning.
[510.56s -> 512.72s]  So what's the problem with this?
[512.72s -> 515.60s]  Well, I would encourage each of you to pause the video now
[515.60s -> 517.32s]  and think about what the problem might be,
[517.32s -> 519.60s]  why this recipe might go wrong,
[519.60s -> 521.12s]  and as a hint,
[521.16s -> 523.04s]  the reason that this doesn't work
[523.04s -> 525.48s]  is not because it's incorrect necessarily,
[525.48s -> 527.40s]  it doesn't work for the same reason
[527.40s -> 531.32s]  that many other kinds of neural networks
[531.32s -> 533.32s]  that have this kind of temporal structure
[533.32s -> 534.84s]  in other fields of deep learning
[534.84s -> 536.44s]  tend to fail if implemented naively.
[536.44s -> 537.68s]  So that's a little hint,
[537.68s -> 539.16s]  maybe consider pausing the video
[539.16s -> 540.12s]  and thinking a little bit about
[540.12s -> 542.08s]  what might be wrong with this recipe.
[544.60s -> 549.44s]  Okay, so the issue is basically something like this.
[549.48s -> 551.60s]  Just like in trajectory optimization,
[551.60s -> 553.20s]  when we talked about shooting methods,
[553.20s -> 557.12s]  we discussed how the actions earlier on in the trajectory
[557.12s -> 559.88s]  have compounding effects much later in the future,
[559.88s -> 561.12s]  whereas the actions at the end
[561.12s -> 562.16s]  have relatively little effect.
[562.16s -> 563.80s]  So you have really big gradients
[563.80s -> 565.28s]  with respect to the actions at the beginning
[565.28s -> 567.56s]  and therefore big grades respect to the policy parameters
[567.56s -> 569.44s]  compared with smaller gradients at the end.
[569.44s -> 570.68s]  And those of you that are familiar
[570.68s -> 573.72s]  with numerical optimization
[573.72s -> 574.96s]  will probably recognize at this point
[574.96s -> 575.96s]  that if you have a situation
[575.96s -> 577.10s]  where some of the variables
[577.10s -> 578.20s]  get hit with really big gradients,
[578.20s -> 580.72s]  some get hit with really small gradients,
[580.72s -> 581.88s]  you're typically in a situation
[581.88s -> 583.60s]  where something is very ill conditioned.
[583.60s -> 585.56s]  And that's in fact what happens here.
[585.56s -> 589.16s]  So visually you can think of it like this,
[589.16s -> 591.12s]  that a small change to that action at the beginning
[591.12s -> 592.72s]  will wiggle the rest of this trajectory
[592.72s -> 594.04s]  in pretty drastic ways.
[594.88s -> 599.80s]  And the problem is similar to the parameter sensitivity
[600.68s -> 602.52s]  that we get with shooting methods
[602.52s -> 603.88s]  in trajectory optimization,
[604.72s -> 606.92s]  except that it's no longer convenient
[606.92s -> 608.88s]  to use second order methods like LQR, right?
[608.88s -> 612.28s]  So when we learned about trajectory optimization
[612.28s -> 614.16s]  and shooting methods, we talked about this LQR method,
[614.16s -> 615.52s]  which looks a little bit like Newton's method.
[615.52s -> 616.96s]  It actually computes qualities
[616.96s -> 618.28s]  that look like second derivatives
[618.28s -> 620.92s]  and actually provide a much better behaved
[620.92s -> 622.36s]  optimization process.
[622.36s -> 623.32s]  Again, those of you that are familiar
[623.32s -> 624.64s]  with numerical optimization
[624.64s -> 626.56s]  will probably recognize what I'm saying now
[626.56s -> 628.52s]  that well, if you have an ill conditioned problem,
[628.52s -> 629.84s]  meaning something where the Hessian
[629.84s -> 632.00s]  has very high curvature,
[632.00s -> 634.08s]  you would probably want to use a second order method
[634.08s -> 636.84s]  that would improve significantly over a first order method.
[637.72s -> 639.96s]  That's more or less the same thing that's happening here.
[639.96s -> 643.08s]  However, with trajectory optimization and shooting methods,
[643.08s -> 646.24s]  using second order techniques was very convenient
[646.24s -> 649.08s]  because we never had to build a giant Hessian.
[649.08s -> 651.64s]  We could just use this dynamic programming procedure
[651.64s -> 653.84s]  to compute the, what was essentially
[653.84s -> 656.96s]  a Hessian vector product very efficiently using LQR.
[656.96s -> 658.88s]  That option is no longer available to us
[658.88s -> 660.20s]  when we're optimizing policies
[660.20s -> 661.32s]  because the policy parameters
[661.32s -> 662.92s]  couple all the time steps together.
[662.92s -> 665.40s]  So you no longer have this convenient optimization procedure
[665.44s -> 668.04s]  that can start at the end and work its way backwards.
[669.28s -> 672.00s]  It is possible to implement second order optimizers
[672.00s -> 673.56s]  in general for these kinds of problems.
[673.56s -> 676.20s]  And they will work better than first order optimizers,
[676.20s -> 677.28s]  but they no longer benefit
[677.28s -> 679.20s]  from the convenient temporal structure in the problem.
[679.20s -> 682.36s]  And they tend to be fairly difficult to use.
[682.36s -> 684.04s]  They're especially difficult to use with neural nets
[684.04s -> 686.12s]  because second order optimizers for neural nets
[686.12s -> 689.32s]  tend to be kind of flaky and difficult to set up.
[689.32s -> 690.92s]  So it's not impossible,
[690.92s -> 693.48s]  but it doesn't have to work very well.
[693.48s -> 695.28s]  From more of a deep learning perspective,
[696.16s -> 697.00s]  the problems are actually very similar
[697.00s -> 698.36s]  to what you get when you try to train
[698.36s -> 699.88s]  recurrent neural networks naively
[699.88s -> 702.08s]  with back propagation through time.
[702.08s -> 705.64s]  So those of you that are familiar with RNN training
[705.64s -> 707.16s]  will probably recognize a lot of what I'm saying,
[707.16s -> 709.84s]  that if we set up a recurrent neural network naively
[709.84s -> 711.92s]  without using fancy architectures
[711.92s -> 714.16s]  like LSTMs or transformers,
[714.16s -> 715.00s]  then we get these problems
[715.00s -> 716.84s]  with vanishing and exploding gradients.
[716.84s -> 721.84s]  Because the derivatives of the later rewards
[721.92s -> 724.16s]  with respect to the earlier policy parameters
[724.16s -> 726.72s]  involve the product of many, many Jacobians.
[726.72s -> 729.76s]  And unless those Jacobians have eigenvalues
[729.76s -> 731.00s]  that are close to one,
[731.00s -> 732.64s]  then the gradients will either explode or vanish.
[732.64s -> 734.28s]  So if the eigenvalues tend to be larger than one,
[734.28s -> 735.12s]  they'll explode.
[735.12s -> 738.08s]  If they tend to be smaller than one, they'll vanish.
[738.08s -> 739.60s]  Very similar to the problem that you get
[739.60s -> 741.88s]  with training recurrent neural networks.
[741.88s -> 744.12s]  Now, at this point, of course, you might wonder
[744.12s -> 745.92s]  if these are similar to the problems that we get
[745.92s -> 747.52s]  when training recurrent neural networks,
[747.52s -> 749.80s]  can we use similar solutions?
[749.84s -> 753.88s]  For example, could we use something like an LSTM model?
[754.88s -> 758.04s]  The problem here is that in an RNN,
[758.04s -> 760.52s]  you essentially get to choose the dynamics.
[760.52s -> 761.80s]  Whereas here, we don't get to choose
[761.80s -> 763.40s]  the dynamics ourselves.
[763.40s -> 766.28s]  So we have to really train F to be as close as possible
[766.28s -> 767.58s]  to the real dynamics.
[767.58s -> 768.76s]  Otherwise we won't get policies
[768.76s -> 771.20s]  that will actually end up working in the real system.
[771.20s -> 774.36s]  So with an LSTM, we could choose a form
[774.36s -> 777.36s]  for those functions that participate
[777.36s -> 778.84s]  in the back propagation through time.
[778.84s -> 779.68s]  So that for example,
[779.68s -> 781.56s]  their derivatives have eigenvalues close to one,
[781.56s -> 784.04s]  maybe their derivatives are dominated by an identity matrix.
[784.04s -> 786.20s]  That's more or less what an LSTM provides you with,
[786.20s -> 787.10s]  but that's not really an option
[787.10s -> 788.44s]  when you're doing model-based RL
[788.44s -> 791.72s]  because you don't get to use any dynamics you want.
[791.72s -> 793.88s]  You really need F to match the real dynamics,
[793.88s -> 795.44s]  which might have high curvature
[795.44s -> 799.24s]  and it might have hessians with eigenvalues
[799.24s -> 800.88s]  that are very far from one.
[800.88s -> 802.16s]  So since you don't get to choose F,
[802.16s -> 804.44s]  you can't really use most of the solutions
[804.44s -> 806.48s]  that have been developed in the deep learning world
[806.48s -> 807.78s]  because most of those solutions have to do
[807.86s -> 809.32s]  with essentially changing the function
[809.32s -> 811.58s]  to make it an easier function to differentiate.
[812.78s -> 815.22s]  So we're actually in a bit of trouble here.
[815.22s -> 818.02s]  While it's certainly plausible
[818.02s -> 821.14s]  that future innovations for model-based RL
[821.14s -> 822.90s]  will make it easier to push gradients
[822.90s -> 824.70s]  through long temporal chains like this,
[824.70s -> 828.22s]  at the moment, differentiating through long chains
[828.22s -> 831.22s]  consisting of learned dynamics and learned policies
[831.22s -> 832.58s]  tends to behave very poorly.
[832.58s -> 834.94s]  And for this reason, gradient-based optimization
[834.94s -> 837.08s]  with model-based RL for policies
[837.24s -> 839.04s]  is kind of a very tricky proposition.
[840.46s -> 842.08s]  So what's the solution?
[842.08s -> 843.80s]  Well, most of what we're going to discuss
[843.80s -> 846.12s]  in today's lecture actually has to do,
[846.12s -> 847.44s]  perhaps surprisingly,
[847.44s -> 849.64s]  with using derivative-free RL algorithms
[849.64s -> 851.70s]  with models that are going to be used
[851.70s -> 853.20s]  to generate synthetic samples.
[854.48s -> 855.96s]  This might seem weirdly backwards.
[855.96s -> 857.36s]  Like essentially what this means is
[857.36s -> 859.12s]  if you're going to learn a model,
[859.12s -> 860.48s]  don't actually make use of the fact
[860.48s -> 862.52s]  that the model is known or has known derivatives.
[862.52s -> 863.54s]  Instead, pretend that the model
[863.54s -> 865.48s]  is essentially a simulator of the environment
[865.48s -> 867.28s]  and use it to simulate a lot more data
[867.28s -> 868.68s]  to speed up model-free RL.
[869.68s -> 870.50s]  It might seem strange
[870.50s -> 871.86s]  that that's kind of the best that we've got,
[871.86s -> 874.80s]  but it actually is often the best that we can do,
[874.80s -> 877.52s]  and it actually tends to work very well.
[877.52s -> 879.52s]  Now, in reality, we can, of course, hybridize
[879.52s -> 880.96s]  to some degree between model-free training
[880.96s -> 882.30s]  and some amount of planning,
[882.30s -> 884.80s]  but just using models to simulate experience
[884.80s -> 886.92s]  for model-free RL can be very effective.
[886.92s -> 888.92s]  It's essentially a kind of model-based acceleration
[888.92s -> 889.80s]  for model-free RL.
[889.80s -> 893.52s]  So the underlying RL methods will be virtually identical
[893.52s -> 894.80s]  to the ones that we talked about before.
[894.96s -> 896.08s]  There'll be policy gradient methods,
[896.08s -> 899.00s]  actor-critic methods, Q-learning methods, and so on,
[899.00s -> 900.70s]  but they'll just get additional data
[900.70s -> 902.24s]  generated by a learned model.
[903.16s -> 905.24s]  So that's what we'll talk about in the next section.
