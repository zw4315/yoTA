# Detected language: en (p=1.00)

[0.00s -> 5.16s]  All right, in the next portion of the lecture, I'll go through a few other novelty seeking
[5.16s -> 9.32s]  exploration methods. So for these, I won't go through them in quite as much detail,
[9.32s -> 13.56s]  but I just want to give you a sense for other techniques that have been put forward
[13.56s -> 19.92s]  in the literature that also exploit the notion of optimism to improve exploration.
[19.92s -> 24.28s]  So the first one I'm going to talk about, you can think of it as a kind of a count-based
[24.28s -> 29.68s]  method with a more sophisticated density model. So this is from this paper by Tang et al. called
[29.68s -> 38.20s]  hash exploration, and the idea here is counting with hashes. So here's the notion. Instead of
[38.20s -> 42.80s]  doing the pseudo-count thing, what if you still do regular counts, but under a different
[42.80s -> 48.16s]  representation? So perhaps what you could do is you could take your states and you compute a
[48.16s -> 55.84s]  kind of a hash that compresses the state, so it's a lossy hash, in such a way that states
[55.84s -> 61.12s]  that are very different get very different hashes, but states that are very similar might be, might
[61.12s -> 67.92s]  map to the same hash. So the idea is that we're going to compress s into a k-bit code via
[67.92s -> 74.52s]  some encoder phi of s, and if k is chosen to be small enough such that the number of states
[74.52s -> 78.84s]  is larger than 2 to the k, then we'll have to compress some states into the same code. And
[78.84s -> 83.76s]  then we'll do counting, but we'll count with respect to these codes. We'll actually count how
[83.76s -> 90.26s]  many times we've seen the same code instead of the same state. And the shorter your code is,
[90.26s -> 96.28s]  the more hash collisions you get, which means the broader your notion of similar,
[96.28s -> 102.72s]  for the purpose of determining if two states are similar, will be. So will similar states get
[102.76s -> 108.76s]  the same hash? Well, maybe. It depends a little bit on the model you choose. So the way they can
[108.76s -> 112.76s]  improve the odds is instead of using some standard hash function that typically aims
[112.76s -> 119.76s]  to minimize hash collisions, you could instead use an autoencoder that is trained so that it
[119.76s -> 124.60s]  gets the maximum reconstruction accuracy. And if you train the autoencoder to maximize
[124.60s -> 131.58s]  reconstruction accuracy, then if it's forced to have hash collisions, it'll produce hash
[131.62s -> 137.58s]  collisions for those settings where that collision results in small reconstruction error. So basically,
[137.58s -> 141.34s]  if it mistakes one state for another, but they still look pretty similar, then that mistake
[141.34s -> 146.34s]  costs the autoencoder a lot less than if the states look very different. So learning the hash
[146.34s -> 152.26s]  basically provides hash collisions that are a little more similarity driven. And then this
[152.26s -> 156.34s]  algorithm will take the bottleneck from the autoencoder, essentially treating the encoder of
[156.58s -> 164.30s]  the autoencoder as phi of s, clamp it to be 0, 1, perform a downsampling step, and that's the code,
[164.30s -> 168.30s]  the k-bit code that they're going to use. And then they just do regular counting on these
[168.30s -> 175.54s]  k-bit codes. And the resulting algorithm actually turns out to work decently well with a variety
[175.54s -> 180.02s]  of different coding schemes. So that's kind of a nice way that you could adapt regular
[180.02s -> 186.98s]  counts if you don't want to deal with pseudo counts. Another thing you could do is you could
[186.98s -> 192.54s]  avoid density modeling altogether by actually exploiting classifiers to give you density scores.
[192.54s -> 198.90s]  So remember that P theta of s needs to be able to output densities, but it doesn't necessarily
[198.90s -> 205.10s]  need to produce great samples. And we can exploit this by devising a class of models that are
[205.10s -> 210.38s]  particularly easy to train, that can't produce samples at all, but can give reasonable densities.
[210.38s -> 216.90s]  So this is from a paper called EX2 by Hu et al. So here's the idea. We're going to try to
[216.90s -> 223.58s]  explicitly compare the new state to past states, and the intuition is that if a classifier can
[223.58s -> 229.18s]  easily distinguish whether the state it's looking at is the new state or a past state, then the
[229.18s -> 234.02s]  new state is very novel and therefore should have low density. If it's very hard to distinguish,
[234.62s -> 239.42s]  that means that the new state looks indistinguishable from past states and therefore has high density.
[239.42s -> 244.90s]  And while this notion is somewhat intuitive and informal, it can actually be made
[244.90s -> 252.06s]  mathematically precise. So if the state is novel, the state is novel if it is easy to
[252.06s -> 258.82s]  distinguish from all previously seen states by a classifier. So for each observed state s,
[258.82s -> 265.18s]  what we're going to do is we will fit a separate classifier to classify that state against all past
[265.18s -> 270.06s]  states in the buffer, and then we'll use the classifier likelihood or the classifier error
[270.06s -> 279.94s]  to obtain a density. So it turns out that if the probability that your classifier assigns
[279.94s -> 285.18s]  to the state is given by d of s, and I have the subscript s because this is a classifier
[285.18s -> 291.06s]  that's trying to classify the state s against all past states. So d subscript s of s is the
[291.06s -> 300.18s]  probability this classifier assigns to the state being a new state. The density of the state
[300.18s -> 306.10s]  turns out can be written as p theta of s is equal to 1 minus ds of s divided by ds of s.
[306.10s -> 311.14s]  And the way that you obtain this equation is you write down the formula for the optimal
[311.14s -> 315.38s]  classifier, which can be expressed in terms of the density ratio, and then do a bit of algebra.
[315.38s -> 323.54s]  So this is the probability that the classifier assigns that s is a positive, meaning that s is
[323.54s -> 328.82s]  a new state. And the classifiers train where the only positive is s and the negatives are
[328.82s -> 333.10s]  all the ds. Now at this point you might be wondering what the heck is going on here,
[333.10s -> 337.06s]  like you have a classifier that just tries to classify whether s is equal to itself,
[337.30s -> 342.58s]  like shouldn't that always output true? Well, remember what counts are doing. What counts
[342.58s -> 346.94s]  are doing is they're counting how many times you've seen that exact same state multiple times.
[346.94s -> 354.82s]  So if you're actually in that regime of counts, and s has a large count, then s will also occur
[354.82s -> 359.70s]  in d. So you'll have one copy of s in the positives, but you might have multiple copies of
[359.70s -> 366.70s]  s in the negatives, which means that the true answer, the true ds of s, is not 1, because if you
[366.74s -> 372.30s]  see the state s, it could be a positive, but it could also be a negative. For example,
[372.30s -> 379.94s]  if the state s occurs in the set of negatives 50% of the time, if literally half your negative
[379.94s -> 389.70s]  states are also s, then ds of s is not 100%, it's actually 75%, because 50% that's positive,
[389.70s -> 397.38s]  25% that is the other half of the negatives. So the larger the count, the lower ds of s will be.
[397.38s -> 404.86s]  And of course, in larger continuous spaces, where the counts are always 1, this model will
[404.86s -> 409.94s]  still produce non-trivial conclusions, because the classifier is not going to overfit. The
[409.94s -> 413.42s]  classifier is actually going to generalize a little bit, which means that if it sees
[413.42s -> 418.74s]  very similar states in the negatives, it will assign a lower probability to the positive. So
[418.78s -> 423.58s]  that's why you can use a classifier to estimate densities like this. If you want to go through
[423.58s -> 428.62s]  the algebra for how to derive the probability from the classifier, check out the paper. It's
[428.62s -> 431.66s]  actually a fairly simple bit of algebra. The intuition is that you first write down
[431.66s -> 436.22s]  the equation for a Bayes optimal classifier, which is an expression in terms of p theta
[436.22s -> 440.58s]  of s, and then you solve that expression to find an equation for p theta of s.
[440.58s -> 448.90s]  Now, as I mentioned before, aren't we just checking if s is equal to s? Well,
[448.90s -> 454.90s]  if there are copies of s present in the data set, then the optimal ds of s is not 1, as I
[454.90s -> 462.50s]  mentioned before. And in fact, the optimal classifier is given by 1 over 1 plus p of s.
[462.50s -> 467.14s]  And again, this is a bit of algebra that you can check. So if you rearrange this to solve
[467.22s -> 474.70s]  for p of s, you get the equation on the right. Now in reality, of course, each state is unique,
[474.70s -> 480.18s]  and your classifier can overfit, so you have to regularize the classifier to ensure that it
[480.18s -> 484.38s]  doesn't overfit and doesn't just assign a probability of 1 all the time. So you would
[484.38s -> 490.42s]  use something like weight decay to regularize your classifier. Now the other problem with
[490.42s -> 495.58s]  this is that you're, you know, as I've described this procedure so far, we're training a totally
[495.58s -> 501.14s]  separate classifier for every single state we see. Now isn't that a bit much? Are we going to go
[501.14s -> 506.42s]  kind of crazy with all those classifiers? Well, one solution we could have is we could
[506.42s -> 511.90s]  instead train an amortized model. So instead of training one classifier for every single
[511.90s -> 516.66s]  state, we can train just a single classifier that is conditioned on the state that it's
[516.66s -> 524.58s]  classifying. So it's an amortized model that takes the exemplar as input, that's x star,
[524.62s -> 530.38s]  and it takes a state that's classifying as input, that's x. And now we just train one network,
[530.38s -> 536.88s]  and we update it with every state that we see. So this is an amortized model. And this basic
[536.88s -> 543.30s]  scheme actually works pretty well. It compares very favorably to some other exploration methods,
[543.30s -> 547.82s]  including the hash-based exploration method I described before, provides maybe an interesting
[547.82s -> 552.74s]  perspective on how the type of density model we use for exploration doesn't necessarily need to
[552.74s -> 560.38s]  be able to produce samples, and it could even be obtained from a classifier. And then in the paper
[560.38s -> 564.78s]  there's some experiments with using this for some visual navigation tasks in VisDoom, where
[564.78s -> 568.86s]  you have to traverse many different rooms before you find the treasure, and a good exploration algorithm
[568.86s -> 572.62s]  should figure out when it's in a novel room, and then seek out more novel rooms that
[572.62s -> 580.86s]  hasn't seen too much. All right. Now there are also more heuristic methods that we could
[580.86s -> 585.50s]  use to estimate quantities that are not really counts, but that kind of serve a similar role
[585.50s -> 590.30s]  as counts in practice and can work pretty well. So remember that P theta of s needs to be able
[590.30s -> 595.62s]  to output densities, but it doesn't necessarily need to produce great samples. In fact, it doesn't
[595.62s -> 600.50s]  even necessarily need to produce great densities. You could just think of it as a score, and you
[600.50s -> 607.90s]  just want that score to be larger for novel states and smaller for non- novel states, or the
[607.90s -> 611.86s]  other way around. So basically you just need some number that is very predictive of whether a state
[611.86s -> 618.14s]  is novel or not. It doesn't even have to be a proper density. So you just need to be able to
[618.14s -> 621.82s]  tell if a state is novel or not, and if that's all you want, there are other ways to get this
[621.82s -> 627.14s]  that are a little more heuristic but can work well. So for example, let's say that we have
[627.14s -> 632.46s]  some target function f star of s comma a. Don't worry about what this function is for now,
[632.46s -> 637.70s]  let's just say it's some scalar value function on states and actions. So maybe it's this function.
[638.70s -> 646.06s]  And we take our buffer of states and actions that we've seen, and we fit an estimate to f star.
[646.06s -> 652.66s]  So we fit some function f hat theta. So f hat theta is trying to match f star on the data.
[652.66s -> 659.22s]  So maybe our data set contains these points. f hat theta might look like this. So it's going to
[659.22s -> 663.18s]  be similar to those points close to the data, but far from the data it's going to make
[663.18s -> 670.18s]  mistakes because it hasn't been trained in those regions. So now we can use the error between f
[670.18s -> 676.62s]  hat and f star as our bonus, because we expect this error to be large when we're very far away
[676.62s -> 682.06s]  from states and actions that we've seen. So close to the data, the two functions should match.
[682.06s -> 689.42s]  Far from the data, f hat theta might make really big mistakes. So then we would say the
[689.42s -> 693.90s]  novelty is low when the error is low, and the novelty is high when the error is high.
[693.90s -> 703.98s]  So then we could ask, well, what kind of function should we use for f star? And there
[703.98s -> 707.94s]  are a number of different choices that have been explored in the literature. So one common
[707.94s -> 716.10s]  choice is to set f star to be the dynamics. So basically f star of sA is s prime. That's very
[716.14s -> 719.98s]  convenient because it's a quantity that clearly has something to do with the dynamics of the MDP,
[719.98s -> 725.74s]  and of course you've observed s prime in your data. So you could essentially train a model and
[725.74s -> 731.14s]  then measure the error of that model as a notion of novelty. This is also related to
[731.14s -> 737.46s]  information gain, which we'll discuss in the next part of the lecture. An even simpler way
[737.46s -> 744.38s]  to do this is to just set f star to be a neural network with parameters phi, where phi is chosen
[744.38s -> 749.98s]  randomly. So this network is not actually trained, it's actually just initialized randomly to obtain
[749.98s -> 758.02s]  an arbitrary but structured function. The point here is that you don't actually need f star to
[758.02s -> 762.62s]  be all that meaningful. You just need it to be something that can serve as a target that varies
[762.62s -> 767.34s]  over the state and action space in ways that are not trivial to model. So that's why just
[767.34s -> 773.14s]  using a random network actually can work pretty well. And this is actually part of the
[773.14s -> 777.94s]  material that will be on homework 5, so it's a good idea to kind of understand why this works.
