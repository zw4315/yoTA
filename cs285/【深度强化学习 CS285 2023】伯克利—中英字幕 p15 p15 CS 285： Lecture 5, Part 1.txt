# Detected language: en (p=1.00)

[0.00s -> 6.34s]  Today we're going to cover our first reinforcement learning algorithm, which
[6.34s -> 11.04s]  is called policy gradient. Now, policy gradients are in some ways kind of the
[11.04s -> 15.08s]  simplest reinforcement learning algorithm in that they directly attempt
[15.08s -> 18.64s]  to differentiate the reinforcement learning objective and then perform
[18.64s -> 22.28s]  gradient descent on the policy parameters to make the policy better.
[22.28s -> 27.20s]  So to start with, let's recap the objective function for reinforcement
[27.20s -> 32.24s]  learning from last time. In reinforcement learning, we have a policy which we're
[32.24s -> 36.56s]  going to call pi. That policy has parameters theta, and the policy defines
[36.56s -> 41.24s]  a distribution over actions a conditioned on either the state's s or the
[41.24s -> 45.16s]  observations o. And I'll come back to the partially observed case later in
[45.16s -> 48.72s]  the lecture, but for now we'll just work on policies that are conditioned on
[48.72s -> 53.88s]  states. If the policy is represented, for example, by a deep neural network,
[53.92s -> 58.60s]  then theta denotes the parameters of the policy, which are the weights in the
[58.60s -> 63.08s]  neural network. This network takes as input the state or observation and
[63.08s -> 69.88s]  produces as output the action. Together, the next state is determined by the
[69.88s -> 74.04s]  transition probabilities, which depend on the current state and the action
[74.04s -> 78.76s]  produced by the policy. And of course then, the next state, sampled
[78.76s -> 82.72s]  according to the transition probabilities, is fed into the policy again to
[82.72s -> 88.12s]  determine the next action, and so on and so on. This process can be used, as we
[88.12s -> 92.24s]  saw last time, to define a trajectory distribution. The trajectory
[92.24s -> 96.44s]  distribution is a probability distribution over a sequence of states
[96.44s -> 102.88s]  and actions. So it's a distribution over s1, a1, s2, a2, s3,
[102.88s -> 109.24s]  a3, s4, etc., etc., etc. And I'm going to use the
[109.24s -> 112.80s]  subscript theta when I write a trajectory distribution to emphasize that the
[112.80s -> 117.08s]  trajectory distribution depends on the policy parameters theta. We can write
[117.08s -> 120.88s]  it via the chain rule of probability as the product of the initial state
[120.88s -> 124.88s]  distribution p of s1, and then a product over all time steps of the
[124.88s -> 130.36s]  policy probability pi theta a t given s t times the transition probability p of
[130.36s -> 137.12s]  s t plus 1 given s t a t. And I will use tau as a notational shorthand.
[137.28s -> 144.04s]  Whenever you see me write tau, that just means s1, a1, s2, a2, s3,
[144.04s -> 151.60s]  etc., etc., etc., all the way out to s capital T, a capital T. Now crucially,
[151.60s -> 155.44s]  when we develop model-free reinforcement learning algorithms of the sort that
[155.44s -> 159.60s]  we'll cover in today's lecture and the subsequent few lectures, we typically
[159.60s -> 164.64s]  do not assume that we know the transition probabilities p of s t plus 1
[164.64s -> 171.20s]  given s t a t, nor the initial state probability p of s1. We just assume that
[171.20s -> 174.52s]  we can interact with the real world, which effectively samples from those
[174.52s -> 181.16s]  distributions. As we saw in the last lecture, the objective of reinforcement
[181.16s -> 185.92s]  learning can be written out as an expectation under this trajectory
[185.92s -> 190.88s]  distribution. So we have our reward function r of s t comma a t, and we
[190.88s -> 194.56s]  would like to take the expected value of the sum of the reward under the
[194.56s -> 199.00s]  trajectory where the trajectories are distributed according to p theta of tau.
[199.00s -> 203.32s]  And then we would like to find the parameters theta that maximize this
[203.32s -> 211.36s]  expectation. Now as we saw in the last lecture, we can push the sum out of
[211.36s -> 215.16s]  the expectation by linearity of expectation and then express the
[215.16s -> 219.36s]  expectation as an expectation over a marginal. And this allows us to define
[219.36s -> 223.88s]  both a finite horizon version of the RL objective and an infinite horizon
[223.88s -> 228.40s]  version. In today's lecture, we will focus on the finite horizon version,
[228.40s -> 232.00s]  although it's quite possible to extend policy gradients to the infinite
[232.00s -> 235.56s]  horizon setting by using value functions, which we will discuss next
[235.56s -> 239.88s]  time. So for now, we'll stick to the finite horizon version where the sum is
[239.88s -> 245.44s]  inside of the expectation, but we'll come back to the other version later on.
[245.44s -> 249.80s]  Okay, before we talk about how we optimize the reinforcement learning
[249.84s -> 255.56s]  objective, let's first talk about how we can evaluate it. So if we have a policy
[255.56s -> 261.40s]  with parameters theta, can we figure out approximately what is the value of
[261.40s -> 266.08s]  the reinforcement learning objective? And I'm going to use J of theta as
[266.08s -> 270.88s]  notational shorthand for the expected value under p theta of tau of the sum
[270.88s -> 274.24s]  of the rewards. So if you see me write J theta, I'm just referring to this whole
[274.24s -> 279.68s]  expectation. So if we don't know p of s1 and we don't know p of s t plus 1
[279.68s -> 287.12s]  given s t, how can we estimate J of theta? So take a moment to think about
[287.12s -> 294.28s]  this. Since we assume that we can run our policy in the real world, which
[294.28s -> 298.00s]  amounts to sampling from the initial state distribution and the transition
[298.00s -> 303.20s]  probabilities, we can evaluate J of theta approximately by simply making
[303.20s -> 310.20s]  rollouts from our policy. We run our policy in the real world n times to
[310.20s -> 316.28s]  collect n sampled trajectories, and if you see me write tau subscript i, that
[316.28s -> 321.00s]  refers to the ith sample, if you see me write s subscript i comma t, that
[321.00s -> 326.72s]  refers to time step t in the ith sample. Having generated these samples
[326.72s -> 331.28s]  from p theta of tau, we can get an unbiased estimate for the expected
[331.28s -> 335.04s]  value of the total reward simply by summing up the rewards along each sample
[335.04s -> 339.24s]  trajectory and then averaging the rewards over the sample trajectory as
[339.24s -> 344.56s]  per this equation. And the more samples we generate, the larger n is, the
[344.56s -> 349.84s]  more accurate will be our estimate of this expected value. So visually you can
[349.84s -> 353.04s]  think of it like this. We'll generate some number of trajectories, in this case
[353.04s -> 357.32s]  three. For each trajectory, we'll sum up their rewards to see which ones are
[357.32s -> 360.64s]  good and which ones are bad, and then we'll average them together, and this will
[360.64s -> 368.96s]  give us an estimate of j of theta. Now, of course, in reality we don't just want
[368.96s -> 372.76s]  to estimate the objective, we actually want to improve it. So to improve the
[372.76s -> 376.52s]  objective, we need to come up with a way to estimate its derivative, and
[376.52s -> 381.60s]  crucially, the estimate of the derivative itself needs to be feasible
[381.60s -> 386.08s]  without knowing the initial state probability nor the transition
[386.08s -> 393.24s]  probability. So again, for notational convenience, I'm going to use p theta of
[393.24s -> 397.04s]  tau to denote the structure distribution, and I'll actually use r of
[397.04s -> 401.80s]  tau as shorthand for the sum of the rewards over all the time steps in the
[401.80s -> 405.60s]  trajectory tau. This will make the notation in the derivation that follows
[405.60s -> 414.20s]  a little bit easier to parse. Now, if I have an expected value, I can expand
[414.24s -> 418.64s]  that expected value as a sum for discrete variables or an integral for
[418.64s -> 422.84s]  continuous variables of the product between the probability and the value.
[422.84s -> 427.04s]  So the expected value of r of tau under p theta of tau is equal to the
[427.04s -> 436.00s]  integral over all trajectories of p theta of tau times r of tau. And now
[436.00s -> 440.02s]  we can start working on our derivative. So our goal is to compute the
[440.02s -> 446.66s]  derivative or gradient of J of theta with respect to theta. And since the
[446.66s -> 450.38s]  differentiation operator is linear, we can push it inside the
[450.38s -> 454.70s]  integral. So this derivative is equal to the integral over all trajectories of
[454.70s -> 461.50s]  grad theta p theta tau times r of tau. And I'll often say in this lecture
[461.50s -> 468.18s]  just p of tau. Usually when I say p of tau, I just mean p theta of tau. Okay, so
[468.18s -> 473.30s]  now so far this doesn't actually give us a practical way to evaluate the policy
[473.30s -> 478.86s]  gradient because grad theta p of tau requires differentiating through the
[478.86s -> 483.82s]  unknown initial state distribution and the unknown transition probabilities. But
[483.82s -> 486.74s]  there's a very useful identity that will allow us to rewrite this
[486.74s -> 491.46s]  equation in a way that we can evaluate using only samples, much like
[491.46s -> 496.10s]  how we evaluated the objective value. So the convenient identity that we will
[496.14s -> 500.18s]  use, and this is basically the only piece of mathematical cleverness in this
[500.18s -> 504.94s]  whole derivation, is that if we have an equation like this, if we have p of tau
[504.94s -> 511.98s]  times grad log p of tau, we can write it as p of tau times grad p of
[511.98s -> 520.26s]  tau over p of tau. This follows directly from simply the equation for the
[520.26s -> 524.00s]  derivative of a logarithm. So if you open a calculus textbook and look up the
[524.00s -> 530.72s]  derivative for, you know, d dx of log x, you'll find that it's basically
[530.72s -> 538.16s]  equal to dx over x, right? So that means that grad log p is grad p over p.
[538.16s -> 542.44s]  But now you'll see that we have a p in the denominator and we have a p in
[542.44s -> 546.60s]  the numerator, so these cancel out, which means that this is equal to grad
[546.60s -> 551.08s]  p. And what we're going to do is we're going to apply this identity in
[551.08s -> 555.64s]  reverse. So we have a grad p here and we'll substitute the left-hand side of
[555.64s -> 564.00s]  this identity to rewrite it as p times grad log p. And now you'll notice that we
[564.00s -> 568.08s]  have an integral over all trajectories of p of tau times some
[568.08s -> 572.16s]  quantity, which means that we can also write it as an expectation. We can write
[572.16s -> 579.88s]  it as an expected value under p of tau of grad log p tau times r of tau. And this
[579.88s -> 582.96s]  suggests that we might be on the right track because when we have an
[582.96s -> 588.00s]  expectation, we can evaluate those expectations using samples. We're not done
[588.00s -> 592.28s]  yet because we still have this grad log p tau term, so let's work on that a
[592.28s -> 597.92s]  little bit. Let's bring up again our equation for the trajectory
[597.92s -> 602.48s]  distribution. So p of tau, which is just another way of writing p of s1,
[602.48s -> 608.36s]  a1, s2, etc., is equal to this product that we saw before. If we
[608.40s -> 613.40s]  take the logarithm of both sides, the logarithm of a product is the sum of
[613.40s -> 618.28s]  logarithms, which means that we can write log p of tau as the sum log p of
[618.28s -> 623.48s]  s1 plus a summation from t equals 1 to capital T of the log probabilities
[623.48s -> 628.28s]  under the policy plus the log transition probabilities. And now we'll
[628.28s -> 634.92s]  substitute this whole thing in for grad log p. And we're
[634.92s -> 638.40s]  taking the derivative of this with respect to theta. Now the derivative with
[638.40s -> 643.64s]  respect to theta of log p of s1 is just 0 because p of s1 does not depend
[643.64s -> 648.84s]  on theta. And the derivative with respect to theta of log p of s t plus 1
[648.84s -> 654.24s]  given s t a t is also 0 because the transition probabilities also do not
[654.24s -> 659.24s]  depend on theta. So that means that after this simplification, the only
[659.24s -> 664.56s]  terms that are left are the log pi theta a t given s t terms, which are
[664.60s -> 668.12s]  actually the only terms that we can evaluate because we know the form of
[668.12s -> 672.92s]  the policy and we can evaluate the policy's own log probabilities. So
[672.92s -> 677.64s]  collecting all the terms that remain and expanding out our notation, we're
[677.64s -> 681.72s]  left with this equation for the policy gradient. The gradient with
[681.72s -> 686.08s]  respect to theta of j of theta is equal to the expectation under p theta
[686.08s -> 693.04s]  of tau of the sum from t equals 1 to capital T of grad theta log pi
[693.08s -> 699.60s]  theta a t given s t times the sum of the rewards. And now everything
[699.60s -> 704.32s]  inside this expectation is known because we have access to the policy
[704.32s -> 708.36s]  pi and we can evaluate the reward for all of our samples. All of the
[708.36s -> 711.40s]  unknown terms, the initial state distribution and the transition
[711.40s -> 714.40s]  probabilities, occur only in the distribution under which the
[714.40s -> 718.00s]  expectation is taken. So that means that if we want to evaluate the
[718.00s -> 721.68s]  policy gradient, we can use the same trick that we used to evaluate
[721.68s -> 725.52s]  the objective value. We can simply run our policy, which will generate
[725.52s -> 731.32s]  samples from p theta of tau, sum up their rewards to determine which
[731.32s -> 736.08s]  trajectory is good or bad, and then multiply those by the sum of
[736.08s -> 741.32s]  grad log pis. And then once we've estimated the gradient in this
[741.32s -> 745.04s]  way, we can improve our policy simply by taking a step of gradient
[745.04s -> 748.72s]  descent, taking the old policy parameters and adding to them the
[748.72s -> 754.84s]  policy gradient, multiplied by a learning rate alpha. If we think
[754.84s -> 757.64s]  back to the anatomy of a reinforcement learning algorithm that we
[757.64s -> 762.32s]  covered before, the orange box here corresponds to the process
[762.32s -> 764.92s]  of generating those samples, which are the ones that we're
[764.92s -> 769.32s]  summing over. The green box refers to summing up the rewards
[769.32s -> 773.00s]  along each sample trajectory. Then we can calculate the policy
[773.00s -> 776.44s]  gradient, and the blue box corresponds to taking one step of
[776.44s -> 782.04s]  gradient descent. Now, this procedure gives us the basic policy
[782.04s -> 786.20s]  gradient algorithm, also known as the REINFORCE algorithm.
[786.20s -> 792.12s]  REINFORCE is the acronym that was given by Williams in the
[792.12s -> 795.64s]  1990s to the first policy gradient method, which consists of
[795.64s -> 800.76s]  three steps. Sample trajectories according to pi theta a given s
[800.76s -> 804.52s]  by running the policy in the real world n times, evaluate the
[804.52s -> 808.68s]  policy gradient as per this equation, and then take a step of
[808.68s -> 814.68s]  gradient descent. So that's the basic policy gradient algorithm.
[814.68s -> 817.16s]  What I've covered so far in this lecture basically gives you
[817.16s -> 820.28s]  all the mathematical tools that you need to understand the
[820.28s -> 822.84s]  basics of policy gradients, but if you try to actually
[822.84s -> 826.12s]  implement the policy gradient, as I've described so far, it
[826.12s -> 828.60s]  probably won't work very well. So in the remainder of the
[828.60s -> 831.48s]  lecture, we'll discuss some of the intuition behind what
[831.48s -> 833.96s]  policy gradients are doing, and then discuss how to actually
[833.96s -> 836.52s]  implement them so that they work well in practice, which
[836.52s -> 839.80s]  you will need to do homework too.
