# Detected language: en (p=1.00)

[0.00s -> 5.40s]  Alright, in the next portion of today's lecture, we're going to talk about the role of uncertainty
[5.40s -> 8.04s]  in model-based reinforcement learning.
[8.04s -> 11.96s]  Uncertainty plays a really important role in model-based RL, because even though the
[11.96s -> 17.48s]  model-based RL version 1.0 algorithm, in principle, can solve the model-based RL
[17.48s -> 20.66s]  problem, in practice it has some pretty major issues.
[20.66s -> 24.60s]  So let's try to understand this with an example.
[24.60s -> 31.00s]  Here is an experiment that we actually did here at Berkeley a couple years back, studying
[31.00s -> 36.50s]  deep model-based reinforcement learning methods, essentially methods like model-based RL version
[36.50s -> 41.36s]  1.5, which I presented before, using a deep neural net model.
[41.36s -> 46.68s]  So these experiments were conducted on the half-chita simulation task, which you guys
[46.68s -> 48.88s]  tried in homework one.
[48.88s -> 56.44s]  And what this first little orange part of the curve shows was what we got from running
[56.44s -> 62.24s]  just the basic model-based RL version 1.5 algorithm, starting from scratch.
[62.24s -> 65.88s]  So this is the algorithm that replans every step, and iteratively collects additional
[65.88s -> 67.56s]  data.
[67.56s -> 72.28s]  And then what we did is we used that to bootstrap a model-free RL learner, and ran
[72.28s -> 77.72s]  it for a lot longer, obviously, and the red part of the curve shows what the model-free
[77.72s -> 79.24s]  learner then did.
[79.24s -> 81.48s]  So a couple of things jump out at us here.
[81.48s -> 85.64s]  First, the model-free learner gets to much better final performance.
[85.64s -> 90.96s]  Second, perhaps more optimistically, the model-based learner can get to some slightly
[90.96s -> 93.40s]  better than zero performance relatively quickly.
[93.40s -> 96.36s]  So the x-axis here is a log scale.
[96.36s -> 99.86s]  The reward for the model-based learner here was about 500, the reward for the model-free
[99.86s -> 102.48s]  learner was about 5,000.
[102.48s -> 104.56s]  Here's what 500 reward looks like.
[104.60s -> 109.36s]  So it's not completely terrible, like it is actually moving forward just slowly.
[109.36s -> 114.08s]  And here's what a reward of 5,000 looks like after the model-free training.
[114.08s -> 115.96s]  So what's going on here?
[115.96s -> 120.36s]  Why is it that the model-based learner is so much worse than the model-free learner?
[120.36s -> 125.92s]  Well, in contrast to regular supervised learning problems, the model-based RL problem
[125.92s -> 130.84s]  with this iterative data collection poses kind of a unique challenge to the neural net training
[130.84s -> 131.84s]  algorithm.
[132.04s -> 136.96s]  See, the trouble is that we need to not overfit when we have small amounts of data over here
[136.96s -> 141.36s]  in the beginning, but we still need to have enough capacity to do well once we have large
[141.36s -> 142.36s]  amounts of data.
[142.36s -> 144.72s]  And that turns out to be very difficult.
[144.72s -> 148.94s]  See, the thing is, we're mitigating the distributional shift problem by using our
[148.94s -> 152.52s]  model to collect additional data, but that means that our model needs to be pretty
[152.52s -> 157.94s]  good even early on when it doesn't have very much data.
[157.94s -> 161.86s]  And high-capacity models like neural nets are very good in the high data regime, but
[161.86s -> 164.82s]  they don't really struggle in the low data regime.
[164.82s -> 169.46s]  So for this reason, they'll do pretty poorly in the early stages, and when they do poorly
[169.46s -> 174.10s]  in the early stages, they essentially don't produce effective exploration, which means
[174.10s -> 177.62s]  they end up actually getting stuck.
[177.62s -> 182.26s]  So why do we have this performance gap?
[182.26s -> 188.62s]  Well, it really comes down to something like an overfitting problem, exacerbated by distributional
[188.62s -> 189.62s]  shift.
[189.62s -> 193.30s]  So this is sort of the classic picture that we think of when we think of overfitting.
[193.30s -> 199.02s]  We have a collection of points formed by a straight line plus noise, and we fit some
[199.02s -> 202.66s]  really powerful function approximators, and we get something like the blue line.
[202.66s -> 207.46s]  Now in reality, model-based RL, things are not quite that clear-cut, and distributional
[207.46s -> 209.82s]  shift also plays a really big role.
[209.86s -> 213.54s]  But basically the issue is along these lines.
[213.54s -> 217.74s]  So in model-based RL, since we're actually planning through our model, if our model
[217.74s -> 222.06s]  makes mistakes that are kind of in the positive direction, so for instance if the y-axis
[222.06s -> 226.82s]  here is the predicted reward for different trajectories, it'll be very tempting for the
[226.82s -> 233.74s]  planner to select those trajectories that result in the largest mistakes in the positive
[233.74s -> 234.74s]  direction.
[234.74s -> 238.90s]  So the planner will essentially exploit mistakes in a model, and if the model overfits, then
[238.98s -> 242.54s]  it has lots of these little holes, lots of these little spurious peaks for the planner
[242.54s -> 244.30s]  to exploit.
[244.30s -> 248.46s]  So it's not quite as simple as the regular overfitting problem, it's actually somewhat
[248.46s -> 256.02s]  worse because the planner is going to exploit all those holes in our overfitted model.
[256.02s -> 261.14s]  Okay so what I'm going to discuss in this part of the lecture is how appropriate uncertainty
[261.14s -> 264.70s]  estimation can help us fix this problem.
[264.70s -> 267.26s]  So how can uncertainty estimation help?
[267.26s -> 271.42s]  Well so with uncertainty estimation what you could imagine we'll be doing is for every
[271.42s -> 277.42s]  state action pair we will be predicting not just a single next state s prime but actually
[277.42s -> 282.10s]  a distribution of possible next states that you could reach under your uncertainty
[282.10s -> 285.58s]  about the model.
[285.58s -> 290.02s]  So the reason this might be a good idea is let's say that you'd like to walk up
[290.02s -> 293.16s]  to the edge of the cliff to get a beautiful view of the ocean.
[293.16s -> 296.34s]  So your goal was here right at the edge of the cliff.
[296.34s -> 302.10s]  Now if you are very confident in your model you might say well if I plan actions that
[302.10s -> 305.18s]  get to the edge of the cliff I'll be standing right on the edge and I'll get the highest
[305.18s -> 306.74s]  reward.
[306.74s -> 312.42s]  But if you're extremely uncertain about your model and you understand this uncertainty
[312.42s -> 316.98s]  then when you take the expected value of the reward under your highly uncertain model
[316.98s -> 321.46s]  you realize that walking right up to the edge of the cliff has a pretty high probability
[321.46s -> 326.56s]  of causing you to fall over the edge which would incur a very large negative reward.
[326.56s -> 330.32s]  So you will automatically choose to stay further back because as you get closer the
[330.32s -> 333.94s]  probability of accidentally falling off increases due to your model uncertainty, due to the
[333.94s -> 337.90s]  fact that maybe you don't know exactly where the cliff is.
[337.90s -> 342.40s]  Now crucially this phenomena can emerge even if we don't do anything special, even
[342.40s -> 347.88s]  if we don't specifically plan to be let's say pessimistic under the model or avoid
[347.88s -> 348.88s]  the worst case.
[348.88s -> 350.38s]  That's not what we're doing here.
[350.38s -> 355.58s]  Even if we just reason about the expected value of the reward under an uncertain model
[355.58s -> 360.88s]  we'll already get this kind of behavior that avoids highly uncertain regions if they
[360.88s -> 364.48s]  can negatively impact the reward.
[364.48s -> 367.76s]  So what this does is it forces the planner to essentially hedge its bets.
[367.76s -> 371.92s]  It forces it to take a sequence of actions that would be good in expectation which sort
[371.92s -> 377.10s]  of means that in all possible futures that are represented by your model uncertainty
[377.10s -> 381.68s]  that sequence of actions is a pretty good one.
[381.68s -> 385.78s]  Now an important implication of this statement is that the way that you model your uncertainty
[385.78s -> 390.86s]  really needs to consider the possible worlds in which you could be in.
[390.86s -> 394.78s]  So this is not really about the setting where the dynamics are noisy, this is about
[394.78s -> 398.18s]  the setting where you don't know what the dynamics are.
[398.18s -> 403.68s]  So there are many possible worlds that could be consistent with your data and you'd like
[403.68s -> 408.88s]  to take actions that are good in expectation under the distribution of those possible
[408.88s -> 412.44s]  worlds given your data set.
[412.44s -> 417.66s]  This is a fairly special kind of uncertainty.
[417.66s -> 423.40s]  So we would expect the reward under high variance predictions to be very low for when
[423.40s -> 427.86s]  we approach the edge of the cliff if we're modeling the uncertainty correctly even
[427.86s -> 433.08s]  if the mean is the same.
[433.08s -> 438.96s]  So the only real change we're going to make is in step three we'll only take actions
[438.96s -> 444.24s]  for which we think we'll get high reward in expectation under our certain dynamics
[444.24s -> 449.68s]  and this is going to avoid that exploitation problem which is going to result in our algorithm
[449.68s -> 454.06s]  doing much more sensible things particularly in those early stages of training when model
[454.06s -> 459.16s]  uncertainty is very high and then the model will adapt and get better and hopefully
[459.16s -> 463.28s]  it'll become more confident in those regions where we're seeing high rewards.
[463.28s -> 467.00s]  So then eventually we'll approach right to the edge of the cliff because we'll gradually
[467.00s -> 468.30s]  refine our model.
[468.30s -> 471.32s]  So intuitively you can think of this as you don't know quite where the cliff is,
[471.32s -> 475.48s]  you walk a fairly safe distance to it, but you collect a little bit more data, refine
[475.48s -> 481.48s]  your understanding, and then you can walk a little closer to it next time.
[481.48s -> 482.80s]  Now there are a few very important caveats.
[482.80s -> 486.44s]  I mentioned before this is a very special kind of uncertainty.
[486.44s -> 493.58s]  So one caveat is that you do need to explore to get better.
[493.58s -> 499.16s]  So if you're very cautious, if you draw a really big kind of interval around your
[499.16s -> 505.80s]  target and you have a particularly nasty reward structure, you might end up never going
[505.80s -> 511.66s]  anywhere near high reward regions and then your exploration might be hampered.
[511.68s -> 518.18s]  So you have to make sure that this uncertainty-aware exploration is not harming exploration too
[518.18s -> 522.26s]  much and we'll talk about exploration much more in a few lectures from now.
[522.26s -> 526.42s]  The other thing to remember is that the expected value is not the same as a pessimistic
[526.42s -> 527.42s]  value.
[527.42s -> 535.30s]  So this is not an algorithm that tries to maximize worst-case error or be robust
[535.30s -> 536.30s]  or anything like that.
[536.30s -> 540.54s]  You could derive robust algorithms based on uncertainty estimation, for example by using
[540.54s -> 543.90s]  a lower confidence bound instead of the expected value, but that is not what we're
[543.90s -> 545.36s]  talking about here.
[545.36s -> 548.50s]  That's a very reasonable thing to do if you're especially concerned about safety,
[548.50s -> 552.26s]  but here we're just talking about taking the expected value.
[552.26s -> 554.82s]  The expected value is also not the same as an optimistic value.
[554.82s -> 558.26s]  So you could be optimistic with respect to your uncertainty and then you'd expect
[558.26s -> 563.62s]  more exploitation, but you would also expect to see some better optimistic exploration
[563.62s -> 567.94s]  strategies and again we'll talk about that a few weeks from now.
[567.94s -> 571.66s]  So the expected value is a pretty good place to start and that's what we're going to do for now.
