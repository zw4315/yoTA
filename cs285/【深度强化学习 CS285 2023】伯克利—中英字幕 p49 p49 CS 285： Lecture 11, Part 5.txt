# Detected language: en (p=1.00)

[0.00s -> 4.92s]  All right. In the last portion of today's lecture, we're going to shift gears a
[4.92s -> 8.84s]  little bit and talk about how we can do model-based reinforcement learning with
[8.84s -> 16.04s]  images. So what happens with complex image observations, things like images
[16.04s -> 20.80s]  in Atari or pictures from a robot's camera performing some manipulation
[20.80s -> 27.04s]  task? Well, with the algorithms that we talked about before, they all have some form
[27.08s -> 31.64s]  of model that predicts the next state from the previous state in action, and
[31.64s -> 37.84s]  then plans over these states. What is hard about doing this with images? Well,
[37.84s -> 41.16s]  first, images have very high dimensionality, which can make prediction
[41.16s -> 46.16s]  difficult. Images also have a lot of redundancy. So, you know, the different
[46.16s -> 49.64s]  pixels in the image for the Atari game are very similar to each other, and
[49.64s -> 54.64s]  that means that the state contains a lot of redundant information. Image-based
[54.64s -> 57.92s]  tasks also tend to have partial observability. So if you observe one
[57.92s -> 61.80s]  frame in an Atari game, you might not know how fast the ball is moving in
[61.80s -> 66.80s]  break-up, for instance, or in which direction. So when we're dealing with
[66.80s -> 71.48s]  images, we typically deal with a POMDP model, and this is the graphical
[71.48s -> 75.36s]  model illustration for a POMDP. It has a distribution of next states given
[75.36s -> 78.84s]  previous states and actions, and distributions of over-observations given
[78.84s -> 83.32s]  states. And typically when we're doing RL with images, we know the
[83.32s -> 89.72s]  observations and actions, but we do not know the states. So we would like to
[89.72s -> 94.68s]  learn the transition dynamics in state space P of ST plus one given STAT, but
[94.68s -> 100.80s]  we don't even know what S is. So perhaps we could separately learn P of
[100.80s -> 106.64s]  OT given ST, and P of ST plus one given ST comma AT. And that could be
[106.64s -> 111.68s]  quite nice, because P of OT given ST handles all the high-dimensional stuff,
[111.92s -> 117.00s]  but it doesn't have to deal with the complexity of temporal dynamics, whereas
[117.00s -> 121.72s]  the P of ST plus one given STAT has to deal with the dynamics, but doesn't
[121.72s -> 124.68s]  have to deal with the high-dimensional stuff. And maybe this
[124.68s -> 129.12s]  separation of roles can give us some viable model-based RL algorithms for
[129.12s -> 133.64s]  image observations. I'll discuss such algorithms briefly and somewhat
[133.64s -> 136.88s]  informally, but then at the end I'll also talk about how maybe some of this
[136.88s -> 140.32s]  is not actually true. Maybe it is not too bad to actually learn dynamics
[140.36s -> 143.76s]  directly on images. So that'll come at the end. But first let's talk about
[143.76s -> 149.36s]  these kind of state space models. So these are sometimes referred to as
[149.36s -> 154.72s]  latent space or latent state models. In general they're state space models. So
[154.72s -> 158.80s]  here we're going to learn two objects. We're going to learn a P of OT given
[158.80s -> 163.92s]  ST, basically how does our state map to an image, that's the observation
[163.92s -> 169.96s]  model, and a P of ST plus one given STAT, which is our dynamics model in
[170.00s -> 175.56s]  our unobserved state space. We will typically also need to learn a reward
[175.56s -> 180.20s]  model, P of RT given STAT, because our reward depends on the state, and since
[180.20s -> 182.88s]  we don't know what the state is, we don't know how the reward depends on
[182.88s -> 188.72s]  it. So we typically also add a reward node to this and learn a reward model.
[188.72s -> 195.36s]  Alright, so how should we train one of these things? Well, if we had a standard
[195.36s -> 198.96s]  fully observed model, we would train it with maximum likelihood. We would
[198.96s -> 202.80s]  basically take our data set of n different transitions, and for each
[202.80s -> 207.36s]  transition we would maximize the log probability of ST plus one comma I
[207.36s -> 214.68s]  given STI and ATI. If we have a latent space model, now we have a P of OT
[214.68s -> 220.20s]  given ST and a P of ST plus one given STAT. So we have to maximize the log
[220.20s -> 223.68s]  probabilities of both of those, and potentially also the reward model if we
[223.68s -> 228.92s]  want to add that in. If we knew the states, then this would be easy. Then we
[228.92s -> 235.52s]  would just add together log P phi ST plus one given STAT to log P phi OT
[235.52s -> 240.28s]  given ST. The problem is that we don't know what S is, so we have to use an
[240.28s -> 244.76s]  expected log likelihood objective, where the expectation is taken over the
[244.76s -> 249.80s]  distribution over the unknown states in our training trajectories. Those of you
[249.80s -> 253.08s]  that are familiar with things like hidden Markov models, it's basically the
[253.08s -> 258.80s]  same idea. So we would need some sort of algorithm that can compute a posterior
[258.80s -> 263.56s]  distribution over states given our images, and then estimate this expected
[263.56s -> 271.24s]  log likelihood using states sampled from that approximate posterior. So the
[271.24s -> 277.92s]  expectation is taken with respect to P of ST comma ST plus one given O1
[277.92s -> 285.90s]  through T and A1 through T at every time step. Okay, so how can we actually do
[285.90s -> 291.40s]  this? Well, one thing we could do is we can actually learn an approximate
[291.40s -> 296.00s]  posterior, and I'm going to say this approximate posterior is parameter psi, and
[296.00s -> 300.20s]  I'm going to note a Q psi, and the approximate posterior will be another
[300.20s -> 304.40s]  neural network that gives the distribution over ST given the
[304.40s -> 308.52s]  observations and actions seen so far. And there are a few choices that you
[308.52s -> 314.16s]  could make. So we call this approximate posterior the encoder, and you can learn
[314.16s -> 317.40s]  a variety of different kinds of posteriors, which one you pick will
[317.40s -> 322.16s]  have some effect on how well your algorithm works. So you could learn kind
[322.16s -> 325.80s]  of a full smoothing posterior, you could learn a neural net that gives you Q
[325.80s -> 330.88s]  psi of ST comma ST plus one given O1 through capital T comma A1
[330.88s -> 335.48s]  through capital T. So this posterior gives you exactly the quantity you want,
[335.48s -> 339.44s]  it's the most powerful posterior you could ask for, but it's also the hardest to
[339.44s -> 345.20s]  learn. On the other extreme you could imagine a very simple posterior that
[345.20s -> 348.40s]  just tries to guess the current state given the current observation, for
[348.40s -> 352.48s]  example if the partial observability effects are minimal, and this is the
[352.48s -> 356.72s]  easiest posterior to train but also the worst in the sense that using it
[356.72s -> 360.60s]  will be the furthest away from the true posterior that you want, which is P
[360.60s -> 366.12s]  of ST comma ST plus one given O1 through T comma A1 through T. So you
[366.12s -> 372.28s]  could ask for a full smoothing posterior or a single step encoder. The
[372.28s -> 375.72s]  full smoothing posterior is the most accurate in the sense that it most
[375.72s -> 379.48s]  accurately represents your uncertainty about the states, but it's also by far
[379.48s -> 383.48s]  the most complicated to train. The single step encoder is by far the
[383.48s -> 388.76s]  simplest but provides the least accurate posterior. In general you would
[388.84s -> 392.36s]  want a more accurate posterior in situations that are more partially
[392.36s -> 397.32s]  observed. So if you believe that your problem is such where the state can be
[397.32s -> 401.32s]  pretty much entirely guessed from the current observation, then a single step
[401.32s -> 404.92s]  posterior is a really good choice, whereas if you have a heavily partially
[404.92s -> 407.80s]  observed setting then you want something closer to a full smoothing
[407.80s -> 411.40s]  posterior, and there are of course a lot of in-between choices like
[412.28s -> 415.16s]  estimating ST given O1 through T comma A1 through T.
[418.28s -> 422.20s]  Now in terms of how to actually train these posteriors, this requires
[422.20s -> 425.64s]  an understanding of something called variational inference, which we'll cover
[425.64s -> 429.24s]  in more detail next week. I'll gloss over how to train
[430.44s -> 435.24s]  these probabilistic encoders in this lecture, and I'll instead focus on a very
[435.24s -> 437.64s]  simple limiting case of the single step encoder.
[438.60s -> 442.44s]  So we're going to talk about the single step encoder, and we're going to talk
[442.44s -> 445.32s]  about a very simple special case of the single step encoder.
[445.96s -> 449.96s]  So if we were to really do this right, then for every time step we would
[449.96s -> 455.72s]  sample ST from Q of ST given OT and ST plus one from Q of ST plus one
[455.72s -> 461.16s]  given OT plus one, and then using those samples maximize log P of ST plus
[461.16s -> 464.60s]  one given STAT and log P of OT given ST.
[465.16s -> 469.48s]  But a very simple special case of this, if you believe that your problem is
[469.48s -> 472.36s]  almost fully observed, is to actually use a deterministic
[472.36s -> 476.28s]  encoder. So instead of outputting a distribution over ST given OT,
[476.28s -> 480.20s]  we would just output a single ST for our current OT.
[480.76s -> 483.80s]  The stochastic case requires variational inference, which I'll
[483.80s -> 486.52s]  discuss next week, but the deterministic case is quite a
[486.52s -> 490.92s]  bit simpler. So the deterministic case can be thought of as a delta function
[490.92s -> 494.92s]  centered at some deterministic encoding g psi of OT.
[496.52s -> 498.92s]  So that means that ST is equal to g psi of OT.
[500.12s -> 503.96s]  And if we use this deterministic encoder, then we can simply substitute
[503.96s -> 507.56s]  that in everywhere where we see an S in the original objective,
[507.56s -> 511.32s]  and we can remove the expectation. So now our objective
[511.32s -> 516.92s]  is to maximize with respect to phi and psi the sum over all of our trajectories,
[516.92s -> 522.12s]  of the sum over all of our time steps of log P g of OT plus one
[522.12s -> 528.52s]  given g of OT comma AT plus log P of OT given g of OT.
[529.88s -> 532.68s]  So the second term can be thought of as a kind of autoencoder.
[532.68s -> 535.72s]  It just says that if you encode OT, you should be able to reconstruct it back
[535.72s -> 540.12s]  out again, and the first term it enforces
[540.12s -> 542.92s]  that the encoded states should obey the learned dynamics.
[543.16s -> 547.72s]  And then you could optimize both phi and psi jointly by back propagating
[547.72s -> 554.92s]  through this whole thing. If the dynamics is stochastic,
[554.92s -> 559.96s]  then you want to use something called the reparameterization trick to make
[559.96s -> 563.16s]  this possible to solve with gradient descent, which I'll cover next week.
[563.16s -> 566.52s]  But you could also use deterministic dynamics in this case and have a fully
[566.52s -> 570.84s]  deterministic state space model of this sort. So the short version is
[571.72s -> 576.20s]  write down the subjective and then optimize it with back propagation and gradient descent.
[579.16s -> 582.12s]  So everything is differentiable, and you could train everything with backprop.
[584.36s -> 590.12s]  All right, so take a minute to think about this formulation, look over the slide,
[590.12s -> 592.28s]  and think about whether everything here makes sense to you.
[593.56s -> 596.84s]  If you have a question about what's going on here, it would be a very good idea to
[596.84s -> 601.88s]  write a comment or question in the comments and then we could discuss this in class.
[603.16s -> 609.00s]  But to briefly summarize, we talked about how if you want to learn stochastic state space models,
[609.00s -> 613.08s]  you need to use an expected log likelihood instead of a standard log likelihood,
[613.08s -> 617.72s]  where the expectation is taken with respect to an encoder which represents the posterior.
[618.36s -> 623.80s]  There are many ways to approximate the posterior, but the absolute simplest one is to use an
[623.88s -> 628.52s]  encoder from observations to states and make it a deterministic encoder, in which case the
[628.52s -> 632.68s]  expectation actually goes away, and you can directly substitute the encoded observation
[632.68s -> 636.60s]  in place of states in your dynamics and observation model objectives.
[637.56s -> 640.60s]  And of course the reward model would work the same way. So if we had a reward model,
[640.60s -> 645.96s]  we would also add a log p of r t given g of o t in here.
[645.96s -> 656.84s]  Okay, so there's our state space model. You can think of g of o t as an additional
[657.48s -> 662.28s]  virtual edge that maps from o to s, and we also have the reward model, so we can add that in
[662.28s -> 667.48s]  there, and then we have a latent space dynamics, image reconstruction, and a latent space reward
[667.48s -> 674.04s]  model. There are many practical methods for using a stochastic encoder to model uncertainty,
[674.04s -> 677.56s]  and in practice those do work better, but for simplicity of exposition,
[677.56s -> 681.00s]  if you think about this as a deterministic encoder, I think that makes a lot of sense.
[683.56s -> 690.44s]  Okay, so how do we use this in an actual model-based RL algorithm? Well, it's actually
[690.44s -> 695.32s]  fairly straightforward. You can just substitute this directly into the model-based RL version 1.5
[695.32s -> 700.20s]  algorithm that I discussed before. You can run your base policy pi zero to collect the data
[700.20s -> 705.80s]  set of transitions. Now these transitions consist of observation, action, and next observation tuples.
[707.08s -> 713.56s]  Then you train your dynamics, reward model, observation model, and encoder together with
[713.56s -> 718.36s]  backpropagation, plan through the model to choose actions that maximize the reward,
[718.36s -> 722.84s]  execute the first planned action, and observe the next resulting observation o prime,
[724.20s -> 728.84s]  append that transition to d, and replan, and that's your inner MPC loop, and then you have
[728.84s -> 734.04s]  your outer data collection loop where every n steps you collect more data and retrain all
[734.04s -> 741.48s]  of your models. All right, a few examples of actual algorithms in the literature that have
[741.48s -> 750.28s]  used this trick. So here is an example by Vater et al. called Embed to Control. This paper used
[750.28s -> 757.40s]  a stochastic encoder, but otherwise the idea is fairly similar, and then they used LQR to
[757.40s -> 765.16s]  construct their plans through the state space model. So here's a video. First they're showing
[765.16s -> 770.12s]  their state space. This is for a kind of a point mass 2d navigation task where you just have to
[770.12s -> 776.68s]  avoid those six little obstacle locations, and what they're showing on the right is an
[776.68s -> 780.52s]  embedding of the state space learned by their model, and you can see that it kind of has a
[780.52s -> 784.84s]  2d decomposition that reflects the 2d structure in the task, even though the observations are
[784.84s -> 790.84s]  images. Here is an inverted pendulum task where they're training on images from the inverted
[790.84s -> 795.16s]  pendulum, and you can see that the state space model has this kind of cool 3d structure
[795.16s -> 802.04s]  reflecting the cyclical nature of the pendulum task. Here is the actual algorithm in action
[802.04s -> 808.12s]  for pendulum swing-up. So on the right they're showing basically one-step predictions from
[808.12s -> 812.28s]  their model, and on the left they're showing the real image, and you can see that it's kind of
[812.28s -> 818.44s]  fuzzy but has some reasonable idea of what's going on. Here is another task which is
[818.44s -> 823.32s]  carpool balancing. So here again you can see the images on the right are a little fuzzier,
[823.32s -> 830.20s]  but they generally have a similar rough idea, and here is a simple reaching task with a three-link
[830.20s -> 837.00s]  muscular arm, and it's trying to reach a particular goal image. So you can see that it kind of
[837.00s -> 839.64s]  reaches out and more or less goes to the right goal image.
[842.28s -> 847.72s]  All right, here's a more recent paper that builds on these ideas to develop a more sophisticated
[847.72s -> 855.24s]  state space model. So here the state space model actually is regularized to be locally linear,
[855.24s -> 859.40s]  which makes it well suited for iterative linearization algorithms like iterative LQR,
[860.76s -> 865.88s]  and this method is tested on some robotics tasks. This was actually done by a student who was an
[865.88s -> 871.24s]  undergraduate here at Berkeley at the time, and here the observations that the robot are
[871.24s -> 875.24s]  seeing are shown in the top left corner, and then it's using LQR with this learned state
[875.24s -> 880.76s]  space model to put the Lego block on the other Lego block. And here's another example
[880.76s -> 884.76s]  of a task where the robot has to use images to push this cup to the desired location,
[885.32s -> 892.60s]  and Laura here, who is one of the authors on this paper, is in real time giving the robot
[892.60s -> 896.36s]  rewards to supervise this reward model by hitting that button on the keyboard.
[900.04s -> 905.96s]  All right, so here's a little bit more of an illustration. This is essentially
[905.96s -> 910.04s]  running pi zero. This is the initial random data collection. From here on out,
[910.04s -> 914.52s]  the model will be trained and then will be used for testing in different positions.
[914.52s -> 917.80s]  So here are some tasks where the object starts in different locations.
[917.80s -> 927.72s]  So here you can see on the left is the encoder and decoder. So this is basically evaluating the
[927.72s -> 931.40s]  observation model, and you can see the observation model reconstructs the images fairly accurately,
[931.96s -> 935.16s]  and on the right is what the robot is actually doing, and this is after about 20
[935.16s -> 938.20s]  minutes of training. So these kinds of algorithms tend to be quite a bit more efficient
[938.20s -> 940.28s]  than model-free algorithms that we discussed before.
[940.28s -> 953.00s]  Okay, now so far we've talked about algorithms that learn a latent state space model. They learn
[953.00s -> 959.32s]  some sort of encoder with an embedding g of o t equals s t. What if we dispense with the
[959.32s -> 963.56s]  embedding altogether and actually go back to the original recipe and model-based RL,
[963.56s -> 967.40s]  but in observation space? So what if we directly learn p of o t plus one
[967.40s -> 974.28s]  given o t a t? If we have partial observability, then we probably need to use our current model.
[974.28s -> 979.64s]  So we need to make o t plus one also depend on old observations, but as long as we do this,
[979.64s -> 983.64s]  we can actually do a pretty decent job of modeling dynamics directly in image space,
[983.64s -> 988.60s]  and there's been a fair bit of work doing this. This is an example actually from a somewhat
[988.60s -> 994.92s]  older paper, now three years ago, showing a robotic arm, and each column shows a different
[994.92s -> 999.08s]  action starting from the same point. So you can see that for different actions, the arm moves left,
[999.08s -> 1004.84s]  right, up and down, and when it contacts objects, it pushes those objects. These kinds of methods
[1004.84s -> 1009.56s]  can work fairly well in more complex settings where learning a compact latent space is very
[1009.56s -> 1013.72s]  difficult. So if you have dozens of objects in the scene, it's not actually clear how to build
[1013.72s -> 1018.28s]  a compact state space for them, but predicting directly in image space can actually work very
[1018.28s -> 1023.00s]  well. And then you could direct the robot to do a particular thing by, for example, telling it,
[1023.00s -> 1027.16s]  you know, this particular point in the image, move it to this location, and it figures out actions
[1027.16s -> 1032.12s]  that lead to that outcome, and you can do things like reach out and grab a stapler. So here is the
[1032.12s -> 1035.80s]  animation of what the model thinks is going to happen, and when it actually goes and does it,
[1035.80s -> 1039.88s]  it reaches out, puts the hand on the stapler, and then pushes it to the desired location.
