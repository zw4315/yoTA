# Detected language: en (p=1.00)

[0.00s -> 7.20s]  Alright, welcome to Lecture 13 of CS285. Today, we're going to talk about exploration. Today's
[7.20s -> 11.56s]  lecture is going to be a little bit on the longer side, but to make up for it, the
[11.56s -> 16.80s]  next lecture, which is going to be Part 2 of exploration, will be quite a bit shorter.
[16.80s -> 20.28s]  So if this lecture feels like it's going on for a while, we're going to give you
[20.28s -> 24.24s]  a little bit of a break for Wednesday's lecture, where it won't be quite as long.
[24.24s -> 28.72s]  Alright, let's get started. So, what's the problem that we're going to talk about
[28.72s -> 35.68s]  today? Well, the problem can be illustrated with an example like this. If you're
[35.68s -> 40.00s]  working on Homework 3, if you're finishing that up now, you might have tried a few
[40.00s -> 43.44s]  different Atari games. Some of these Atari games are actually pretty easy, so
[43.44s -> 48.60s]  if you want to play Pong or Breakout, mostly your your Homework 3 Q
[48.60s -> 53.88s]  learning implementation will probably work pretty well on those tasks. But some
[53.88s -> 59.12s]  other Atari games are actually quite a bit hard. So this game, for example, is
[59.12s -> 64.12s]  almost impossible. If you try to run it, this is called Montezuma's Revenge, if
[64.12s -> 66.64s]  you try to run your Q learning implementation on this game, you'll
[66.64s -> 70.86s]  probably find that it doesn't get very far. So why is that? Why is the game on
[70.86s -> 75.40s]  the right so much harder than the game on the left? Well, it's not
[75.40s -> 79.72s]  because the game itself is necessarily harder. For a person playing Montezuma's
[79.72s -> 82.48s]  Revenge, you know, I've played it myself, I don't think it's a very good game,
[82.92s -> 87.52s]  but it's not a particularly difficult one. In fact, getting that trick shot in
[87.52s -> 91.08s]  Breakout where it bounces around up top is probably harder actually than playing
[91.08s -> 96.72s]  Montezuma's Revenge. But it's very hard for an RL agent to play this game.
[96.72s -> 103.84s]  So in Montezuma's Revenge, the goal is to traverse this pyramid that's
[103.84s -> 107.32s]  made up of multiple different rooms, and each room has a different challenge. So
[107.32s -> 110.84s]  in this first room, there's a skull that bounces around that kills you if
[110.88s -> 113.88s]  you step on it, and you have to go fetch this key, and then open one of the doors
[113.88s -> 118.68s]  at the top. Now we understand some of these things. We understand that the key
[118.68s -> 122.32s]  is a good thing, that keys open doors. We might not know what exactly the
[122.32s -> 125.04s]  skull is supposed to do, but we kind of know that skulls are probably not
[125.04s -> 129.66s]  good things, and touching the skull is probably not a good idea. Now in the
[129.66s -> 134.36s]  game, you get a reward for getting the key. You also get a reward for
[134.36s -> 139.84s]  opening the door. Getting killed by the skull actually doesn't do anything, so you
[139.88s -> 143.06s]  lose a life, but you don't actually get a negative reward for that. If you lose
[143.06s -> 146.68s]  all your lives, then you start over. That's also not obvious whether that's
[146.68s -> 149.64s]  good or bad, because when you start over, you might get another
[149.64s -> 152.52s]  opportunity to pick up the key. And maybe that's good, because then you get
[152.52s -> 156.12s]  the reward for the key again. So the reward structure of the game doesn't
[156.12s -> 160.48s]  really guide you each step of the way, and while we know ourselves that some
[160.48s -> 164.08s]  of these things are good or bad, the agent really doesn't. And the agent
[164.08s -> 167.04s]  might figure out that a good way to keep getting reward is to keep
[167.24s -> 170.56s]  getting killed by the skull, so they can pick up the key again, instead of moving
[170.56s -> 175.08s]  on to the next room. The trouble is that finishing the game only weakly
[175.08s -> 178.60s]  correlates with rewarding events. It's not that you get little pieces of
[178.60s -> 181.04s]  reward when you're on the right track, and negative reward when you're on the
[181.04s -> 185.52s]  wrong track. So we know what to do, because we understand what all these
[185.52s -> 188.52s]  little sprites and pictures mean, but the RL algorithm has to figure it out
[188.52s -> 194.04s]  through trial and error. To try to understand kind of how the algorithm
[194.04s -> 197.68s]  feels when trying to play one of these games, let's think of a different example,
[197.68s -> 203.16s]  an example that's a lot less intuitive for humans. So there's a card game called
[203.16s -> 209.92s]  Mao. It's also similar in principle to a game called Calvin Ball. The idea is
[209.92s -> 213.76s]  that the only rule you may be told is this one. So when you start playing
[213.76s -> 219.24s]  the game, you just don't know the rules of the game, and one of the players
[219.24s -> 222.84s]  who's the chairman can call you out for not following a rule, but they don't
[222.88s -> 226.52s]  explain the rule to you, they just tell you that you incur a penalty for failing to
[226.52s -> 231.36s]  follow a rule. And you can only discover the rules through trial and error. And then
[231.36s -> 235.68s]  this makes the game very frustrating and quite demanding. So even though the
[235.68s -> 239.68s]  rules might be fairly simple, because you don't know those rules and you have to
[239.68s -> 243.16s]  discover them through trial and error, the game ends up being very, very challenging.
[243.16s -> 247.32s]  And the rules don't always make sense to you, so the whole point of this
[247.32s -> 252.40s]  game is for other players to make up rules that are kind of weird and
[252.48s -> 259.08s]  counterintuitive. So temporarily extended tasks like Montezuma's Revenge or the
[259.08s -> 263.80s]  game Mao can become increasingly difficult based on how extended the task
[263.80s -> 269.08s]  is and how little you know about the rules. Essentially even seemingly
[269.08s -> 271.92s]  simple tasks where you don't know the rules and you have to discover them through trial and
[271.92s -> 275.56s]  error as a result of poorly shaped rewards can prove to be exceptionally
[275.56s -> 280.36s]  challenging. And imagine taking this a step further. Imagine that your goal in
[280.36s -> 284.32s]  life was to win 50 games of Mao. So you're just going about your day, you know,
[284.32s -> 287.08s]  you can go to class, you can do your homework, but if you happen to win 50
[287.08s -> 290.36s]  games of Mao, you're gonna get a million dollars. Now you're pretty
[290.36s -> 294.84s]  unlikely to just sort of randomly go and do this. So this is essentially the
[294.84s -> 299.08s]  exploration problem. The exploration problem relates to this setting where
[299.08s -> 303.32s]  you have temporally delayed rewards, where the structure of the task doesn't
[303.32s -> 306.24s]  really tell you what are the things you need to do to get larger rewards in the
[306.24s -> 313.68s]  future. All right, here's another example that looks very different at
[313.68s -> 318.28s]  first but actually describes kind of a similar type of problem. So this is a
[318.28s -> 321.76s]  continuous control task. So here this robotic hand is supposed to pick up a
[321.76s -> 325.68s]  ball and move it to this location. Now this is also a difficult exploration
[325.68s -> 330.04s]  problem because in order to figure out how to get reward by putting the
[330.04s -> 333.24s]  object in the right place, the hand needs to essentially wiggle the joints
[333.24s -> 337.76s]  and the fingers randomly. And again, just like a priori, we don't understand the
[337.76s -> 342.92s]  rules of the game Mao. Here the hand doesn't understand that moving and
[342.92s -> 345.68s]  picking up objects is actually a thing. All it knows is that it can wiggle its
[345.68s -> 348.96s]  fingers around and the reward is so delayed that it gets very little
[348.96s -> 355.64s]  intermediate signal for actually grasping objects. All right, so let's
[355.64s -> 360.20s]  talk a little bit more about this exploration thing. In RL we often refer
[360.20s -> 365.64s]  to the exploration versus exploitation problem as one where at each trial the
[365.64s -> 368.88s]  agent has to essentially choose whether they want to do a better
[368.88s -> 372.04s]  job of exploring by trying something they don't know how to do yet or
[372.04s -> 374.48s]  whether they just want to do the thing that gets them the largest reward.
[374.48s -> 378.32s]  So the agent in Montezuma's Revenge that's just going after the key each
[378.32s -> 382.64s]  time they die is essentially performing a kind of exploitation. They know one
[382.64s -> 386.12s]  thing that gives them reward, which is the key, and they know one way to get
[386.12s -> 390.52s]  that reward of just to die and get the key again, and they're just capitalizing
[390.52s -> 393.44s]  on that, getting the rewards they know how to get, instead of trying to find
[393.44s -> 397.88s]  better rewards elsewhere. So there are two potential definitions of the
[397.88s -> 401.88s]  exploration problem in light of this. The first is how can an agent discover
[401.88s -> 405.72s]  high reward strategies that require a temporally extended sequence of complex
[405.72s -> 410.40s]  behaviors that individually are not rewarding? And the second is how can an
[410.40s -> 413.92s]  agent decide whether to attempt new behaviors to discover ones with
[413.92s -> 417.68s]  higher reward or continue to do the best thing it knows so far? And these
[417.68s -> 423.08s]  are really the same problem, because if you want to discover temporally
[423.08s -> 426.96s]  extended sequences of behaviors that lead to high reward, you need to decide
[426.96s -> 430.36s]  whether you should be exploring more or whether you've already found the
[430.36s -> 433.84s]  most temporally extended sequence and you should just keep doing that, or maybe
[433.84s -> 437.56s]  refine how well you do that. So they're actually the same problem.
[437.56s -> 440.40s]  Exploitation is doing what you know will yield the highest reward,
[440.40s -> 443.32s]  exploration is doing things you haven't done before in the hopes of
[443.36s -> 446.48s]  getting even higher reward, and the trouble is you don't know which one of
[446.48s -> 450.24s]  those you should be doing. And of course they're not totally disjoint, so for
[450.24s -> 453.40s]  example in some cases you might want to exploit a little bit so that you can
[453.40s -> 456.92s]  explore further. If you figured out how to go to the second room in Montezuma's
[456.92s -> 460.68s]  Revenge, a good way to explore is to exploit a bit to go into that second
[460.68s -> 465.64s]  room and then explore from there. So it's not like you just have to flip a
[465.64s -> 469.36s]  coin and decide between exploitation and exploration, it's really kind of a dynamic and
[469.40s -> 474.96s]  persistent decision you have to keep making. So here are a few examples which
[474.96s -> 479.24s]  I borrowed from some of David Silver's lecture notes. Imagine that you have to
[479.24s -> 482.64s]  select which restaurant to go to, perhaps not something that you're doing in
[482.64s -> 486.40s]  2020, but you know in the previous year we lived in, back when going to
[486.40s -> 490.92s]  restaurants was a thing, exploitation would mean that you go to your favorite
[490.92s -> 495.20s]  restaurant, exploration means you try a new restaurant. Now this example makes
[495.20s -> 498.84s]  it seem very binary and I think that binary sense is a little misleading
[498.84s -> 502.68s]  because in reality it might be more complex than that, like the example of
[502.68s -> 506.64s]  Montezuma's Revenge I mentioned before, where the best way to explore might
[506.64s -> 509.68s]  actually be to exploit a little bit and then explore from the last day you
[509.68s -> 515.44s]  landed. Online ad placement, this is a classic exploration exploitation
[515.44s -> 518.96s]  trade-off problem, exploitations mean you show the most successful ad, the one
[518.96s -> 522.84s]  that makes you the most money, exploration means you show a different
[522.84s -> 528.36s]  perhaps randomly chosen advertisement. Oil drilling, exploitation, maybe you drill
[528.40s -> 532.24s]  at the best-known location, exploration, find a new location to drill at which
[532.24s -> 538.28s]  might not contain oil or it might contain even more oil. Now exploration is
[538.28s -> 542.96s]  very hard, both practically and also theoretically, it's a theoretically hard
[542.96s -> 547.56s]  and intractable problem. So a question that we might ask when we go to
[547.56s -> 552.28s]  devise exploration algorithms is, can we derive an optimal exploration strategy?
[552.28s -> 558.12s]  And that's actually what we're going to talk about in today's lecture, but in
[558.12s -> 563.28s]  order to do that we have to understand what does optimal even mean. So one of
[563.28s -> 566.12s]  the ways that we could define the optimality of our exploration strategy
[566.12s -> 570.28s]  is in terms of regret against a Bayes optimal strategy, and we'll make this
[570.28s -> 574.40s]  more formal later, but intuitively you could imagine a perfect Bayesian agent
[574.40s -> 579.72s]  that maintains the uncertainty about how the world works and therefore makes
[579.72s -> 583.52s]  optimal exploration decisions, maybe optimal decisions to optimally resolve
[583.52s -> 587.08s]  the unknowns about the world. And now such an optimal Bayesian agent would be
[587.08s -> 590.72s]  intractable, it would require estimating a really complex posterior
[590.72s -> 594.60s]  over your MDPs, but you could use this as a gold standard and for your
[594.60s -> 599.24s]  practical exploration algorithm measure its regret against the Bayes optimal
[599.24s -> 606.36s]  hypothetical agent. We can kind of place different problem settings on a
[606.36s -> 609.64s]  spectrum from theoretically tractable to theoretically intractable.
[609.64s -> 614.56s]  Theoretically tractable means that we can quantify or understand whether
[614.56s -> 617.88s]  a given exploration strategy is optimal, meaning that it's close to this
[617.88s -> 621.64s]  Bayes optimal strategy, or suboptimal, meaning it has much worse regret than
[621.64s -> 625.04s]  the Bayes optimal strategy. Intractable means that we cannot make
[625.04s -> 630.96s]  this estimate exactly in that setting. So the most theoretically tractable
[630.96s -> 634.52s]  problems are what are called multi-armed bandit problems. You can think of
[634.52s -> 639.48s]  multi-armed bandit problems as one-time step stateless RL problems. So
[639.48s -> 642.68s]  in RL you have a state and an action, and the action leads to the next state.
[642.92s -> 647.20s]  In a bandit, you only take one action, and then the episode terminates, and there is
[647.20s -> 650.88s]  no state. So you just have to decide on an action. And these are the most
[650.88s -> 654.36s]  theoretically tractable problems, because in multi-armed bandits we can
[654.36s -> 657.96s]  actually understand which exploration strategies are theoretically optimal, and
[657.96s -> 661.80s]  which ones are not optimal, in terms of their regret versus the Bayes
[661.80s -> 667.44s]  optimal agent. Then the next step up are the contextual bandit problems.
[667.44s -> 671.88s]  Contextual bandit problems are just like multi-armed bandits, only they do
[671.96s -> 674.92s]  have a state. So they still only have one time step, you still only take one
[674.92s -> 678.36s]  action, your action only affects your reward, it does not affect the next state,
[678.36s -> 682.60s]  but you have some context which is kind of like your state. So
[682.60s -> 686.20s]  add placement could be one such problem. You observe something about the
[686.20s -> 688.92s]  user, maybe you have a feature vector about the user, and then you have to
[688.92s -> 695.08s]  select which add to show to that user. Next step up are small finite
[695.08s -> 699.56s]  MDPs. So these are MDPs that can be solved exactly, maybe using value
[699.56s -> 703.40s]  iteration. These are not nearly as theoretically tractable as bandits, but
[703.40s -> 707.40s]  there are some things we could say about exploration in small finite MDPs.
[707.40s -> 711.56s]  And then of course the next step up, the setting we're really concerned with
[711.56s -> 716.40s]  in deep RL, are large infinite MDPs, perhaps with continuous state spaces
[716.40s -> 720.36s]  or very large state spaces like images. And generally for these problems
[720.36s -> 724.52s]  there isn't much that we can say theoretically. But what we can do is we
[724.52s -> 727.76s]  can take inspiration from the theoretically principled algorithms
[728.40s -> 732.56s]  that we can devise in the bandits setting, and then kind of adapt similar
[732.56s -> 735.84s]  techniques in the large infinite MDPs and hope that they work well.
[738.40s -> 743.84s]  So what makes an exploration problem tractable? Well for multi-armed bandits
[743.84s -> 747.28s]  and contextual bandits, one of the things we can do is we can
[747.28s -> 752.32s]  formalize the exploration problem as another kind of MDP, or rather a
[752.32s -> 756.32s]  partially observed MDP, a POMDP. So while the multi-armed band is a
[756.32s -> 759.84s]  single step problem, you can view the problem of exploring in the multi-armed
[759.84s -> 763.20s]  bandit as a multi-step problem, because even though your actions don't
[763.20s -> 766.64s]  affect your state, they do affect what you know. So if you
[766.64s -> 769.52s]  explicitly reason about the evolution of your beliefs,
[769.52s -> 773.44s]  that now forms a temporal process which is technically a partially observed
[773.44s -> 776.40s]  MDP, and then you could solve it using
[776.40s -> 779.36s]  POMDP methods.
[781.36s -> 784.72s]  And because these multi-armed bandits are
[784.72s -> 788.72s]  fairly simple, even the POMDP can actually be solved tractably,
[788.72s -> 793.44s]  at least in theory. And then the next step are small finite MDPs.
[793.44s -> 797.28s]  Here you can frame exploration as Bayesian model identification,
[797.28s -> 800.64s]  and then reason explicitly about things like value of information,
[800.64s -> 804.56s]  kind of extending similar ideas to the ones we had in bandits.
[804.56s -> 808.48s]  For large or infinite MDPs, these optimal methods don't work
[808.48s -> 811.52s]  in the sense that we can't prove anything about them, but we can still
[811.60s -> 815.12s]  take inspiration from the optimal methods in the simpler settings
[815.12s -> 818.96s]  and adapt them to these larger settings, and find that they actually work well, at
[818.96s -> 821.52s]  least empirically, even though we can't say anything about them
[821.52s -> 824.16s]  theoretically.
[824.48s -> 827.76s]  And of course we use lots of hacks, as we always do in deep reinforcement
[827.76s -> 829.52s]  learning. And that's the theme that you're
[829.52s -> 832.16s]  going to find in this lecture, that we'll have some very principled
[832.16s -> 835.76s]  approaches in simpler, smaller problems like multi-armed bandits.
[835.76s -> 839.12s]  We'll sort of adapt those approaches by analogy in larger MDPs,
[839.12s -> 843.76s]  and then use some hacks to make them work well in practice.
[844.16s -> 848.48s]  Okay, so let's start with a little discussion of bandits.
[848.48s -> 852.32s]  What's a bandit anyway? So the bandits that we're talking about when we talk
[852.32s -> 856.64s]  about exploration are not these guys. The bandit is
[856.64s -> 859.76s]  actually kind of the drosophila of exploration problems. So in the same way
[859.76s -> 863.44s]  that biologists study fruit flies as their kind of simple model
[863.44s -> 866.64s]  organism, in reinforcement learning we study the bandit
[866.64s -> 870.16s]  as our simple model organism. And the bandit that we're referring to
[870.16s -> 873.68s]  is this thing. So the term multi-armed bandit is kind of one of these quaint
[873.68s -> 878.16s]  American colloquialisms that stems from the term
[878.16s -> 881.68s]  one-armed bandit. So the one-armed bandit is a slot machine.
[881.68s -> 885.12s]  It's a machine in a casino where you pull the lever, and with some random
[885.12s -> 888.32s]  probability this thing will produce some reward.
[888.32s -> 892.24s]  Maybe you'll get, you'll lose your money, or maybe you'll get money.
[892.24s -> 896.72s]  The multi-armed bandit, so in a one-armed bandit you have only one action
[896.72s -> 899.12s]  just to pull the arm, and you don't know what the reward
[899.12s -> 901.60s]  for pulling that arm is, and the reward in general will be stochastic, so
[901.60s -> 905.20s]  it's really a reward distribution. You can think of a multi-armed bandit
[905.20s -> 908.72s]  as a bank of different slot machines, and the decision you have to make is
[908.72s -> 912.08s]  which slot machine to play. So you have n of these machines, and
[912.08s -> 914.64s]  different machines will give different payoffs, they'll have different reward
[914.64s -> 917.52s]  distributions. Now just because you pulled one of the
[917.52s -> 921.44s]  arms doesn't mean that's a bad arm. Maybe you pulled that arm and you got
[921.44s -> 923.92s]  very little money, but that's just because you got unlucky.
[923.92s -> 927.12s]  Maybe in general that machine gives very high payoff, and if you pull the
[927.12s -> 930.32s]  arm repeatedly on average you might make a lot of money.
[930.32s -> 932.64s]  So you don't know the reward for each arm, you don't know the reward
[932.64s -> 938.00s]  distribution for each arm. So you could assume that the reward of
[938.00s -> 942.48s]  each arm is distributed according to some probability distribution,
[942.48s -> 946.00s]  and then you could imagine even learning this probability distribution.
[946.00s -> 950.32s]  So there's an unknown per action distribution for each arm.
[950.32s -> 954.88s]  So how can we define the bandit? Well, we assume that the reward for each action
[954.88s -> 958.16s]  is distributed according to some distribution, and distribution for
[958.16s -> 961.28s]  action ai is parametrized by a parameter vector
[961.28s -> 964.88s]  theta i. So for example, if your rewards are zero
[964.88s -> 968.16s]  one, you might be in a setting where the probability of getting reward one
[968.16s -> 971.52s]  is theta i, which is just a number, and the probability of getting reward
[971.52s -> 976.40s]  zero is one minus theta i. If your rewards are continuous, maybe
[976.40s -> 979.84s]  you have some continuous distribution.
[980.00s -> 983.20s]  And you don't really know what the theta i's are, but you could assume that
[983.20s -> 985.84s]  you have a prior on them. You could use an uninformative prior
[985.84s -> 989.28s]  if you like, but in general we'd say we have some prior p of theta.
[989.28s -> 994.56s]  Okay, so that's defining our bandit. Now the cool thing about this is that
[994.56s -> 998.88s]  you could also view this as defining a POMDP for exploration,
[998.88s -> 1003.68s]  where the state is the vector of thetas for all of
[1003.68s -> 1008.08s]  your actions. Now you don't know the state, but if you
[1008.08s -> 1010.32s]  knew the state, then you could figure out what the
[1010.32s -> 1014.56s]  right action is. So instead of knowing the state, you
[1014.56s -> 1018.32s]  have a belief. So you have some belief p-hat over theta
[1018.32s -> 1022.40s]  one through theta n, and you can update your belief each
[1022.40s -> 1025.20s]  time you pull an arm. So each time you pull an arm, you observe the reward of
[1025.20s -> 1028.00s]  that arm, and you can update your belief about the
[1028.00s -> 1033.04s]  theta corresponding to that arm. And you could solve this POMDP to
[1033.04s -> 1037.20s]  basically figure out what is the right sequence of actions to maximize your
[1037.20s -> 1041.28s]  reward in this POMDP. And this will yield the optimal
[1041.28s -> 1044.80s]  exploration strategy, because if it is the optimal policy in the POMDP,
[1044.80s -> 1048.88s]  it is the optimal thing to do under this kind of uncertainty.
[1048.88s -> 1052.80s]  And that will be the optimal exploration strategy, the best exploration strategy
[1052.80s -> 1055.92s]  you could possibly have.
[1056.56s -> 1060.72s]  Now this is overkill. The belief state is huge,
[1060.80s -> 1066.88s]  even for a simple POMDP with binary rewards. Remember, your belief state is
[1066.88s -> 1069.92s]  not the vector of thetas. It's actually a probability distribution
[1069.92s -> 1073.60s]  over thetas. So even in the simple binary reward
[1073.60s -> 1077.28s]  bandit, the thetas correspond to probability of getting a reward of one.
[1077.28s -> 1080.96s]  The p-hat of theta now needs to be some parametric class, maybe a bunch of
[1080.96s -> 1083.92s]  beta distributions. You could have covariances between the
[1083.92s -> 1087.44s]  different thetas. So it's potentially a really complex belief
[1087.44s -> 1089.76s]  state.
[1090.08s -> 1093.52s]  And the cool thing about bandits is that you can provably do
[1093.52s -> 1099.04s]  very well with much simpler strategies than solving this full POMDP. And the
[1099.04s -> 1102.96s]  way that you would quantify doing well is by quantifying the regret of your
[1102.96s -> 1107.68s]  strategy relative to how well actually solving the POMDP does.
[1107.68s -> 1111.68s]  So when we say that a particular exploration strategy is optimal,
[1111.68s -> 1115.60s]  what we really mean is that it is not much worse than actually solving the
[1115.68s -> 1119.12s]  POMDP. And not much worse is usually defined in
[1119.12s -> 1123.28s]  a kind of an O, in a big O sense.
[1124.16s -> 1127.68s]  So how do we measure the goodness of an exploration algorithm? Well, we do it in
[1127.68s -> 1130.48s]  terms of regret. And regret is the difference from the
[1130.48s -> 1136.24s]  optimal policy at time step capital T. So you can write the regret
[1136.24s -> 1140.64s]  as capital T times the expected value
[1140.64s -> 1144.56s]  of the reward of A star, that's the optimal policy,
[1144.56s -> 1148.08s]  minus the sum of rewards that you actually got. So the optimal policy will
[1148.08s -> 1151.36s]  always take A star, and that means that if you're going
[1151.36s -> 1155.12s]  for capital T steps, it'll be capital T times the expected
[1155.12s -> 1159.52s]  reward of A star. So that's what the optimal policy will do.
[1159.52s -> 1162.80s]  And then your regret is the difference between that and the sum of rewards
[1162.80s -> 1167.52s]  that you've actually gotten from running your strategy.
[1168.64s -> 1172.32s]  So this is the expected reward of the best action, the best you can hope for
[1172.32s -> 1176.16s]  in expectation, and this is the actual reward of the
[1176.16s -> 1179.76s]  action that was actually taken.
[1179.84s -> 1183.04s]  All right, so in the next portion I'm going to talk about how we can
[1183.04s -> 1187.04s]  minimize regret in terms of closing the gap between our
[1187.04s -> 1192.64s]  tractable strategies and this POMDP that we've defined.
