# Detected language: en (p=1.00)

[0.00s -> 5.76s]  The next class of methods that I'm going to describe take a different approach to address
[5.76s -> 8.36s]  the outer distribution action problem.
[8.36s -> 12.16s]  Instead of trying to somehow control the actor, what these methods are going to do
[12.16s -> 18.92s]  is directly repair those overestimated actions in the Q function.
[18.92s -> 24.84s]  So as we discussed before, the problem is that when you end up picking outer distribution
[24.84s -> 30.80s]  actions in the Bellman backup, you select the ones that have maximal value, and therefore
[30.80s -> 36.80s]  they're the ones that maximally overestimate erroneously.
[36.80s -> 41.60s]  So the picture that I had before in the Monday lecture was that if this green curve represents
[41.60s -> 44.76s]  the true function and the blue curve is your fit, even though the blue curve is a
[44.76s -> 49.48s]  good fit in most places, if you pick the point where the blue curve is maximized, it'll
[49.48s -> 53.46s]  be the one with the largest error in the positive direction.
[53.46s -> 54.80s]  So here's an idea.
[54.80s -> 59.18s]  What if we modify the objective for training the Q function, have our standard Bellman
[59.18s -> 64.38s]  error minimization just like before, and then another term that explicitly seeks to
[64.38s -> 69.60s]  find actions with high Q value and then minimize their value.
[69.60s -> 75.82s]  So notice that mu here is selected so that the expected Q values under mu are as large
[75.82s -> 79.86s]  as possible, and then Q is trained to minimize those values.
[79.86s -> 84.02s]  So intuitively what this procedure should do is find these erroneous peaks and push
[84.02s -> 85.02s]  them down.
[85.02s -> 91.02s]  In fact, it can actually be shown that if alpha is chosen appropriately, the resulting
[91.02s -> 96.10s]  Q function is a lower bound on the true Q function.
[96.10s -> 101.62s]  Now we can actually work out a better objective than this.
[101.62s -> 104.62s]  The problem with this objective is that it can be a little too pessimistic because we're
[104.62s -> 110.28s]  pushing down all the Q values, which means that the Q function will recover, will basically
[110.28s -> 114.24s]  never be the right Q function no matter how much data we have.
[114.24s -> 119.30s]  So what we can do instead is we can have this term that always pushes down the Q values,
[119.30s -> 124.34s]  and we can add an additional term that pushes up on Q values in the data set.
[124.34s -> 128.86s]  Now this might at first seem like a really strange thing because we're combating overestimation,
[128.86s -> 134.66s]  and somehow we're doing this by actually maximizing Q values in the data set.
[134.66s -> 137.84s]  But let's imagine what this procedure will do in practice.
[137.84s -> 143.54s]  If all of the large Q values are for actions that are in the data set, then these
[143.54s -> 148.42s]  two terms should more or less balance out because mu will select actions in the data
[148.42s -> 152.74s]  set to minimize their value, and then the second term will maximize their value, and
[152.74s -> 158.34s]  the two will basically balance out and have very little net effect.
[158.34s -> 163.10s]  If however the large Q values are for actions that are very different from the actions in
[163.10s -> 167.70s]  the data set, the first term will push them down, and the second term will instead
[167.70s -> 171.70s]  push up on the actions in the data set, which means that the next time around mu
[171.70s -> 174.58s]  is going to select actions closer to the data set.
[174.58s -> 179.74s]  So in a sense you can think of it as a kind of a feedback process where the more we
[179.74s -> 184.02s]  end up with large Q values out of distribution, the more these two terms together push
[184.02s -> 187.58s]  them back into the distribution, and once they're pushed all the way back, then the two
[187.62s -> 191.34s]  terms more or less cancel out.
[191.34s -> 197.38s]  With this more nuanced version of the objective, it's no longer guaranteed the learned Q function
[197.38s -> 202.26s]  is a lower bound on the true Q function for all states and actions, but it turns
[202.26s -> 207.54s]  out that it's still guaranteed in expectation over the policy for all states, which is
[207.54s -> 210.82s]  all we really want, because ultimately what we care about is not overestimating the
[210.82s -> 215.18s]  value of the current policy.
[215.18s -> 220.82s]  So this combined objective here I'm going to refer to as LCQL, and what I'll discuss
[220.82s -> 226.38s]  next is how to actually implement this in practice.
[226.38s -> 232.50s]  So the general structure of an algorithm using this idea will be something like this.
[232.50s -> 237.10s]  Update your approximate Q function using LCQL and using your data set, and step two
[237.10s -> 243.78s]  is update your policy in the usual way to maximize the expected value, and if the
[243.82s -> 248.58s]  actions are discrete, the policy is just the argmax policy, so that's basically exactly
[248.58s -> 249.58s]  the same as in Q-learning.
[249.58s -> 254.30s]  In practice, when you code the sub, you wouldn't even have the explicit policy computation
[254.30s -> 258.26s]  in step two, you would just plug in the max.
[258.26s -> 263.70s]  But if the actions are continuous, then you could have a separate explicit actor pi theta
[263.70s -> 266.22s]  and update that actor in the usual way.
[266.22s -> 270.26s]  So in the discrete action cases, this really does look like Q-learning with two additional
[270.26s -> 271.26s]  terms.
[271.26s -> 274.78s]  In the continuous action case, typically you would do this as a Q-function actor-critic
[274.78s -> 276.50s]  algorithm.
[276.50s -> 281.34s]  So the actor training in that case is exactly the same as in a regular actor-critic method.
[281.34s -> 285.58s]  The only thing that changes is the loss function for the critic with the addition
[285.58s -> 293.38s]  of these two terms, the pushing down and the pushing up term.
[293.38s -> 296.42s]  So this is the critic objective.
[296.42s -> 301.22s]  Now in practice, if you want to actually implement this, what you would do is you would
[301.22s -> 307.38s]  add a little regularizer for mu, let's call it R of mu, and for appropriate choices of
[307.38s -> 310.80s]  that regularizer, it turns out that there are very convenient ways to implement the
[310.80s -> 315.82s]  subjective that don't require actually computing mu directly.
[315.82s -> 322.42s]  So a very common choice for this regularizer is to use the entropy of mu.
[322.42s -> 330.06s]  This essentially amounts to saying we want to get mu to have high entropy, and we wanted
[330.06s -> 333.10s]  to capture actions with high Q-values.
[333.10s -> 337.84s]  So it's a kind of maximum entropy regularization.
[337.84s -> 341.42s]  It turns out that if you do this, the optimal choice for mu is proportional to the
[341.42s -> 343.42s]  exponential of the Q-values.
[343.42s -> 349.82s]  Furthermore, it turns out to also be the case that the expected value under mu of
[349.82s -> 356.22s]  the Q-function is the log sum exp of the Q-values.
[356.22s -> 365.10s]  So that's kind of a convenient property, and if you have discrete actions and you use
[365.10s -> 369.18s]  this maximum entropy regularization, then you don't even need to construct mu
[369.18s -> 377.46s]  explicitly, you can simply minimize the log sum exp of the Q-values at every state.
[377.50s -> 380.42s]  So that's very convenient.
[380.42s -> 384.34s]  So for discrete actions, just literally calculate this quantity directly.
[384.34s -> 389.30s]  For continuous actions, you could use importance sampling to estimate the expected value under
[389.30s -> 391.54s]  mu of the Q-function.
[391.54s -> 395.78s]  So you could sample actions either uniformly or random or from the current policy and
[395.78s -> 400.82s]  then re-weight them by using the exponential Q, by basically leveraging the fact that
[400.82s -> 406.18s]  the optimal mu is proportional to the exponential Q, and then use that to estimate this expectation.
[406.18s -> 409.78s]  And both of these are perfectly valid choices, so for discrete action, just directly using
[409.78s -> 412.58s]  this log sum exp formula tends to work really well.
[412.58s -> 417.26s]  For continuous actions, something like importance sampling tends to work pretty well.
[417.26s -> 420.10s]  And even though it's importance sampling and we would ordinarily be worried about
[420.10s -> 423.90s]  high variance, remember that here we're not multiplying together importance weights
[423.90s -> 426.42s]  over many time steps, it's just for one time step.
[426.42s -> 430.70s]  So that actually works decently well.
[430.70s -> 433.54s]  So that's basically what you would need to implement CQL.
[433.54s -> 439.82s]  You would use the general design on this slide to implement the critic update, and then the
[439.82s -> 442.98s]  structure of the algorithm would alternate between updating the critic and updating the
[442.98s -> 447.02s]  actor if you're doing continuous actions, or if you're doing discrete actions, just
[447.02s -> 450.34s]  use the argmax policy on the second line of the critic objective.
