# Detected language: en (p=1.00)

[0.00s -> 3.44s]  He brings us to this meeting here with us at UC Berkeley.
[3.44s -> 6.36s]  And actually, a lot of the materials
[6.36s -> 9.04s]  that we covered in our offline RL lectures
[9.04s -> 11.76s]  was work that was done in part by Aviro.
[11.76s -> 14.80s]  And also, pretty much all the materials in our RL theory
[14.80s -> 17.64s]  lecture was just copied from a lecture that Aviro made.
[17.64s -> 21.28s]  So he's contributed to a very large portion of this class.
[21.28s -> 24.24s]  And today, he'll tell us about pre-training and utilizing
[24.24s -> 27.64s]  large models with offline RL.
[27.64s -> 30.16s]  Thank you.
[30.16s -> 33.16s]  Yeah, so today, I'll talk to you about offline reinforcement
[33.16s -> 36.22s]  learning for pre-training and utilizing large models.
[36.22s -> 38.76s]  And these are large models not in the context of NLP
[38.76s -> 43.08s]  or language, but for robotics, for many decision-making
[43.08s -> 44.72s]  problems down the line.
[44.72s -> 48.36s]  So yeah, so you guys have all studied offline reinforcement
[48.36s -> 48.84s]  learning.
[48.84s -> 51.00s]  I think there are two lectures on this now.
[51.00s -> 52.96s]  The idea behind offline RL is simple.
[52.96s -> 57.12s]  You don't want to learn by online trial
[57.16s -> 58.48s]  interaction with the environment,
[58.48s -> 60.84s]  but rather you exist in interaction data
[60.84s -> 63.12s]  for learning policies that can maximize the world.
[63.12s -> 64.64s]  So this is the standard paradigm.
[64.64s -> 66.52s]  You looked at many algorithms for doing this,
[66.52s -> 68.92s]  model-based algorithms, Q-learning, et cetera,
[68.92s -> 69.76s]  et cetera.
[69.76s -> 71.64s]  But I'm not going to those algorithms here.
[71.64s -> 73.04s]  But this is the general paradigm
[73.04s -> 75.72s]  that all of those algorithms try to follow.
[75.72s -> 79.44s]  Now, what this kind of reminds me
[79.44s -> 81.76s]  when I think about this in the context of general machine
[81.76s -> 84.24s]  learning is the standard ML pipeline
[84.24s -> 86.36s]  of taking some training data.
[86.52s -> 88.72s]  In this case, it was existing offline data,
[88.72s -> 91.24s]  training a model on this data, and then deploying this model
[91.24s -> 92.44s]  down on the real task.
[92.44s -> 95.12s]  So this kind of a paradigm of offline RL
[95.12s -> 97.72s]  addressed to the standard or typical machine learning
[97.72s -> 100.64s]  pipeline of data, model, train it with some objective,
[100.64s -> 102.64s]  and then deploy the model.
[102.64s -> 105.96s]  But if you look at what currently we do in machine
[105.96s -> 109.12s]  learning, what is becoming more and more popular and more
[109.12s -> 112.44s]  and more utilized out there, is a slightly different paradigm
[112.44s -> 114.56s]  where the picture changes quite a bit.
[114.56s -> 118.76s]  So rather than going from data directly to data to,
[118.76s -> 120.12s]  rather than data to train a model
[120.12s -> 121.72s]  and then to deploy the model, now
[121.72s -> 123.88s]  we want to do something called pre-training, which
[123.88s -> 126.08s]  is take lots and lots of data, which is probably not
[126.08s -> 128.40s]  very related to your downstream task.
[128.40s -> 130.36s]  But you want to train a general model in all
[130.36s -> 131.12s]  of this data.
[131.12s -> 133.20s]  And then when it comes to using this model,
[133.20s -> 136.12s]  you want to run this model not directly in the real world
[136.12s -> 137.60s]  or directly on your real problem,
[137.60s -> 139.64s]  but rather we want to fine-tune this model
[139.64s -> 141.44s]  to a downstream task that we care about.
[141.44s -> 143.80s]  So we want to learn generalist models via pre-training
[143.84s -> 146.44s]  and then utilize these generalist models
[146.44s -> 148.84s]  via some kind of fine-tuning on a downstream task.
[148.84s -> 152.16s]  And the classic example of this is large-language models,
[152.16s -> 155.36s]  all sorts of foundation models that you can put.
[155.36s -> 156.96s]  So what I wanted to talk about today
[156.96s -> 159.96s]  was how can you move to a similar paradigm
[159.96s -> 161.40s]  when you think about offline art,
[161.40s -> 163.32s]  when you think about decision-making problems?
[163.32s -> 165.56s]  How can offline reinforcement learning give you
[165.56s -> 167.56s]  a way to realize this sort of a paradigm
[167.56s -> 170.76s]  for decision-making and control type of tasks?
[170.76s -> 173.40s]  So what I want to do now is not go away
[173.40s -> 175.88s]  from this paradigm of taking existing data
[175.88s -> 178.84s]  and producing policies that maximize the world,
[178.84s -> 181.76s]  but rather think of offline art as this other paradigm,
[181.76s -> 184.52s]  which can take not just your given task data,
[184.52s -> 186.72s]  but all possible relevant data for your problem.
[186.72s -> 189.80s]  So this could include any kind of robot data
[189.80s -> 192.00s]  that exists out there, any kind of game-playing data
[192.00s -> 193.92s]  that exists out there, any kind of data
[193.92s -> 196.88s]  from hospitals for hospital decision-making problems,
[196.88s -> 197.64s]  et cetera.
[197.64s -> 200.44s]  So all of that data put together now
[201.00s -> 203.04s]  to not just produce policies that are good
[203.04s -> 204.20s]  at maximizing reward,
[204.20s -> 206.44s]  but rather good retrained initializations,
[206.44s -> 208.28s]  good features, good representations,
[208.28s -> 210.20s]  but whatever you can think of,
[210.20s -> 213.00s]  which is useful for now fine-tuning
[213.00s -> 215.68s]  to a downstream scenario that you care about.
[215.68s -> 217.28s]  So what I want to talk about in today's talk
[217.28s -> 219.28s]  is how can you realize such a pipeline?
[219.28s -> 220.80s]  How can you think of offline art and methods
[220.80s -> 222.40s]  and how you can extend all those recipes
[222.40s -> 224.28s]  you have learned about in this class
[224.28s -> 226.40s]  to enable something like this in our picture
[226.40s -> 227.96s]  instead of that picture above?
[228.68s -> 233.68s]  So more specifically, we dissect this sort of high-level goal
[235.20s -> 236.92s]  into three different parts.
[236.92s -> 239.08s]  And to motivate those three different parts,
[239.08s -> 241.92s]  we look at what different components
[241.92s -> 243.00s]  in this picture look like.
[243.00s -> 245.64s]  So if I want to go from this picture of taking some data,
[245.64s -> 249.40s]  training a model on that and producing a downstream policy
[249.40s -> 252.32s]  to the second picture, what do my components look like?
[252.32s -> 254.68s]  The first component is being able to learn
[254.68s -> 256.32s]  from arbitrary sources of data.
[257.00s -> 258.96s]  So if I want to realize this picture,
[258.96s -> 261.08s]  I want to take not just data for my given task,
[261.08s -> 264.24s]  but arbitrary data sources, and I'll train on it.
[264.24s -> 266.40s]  And so one concrete illustration of this
[266.40s -> 269.32s]  that we talked about today is this setting
[269.32s -> 272.24s]  of using human video data for robotics.
[272.24s -> 274.12s]  So if I want to train a robot,
[274.12s -> 275.76s]  but I don't just want to use robot data,
[275.76s -> 279.36s]  but lots and lots of internet-based video data,
[279.36s -> 281.40s]  how can I use it for learning good policies?
[281.40s -> 283.24s]  I'll talk about that then.
[283.24s -> 284.88s]  I'll also talk about scaling up.
[284.88s -> 286.76s]  So when there's more and more data,
[286.76s -> 288.44s]  you want to use bigger and bigger models.
[288.44s -> 291.04s]  So how can you enable these offline reinforcement learning
[291.04s -> 294.48s]  methods to scale up to be able to train
[294.48s -> 295.60s]  large and large models?
[295.60s -> 299.00s]  So I'll talk about some stuff along that axis.
[299.00s -> 302.80s]  And I'll talk about some initial stuff
[302.80s -> 305.16s]  you've done for building fine-tuning algorithms,
[305.16s -> 307.24s]  so algorithms that can allow us to take
[307.24s -> 310.36s]  general initializations and then train them
[310.36s -> 313.88s]  with limited amount of data for your given downstream problem
[313.88s -> 316.72s]  to make your initialization be better and better
[316.72s -> 319.04s]  at that given task that you care about.
[319.04s -> 322.40s]  So data, scaling, and algorithm.
[322.40s -> 325.36s]  These are the three topics that we talked about today.
[325.36s -> 326.48s]  So let's get started.
[326.48s -> 328.84s]  I decided to switch the order a bit.
[328.84s -> 330.60s]  So I'm going to start with scaling first,
[330.60s -> 335.44s]  and then we'll talk about data and fine-tuning.
[335.44s -> 338.08s]  So scaling up, right?
[338.08s -> 342.56s]  So if you look at scaling and reinforcement learning,
[342.60s -> 345.68s]  here's a plot that I copied from the Cynthia paper, which
[345.68s -> 348.48s]  at this point is super, super old.
[348.48s -> 351.80s]  And these are all these dots that you see up on the slide
[351.80s -> 354.32s]  are basically a number of self-supervised learning
[354.32s -> 357.68s]  methods, which were out there state of the art at the time
[357.68s -> 361.12s]  when the Cynthia paper came out in 2019.
[361.12s -> 363.24s]  If you look at most of work in reinforcement learning
[363.24s -> 366.00s]  community, all of them usually train
[366.00s -> 367.28s]  models that are very small.
[367.28s -> 370.96s]  So they train models that all lie within this gray colored
[371.84s -> 374.92s]  box out there on this plot, whereas supervised learning
[374.92s -> 376.80s]  or just normal self-supervised learning
[376.80s -> 380.00s]  has gone way beyond this particular box.
[380.00s -> 382.60s]  So what I'm going to talk about today is obviously not
[382.60s -> 383.36s]  that revolutionary.
[383.36s -> 385.52s]  It doesn't move this box all the way from here
[385.52s -> 388.36s]  to massive models, but it does allow
[388.36s -> 391.68s]  us to go from whatever we had back in the RL community
[391.68s -> 393.88s]  at that time to models that are much bigger,
[393.88s -> 396.44s]  or about double the size of the models that people trained
[396.44s -> 396.96s]  on.
[396.96s -> 399.84s]  And remember that this was all done on not that much compute,
[399.84s -> 401.32s]  so we did not obviously have compute
[401.32s -> 403.24s]  to train the very, very big models.
[403.24s -> 405.32s]  But it is something better than what
[405.32s -> 409.60s]  existed out at the time that we are doing this work.
[409.60s -> 413.52s]  So before I go into the technical content,
[413.52s -> 415.20s]  I'll just quickly set up some notation,
[415.20s -> 417.28s]  and then we'll discuss the technical content.
[417.28s -> 420.76s]  So remember that here we are dealing
[420.76s -> 422.68s]  with this offline data set or this interaction
[422.68s -> 424.88s]  data that already exists.
[424.88s -> 426.56s]  We'll assume that the data consists
[426.56s -> 429.96s]  of tuples of four elements.
[429.96s -> 432.84s]  So here those four elements are the observation
[432.84s -> 435.12s]  xi or the state that is visible to your learning
[435.12s -> 438.16s]  algorithm, ai, which is an action that
[438.16s -> 441.16s]  was taken by the behavior policy, so the data collection
[441.16s -> 443.24s]  policy in your data set.
[443.24s -> 445.84s]  R of xi ai is your instantaneous reward value.
[445.84s -> 448.28s]  So this is the reward value in your data set
[448.28s -> 450.12s]  for this particular transition.
[450.12s -> 452.72s]  And xi plus 1 is the next observation
[452.72s -> 455.64s]  or the next state which was obtained when
[455.64s -> 458.96s]  you executed this action at that particular state xi.
[458.96s -> 461.40s]  So this is just a transition that
[461.40s -> 465.12s]  exists in typical replay buffers of policy Q-learning
[465.12s -> 466.00s]  style algorithms.
[466.00s -> 468.04s]  So we have access to a data set that is organized
[468.04s -> 470.68s]  in the form of these transition tuples, xi ai,
[470.68s -> 473.64s]  R, and xi plus 1.
[473.64s -> 475.60s]  OK, so back to scaling up.
[475.60s -> 479.24s]  So to start motivating why or what is even
[479.24s -> 481.24s]  the thing to do in scaling, should I
[481.24s -> 482.60s]  just throw more data in a bigger
[482.60s -> 484.20s]  model than the method would work?
[484.20s -> 486.76s]  To understand if that simple decision just works,
[486.76s -> 491.84s]  what we did was we tried to run a simple experiment where
[491.84s -> 495.36s]  we said, OK, rather than doing the standard domain
[495.36s -> 499.32s]  of Atari games, which you guys might have for sure done
[499.32s -> 501.36s]  in a homework assignment, I believe,
[501.36s -> 504.64s]  rather than training policies for one single Atari game,
[504.64s -> 506.76s]  what if we tried to just make the problem harder
[506.76s -> 508.64s]  such that it requires larger models to be
[508.64s -> 510.28s]  able to perform effectively?
[510.28s -> 512.52s]  And we made our problem harder by now,
[512.52s -> 515.24s]  instead of training one single policy on one game,
[515.24s -> 516.64s]  we are training one single policy
[516.64s -> 518.24s]  to play multiple games today.
[518.24s -> 520.40s]  This is just a simple baby step
[520.40s -> 523.16s]  instantiation of scaling up the problem
[523.16s -> 525.08s]  by simply trying to play in one policy
[525.08s -> 528.24s]  to play multiple games together.
[528.24s -> 530.16s]  Now, just this instantiation actually
[530.16s -> 532.28s]  gives you quite a bit of complexity.
[532.28s -> 534.88s]  So you now need a method that can deal with or that
[534.88s -> 536.82s]  can train on about 2 billion data points
[536.82s -> 538.80s]  because you can concatenate all these data
[538.80s -> 540.60s]  sets that exist for each game, and you'll
[540.60s -> 542.48s]  get 2 billion data points easily.
[542.52s -> 544.06s]  And all this data is quite suboptimal.
[544.06s -> 545.72s]  So you want to be able to take this data
[545.72s -> 547.68s]  and produce a good and a better policy,
[547.68s -> 550.00s]  hopefully, than anything that you've seen in this data.
[550.00s -> 552.20s]  So it's a challenging problem.
[552.20s -> 554.16s]  Turns out that if you take this problem
[554.16s -> 559.88s]  and benchmark some very simple RL and control methods,
[559.88s -> 561.64s]  you do find that when you simply
[561.64s -> 564.12s]  do imitation learning or filtered imitation learning
[564.12s -> 567.36s]  style approaches, there's actually quite a bit of benefit
[567.36s -> 569.48s]  that you had when you scale up your model.
[569.52s -> 572.52s]  So this plot, it's plotting the performance increase
[572.52s -> 574.72s]  that happens when you go from a smaller model
[574.72s -> 576.64s]  to a larger model for the learning algorithm.
[576.64s -> 579.16s]  And the blue bar that you see here
[579.16s -> 580.88s]  is a supervised learning style algorithm.
[580.88s -> 583.32s]  It's based on imitation learning.
[583.32s -> 585.44s]  So you see that when you go from a smaller
[585.44s -> 586.84s]  model to a large model, there's
[586.84s -> 589.96s]  a huge boost in performance with imitation learning style
[589.96s -> 592.56s]  methods in this particular scenario.
[592.56s -> 595.00s]  But if you take off-the-shelf offline RL methods,
[595.00s -> 597.08s]  in this case, it's reserved-active learning.
[597.08s -> 598.84s]  I did not put the equation for it here,
[599.36s -> 602.08s]  because I know you guys have covered this in the lecture.
[602.08s -> 604.28s]  You find that actually with offline RL methods,
[604.28s -> 606.40s]  the amount of performance increase
[606.40s -> 610.12s]  as a result of scaling your model capacity is quite low.
[610.12s -> 611.68s]  And in fact, if you do this a lot,
[611.68s -> 613.08s]  your performance might even start
[613.08s -> 615.16s]  to degrade when you use the larger model.
[615.16s -> 617.48s]  And for those of you who are interested in the details,
[617.48s -> 620.04s]  the model here is basically going from a smaller ResNet
[620.04s -> 622.60s]  to a larger ResNet, because these games require
[622.60s -> 625.36s]  to train on pixel images and convert that
[625.36s -> 626.32s]  into discrete actions.
[626.32s -> 628.36s]  So you can use the ResNet architecture in this case
[628.40s -> 630.08s]  that we used.
[630.08s -> 632.64s]  So clearly, the thing that is visible here
[632.64s -> 635.20s]  is that performance of the training algorithm
[635.20s -> 637.20s]  or performance of the policy that you get
[637.20s -> 639.72s]  starts to decrease when you scale up model capacity
[639.72s -> 642.32s]  for offline RL methods, whereas that's not quite
[642.32s -> 644.36s]  the case with just imitation learning methods,
[644.36s -> 647.00s]  as you see up there.
[647.00s -> 649.88s]  So this was kind of a counter-intuitive finding,
[649.88s -> 652.04s]  and we didn't know what to do about it.
[652.04s -> 655.96s]  So we tried to sort of ask some basic questions that
[655.96s -> 658.76s]  can help us understand why supervised learning
[658.76s -> 663.08s]  methods can scale up well, but RL methods may not be able to.
[663.08s -> 665.56s]  So in particular, rather than thinking
[665.56s -> 670.00s]  about absolute reasons for why RL or offline RL methods may
[670.00s -> 672.48s]  be hard to get to work with large models,
[672.48s -> 675.64s]  we tried to do a comparative understanding of why
[675.64s -> 677.84s]  supervised learning methods can scale well,
[677.84s -> 681.00s]  but offline RL methods may not be very good at scaling up.
[681.00s -> 682.76s]  So we tried to do a comparative analysis
[682.76s -> 684.88s]  with the hope that, OK, if you can understand
[684.88s -> 688.40s]  why this difference exists, we can hopefully somehow compensate
[688.40s -> 690.28s]  for the difference in our learning algorithm
[690.28s -> 693.80s]  when we run these offline RL methods.
[693.80s -> 698.88s]  So if you see one of the lines of a theoretical machine
[698.88s -> 702.96s]  learning literature, they propose an explanation for why
[702.96s -> 705.16s]  with very large models, with very large or very
[705.16s -> 706.66s]  overparameterized models, meaning
[706.66s -> 708.08s]  having many, many more parameters
[708.08s -> 710.64s]  than the intrinsic dimensions of the data,
[710.64s -> 713.32s]  why even in such cases supervised learning methods
[713.32s -> 715.32s]  can work pretty well.
[715.32s -> 717.12s]  And this is this theory of what's
[717.12s -> 719.88s]  called implicit regularization.
[719.88s -> 722.16s]  So the idea is that if you are training
[722.16s -> 724.16s]  a model with supervised learning that's called,
[724.16s -> 726.04s]  let's say that model has parameters theta,
[726.04s -> 729.48s]  and you're minimizing some lost function L theta.
[729.48s -> 732.04s]  And the biggest assumption here is that theta is very, very
[732.04s -> 732.52s]  big.
[732.52s -> 733.92s]  So you have many, many more parameters,
[733.92s -> 735.48s]  meaning you have a very large model,
[735.48s -> 737.48s]  and you're training this model on your data.
[737.48s -> 740.20s]  Turns out that if you do this with gradient descent
[740.20s -> 743.52s]  and more particularly stochastic gradient descent,
[743.52s -> 745.40s]  in supervised learning, it turns out
[745.40s -> 748.16s]  that you end up finding solutions that don't just
[748.16s -> 750.64s]  minimize this loss, L theta, but that
[750.64s -> 752.64s]  minimize a regularized version of this loss.
[752.64s -> 754.92s]  So you end up finding solutions that minimize L theta
[754.92s -> 756.56s]  plus this R theta.
[756.56s -> 757.96s]  And this R theta is not something
[757.96s -> 761.36s]  that you added during training, but it's just what's
[761.36s -> 762.68s]  called an implicit regularizer.
[762.68s -> 764.44s]  It's a regularizer that comes up
[764.44s -> 767.28s]  because your learning procedure, in this case stochastic gradient
[767.32s -> 770.32s]  descent, ends up preferring certain solutions
[770.32s -> 771.72s]  in your parameter landscape.
[771.72s -> 772.88s]  So think of it in this way.
[772.88s -> 774.26s]  There are many possible solutions
[774.26s -> 776.36s]  your learning can converge to, but you end up
[776.36s -> 779.16s]  finding solutions that minimize this regularizer
[779.16s -> 781.92s]  because your optimizer, in this case SGD,
[781.92s -> 786.96s]  ends up finding those solutions over other possible solutions.
[786.96s -> 790.12s]  So one of the intuitive characterizations
[790.12s -> 792.20s]  of this implicit regularizer term
[792.20s -> 797.44s]  is sort of the preference towards simple solutions.
[797.44s -> 801.32s]  So if you take an extremely hugely parameterized linear
[801.32s -> 803.16s]  regression problem, let's say, so it's a supervised learning
[803.16s -> 805.16s]  problem, what ends up happening typically
[805.16s -> 807.60s]  is you would end up finding solutions with a low parameter
[807.60s -> 810.26s]  norm, with a low norm of the parameter, the linear regression
[810.26s -> 811.64s]  parameters that you're training.
[811.64s -> 814.64s]  And that's an example of this implicit regularization
[814.64s -> 815.16s]  phenomena.
[815.16s -> 818.00s]  So you're finding solutions that are in some way simple,
[818.00s -> 820.44s]  and these simple solutions tend to generalize well,
[820.44s -> 822.84s]  even though you have a lot and lot of parameters
[822.84s -> 827.88s]  in your training, in your model that you're training.
[827.88s -> 829.44s]  So this is one of the explanations
[829.44s -> 831.66s]  for why you can still find good solutions
[831.66s -> 833.90s]  with supervised learning, even though you have a very large
[833.90s -> 835.82s]  model without overfitting, without running
[835.82s -> 838.58s]  into any other spurious issues, because this implicit
[838.58s -> 841.80s]  regularizer ends up finding good solutions.
[841.80s -> 844.16s]  Any questions here, by the way?
[844.16s -> 845.78s]  I realize that this might be something
[845.78s -> 848.04s]  that's probably not covered in this class,
[848.08s -> 850.40s]  so any questions here, actually?
[854.88s -> 856.48s]  OK, great.
[856.48s -> 861.52s]  So yeah, so there's one answer for why supervised learning
[861.52s -> 863.88s]  methods can work pretty well with large models, which
[863.88s -> 865.96s]  is that this implicit regularization
[865.96s -> 870.16s]  phenomena ends up referring solutions that perform well.
[870.16s -> 872.64s]  So what we tried to do in this line of work was to go
[872.64s -> 876.96s]  back now and see what this theory says
[877.00s -> 878.60s]  for the case of offline regularization.
[878.60s -> 880.84s]  So I hope, as I said earlier, that if you can figure out
[880.84s -> 883.08s]  the delta between offline RL and supervised learning,
[883.08s -> 886.80s]  we can hope to address this problem with offline RL
[886.80s -> 889.20s]  and scaling by taking insights from what
[889.20s -> 891.12s]  has made supervised learning work pretty well
[891.12s -> 893.48s]  with large models.
[893.48s -> 897.16s]  So what we did was we tried to theoretically derive
[897.16s -> 899.08s]  what this implicit regularizer looks
[899.08s -> 901.48s]  like for the case of offline RL.
[901.48s -> 903.36s]  And in particular, we are considering
[903.36s -> 906.12s]  these methods that train Q functions in this case.
[906.72s -> 908.16s]  So you have a neural network that
[908.16s -> 910.88s]  produces a Q function as output, taking as input
[910.88s -> 912.64s]  the state and the action.
[912.64s -> 915.84s]  And one piece of notation I'm going to define here,
[915.84s -> 918.68s]  I'm going to consider the last but one day features
[918.68s -> 920.64s]  on this network, or the last but one day
[920.64s -> 923.92s]  activations of this network as phi theta of xk.
[923.92s -> 925.72s]  So they are the features that you learn.
[925.72s -> 928.00s]  And this is sort of maybe somewhat,
[928.00s -> 930.44s]  I hope you can define features in different ways as well.
[930.44s -> 932.12s]  But for the context of today's talk,
[932.12s -> 934.80s]  for simplicity of understanding, let's stick to that definition
[934.80s -> 937.40s]  as what we will call Q.
[937.40s -> 941.32s]  So it turns out that in the case of reinforcement
[941.32s -> 943.80s]  learning, there's a major difference
[943.80s -> 947.72s]  between supervised objectives and Q-learning objectives that
[947.72s -> 949.96s]  cause this regularization to be very different.
[949.96s -> 953.32s]  This regularization is actually very differently manifested
[953.32s -> 955.36s]  for RL and supervised learning.
[955.36s -> 957.92s]  And to understand where that difference comes from,
[957.92s -> 960.28s]  I'm going to write down a few equations here.
[960.28s -> 962.32s]  So typically, when you train with Q-learning,
[962.36s -> 965.16s]  you take a Q function, a network Q theta there,
[965.16s -> 967.72s]  and you try to regress it to some targets one.
[967.72s -> 970.92s]  These are target values of your internet.
[970.92s -> 974.20s]  In supervised learning, these target values
[974.20s -> 975.32s]  are just constants.
[975.32s -> 977.68s]  They are just like, imagine a regression problem,
[977.68s -> 979.52s]  you're just going to regress to some constant values.
[979.52s -> 981.06s]  Let's say in this case, it's just
[981.06s -> 983.24s]  a reward for a given problem.
[983.24s -> 985.08s]  But for Q-learning or for offline RL,
[985.08s -> 986.96s]  these values are actually computed
[986.96s -> 990.00s]  with a previous copy of your own Q function
[990.48s -> 993.64s]  that you're training out there on the left-hand side of that,
[993.64s -> 996.40s]  or which is present in that objective out there.
[996.40s -> 998.28s]  So in some ways, your target value
[998.28s -> 1001.40s]  depends also on the Q function that you are training.
[1001.40s -> 1003.08s]  And the Q function that you're training
[1003.08s -> 1004.72s]  affects the subsequent target values,
[1004.72s -> 1007.20s]  creating a cyclic loop that's going
[1007.20s -> 1009.32s]  to make things very different from the learning
[1009.32s -> 1011.48s]  dynamics of supervised learning where you are trying
[1011.48s -> 1014.08s]  to regress to fixed targets.
[1014.08s -> 1016.76s]  Because of this difference, actually, if you write down
[1016.76s -> 1019.48s]  or if you calculate this regularizer
[1019.68s -> 1021.84s]  that I was talking about, it ends up
[1021.84s -> 1024.84s]  being very different for Q-learning or offline RL
[1024.84s -> 1027.60s]  and just standard supervised learning.
[1027.60s -> 1030.60s]  So for standard supervised learning, one example,
[1030.60s -> 1032.08s]  under certain modeling assumptions,
[1032.08s -> 1034.36s]  you can show that the regularizer will end up
[1034.36s -> 1037.24s]  preferring solutions with a low norm, with a low feature
[1037.24s -> 1037.72s]  norm.
[1037.72s -> 1040.48s]  So your regularizer is a function
[1040.48s -> 1043.44s]  that penalizes the norm of the features
[1043.44s -> 1045.76s]  phi, theta, for them that way.
[1045.76s -> 1047.24s]  This is actually good because if you
[1047.24s -> 1049.24s]  think of things like weight decay,
[1049.28s -> 1050.84s]  they are very similar in motivation.
[1050.84s -> 1052.28s]  This is kind of like weight decay,
[1052.28s -> 1055.28s]  but not on weights, but on features instead.
[1055.28s -> 1056.84s]  On the other hand, for offline RL now,
[1056.84s -> 1059.56s]  you have a much more complicated regularizer.
[1059.56s -> 1061.24s]  And in particular, there are two terms,
[1061.24s -> 1063.00s]  which we'll go over just in a bit.
[1063.00s -> 1064.72s]  But the regularizer is not the same
[1064.72s -> 1065.76s]  in a supervised learning.
[1065.76s -> 1069.56s]  And the reason is because of this regression to target values.
[1069.56s -> 1072.20s]  So the first term of this RL regularizer
[1072.20s -> 1073.16s]  is actually the same.
[1073.16s -> 1074.56s]  It's a term that tries to penalize
[1074.56s -> 1077.88s]  the norms of the features that you're training.
[1077.88s -> 1079.16s]  So that's a good term in some ways,
[1079.16s -> 1081.48s]  because you're trying to learn low norm features.
[1081.48s -> 1085.44s]  You're not trying to overfit in weird ways to your data.
[1085.44s -> 1089.00s]  But this second term now, it's a term that actually,
[1089.00s -> 1090.92s]  if you look at it a bit more carefully,
[1090.92s -> 1096.08s]  it looks like a dot product between phi theta at xi,
[1096.08s -> 1098.40s]  which is your state at the current step,
[1098.40s -> 1100.76s]  and phi theta at xi plus 1, which
[1100.76s -> 1103.08s]  is your states at the next step.
[1103.08s -> 1106.28s]  So it's something that kind of combines together
[1106.28s -> 1108.40s]  features that a given state action
[1108.40s -> 1112.00s]  x comma a with x prime, or the next state, xi plus 1
[1112.00s -> 1112.88s]  in this case.
[1112.88s -> 1115.12s]  And if you think about it, while the derivation is not
[1115.12s -> 1118.80s]  here, if you think about it, this is precisely something
[1118.80s -> 1121.40s]  that you would get from a target value-like thing,
[1121.40s -> 1123.60s]  because the target value has xi plus 1
[1123.60s -> 1127.64s]  weighed under your Q2 function, whereas the Q theta xa up there
[1127.64s -> 1130.08s]  in the objective is weighed on your current state and current
[1130.08s -> 1131.16s]  function.
[1131.16s -> 1133.96s]  So that's sort of the intuition for why this term comes from.
[1133.96s -> 1136.24s]  But now, if you look at what this term does,
[1136.24s -> 1139.20s]  it is actually a term that looks
[1139.20s -> 1141.24s]  like a dot product between two vectors,
[1141.24s -> 1144.08s]  dot product between phi theta at xi ai and phi theta
[1144.08s -> 1146.16s]  at xi plus 1 at xi plus 1.
[1146.16s -> 1149.08s]  And you have a negative sign in front of this.
[1149.08s -> 1153.96s]  Remember that if you recall the equation from the code,
[1153.96s -> 1156.32s]  here, you were trying to minimize the regularizer
[1156.32s -> 1159.36s]  when you run SVD on that loss.
[1159.36s -> 1161.44s]  So equivalently, here you will be trying
[1161.44s -> 1164.20s]  to minimize the RL regularizer.
[1164.24s -> 1166.28s]  Minimizing that regularizer with what
[1166.28s -> 1167.72s]  does that mean for the second term?
[1167.72s -> 1170.52s]  It basically means I'm going to maximize the second term
[1170.52s -> 1173.76s]  because there's a minus sign in front of it.
[1173.76s -> 1174.88s]  What does that mean?
[1174.88s -> 1176.76s]  That basically means I'm going to maximize
[1176.76s -> 1180.08s]  the dot product of features at xi ai and xi plus 1 ai
[1180.08s -> 1183.20s]  plus 1, which means I'm going to actually try to increase
[1183.20s -> 1185.60s]  the lengths of these vectors, because an easy way
[1185.60s -> 1187.80s]  to maximize dot product of two vectors
[1187.80s -> 1191.12s]  is to increase their lengths if the lengths are not bounded.
[1191.12s -> 1193.16s]  So in some ways, the second term
[1193.16s -> 1197.40s]  has an effect of conflicting with the first term, where
[1197.40s -> 1200.16s]  the first term is trying to reduce magnitude of features.
[1200.16s -> 1202.40s]  The second term is trying to increase the magnitude
[1202.40s -> 1205.32s]  of those features in this case,
[1205.32s -> 1208.68s]  because xi ai, 5 feet of xi ai appear in both the terms
[1208.68s -> 1210.04s]  in this case.
[1210.04s -> 1211.64s]  So there's some clear difference that
[1211.64s -> 1214.48s]  happens with this RL regularizer in comparison
[1214.48s -> 1216.40s]  with the supervised learning regularizer, which
[1216.40s -> 1218.20s]  you see up there.
[1218.20s -> 1219.04s]  Any questions here?
[1219.04s -> 1221.44s]  So I think I did a bunch of derivation orally.
[1221.44s -> 1225.28s]  So any questions?
[1225.28s -> 1225.76s]  Yes?
[1225.76s -> 1228.00s]  Is it just like what the implicit regularization
[1228.00s -> 1229.40s]  is approximately equal, so it's just
[1229.40s -> 1231.12s]  like actually grounded in?
[1231.12s -> 1232.16s]  That's a good question.
[1232.16s -> 1233.64s]  So you should check out the paper.
[1233.64s -> 1236.32s]  It's actually grounded in the, so it's,
[1236.32s -> 1237.88s]  so remember that here we're dealing
[1237.88s -> 1240.12s]  with stochastic range, in the sense that that's
[1240.12s -> 1240.92s]  not normal specifically, so you can't write down
[1240.92s -> 1242.32s]  the exact precise regularizer.
[1242.32s -> 1245.16s]  But you can say that the overall regularization effect is
[1245.16s -> 1248.84s]  in a band around the solution that you would get if you
[1248.84s -> 1250.40s]  were to minimize the exact regularization,
[1250.40s -> 1255.96s]  where that band depends on, the volume of that band
[1255.96s -> 1259.32s]  depends on the learning rate and a bunch of other things
[1259.32s -> 1262.20s]  that are dictated by the noise regularizer.
[1262.20s -> 1262.72s]  Thank you.
[1262.72s -> 1265.40s]  Yeah.
[1265.40s -> 1266.32s]  OK.
[1266.32s -> 1268.68s]  Great.
[1268.68s -> 1270.80s]  So the TLDR of this is basically,
[1270.80s -> 1273.92s]  the regularizer has two terms, where one term now
[1273.92s -> 1275.96s]  starts to conflict with the learning
[1275.96s -> 1279.00s]  that you would have gotten with supervised learning.
[1279.00s -> 1279.56s]  OK.
[1279.60s -> 1282.32s]  So this is what is happening.
[1282.32s -> 1284.60s]  And we have clearly learned to see that, OK,
[1284.60s -> 1286.68s]  the supervised learning regularizer actually
[1286.68s -> 1288.92s]  is not that harmful because supervised learning methods
[1288.92s -> 1290.12s]  clearly do scale.
[1290.12s -> 1292.36s]  Whereas in RL, we find that these methods,
[1292.36s -> 1293.88s]  or at least our empirical design showed
[1293.88s -> 1295.80s]  that these methods did not scale.
[1295.80s -> 1297.40s]  So what can we do about it?
[1297.40s -> 1299.12s]  So we do a very empirical approach here.
[1299.12s -> 1301.48s]  And this is not, so this part is actually not something
[1301.48s -> 1303.48s]  that I can prove of the show.
[1303.48s -> 1305.36s]  But the empirical approach was very simple.
[1305.36s -> 1308.08s]  It said, well, if this second term is the only other term
[1308.08s -> 1311.12s]  that exists in the case of RL and not in supervised learning,
[1311.12s -> 1313.56s]  and somehow this is correlated with the fact
[1313.56s -> 1316.08s]  that RL does not scale very well, at least so far,
[1316.08s -> 1318.24s]  and supervised learning does, then what if we simply
[1318.24s -> 1319.40s]  try to undo this now?
[1319.40s -> 1321.24s]  What if we simply try to remove this term
[1321.24s -> 1325.48s]  by adding an explicit loss to my training algorithm that
[1325.48s -> 1326.72s]  undoes this second term?
[1326.72s -> 1329.20s]  So if I were to just simply add this term back,
[1329.20s -> 1332.04s]  such that my regularization, the net regularization,
[1332.04s -> 1334.44s]  looks the same in supervised learning and reinforcement
[1334.44s -> 1335.96s]  learning.
[1335.96s -> 1336.92s]  We did exactly that.
[1336.96s -> 1339.20s]  What we did was we simply took a Q-learning algorithm,
[1339.20s -> 1341.08s]  in this case, any offline RL algorithm,
[1341.08s -> 1343.52s]  and added the second term as a regularizer
[1343.52s -> 1347.48s]  to my training method in this case.
[1347.48s -> 1351.00s]  Turns out that just doing that, on this particular example
[1351.00s -> 1354.44s]  that we were talking about, it helped us improve
[1354.44s -> 1355.68s]  the scalability of our math.
[1355.68s -> 1359.20s]  So while these methods, while these offline RL methods
[1359.20s -> 1361.64s]  initially were reducing in performance
[1361.64s -> 1363.48s]  as they scaled up model capacity,
[1363.48s -> 1366.52s]  now if you added this regularizer to the training
[1366.52s -> 1368.24s]  and ran the same offline RL algorithm,
[1368.24s -> 1371.72s]  you can actually benefit from model capacity increase.
[1371.72s -> 1374.76s]  So this is performance increase in performance.
[1374.76s -> 1376.76s]  So percentage increase in performance
[1376.76s -> 1378.00s]  by scaling up model capacity.
[1378.00s -> 1379.68s]  And you can clearly see it now looks
[1379.68s -> 1382.00s]  very similar to how the performance of supervised
[1382.00s -> 1384.36s]  learning stays in this case.
[1384.36s -> 1386.00s]  And not just that, but if you
[1386.00s -> 1389.68s]  look at absolute performance of different methods,
[1389.68s -> 1392.28s]  so these are methods which include imitation learning,
[1392.28s -> 1394.26s]  the best prior method based on decision
[1394.26s -> 1397.94s]  transformer, which is some sort of filtered or fancy
[1397.94s -> 1401.78s]  mutation, as well as sort of just literally replaying
[1401.78s -> 1403.94s]  trajectories from the data, so the average performance
[1403.94s -> 1405.38s]  of the behavior policy.
[1405.38s -> 1406.94s]  Compared to all of this, we found
[1406.94s -> 1409.86s]  that combining this idea of regularization
[1409.86s -> 1411.66s]  with conservative Q-learning, which
[1411.66s -> 1414.10s]  is an offline RL approach we based off of,
[1414.10s -> 1416.34s]  it helped us get much better performance
[1416.34s -> 1418.14s]  on this particular benchmark.
[1418.14s -> 1420.46s]  In fact, this was sort of the first offline RL-based
[1420.46s -> 1422.10s]  approach, which is based on Q-learning,
[1422.66s -> 1424.74s]  to surpass the performance of the behavior
[1424.74s -> 1429.50s]  policy using a large enough model on this particular task.
[1429.50s -> 1431.62s]  So the high level intuition here
[1431.62s -> 1434.38s]  is even such simple looking tasks,
[1434.38s -> 1437.70s]  like Atari games, which are so standard for RL,
[1437.70s -> 1439.90s]  even they can be very hard when you think about scaling
[1439.90s -> 1441.66s]  if you just combine them together.
[1441.66s -> 1443.30s]  And it was very surprising to us
[1443.30s -> 1445.66s]  that no existing method was able to train
[1445.66s -> 1447.94s]  large enough models, even with all the best tweaks
[1447.94s -> 1449.94s]  that we would have discovered on this domain
[1449.94s -> 1453.66s]  in the single game setting with offline RL.
[1453.66s -> 1456.34s]  But with just this regularizer and the same exact other
[1456.34s -> 1458.58s]  setups, we were able to get much better performance
[1458.58s -> 1461.74s]  with large enough models in this setting.
[1461.74s -> 1464.42s]  OK, any questions here?
[1464.42s -> 1464.90s]  Yes.
[1464.90s -> 1469.26s]  Was adding this term both with small models?
[1469.26s -> 1470.66s]  That's a good question.
[1470.66s -> 1473.34s]  So we did try some small models.
[1473.34s -> 1478.62s]  I think I would say that it depends
[1478.66s -> 1480.38s]  on how small you are going with.
[1480.38s -> 1482.78s]  So if you take one single game and you
[1482.78s -> 1486.18s]  take a regular network, I think it does help.
[1486.18s -> 1489.10s]  But then if you take many games and a regular network,
[1489.10s -> 1490.18s]  it doesn't quite help.
[1490.18s -> 1493.54s]  So a regular network meaning a small enough model, basically.
[1493.54s -> 1496.54s]  So I think it's a little hard to say exactly.
[1496.54s -> 1499.50s]  And the reason is because it's hard to quantify precisely
[1499.50s -> 1501.54s]  when a model capacity is bigger
[1501.54s -> 1504.78s]  than the intrinsic capacity of the data.
[1504.78s -> 1506.78s]  So I think this is much more reliably helping
[1506.78s -> 1508.38s]  when you have a large enough model.
[1508.38s -> 1510.30s]  But it's hard to quantify.
[1510.30s -> 1512.06s]  Or if I were to do the counterfactual,
[1512.06s -> 1513.46s]  I think it's hard for me to draw
[1513.46s -> 1515.82s]  the line between where the model capacity exceeded
[1515.82s -> 1516.98s]  the data or not.
[1516.98s -> 1518.02s]  So that's the thing.
[1518.02s -> 1518.50s]  Yeah.
[1518.50s -> 1521.50s]  But how would it be harmful to performance?
[1521.50s -> 1522.50s]  That's a good question.
[1522.50s -> 1524.00s]  So I don't think we ever found it
[1524.00s -> 1526.34s]  to be harmful to performance.
[1526.34s -> 1528.74s]  So I did not cover one of the details here, which is,
[1528.74s -> 1529.90s]  well, if you're adding back this term,
[1529.90s -> 1532.06s]  you probably have a waiting term in that hyperparameter.
[1532.06s -> 1533.70s]  So that's a regularizer, right?
[1533.70s -> 1535.94s]  You can also do it in a way where
[1535.94s -> 1537.74s]  you use it as a normalization there
[1537.74s -> 1539.42s]  in your model, which is something
[1539.42s -> 1541.94s]  that I did not cover here and have discussed later.
[1541.94s -> 1543.86s]  In that case, you don't have a hyperparameter
[1543.86s -> 1545.10s]  and it won't help.
[1545.10s -> 1547.14s]  It won't hurt your performance.
[1547.14s -> 1549.86s]  So it's going to be pretty neutral if nothing happens
[1549.86s -> 1554.26s]  or it only improves if things work better.
[1554.26s -> 1556.50s]  OK.
[1556.50s -> 1557.62s]  OK, sounds good.
[1557.62s -> 1558.78s]  OK, let's move on.
[1558.78s -> 1560.14s]  So the takeaways of this section
[1560.14s -> 1561.62s]  are that large models, if you just
[1561.62s -> 1564.18s]  instantiate offline neural lively, they can actually hurt.
[1564.18s -> 1567.70s]  But then the simple sort of regularization schemes
[1567.70s -> 1571.18s]  or equivalent schemes that undo these bad parts of,
[1571.18s -> 1572.82s]  in this case, implicit regularization,
[1572.82s -> 1574.78s]  or more generally, I think anything
[1574.78s -> 1576.98s]  that explains the difference between supervised learning
[1576.98s -> 1579.46s]  and offline abstract objectives, undoing
[1579.46s -> 1582.82s]  bad parts of resulting terms from such an analysis
[1582.82s -> 1584.34s]  can already help us get methods
[1584.34s -> 1589.74s]  that can scale up pretty well in this sense.
[1589.74s -> 1593.82s]  OK, so that's the end for this part.
[1593.86s -> 1597.54s]  Let's go on to the next part now, which is about,
[1597.54s -> 1599.26s]  so far we talked about scaling, right?
[1599.26s -> 1600.62s]  So we were in a setting where we were thinking
[1600.62s -> 1601.50s]  about Atari games.
[1601.50s -> 1603.86s]  We still had all the data that kind of looks
[1603.86s -> 1606.50s]  like transitions and actions and rewards.
[1606.50s -> 1608.30s]  Everything was given to us.
[1608.30s -> 1611.10s]  But now we want to talk about a different question
[1611.10s -> 1614.82s]  of what can you do when you have arbitrary data out there.
[1614.82s -> 1617.46s]  So if your learning algorithm does not just
[1617.46s -> 1619.70s]  see data that is organized in the form of transitions
[1619.70s -> 1621.54s]  but something else.
[1621.54s -> 1624.06s]  And for this, I'm going to talk about one special case
[1624.06s -> 1626.86s]  of using human videos for robots.
[1626.86s -> 1630.38s]  So how can you pre-train robots with human videos?
[1630.38s -> 1633.10s]  So before I go into human videos,
[1633.10s -> 1636.54s]  let's look at what does an offline RL-based pre-training
[1636.54s -> 1639.26s]  or pre-training scheme for robots,
[1639.26s -> 1641.26s]  how could they even look like?
[1641.26s -> 1643.10s]  So if you only had, let's say, robot data,
[1643.10s -> 1645.94s]  what would you do with offline RL for pre-training robots?
[1645.94s -> 1649.38s]  Well, a simple way would be to take a bunch of robot data.
[1649.42s -> 1651.02s]  And you can take data which is already
[1651.02s -> 1653.42s]  collected in various maps.
[1653.42s -> 1655.74s]  You can, and this data could just
[1655.74s -> 1659.02s]  come from human tell-you operators collecting
[1659.02s -> 1660.58s]  rollouts for you.
[1660.58s -> 1661.94s]  You can take this data.
[1661.94s -> 1666.30s]  You can simply annotate the last state of every trajectory
[1666.30s -> 1668.26s]  or every rollout as having a plus one reward.
[1668.26s -> 1670.02s]  Because remember, these are demonstrations.
[1670.02s -> 1672.34s]  A human actually showed how to solve a task.
[1672.34s -> 1673.94s]  And you can simply now take this data
[1673.94s -> 1675.50s]  and run offline RL on it.
[1675.50s -> 1677.34s]  So this is just giant multi-task data.
[1677.34s -> 1680.06s]  You can put it into a replay buffer,
[1680.06s -> 1681.46s]  do task ID conditioning.
[1681.46s -> 1683.90s]  So you can define for each particular transition
[1683.90s -> 1686.14s]  and what task it comes from as a one-hot vector
[1686.14s -> 1688.62s]  and run offline RL on it.
[1688.62s -> 1691.58s]  Once you have this, you would have a general network
[1691.58s -> 1693.54s]  potentially, general policy, such
[1693.54s -> 1696.78s]  that now when you are given some amount of data
[1696.78s -> 1699.10s]  for a target domain, for a target task that you care
[1699.10s -> 1701.26s]  about, so these are probably not tasks that you care about.
[1701.26s -> 1703.70s]  These are tasks that just exist in your data set.
[1703.70s -> 1705.82s]  When you have some limited amount of data for your task
[1705.82s -> 1708.70s]  that you care about, you can now mix some batches
[1708.70s -> 1711.74s]  from pre-training and this new data
[1711.74s -> 1714.70s]  and simply fine-tune this model.
[1714.70s -> 1716.98s]  So this is a very simple offline RL-based recipe
[1716.98s -> 1719.10s]  for pre-training robot policies.
[1719.10s -> 1722.54s]  Take some broad robot data, pre-train via offline RL
[1722.54s -> 1724.46s]  on that data, and simply keep
[1724.46s -> 1727.06s]  running the same offline RL algorithm or input.
[1727.06s -> 1728.62s]  You can also do online fine-tuning
[1728.62s -> 1730.02s]  if you want to, although that's
[1730.02s -> 1732.38s]  not quite relevant for this section of this talk.
[1732.38s -> 1734.38s]  You can take this offline RL method
[1734.38s -> 1737.86s]  and run that now on your task-specific data at test time
[1737.86s -> 1739.90s]  when you are supposed to utilize this pre-trained
[1739.90s -> 1741.10s]  initialization.
[1741.10s -> 1742.82s]  So this is a very general-purpose recipe
[1742.82s -> 1745.18s]  for pre-training via offline RL.
[1745.18s -> 1747.06s]  This only uses robot data so far.
[1747.06s -> 1748.82s]  It has data which shows you actions.
[1748.82s -> 1750.32s]  It has data which shows you rewards.
[1750.32s -> 1753.06s]  It's organized in the form of projectors.
[1753.06s -> 1755.34s]  But for this part, what we want to do
[1755.34s -> 1756.90s]  is we want to take these recipes
[1756.90s -> 1759.66s]  and extend it to use human videos.
[1759.66s -> 1761.22s]  Why do we want to use human videos?
[1761.22s -> 1764.26s]  Well, if you think about all the data that exists out there,
[1764.26s -> 1767.78s]  even if you include multitasked robot data or data
[1767.78s -> 1770.56s]  from other robots, it's very tiny compared
[1770.56s -> 1773.06s]  to human video data that exists out there on YouTube
[1773.06s -> 1776.38s]  or in well-curated data sets such as ego40.
[1776.38s -> 1778.62s]  So it makes sense to utilize human video data
[1778.62s -> 1781.38s]  because it shows how humans interact with the real world,
[1781.38s -> 1784.06s]  and that would be useful for robot control.
[1784.06s -> 1786.50s]  But the challenge that comes up now is most of the video
[1786.50s -> 1789.02s]  data that exists out there has no actions in it.
[1789.02s -> 1792.06s]  And more so, humans are not robots.
[1792.06s -> 1795.70s]  Humans have, or robots are not humans.
[1795.70s -> 1797.10s]  Humans have five fingers.
[1797.10s -> 1799.38s]  Typical robots mostly operate on parallel geographers.
[1799.38s -> 1802.26s]  So there's a huge difference in capabilities and embodiments
[1802.26s -> 1804.62s]  that exist out there.
[1804.62s -> 1805.70s]  So what can we do?
[1805.70s -> 1808.34s]  If I'm given such data, can I still keep running
[1808.34s -> 1810.30s]  offline algorithm of this data?
[1810.30s -> 1811.86s]  The answer is not quite clear.
[1811.86s -> 1813.72s]  But in this line of work, what we did
[1813.72s -> 1815.14s]  was we showed that you can still
[1815.14s -> 1818.42s]  keep running offline algorithm methods for not learning
[1818.42s -> 1820.74s]  policies at this point, but rather for learning
[1820.74s -> 1823.30s]  useful representations or features.
[1823.30s -> 1826.50s]  So it turns out the very same Q-learning, value-learning sort
[1826.50s -> 1828.58s]  of algorithms that we talked about so far,
[1828.58s -> 1830.46s]  they can still give you useful features
[1830.46s -> 1833.66s]  or representations for now doing robot control
[1833.66s -> 1835.38s]  if you are willing to pre-train on videos
[1835.38s -> 1840.26s]  and then keep running the same scheme on robot data.
[1840.26s -> 1841.90s]  So what does this look like?
[1841.90s -> 1845.66s]  So here's a general pipeline that you can follow.
[1845.66s -> 1847.62s]  You can first start with all video data
[1847.62s -> 1850.38s]  that exists out there, train some kind of value function
[1850.50s -> 1853.62s]  on the data, and talk about what value functions can be trained.
[1853.62s -> 1854.98s]  This value function will give you
[1854.98s -> 1859.74s]  a useful encoder of images or frames of video
[1859.74s -> 1861.74s]  into representations, so a useful encoder
[1861.74s -> 1865.50s]  that compresses information and images into useful vectors.
[1865.50s -> 1869.26s]  You can now take this encoder and use
[1869.26s -> 1872.14s]  it to initialize an off-the-shelf offline
[1872.14s -> 1873.66s]  algorithm for pre-training robots.
[1873.66s -> 1875.42s]  So you can take this.
[1875.42s -> 1877.18s]  You can have Q-values and policies
[1877.18s -> 1880.10s]  on top of this encoder such that the policy that you
[1880.14s -> 1882.06s]  obtained now by training on robot data
[1882.06s -> 1884.26s]  can then be fine-tuned on your desired task.
[1884.26s -> 1885.90s]  So remember that pre-training recipe
[1885.90s -> 1887.54s]  I showed you, train on broad robot data
[1887.54s -> 1889.14s]  and then fine-tune on target data?
[1889.14s -> 1891.06s]  You can just add a new field before it,
[1891.06s -> 1893.94s]  which now trains this visual encoder part
[1893.94s -> 1896.82s]  of your network with videos.
[1896.82s -> 1899.22s]  And the last function here is just
[1899.22s -> 1902.38s]  simply training a value function which does not require actions,
[1902.38s -> 1903.86s]  because a value function is just
[1903.86s -> 1905.90s]  a function of your state or your observation
[1905.90s -> 1907.10s]  and not the action.
[1907.10s -> 1910.90s]  It's not a Q-function, this is.
[1910.90s -> 1915.54s]  OK, so now the question is, well,
[1915.54s -> 1917.22s]  why should this give you good features?
[1917.22s -> 1919.42s]  Why should this give you useful encodings?
[1919.42s -> 1923.86s]  Well, the answer is, if you train a visual encoder with value
[1923.86s -> 1926.10s]  functions, you are implicitly accounting
[1926.10s -> 1927.54s]  for the dynamics of the world.
[1927.54s -> 1929.22s]  The value functions, when you train them
[1929.22s -> 1931.42s]  with Bellman and back-ups with Bellman equations,
[1931.42s -> 1933.14s]  they are accounting for the dynamical structure,
[1933.14s -> 1934.98s]  the sequential structure of the world, which
[1934.98s -> 1936.46s]  is presumably useful because you're training
[1936.50s -> 1939.30s]  a similar kind of an object, a Q-function, or a policy
[1939.30s -> 1940.70s]  in the second and third place.
[1940.70s -> 1942.98s]  So that's why we would hope that training with value
[1942.98s -> 1944.94s]  functions could give you a useful representation,
[1944.94s -> 1947.66s]  useful feature for downstream policy learning,
[1947.66s -> 1949.82s]  even though you don't have any actions out there
[1949.82s -> 1953.02s]  on the data.
[1953.02s -> 1955.30s]  OK, so before I go into how can you
[1955.30s -> 1957.62s]  train these value functions, I want to just pause
[1957.62s -> 1961.34s]  and see if there's any questions.
[1961.34s -> 1962.34s]  Yes, yeah.
[1963.34s -> 1967.42s]  That's a good point.
[1967.42s -> 1971.82s]  I'm going to cover that just in a second, yeah.
[1971.82s -> 1976.46s]  OK, so yeah, so now the point is like, well, OK,
[1976.46s -> 1978.70s]  I talked about all this intuition for value learning,
[1978.70s -> 1979.62s]  but what can you do?
[1979.62s -> 1982.90s]  What kind of value functions can you even train on there?
[1982.90s -> 1984.58s]  A simple value function technically
[1984.58s -> 1986.66s]  looks like a function of the state, which
[1986.66s -> 1988.66s]  tells you some cumulative reward that you
[1988.66s -> 1991.66s]  would get by executing a given policy, in that case,
[1991.66s -> 1994.46s]  pi, for a given reward function, because the value
[1994.46s -> 1998.18s]  function sums up the cumulative reward for a given reward.
[1998.18s -> 1999.78s]  Now, in this case, I can choose
[1999.78s -> 2001.98s]  to define some kind of a reward function potentially,
[2001.98s -> 2003.86s]  and I can just say, well, my reward function
[2003.86s -> 2007.30s]  is whenever the human hand is close to a particular object,
[2007.30s -> 2009.26s]  some specific function like that.
[2009.26s -> 2010.70s]  But that's not quite good here,
[2010.70s -> 2012.42s]  because it will still give me policies,
[2012.42s -> 2014.34s]  or it will still give me value functions that
[2014.34s -> 2017.46s]  are very specific to the particular reward
[2017.46s -> 2018.58s]  function I define.
[2018.58s -> 2020.58s]  I can just define arbitrary reward functions here,
[2020.62s -> 2022.70s]  but they are going to be very specific to one
[2022.70s -> 2024.86s]  particular capability, whereas in the downstream,
[2024.86s -> 2027.82s]  my robot would be required to do many other tasks that are
[2027.82s -> 2030.70s]  not quite taught by the robot.
[2030.70s -> 2033.34s]  So somehow I want to go away from this particular way
[2033.34s -> 2036.46s]  of defining values, and I want to not have a reward
[2036.46s -> 2038.54s]  function out there at all.
[2038.54s -> 2040.70s]  So for doing that, what I can do is I can instead
[2040.70s -> 2043.42s]  try to take a general formulation of value functions,
[2043.42s -> 2045.34s]  such as goal-reaching value.
[2045.34s -> 2047.86s]  I can say, well, I want to train a value function
[2048.58s -> 2052.22s]  on video data that tells me what's
[2052.22s -> 2055.70s]  the total value I would get if I were to execute a given policy
[2055.70s -> 2058.90s]  pi for reaching arbitrary frames,
[2058.90s -> 2061.38s]  or reaching arbitrary goals in the video as well.
[2061.38s -> 2063.06s]  This is a very general reward function,
[2063.06s -> 2064.42s]  because it's a goal-reaching reward function.
[2064.42s -> 2066.94s]  I can define goals as any arbitrary frame in the video
[2066.94s -> 2070.30s]  that I have seen, even across frames, across not appearing
[2070.30s -> 2073.26s]  in the same video clip at all.
[2073.26s -> 2077.02s]  But that also ends up being quite specific.
[2077.06s -> 2078.74s]  And the reason why this is quite specific
[2078.74s -> 2080.90s]  is because it only considers one policy.
[2080.90s -> 2083.34s]  I'm only training a value function for one policy, which
[2083.34s -> 2085.42s]  means I'm only learning features that can represent
[2085.42s -> 2087.50s]  values for one particular policy pi,
[2087.50s -> 2089.30s]  even though it's for the multiple reward
[2089.30s -> 2091.78s]  functions in this case.
[2091.78s -> 2093.38s]  So well, I also don't want to do this,
[2093.38s -> 2095.06s]  because this is too specific.
[2095.06s -> 2097.02s]  So the obvious next step is that I can say, OK,
[2097.02s -> 2099.34s]  what if I train value functions, these goal-reaching value
[2099.34s -> 2101.74s]  functions, that can work well for all policies?
[2101.74s -> 2104.18s]  So I want to train a value function that can,
[2104.18s -> 2105.78s]  I want to train this network that
[2105.78s -> 2108.66s]  can represent value functions for all possible policies pi
[2108.66s -> 2111.46s]  and for all possible goals in this case,
[2111.46s -> 2114.38s]  so for all possible goal-reaching reward functions.
[2114.38s -> 2116.62s]  This ends up being overly broad.
[2116.62s -> 2119.34s]  And the reason is because when you think about robots,
[2119.34s -> 2120.98s]  they're not going to operate, and they're not
[2120.98s -> 2123.50s]  going to run any arbitrary sequence of random actions
[2123.50s -> 2123.86s]  out there.
[2123.86s -> 2124.94s]  All they're going to do is they're
[2124.94s -> 2127.46s]  going to run some very specific kinds of actions
[2127.46s -> 2129.70s]  that are going to accomplish tasks out there, not
[2129.70s -> 2131.38s]  arbitrary random actions, which
[2131.38s -> 2133.22s]  is what this particular formulation will still
[2133.22s -> 2134.94s]  consider, because it's training value
[2134.94s -> 2138.46s]  functions for all possible policies.
[2138.46s -> 2140.06s]  So what we ended up doing in this case
[2140.06s -> 2143.58s]  was a slightly different form, where we now
[2143.58s -> 2148.78s]  model a value function that only models the value,
[2148.78s -> 2151.02s]  the total reward that you will get for reaching
[2151.02s -> 2155.06s]  a particular goal G, when your policies are optimal policies,
[2155.06s -> 2157.58s]  optimal policies for some subset of tasks
[2157.58s -> 2158.86s]  that you care about.
[2158.86s -> 2161.58s]  So imagine if you only had optimal goal-reaching policies,
[2161.58s -> 2163.18s]  meaning given a goal, these policies
[2163.18s -> 2165.10s]  reach that goal as efficiently as possible.
[2165.10s -> 2166.74s]  You're only modeling value functions
[2166.74s -> 2170.34s]  for those particular policies for all goal-reaching reward
[2170.34s -> 2172.18s]  functions.
[2172.18s -> 2174.14s]  So in this case, the choice of reward function
[2174.14s -> 2176.10s]  is every goal-reaching reward function.
[2176.10s -> 2177.78s]  And the choice of policies are only
[2177.78s -> 2179.78s]  optimal policies which are useful in some way
[2179.78s -> 2181.86s]  rather than just arbitrary random policy.
[2181.86s -> 2183.90s]  So this strikes the right balance, or at least
[2183.90s -> 2186.22s]  empirically, this strikes a good balance between breadth
[2186.22s -> 2189.42s]  and depth, or breadth and specificity of the value
[2189.42s -> 2191.54s]  function that we're training.
[2191.74s -> 2193.94s]  I would encourage you to check out some more works.
[2193.94s -> 2196.74s]  So this is actually inspired from this work, the first one
[2196.74s -> 2200.66s]  here, which actually models this as an intent condition
[2200.66s -> 2201.66s]  value function.
[2201.66s -> 2203.66s]  But you should check that out for more detail.
[2203.66s -> 2206.18s]  And there's many other related works that's run out there.
[2206.18s -> 2207.58s]  But the high-level idea is you
[2207.58s -> 2209.38s]  want to choose a family of reward functions
[2209.38s -> 2212.14s]  and a family of policies for which you model values,
[2212.14s -> 2214.18s]  such that you can get good features that
[2214.18s -> 2216.30s]  are useful for a breadth of downstream tasks.
[2216.30s -> 2217.74s]  But then they're not too broad,
[2217.74s -> 2220.86s]  that they are just arbitrarily wide and not super useful.
[2220.90s -> 2222.26s]  Does that answer your question?
[2222.26s -> 2224.66s]  Yeah.
[2224.66s -> 2228.26s]  OK, so what we did was we simply used this value
[2228.26s -> 2229.90s]  function.
[2229.90s -> 2232.10s]  We put that into this pipeline.
[2232.10s -> 2234.22s]  So your value function now is this value function
[2234.22s -> 2237.78s]  for the on-board reaching tasks for certain optimal policies.
[2237.78s -> 2239.30s]  You get your visual encoder, throw it
[2239.30s -> 2241.50s]  into an offline RL pre-training scene.
[2241.50s -> 2243.74s]  So now you initialize your offline RL method
[2243.74s -> 2245.78s]  with this visual encoder you got.
[2245.78s -> 2247.50s]  Train a Q function and a policy.
[2247.50s -> 2249.90s]  In this case, it was conservative Q-learning.
[2249.90s -> 2253.42s]  And now you're ready to take that initialization,
[2253.42s -> 2255.34s]  find it on your targeted task on which you
[2255.34s -> 2258.22s]  will measure performance.
[2258.22s -> 2263.74s]  OK, so yeah, so before I go into some robot videos
[2263.74s -> 2265.98s]  and quantitative evaluations, it turns out
[2265.98s -> 2268.98s]  that this actually gives you useful features.
[2268.98s -> 2271.26s]  So if you were to plot, if you
[2271.26s -> 2273.70s]  were to take trajectories in your data set
[2273.70s -> 2276.54s]  and trajectories which are out of distribution on your data
[2276.54s -> 2277.82s]  set, so if you have a robot data set,
[2277.82s -> 2279.46s]  you can take some trajectories from it
[2279.50s -> 2281.82s]  and take some trajectories that are not in it.
[2281.82s -> 2283.82s]  And if you plot the values that you
[2283.82s -> 2286.14s]  learned as a function of the time step,
[2286.14s -> 2288.38s]  a particular state appears in the trajectory.
[2288.38s -> 2291.74s]  So think of this as essentially the time step
[2291.74s -> 2294.54s]  after the particular state appears in your data,
[2294.54s -> 2295.66s]  in your robot data.
[2295.66s -> 2298.98s]  And the y-axis is plotting the value function for that state
[2298.98s -> 2301.62s]  or the Q function for that state.
[2301.62s -> 2303.78s]  In this case, we find the Q values are much better.
[2303.78s -> 2306.06s]  So how do you read this plot?
[2306.06s -> 2309.70s]  In this case, the Q values, the ground truth Q values,
[2309.70s -> 2313.06s]  should actually increase as you go further into your data.
[2313.06s -> 2315.10s]  So they should always have an increasing trend,
[2315.10s -> 2316.98s]  monotonically increasing as you increase
[2316.98s -> 2319.26s]  the x-axis value, the time step.
[2319.26s -> 2320.78s]  And in this case, what we found
[2320.78s -> 2323.26s]  was our approach in videos, which
[2323.26s -> 2325.38s]  is the first column for the VPDR,
[2325.38s -> 2328.18s]  it ended up learning the value functions, which
[2328.18s -> 2331.70s]  are very consistent with this monotonically increasing trend.
[2331.70s -> 2334.10s]  Even on 4D data or out of distribution data,
[2334.10s -> 2335.82s]  there are some spikes here and there,
[2335.82s -> 2337.78s]  it's much better than not training
[2337.78s -> 2340.06s]  on video data at all, which is this place,
[2340.06s -> 2341.94s]  and other methods for training on video data,
[2341.94s -> 2343.42s]  which is this other code.
[2343.42s -> 2346.58s]  So here you see the values have this non-monotonic shape
[2346.58s -> 2348.42s]  as you increase the x-axis value.
[2348.42s -> 2349.86s]  But in this case, with our method,
[2349.86s -> 2352.46s]  it's only increasing throughout.
[2352.46s -> 2354.02s]  So it does learn useful features that
[2354.02s -> 2356.50s]  are good for representing downstream values
[2356.50s -> 2358.94s]  on the robot data.
[2358.94s -> 2360.62s]  You can also quantitatively measure this.
[2360.62s -> 2361.90s]  So you can actually measure some kind
[2361.90s -> 2364.06s]  of mean squared error between the ground truth value
[2364.06s -> 2366.50s]  function and the values that you learned.
[2366.50s -> 2371.10s]  And again, you find that using this approach with videos
[2371.10s -> 2373.58s]  actually works and gives you the smallest mean squared error
[2373.58s -> 2376.58s]  compared to the other approaches in this case.
[2376.58s -> 2378.42s]  So overall, these features are quite useful
[2378.42s -> 2381.18s]  for learning downstream value functions.
[2381.18s -> 2385.14s]  And when we actually ran some experiments with it
[2385.14s -> 2387.50s]  on real robots, when we took these policies,
[2387.50s -> 2389.46s]  finding them on a given target task,
[2389.46s -> 2390.98s]  you can actually find these policies
[2390.98s -> 2395.94s]  generalize pretty well in terms of object and zipper
[2395.94s -> 2396.66s]  variability.
[2396.66s -> 2398.50s]  Well, I think I need to play this videos.
[2398.50s -> 2402.86s]  Oh, OK.
[2402.86s -> 2405.70s]  I think I need to probably play this from here.
[2409.06s -> 2411.18s]  Yeah, so I mean, the policies end up
[2411.18s -> 2413.06s]  performing much better, much more robustly
[2413.06s -> 2416.74s]  than prior ways of, OK.
[2417.14s -> 2424.14s]  I think I'm here.
[2424.14s -> 2429.18s]  Yeah, so yeah, so these other approaches here.
[2429.18s -> 2430.90s]  The first three are basically different way
[2430.90s -> 2432.14s]  of playing on video.
[2432.14s -> 2435.62s]  PDR, which is just not using video, just using robot data.
[2435.62s -> 2438.70s]  And this is R approach, which uses videos and robots.
[2438.70s -> 2441.18s]  And you can see that this approach is, in general,
[2441.18s -> 2443.70s]  much more smooth and robust compared
[2443.70s -> 2446.70s]  to some of the other approaches that exist for either training
[2446.70s -> 2450.06s]  on video or not using video data at all.
[2450.06s -> 2452.46s]  And the same thing is true with distractor objects.
[2452.46s -> 2454.90s]  When you have other distractor objects in the scene,
[2454.90s -> 2458.74s]  the same thing is true in that case as well.
[2458.74s -> 2462.42s]  Quantitatively, we did a comprehensive evaluation.
[2462.42s -> 2467.34s]  And we found that actually using videos with R approach,
[2467.34s -> 2469.54s]  with training design functions, is actually
[2469.54s -> 2473.42s]  better than either not using videos, which is this column,
[2473.42s -> 2475.78s]  or doing other ways of using videos,
[2475.82s -> 2478.66s]  which are all of these different methods from prior work.
[2478.66s -> 2481.02s]  These different methods use self-supervised learning
[2481.02s -> 2481.50s]  on video.
[2481.50s -> 2484.70s]  So you train a master autoencoder
[2484.70s -> 2488.06s]  to get some useful representations by just simply
[2488.06s -> 2490.26s]  compressing and trying to reconstruct back
[2490.26s -> 2491.82s]  the images in your video frames.
[2491.82s -> 2493.98s]  Or you could do some sort of contrastive learning.
[2493.98s -> 2496.54s]  And all of those actually perform much worse
[2496.54s -> 2498.02s]  than what I would propose.
[2501.30s -> 2502.66s]  OK, any questions here?
[2505.78s -> 2509.06s]  Great.
[2509.06s -> 2511.94s]  OK, so I think I have nine minutes.
[2511.94s -> 2515.10s]  Let's see how much we can cover in this third section, which
[2515.10s -> 2517.22s]  is about fine-tuning or utilizing
[2517.22s -> 2518.82s]  these pre-trained initializations
[2518.82s -> 2520.58s]  from offline RL.
[2520.58s -> 2523.10s]  This is going to be a little bit more algorithmic.
[2523.10s -> 2526.66s]  So I'm happy to just talk about things at high level.
[2526.66s -> 2529.62s]  And then I can take questions after 6,
[2529.62s -> 2531.74s]  because I think we need to leave the room at 6.
[2531.74s -> 2534.98s]  But this part is based on this paper called CalQL, which
[2535.98s -> 2541.58s]  is going to be presented in Europe's next month.
[2541.58s -> 2544.98s]  OK, so the setting here that we care about
[2544.98s -> 2547.90s]  is online improvement of offline RL policy.
[2547.90s -> 2551.98s]  So if I gave you some data, you can run offline RL on it.
[2551.98s -> 2553.54s]  This is giant pre-trained data.
[2553.54s -> 2555.94s]  So you can get a pre-trained policy initialization.
[2555.94s -> 2557.62s]  Now I want to improve this policy
[2557.62s -> 2560.06s]  via limited amounts of online interaction
[2560.06s -> 2561.74s]  with the given task of interest.
[2561.74s -> 2563.90s]  So I want to specialize this policy on that task
[2563.90s -> 2568.46s]  with limited amounts of online actively collected interaction.
[2568.46s -> 2570.98s]  So in this setting, if you were to take a first step
[2570.98s -> 2573.26s]  and say, well, a simple approach to tapping the setting
[2573.26s -> 2575.70s]  is to simply take the same offline RL method
[2575.70s -> 2577.82s]  I was running and continue running it
[2577.82s -> 2580.62s]  with new online or on-policy data coming
[2580.62s -> 2581.78s]  into the replay buffer.
[2581.78s -> 2583.62s]  If I were to do this, you'd find
[2583.62s -> 2585.82s]  that most classes of methods out there
[2585.82s -> 2590.66s]  will show two different kinds of trends in their results.
[2590.66s -> 2592.86s]  So just to understand what this plot means,
[2592.86s -> 2594.18s]  I'll explain some of the reasons.
[2594.18s -> 2595.66s]  I'm plotting the on the x-axis.
[2595.66s -> 2598.30s]  I'm plotting the number of environment samples
[2598.30s -> 2600.10s]  collected during online fine-learning.
[2600.10s -> 2602.58s]  So the offline pre-training is done already.
[2602.58s -> 2604.58s]  And I'm just simply collecting new online data
[2604.58s -> 2607.30s]  to improve your policy, to refine your policy.
[2607.30s -> 2609.30s]  The y-axis is plotting the performance.
[2609.30s -> 2611.22s]  In this case, it's score or return.
[2611.22s -> 2612.50s]  They're about the same.
[2612.50s -> 2615.54s]  And the performance at zero starts off quite high
[2615.54s -> 2617.54s]  because you've already done offline pre-training
[2617.54s -> 2619.46s]  and your policy gets some non-trivial success.
[2619.46s -> 2622.70s]  So your policy gets about 50% success.
[2622.74s -> 2627.22s]  And I'm plotting two algorithms, IQL and the CQL
[2627.22s -> 2628.38s]  conservative Q-learning.
[2628.38s -> 2629.98s]  I think both of these algorithms
[2629.98s -> 2631.30s]  have been covered here.
[2631.30s -> 2632.74s]  And these algorithms are simply being
[2632.74s -> 2636.02s]  run as you start collecting online data.
[2636.02s -> 2637.86s]  The online data is put into the replay buffer
[2637.86s -> 2639.18s]  of the offline algorithm.
[2639.18s -> 2642.62s]  So it's a particular data set that you'll train on.
[2642.62s -> 2644.14s]  So what you can see clearly is
[2644.14s -> 2646.58s]  that there's two things happening here.
[2646.58s -> 2650.06s]  First, with CQL or conservative Q-learning,
[2650.06s -> 2652.58s]  you see there's a big dip in performance
[2653.46s -> 2657.94s]  So your model initially was getting 0.5, roughly.
[2657.94s -> 2660.34s]  It ended up suffering a big dip and then recovers back
[2660.34s -> 2664.34s]  to get an overall improvement eventually.
[2664.34s -> 2667.46s]  Whereas with IQL, you find that the improvement
[2667.46s -> 2669.90s]  is consistent, but it's much more slow.
[2669.90s -> 2672.18s]  It's much more sort of does not have a high slope.
[2672.18s -> 2676.58s]  It's like a lower slope than the CQL eventually.
[2676.58s -> 2680.70s]  So clearly, something seems kind of not quite right here
[2680.70s -> 2682.82s]  because ideally, what I would expect is a curve
[2682.82s -> 2684.90s]  that sort of surpasses all of these curves,
[2684.90s -> 2688.10s]  improves quite quickly, reaches the optimal performance
[2688.10s -> 2689.98s]  of one as quickly as possible.
[2689.98s -> 2692.42s]  But then it's also not very slow
[2692.42s -> 2694.98s]  and does not have any performance.
[2694.98s -> 2696.62s]  So with this optimization, we said,
[2696.62s -> 2697.94s]  well, something was wrong here.
[2697.94s -> 2700.26s]  And we wanted to dig deeper into understanding
[2700.26s -> 2702.38s]  why this data happens for these different methods.
[2702.38s -> 2703.78s]  And what can we do about this
[2703.78s -> 2706.98s]  to build a better fine-tuning algorithm?
[2706.98s -> 2709.22s]  So specifically, when you restrict yourself
[2709.22s -> 2711.78s]  to the case of CQL, what we did was
[2711.78s -> 2714.02s]  we tried to understand this unlearning phenomenon.
[2714.02s -> 2716.22s]  We tried to understand why this performance dip happens
[2716.22s -> 2717.86s]  right in the middle.
[2717.86s -> 2719.22s]  And for that, we plotted a bunch
[2719.22s -> 2723.94s]  of metrics, which includes many, many metrics.
[2723.94s -> 2725.50s]  Not all of them are plotted here.
[2725.50s -> 2727.90s]  But one metric that we found quite correlated
[2727.90s -> 2731.10s]  with performance dip was the average Q value.
[2731.10s -> 2733.02s]  So if you take your data set, which
[2733.02s -> 2735.18s]  includes the offline and offline data now,
[2735.18s -> 2738.54s]  you plot the average Q value of the learned Q network
[2738.70s -> 2739.58s]  on that data.
[2739.58s -> 2743.30s]  You find that this Q value has this sharp correction phase
[2743.30s -> 2746.54s]  where it changes in magnitude, precisely at the same time
[2746.54s -> 2748.46s]  when the return has this dip and the return
[2748.46s -> 2750.70s]  of the top spot.
[2750.70s -> 2752.42s]  So this was sort of an initial signal
[2752.42s -> 2754.66s]  that something about this unlearning
[2754.66s -> 2757.10s]  is related to shifts in magnitudes of these two
[2757.10s -> 2760.66s]  values that you see up here.
[2760.66s -> 2763.18s]  This is pretty much like an adjustment and scaling.
[2763.18s -> 2764.74s]  So your values are negative 40.
[2764.74s -> 2766.58s]  And now your values become minus 10
[2766.58s -> 2770.30s]  as you train for the online data.
[2770.30s -> 2773.82s]  So what we did was we tried to understand now why this happens.
[2773.82s -> 2775.62s]  And I want to first give you some intuition.
[2775.62s -> 2777.94s]  And this is a bit imprecise, but it
[2777.94s -> 2779.34s]  does the job of the intuition.
[2779.34s -> 2783.38s]  And then I'll show you some concrete evidence of this.
[2783.38s -> 2785.82s]  So if you think about what CQL does,
[2785.82s -> 2788.54s]  I want to explain why these Q values are super low
[2788.54s -> 2790.90s]  and why they adjust in scale.
[2790.90s -> 2793.74s]  So first question, why are these values super low?
[2793.74s -> 2795.50s]  If you look at what CQL does,
[2795.54s -> 2797.58s]  it basically trains your Q function
[2797.58s -> 2799.70s]  to fit the target Q values, which
[2799.70s -> 2802.82s]  is this temporal difference error term, Bellman error.
[2802.82s -> 2806.66s]  And it pushes down the high Q values
[2806.66s -> 2809.42s]  on outer distribution actions.
[2809.42s -> 2812.42s]  The TD term in this case is a relative loss.
[2812.42s -> 2815.26s]  All it says is your Q values must fit the target Q
[2815.26s -> 2817.78s]  values, which are coming from a previous snapshot
[2817.78s -> 2819.86s]  of the same function.
[2819.86s -> 2823.34s]  Whereas this particular expectation
[2823.34s -> 2825.10s]  is computed only over your data set.
[2825.22s -> 2828.06s]  It's not computed over all possible state action pairs,
[2828.06s -> 2829.94s]  such that minimizing that loss will give you
[2829.94s -> 2831.70s]  the unique 3D optimal Q function.
[2831.70s -> 2835.30s]  It's only computed over a limited set of samples.
[2835.30s -> 2836.78s]  So if you were to plot, let's say,
[2836.78s -> 2838.42s]  the Q value at a particular state
[2838.42s -> 2842.10s]  as a function of actions, assume 1D actions for now,
[2842.10s -> 2844.82s]  and assume that this is the ground truth Q function,
[2844.82s -> 2847.10s]  you'll end up seeing that many different Q functions,
[2847.10s -> 2849.06s]  many different learned Q functions shown in red,
[2849.06s -> 2850.42s]  can attain the same Q value.
[2850.42s -> 2852.30s]  They'll all attain a similar relative loss,
[2852.30s -> 2854.18s]  because they'll do their own target values.
[2854.18s -> 2857.10s]  They have a similar difference.
[2857.10s -> 2859.78s]  But CQL, in this case, because of this absolute term,
[2859.78s -> 2862.14s]  which minimizes the Q value magnitude,
[2862.14s -> 2863.66s]  is not going to find any of this.
[2863.66s -> 2866.46s]  It's going to find the smallest Q value possible,
[2866.46s -> 2868.86s]  the Q function of the smallest Q value possible, which
[2868.86s -> 2871.70s]  attains the same relative loss.
[2871.70s -> 2873.82s]  And that's because the regularizer that you're adding
[2873.82s -> 2874.74s]  is an absolute loss.
[2874.74s -> 2877.34s]  It just simply minimizes the value.
[2877.34s -> 2878.70s]  So the high limit takeaway here
[2878.70s -> 2880.74s]  is the reason why these Q values are super low
[2880.74s -> 2883.06s]  is because of many solutions which can all
[2883.06s -> 2886.26s]  be of a similar shape, meaning the Q values across actions
[2886.26s -> 2888.02s]  look similar.
[2888.02s -> 2889.78s]  CQL, or any pessimistic algorithm,
[2889.78s -> 2891.70s]  is going to find the one with the smallest
[2891.70s -> 2893.58s]  absolute value in this case.
[2893.58s -> 2896.14s]  So it's the smallest function.
[2896.14s -> 2899.46s]  When you have this smallest possible Q value now,
[2899.46s -> 2901.82s]  and you, let's say, do online data collection,
[2901.82s -> 2903.18s]  so you're going to, let's say,
[2903.18s -> 2907.26s]  query a new action and run this action in the real world,
[2907.26s -> 2910.14s]  you're going to, now when you collect data,
[2910.14s -> 2911.90s]  you're going to see the actual reward
[2911.90s -> 2913.50s]  for this particular action.
[2913.50s -> 2915.86s]  And when you update your Q function on this data,
[2915.86s -> 2919.02s]  what will happen is you'll end up inducing erroneous peaks,
[2919.02s -> 2922.38s]  which look super high under your Q function.
[2922.38s -> 2924.90s]  And this is because this, when you execute this action
[2924.90s -> 2926.90s]  during expiration, you see it's actual ground truth
[2926.90s -> 2929.26s]  award value that you hadn't seen in your offline data set,
[2929.26s -> 2931.14s]  because your offline data set did not show you
[2931.14s -> 2933.42s]  any of that actual value.
[2933.42s -> 2935.86s]  So with that Q function in mind now,
[2935.86s -> 2939.26s]  your bad action supposedly appears very good
[2939.26s -> 2941.46s]  under your Q function, such that now if you
[2941.50s -> 2943.02s]  were to run your policy optimization,
[2943.02s -> 2945.02s]  find a peak in this Q function, you're
[2945.02s -> 2947.86s]  going to find a wrong action, a bad action, which
[2947.86s -> 2950.34s]  is actually not super good.
[2950.34s -> 2952.58s]  And they raised this policy.
[2952.58s -> 2954.50s]  So earlier, the policy was finding
[2954.50s -> 2956.30s]  the peak in this red curve, which
[2956.30s -> 2958.18s]  is the peak in this blue curve, which
[2958.18s -> 2959.62s]  is the ground truth Q function.
[2959.62s -> 2961.26s]  Now your policy is going to find
[2961.26s -> 2962.30s]  the peak in this red curve, which
[2962.30s -> 2964.06s]  is not the peak in the blue curve, which
[2964.06s -> 2965.38s]  is the ground truth Q function.
[2965.38s -> 2967.30s]  So clearly, some kind of unlearning happened.
[2967.30s -> 2968.86s]  You were already pretty good.
[2968.86s -> 2971.30s]  Now you are not going to be as good in this case.
[2971.34s -> 2975.42s]  Because you will see an error in this peak in there.
[2975.42s -> 2978.90s]  I think I'm super short on time.
[2978.90s -> 2980.14s]  OK, one moment.
[2980.14s -> 2981.06s]  Yeah.
[2981.06s -> 2982.98s]  OK, any question here, by the way?
[2988.90s -> 2991.02s]  OK, great.
[2991.02s -> 2995.26s]  So yeah, so basically what we did, our method
[2995.26s -> 2995.94s]  was very simple.
[2995.94s -> 2999.46s]  It said, well, what if we can somehow
[2999.50s -> 3002.78s]  prevent such peaks from occurring by simply making sure
[3002.78s -> 3005.86s]  that my training procedure, my offline training procedure,
[3005.86s -> 3008.06s]  never finds such a low Q value function.
[3008.06s -> 3011.14s]  So never find such a function which attains the smallest Q
[3011.14s -> 3012.78s]  value, as I saw here.
[3012.78s -> 3014.18s]  If I were to find a function that
[3014.18s -> 3017.10s]  looks like this particular curve, this first red curve,
[3017.10s -> 3019.78s]  then none of this will happen because my Q values are not
[3019.78s -> 3022.74s]  that small, such that seeing the reward value here
[3022.74s -> 3025.38s]  will update my Q function error list.
[3025.38s -> 3026.82s]  So that's precisely what we did.
[3026.82s -> 3032.10s]  And well, rather than training with CQL,
[3032.10s -> 3035.74s]  we can train with something that prevents the Q values
[3035.74s -> 3037.30s]  from being so low.
[3037.30s -> 3040.54s]  And the way we did this was to say that we took the same CQL
[3040.54s -> 3042.02s]  algorithm and put the constraint that
[3042.02s -> 3043.90s]  said that the Q values that you're learning
[3043.90s -> 3046.78s]  should never be lower than some reference Q function
[3046.78s -> 3048.58s]  that you could specify exactly.
[3048.58s -> 3050.66s]  So you just constrain your lower bound Q values
[3050.66s -> 3053.38s]  to always be above that orange line in that case.
[3053.38s -> 3055.62s]  In which case, now this issue happens
[3055.62s -> 3058.90s]  because even when you update your Q values on those actions,
[3058.90s -> 3061.38s]  you're going to have a small little dip
[3061.38s -> 3062.98s]  and coming back up there.
[3062.98s -> 3065.74s]  But that's still going to be lower than the highest
[3065.74s -> 3067.58s]  point in this Q function that you trained,
[3067.58s -> 3070.94s]  which is short and gradual.
[3070.94s -> 3074.10s]  So the bad action does not appear any more optimal
[3074.10s -> 3076.66s]  as it was in the earlier part.
[3076.66s -> 3078.90s]  And it turns out that a very simple choice for this Q
[3078.90s -> 3080.10s]  function, this reference Q function,
[3080.10s -> 3082.54s]  is simply the Q function of the behavior policy.
[3082.54s -> 3084.94s]  So Q function of your data set or data generating policy
[3084.98s -> 3087.30s]  that you can compute with relative ease
[3087.30s -> 3090.54s]  without the need for any Q learning altogether at all,
[3090.54s -> 3092.78s]  is simply compute the return-to-go estimates
[3092.78s -> 3096.46s]  and use them to lower bound your Q function in that picture.
[3096.46s -> 3099.54s]  And that works pretty well.
[3099.54s -> 3102.90s]  OK, I think I just had results here.
[3102.90s -> 3104.66s]  Yeah, so clearly all the results is
[3104.66s -> 3107.34s]  that this kind of alleviates this issue.
[3107.34s -> 3109.90s]  So now you get curves here and there
[3109.90s -> 3112.30s]  that improve over the course of training that don't have this
[3112.30s -> 3116.42s]  dip as much and are better and faster than other methods,
[3116.42s -> 3121.82s]  including these methods that I described in the first slide.
[3121.82s -> 3124.18s]  Yeah, and then you can utilize more gradient steps
[3124.18s -> 3125.06s]  here.
[3125.06s -> 3126.82s]  I guess I'm going to skip the details,
[3126.82s -> 3129.18s]  but that will further improve efficiency.
[3129.18s -> 3132.50s]  And in fact, this gives you the smallest regret,
[3132.50s -> 3135.14s]  the smallest cumulative regret over the course of training,
[3135.14s -> 3137.62s]  which is a standard measure for measuring sample efficiency
[3137.62s -> 3139.34s]  of different algorithms.
[3139.34s -> 3140.82s]  And if you're also interested, you
[3140.82s -> 3143.22s]  should check out some other recent work that came out
[3143.22s -> 3145.74s]  from Stanford that takes this approach
[3145.74s -> 3149.38s]  and uses this in conjunction with VLMs
[3149.38s -> 3154.02s]  and so on for actually doing this on a real robot.
[3154.02s -> 3156.18s]  We also did something on the real robot where
[3156.18s -> 3158.82s]  we took this microwave door open task
[3158.82s -> 3161.54s]  and we tried to online fine tune it.
[3161.54s -> 3164.94s]  So the offline policy is kind of OK if we took the gripper,
[3164.94s -> 3166.86s]  but then it's unable to open it.
[3166.90s -> 3170.78s]  And if you now do things with CalQL in this case,
[3170.78s -> 3174.34s]  you can slowly get behavior that now directly touches the door
[3174.34s -> 3177.42s]  and then eventually sort of opens the door fully
[3177.42s -> 3178.18s]  in 20k steps.
[3178.18s -> 3179.34s]  So this is fully online.
[3179.34s -> 3181.54s]  The policy is running online, collecting its own data
[3181.54s -> 3182.54s]  and improving over it.
[3182.54s -> 3185.22s]  But you start from the offline initialization.
[3185.22s -> 3186.62s]  So here, unlearning matters a lot
[3186.62s -> 3189.74s]  because if you unlearn, then within 20k steps,
[3189.74s -> 3191.62s]  you would not be able to recover and get back
[3191.62s -> 3192.62s]  a good solution.
[3192.62s -> 3194.72s]  But now you can, there's no unlearning
[3194.72s -> 3196.64s]  because you're constantly improving.
[3196.64s -> 3198.92s]  That kind of shows the point that this method
[3198.92s -> 3202.04s]  was designed to address.
[3202.04s -> 3204.72s]  Yeah, OK, so that's sort of the end.
[3204.72s -> 3206.36s]  So the challenge in fine tuning
[3206.36s -> 3208.56s]  typically stems from slow policy improvement
[3208.56s -> 3211.60s]  or this initial unlearning.
[3211.60s -> 3214.24s]  A simple way to address this and to get a curve that
[3214.24s -> 3215.96s]  does not look like slow or unlearned,
[3215.96s -> 3217.54s]  but rather looks like this blue curve
[3217.54s -> 3220.82s]  is to calibrate the scale of these Q values
[3220.82s -> 3224.70s]  by simply saying they should not be as low as what
[3224.70s -> 3226.58s]  you can get from CQL.
[3226.58s -> 3229.66s]  And that kind of gets you a best of all sort of an algorithm
[3229.66s -> 3232.10s]  like this blue curve here.
[3232.10s -> 3233.94s]  Yeah, that's basically it.
[3233.94s -> 3237.18s]  I think I was a bit fast, but happy to take any questions
[3237.18s -> 3237.82s]  after this.
[3237.82s -> 3238.32s]  Yeah.
[3242.86s -> 3244.62s]  I don't see one getting picked out yet.
[3244.62s -> 3246.66s]  So that means we have some time for questions.
[3249.20s -> 3250.54s]  Yes.
[3250.54s -> 3252.14s]  To the first part of the lecture,
[3252.14s -> 3255.58s]  what do you think the stochasticity of the set
[3255.58s -> 3257.42s]  gives some sort of implicit regularization?
[3257.42s -> 3261.74s]  Does that apply in general for stochastic optimizers?
[3261.74s -> 3265.10s]  And similarly, are there any kind
[3265.10s -> 3268.78s]  of any where you do specific optimizers for part
[3268.78s -> 3272.14s]  out that would have to get better implicit regularization?
[3272.14s -> 3273.70s]  That's a great question.
[3273.70s -> 3275.90s]  So I'm not entirely sure about,
[3275.90s -> 3278.78s]  so it applies in general stochastic optimizers, yes.
[3278.78s -> 3281.78s]  I personally am not very sure about exact work
[3281.82s -> 3284.26s]  on stochastic optimizers for RL, but I
[3284.26s -> 3288.62s]  think that could be a great direction to work on, yeah.
[3288.62s -> 3291.06s]  I think the bigger question that I would have personally
[3291.06s -> 3294.32s]  there would be, first of all, I
[3294.32s -> 3296.20s]  guess you need a characterization of what exact
[3296.20s -> 3297.66s]  terms matter.
[3297.66s -> 3300.66s]  And I think RL was kind of one of the very few first works
[3300.66s -> 3302.24s]  probably which characterized something
[3302.24s -> 3303.82s]  like that in any setting for RL.
[3303.82s -> 3305.40s]  It was still not a simpler setting.
[3305.40s -> 3306.98s]  There were some assumptions made.
[3306.98s -> 3309.10s]  But I think you need that to be able to then derive
[3309.10s -> 3310.70s]  the right optimizer.
[3310.74s -> 3312.98s]  But yeah, I think there's some work now
[3312.98s -> 3314.54s]  on characterizing implicit regularizers
[3314.54s -> 3316.46s]  so you can start off by building the optimizer.
[3316.46s -> 3317.78s]  Certainly, yeah.
[3321.34s -> 3324.62s]  Any other questions?
[3324.62s -> 3325.22s]  Yes.
[3325.22s -> 3327.38s]  Is it just a coincidence that the two parts
[3327.38s -> 3331.06s]  of the implicit regularization are weighted the same?
[3331.06s -> 3332.10s]  That's a good question.
[3332.10s -> 3335.62s]  So yeah, I think I was probably a bit emphasized there.
[3335.62s -> 3337.86s]  So there's a gamma discounting there,
[3337.86s -> 3340.46s]  which probably makes sense now, I think.
[3340.46s -> 3345.02s]  So I didn't put it up here because I did not
[3345.02s -> 3346.42s]  need to use gamma notation at all.
[3346.42s -> 3347.94s]  So there's a gamma in there.
[3347.94s -> 3351.14s]  But basically, what happens is happening is,
[3351.14s -> 3353.94s]  if you had a gamma here, that would be a good thing.
[3357.10s -> 3358.38s]  Any other questions?
[3362.66s -> 3363.16s]  Excellent.
[3363.16s -> 3364.58s]  Let's give a round of applause.
[3368.10s -> 3370.26s]  And see you all for our final guest lecture.
[3370.26s -> 3372.18s]  This Wednesday when Professor Dorsey
[3372.18s -> 3374.34s]  City from Stanford will be speaking
[3374.34s -> 3376.70s]  about interactive learning.
