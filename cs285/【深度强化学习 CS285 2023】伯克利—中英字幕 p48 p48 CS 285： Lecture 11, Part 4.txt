# Detected language: en (p=1.00)

[0.96s -> 6.08s]  All right, next let's briefly discuss how we can use these uncertainty-aware models
[6.08s -> 11.12s]  for control and go through a few examples of some papers that have actually used things like this.
[12.88s -> 18.00s]  All right, so let's say that we've trained our uncertainty-aware model, perhaps by using a
[18.00s -> 24.08s]  bootstrap ensemble, and now we'd like to use it in our model-based RL version 1.5 algorithm
[24.08s -> 32.72s]  to actually make decisions. So before, when we were planning, we were essentially optimizing
[32.72s -> 39.60s]  the following objective. We were optimizing the sum from t equals 1 to h of the reward at s t a t,
[39.60s -> 45.84s]  where s t plus 1 is equal to f of s t a t. So whether you use random shooting,
[45.84s -> 49.36s]  c a m, whatever, this is essentially the problem that you're solving.
[52.40s -> 59.92s]  Now we have n possible models, and what we would like to do is choose a sequence of actions a1
[59.92s -> 67.36s]  through a h that maximizes the reward on average over all the models. So now our objective
[67.36s -> 74.24s]  is the sum over the models times 1 over n of the sum over the time steps for the rewards
[74.24s -> 81.84s]  for the states predicted by that model, where s t comma i is given by the dynamics for model i.
[85.04s -> 90.00s]  So this is the case if you learn a distribution over deterministic models.
[91.76s -> 96.40s]  If you have stochastic models, then you would have an expectation for each model with respect
[96.40s -> 101.84s]  to its distribution. So in general, for some candidate action sequence a1 through a h,
[102.40s -> 108.00s]  step one is to sample a model from p of theta given d, which if you have a bootstrap ensemble
[108.00s -> 116.08s]  amounts to just choosing one of the m models randomly. Step two, at each time step, sample
[116.08s -> 121.92s]  s t plus 1 from p of s t plus 1 given s t a t and that parameter vector that you sampled
[121.92s -> 127.92s]  from the posterior. Step three, calculate the reward as the sum over all the time steps
[128.80s -> 134.96s]  for those predicted states. And then step four is to repeat steps one through three to accumulate
[134.96s -> 141.44s]  the average reward as necessary. So this would be the recipe for any general representation for
[141.44s -> 145.76s]  the posterior p theta given d. If you have a bootstrap ensemble, you can also sum over all
[145.76s -> 150.72s]  the models instead of sampling them. That can be simpler if you have a small number of models.
[150.72s -> 154.72s]  If you estimate your posterior with some other method, like a Bayesian neural net, then you
[154.72s -> 159.52s]  can sample multiple different random parameter vectors and estimate the reward for each one.
[161.04s -> 168.24s]  Now this is not the only option you could have. This is a sampling procedure for evaluating
[168.24s -> 175.76s]  the reward. You could imagine other procedures. For instance, you could evaluate possible next
[175.76s -> 180.72s]  states from every model at every time step and then perform something like a moment matching
[180.72s -> 185.68s]  approximation to figure out an estimate of the actual state distribution of the actual
[185.68s -> 190.48s]  distribution p of s t plus 1, for instance by estimating its mean and variance. And other
[191.04s -> 199.04s]  methods have done things like that as well. But a simple procedure is to use this process
[199.04s -> 204.80s]  that I have on the slide to evaluate the total reward of every candidate action sequence a1
[204.80s -> 210.16s]  through a h and then optimize over the action sequences using your favorite optimization method
[210.16s -> 212.08s]  like random shooting or cross-entropy method.
[214.48s -> 219.28s]  It's also possible to adapt continuous optimization methods like LQR to this setting.
[220.00s -> 223.76s]  In that case, something called the reparameterization trick can be very useful,
[223.76s -> 225.60s]  which we will cover next week.
[228.08s -> 232.64s]  Okay, so take a moment to look over the slide, make sure that this really makes sense to you.
[232.64s -> 235.92s]  This is a pretty important slide to understand if you want to know how to implement
[236.64s -> 239.44s]  model-based RL with epistemic uncertainty.
[240.16s -> 249.44s]  Okay, so does this basic scheme work? Well, here are some plots I'm going to show. These are from
[249.44s -> 253.44s]  a paper called Deep Reinforcement Learning in a Handful of Trials. So before we saw on the
[253.44s -> 259.52s]  half-cheated task how this model-based RL version 1.5 algorithm could get us from a reward
[259.52s -> 265.52s]  of 0 to about 500. If we actually implement epistemic uncertainty using bootstrap ensembles,
[266.16s -> 271.44s]  then we can get model-based RL to get a reward over 6,000 in about the same amount of time as
[271.44s -> 277.68s]  the 1.5 algorithm. So especially in low data regimes, these epistemic uncertainty estimates
[277.68s -> 283.68s]  really do make a really big difference in performance. Here's a more recent illustration of
[283.68s -> 289.52s]  a model-based RL method with an ensemble of models. This is actually a real world
[289.52s -> 295.12s]  robotic experiment where this robotic hand is manipulating objects in the palm. This is using
[295.60s -> 302.00s]  model-based RL version 1.5 with a particularly sophisticated model and an ensemble for uncertainty
[302.00s -> 307.92s]  estimation. And this hand learns directly by interacting with these objects, and in about
[307.92s -> 314.24s]  three hours they can perform a full turn with both objects in the palm. So this is 1.5 hours,
[316.72s -> 320.08s]  and there's two hours, and it can do a full 180 turn,
[321.68s -> 323.92s]  and then after four hours it can do it pretty reliably.
[325.36s -> 329.52s]  So something about the uncertainty estimation really does seem to work, and it does seem to
[329.52s -> 333.36s]  be quite important for these model-based RL methods. So if you want to implement model-based
[333.36s -> 338.16s]  RL, highly recommend it to consider epistemic uncertainty estimation.
[340.32s -> 343.76s]  If you want to learn more about epistemic uncertainty and model-based RL,
[343.76s -> 348.56s]  here are a few suggested readings. This paper by Mark Dezenroth called PILCO,
[349.36s -> 356.88s]  this was an older paper from around 2011. This paper uses Gaussian processes rather than neural
[356.88s -> 361.52s]  nets for model learning, but this was sort of one of the foundational papers in really
[361.52s -> 365.76s]  establishing the importance of epistemic uncertainty estimation in model-based reinforcement
[365.76s -> 369.68s]  learning, and has some good discussion of why it matters. So I would encourage you to read this
[369.68s -> 377.04s]  if you're interested in the topic. More recent papers, this is the model-based RL version 1.5
[377.12s -> 380.96s]  paper that I mentioned before that does kind of okay but not great on half cheetah.
[381.60s -> 388.64s]  This is the paper that introduced the ensembles for model-based RL and managed to get comparable
[388.64s -> 393.52s]  results to model-free RL. This is a paper that I would encourage you to check out if you're
[393.52s -> 398.48s]  especially interested in the intersection of model-based and model-free RL that use models
[398.48s -> 404.40s]  for estimating value functions, and this is another closely related paper that also integrates
[404.40s -> 413.28s]  epistemic uncertainty at multiple different points in the previous method.
