# Detected language: en (p=1.00)

[0.00s -> 6.84s]  All right, welcome to Lecture 20 of CS285. Today we're going to talk about inverse reinforcement
[6.84s -> 12.54s]  learning. So, so far, every time that we've had to take on a reinforcement learning problem,
[12.54s -> 18.92s]  we also always assumed that a reward function was provided for us, and typically
[18.92s -> 22.12s]  if you were to use these reinforcement learning algorithms, you would program a
[22.12s -> 28.84s]  reward function by hand, manually. What if instead you have a task where the reward
[28.88s -> 34.80s]  function is difficult to specify manually, but you have access to data of humans or
[34.80s -> 39.16s]  in general some kind of expert performing that task successfully? Could you back
[39.16s -> 42.48s]  out their reward function from observing their behavior and then
[42.48s -> 47.56s]  re-optimize that reward function with reinforcement learning algorithms?
[47.56s -> 52.32s]  What we're going to learn about today is how we can apply this approximate
[52.32s -> 57.84s]  model of optimality, formalized as an inference problem from last time, to
[57.84s -> 61.60s]  learn a reward function rather than just directly learning a policy from a known reward,
[61.60s -> 67.20s]  and this is called the inverse reinforcement learning problem. So the goals for today will
[67.20s -> 71.92s]  be to understand the inverse reinforcement learning problem definition, understand how
[71.92s -> 76.84s]  probabilistic models of behavior can be used to derive inverse reinforcement learning
[76.84s -> 82.32s]  algorithms, and understand a few practical inverse reinforcement learning methods that we can
[82.32s -> 87.80s]  actually use in high-dimensional problems of the sort that we encounter in deep reinforcement
[87.80s -> 94.20s]  learning. All right, so one of the things that I mentioned in the previous lecture is that
[94.20s -> 100.44s]  optimal control and reinforcement learning could serve as a model of human behavior, and
[100.44s -> 105.80s]  there's actually a very long history going back over a hundred years of scientists trying
[105.80s -> 112.56s]  to study human motion, human decision-making, and human behavior through the lens of optimal
[112.56s -> 119.00s]  decision-making and rationality. In fact, one of the definitions of rational behavior is that
[119.00s -> 125.84s]  rational behavior can be framed as maximizing a well-defined utility function. It turns out
[125.84s -> 131.96s]  that any rational decision-making strategy, for instance, one where if you prefer A over
[131.96s -> 136.92s]  B and you prefer B over C, then you prefer A over C, any strategy that is rational in
[136.92s -> 142.56s]  a sense, can be explained with a well-defined set of scalar-valued utilities, whereas
[142.56s -> 148.36s]  an irrational strategy, such as, for instance, if you prefer apples over bananas and you
[148.36s -> 153.84s]  prefer bananas over oranges, but then you prefer oranges over apples, that is irrational
[153.84s -> 160.04s]  and that in fact cannot be explained with a well-defined set of scalar-valued utilities.
[160.04s -> 164.92s]  So if we want to explain human motion, human decision-making, and so on through the lens
[164.92s -> 169.86s]  of optimality, what we could do is we could write down the equations that describe optimal
[169.86s -> 174.60s]  decision-making, either in the deterministic case, as we learned about in the optimal control
[174.60s -> 180.36s]  lecture, or in the stochastic case, and then we could ask, assuming that the human
[180.36s -> 185.48s]  is solving this optimization problem, what can we plug in in place of R so that the
[185.48s -> 191.36s]  solution to this optimization problem matches the behavior the human actually exhibited?
[191.36s -> 196.16s]  And in fact, studies in neuroscience, motor control, psychology, and so on have applied
[196.16s -> 202.04s]  this basic model, and although, as we discussed last week, the classic model of optimality
[202.04s -> 206.40s]  is sometimes a poor fit for human decision-making because humans are often not deterministic
[206.40s -> 211.32s]  and not perfectly optimal, a soft model of optimality can explain human behavior quite
[211.32s -> 213.08s]  well in many cases.
[213.08s -> 217.56s]  And in fact, the notion that optimality is a good framework for thinking about human
[217.56s -> 223.00s]  decision-making and human motor control has been extremely influential in studies
[223.00s -> 226.32s]  of human behavior and neuroscience.
[226.32s -> 233.72s]  All right, that's maybe the kind of more intellectual motivation, but we could also
[233.72s -> 235.32s]  ask a practical question.
[235.32s -> 238.16s]  Why should we worry about learning reward functions?
[238.16s -> 243.12s]  Well, one perspective we can take on it is the imitation learning perspective.
[243.12s -> 247.92s]  So a standard way to go about imitation learning problems, as we discussed in the
[247.92s -> 253.12s]  beginning of the course, would be to demonstrate a behavior that you want to a robot or to
[253.12s -> 258.64s]  whatever your agent is, your autonomous car, your e-commerce agent, whatever it is,
[258.64s -> 263.12s]  and have it simply imitate that behavior through behavioral cloning.
[263.12s -> 269.76s]  However, when humans imitate other humans, we don't actually do it this way.
[269.76s -> 276.12s]  If you imagine teaching a robot through imitation learning, you would maybe actually like teleoperate
[276.12s -> 280.24s]  the robot or move its arms through the motions that you want it to perform.
[280.24s -> 283.12s]  But when you think of a person imitating somebody, it's not like you need someone to
[283.12s -> 287.64s]  like hold you and move your body in exactly the way that it's needed for you to accomplish
[287.64s -> 288.64s]  the task.
[288.64s -> 289.64s]  No, that's not what you do.
[289.64s -> 293.92s]  You watch somebody and you figure out what is it that they're trying to do, and then
[293.92s -> 300.12s]  you attempt to emulate not their direct motions, but rather their intentions.
[300.12s -> 304.04s]  So standard imitation learning deals with copying the actions performed by the expert
[304.04s -> 309.56s]  without reasoning about the purpose of those actions, without reasoning about their outcomes.
[309.56s -> 311.36s]  Human imitation learning is very different.
[311.36s -> 314.52s]  When humans imitate, we copy the intent of the expert.
[314.52s -> 318.68s]  We might do what they did, but differently, because we understand why they took the
[318.68s -> 322.92s]  actions they did and what outcome they were seeking.
[322.92s -> 327.44s]  And this might result in actually very different actions from the ones the expert took, but
[327.44s -> 329.76s]  the same outcome.
[329.76s -> 331.60s]  Here is a nice video that I think illustrates this point.
[331.60s -> 333.56s]  This is a psychology experiment.
[333.56s -> 338.60s]  The subject of the psychology experiment is the child in the lower right corner of the
[338.60s -> 339.60s]  frame.
[339.60s -> 344.04s]  Now, if you put yourself in the place of that child, imagine what you would do upon
[344.04s -> 345.32s]  seeing this.
[345.32s -> 351.72s]  Well, you would infer the intentions of the experimenter here, and you would not perform
[351.72s -> 353.94s]  the action that the experimenter is performing.
[353.94s -> 358.52s]  You would instead perform the action that leads to the desired outcome.
[358.52s -> 373.12s]  The outcome that you inferred is the outcome that they are going for.
[373.12s -> 382.16s]  So can we figure out how to enable reinforcement learning agents to do things like this?
[382.16s -> 386.28s]  There's another perspective we can take to think about why inverse reinforcement learning
[386.28s -> 391.52s]  is important, and that's the more reinforcement learning-centric perspective.
[391.52s -> 395.72s]  In many of the reinforcement learning tasks that we want to tackle, such as the games
[395.72s -> 400.96s]  that you guys had to work with for Homework 3, the objective is fairly natural.
[400.96s -> 404.60s]  So if you want to play a game as well as possible, it makes sense that your reward
[404.60s -> 406.32s]  function would be the score in the game.
[406.32s -> 409.16s]  The score is printed right there on the image.
[409.16s -> 413.28s]  So it's not a huge stretch to say, that's my reward function.
[413.28s -> 416.84s]  But in many other scenarios, the reward function is much less obvious.
[416.84s -> 421.08s]  Imagine, for instance, an autonomous car navigating down the freeway.
[421.08s -> 425.84s]  Now this autonomous car has to balance a number of different competing factors.
[425.84s -> 430.00s]  It has to reach the destination, go at a particular speed, it needs to not violate
[430.04s -> 434.32s]  the laws of traffic, it needs to not annoy other drivers, and all these different factors
[434.32s -> 439.64s]  have to be balanced against each other to drive appropriately, safely, and in a way
[439.64s -> 441.96s]  that is comfortable to the passengers.
[441.96s -> 446.00s]  And writing down a single equation that describes that might be very hard, but asking a professional
[446.00s -> 449.40s]  driver to demonstrate it is comparatively much, much easier.
[449.40s -> 456.00s]  So it's very appealing to think about learning reward functions in these kinds of scenarios.
[457.00s -> 462.16s]  Okay, so inverse reinforcement learning refers to the problem of inferring a reward function
[462.16s -> 467.52s]  from demonstrations, such as, for instance, in this driving scenario, where you have
[467.52s -> 472.08s]  a professional driver demonstrate a good driving policy, and then you want to figure
[472.08s -> 476.16s]  out what's a good reward function to extract from this to give to your reinforcement
[476.16s -> 479.16s]  learning agent.
[479.16s -> 483.68s]  Now by itself, inverse reinforcement learning, as I've stated it, is unfortunately a very
[483.68s -> 485.40s]  underspecified problem.
[485.40s -> 489.00s]  And the reason for this is that for any given pattern of behavior, there are actually
[489.00s -> 492.76s]  infinitely many different reward functions that explain that behavior.
[492.76s -> 497.12s]  This is perhaps most obvious if I give you an example.
[497.12s -> 501.80s]  Let's consider this really simple grid world with 16 states.
[501.80s -> 506.24s]  If I have this demonstration, and I ask you, what was the reward function of the agent
[506.24s -> 511.12s]  who performed this demonstration, what would be your answer?
[511.56s -> 514.36s]  Now you might object at this point, you might say, well, what the heck is going on here?
[514.36s -> 516.84s]  There's just some arrows drawn on a grid.
[516.84s -> 520.76s]  In the autonomous driving scenario, the semantics of that task are much richer.
[520.76s -> 524.08s]  There are other cars, stop signs, traffic lights.
[524.08s -> 529.56s]  But remember, the algorithm doesn't have all of those semantics that you have.
[529.56s -> 533.68s]  Just like in the exploration lecture, when we talked about Montezuma's revenge, exploration
[533.68s -> 537.80s]  is hard because we lack the semantics, we have the semantics that allow us to make
[537.80s -> 541.44s]  sense of the world, but the algorithm lacks those semantics.
[541.44s -> 545.52s]  Similarly, in the case of inverse reinforcement learning, to the algorithm, these are all just
[545.52s -> 546.84s]  states and actions.
[546.84s -> 551.60s]  It has no way to understand that meaningful reward functions have something to do with
[551.60s -> 558.20s]  the laws of traffic and not with the particular GPS coordinate at which you're looking at.
[558.20s -> 563.20s]  So I want to show you this example because I want to construct a scene where we intentionally
[563.20s -> 569.28s]  divorced the problem of recovering the reward from any of our own prior semantic knowledge.
[569.28s -> 576.00s]  Okay, so for this grid world, think about what the reward function might be.
[576.00s -> 578.40s]  Take a guess.
[578.40s -> 582.56s]  So one very reasonable guess is that the agent gets a big reward for reaching this
[582.56s -> 586.56s]  particular square and a bad reward everywhere else.
[586.56s -> 589.92s]  So that would definitely explain why they did what they did.
[589.92s -> 592.04s]  But there's another explanation.
[592.04s -> 594.84s]  What if they get a big reward for reaching this square?
[594.84s -> 599.24s]  If you only observe a trajectory consisting of four steps, both of these rewards explain
[599.24s -> 604.84s]  the expert's behavior equally well.
[604.84s -> 607.24s]  What if instead they have this reward function?
[607.24s -> 611.96s]  A big reward for anything in the lower half and a big penalty for crossing those
[611.96s -> 612.96s]  darker squares.
[612.96s -> 616.52s]  That would also explain their behavior.
[616.52s -> 620.12s]  Indeed their behavior could even be explained by a general reward function that just says
[620.20s -> 626.28s]  you have a reward of negative infinity for taking any action other than the ones in the demonstration.
[626.28s -> 631.32s]  So there are in general infinitely many rewards for which the observed behavior would
[631.32s -> 635.56s]  be optimal in the traditional sense.
[635.56s -> 642.36s]  Okay, so before we talk about how to clear up this ambiguity, let's define the inverse
[642.36s -> 646.28s]  reinforcement learning problem more formally.
[646.28s -> 650.04s]  To define inverse reinforcement learning more formally, we can do it like this, and I'm
[650.04s -> 654.28s]  going to, on the left side of the slide, I'm going to present the formalism for regular
[654.28s -> 655.48s]  forward reinforcement learning.
[655.48s -> 659.08s]  On the right side, the formalism for inverse reinforcement learning, so that you can see
[659.08s -> 662.04s]  them side by side and compare.
[662.04s -> 664.04s]  So first, what are we given?
[664.04s -> 668.60s]  In forward reinforcement learning and inverse reinforcement learning, in both cases we are
[668.60s -> 673.00s]  given a state space and an action space.
[673.00s -> 676.36s]  Sometimes we are given the transition probabilities, and sometimes not.
[676.36s -> 679.80s]  Sometimes we have to infer them from experience.
[679.80s -> 685.48s]  In forward reinforcement learning, we are given a reward function, and our goal is to
[685.48s -> 689.24s]  learn pi-star, the optimal policy for that reward function.
[689.24s -> 695.52s]  In inverse reinforcement learning, we are given trajectories tau sampled by running the optimal
[695.52s -> 696.52s]  policy.
[696.52s -> 700.12s]  We don't necessarily know what the optimal policy is, but we do assume that our sample
[700.12s -> 706.08s]  trajectories came from that policy, or some approximation thereof.
[706.08s -> 713.00s]  And our goal is to learn the reward function r-psi that pi-star optimized to produce those
[713.00s -> 722.48s]  taus, where psi is a parameter vector that parameterizes the reward.
[722.48s -> 726.12s]  Now there are many different choices we could make for the reward parameterization.
[726.12s -> 731.44s]  In the kind of more classical inverse reinforcement learning literature, a very common choice
[731.44s -> 735.96s]  is to use a linear reward function, a reward function that is a weighted combination
[735.96s -> 738.76s]  of features.
[738.76s -> 744.52s]  You can equivalently write it as an inner product, psi transpose times bold F, where
[744.52s -> 747.40s]  bold F is a vector of features.
[747.40s -> 752.00s]  You could intuitively think of these features as a bunch of things that the agent wants
[752.00s -> 755.84s]  or does not want, and then what you're trying to determine is precisely how much
[755.84s -> 761.44s]  do they want or not want each of those things.
[761.44s -> 766.52s]  Now these days in the world of deep reinforcement learning, we might also want to deal with
[766.52s -> 768.76s]  neural network reward functions.
[768.76s -> 776.52s]  Reward functions that map states and actions via a deep neural network, a nonlinear function
[776.52s -> 781.08s]  to a scalar-valued reward, and that are parameterized by some parameter vector psi,
[781.08s -> 786.52s]  which denotes the parameters of that neural network.
[786.52s -> 789.76s]  And then once we've recovered the reward function in inverse reinforcement learning,
[789.76s -> 794.40s]  typically what we would want to do is use that reward function to learn the corresponding
[794.40s -> 798.12s]  optimal policy of pi star.
[798.12s -> 799.28s]  All right.
[799.28s -> 805.12s]  So first, before I talk about the main topic of today's lecture, which is going
[805.12s -> 808.60s]  to be inverse reinforcement learning algorithms based on the probabilistic model that we
[808.60s -> 813.12s]  saw in the previous lecture, I want to provide a little bit of kind of historical
[813.12s -> 817.96s]  background to discuss some ways that people have thought about solving the inverse reinforcement
[817.96s -> 822.96s]  learning problem prior to kind of the modern age of deep learning.
[822.96s -> 827.64s]  So many of the previous algorithms for inverse reinforcement learning were focused around
[827.64s -> 830.52s]  something called feature matching.
[830.52s -> 834.20s]  The main algorithms that I'll discuss today are based around the maximum entropy principle
[834.20s -> 839.44s]  and draw on the graphical model that I presented last lecture, and this is different
[839.44s -> 840.44s]  from feature matching.
[840.44s -> 845.20s]  However, I will first describe the feature matching algorithms just to provide some context
[845.24s -> 849.56s]  and just to give you guys a broader overview of the literature.
[849.56s -> 853.84s]  So classically, when people started thinking about the inverse reinforcement learning problem,
[853.84s -> 854.84s]  they approached it like this.
[854.84s -> 858.00s]  They said, well, let's say that we have some features and we're going to learn a
[858.00s -> 861.80s]  linear reward function in those features.
[861.80s -> 865.72s]  If the features f are important, what if the way that we disambiguate the inverse
[865.72s -> 872.24s]  reinforcement learning problem is by saying, let's learn a reward function for which the
[872.28s -> 877.44s]  optimal policy has the same expected values for those features.
[877.44s -> 883.24s]  So the features are functions of states and actions, and you could say, let's let pi
[883.24s -> 887.92s]  r psi be the optimal policy for our learned reward r psi.
[887.92s -> 892.68s]  And then we're going to select psi such that the expected value under pi r psi of
[892.68s -> 896.68s]  our feature vector is equal to its expected value under pi star.
[896.68s -> 899.00s]  Now, that's very reasonable.
[899.00s -> 904.48s]  That's just saying that, well, if you saw that the, let's say, the optimal driver driving
[904.48s -> 910.76s]  a car rarely experienced a crash, rarely ran red lights, and frequently overtook on
[910.76s -> 914.88s]  the left rather than the right, then matching the expected values of those features will
[914.88s -> 921.48s]  probably give you somewhat similar behavior if you were given the right features.
[921.48s -> 927.60s]  Now unfortunately, this formulation is still ambiguous.
[927.60s -> 931.84s]  So you could do this fairly easily because you have trajectories sampled from the optimal
[931.84s -> 935.56s]  policy, so while you don't know the optimal policy itself, you can approximate the right-hand
[935.56s -> 941.00s]  side by averaging the feature vectors in your demonstrated trajectories.
[941.00s -> 945.16s]  But it's still ambiguous because multiple different psi vectors could still result in
[945.16s -> 947.60s]  the same feature expectations.
[947.60s -> 950.60s]  So think back to the example with the grid roll that I gave before.
[950.60s -> 954.40s]  All of those different reward functions result in the same exact policy, which means
[954.40s -> 958.68s]  they would all have the same exact expected values.
[958.68s -> 963.76s]  So one way that people thought about disambiguating this further is by using a maximum margin
[963.76s -> 965.68s]  principle.
[965.68s -> 971.16s]  So the maximum margin principle for inverse RL is very similar to the maximum margin principle
[971.16s -> 978.04s]  for support vector machines, and it states that you should choose psi so as to maximize
[978.04s -> 986.28s]  the margin between the observed policy pi star and all other policies.
[986.28s -> 992.56s]  So if the reward is psi transpose f, then the expected reward is psi transpose times
[992.56s -> 997.42s]  the expected value of f, and you would like to pick psi, so it's psi transpose times
[997.42s -> 1003.28s]  the expected value of f, meaning the expected reward under pi star is greater than or
[1003.28s -> 1009.72s]  equal to the expected reward under any other policy plus the largest possible margin.
[1009.72s -> 1015.40s]  And you would choose the margin and the psi so as to maximize this.
[1015.40s -> 1021.56s]  So this is basically saying, find me a weight vector psi so that the expert's policy
[1021.56s -> 1026.40s]  is better than all other policies by the largest possible margin.
[1026.40s -> 1030.12s]  Now this is a little bit of a heuristic because this doesn't necessarily mean that
[1030.12s -> 1036.12s]  you will recover the expert's weight vector, the expert's true reward function, but it's
[1036.12s -> 1037.12s]  a reasonable heuristic.
[1037.12s -> 1042.88s]  It's saying, you know, if you have two different rewards that have the same feature
[1042.88s -> 1047.44s]  expectations as the expert, pick the one that makes the expert look better than all
[1047.44s -> 1050.40s]  the other policies.
[1050.40s -> 1058.00s]  So don't pick a reward for which the expert is just a little bit better than the alternatives.
[1058.00s -> 1064.12s]  Now the trouble with this formulation still is that if the space of policies is very large
[1064.12s -> 1069.16s]  and continuous, there are likely other policies that are very, very similar to the expert's
[1069.16s -> 1070.16s]  policy.
[1070.16s -> 1074.40s]  In fact, there are likely other policies that are almost identical.
[1074.40s -> 1078.86s]  So just maximizing the margin against all other policies is maybe not such a good idea
[1078.86s -> 1083.36s]  by itself, and perhaps you want to weight this by some similarity between pi star and
[1083.36s -> 1084.36s]  pi.
[1084.40s -> 1089.68s]  So maybe what you want is to maximize the margin more against policies that are more
[1089.68s -> 1093.36s]  distinct from the expert, whereas the margin against other policies that are very similar
[1093.36s -> 1096.64s]  to the expert could be pretty small.
[1096.64s -> 1100.12s]  Now fortunately, this is again very similar to the kind of problems that we encounter
[1100.12s -> 1104.88s]  in support vector machines, and much of the literature on this feature-matching
[1104.88s -> 1110.20s]  IRL actually borrowed techniques from support vector machines to solve this problem.
[1110.20s -> 1113.60s]  So for those of you that are familiar with SVMs, you'll probably recognize this.
[1113.84s -> 1116.92s]  If you're not familiar with SVMs, don't worry about it too much, you don't really need
[1116.92s -> 1122.28s]  to know this, but it's a good kind of side note for you to be aware of the literature.
[1122.28s -> 1127.48s]  So the SVM trick basically takes a maximum margin problem like this, which is generally
[1127.48s -> 1134.16s]  difficult to solve, and reformulates it as the problem of minimizing the length of
[1134.16s -> 1139.52s]  the weight vector, where the margin is always 1.
[1139.52s -> 1141.74s]  It's a little subtle why you can do this.
[1141.74s -> 1146.54s]  It requires a little bit of Lagrangian duality, but you can take me at my word that these
[1146.54s -> 1149.38s]  two problems are equivalent.
[1149.38s -> 1153.10s]  And then it turns out that if you want to incorporate the similarity between policies
[1153.10s -> 1157.42s]  into the second problem, all you do is you replace that 1 by some measure of divergence
[1157.42s -> 1158.42s]  between policies.
[1158.42s -> 1163.46s]  So that means that if you have another policy pi that is identical to pi*, then it's okay
[1163.46s -> 1167.14s]  for the left-hand side and right-hand side to be equal because d will be 0.
[1167.14s -> 1170.46s]  But as the policies become more and more different, then you want to increase the margin
[1170.46s -> 1174.06s]  to those policies.
[1174.06s -> 1178.50s]  So one good choice for d could be the difference in their feature expectations.
[1178.50s -> 1182.82s]  Another good choice could be their expected KL divergence.
[1182.82s -> 1184.82s]  Now there are still a few issues with this formulation.
[1184.82s -> 1188.54s]  It does lead to some practical inverse reinforcement learning algorithms that we could actually
[1188.54s -> 1190.18s]  implement and try to use.
[1190.18s -> 1193.88s]  But these inverse reinforcement learning algorithms will have a number of shortcomings.
[1193.88s -> 1199.86s]  One major shortcoming is that maximizing the margin is a bit arbitrary.
[1199.86s -> 1204.94s]  What it basically says is you should find a reward function for which the expert's policy
[1204.94s -> 1207.58s]  is not just better than the alternatives by a small amount, you don't just want to
[1207.58s -> 1211.54s]  find a reward function for which the expert is like tied with some very different policy,
[1211.54s -> 1215.64s]  you want to find the reward function for which the expert's behavior is very clearly
[1215.64s -> 1218.96s]  the better choice.
[1218.96s -> 1221.46s]  But this doesn't say why you want to do that.
[1221.46s -> 1224.06s]  Presumably the reason you'd want to do that is because you're making some assumption
[1224.06s -> 1225.06s]  about the expert.
[1225.06s -> 1229.18s]  Maybe one assumption that you're implicitly making is that the expert intentionally demonstrated
[1229.22s -> 1232.78s]  the things that make it easy to figure out their reward.
[1232.78s -> 1237.70s]  But the notion of maximizing the margin is a heuristic response to that.
[1237.70s -> 1244.26s]  The assumption about the expert's behavior is not actually made explicit here.
[1244.26s -> 1248.54s]  The other problem is that this formulation doesn't really give us a clear model of
[1248.54s -> 1249.54s]  expert sub-optimality.
[1249.54s -> 1254.78s]  It doesn't explain why the expert might sometimes do things that are not actually optimal.
[1254.78s -> 1259.14s]  Now those of you that are familiar with support vector machines might remember that
[1259.14s -> 1263.86s]  in the case where the classes are not perfectly separable, you can do things like adding slack
[1263.86s -> 1267.58s]  variables to account for some degree of sub-optimality.
[1267.58s -> 1272.70s]  But adding such slack variables in this setting is still largely heuristic.
[1272.70s -> 1278.54s]  It's not really a clear model of the expert's behavior, it's just heuristically modifying
[1278.54s -> 1283.46s]  the problem to make it possible to accommodate sub-optimal experts.
[1283.46s -> 1287.26s]  And lastly, this results in kind of a messy constrained optimization problem, which is
[1287.26s -> 1291.62s]  not a big deal if you have linearly parametrized reward functions, but it does become a really
[1291.62s -> 1295.74s]  big problem for deep learning if you want to have reward functions represented by neural
[1295.74s -> 1296.74s]  networks.
[1296.74s -> 1301.46s]  However, if you want to learn more about these kinds of methods, there are a few readings
[1301.46s -> 1302.74s]  I might suggest.
[1302.74s -> 1307.14s]  You can check out, for example, a classic paper by B. Lening called Apprenticeship Learning
[1307.14s -> 1311.46s]  by Inverse Reinforcement Learning, as well as a paper by Ratliff et al. called Maximum
[1311.46s -> 1316.46s]  Marginal Planning, which are very representative of this class of feature matching and margin
[1316.46s -> 1319.86s]  maximizing inverse RL methods.
[1319.86s -> 1326.54s]  However, the main topic for today's discussion will actually build on probabilistic models
[1326.54s -> 1328.46s]  of expert behavior.
[1328.46s -> 1333.34s]  So from the previous lecture, we saw that we could actually model sub-optimal behavior
[1333.34s -> 1338.26s]  as inference in a particular graphical model that had states, actions, and these additional
[1338.26s -> 1340.62s]  optimality variables.
[1340.62s -> 1345.50s]  So the probability distributions in this model are the initial state distribution p
[1345.50s -> 1353.42s]  of s1, the transition probabilities p of s t plus 1 given s t a t, and the optimality
[1353.42s -> 1359.94s]  probabilities, which we chose to be equal to the exponential of the reward.
[1359.94s -> 1364.38s]  Now before we were concerned with the problem, what is the probability of a trajectory given
[1364.38s -> 1368.70s]  that the expert was acting optimally?
[1368.78s -> 1371.98s]  So this is what we saw before.
[1371.98s -> 1377.38s]  We said that, well, if you don't assume optimality, then any physically consistent
[1377.38s -> 1379.30s]  trajectory is equally likely.
[1379.30s -> 1382.86s]  But if you make an assumption of optimal behavior, then you could say, what's the
[1382.86s -> 1386.26s]  probability of a trajectory given that the expert was optimal?
[1386.26s -> 1390.94s]  And we saw that that had this nice interpretation that the most optimal trajectory was the
[1390.94s -> 1395.78s]  most likely, and the sub-optimal trajectories became exponentially less likely.
[1395.78s -> 1403.54s]  And we talked about how that might be a good model of sub-optimal, expert, or monkey behavior.
[1403.54s -> 1406.66s]  But now what we're going to be doing is we're actually going to be using this model
[1406.66s -> 1408.62s]  to learn reward functions.
[1408.62s -> 1415.02s]  So instead of asking, what is the probability of a trajectory given a reward, which has
[1415.02s -> 1419.22s]  inference in this model, we'll instead do learning in this model, where we say, given
[1419.22s -> 1424.62s]  the trajectories, can we learn the parameters of r so that the likelihood of those trajectories
[1424.66s -> 1427.06s]  under this graphical model is maximized?
[1427.06s -> 1429.02s]  And that's what I'll discuss in the next part of the lecture.
