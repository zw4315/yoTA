# Detected language: en (p=1.00)

[0.00s -> 6.88s]  All right, next let's get started discussing some reinforcement learning algorithms.
[6.88s -> 10.08s]  There are quite a few reinforcement learning algorithms that we'll cover in this course,
[10.08s -> 15.48s]  but at a high level, all of these algorithms will share more or less the same high-level
[15.48s -> 18.42s]  anatomy.
[18.42s -> 22.80s]  They will consist of three basic parts.
[22.80s -> 28.92s]  The first part, which I will always draw in orange, is to generate samples.
[28.92s -> 33.46s]  So reinforcement learning is about learning through trial and error, in some sense.
[33.46s -> 39.06s]  And trial, the trial part of trial and error means actually attempting to run your policy
[39.06s -> 44.04s]  in your environment, actually have it interact with your Markov decision process, and collect
[44.04s -> 45.04s]  samples.
[45.04s -> 46.04s]  What are samples?
[46.04s -> 48.28s]  Well, samples are trajectories.
[48.28s -> 52.30s]  So you will interact with your MDP, and typically what you will be doing is you will
[52.30s -> 56.88s]  be sampling trajectories from the trajectory distribution, typically the one induced by
[56.88s -> 61.08s]  your policy, although when we talk about exploration later, sometimes we might actually choose
[61.08s -> 64.92s]  to sample from a slightly different trajectory distribution than the one that our policy
[64.92s -> 66.20s]  would define.
[66.20s -> 71.20s]  But for now let's just assume that generating samples means sampling trajectories from
[71.20s -> 75.94s]  the trajectory distribution defined by your policy, which simply means running your policy
[75.94s -> 79.60s]  in the environment.
[79.60s -> 86.20s]  Then we will have the green box, and the green box corresponds to learning some kind
[86.20s -> 87.24s]  of model.
[87.24s -> 91.76s]  This could be literally a model of the dynamics in a model-based RL algorithm, or it can
[91.76s -> 99.12s]  be some sort of more implicit model, such as a value function, and this green box
[99.12s -> 103.80s]  basically corresponds to estimating something about your current policy, something about
[103.80s -> 110.00s]  how your policy is doing, how well it's performing, what kind of rewards it's attaining.
[110.00s -> 113.60s]  And then we'll have the blue box, which is where you have to change your policy
[113.60s -> 115.86s]  to make it better.
[115.86s -> 118.10s]  And then you repeat this process.
[118.10s -> 122.42s]  Pretty much all of the algorithms that we will cover will have these three parts.
[122.42s -> 125.46s]  In some cases, one of these parts might be very, very simple.
[125.46s -> 128.60s]  In some cases, some of them might be very complex.
[128.60s -> 132.22s]  But all three of them are generally going to be present.
[132.22s -> 137.12s]  So here's some simple examples.
[137.12s -> 143.56s]  Let's say that we're going to run our policy, and we'll generate sample trajectories
[143.56s -> 148.94s]  denoted by these black lines, and we'll evaluate them to see if they're good or bad.
[148.94s -> 151.58s]  So evaluate means just sum up their rewards.
[151.58s -> 154.76s]  Summing up their rewards is what happens in the green box.
[154.76s -> 158.36s]  And then when we improve the policy, we might try to make the good trajectories,
[158.36s -> 162.68s]  kind of the green checkmarks, be more likely, and the bad trajectories, the red ones,
[162.68s -> 163.76s]  be less likely.
[163.76s -> 166.08s]  So that's the improvement step.
[166.08s -> 171.26s]  Now what I'm describing here is the basic high-level scheme for a policy gradient algorithm,
[171.26s -> 175.00s]  which we'll learn about in detail in the next lecture.
[175.00s -> 180.74s]  So in a policy gradient algorithm, the green box, the box where we estimate something about
[180.74s -> 182.94s]  our policy, is very, very simple.
[182.94s -> 187.78s]  It simply consists of summing up the rewards along trajectories that we sampled,
[187.78s -> 189.40s]  and that tells us how good our policy is.
[189.40s -> 194.06s]  So the green box is just a summation.
[194.06s -> 199.40s]  The blue box might involve calculating the gradient of the reward of your policy,
[199.44s -> 201.64s]  and we'll talk about how to do that in the next lecture,
[201.64s -> 205.12s]  and applying that gradient to the policy parameters theta.
[205.12s -> 210.00s]  So that's a very simple trial-and-error style reinforcement learning algorithm.
[210.00s -> 214.44s]  Run your policy, get some trajectories, measure how good those trajectories are,
[214.44s -> 222.04s]  and then modify the policy to make the better trajectories have a higher probability.
[222.04s -> 227.52s]  You could also imagine doing a model-based RL procedure.
[227.52s -> 230.16s]  You can kind of think of this as RL by backprop.
[230.16s -> 233.52s]  So maybe in the green box, you learn a model.
[233.52s -> 239.60s]  You learn some other neural network, f phi, such that ST plus one is approximately equal
[239.60s -> 246.00s]  to f phi of STAT, and you train it, f phi, on data generated in the orange box with
[246.00s -> 247.78s]  supervised learning.
[247.78s -> 252.40s]  So maybe you have a whole other neural network that goes from STAT to ST plus one.
[252.40s -> 256.44s]  Now this green box is much more complex than the one we had on the previous slide.
[256.44s -> 260.20s]  On the previous slide, we would just sum over the rewards in our trajectories.
[260.20s -> 262.56s]  Here we're actually fitting a whole other neural network.
[262.56s -> 264.50s]  The summation might take a millisecond.
[264.50s -> 269.60s]  This might take minutes to train, or maybe even hours if it's using images.
[269.60s -> 278.72s]  And then in the blue box, we might backprop through f and r to train the policy pi
[278.72s -> 279.84s]  theta.
[279.84s -> 283.60s]  So if we want to calculate the reward of the policy, we can basically compose the policy
[283.60s -> 289.04s]  with f, actually use our automatic differentiation software to calculate the reward and backprop
[289.04s -> 291.84s]  through all of them to optimize the policy.
[291.84s -> 295.84s]  We're going to cover methods that do some variant of this when we talk about model-based
[295.84s -> 298.36s]  reinforcement learning much later in the course.
[298.36s -> 301.08s]  If the details of this don't currently make sense, don't worry about it.
[301.08s -> 304.88s]  The only point of this slide is to explain to you the different incarnations of the
[304.88s -> 309.56s]  green box and the blue box that we might see in some very different reinforcement learning
[309.56s -> 310.56s]  algorithms.
[311.52s -> 317.08s]  All right, now which parts in this whole process are expensive and which parts might
[317.08s -> 318.92s]  be cheap?
[318.92s -> 327.04s]  Well, the orange box, its cost in terms of time and computation depends a great deal
[327.04s -> 329.28s]  on what kind of problem you're solving.
[329.28s -> 334.12s]  If you're collecting samples by running a real world system like a real robot, a real car,
[334.12s -> 339.10s]  a real power grid, a real chemical plant, whatever, the orange box can be potentially
[339.14s -> 340.66s]  extremely expensive, right?
[340.66s -> 345.42s]  Because you have to collect data in real time, at least until we invent time travel.
[345.42s -> 349.30s]  And if you need like thousands of samples each iteration of your RL algorithm, this can
[349.30s -> 351.94s]  be very, very costly.
[351.94s -> 356.26s]  On the other hand, if you're collecting samples in the MuJoCo simulator that all of you are
[356.26s -> 362.12s]  using for homework one, then the MuJoCo simulator can run up to 10,000x real time.
[362.12s -> 365.78s]  So the cost of the orange box might actually be trivial.
[365.78s -> 371.02s]  So depending on which of these regimes you're in or where you are on the spectrum, you
[371.02s -> 375.62s]  might care more or less about how many samples you need in the orange box, which will influence
[375.62s -> 377.38s]  your choice of reinforcement learning algorithms.
[377.38s -> 383.26s]  So this can range from prohibitively expensive to trivially cheap, depending on how you're
[383.26s -> 384.26s]  learning.
[384.26s -> 389.02s]  The green box also could range from extremely cheap to extremely expensive.
[389.02s -> 393.34s]  So if you're just estimating the return of your policy by summing up the rewards that
[393.34s -> 395.38s]  you obtained, this is very, very cheap.
[395.38s -> 397.58s]  It's just a summation operator.
[397.58s -> 401.90s]  If you are learning an entire model by training a whole other neural net, this might be very
[401.90s -> 402.90s]  expensive.
[402.90s -> 409.46s]  It might require, you know, a big supervised learning run in the inner loop of your RL algorithm.
[409.46s -> 414.22s]  Similarly in the blue box, if you're just taking one gradient step, this might be
[414.22s -> 416.66s]  fairly cheap.
[416.66s -> 421.30s]  If you have to backprop through your model and your policy, like I discussed in the
[421.30s -> 425.58s]  model-based slide, this might be very expensive.
[425.58s -> 428.86s]  And there will be algorithms that fall at different points of the spectrum for different
[428.86s -> 429.86s]  boxes.
[429.86s -> 434.86s]  For instance, a Q-learning algorithm, which we'll cover a couple of weeks from now, basically
[434.86s -> 439.14s]  spends all of its effort in the green box, and the blue box is just an art max.
