# Detected language: en (p=1.00)

[0.00s -> 7.02s]  In the next section of today's lecture, we're going to talk about meta-learning algorithms.
[7.02s -> 11.40s]  Meta-learning is a kind of a logical extension of multi-task learning, where instead of
[11.40s -> 14.94s]  simply learning how to solve a variety of tasks, we're going to use many different
[14.94s -> 18.20s]  tasks to learn how to learn new tasks more quickly.
[18.20s -> 22.62s]  So I'll first give a general introduction to meta-learning in a kind of more conventional
[22.62s -> 26.64s]  supervised learning setting, and then I'll discuss how these ideas can be instantiated
[26.64s -> 28.32s]  in our realm.
[28.32s -> 30.30s]  So what is meta-learning?
[30.30s -> 34.34s]  If you've learned a hundred tasks already, can you figure out how to learn new tasks
[34.34s -> 36.16s]  more effectively?
[36.16s -> 41.92s]  In this case, having multiple tasks becomes a huge advantage, because if you can generalize
[41.92s -> 47.14s]  the learning process itself from multiple tasks, then you can drastically accelerate
[47.14s -> 49.46s]  acquisition of a new task.
[49.46s -> 53.62s]  So meta-learning essentially amounts to learning to learn, and in practice it's very closely
[53.62s -> 56.96s]  related to multi-task learning, it has many different formulations, although those
[56.96s -> 60.34s]  formulations can be summarized under the same umbrella.
[60.34s -> 65.10s]  So the many different formulations could involve things like learning an optimizer,
[65.10s -> 69.86s]  learning an RNN that reads in a bunch of experience and then solves a new task,
[69.86s -> 74.08s]  or even just learning a representation in such a way that it could be fine-tuned more
[74.08s -> 77.06s]  quickly to a new task.
[77.06s -> 79.74s]  So these might seem like very, very different things.
[79.74s -> 85.10s]  This is a cartoon from a blog post by Ka Lee that illustrates the learning an optimizer
[85.10s -> 87.78s]  kind of idea.
[87.78s -> 92.14s]  Even though these seem like very different things, they can actually be instantiated
[92.14s -> 96.56s]  in the same framework, and many of the different techniques for solving meta-learning
[96.56s -> 100.68s]  problems, once you kind of drill down into the details, actually end up looking a lot
[100.68s -> 106.88s]  like simply different architectural choices for the same basic algorithmic scaffold.
[106.88s -> 112.12s]  Okay, so why is meta-learning a good idea?
[112.14s -> 115.98s]  Super reinforcement learning, especially model-free learning, requires a huge number of samples.
[115.98s -> 121.78s]  So if you can meta-learn a faster reinforcement learner, then you can learn new tasks efficiently.
[121.78s -> 125.02s]  So what might a meta-learner do differently?
[125.02s -> 130.86s]  Well, a meta-learned RL method might explore more intelligently because something about
[130.86s -> 134.98s]  solving those prior tasks tells it how to structure its exploration to acquire a
[134.98s -> 136.50s]  new task quickly.
[136.50s -> 140.26s]  It might avoid trying actions that it knows are useless, so maybe it doesn't know how
[140.26s -> 143.36s]  to solve the new task precisely, but it knows that certain kinds of behaviors are just
[143.36s -> 145.64s]  never good to do.
[145.64s -> 148.52s]  It might also acquire the right features more quickly, so maybe it was trained in
[148.52s -> 154.06s]  such a way that the network can change rapidly to modify its feature representations
[154.06s -> 157.76s]  for the new task.
[157.76s -> 163.22s]  Let me describe a very basic recipe to set up meta-learning for a supervised learning
[163.22s -> 164.68s]  problem.
[165.66s -> 171.78s]  This recipe will, I think, demystify a lot of the question marks that surround meta-learning.
[171.78s -> 176.70s]  This is an image from a paper by Rabi and La Rochelle from 2017, and it illustrates
[176.70s -> 179.38s]  how meta-learning for image recognition could work.
[179.38s -> 182.88s]  I realize that image recognition is pretty different from RL, but we'll see that very
[182.88s -> 186.52s]  similar principles will actually work in RL as well.
[186.52s -> 190.62s]  So in regular supervised learning, you would have a training set and a test set.
[190.62s -> 195.48s]  In meta-learning, what we're going to have is actually a set of training sets and a set
[195.48s -> 198.40s]  of test sets.
[198.40s -> 205.36s]  Meta-training refers to the set of datasets that we'll use for the meta-learning process.
[205.36s -> 209.22s]  Meta-testing is referring to what we're going to see when we get the new task.
[209.22s -> 213.10s]  So meta-training is basically source domain, meta-testing is target domain.
[213.10s -> 218.48s]  Each of the training sets during meta-training contains some image classes, and the test
[218.48s -> 220.94s]  set contains test images of those classes.
[220.94s -> 225.02s]  So in this example, let's say that we have five classes in every task, but those classes
[225.02s -> 226.26s]  mean different things.
[226.26s -> 230.78s]  So for that first task, class zero is bird, class one is mushroom, class two is dog,
[230.78s -> 235.26s]  class three is person, class four is piano, and then in the test set, there's a dog
[235.26s -> 242.90s]  and a piano, and then in the second task, class zero is gymnast, class one is landscape,
[242.90s -> 247.42s]  class two is tank, class three is barrel, et cetera.
[247.42s -> 251.36s]  These assignments can be either done by hand or they can be randomized and arbitrary, in
[251.36s -> 255.84s]  this case this is random, and the idea is that you're going to look at those different
[255.84s -> 261.12s]  training sets and then you're going to use their corresponding test sets to meta-train
[261.12s -> 266.08s]  some sort of model that will be able to then take in a new training set for some
[266.08s -> 269.26s]  new class that you've never seen before and then do well on its corresponding test
[269.26s -> 273.44s]  set.
[273.44s -> 275.68s]  So here's how we can look at this.
[275.68s -> 281.94s]  Another supervised learning takes in some input x and produces a prediction y.
[281.94s -> 286.68s]  So the input x might be, for example, an image and the output y might be a label.
[286.68s -> 291.30s]  Supervised meta-learning could be thought of as just a function that takes in an entire
[291.30s -> 296.86s]  training set dtrain as well as a test image x and produces the label for that test
[296.86s -> 299.86s]  image y.
[299.86s -> 301.96s]  So it's not actually all that different.
[301.96s -> 305.90s]  It's just some kind of function that will read in that training set and a test image
[305.90s -> 309.18s]  and make a prediction on the test image.
[309.18s -> 312.36s]  Of course, you have to resolve a few questions if you want to accentuate this.
[312.36s -> 315.06s]  For example, how do you read in the training set?
[315.06s -> 317.06s]  There are many options for this.
[317.06s -> 320.02s]  Things like recurrent neural networks or transformers can work pretty well for this.
[320.02s -> 325.58s]  So you could imagine a recurrent neural network that reads in x1, y1, x2, y2, x3, y3,
[325.58s -> 331.62s]  which are the training image label tuples, then reads in the test image xtest and then
[331.62s -> 334.68s]  predicts the test label ytest.
[334.68s -> 340.16s]  So you have this little few shot training set, a test input, and a test label.
[340.16s -> 344.46s]  We'll talk more about the specifics of this later.
[344.46s -> 348.12s]  But first, let's talk about what is it that's being learned.
[348.12s -> 352.96s]  So if you're learning to learn, and then you take that and you deploy it on your
[352.96s -> 355.64s]  target domain, what is learning?
[355.64s -> 358.12s]  So meta-learning is training this f.
[358.12s -> 360.36s]  What is the learning part?
[360.38s -> 363.82s]  Well, to try to understand this, let's imagine the following schematic picture
[363.82s -> 364.90s]  for generic learning.
[364.90s -> 368.86s]  Generic learning, you have some kind of parameter theta, and you're going to find
[368.86s -> 372.38s]  the theta that minimizes some loss function on the training set.
[372.38s -> 375.06s]  And let's call this process of finding this argmin flearn.
[375.06s -> 380.78s]  So flearn takes in a training set and outputs the argmin of your model
[380.78s -> 383.38s]  parameters theta.
[383.40s -> 393.88s]  Generic meta-learning can be thought of as finding the argmin of the loss over your
[393.88s -> 401.36s]  test set for some parameters phi, where these parameters phi are a function
[401.36s -> 403.44s]  of your training set, okay?
[403.44s -> 407.00s]  So you have some function f theta, which is now a learned function,
[407.00s -> 409.28s]  it's no longer a fixed learning algorithm.
[409.28s -> 413.32s]  F theta takes in a training set, and it produces some parameters phi.
[413.32s -> 416.30s]  And those parameters phi are everything you need to know about the training set
[416.30s -> 418.10s]  to do well on the test set.
[418.10s -> 425.34s]  And the way you do meta-learning is you train f theta so that the loss on the
[425.34s -> 429.42s]  test sets of the meta-training tasks is minimized.
[429.42s -> 431.50s]  So it's kind of a second-order thing.
[431.50s -> 436.78s]  You're going to train f theta, which reads in dtrain, so that the output
[436.78s -> 441.10s]  of f theta works well on dtest.
[441.12s -> 445.44s]  So f theta then becomes the learning procedure.
[445.44s -> 448.96s]  So what is f theta for the RNN example?
[448.96s -> 452.64s]  Well, f theta is the part of the RNN that reads in the training set.
[452.64s -> 456.00s]  So you can think of the parameters of f theta, this theta star as being the
[456.00s -> 459.20s]  parameters of this RNN, and it's going to produce some sort
[459.20s -> 460.08s]  of hidden activation.
[460.08s -> 462.72s]  So once it reads in that training set, it has some hidden activation,
[462.72s -> 465.00s]  hi, for the task i.
[465.00s -> 468.48s]  And that hidden activation is then given to a little classifier that takes in the
[468.50s -> 473.42s]  hidden activation and a test image x and produces y.
[473.42s -> 478.74s]  So this little bit at the end, that's your classifier.
[478.74s -> 483.22s]  And its parameters, phi, are simply the combination of h, the hidden
[483.22s -> 486.06s]  activations, and its own parameters, theta p.
[486.06s -> 488.82s]  So it has its own parameters, it's a little neural net, so that's theta p.
[488.82s -> 493.38s]  And it takes in the hidden activations from the RNN encoder, and that's hi.
[493.38s -> 498.34s]  So that is what is phi i for this RNN meta-learner.
[499.20s -> 501.36s]  Now, there are other kinds of meta-learners you could devise, and they will have
[501.36s -> 504.40s]  different notions of phi i, but this is kind of the simplest.
[504.40s -> 509.00s]  That the parameters that you, quote unquote, learn for a new task are simply
[509.00s -> 514.72s]  the hidden state of this RNN and the parameters of that top layer.
[519.28s -> 522.60s]  So the process of learning a new task basically then amounts to just running
[522.60s -> 524.44s]  the RNN forward and getting the hidden state.
[529.32s -> 532.80s]  So just to recap precisely how this works and how it's trained.
[532.80s -> 536.28s]  You have an RNN, it reads in a sequence of images and their labels.
[536.28s -> 537.72s]  It produces a hidden state.
[537.72s -> 541.20s]  That hidden state goes to a little neural network that takes in a test
[541.20s -> 543.24s]  image and produces its label.
[543.24s -> 546.68s]  The meta-training process trains the parameters of all of these networks.
[546.68s -> 548.56s]  It trains the parameters of the RNN encoder, and
[548.56s -> 550.76s]  it trains the parameters of that little thing at the end.
[553.40s -> 556.68s]  When you go to the target task, which we call meta-test time,
[556.68s -> 559.18s]  you get a training set for the target task.
[559.18s -> 562.14s]  That training set is then encoded using that RNN encoder,
[562.14s -> 564.10s]  which produces a new HI.
[564.10s -> 569.58s]  That new HI is then concatenated with the parameters of the little classifier
[569.58s -> 573.18s]  at the end, which is not adapted to the new task, and that's phi i.
[573.18s -> 576.06s]  And then your prediction depends only on phi i.
[576.06s -> 579.50s]  So in practice, this is a very long way of explaining something very simple,
[579.50s -> 582.22s]  which is that in practice you just run this RNN forward and you get an answer.
[583.42s -> 585.38s]  But this is the explanation of how it relates to meta-learning.
