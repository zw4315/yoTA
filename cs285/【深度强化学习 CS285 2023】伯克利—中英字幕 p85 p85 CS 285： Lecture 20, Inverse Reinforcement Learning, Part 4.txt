# Detected language: en (p=1.00)

[0.00s -> 6.56s]  All right, in the last part of today's lecture, we're going to talk about a deeper relationship
[6.56s -> 11.44s]  between these kinds of approximate inverse reinforcement learning methods and another
[11.44s -> 17.20s]  class of algorithms that also learn distributions called generative adversarial networks, and
[17.20s -> 20.56s]  we'll see that exploring this connection actually gives us a lot of clarity on other
[20.56s -> 25.40s]  ways that we design IRL and imitation algorithms.
[25.40s -> 28.54s]  So one of the things that some of you might have recognized is that the structure
[28.58s -> 33.46s]  of the algorithm that I described in the previous part of the lecture looks a bit like a game.
[33.46s -> 38.94s]  We have this initial policy, we produce samples from that policy, we have human demonstrations
[38.94s -> 43.96s]  and we get samples from the human demonstrations, we combine these samples to produce some
[43.96s -> 48.62s]  kind of reward function that makes the human demonstrations look good and the policy
[48.62s -> 56.58s]  samples look bad, and then we change the policy so that the policy actually optimizes
[56.62s -> 61.10s]  that reward function, making it harder to distinguish from the demos.
[61.10s -> 65.50s]  So you can kind of think of it as the reward function is trying to make human demos look
[65.50s -> 68.66s]  very different from policy samples according to the current reward because it's trying
[68.66s -> 73.86s]  to give high reward to the samples and low reward, sorry, the other way around, high
[73.86s -> 79.10s]  reward to the human demos and low reward to the policy samples, and the policy's
[79.10s -> 84.10s]  trying to make the opposite, it's trying to make its samples look good according to
[84.10s -> 89.54s]  the reward, ideally as good as the human samples. So you can almost think of it as a kind of a game
[89.54s -> 93.70s]  being played between the policy and the reward function, where the policy is trying to fool the
[93.70s -> 97.42s]  reward function to thinking that it's just as good as the human and the reward function
[97.42s -> 101.48s]  is trying to find a reward that will allow to distinguish the human from the policy.
[101.48s -> 110.14s]  In fact, this connection is not just superficial, the connection between inverse RL and
[110.18s -> 117.10s]  games can be made formal, and inverse RL can be related to something called generative adversarial networks.
[117.10s -> 123.02s]  So a generative adversarial network, it's many things. It's a method for turning horses into
[123.02s -> 129.86s]  zebras, a method for producing very realistic faces, a method for turning line drawings
[129.86s -> 134.46s]  into cats, but what it really is is an approach to generative modeling. It's an
[134.46s -> 139.78s]  approach to learn a neural network that captures a particular given data distribution, such
[139.78s -> 144.66s]  as the distribution of realistic faces, or realistic cats, or realistic zebras.
[146.42s -> 150.10s]  For those of you that are not familiar with generative adversarial networks, they consist
[150.10s -> 156.66s]  of two neural networks, a generator network which takes in a random sample, random noise z,
[156.66s -> 164.90s]  and turns it into some sample x, which ideally should correspond to samples that resemble
[164.98s -> 170.42s]  the data distribution. So if you train this system on faces, the sample x should look
[170.42s -> 179.38s]  like realistic faces. The data consists of samples from the true unknown distribution
[179.38s -> 184.66s]  p star of x, and the discriminator, there's a second network called the discriminator,
[184.66s -> 190.94s]  which is a binary classifier that tries to assign the label true to all of the samples
[190.94s -> 195.66s]  from the data to all the samples from p star, and the label false to all the samples from the
[195.66s -> 204.30s]  generator p theta. So here d psi of x is the binary classifier that represents the discriminator,
[204.30s -> 211.26s]  and what, and here d psi of x is basically the probability that this sample is true, meaning
[211.26s -> 215.98s]  the probability that discriminator thinks this sample is a real sample from p star rather than
[215.98s -> 220.54s]  a fake sample from the generator. So the objective for the discriminator is to maximize
[220.54s -> 226.70s]  the log probability of the samples being true on p star and minimize the log probability,
[226.70s -> 232.14s]  or equivalently maximize the log of 1 minus the probability for all the samples from p theta.
[232.14s -> 236.30s]  So it's trying to make the p theta samples look fake, the p star samples look real.
[238.06s -> 243.82s]  So it's just another neural network that takes in x and outputs the probability of a Bernoulli
[243.82s -> 249.58s]  variable. And then the generator is trained to fool the discriminator. It's trained to produce
[249.58s -> 253.90s]  images x for which the discriminator gives a high probability of them being real.
[255.10s -> 258.86s]  Now this is very much like the inverse RL procedure that I outlined before.
[259.98s -> 264.30s]  In fact, you can frame inverse RL as a kind of GAN.
[266.78s -> 270.62s]  So one choice you have to make is what kind of discriminator should you use.
[272.54s -> 277.66s]  So in GANs, we can actually show that the optimal discriminator, the Bayes optimal
[277.74s -> 283.18s]  discriminator, at convergence should represent the density ratio between p star and p theta.
[283.74s -> 287.90s]  Now in practice, we usually don't have an optimal discriminator, but if we were to train
[287.90s -> 293.90s]  the discriminator convergence, we would expect it to converge to a network that for every x
[295.02s -> 299.34s]  gives the probability as being p star of x divided by p theta of x plus p star of x.
[300.70s -> 304.06s]  Now you might say, okay, this is like, this seems kind of weird, like shouldn't the
[304.06s -> 308.86s]  discriminator convergences give probability of 1 to all the samples from p star? Well,
[308.86s -> 315.10s]  not necessarily, because if p theta generates some images that are identical to images that
[315.10s -> 319.82s]  p star might produce, that are identical to images that might come from the real data
[319.82s -> 324.46s]  distribution, then the discriminator can't give them a probability of 1.0 because they might
[324.46s -> 330.06s]  actually be fake. So it has to produce probabilities according to this ratio. If p theta is very bad,
[330.06s -> 335.66s]  this is not a problem, because usually in that case, the realistic images will all have a very
[335.66s -> 340.22s]  low probability of p theta, and the fake images will have very low probability of p star, but as
[340.22s -> 345.58s]  p theta gets better and better, the discriminator is going to produce values other than 0 and 1.
[345.58s -> 350.38s]  In fact, at convergence, when p theta of x is actually equal to p star, you would expect
[350.38s -> 356.86s]  this discriminator to produce probabilities that are always 0.5. Okay, this may be a little bit
[356.86s -> 363.58s]  of an academic exercise. We can actually use this inference to cast inverse RL as a kind of GAN
[363.58s -> 370.94s]  with a very peculiar kind of discriminator. So for IRL, the optimal policy is going to approach
[370.94s -> 377.74s]  p theta, which is proportional to p of tau times the exponential of r psi of tau. So what
[377.74s -> 380.94s]  we're going to do is we're going to choose this parametrization for our discriminator.
[381.50s -> 388.78s]  We're going to say the discriminator is equal to p of tau times 1 over z times the
[388.78s -> 395.18s]  exponential reward, so that's just the optimal policy distribution, divided by p theta of tau
[395.18s -> 401.98s]  plus p of tau times 1 over z times the exponential reward. So we've basically just
[401.98s -> 407.82s]  directly used the formula for the optimal discriminator, replacing p star with p of tau
[407.82s -> 412.38s]  times the exponential reward, which is reasonable because that's what we'd expect to get at convergence.
[414.38s -> 420.78s]  And if we expand the equation of p theta of tau, just as before, the trajectory probabilities
[420.78s -> 426.54s]  which contain all those initial state and dynamics terms will cancel out, leaving us
[426.54s -> 431.34s]  with a discriminator that has the form 1 over z times the exponential reward divided by the
[431.34s -> 435.90s]  product of the policy's probabilities plus 1 over z times the exponential reward.
[437.82s -> 443.82s]  And something to note here is that this discriminator will only be equal to 0.5 when the policy probabilities
[444.78s -> 449.26s]  are equal to the exponential reward, which means that the policy has converged.
[451.66s -> 456.06s]  And then what we're going to do is we're going to optimize this discriminator, this ratio,
[456.06s -> 460.62s]  with respect to the parameters of the reward, with respect to psi. So we'll basically pick the
[460.62s -> 467.74s]  reward such that this ratio is largest for the human samples and smallest for the policy samples.
[467.82s -> 474.22s]  So the objective for training psi will still be exactly the same as the GAN,
[474.94s -> 480.78s]  maximize the expected value of log d psi under the data distribution p star, and maximize the
[480.78s -> 487.10s]  log of 1 minus d psi under the current policy samples, except that d psi now is not just a
[487.10s -> 492.30s]  neural network that outputs a binary probability, but it has the form of this ratio inside of it
[492.30s -> 493.98s]  and what we're optimizing are the r's.
[498.46s -> 502.94s]  And it turns out that this actually works if you actually optimize z with respect to the same
[502.94s -> 506.30s]  objective as psi. So you don't actually have to calculate the partition function z,
[506.30s -> 510.22s]  you can actually optimize it as part of the same r max, because that turns out to actually
[510.22s -> 514.22s]  yield the correct answer. The derivation of that is a little bit more involved, but you can find
[514.22s -> 517.74s]  that in the paper at the bottom called the connection between generative adversarial networks,
[517.74s -> 520.38s]  inverse reinforcement learning, and energy-based models.
[523.74s -> 526.94s]  And the interesting thing about this derivation is that we don't actually need importance weights
[526.94s -> 532.46s]  anymore, they're actually subsumed into the partition function z, which we optimize
[532.46s -> 537.82s]  along with our reward function parameters. And then the policy is optimized just like
[537.82s -> 546.62s]  the generator in GAN to maximize the reward. So we have our generator slash policy,
[546.62s -> 551.90s]  we generate samples from that policy, we have our data slash demonstrations, which are samples
[551.90s -> 557.98s]  from p star of t of tau, we train our discriminator with respect to the standard GAN objective,
[557.98s -> 561.10s]  but the discriminator has this funny form that we had on the previous slide,
[562.38s -> 565.90s]  and we optimize the policy to maximize the expected reward
[566.46s -> 569.98s]  and entropy. So the policy changes to make it harder to distinguish from the demos.
[572.38s -> 579.02s]  All right, now if we actually instantiate this kind of idea in a practical algorithm,
[579.02s -> 584.06s]  which was done among other papers in this paper linked at the bottom called learning robust rewards
[584.06s -> 588.06s]  with adversarial inverse reinforcement learning, one of the things we can do is we can study
[588.06s -> 591.34s]  whether the rewards that we recover can actually generalize in meaningful ways.
[592.14s -> 597.74s]  So if, for example, we have a demonstration for this angry-looking quadrupedal ant,
[599.02s -> 602.78s]  and then we recover the reward function from it and then apply that reward function
[602.78s -> 607.66s]  to a modified ant where maybe two of the legs are disabled, what it will figure out is that
[607.66s -> 612.06s]  it can still maximize that reward function, but using a very different gate than the expert
[612.06s -> 615.66s]  demonstrated. So that's one of the benefits of inverse reinforcement learning is that if you
[615.66s -> 620.54s]  actually recover the expert's reward function, you can re-optimize that reward function in new
[620.54s -> 625.42s]  conditions and get meaningful behavior, whereas just copying the actions would not have resulted
[625.42s -> 631.50s]  in meaningful behavior. So what can we learn from the demonstrations to enable better transfer?
[631.50s -> 635.82s]  Well, what we need to do is we need to decouple the goal, the reward function from the
[635.82s -> 638.30s]  dynamics, and that's exactly what inverse RL does.
[641.50s -> 647.50s]  All right, now one question we could ask at this point is that in order to connect GANs and
[647.50s -> 652.38s]  inverse RL, we had to use this very fine type of discriminator. Now using that fine type of
[652.38s -> 656.78s]  discriminator was actually advantageous to us because it allowed us to recover a reward function
[656.78s -> 661.34s]  which could then generalize to new situations. But if we don't need the reward function,
[661.34s -> 665.58s]  if we just want to copy the expert's policy, can we just use a regular discriminator?
[666.78s -> 672.46s]  So just like before, we have samples from the policy and samples from the data,
[672.46s -> 679.02s]  but now D is just a regular binary neural net classifier, just like in regular GANs,
[679.02s -> 684.22s]  and then the policy maximizes the expected value of log D to make it harder to distinguish from
[684.22s -> 688.78s]  demos. This idea was introduced in a paper by Ho and Ehrman called Journal of Adversarial
[688.78s -> 692.86s]  Imitation Learning. This algorithm is no longer an inverse reinforcement learning algorithm
[692.86s -> 698.38s]  because it doesn't recover a reward function, but it does recover the expert's policy. So it's
[698.38s -> 703.82s]  a well-defined imitation learning method. So there are a number of trade-offs. It's often
[703.82s -> 708.14s]  simpler to set up the optimization for this kind of approach because there are fewer moving
[708.14s -> 712.30s]  parts, but the discriminator at convergence doesn't really know anything, right? So here
[712.30s -> 717.34s]  at convergence the discriminator will just be 0.5, and you generally can't re-optimize the
[717.34s -> 721.26s]  reward in new settings. So you can't guarantee that you will recover the expert's reward
[721.26s -> 726.14s]  function, but you can guarantee that if everything works correctly you will recover their policy.
[727.82s -> 735.02s]  So just to summarize this, we can cast IRL's adversarial imitation. In classic deep IRL
[735.02s -> 741.10s]  methods like added cost learning, we have the policy's attempts in the human demonstrations,
[741.10s -> 746.22s]  and the reward function tries to minimize the reward of the policy samples and maximize
[746.22s -> 750.62s]  the reward of the human demonstrations, and it learns the distribution p of tau
[750.62s -> 757.18s]  such that the demos have maximum likelihood. The generative adversarial imitation learning approach
[757.18s -> 762.22s]  instead uses a classifier which tries to assign the label false to all the policy samples and
[762.22s -> 768.30s]  true to all the human demonstrations, and d of tau is the probability that tau is a demo basically,
[768.30s -> 773.34s]  and then you use log d of tau as your reward. They're basically the same thing except that for
[773.34s -> 777.66s]  one of them you can recover a reward function, whereas for the other one you don't recover the
[777.66s -> 783.18s]  reward, but you do recover the policy. So the difference is that the discriminator for guided
[783.18s -> 788.86s]  cost learning and other IRL methods just has this particular special form, whereas for the
[788.86s -> 792.86s]  generative adversarial imitation learning approach, the discriminator is just a binary classifier.
[794.70s -> 798.06s]  Now these things have been used in a number of different settings, so you could, for example,
[798.06s -> 802.46s]  combine it with a kind of clustering approach and recover multiple different behavior clusters
[802.46s -> 809.58s]  from heterogeneous demonstrations. You can even perform inverse reinforcement learning or imitation
[809.58s -> 815.18s]  from images and copy simulated locomotion gates and things like that.
[817.50s -> 820.54s]  If you want to read more about inverse reinforcement learning, here are some suggested
[820.54s -> 825.26s]  readings. So these are some of the classic papers on inverse RL, apprenticeship learning
[825.26s -> 829.42s]  via inverse reinforcement learning, and maximum entropy inverse reinforcement learning,
[830.06s -> 833.34s]  and here are some of the more modern papers. So guided cost learning, that's the paper that I
[833.34s -> 838.78s]  discussed that proposes a method to actually scale up max and IRL to the high dimensional deep
[838.78s -> 845.42s]  learning setting. This next paper, deep max symmetric inverse RL, performs inverse RL in
[845.42s -> 850.86s]  small tabular domains but with deep networks. Generative adversarial imitation learning doesn't
[850.86s -> 855.50s]  perform inverse RL, but it does recover the policy, and then learning robust rewards with
[855.50s -> 861.10s]  adversarial inverse RL instantiates the GAN method and studies transform.
