# Detected language: en (p=1.00)

[0.00s -> 6.44s]  All right, so today's lecture is going to be on advanced policy gradient algorithms,
[6.44s -> 11.88s]  and this is probably going to be one of the more technically nuanced lectures in the course,
[11.88s -> 16.72s]  so if this material goes by fast, if it's a little difficult to follow, you know, please
[16.72s -> 20.76s]  make sure to ask questions in the comments, and you know, don't be concerned if you
[20.76s -> 25.16s]  have to go through it a couple of times to really get the full gist of it.
[25.16s -> 30.08s]  So the goal in today's lecture is going to be to combine some of the policy gradient
[30.08s -> 35.88s]  ideas that we discussed before with some more recent concepts that we cover in the course,
[35.88s -> 41.28s]  like policy iteration, to provide a new perspective on policy gradient methods and a little bit
[41.28s -> 46.56s]  of analysis about when and why we would expect policy gradients to work.
[46.56s -> 51.88s]  Now this lecture is primarily about policy gradients, but the insights that I think we
[51.88s -> 56.44s]  get from this type of analysis can also be used for things like actor-critic algorithms,
[56.44s -> 60.66s]  and more generally, to gain a deeper understanding of RL methods.
[60.66s -> 65.40s]  So if you have an interest in reinforcement learning theory, in understanding how and
[65.40s -> 70.30s]  why to design algorithms in a certain way, and if you just want to get kind of more
[70.30s -> 74.52s]  in-depth knowledge about policy gradients, this is the lecture for you.
[74.52s -> 80.70s]  All right, so let's just recap the policy gradient material that we had before.
[80.70s -> 85.86s]  So initially we covered the basic reinforce algorithm, where the procedure is that we
[85.86s -> 92.62s]  sample some trajectories from our current policy, our current suboptimal policy.
[92.62s -> 97.28s]  Then for every one of those trajectories and every time step along each of those trajectories,
[97.28s -> 101.82s]  we calculate the reward to go, and then multiply the grad log pis by the reward to
[101.82s -> 106.72s]  go to get an estimate of the gradient, and then we perform gradient ascent.
[106.72s -> 113.04s]  And then we saw, for instance, in the actor-critic lecture that this reward to go could be computed
[113.04s -> 118.44s]  in various ways, using the Monte Carlo estimator as shown here, or with more sophisticated
[118.44s -> 124.92s]  methods that involve actually learning value function estimators.
[124.92s -> 128.08s]  So of course, like all of the algorithms that we covered, policy gradients follow
[128.08s -> 130.04s]  the same basic recipe.
[130.04s -> 135.24s]  They generate samples in the orange box, fits an estimate of the reward to go, either with
[135.28s -> 139.56s]  the learn value function or just with Monte Carlo estimates in the green box, and then
[139.56s -> 144.88s]  perform gradient ascent in the blue box.
[144.88s -> 149.12s]  So this is kind of the most generic form of the policy gradient, where you have many
[149.12s -> 153.40s]  choices about what goes in for Q-hat.
[153.40s -> 155.32s]  So that's our reward to go.
[155.32s -> 161.72s]  Okay, so that's just a recap of what we covered before.
[161.72s -> 166.68s]  The question we're going to ask now is, why is it that policy gradient actually works?
[166.68s -> 170.84s]  Why is it that we can count on policy gradient to improve our policy?
[170.84s -> 173.80s]  Now the obvious answer is that, well, you just calculated a gradient and then you're
[173.80s -> 177.24s]  doing gradient ascent, so if gradient ascent works, then policy gradient should
[177.24s -> 178.88s]  work too.
[178.88s -> 182.04s]  But there's a bit more to it, and if we do a little bit of analysis, we can actually
[182.04s -> 188.56s]  get some idea of how and why we should expect the policy gradient procedure to work.
[188.56s -> 193.24s]  So one of the ways that we can think of policy gradient more conceptually, so there
[193.24s -> 197.48s]  was this kind of reinforce method on the previous slide, which is a particular instance
[197.48s -> 202.28s]  of policy gradient, but conceptually we can think of a more general way of looking
[202.28s -> 208.84s]  at policy gradient, where we have one step, which is to estimate the approximate advantage
[208.84s -> 213.96s]  for state action tuples for our current policy pi, and there are many ways to perform
[213.96s -> 220.68s]  that estimate using Monte Carlo returns or using learned value functions, and then use
[220.68s -> 225.92s]  this advantage estimate a-hat to somehow improve the policy and get a new and improved
[225.92s -> 230.92s]  policy pi prime, and then repeat this process.
[230.92s -> 234.80s]  Now this way of looking at policy gradient is basically equivalent to what I had on the
[234.80s -> 240.76s]  previous slide, so on the previous slide we were estimating a-hat by generating samples
[240.76s -> 246.12s]  and summing up the rewards to go, and then we were using a-hat to improve the policy
[246.12s -> 250.28s]  by calculating the policy gradient and doing a step of gradient ascent.
[250.28s -> 254.36s]  But it's important to recognize that what we were really doing is in some sense altering
[254.36s -> 259.00s]  these two steps, alternating the estimation of a-hat and using the a-hat estimate to
[259.00s -> 261.48s]  improve the policy.
[261.48s -> 265.76s]  And when I write it out in this way, then it perhaps becomes a little bit more apparent
[265.76s -> 272.08s]  that policy gradients are themselves very related to another algorithm that we learned
[272.08s -> 277.52s]  about when we discussed value-based methods last week.
[277.52s -> 281.36s]  Take a moment to think back to the discussion from last week and what kinds of algorithms
[281.36s -> 282.64s]  we covered.
[282.64s -> 287.08s]  What algorithm do you remember that had this type of structure, where we alternate between
[287.08s -> 292.28s]  estimating the value of the current policy and then using that estimated value to improve
[292.28s -> 298.00s]  that policy?
[298.00s -> 302.40s]  So that was basically the idea behind the policy iteration algorithm, and we covered
[302.40s -> 307.44s]  the policy iteration algorithm when we discussed value-based methods, primarily as a way to
[307.44s -> 312.08s]  set the stage for what would come next, which was Q-learning.
[312.08s -> 316.24s]  But if we think back to policy iteration, we might recognize that policy gradients
[316.24s -> 320.80s]  and policy iteration actually look very similar.
[320.80s -> 325.00s]  One of the main differences is that in policy iteration, when we calculated the new policy
[325.00s -> 328.80s]  pi-prime, we used this argmax rule.
[328.80s -> 333.96s]  Remember that we would always pick the policy pi-prime that assigns a probability of one
[333.96s -> 337.96s]  to the action that is the argmax of the current advantage.
[337.96s -> 342.32s]  In some sense, policy gradients makes a much gentler update.
[342.32s -> 347.96s]  It doesn't immediately jump to the argmax, but it improves a little bit in the direction
[347.96s -> 350.56s]  where the advantages are large, right?
[350.56s -> 354.60s]  Because if you look at the policy gradient expression at the top of the slide, we take
[354.60s -> 359.80s]  grad log pi times a-hat, which means that actions with a large a-hat will get larger
[359.80s -> 364.76s]  probabilities, and actions with a lower a-hat will get lower probabilities.
[364.76s -> 370.64s]  So you could almost think of policy gradients as a kind of softened version of the policy
[370.64s -> 375.48s]  iteration procedure we discussed before, where the policy iteration procedure from last week
[375.48s -> 380.16s]  would simply immediately assign a probability of one to the highest advantage action,
[380.16s -> 385.12s]  whereas policy gradients changes the policy parameters to move towards parameters that
[385.12s -> 389.88s]  give higher probabilities to the argmax, and lower probabilities to actions with worse
[389.88s -> 394.38s]  advantages, but it doesn't immediately jump all the way to one and zero.
[394.38s -> 398.64s]  And that might be desirable if you think that your advantage estimator is not perfect.
[398.64s -> 402.92s]  If your advantage estimator is not perfect, perhaps what you would like to do is
[402.92s -> 407.68s]  just slightly change your policy in the direction suggested by your advantage estimator,
[407.68s -> 413.86s]  and then collect some more transitions, some more samples, and improve your advantage estimator.
[413.86s -> 419.28s]  So in a sense, what we're going to discuss today is how to formalize this notion and
[419.28s -> 424.80s]  explain why we should expect this kind of softened policy iteration procedure, which
[424.80s -> 430.08s]  is policy gradients, to work well.
[430.08s -> 437.24s]  Okay, so let's try to reinterpret the policy gradient as policy iteration.
[437.24s -> 441.78s]  To get started on this, I'm going to show you a little calculation.
[441.78s -> 446.88s]  I'm going to show you how we can write this expression, the difference between the
[446.88s -> 452.58s]  RL objective for some new parameter theta prime minus the objective for some old parameter,
[452.58s -> 458.04s]  as an expression that describes the expected advantage under the new policy, where the
[458.04s -> 461.90s]  advantage is taken from the old policy.
[461.90s -> 468.94s]  So J theta here represents the reinforcement learning objective, which is the expected
[468.94s -> 475.02s]  value under the trajectory distribution induced by a parameter vector theta of the total
[475.02s -> 476.94s]  reward under that distribution.
[476.94s -> 481.74s]  And we'll just go with discount rewards for now, just to keep everything complete.
[481.74s -> 485.30s]  And this is the version where the discount starts from step one.
[485.30s -> 487.94s]  I know that's not the version that we actually use in practice, but this will make the
[487.94s -> 491.34s]  calculations much more accessible.
[491.34s -> 495.66s]  And the claim that I'm going to try to show is that the difference between J theta
[495.66s -> 503.86s]  prime and J theta is the expected value under the trajectory distribution of the previous
[503.86s -> 509.02s]  sorry, under the trajectory distribution of the new policy of the advantage of the
[509.02s -> 510.42s]  previous policy.
[510.42s -> 514.82s]  Okay, let's unpack this statement a little bit to try to get some intuition for why
[514.82s -> 517.78s]  we even want to care about this.
[517.78s -> 523.38s]  So J theta prime minus J theta represents the improvement in the RL objective that
[523.38s -> 527.70s]  we get from going from some old parameter theta to some new parameter theta prime.
[527.70s -> 532.54s]  So you could think of this as maybe we're doing policy iteration, theta prime represents
[532.54s -> 536.18s]  the parameters of the new improved policy, and theta represents the parameters of the
[536.18s -> 538.34s]  old policy.
[538.34s -> 543.54s]  So if we can make J theta prime minus J theta large with respect to theta prime,
[543.54s -> 547.14s]  then we're improving the policy a lot.
[547.18s -> 550.50s]  Now those of you that are paying close attention might realize that there's something a little
[550.50s -> 554.82s]  strange with that statement because if we're maximizing J theta prime minus J theta with
[554.82s -> 558.98s]  respect to theta prime, the J theta is actually irrelevant.
[558.98s -> 563.22s]  And that's absolutely true, so maximizing J theta prime with respect to theta prime
[563.22s -> 569.62s]  is exactly the same as maximizing J theta prime minus J theta with respect to theta prime.
[569.62s -> 575.90s]  So of course the real goal of this derivation is to show that if we maximize the right
[575.94s -> 580.90s]  hand side of this equation, the expectation under pi theta prime of the advantage of
[580.90s -> 585.78s]  pi theta, then we're actually maximizing J theta prime minus J theta, which means
[585.78s -> 591.58s]  that we're actually maximizing J theta prime, which is actually what we want.
[591.58s -> 597.46s]  So the left hand side is the improvement in the return of the policy in going from
[597.46s -> 603.50s]  theta to theta prime, which is what we'll want to optimize with respect to theta prime.
[603.50s -> 609.66s]  So the right hand side of this equation is expressing the expected value under the trajectory
[609.66s -> 613.66s]  distribution induced by the new policy, the one that you're optimizing, of the
[613.66s -> 618.22s]  advantage of the old policy. And why do we care about this quantity? Well
[618.22s -> 622.66s]  because that's essentially what policy iteration does. Policy iteration computes the
[622.66s -> 628.22s]  advantage of the old policy, a pi theta, and then uses that advantage to find a
[628.26s -> 635.98s]  new improved policy, pi theta prime. So if we can show that maximizing the expected
[635.98s -> 640.06s]  value of the advantage of the old policy with respect to a new policy in
[640.06s -> 645.78s]  fact actually optimizes the improvement in the RL objective, then we will have
[645.78s -> 650.02s]  proven that using the advantage of the old policy and maximizing it under
[650.02s -> 655.90s]  the new policy is a correct way to optimize your new policy. So that's why
[655.94s -> 661.38s]  we'd like to show this claim. Take a moment to think about this. Make sure
[661.38s -> 665.50s]  that it makes sense to you why this claim is important, why we want to show
[665.50s -> 671.90s]  this. If it's not clear why this is an important claim to prove, please write
[671.90s -> 675.82s]  a question about it in the comments.
[676.82s -> 682.02s]  Okay, so let's get started with our proof. So our goal is to prove this
[682.02s -> 689.58s]  claim and to start on the path there I'm going to substitute in the equations
[689.58s -> 694.10s]  for the RL objectives. So remember that one way that we can express the RL
[694.10s -> 698.98s]  objective is as the expected value of the value function under the initial
[698.98s -> 703.50s]  state distribution, because the value function captures the expected reward
[703.50s -> 707.90s]  that pi theta would get if it starts in some state s0, so if we average
[707.90s -> 713.14s]  over all the s0s, then we'll get the RL objective. And the reason that I want
[713.14s -> 719.10s]  to write it this way is because now p s0 doesn't depend on theta, so everything
[719.10s -> 722.26s]  that depends on theta is inside the expectation, the distribution with
[722.26s -> 729.80s]  respect to which the expectation is taken does not depend on theta. So what
[729.80s -> 733.74s]  that means is I can change the distribution under which the
[733.74s -> 738.66s]  expectation is taken to be any distribution whose marginal over the
[738.66s -> 744.78s]  initial state is p s0, and this includes p theta tau, p theta prime tau,
[744.78s -> 749.50s]  and p any theta tau. Basically the trajectory distribution for any policy
[749.50s -> 753.94s]  must have the same initial state marginal. So what I can do going from
[753.94s -> 759.18s]  the first line to the second line is I can replace p of s0 with
[759.18s -> 763.46s]  any trajectory distribution that has the same initial state marginal, and the
[763.50s -> 768.26s]  one that I'm going to pick is p theta prime of tau. Why am I putting in p theta
[768.26s -> 772.26s]  prime of tau? Well because my goal ultimately is to turn this into an
[772.26s -> 775.82s]  expectation under p theta prime of tau, that's the right-hand side of our
[775.82s -> 780.44s]  claim, and I can put in p theta prime of tau here because it has the
[780.44s -> 784.06s]  same initial state marginal, and the quantity inside the expectation only
[784.06s -> 789.86s]  depends on s0. So in a sense that value doesn't change if I take the
[789.86s -> 797.78s]  expectation with respect to different policies. And now what I'm going to do
[797.78s -> 803.82s]  is I'm going to use a little telescoping sums trick. So I'm going to
[803.82s -> 811.22s]  replace v pi theta of s0 with this funny quantity. So let's look at this
[811.22s -> 817.14s]  quantity. This is a difference of two sums. The first sum is a sum from
[817.14s -> 822.34s]  0 to infinity, and the second sum is a sum from 1 to infinity. And what I'm
[822.34s -> 828.30s]  summing in both cases is gamma to the power t times v pi theta evaluated at
[828.30s -> 833.54s]  st. So I'm summing the same thing in both cases, just that one sum runs from
[833.54s -> 837.74s]  0 to infinity and the other runs from 1 to infinity. So that means when I take
[837.74s -> 841.46s]  the difference of these two sums, all the elements in the first sum from 1 to
[841.46s -> 845.58s]  infinity get cancelled out by the second sum. So that means that the
[845.62s -> 854.46s]  difference of these two infinite sums is just v pi theta evaluated at s0. Okay, so
[854.46s -> 859.18s]  take a moment to think about this. Make sure that it's clear to you why v
[859.18s -> 864.22s]  pi theta at s0 is equal to the difference between these two infinite
[864.22s -> 872.62s]  sums. Okay, so let's proceed. So next what I'm going to do is I'm going to
[872.66s -> 877.58s]  rearrange the term in these sums a little bit. So what I would like to do
[877.58s -> 882.90s]  is I would like to write this sum essentially as a bunch of terms that
[882.90s -> 890.34s]  look a little bit like advantages. So I'm going to take all of the terms that
[890.34s -> 896.26s]  are getting subtracted off on the right-hand side. So first notice that the
[896.26s -> 899.78s]  sign is switched, so I switch the minus to a plus. So when I switch the
[899.78s -> 906.50s]  minus to a plus, then that becomes the sum from 1 to infinity minus the sum
[906.50s -> 911.06s]  from 0 to infinity, and then I'm going to group the terms. So for every one of
[911.06s -> 917.10s]  those steps, step 1, step 2, step 3, step 4, the term from the second sum, the
[917.10s -> 921.82s]  one from 1 to infinity, has an extra factor of gamma. The term from the
[921.82s -> 925.38s]  first sum lacks that factor, and the term from the second sum is evaluated at
[925.38s -> 931.54s]  times step t plus 1. The term from the first sum is evaluated at times step t. So if I
[931.54s -> 935.38s]  switch the sign, notice that it's plus expectation under p theta prime. Now
[935.38s -> 940.26s]  that means that the the terms inside the parentheses now become a sum from
[940.26s -> 945.18s]  1 to infinity minus the sum from 0 to infinity, and then I'm going to pair
[945.18s -> 949.18s]  off these terms. So for the first term and the first sum, that's
[949.18s -> 952.98s]  evaluating s1. For the first term and the second sum, it's evaluating s0, and
[953.02s -> 958.30s]  they're off by one factor of gamma, so the first term becomes gamma vs1 minus
[958.30s -> 964.90s]  vs0. The second term becomes gamma squared vs2 minus gamma vs0, and I'll
[964.90s -> 968.46s]  take the gamma to the t term out so that I get an expression that looks
[968.46s -> 972.48s]  like this. So it's just a little bit of algebra and rearrangement, but if
[972.48s -> 975.38s]  this rearrangement is unclear to you, please take a moment to think about
[975.38s -> 981.38s]  it and write about it in the comments. Okay, and what I'm going to do
[981.38s -> 986.82s]  next is I'm going to substitute in the definition for j theta prime, and this
[986.82s -> 990.78s]  definition is taken from the thing in the top right corner of the slide. So
[990.78s -> 994.26s]  all I've done on this line is I've simply replaced j theta prime with the
[994.26s -> 1000.06s]  definition of j theta prime. So now at this point, if you look at these two
[1000.06s -> 1004.74s]  equations, it might be apparent to you what I'm about to do, right? I have
[1004.74s -> 1010.90s]  two expectations, both under p theta prime of tau, both from 0 to infinity,
[1010.90s -> 1015.50s]  both have a gamma to the t power in front of them, and one of them has an r,
[1015.50s -> 1021.46s]  and the other one is gamma v s t plus 1 minus v s t. So I can group these two
[1021.46s -> 1026.82s]  expectations together, I can distribute the expectation out, and I get a sum
[1026.82s -> 1030.94s]  from 0 to infinity, gamma to the power t, and then a quantity inside the
[1030.94s -> 1036.26s]  parentheses, which is exactly the advantage function. And crucially, because
[1036.26s -> 1041.74s]  the value function here is the value for pi theta, the advantage is the
[1041.74s -> 1045.58s]  advantage for pi theta as well. So that means that this is equal to the
[1045.58s -> 1050.86s]  expected value under p theta prime of tau of the sum from 0 to infinity of
[1050.86s -> 1057.02s]  gamma to the t of the advantage of pi theta, s t a t. So this proves the
[1057.02s -> 1061.58s]  claim that we want to prove. And just to make sure everyone's on the same page,
[1061.58s -> 1067.00s]  concretely what this proves is that if we maximize the expected value of the
[1067.00s -> 1072.26s]  advantage of an old policy with respect to an expectation for the new
[1072.26s -> 1077.02s]  policy, then we will optimize j theta prime. We'll actually optimize the
[1077.02s -> 1081.30s]  reinforcement learning objective. So essentially this proof shows that
[1081.52s -> 1089.46s]  pulse iteration does the right thing. Okay, so what does this all have to do
[1089.46s -> 1096.50s]  with policy gradients? Well, so what we've shown is that the RL objective,
[1096.50s -> 1100.02s]  maximizing the RL objective is the same as maximizing this equation, where the
[1100.02s -> 1103.98s]  advantage is taken under pi theta and the expectation is taken under pi
[1103.98s -> 1112.82s]  theta prime. So this is our equation. And this equation can be written out as a
[1112.82s -> 1117.02s]  sum over all of our time steps of the expectation under the state action
[1117.02s -> 1121.34s]  marginal of the advantage of theta prime. And the state action marginal
[1121.34s -> 1125.22s]  itself can be written as an expectation with respect to states distributed
[1125.22s -> 1129.90s]  according to pi theta prime of s t and actions distributed according to pi
[1129.90s -> 1137.26s]  theta prime of a t given s t. Now at this point if we wanted to actually
[1137.26s -> 1142.20s]  write down a policy gradient procedure for optimizing this objective, we could
[1142.20s -> 1145.22s]  recall the importance sampling derivation that we had before in the
[1145.22s -> 1151.42s]  policy gradient lecture and write the inner expectation as an expectation under
[1151.42s -> 1155.18s]  pi theta under our old policy but with the addition of these importance
[1155.18s -> 1159.60s]  weights. And now we're getting to something that's very close to the
[1159.60s -> 1162.80s]  policy gradient expressions that we had before because remember we could get
[1162.80s -> 1167.00s]  policy gradient just by differentiating an equation very similar to this with
[1167.00s -> 1171.72s]  respect to theta prime at the value theta equals theta prime. The only
[1171.72s -> 1174.10s]  difference is that our states are still distributed according to theta
[1174.10s -> 1178.92s]  prime, not according to p theta. See we have access to p theta. We can sample
[1178.92s -> 1182.24s]  from p theta. We can't sample from p theta prime because we don't yet know
[1182.24s -> 1187.28s]  what theta prime will be. So it's a big problem here that the
[1187.32s -> 1192.00s]  expectation over states is with respect to theta prime and not theta. If we could
[1192.00s -> 1196.88s]  just use theta instead of theta prime then we would recover the policy
[1196.88s -> 1200.88s]  gradient expression that we had before. So that's the problem that we're left
[1200.88s -> 1206.12s]  with. Essentially we need to somehow ignore the fact that we need to use
[1206.12s -> 1210.44s]  state sampled from p theta prime of s t and instead get away with using state
[1210.44s -> 1218.88s]  sample from p theta of s t. So this is our problem. If we could only get rid of
[1218.88s -> 1223.64s]  that theta then the only remaining theta prime would be in the importance
[1223.64s -> 1226.72s]  weight and then if we differentiate it we would recover the policy gradient
[1226.72s -> 1231.68s]  and show the policy gradient and policy iteration are in fact equivalent.
[1231.68s -> 1235.44s]  So why do we want this to be true? Well we want this to be true because
[1235.64s -> 1240.68s]  if we take this quanium and call it a bar theta prime and we can show that
[1240.68s -> 1245.28s]  j theta prime minus j theta is approximately equal to a bar theta prime
[1245.28s -> 1250.24s]  that means that we can find theta prime by taking the arg max of a bar
[1250.24s -> 1258.08s]  and that means that we can use a hat to get an improved policy pi prime. So
[1258.08s -> 1263.76s]  is this true and when is this true? The claim which I'm going to show in the
[1263.80s -> 1269.84s]  next part of this lecture is that p theta s t is close to p theta prime of s t
[1269.84s -> 1275.36s]  when pi theta is close to pi theta prime. Now that might seem like a kind
[1275.36s -> 1278.40s]  of obvious statement but it's actually not quite obvious to prove this
[1278.40s -> 1282.16s]  statement in a non-vacuous way.
