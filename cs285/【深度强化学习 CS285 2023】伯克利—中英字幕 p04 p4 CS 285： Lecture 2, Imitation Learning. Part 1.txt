# Detected language: en (p=1.00)

[0.00s -> 7.00s]  Hi, welcome to Lecture 2 of CS285. Today we're going to talk about supervised learning of behaviors.
[7.00s -> 11.00s]  So let's start with a little bit of terminology and notation.
[11.00s -> 19.00s]  So we're going to see a lot of terminology in this lecture denoting policies that we're going to be learning from data.
[19.00s -> 25.00s]  And we're not going to talk about reinforcement learning just yet, we're going to talk about supervised learning methods for learning policies.
[25.00s -> 30.00s]  But we'll get started with a lot of the same terminology that we'll use in the rest of the course.
[30.00s -> 40.00s]  So typically, if you want to represent your policy, you have to represent a mapping from whatever the agent observes to its actions.
[40.00s -> 45.00s]  Now this is not such a strange object, those of you that are familiar with supervised learning,
[45.00s -> 50.00s]  you can think of this as much the same way that you represent, for example, an image classifier.
[50.00s -> 58.00s]  An image classifier maps from inputs x to outputs y, a policy maps from observations o to outputs a.
[58.00s -> 63.00s]  But other than changing the names of the symbols, in principle, things haven't actually changed all that much.
[63.00s -> 69.00s]  So in the same way that you might train an image classifier that looks at a picture and outputs the label of that picture,
[69.00s -> 73.00s]  you could train a policy that looks at an observation and outputs an action. Same principle.
[73.00s -> 79.00s]  We're going to use the letter pi to denote the policy and the subscript theta to denote the parameters of that policy,
[79.00s -> 83.00s]  which might be, for example, the weights in a neural network.
[83.00s -> 90.00s]  Now typically, in a control problem, in a decision-making problem, things exist in the context of a temporal process.
[90.00s -> 95.00s]  So at this instant in time, I might look at the image from my camera and make a decision about what to do,
[95.00s -> 99.00s]  and then at the next instant in time, I might see a different image and make a different decision.
[99.00s -> 107.00s]  So typically, we will write both the inputs and outputs with a subscript, lowercase t, to denote the time step t.
[108.00s -> 113.00s]  For almost all of the discussion in this course, we're going to operate on discrete time,
[113.00s -> 119.00s]  meaning that t, you can think of it as an integer that starts with zero and is then incremented with every time step.
[119.00s -> 124.00s]  But of course, in a real physical system, t might correspond to some continuous notion of time.
[124.00s -> 129.00s]  For example, t equals zero might be zero milliseconds, and to the control process,
[129.00s -> 135.00s]  t equals one might be 200 milliseconds, t equals two might be 400 milliseconds, and so on.
[135.00s -> 138.00s]  Now in an actual sequential decision-making process, of course,
[138.00s -> 143.00s]  the action that you choose will affect the observations that you see in the future.
[143.00s -> 149.00s]  And your actions are not going to be image labels like they are, for example, in the standard image classification task,
[149.00s -> 153.00s]  but they're going to be decisions, decisions that are bearing on future outcomes.
[153.00s -> 158.00s]  So instead of predicting whether the picture is a picture of a tiger, you might predict a choice of action,
[158.00s -> 161.00s]  like run away or ignore it or do something else.
[161.00s -> 164.00s]  But this doesn't really change the representation of the policy.
[164.00s -> 169.00s]  If you have a discrete action space, you would still represent the policy in basically the same exact way
[169.00s -> 173.00s]  that you represent an image classifier if your inputs are images.
[173.00s -> 176.00s]  You could also have continuous action spaces.
[176.00s -> 180.00s]  And in that case, perhaps the output would not be a discrete label,
[180.00s -> 183.00s]  maybe it would be the parameters of a continuous distribution.
[183.00s -> 188.00s]  A very common choice here is to represent the distribution over A as a Gaussian distribution,
[188.00s -> 192.00s]  which means that the policy would output the mean and the covariance for that Gaussian.
[192.00s -> 195.00s]  But there are many other choices we could make.
[195.00s -> 202.00s]  So, to recap our terminology, we're going to have observations which we denote with the letter O
[202.00s -> 206.00s]  and the subscript T to denote that it's the observation of time T.
[206.00s -> 211.00s]  Our output will be actions, which we denote with the letter A and a subscript T.
[211.00s -> 215.00s]  And our goal will be to learn policies that, in the most general sense,
[215.00s -> 219.00s]  are going to be distributions over A given O.
[219.00s -> 224.00s]  Now, something I want to note here, because this is sometimes a source of confusion,
[224.00s -> 227.00s]  a policy needs to provide us with an action to take.
[227.00s -> 230.00s]  In the most general case, policies are distributions,
[230.00s -> 236.00s]  meaning that they assign a probability to all the possible actions given a particular observation.
[236.00s -> 241.00s]  Of course, a policy could be deterministic, meaning that it prescribes a single action for a given observation.
[241.00s -> 246.00s]  That's a special case of a distribution, it's just a distribution that assigns a probability of 1 to something
[246.00s -> 248.00s]  and a probability of 0 to everything else.
[248.00s -> 253.00s]  So, in most cases, we'll actually talk about stochastic policies,
[253.00s -> 255.00s]  policies that specify a distribution over actions,
[255.00s -> 259.00s]  but keep in mind that this is fully general in the sense that deterministic policies
[259.00s -> 261.00s]  are simply a special case of these distributions.
[261.00s -> 264.00s]  And it's very convenient to talk about distributions here for the same reason
[264.00s -> 267.00s]  that we tend to talk about distributions in supervised learning.
[267.00s -> 269.00s]  So in supervised learning, if you're classifying images,
[269.00s -> 273.00s]  perhaps you only really want to predict one label for a given image,
[273.00s -> 275.00s]  but you might still learn distribution over labels
[275.00s -> 277.00s]  and then just take the most likely output,
[277.00s -> 279.00s]  and that makes training these things a lot more convenient.
[279.00s -> 282.00s]  And it's the same way with decision-making and control,
[282.00s -> 286.00s]  that training these policies as probability distributions
[286.00s -> 291.00s]  often is much more convenient, even if in the end, we only want the single best action.
[293.00s -> 295.00s]  Now, one more term that we have to introduce,
[295.00s -> 298.00s]  and here we're going to start getting to some of the idiosyncrasies
[298.00s -> 301.00s]  of sequential decision-making, is the notion of a state.
[303.00s -> 307.00s]  The state is going to be denoted with the letter S and also the subscript T,
[307.00s -> 311.00s]  and a state is, in general, a distinct thing from the observation.
[311.00s -> 314.00s]  Understanding this distinction will be very important
[314.00s -> 316.00s]  for certain types of reinforcement learning algorithms.
[316.00s -> 320.00s]  It's not so important for today's lecture because for imitation learning,
[320.00s -> 322.00s]  we often don't need to make this distinction,
[322.00s -> 325.00s]  although even here it'll be important when we try to understand
[325.00s -> 329.00s]  the theoretical underpinnings of some of these imitation learning methods.
[332.00s -> 335.00s]  And sometimes when we learn policies,
[335.00s -> 341.00s]  we'll write policies as distributions over A given S rather than given O.
[341.00s -> 345.00s]  I will try to point out when this is happening and why,
[345.00s -> 348.00s]  but to understand the difference between these two objects,
[348.00s -> 351.00s]  let's talk about the difference between states and observations,
[351.00s -> 353.00s]  and then we'll come back to this.
[353.00s -> 357.00s]  And typically we'll refer to policies that are conditioned on a full state
[357.00s -> 361.00s]  as fully observed policies, as opposed to policies conditioned on observation,
[361.00s -> 363.00s]  which might have only partial information.
[363.00s -> 365.00s]  So, what do I mean by this?
[365.00s -> 371.00s]  Well, let's say that you are observing a picture of a cheetah chasing a gazelle,
[371.00s -> 375.00s]  and you need to make some decision about what to do in this situation.
[375.00s -> 380.00s]  Now, the picture consists of pixels, so they're recordings from a camera.
[380.00s -> 384.00s]  We know that underneath those pixels there are actual physical events taking place,
[384.00s -> 388.00s]  that maybe the cheetah has a position and velocity and so does the gazelle,
[388.00s -> 392.00s]  but the input, technically, is just an array of pixels.
[392.00s -> 394.00s]  So that's the observation.
[394.00s -> 398.00s]  The state is what produced that observation,
[398.00s -> 402.00s]  and the state is a concise and complete physical description of the world.
[402.00s -> 404.00s]  So if you knew the positions and velocities
[404.00s -> 407.00s]  and maybe like the mental state of the cheetah and the gazelle,
[407.00s -> 409.00s]  you could figure out what they're going to do next.
[409.00s -> 414.00s]  Observation sometimes contains everything you need to infer the state, but not necessarily.
[414.00s -> 418.00s]  So, for example, maybe there's a car driving in front and you don't see the cheetah.
[418.00s -> 422.00s]  The cheetah is still there, the state hasn't changed just because it's not visible,
[422.00s -> 424.00s]  but the observation might have changed.
[424.00s -> 429.00s]  So, in general, it might not be possible to perfectly infer the current state S-T
[429.00s -> 431.00s]  from the current observation O-T,
[431.00s -> 435.00s]  whereas going the other way, going from S-T to O-T,
[435.00s -> 438.00s]  by definition of what a state is, is always possible,
[438.00s -> 442.00s]  because a state always encodes all the information you need to produce the observation.
[442.00s -> 444.00s]  So, if we would help to think about it this way,
[444.00s -> 446.00s]  if you imagine this was a simulation,
[446.00s -> 449.00s]  S-T might be the entire state of the computer's memory,
[449.00s -> 451.00s]  encoding the full state of the simulator,
[451.00s -> 454.00s]  whereas the observation is just an image that is rendered out
[454.00s -> 457.00s]  based on that state on the computer screen.
[457.00s -> 460.00s]  So going from observation back to state might not be possible
[460.00s -> 463.00s]  if some things are not visible.
[463.00s -> 467.00s]  Now, if we want to make this a little bit more precise, and we can,
[467.00s -> 470.00s]  we can describe this in the language of probabilistic graphical models.
[470.00s -> 473.00s]  So, in the language of probabilistic graphical models,
[473.00s -> 477.00s]  we can draw a graphical model that represents the relationship
[477.00s -> 481.00s]  between states, observations, and actions.
[481.00s -> 487.00s]  For those of you that took some course that covers Bayes nets,
[487.00s -> 489.00s]  this will look familiar.
[489.00s -> 492.00s]  For those of you that haven't, roughly speaking in these pictures,
[492.00s -> 497.00s]  the edges denote conditional independence relationships.
[497.00s -> 501.00s]  So if there's an edge, then the variable is not independent of its parents,
[501.00s -> 506.00s]  and in some cases, these things can encode dependencies.
[506.00s -> 509.00s]  I won't get into the details of how to understand probabilistic graphical models.
[509.00s -> 512.00s]  If you haven't covered this part, this won't entirely make sense to you,
[512.00s -> 515.00s]  but hopefully the verbal explanation of the relationship
[515.00s -> 518.00s]  between these variables will still make sense.
[518.00s -> 523.00s]  So, the policy pi theta is, at least for the partial observed case,
[523.00s -> 526.00s]  our relationship between O and A, so it gives the conditional distribution
[526.00s -> 530.00s]  over A given O, so that's this edge in the graph.
[530.00s -> 534.00s]  The state is what determines how you transition to the next state.
[534.00s -> 537.00s]  So the state and action together provide a probability distribution
[537.00s -> 541.00s]  over the next state, P of ST plus one, given STAT.
[541.00s -> 544.00s]  That is sometimes referred to as the transition probabilities or the dynamics.
[544.00s -> 547.00s]  You can think of this as basically the physics of the underlying world.
[547.00s -> 550.00s]  So when we write down equations of motion and physics,
[550.00s -> 553.00s]  we don't write down equations describing how image pixels move around.
[553.00s -> 556.00s]  We write down equations about how rigid bodies move and things like that.
[556.00s -> 559.00s]  So that's referring to S, the state, the position of the cheetah,
[559.00s -> 560.00s]  the velocity of the cheetah.
[560.00s -> 564.00s]  So the cheetah might transition to a different position based on its current velocity
[564.00s -> 567.00s]  and maybe based on how hungry the cheetah is and what it's trying to do.
[567.00s -> 569.00s]  And that's all captured in the state.
[569.00s -> 576.00s]  And then something to note about the state is that the state S3 here
[576.00s -> 581.00s]  is conditionally independent of the state S1 if you know the state S2.
[581.00s -> 583.00s]  So let me say that again because that might have been a little bit unclear.
[583.00s -> 587.00s]  If you know the state S2 and you need to figure out the state S3,
[587.00s -> 589.00s]  then S1 doesn't give you any additional information.
[589.00s -> 594.00s]  That means that S3 is conditionally independent of S1 given S2.
[594.00s -> 596.00s]  This is what is referred to as the Markov property
[596.00s -> 600.00s]  and it's one of the most fundamental defining features of a state.
[600.00s -> 602.00s]  Essentially, if you know the state now,
[602.00s -> 604.00s]  then the state in the past does not matter to you
[604.00s -> 606.00s]  because you know everything about the state of the world.
[606.00s -> 610.00s]  And that actually makes sense if you think back to that analogy about the computer simulator.
[610.00s -> 613.00s]  If you know the full state of the memory of the computer,
[613.00s -> 617.00s]  that's all you really need in order to predict future states
[617.00s -> 619.00s]  because the past memory of the computer doesn't matter.
[619.00s -> 624.00s]  The computer is only going to be making its simulation steps based on what's in memory now.
[624.00s -> 627.00s]  The computer itself has no access to its memory in the past, only its memory now,
[627.00s -> 633.00s]  so it makes sense that the future is independent of the past given the present.
[633.00s -> 636.00s]  So this is referred to as the Markov property and it's very, very important.
[636.00s -> 639.00s]  The Markov property essentially defines what it means to be a state.
[639.00s -> 641.00s]  A state is that which captures everything you need to know
[641.00s -> 645.00s]  to predict the future without knowing the past.
[645.00s -> 647.00s]  That doesn't mean that the future is perfectly predictable.
[647.00s -> 651.00s]  The future might still be random, there might be stochasticity,
[651.00s -> 657.00s]  but knowing the past doesn't help you resolve that randomness.
[657.00s -> 662.00s]  Okay, so just to finish this discussion,
[662.00s -> 666.00s]  now it's hopefully clear what the distinction between policies that operate on observations,
[666.00s -> 672.00s]  pi of AT given OT, and policies that operate on states, pi of AT given ST is.
[672.00s -> 676.00s]  So some algorithms, especially some of the later reinforced learning algorithms we'll describe,
[676.00s -> 679.00s]  can only learn policies that operate on states,
[679.00s -> 684.00s]  meaning that they require the input into the policy to satisfy the Markov property,
[684.00s -> 686.00s]  to fully encode the entire state of the system.
[686.00s -> 690.00s]  Some algorithms will not require this.
[690.00s -> 693.00s]  Some algorithms will be perfectly happy to operate on partial observations
[693.00s -> 696.00s]  that are perhaps insufficient to infer the state.
[696.00s -> 699.00s]  I'll try to make this distinction every time I present an algorithm,
[699.00s -> 704.00s]  but I will warn you right now that reinforcement learning practitioners and researchers
[704.00s -> 708.00s]  have a very bad habit of often confounding O and S.
[708.00s -> 711.00s]  So sometimes people will refer to O as S, they'll say,
[711.00s -> 714.00s]  oh, this is my state when in fact they mean this is my observation,
[714.00s -> 718.00s]  sometimes vice versa, and sometimes they'll make this distinction very unclear.
[718.00s -> 721.00s]  Sometimes they'll switch back and forth between observations and states.
[721.00s -> 725.00s]  So this confusion often happens if everything is going well.
[725.00s -> 729.00s]  This confusion is benign because it's typically,
[729.00s -> 732.00s]  this kind of confusion typically happens for algorithms where it doesn't matter
[732.00s -> 735.00s]  whether it's state or observation, so then it's kind of okay to mix them.
[735.00s -> 738.00s]  I'll try not to mix them, but sometimes I'll fall into old habits
[738.00s -> 741.00s]  and mix them anyway, in which case I'll do my best to tell you.
[741.00s -> 744.00s]  But be warned that O and S gets mixed up a lot.
[744.00s -> 746.00s]  If you want to be fully rigorous and fully correct,
[746.00s -> 750.00s]  this slide explains the difference.
[750.00s -> 754.00s]  So as an aside on notation, in this class we use the standard
[754.00s -> 759.00s]  reinforcement learning notation where S denotes states and A denotes actions.
[759.00s -> 762.00s]  This kind of terminology goes back to the study of dynamic programming,
[762.00s -> 766.00s]  which was pioneered in the 50s and 60s, principally in the United States
[766.00s -> 770.00s]  by folks like Richard Bellman, and I believe the S and A notation
[770.00s -> 774.00s]  actually was first used in his work, although I could be wrong about that.
[774.00s -> 777.00s]  Those of you that have more of a controls or robotics background
[777.00s -> 781.00s]  might be familiar with a different notation, which means exactly the same thing.
[781.00s -> 784.00s]  So if you've seen the symbol X used to denote state,
[784.00s -> 788.00s]  such as a configuration of a robot or a controlled system,
[788.00s -> 792.00s]  and you need the symbol U to denote the action,
[792.00s -> 795.00s]  don't be concerned, it means exactly the same thing.
[795.00s -> 800.00s]  This kind of notation is more commonly used in controls.
[800.00s -> 804.00s]  A lot of it goes back to the study of optimal control and optimization,
[804.00s -> 808.00s]  much of which was actually pioneered in the Soviet Union by folks like Leopold Teregin,
[808.00s -> 813.00s]  and much like the word action begins with the symbol A,
[813.00s -> 816.00s]  the word action also begins with the symbol U in Russian,
[816.00s -> 819.00s]  and that's the word, so that's why we have U, X,
[819.00s -> 823.00s]  well, because it's a commonly used variable in algebra.
[823.00s -> 825.00s]  Okay, so that's the set.
[825.00s -> 830.00s]  Now let's actually talk about imitation, the main topic of today's discussion.
[830.00s -> 835.00s]  So our goal will be to learn policies, which are distributions over A given O,
[835.00s -> 840.00s]  and to do this using supervised learning algorithms.
[840.00s -> 844.00s]  So since getting data of people running away from tigers
[844.00s -> 846.00s]  is not something that you can do very readily,
[846.00s -> 849.00s]  I'm going to use a different running example throughout today's lecture,
[849.00s -> 851.00s]  which is a kind of autonomous driving example.
[851.00s -> 856.00s]  So our observations will be images from a dashboard-mounted camera on a vehicle,
[856.00s -> 861.00s]  and our actions will be steering commands, turning left or turning right.
[861.00s -> 865.00s]  And you could imagine collecting data by having humans drive cars,
[865.00s -> 870.00s]  record their steering wheel commands, and record images from their camera,
[870.00s -> 872.00s]  and use this to create a data set.
[872.00s -> 876.00s]  So every single time step, your camera records an image,
[876.00s -> 878.00s]  and you record the steering wheel angle,
[878.00s -> 882.00s]  and you create a training tuple out of this, an input O and an output A.
[882.00s -> 884.00s]  And you collect this into a training set,
[884.00s -> 887.00s]  where As are labels and Os are inputs.
[887.00s -> 891.00s]  And now you can use this training set the same way that you use a labeled data set
[891.00s -> 893.00s]  in, let's say, image classification,
[893.00s -> 898.00s]  and just train a deep neural network to predict distributions over A given O
[898.00s -> 900.00s]  using supervised learning.
[900.00s -> 904.00s]  That is the essence of the most basic kind of imitation learning method.
[904.00s -> 907.00s]  We sometimes call this kind of algorithm behavioral cloning
[907.00s -> 912.00s]  because we're attempting to clone the behavior of the human demonstrator.
[912.00s -> 914.00s]  So that's a very basic algorithm.
[914.00s -> 916.00s]  Now, from what I've told you just now,
[916.00s -> 920.00s]  you can already implement a basic method for learning policies.
[920.00s -> 922.00s]  And what we'll discuss for the rest of today's lecture is,
[922.00s -> 924.00s]  does this method work?
[924.00s -> 926.00s]  Why does it work and when?
[926.00s -> 928.00s]  How can we make it work more often?
[928.00s -> 930.00s]  And can we develop better algorithms that are a little smarter
[930.00s -> 932.00s]  than just straight up using supervised learning
[932.00s -> 936.00s]  that will work more consistently?
[936.00s -> 938.00s]  So supervised learning produces this policy,
[938.00s -> 942.00s]  just like supervised learning might produce an image classifier.
[942.00s -> 946.00s]  This is called behavioral cloning.
[946.00s -> 952.00s]  Now, these kinds of methods have been around for a very, very long time.
[952.00s -> 960.00s]  One of the first, what we call large scale, larger scale learning-based control methods
[960.00s -> 965.00s]  was actually an imitation learning method called ALVEN developed in 1989,
[965.00s -> 968.00s]  which stands for Autonomous Land Vehicle in a Neural Network.
[968.00s -> 972.00s]  And that was what we would today call a deep RL method for learning-based control.
[972.00s -> 976.00s]  It used data from human drivers to train a neural network
[976.00s -> 980.00s]  with a whole heaping load of hidden units, five whole hidden units,
[980.00s -> 988.00s]  to look at a 30x32 observation of a road and output commands to drive a vehicle.
[988.00s -> 992.00s]  And it could drive on roads, it could follow lanes, it could do some basic stuff,
[992.00s -> 996.00s]  you know, probably wouldn't be able to handle traffic laws very well,
[996.00s -> 1002.00s]  but it was a very rough sketch on autonomous driving system quite a long time ago.
[1002.00s -> 1009.00s]  But if we want to ask more precisely whether using these behavioral cloning methods
[1009.00s -> 1013.00s]  in general is guaranteed to work, the answer unfortunately is no.
[1013.00s -> 1018.00s]  We'll describe, we'll discuss the formal reasons for this in a lot more detail,
[1018.00s -> 1022.00s]  but to give you a little bit of intuition to get us started, let's think about it like this.
[1022.00s -> 1025.00s]  I'll draw a lot of plots of this sort in today's lecture.
[1025.00s -> 1030.00s]  In these plots, one of the axes represents the state.
[1030.00s -> 1032.00s]  So imagine the state is one dimensional.
[1032.00s -> 1034.00s]  Of course, in reality, the state is not really one dimensional,
[1034.00s -> 1036.00s]  but for visualization, that's what it's going to be.
[1036.00s -> 1039.00s]  And the other axis is time.
[1039.00s -> 1042.00s]  Now, in this kind of state-time diagram,
[1042.00s -> 1046.00s]  you can think of this black curvy line as one of the training trajectories.
[1046.00s -> 1049.00s]  In reality, of course, you would have many training trajectories,
[1049.00s -> 1051.00s]  but for now, let's say you have just one.
[1051.00s -> 1054.00s]  And now let's imagine that you train on this training trajectory,
[1054.00s -> 1058.00s]  you get your policy, and then you're going to run your policy from the same initial state.
[1058.00s -> 1063.00s]  Okay, so the red curve will represent the execution of that policy.
[1063.00s -> 1068.00s]  And let's say you did a really good job, so you took all of your lessons from CS189,
[1068.00s -> 1072.00s]  and you took care to make sure that you're not overfitting and you're not underfitting.
[1072.00s -> 1075.00s]  But of course, your policy will still make at least a small mistake, right?
[1075.00s -> 1077.00s]  Every learned model is not perfect.
[1077.00s -> 1083.00s]  It'll make tiny mistakes, even in states that are very similar to ones that were seen in training.
[1083.00s -> 1086.00s]  And the problem is that when it makes those tiny mistakes,
[1086.00s -> 1089.00s]  it'll go into states that are different from the ones that it saw in training.
[1089.00s -> 1093.00s]  So if the training data involves driving quite straight on the road in the middle of the lane,
[1093.00s -> 1096.00s]  and this policy makes a little deviation, goes a little bit off-center,
[1096.00s -> 1100.00s]  now it's seeing something unfamiliar that's a little different than what it saw before.
[1100.00s -> 1104.00s]  And when it sees something that's a little different, it's more likely to make a slightly bigger mistake.
[1104.00s -> 1109.00s]  And the amount by which these mistakes increase might be very small at first,
[1109.00s -> 1114.00s]  but each additional mistake puts you in a state that's more and more unfamiliar,
[1114.00s -> 1117.00s]  which means that the magnitude of the mistake will increase.
[1117.00s -> 1120.00s]  And that means that by the end, if you have a very long trajectory,
[1120.00s -> 1125.00s]  you might be in extremely unfamiliar states, and therefore you might be making extremely large mistakes.
[1125.00s -> 1129.00s]  This doesn't happen in supervised learning, in regular supervised learning,
[1129.00s -> 1133.00s]  and the reason it doesn't happen is actually something that we discussed in Lecture 1.
[1133.00s -> 1136.00s]  There's a particular assumption that we make in supervised learning,
[1136.00s -> 1140.00s]  some of you might remember it, if you think you might remember it,
[1140.00s -> 1143.00s]  maybe you can pause this video and think about this a little bit.
[1143.00s -> 1145.00s]  And when you unpause, I'll tell you the answer.
[1145.00s -> 1148.00s]  The answer of course is the IID property.
[1148.00s -> 1154.00s]  In regular supervised learning, we assume that each training point doesn't affect the other training points,
[1154.00s -> 1161.00s]  which means that the label you output for example number 1 has no bearing on the correct solution for example number 2.
[1161.00s -> 1164.00s]  But of course that's not the case here, because here when you select an action,
[1164.00s -> 1168.00s]  it actually changes the observation that you will observe at the next time step.
[1168.00s -> 1174.00s]  So it's violating a fairly fundamental assumption that is always assumed in regular supervised learning.
[1175.00s -> 1180.00s]  However, in reality, naive behavioral cloning methods can actually work pretty well.
[1180.00s -> 1184.00s]  These are some results from a fairly old paper at this point from NVIDIA,
[1184.00s -> 1187.00s]  where they attempted a behavioral cloning approach for autonomous driving,
[1187.00s -> 1190.00s]  a kind of modernized version of Albin, and initially they had a lot of trouble.
[1190.00s -> 1195.00s]  Their car was giving them a lot of bad turns, running into traffic cones, etc.
[1195.00s -> 1199.00s]  But after they collected a lot of training data, 3000 miles of training data,
[1199.00s -> 1203.00s]  they could actually get a vehicle that would follow lanes reasonably competently.
[1203.00s -> 1207.00s]  It could drive around the cones, it could follow roads, and they always have a safety driver in there,
[1207.00s -> 1210.00s]  and it's not by any means a complete autonomous driving system.
[1210.00s -> 1218.00s]  But it certainly seems like the pessimistic picture on the previous slide might not actually hold in practice, at least not always.
[1218.00s -> 1222.00s]  So, what is it that this paper actually did? What is it that made it work?
[1222.00s -> 1226.00s]  Well, there are a lot of complex decisions in any machine learning system,
[1226.00s -> 1230.00s]  but one decision that I wanted to tell you about a little bit that maybe kind of sets the tone
[1230.00s -> 1233.00s]  for some of the ideas that I'll discuss in the rest of the lecture,
[1233.00s -> 1237.00s]  is a diagram that's buried deeper down in that paper that shows that, well,
[1237.00s -> 1242.00s]  okay, so they've got their recorded steering angle, they've got some convolutional neural net, that's pretty typical,
[1242.00s -> 1247.00s]  and they have some cameras, but they have this center camera, left camera, and right camera,
[1247.00s -> 1250.00s]  and this random shift in rotation. What's up with that?
[1250.00s -> 1256.00s]  Well, there's a little detail in how the policy is trained in that work, and the detail is this.
[1256.00s -> 1260.00s]  So their car actually has three cameras. It has a regular forward-facing camera,
[1260.00s -> 1265.00s]  which is the one that's actually going to be driving the car, and then they also have a left-facing camera.
[1265.00s -> 1268.00s]  And they take the images from the left-facing camera, and they label them,
[1268.00s -> 1273.00s]  not with the steering command that the human actually executed during data collection,
[1273.00s -> 1277.00s]  but with a modified steering command that steers a little bit to the right.
[1277.00s -> 1281.00s]  So imagine what that camera sees. What the camera sees when the car is driving straight on the road
[1281.00s -> 1286.00s]  is an image similar to what the car would have seen if it swerved to the left.
[1286.00s -> 1291.00s]  And they synthetically label that with an action that corrects and swerves back to the right.
[1291.00s -> 1293.00s]  And they do the same thing for the right-facing camera.
[1293.00s -> 1299.00s]  They label it with an action that's a little bit to the left of the one that the human driver actually used.
[1299.00s -> 1303.00s]  And you can kind of imagine how this might correct some of the issues that I discussed before,
[1303.00s -> 1309.00s]  because if the policy makes a little mistake, and it drives a little further to the left than it should have,
[1309.00s -> 1313.00s]  now it's going to see something similar to what that left-facing camera would have seen.
[1313.00s -> 1318.00s]  And now that state is not so unfamiliar, because it has been seen before in those left-facing cameras.
[1318.00s -> 1323.00s]  Now it's being seen in the front-facing camera, but the policy doesn't know which camera it's looking through.
[1323.00s -> 1327.00s]  It just knows that it's similar to that image before that was labeled with a turn to the right.
[1327.00s -> 1330.00s]  So it will correct.
[1330.00s -> 1335.00s]  Okay, so why did I want to tell you this? What's the moral of the story?
[1335.00s -> 1342.00s]  And what does that tell us about how we can actually make naive behavioral cloning methods work pretty well in practice?
[1342.00s -> 1349.00s]  Well, the moral of the story is that imitation learning via behavioral cloning is not in general guaranteed to work.
[1349.00s -> 1355.00s]  And we'll make this actually precise, and we'll describe precisely how bad the problem really is.
[1355.00s -> 1357.00s]  And this is different from supervised learning.
[1357.00s -> 1362.00s]  So for supervised learning, you can derive various sample complexity and correctness bounds.
[1362.00s -> 1368.00s]  Of course, when deep neural nets are in the picture, those bounds often make strong assumptions that might not hold in practice,
[1368.00s -> 1370.00s]  but at least that's a fairly well understood area.
[1370.00s -> 1373.00s]  And this generally doesn't hold in behavioral cloning.
[1373.00s -> 1376.00s]  And the reason fundamentally is the IID assumption.
[1376.00s -> 1382.00s]  The fact that individual outputs will influence future inputs in the sequential setting,
[1382.00s -> 1386.00s]  but they will not in the classic supervised learning setting.
[1386.00s -> 1390.00s]  We can formalize why with a bit of theory, and we'll talk about that today.
[1391.00s -> 1393.00s]  And we can address the problem in a few ways.
[1393.00s -> 1397.00s]  First, we can be smart about how we collect and augment our data.
[1397.00s -> 1399.00s]  And that is what that paper from NVIDIA did,
[1399.00s -> 1402.00s]  arguably with a technique similar to data augmentation,
[1402.00s -> 1408.00s]  where instead of simply directly using the true observations that the human driver observed together with their actions,
[1408.00s -> 1412.00s]  they add some additional kind of fake observations from these left and right facing cameras
[1412.00s -> 1415.00s]  with synthetically altered actions to fix up the problem.
[1416.00s -> 1421.00s]  We can also use very powerful models that make very few mistakes.
[1421.00s -> 1425.00s]  Remember that the issue originally was due to the fact that we made those small mistakes in the beginning,
[1425.00s -> 1427.00s]  which then build up over time.
[1427.00s -> 1431.00s]  If we can minimize the mistakes, if we use very powerful models,
[1431.00s -> 1436.00s]  perhaps in combination with the first bullet point, then we can mitigate the issue as well.
[1437.00s -> 1440.00s]  There are some other solutions that are maybe a little bit more exotic,
[1440.00s -> 1442.00s]  but can be very useful in some cases.
[1442.00s -> 1446.00s]  For instance, sometimes switching to more of a multi-task learning formulation,
[1446.00s -> 1448.00s]  learning multiple tasks at the same time,
[1448.00s -> 1453.00s]  can, perhaps surprisingly, actually make it easier to perform imitation learning.
[1454.00s -> 1456.00s]  And then we can also change the algorithm.
[1456.00s -> 1461.00s]  We can use a more sophisticated algorithm that directly solves this problem,
[1461.00s -> 1465.00s]  this compounding errors problem, and we'll discuss one such algorithm called Dagger.
[1465.00s -> 1468.00s]  Now that typically involves changing the learning process.
[1468.00s -> 1471.00s]  In the case of Dagger, it actually changes how the data is collected,
[1471.00s -> 1474.00s]  but it can provide a more principled solution to these issues,
[1474.00s -> 1477.00s]  and you will actually implement this algorithm in your homework.
[1478.00s -> 1482.00s]  So that's what I'll discuss next for the rest of the lecture,
[1482.00s -> 1487.00s]  and the first part will be a discussion of the theory in part 2.
