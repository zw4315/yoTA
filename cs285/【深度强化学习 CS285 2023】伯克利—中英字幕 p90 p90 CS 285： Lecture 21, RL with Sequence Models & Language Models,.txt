# Detected language: en (p=1.00)

[0.00s -> 5.36s]  Alright, in the third part of today's lecture, we're going to talk about multi-step reinforcement
[5.36s -> 9.70s]  learning with language models, where we'll combine some of the ideas from the POMDP discussion
[9.70s -> 13.12s]  as well as the language model discussion from before.
[13.12s -> 19.32s]  So here's an example of a multi-turn RL problem with language models.
[19.32s -> 23.24s]  This is an example of a task called visual dialogue, which is a benchmark introduced
[23.24s -> 25.44s]  in a paper from 2017.
[25.44s -> 28.60s]  The idea here is that there is a questioner who is the bot, and the answer, which
[28.64s -> 33.16s]  is considered part of the environment, and the answer has a particular picture in mind,
[33.16s -> 37.20s]  and the questioner has to ask the questions to try to guess which picture it is.
[37.20s -> 41.52s]  So this is purely a language task for the questioner, and the questioner needs to select
[41.52s -> 46.82s]  appropriate questions to gather information so that at the end they can figure out what
[46.82s -> 50.20s]  image the answer was thinking.
[50.20s -> 54.66s]  Now you could imagine structuring this as a POMDP, where the observations are the
[54.70s -> 59.46s]  things that are said by the answer, and the actions are the questions that the questioner
[59.46s -> 60.46s]  selects.
[60.46s -> 64.06s]  And this is now a sequential process, there are multiple time steps, and at the end
[64.06s -> 65.06s]  there's a reward.
[65.06s -> 69.54s]  So the action is what the bot says, it's a sentence like any people in the shant.
[69.54s -> 75.66s]  The observation is what the answer or simulated human says, like they aren't.
[75.66s -> 78.90s]  The state now would be a history state, just like in our discussion in the first
[78.90s -> 83.02s]  part, so that would be the sequence of past observations and actions.
[83.02s -> 88.82s]  And the reward is the outcome of the dialogue, did the questioner guess the correct answer
[88.82s -> 90.24s]  at the end.
[90.24s -> 93.78s]  So the multi-step nature of this task is very important, now we're back in the full
[93.78s -> 97.98s]  RL setting, because the questioner isn't just going to ask questions that greedily
[97.98s -> 101.58s]  get them the answer, they're going to ask questions to gather information so that
[101.58s -> 103.90s]  they can guess the right answer at the very end.
[103.90s -> 108.50s]  Obviously they shouldn't ask the same question multiple times, they should think about what
[108.50s -> 112.14s]  information they've already gathered, what information remains open, and proceed accordingly.
[113.78s -> 117.90s]  Now these kinds of multi-term problems show up in a number of places, they of course show
[117.90s -> 122.54s]  up in dialogue systems, where you might be interacting with a human to achieve some
[122.54s -> 128.66s]  final delay goal, assistant chatbots, where you might have multiple turns of interaction
[128.66s -> 133.06s]  to arrive at a solution, tool use settings, where instead of talking to a person you
[133.06s -> 137.90s]  might be outputting text that goes into some tool, like a database, a Linux terminal,
[137.90s -> 144.66s]  a calculator, something that uses that tool to then produce an answer to a given query.
[144.66s -> 149.94s]  Playing text games, maybe you produce actions that go into a text adventure game, which
[149.94s -> 153.54s]  then responds with programmed observations.
[153.54s -> 157.30s]  So these are all examples of multi-term RL problems.
[157.30s -> 163.14s]  Now this is not the same as RLHS from before, RL from human feedback, RL from human preferences
[163.14s -> 167.48s]  that we saw in the previous section, learns from human preferences, here we're learning
[167.48s -> 170.52s]  about the outcome of the entire multi-step interaction.
[170.52s -> 174.52s]  The reward only appears at the end after multiple turns.
[174.52s -> 179.28s]  The episode in the previous section was a single answer, so it was a one-turn banded
[179.28s -> 181.12s]  problem with a state and an action.
[181.12s -> 185.74s]  Here we have multiple turns, multiple observation and actions.
[185.74s -> 191.12s]  The partial observability now matters because we need to pay attention not just to the
[191.12s -> 196.44s]  latest response from the human, but perhaps all the previous responses and all the questions
[196.44s -> 198.66s]  we asked before.
[198.66s -> 202.76s]  So this is now putting us into a different regime.
[202.76s -> 205.84s]  How can we train policies to handle this?
[205.84s -> 208.56s]  Well we could use policy gradients just like before.
[208.56s -> 211.48s]  Policy gradients are a viable way to train multi-turn policies, that's what we introduced
[211.48s -> 215.04s]  them for, and we also learned in the first section that policy gradients can actually
[215.04s -> 219.28s]  handle partial observability, we can give the policy a history of observations, so
[219.28s -> 224.16s]  we can also use those history states, that is quite feasible.
[224.20s -> 227.84s]  One issue that we run into, however, with policy gradients is if we are training a dialogue
[227.84s -> 233.16s]  agent that talks to a human, then we need to get samples from the human for every rollout.
[233.16s -> 237.36s]  This is different from the human preferences setting that we were in before.
[237.36s -> 241.52s]  Because we had that reward model, we could optimize against the reward model with multiple
[241.52s -> 246.76s]  iterations consisting of sampling and optimization, and only occasionally get more preferences.
[246.76s -> 250.40s]  But if we're using policy gradients for a dialogue task where every single episode
[250.44s -> 254.52s]  requires talking to a human, now we need to interact with the human a lot more.
[254.52s -> 258.72s]  So even though with the preference we still need a human input, we need a lot more of
[258.72s -> 262.60s]  it if we want to optimize a dialogue agent with policy gradients.
[262.60s -> 264.24s]  So it could work, but it's expensive.
[264.24s -> 266.96s]  Of course it's a lot easier if you're not interacting with a human, but instead are
[266.96s -> 270.72s]  interacting with tools such as a database.
[270.72s -> 273.60s]  Value-based methods, however, are a very appealing option because with value-based
[273.60s -> 277.84s]  methods you could use offline RL techniques like the ones that we learned about before
[277.84s -> 282.80s]  in the course, and actually train your dialogue agent directly with data of, for
[282.80s -> 287.36s]  example, humans talking to other humans, or past deployments of a bot.
[287.36s -> 290.84s]  So value-based methods are actually a very appealing option for dialogue.
[290.84s -> 294.80s]  So in this part of the lecture, I'll actually focus on discussing value-based
[294.80s -> 299.12s]  methods, though I will say that policy gradient methods could be used directly.
[299.12s -> 301.48s]  There's not much more to say about that, however, because they would work
[301.48s -> 304.92s]  exactly the same way as they did before.
[304.92s -> 307.32s]  So let's talk about value-based methods.
[307.32s -> 311.60s]  And for value-based methods, we have to make a choice, which is what constitutes a time step.
[311.60s -> 315.20s]  So in the very beginning of the previous section, I discussed how there are design
[315.20s -> 318.68s]  choices to be made about how to turn the language problem into an MDP.
[318.68s -> 324.20s]  And here, there is a particularly delicate choice that we can make, which we go either way.
[324.20s -> 330.36s]  So the first choice is to have every utterance be a time step, meaning that the first
[330.36s -> 334.44s]  thing that the human says, like two zebras are walking around their pen in the zoo,
[334.44s -> 335.84s]  that's observation one.
[335.84s -> 340.24s]  The first sentence that the questioner says, like any people in the shot, that's action one.
[340.24s -> 344.00s]  So actions and observations are entire sentences.
[344.00s -> 349.44s]  This is perhaps most directly analogous to the setting that we had in the previous section.
[349.44s -> 355.84s]  This is a natural choice because the actions are, because we go in alternating fashion,
[355.84s -> 357.56s]  action, observation, action, observation.
[357.56s -> 359.84s]  The observation is always outside of the agent's control.
[359.84s -> 361.92s]  The action is always entirely under its control.
[361.92s -> 364.12s]  The horizons are typically going to be relatively short.
[364.12s -> 368.92s]  So if the dialogue involves 10 back and forth questions and answers, then we're going to
[368.92s -> 370.88s]  have 10 time steps.
[370.88s -> 372.88s]  The problem is that the action space is huge.
[372.88s -> 377.44s]  The action space is the entire space of utterances that the bot could say.
[377.44s -> 383.12s]  An alternative choice is to consider each token to be a time step.
[383.12s -> 388.60s]  So in this case, or an entire utterance from the bot, for example, any people in the
[388.60s -> 393.32s]  shot, every single token in this utterance is a separate action time step.
[393.32s -> 398.16s]  And this is a little bit peculiar because, of course, each of those actions is under its control.
[398.16s -> 401.04s]  So after action one, it immediately gets to choose action two.
[401.04s -> 403.64s]  There's no additional observation.
[403.64s -> 408.52s]  We would still concatenate action one to our state history, and the next action would be
[408.52s -> 410.76s]  selected given the entire history.
[410.76s -> 416.24s]  And then every single token in the response is a separate observation.
[416.24s -> 419.88s]  Now, this has a very big advantage, which is now at every time step, we have a simple
[419.88s -> 420.92s]  discrete action space.
[421.00s -> 424.48s]  So the action space at any time step is just a set of possible tokens.
[424.48s -> 427.76s]  It's a large set, but it's quite easy to enumerate.
[427.76s -> 432.92s]  Whereas the set of actions in the per utterance setting is the set of all possible
[432.92s -> 436.48s]  sequences, which is exponentially large, exponential in the horizon.
[436.48s -> 440.40s]  The problem when we use per token time steps is that our horizon now is much, much longer.
[440.40s -> 444.00s]  So whereas before our horizon might be on the order of 10 steps, now it's going to be
[444.00s -> 447.60s]  possibly thousands of steps, even for a relatively short dialogue.
[447.60s -> 449.60s]  Both options have been explored in the literature.
[449.68s -> 453.08s]  There's no single established standard as to which one is better.
[453.08s -> 457.88s]  So I'll discuss both of them and maybe tell you a little bit about their pros and cons.
[457.88s -> 463.64s]  So let's start with value-based RL with per utterance time steps.
[463.64s -> 467.24s]  Here is an example, a slice of our dialogue.
[467.24s -> 468.96s]  And let's say that we're at this step.
[468.96s -> 473.24s]  Let's say that we're at the stage where the bot is saying, are they facing each other?
[473.24s -> 477.84s]  What we're going to do is we're going to take the history of the conversation up
[477.88s -> 480.64s]  until this point, which constitutes the state.
[480.64s -> 482.64s]  That's the entire dialogue history ST.
[482.64s -> 487.08s]  And we're going to pass it through some kind of sequence model.
[487.08s -> 489.40s]  So it's some kind of, it could be a pre-trained language model.
[489.40s -> 491.68s]  It could be something like BERT.
[491.68s -> 493.60s]  There are a variety of choices.
[493.60s -> 497.12s]  And the sequence model is going to output some sort of embedding.
[497.12s -> 501.72s]  And then we're also going to take our candidate action, are they facing each other?
[501.72s -> 504.16s]  And we're going to also pass it through a sequence model.
[504.16s -> 508.16s]  And this could be a separate sequence model, or it could be the same one.
[508.16s -> 511.20s]  And we're going to get embeddings of both of these things that are going to be fed into
[511.20s -> 514.72s]  some learned function that outputs the Q value.
[514.72s -> 518.44s]  It's perhaps most straightforward to actually have two separate encoders for the state
[518.44s -> 521.24s]  and the action, but they could also be encoded with the same encoder.
[521.24s -> 524.12s]  And at the end, we have to predict a single number for them, which is the Q value.
[524.12s -> 525.80s]  So this is the critic.
[525.80s -> 530.36s]  Now, typically in this design, we could use either an actor-critic architecture,
[530.36s -> 533.56s]  or we would have a separate actor network that is trained to maximize this critic.
[533.56s -> 538.36s]  And that could be trained with, for example, one of the algorithms in the previous section,
[538.36s -> 542.48s]  treating this Q in place of the reward as the one-step objective.
[542.48s -> 550.28s]  Or we could directly decode from the Q function to find the action that has the highest Q value.
[550.28s -> 551.96s]  And it's a little tricky how to do that.
[551.96s -> 554.36s]  We could do that with something like beam search.
[554.36s -> 560.20s]  We could also sample from a supervised train model and take the sample of highest Q value.
[560.20s -> 568.60s]  And then we would train this Q function using our estimate of the maximum for the next time step.
[568.60s -> 571.48s]  So the maximum for the next time step could come from doing beam search,
[571.48s -> 576.68s]  it could come from using an actor, it could also come from sampling from a supervised train model,
[576.68s -> 580.60s]  and then taking the sample of the largest Q value as an approximation to the max.
[580.60s -> 585.40s]  So all of those are valid options, and different methods in literature have explored different choices for that.
[585.40s -> 588.40s]  So I'll summarize a few previous papers at the end of this section
[588.40s -> 590.76s]  and tell you what the concrete papers actually did.
[590.76s -> 596.00s]  So there's no one way of doing this, there's a variety of choices.
[596.00s -> 599.48s]  For per-token time steps, things are perhaps a little bit simpler.
[599.48s -> 602.60s]  So let's say that we're at this point in the decoding process,
[602.60s -> 604.60s]  we're generating the token corresponding to facing.
[604.60s -> 606.88s]  And remember, of course, in reality, words aren't tokens,
[606.88s -> 612.16s]  tokens actually correspond to multiple characters, but not entire words.
[612.16s -> 617.00s]  But let's pretend that tokens are words, and let's pretend that we're at the word facing.
[617.00s -> 622.12s]  So we're going to want to do this backup, the Bellman backup, over individual tokens.
[622.12s -> 627.56s]  Now things work much more like supervised language models.
[627.56s -> 632.32s]  So we have these tokens, and we have a number for every possible token at this time step,
[632.32s -> 636.72s]  except instead of that number corresponding to the probability of that token being the next token,
[636.72s -> 639.12s]  the number is actually its Q value.
[639.12s -> 645.04s]  So the number associated with the token for facing is the Q value you would get
[645.04s -> 648.44s]  if your history is the entire previous history of the conversation,
[648.44s -> 651.36s]  and then you select the token facing as the next action.
[651.36s -> 657.16s]  So your loss would take in the token facing at the next step,
[657.16s -> 661.84s]  maximize over the possible tokens at the next time step if the agent chooses that token,
[661.84s -> 667.36s]  or simply take the dataset token if it's chosen by the environment,
[667.36s -> 673.44s]  add the reward to that, and then use that as the target value in the loss.
[673.44s -> 676.52s]  So this essentially implements per token Q-learning.
[676.52s -> 681.16s]  So to explain that again, at the token for they,
[681.16s -> 686.92s]  the output is the Q value of every possible token being chosen at the next time step,
[686.92s -> 690.08s]  and to compute the target for that Q value,
[690.08s -> 693.04s]  we would actually input that token at the next time step,
[693.04s -> 698.36s]  see all of our possible next token values,
[698.36s -> 701.68s]  and take a max over them if the agent gets to choose the next one,
[701.68s -> 705.20s]  or take the value of the dataset token if it's chosen by the environment,
[705.20s -> 708.76s]  add the reward, and then treat that as our target.
[708.76s -> 713.04s]  So in some ways it's simpler, but remember that our horizon gets to be a lot longer.
[713.04s -> 714.56s]  So we have simple discrete actions.
[714.56s -> 717.76s]  The rule is arguably less complex than it is for per utterance
[717.76s -> 720.72s]  because we don't have to deal with actors, we don't have to deal with all that other stuff,
[720.72s -> 724.56s]  but our horizon is very long.
[724.56s -> 727.92s]  So putting it all together, the usual value-based details apply.
[727.92s -> 732.96s]  So we would typically need a target network for either the per utterance or the per token version.
[732.96s -> 734.56s]  We would typically use a replay buffer.
[734.56s -> 737.28s]  We would typically do things like use the double queue trick and so on.
[737.28s -> 742.16s]  So all the same considerations apply as they did for regular value-based methods.
[742.16s -> 744.64s]  And we could use this with either online or offline RL.
[744.64s -> 748.48s]  To my knowledge, these methods have primarily been studied for offline RL,
[748.48s -> 754.56s]  in which case you would use something like CQL or IQL to make it work properly.
[754.56s -> 758.88s]  And the details basically require handling a distributional shift in some way.
[758.88s -> 760.64s]  So you could use policy constraints.
[760.64s -> 763.76s]  If you have an actor, then you would use a KL divergence on the actor.
[763.76s -> 768.16s]  If you are just using value-based methods, you could use a CQL-style penalty on the Q values,
[768.16s -> 773.84s]  which conveniently for the per token version amounts to basically putting the standard supervised cross-entropy loss.
[773.84s -> 776.96s]  If it's not clear why that's the case, you can work that out.
[776.96s -> 780.08s]  Just write down the CQL objective, and with discrete actions,
[780.08s -> 783.76s]  you'll see that it actually works out to be the same as a cross-entropy loss.
[783.76s -> 788.80s]  You could also do an IQL-style backup, and that's also a decent option.
[788.80s -> 791.92s]  But there's no single best answer yet as to which of these is the better choice.
[791.92s -> 797.84s]  So this is very much kind of at the bleeding edge of current research as of 2023.
[797.84s -> 801.28s]  Okay, so this was a little bit abstract.
[801.28s -> 805.44s]  To see concrete algorithms that you could actually implement to make this work,
[805.44s -> 807.52s]  let's go through some examples.
[807.52s -> 811.52s]  So one example, which is a somewhat older paper by Natasha Jakes,
[811.52s -> 815.36s]  called Human-Centric Dialogue Training via Offline Reinforcement Learning,
[815.36s -> 818.40s]  uses an actor-critic plus policy constraint architecture.
[818.40s -> 825.84s]  So there is an actor network, which has a KL divergence penalty to stay close to the data distribution.
[825.84s -> 828.48s]  The rewards for this come from human-user sentiment analysis.
[828.48s -> 832.80s]  So the chatbot is actually trying to optimize the sentiment elicited from humans,
[832.80s -> 838.56s]  and the reward is automatically computed using a sentiment analyzer applied to the human responses.
[838.56s -> 842.32s]  And this uses the per-utterance timestep formulation.
[842.32s -> 845.76s]  Another example, CHAI, a chatbot AI for a task-oriented dialogue
[845.76s -> 848.64s]  with offline reinforcement learning by Siddharth Verma.
[848.64s -> 852.56s]  This one uses a Q function with a CQL-like penalty,
[852.56s -> 857.76s]  and it uses rewards from the task, in this case the Cregis negotiation task,
[857.76s -> 862.48s]  so the reward just comes from the total revenue made by selling an item.
[862.48s -> 864.40s]  And a timestep here is one utterance.
[864.40s -> 867.20s]  So the way that the maximization is done over the next timestep
[867.20s -> 872.64s]  is actually by sampling multiple possible responses from a supervised trained language model,
[872.64s -> 874.64s]  in this case a GBG2-style model,
[874.64s -> 879.12s]  and then taking the max over the Q values of the sampled utterances.
[879.76s -> 882.24s]  So this is not an exact max, it's an approximate max
[882.24s -> 885.52s]  by using samples from a pre-trained language model.
[887.44s -> 890.88s]  Another more recent example is Offline RL for Natural Language Generation
[890.88s -> 893.68s]  with Implicit Language Q-Learning by Snell et al. 2022.
[894.24s -> 896.80s]  This one uses a Q function trained with actually a combination
[896.80s -> 901.84s]  of both IQL and CQL, so it uses an IQL backup with a CQL penalty.
[902.96s -> 908.56s]  And then the policy is actually extracted by, again, taking a supervised trained model,
[908.56s -> 910.32s]  sampling from that supervised trained model,
[910.32s -> 912.32s]  and then taking the sample with the largest Q value.
[913.04s -> 914.88s]  And the rewards, again, come from the task.
[914.88s -> 917.60s]  This one is evaluated on the visual dialogue task from before,
[917.60s -> 920.96s]  where the reward corresponds to whether the agent gets the correct answer or not.
[922.88s -> 926.00s]  So if you want to learn more about specific value-based algorithms,
[926.00s -> 929.84s]  I would encourage you to check out these papers and see the particular details they chose.
[929.84s -> 932.80s]  So my description of the methods was a little bit abstract and generic,
[932.80s -> 935.04s]  the particular instantiations covered in these papers.
[936.72s -> 942.72s]  And the Snell et al. formulation uses each time step, each token as a time step.
[944.40s -> 950.40s]  Okay, so to recap, multi-step language interactions like dialogue are POMDP,
[951.20s -> 953.76s]  which means that we need to do something like using dialogue,
[953.76s -> 956.64s]  using history states as our state representation.
[958.40s -> 962.48s]  Time steps can be defined as either per utterance or per token, and they have their pros and cons.
[963.04s -> 966.88s]  In principle, any RL method could be used once we switch to using history states,
[966.88s -> 971.28s]  but in practice, especially if we have dialogue agents that need to talk to humans,
[971.28s -> 973.60s]  we might really prefer an offline RL formulation,
[973.60s -> 977.60s]  because otherwise we would have to interact with humans every time we generate more samples.
[979.12s -> 980.56s]  Of course, that's not necessarily the case,
[980.56s -> 983.44s]  because if we're doing something like text games or tool use,
[983.44s -> 985.28s]  then online methods are actually quite feasible.
[987.60s -> 991.36s]  Value-based methods either treat utterances or tokens as actions,
[991.36s -> 993.44s]  and they build Q functions with history states.
[994.56s -> 999.28s]  And we have to apply the same details and tricks as regular offline value-based methods,
[999.28s -> 1003.20s]  so that includes things like target networks, it includes tricks like double Q learning,
[1003.20s -> 1008.88s]  it includes the various offline regularization methods like policy constraints, CQL, or IQL.
[1008.88s -> 1012.88s]  There's no single established standard for what is the best method of this sort,
[1012.88s -> 1018.08s]  and there are a variety of different choices with different pros and cons.
