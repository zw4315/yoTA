# Detected language: en (p=1.00)

[0.00s -> 5.00s]  All right, so now let's get into the main technical part of today's lecture, which
[5.00s -> 11.00s]  is to discuss the variational inference framework. So this framework is basically
[11.00s -> 16.40s]  concerned with this question. How do we calculate p of z given xi? But in the
[16.40s -> 19.48s]  process of deriving this, we'll also see why the expected logarithm
[19.48s -> 25.08s]  likelihood is actually a reasonable objective. So let's think about making
[25.08s -> 29.48s]  some crude approximation. So p of z given xi is in general a pretty
[29.48s -> 33.00s]  complex distribution, right, because a single point x might come from many
[33.00s -> 37.52s]  different places in the space of z's. But let's make a really simplistic
[37.52s -> 42.32s]  approximation. Let's say that we're going to approximate p of z given xi
[42.32s -> 48.12s]  with some distribution Q i of z, which is a Gaussian, or in general some very
[48.12s -> 52.80s]  simple tractable parametrized distribution class. And notice that I'm
[52.80s -> 58.08s]  calling it Q subscript i of z. So it's a distribution over z, and it is a
[58.60s -> 65.64s]  distribution that's going to be specific to this point xi. All right, so instead of
[65.64s -> 68.52s]  having this complicated thing, we're going to try to approximate it with
[68.52s -> 72.56s]  just a single peak. And I chose this picture intentionally just to make it
[72.56s -> 77.12s]  clear that this approximation is not necessarily going to be a good one, but
[77.12s -> 81.18s]  we'll try to find the best possible fit within this simple distribution
[81.18s -> 87.72s]  class, the Gaussian distribution class. It turns out that if you approximate p
[87.76s -> 96.60s]  of z given xi with any Q i z, you can actually construct a lower bound on the
[96.60s -> 100.92s]  log probability of xi. And this is going to be a very powerful idea
[100.92s -> 105.88s]  because if you can construct a lower bound on the log probability of xi, then
[105.88s -> 111.56s]  maximizing that bound will push up on the log probability of xi. Now in
[111.56s -> 114.84s]  general maximizing lower bounds does not increase the quantity you care
[114.88s -> 118.84s]  about, but if the bound is sufficiently tight, then it does. And we'll see later
[118.84s -> 123.32s]  that under some conditions the bound is in fact tight. But for now let's just,
[123.32s -> 126.80s]  let's not worry about tightness, let's just see how we can get a bound
[126.80s -> 133.92s]  by using Q i of z. So we can write out the log probability of xi as the log
[133.92s -> 141.08s]  of the integral over all values of z of p of xi given z times p of z. And
[141.08s -> 145.16s]  you know, the usual trick, if you want to bring in some quantity that is not
[145.16s -> 148.64s]  currently in the equation, is to multiply by that quantity divided by
[148.64s -> 153.60s]  itself. So Q i z over Q i z is equal to 1, so we can multiply that in whenever
[153.60s -> 163.80s]  we want. So now we can notice that we have some quantity multiplied by Q i of
[163.80s -> 167.16s]  z, so we can write that quantity as an expected value under Q i of z. So we
[167.16s -> 171.08s]  basically take the numerator in that ratio, that becomes, that turns into an
[171.08s -> 174.76s]  expectation, and then everything else is left behind. So we have log of the
[174.76s -> 180.68s]  expected value under Q i of z of p x i given z times p of z divided by Q i
[180.68s -> 186.68s]  of z. Alright, so so far we haven't made any approximation, these are just,
[186.68s -> 191.16s]  this is just a little bit of algebraic manipulation. Next what we're going to
[191.16s -> 196.40s]  do is we're going to use Jensen's inequality. Jensen's inequality is a
[196.44s -> 203.28s]  way to relate convex or concave functions applied to linear combinations.
[203.28s -> 206.44s]  So what I have written out on the slide is a special case of Jensen's
[206.44s -> 210.64s]  inequality for the logarithm, which is a concave function, but in general
[210.64s -> 213.96s]  this inequality would hold true for any concave function. If you have a
[213.96s -> 218.52s]  convex function, then it holds true but the inequality goes the other way. So
[218.52s -> 223.20s]  for the case of logarithms, Jensen's inequality says that the logarithm of an
[223.20s -> 227.60s]  expected value over some variable y is greater than or equal to the expected
[227.60s -> 232.40s]  value of the logarithm of that variable. If this seems a little counterintuitive to
[232.40s -> 236.56s]  you, something you could consider is trying to draw a picture. So the
[236.56s -> 240.68s]  logarithm is a concave function, so it kind of goes like that, and if you
[240.68s -> 245.68s]  imagine the logarithm of a sum of functions, because the logarithm, the
[245.68s -> 249.96s]  rate at which the logarithm increases always decreases, then that sum of
[250.08s -> 254.40s]  functions, the logarithm of that sum of functions, will be greater than or equal to the sum of
[254.40s -> 258.36s]  the logarithms because of the rate of decrease. So if this is a little
[258.36s -> 262.24s]  unclear to you, try drawing out a picture of this, you know, of multiple
[262.24s -> 268.68s]  different logarithm functions getting summed together. Okay, so we can directly
[268.68s -> 273.48s]  apply Jensen's inequality to the result from the previous slide, and the way
[273.48s -> 278.64s]  that we do that is by noting that we have the log of the expected value
[278.64s -> 282.88s]  of some quantity, so applying that Jensen's inequality simply pushes the
[282.88s -> 287.08s]  expected value outside of the log and replaces the equality with a greater
[287.08s -> 291.16s]  than or equal to sign. So that means that our previous result is lower
[291.16s -> 297.40s]  bounded by the expected value under qi of z of the logarithm of the ratio p
[297.40s -> 304.56s]  of xi given z times p of z divided by qi of z. But now of course we know
[304.56s -> 309.12s]  that logarithms of products can be written out as sums of logarithms, so we
[309.12s -> 315.00s]  can equivalently write this out as the expected value under qi of z of log p xi
[315.00s -> 321.36s]  given z plus log p of z minus the expected value under qi of z of log qi
[321.36s -> 325.84s]  of z. And the reason I wrote it out like this is because I wanted to collect all
[325.84s -> 329.96s]  the terms that depend on p in the first part and all the terms that
[329.96s -> 335.48s]  depend on q in the second part. Now the nice thing about this equation here is
[335.48s -> 341.52s]  that everything is tractable, and this is true for any qi of z, so we
[341.52s -> 344.64s]  could just pick some random qi of z and we have a lower bound, although not
[344.64s -> 348.72s]  all qi's will of course lead to the best lower bounds, but we can pick some qi
[348.72s -> 353.96s]  of z, sample from it to evaluate the first expectation, and then the second
[353.96s -> 357.44s]  expectation you'll note is actually the equation for the entropy of qi of z,
[357.48s -> 361.72s]  which for many simple distributions like Gaussians has a closed form solution.
[361.72s -> 373.92s]  Okay, so we can replace that second term with just the entropy of qi. So maximizing
[373.92s -> 378.96s]  this could maximize log p xi, although as I mentioned before, you need to show
[378.96s -> 385.88s]  that the bound is not too loose. Now let me make a brief aside to sort of
[385.92s -> 389.32s]  recap some of the information theoretic quantities that we're encountering here.
[389.32s -> 392.92s]  Much of this we already saw. We already talked about entropy, for example, in the
[392.92s -> 396.12s]  exploration lectures, but I just want to briefly recap it because this stuff
[396.12s -> 399.12s]  is really important for getting a good intuition for what variational
[399.12s -> 405.10s]  inference is actually doing. So entropy, the entropy of some distribution, is the
[405.10s -> 410.40s]  negative expected value of the log probability of that distribution, and
[410.40s -> 415.00s]  here is an equation for the entropy of a Bernoulli distribution, so the
[415.00s -> 419.08s]  probability of a binary event, and you can see that the entropy goes up as the
[419.08s -> 423.36s]  probability of that event approaches 0.5, and it goes down to 0 if the event is
[423.36s -> 427.00s]  guaranteed to happen, so probability equals 1, or guaranteed not to happen,
[427.00s -> 433.48s]  probability equals 0. So one intuition for the entropy is how random is the
[433.48s -> 436.92s]  random variable. So this makes a lot of sense in the case of the Bernoulli
[436.92s -> 440.76s]  variable here, when it's 0.5, the variable is in some sense the most
[440.80s -> 446.00s]  random, the most unpredictable, and it has the highest entropy. The second
[446.00s -> 449.72s]  intuition is how large is the log probability in expectation under
[449.72s -> 455.04s]  itself. So if you mostly see low log probabilities in expectation under
[455.04s -> 458.56s]  yourself, that means that there are many many places to which you assign,
[458.56s -> 463.92s]  you know, roughly equal probabilities. If you mostly see very high log
[463.92s -> 467.16s]  probabilities, that means that you really concentrate around a few points
[467.32s -> 473.96s]  that you assign that probability to. So the top example has high entropy,
[473.96s -> 479.16s]  because log probabilities are generally lower everywhere, and the bottom one has
[479.16s -> 482.68s]  higher entropy, or lower entropy, because the log probability is very high
[482.68s -> 490.44s]  in just a few places, and that's a low entropy distribution. All right,
[490.44s -> 496.76s]  so then we could ask the question for the variational lower bound that we saw
[496.84s -> 501.00s]  on the previous slide, what do we expect it to actually do? So it's the expected
[501.00s -> 509.40s]  value of some quantity plus the entropy of Q i. So if this graph is showing p x
[509.40s -> 516.68s]  i comma z, so the thing inside the first expectation, you could imagine that
[516.68s -> 522.76s]  the expected value of this function would be maximized just by
[522.76s -> 526.96s]  putting a lot of probability mass on the tallest peak. Right, so this is what we
[526.96s -> 529.80s]  would get if we just maximize the first part. We just want to find a
[529.80s -> 534.44s]  distribution over z, inside of which we have the largest values of p of x
[534.44s -> 539.52s]  i comma z. But we're also trying to maximize the entropy of this
[539.52s -> 543.36s]  distribution, so we don't want to make it too skinny. If we're also trying
[543.36s -> 546.20s]  to maximize the entropy, then we want to spread out as much as possible
[546.20s -> 553.64s]  while still remaining in regions where p of x i comma z is large. So because we
[553.64s -> 557.12s]  have that second term, we get something that kind of spreads out. And the
[557.12s -> 564.08s]  intuition is that because of this, the Q i of z that maximizes this quantity
[564.08s -> 574.72s]  will kind of cover the p x i comma z distribution. Now the other concept I
[574.76s -> 580.24s]  want to recap here is KL divergence. KL divergence between two distributions q
[580.24s -> 586.28s]  and p is given by the expected value under the first distribution of the log
[586.28s -> 589.48s]  of the ratio of the probability of the first distribution divided by the
[589.48s -> 595.52s]  second. And again by exploring the fact that the logarithm of a product is a
[595.52s -> 601.04s]  sum of logarithms, we can write this out as the expected value under q of q
[601.20s -> 609.48s]  of x minus the expected value under q of log p of x, which we could rewrite in a
[609.48s -> 612.12s]  manner that looks a lot more like the equation on the previous slide if we
[612.12s -> 616.16s]  just trade places and recognize the expected value under q of log q is
[616.16s -> 621.88s]  just the negative entropy. So the KL divergence is the negative of the
[621.88s -> 629.68s]  expected value under q of log p of x and the entropy of q. One intuition for
[629.72s -> 633.40s]  what the KL divergence measures is how different two distributions are. You will
[633.40s -> 639.08s]  notice that when q and p are equal, the KL divergence is zero. It's easy to see
[639.08s -> 645.88s]  why it's zero because you have q over p equals one, log of one is zero. And the
[645.88s -> 649.00s]  second intuition is how small is the expected log probability of one
[649.00s -> 655.84s]  distribution over the other minus entropy. Now why entropy? Well for the
[655.84s -> 658.48s]  same reason that we saw before, because if you don't have the entropy term
[658.48s -> 664.68s]  then q will just want to sit at the most likely point under p, but
[664.68s -> 671.80s]  if we have the entropy then it wants to cover it. So the variational
[671.80s -> 675.56s]  approximation says that log p of xi is greater than or equal to the
[675.56s -> 682.20s]  expected value under qi of z of log pi of xi given z plus log p of z plus
[682.20s -> 686.92s]  the entropy of q. And we call this the evidence lower bound or variational
[687.00s -> 693.28s]  lower bound, which I'm going to denote as Li of p comma qi. And as we saw in the
[693.28s -> 699.60s]  previous slide, it's also the negative KL divergence. So what makes for a good qi
[699.60s -> 705.52s]  of z? Well the intuition is that a good qi of z should approximate p of z
[705.52s -> 710.16s]  given xi, because then you get the tightest bound. Approximate in what
[710.16s -> 716.00s]  sense? Well you can compare them in terms of KL divergence. So you can say
[716.08s -> 719.80s]  KL divergence measures the difference between two distributions. When the KL
[719.80s -> 723.60s]  divergence is zero, then the two distributions are exactly equal, so let's
[723.60s -> 728.96s]  pick qi to minimize the KL divergence between qi of z and the
[728.96s -> 734.88s]  posterior p of z given x. Why? Well because if we write out the KL
[734.88s -> 740.32s]  divergence using the definition from before, we'll see that it is equal to
[740.32s -> 749.28s]  the expected value under qi of z of log qi of z divided by p of z given xi. Now p
[749.28s -> 755.60s]  of z given xi can be written as p of xi, p of xi comma z divided by p of
[755.60s -> 759.20s]  xi, and since we're doing 1 over that, we flip the ratio and we get this
[759.20s -> 766.60s]  equation here. And again applying the property that the sum of log, the
[766.60s -> 772.72s]  log of a product is the sum of logs, we get this equation on this side. So we
[772.72s -> 777.84s]  have the first term, the negative expected value under qi of z of log p xi
[777.84s -> 781.96s]  given z plus log p of z, then we have the entropy term, and then we have
[781.96s -> 789.80s]  this prior term. So substituting in the equation for entropy, we get this. So that
[789.80s -> 794.84s]  means that the KL divergence between these two quantities is equal to the
[794.88s -> 801.84s]  negative variational lower bound plus the log probability of xi. Notice
[801.84s -> 809.56s]  however that log probability of xi doesn't actually depend on qi. So we
[809.56s -> 814.52s]  can rearrange the terms a little bit, and we can express log p of xi as
[814.52s -> 819.16s]  being equal to the KL divergence between qi and p plus the evidence
[819.16s -> 824.80s]  lower bound. And this is not an inequality, this is all exact. Now we know that
[824.80s -> 829.88s]  KL divergences are always positive, so this is actually another way to derive
[829.88s -> 834.80s]  the evidence lower bound, right, because you know that log p of xi is
[834.80s -> 838.60s]  equal to some positive quantity plus L, which means that L is a lower bound on
[838.60s -> 844.20s]  log p of xi. But furthermore, this equation shows that if we drive that KL
[844.20s -> 848.20s]  divergence to zero, then the evidence lower bound is actually equal to log p
[848.20s -> 854.20s]  of xi, which means that minimizing that KL divergence is an effective way to
[854.20s -> 862.68s]  tighten the bound. So this justifies why we want to choose qi of z to
[862.68s -> 867.12s]  approximate p of z given xi, and it also justifies why we want to use the
[867.12s -> 870.64s]  expected log likelihood, because when we use the expected log likelihood,
[870.64s -> 875.92s]  that's like taking the expectation under p of z given x, which is the point at
[875.92s -> 884.44s]  which the bound is tightest. Okay, so here's the equation we had before. We
[884.44s -> 890.96s]  use it to derive this bound, and the KL divergence we can write out like this.
[890.96s -> 896.88s]  This is what we saw on the previous slide. So that means that the KL
[896.88s -> 902.76s]  divergence is given by the negative variational lower bound plus this log p
[902.76s -> 908.40s]  of xi term. Now log p of xi doesn't depend on qi, so if we want to optimize
[908.40s -> 916.88s]  qi over z to minimize the KL divergence, we can equivalently optimize
[916.88s -> 924.44s]  the same evidence lower bound. So that's pretty appealing. Maximizing the
[924.44s -> 928.36s]  same evidence lower bound with respect to qi minimizes KL divergence and
[928.36s -> 932.36s]  once the titans are bound, maximizing with respect to p increases the log
[932.36s -> 937.20s]  likelihood. So now this immediately suggests a practical learning algorithm.
[937.20s -> 941.60s]  Take your variational lower bound, your evidence lower bound, maximize it
[941.60s -> 945.32s]  with respect to qi to get the tightest bound, and then maximize it with
[945.32s -> 950.44s]  respect to p to improve your model, to improve your log likelihood, and then
[950.44s -> 958.28s]  alternate these two steps. Okay, so just to recap this, our goal is to
[958.28s -> 964.80s]  maximize the log p theta of xi, but that's intractable, so instead we're
[964.80s -> 969.92s]  going to maximize the evidence lower bound. So for each xi, we'll calculate
[969.92s -> 975.96s]  the gradient with respect to the model parameters by sampling z's from our qi
[975.96s -> 981.72s]  of xi, and then using those samples to estimate the gradient. So this is a
[981.72s -> 987.08s]  single sample version, so you sample one z from qi of xi, and then assuming
[987.08s -> 990.40s]  your prior p of z doesn't depend on theta, then the gradient is just the
[990.40s -> 996.72s]  gradient of log p theta xi given that z, and then you improve your theta. And
[996.72s -> 1004.16s]  then you update qi to maximize the same evidence lower bound. So this is the
[1004.16s -> 1009.24s]  stochastic gradient descent version of variational inference. Just to state
[1009.24s -> 1012.64s]  this again so that we're all on the same page, in order to estimate the
[1012.64s -> 1018.40s]  gradient grad theta of li p comma qi, sample a z from the approximate
[1018.40s -> 1025.52s]  posterior qi, calculate the gradient grad theta li p comma qi as grad
[1025.52s -> 1030.72s]  theta log p theta xi given z, and then take that gradient step, and then
[1030.72s -> 1040.00s]  update your qi to maximize li p comma qi. Alright, so everything here is
[1040.00s -> 1044.88s]  straightforward except for the last line. How do you actually improve your qi?
[1044.88s -> 1051.76s]  Well, let's say that qi is given by a Gaussian distribution with mean mu i
[1051.76s -> 1058.08s]  and variance sigma i. Well, you could actually calculate the gradient with
[1058.08s -> 1063.88s]  respect to the mean and variance of the evidence lower bound, and then do
[1063.88s -> 1071.48s]  gradient descent on mu i and sigma i. So what's the problem with this? Well, how
[1071.48s -> 1076.08s]  many parameters do we have? Remember that we have a separate qi for every
[1076.08s -> 1081.80s]  data point xi, and they each have a different mu and a different sigma. This
[1081.80s -> 1086.08s]  is not a big deal if you have a few thousand data points, but it becomes a
[1086.08s -> 1088.96s]  big problem if you have millions of data points. And in the deep learning
[1088.96s -> 1094.80s]  setting, typically we would have a very large number of data points. So the total
[1094.80s -> 1098.08s]  number of parameters if we have this Gaussian distribution is the number of
[1098.08s -> 1103.52s]  parameters in the model theta plus the dimensionality of the mean plus the
[1103.52s -> 1111.44s]  dimensionality of the variance times the number of data points n. Okay, so
[1111.44s -> 1116.96s]  that's maybe a little too large. n might be a pretty large number, and this might
[1116.96s -> 1122.76s]  be intractable. But remember that our intuition is that qi of z needs to
[1122.76s -> 1128.28s]  somehow approximate the posterior p of z given xi. So what if instead of
[1128.28s -> 1131.96s]  learning a separate qi of z, a separate mean and variance for every data point,
[1131.96s -> 1138.16s]  what if you train a separate neural network model that approximates qi of z?
[1138.36s -> 1142.40s]  So instead of having a separate qi of z for every data point xi, we have one
[1142.40s -> 1150.56s]  network q of z given xi which aims to approximate the posterior. So then in
[1150.56s -> 1153.68s]  our generative model, we have one neural network that maps from z to x
[1153.68s -> 1161.72s]  and another neural network that maps from x to z. And that neural network
[1161.72s -> 1165.52s]  gives us a posterior estimate with a mean and variance that are given by
[1165.52s -> 1172.80s]  neural network functions of x. So that's the idea behind amortized variational
[1172.80s -> 1177.60s]  inference, and that's what I'll talk about in the next part of the lecture.
