# Detected language: en (p=1.00)

[0.00s -> 5.36s]  Okay, so in the next part of the lecture, I'm going to discuss some algorithms for
[5.36s -> 10.16s]  open-loop planning that make kind of minimal assumptions about the dynamics
[10.16s -> 14.04s]  model. So they require you to know the dynamics model, but otherwise they don't
[14.04s -> 17.68s]  make any assumption about it being continuous or discrete, stochastic or
[17.68s -> 23.08s]  deterministic, or about whether it's even differentiable. So for now we're
[23.08s -> 27.64s]  going to concentrate on the open-loop planning problem, where you are given a
[27.80s -> 31.80s]  state and you have to produce a sequence of actions that will maximize your reward
[31.80s -> 35.48s]  when you start from that state. So this won't be a very good idea for taking that
[35.48s -> 41.20s]  math test, but it can be a pretty good idea for many other practical problems.
[41.20s -> 46.32s]  Okay, so we'll start with a class of methods that can broadly be
[46.32s -> 49.92s]  considered stochastic optimization methods. These are often sometimes called
[49.92s -> 54.96s]  black-box optimization methods. So what we're going to do is we're going
[55.00s -> 59.04s]  to first abstract away the temporal structure and optimal controller
[59.04s -> 64.56s]  planning. So these methods are black-box, meaning that to them the optimization
[64.56s -> 67.56s]  problem you are solving is a black-box. So they don't care about the fact
[67.56s -> 70.88s]  that you have different time steps, that you have a trajectory distribution
[70.88s -> 74.52s]  over time, all they care about is that you have some maximization or
[74.52s -> 79.08s]  minimization problem. So the way we'll abstract this away is we'll say that
[79.08s -> 83.64s]  we're just solving some problem over some variables a1 through a capital T
[83.84s -> 90.08s]  with an objective that I'm going to denote as J. So J quantifies the expected
[90.08s -> 94.96s]  reward, but for these algorithms they don't really care about that. And in
[94.96s -> 98.92s]  fact they don't even care that the actions are a sequence, so we're going
[98.92s -> 102.96s]  to represent the entire sequence of actions as the capital A. So you can
[102.96s -> 106.28s]  think of capital A as basically just the concatenation of a1 through a
[106.28s -> 110.76s]  capital T. So this is just an arbitrary unconstrained optimization
[110.76s -> 116.72s]  problem. A very simple way to address this problem, which might at first seem
[116.72s -> 123.28s]  really silly, is to basically just guess and check. So you just pick a set of
[123.28s -> 127.36s]  action sequences from some distribution, maybe even uniformly at random, so just
[127.36s -> 134.28s]  pick a1 through a capital N, and then you choose your action sequence to
[134.28s -> 139.48s]  choose some ai based on which one is the arg max with respect to the index of
[139.48s -> 143.96s]  Ja. Basically choose the best action sequence instead of maximizing over
[143.96s -> 148.88s]  let's say a large or continuous valued sequence of actions, you just maximize
[148.88s -> 152.48s]  over a discrete index from 1 to N. That's very very easy to do, just check
[152.48s -> 156.64s]  each of those action sequences and take the one that gets the largest
[156.64s -> 162.40s]  reward, hence guessing check. This is sometimes referred to as the random
[162.40s -> 166.16s]  shooting method. Shooting because you can think of this procedure
[166.16s -> 169.44s]  where you pick this action sequence as sort of as random as shooting of the
[169.44s -> 172.48s]  environment, you say well if I just try this with a new sequence of actions
[172.48s -> 177.88s]  what will I get? This might seem like a really bad way to do control, but in
[177.88s -> 182.76s]  practice for low dimensional systems and small horizons this can actually work
[182.76s -> 188.36s]  very well, and it has some pretty appealing advantages. Take a moment to
[188.36s -> 193.20s]  think about what these advantages might be. So one major advantage of this
[193.24s -> 198.44s]  approach is that it is very very simple to implement. Coding this up takes just a
[198.44s -> 205.04s]  few minutes. It's also often quite efficient on modern hardware because you
[205.04s -> 208.20s]  know later on when we talk about learned models when your model is
[208.20s -> 212.08s]  represented by something like a neural network it can actually be quite nice
[212.08s -> 216.56s]  to be able to evaluate the value of multiple different action sequences in
[216.56s -> 221.52s]  parallel. You can essentially treat a1 through an as a kind of mini-badge and
[221.56s -> 225.60s]  evaluate the returns through your neural network model all simultaneously and
[225.60s -> 230.24s]  then the argmax is a max reduction. So there are typically very very fast ways
[230.24s -> 236.84s]  to implement these methods on modern GPUs with modern deep learning frameworks.
[236.84s -> 241.52s]  What's the disadvantage of this approach? Well you might not pick very
[241.52s -> 244.48s]  good actions because you're essentially relying on getting lucky. You're
[244.48s -> 248.80s]  relying on one of those randomly sampled action sequences being very good.
[248.80s -> 254.68s]  So one way that we can dramatically improve this random shooting method
[254.68s -> 258.44s]  while still retaining many of its benefits is something called the cross
[258.44s -> 262.84s]  entropy method or CEM. The cross-entropy method is quite a good
[262.84s -> 267.84s]  choice if you want a black box optimization algorithm for these kinds
[267.84s -> 272.64s]  of control problems in low to moderate dimensions with low to moderate time
[272.64s -> 277.60s]  horizons. So our original recipe with random shooting was to choose a
[277.68s -> 281.80s]  sequence of actions from some distribution like the uniform distribution and then
[281.80s -> 286.72s]  pick the argmax. So what we're going to do in cross-entropy method is we're
[286.72s -> 291.04s]  going to be a bit smarter about selecting this distribution. Instead of
[291.04s -> 294.72s]  sampling the actions completely at random from the let's say the uniform
[294.72s -> 299.28s]  distribution over all valid actions, we'll instead select this distribution to
[299.28s -> 303.68s]  focus in on the regions where you think the good actions might lie and this
[303.68s -> 307.96s]  will be an iterative process. So the way that we're going to do better
[307.96s -> 311.92s]  intuitively will be like this. Let's say that we generated four samples
[311.92s -> 315.80s]  and here's what those samples look like. So the horizontal axis is A, the
[315.80s -> 319.48s]  vertical axis J of A. What we're going to do is we're going to fit a new
[319.48s -> 323.24s]  distribution to the region where the better samples seem to be located and
[323.24s -> 326.84s]  then we'll generate more samples from that new distribution, refit the
[326.84s -> 331.92s]  distribution again, and repeat. And in doing this repeatedly we're going to
[331.96s -> 335.40s]  hopefully arrive at a much better solution because each time we generate
[335.40s -> 338.64s]  more samples we're focusing in on the region where the good samples seem to
[338.64s -> 343.12s]  lie. So one way that we can instantiate this idea, for example
[343.12s -> 348.64s]  with continuous valued actions, is we can iteratively repeat the
[348.64s -> 353.64s]  following steps. Sample your actions from some distribution P of A where
[353.64s -> 358.40s]  initially P of A might just be the uniform distribution. Then evaluate the
[358.40s -> 361.92s]  return of each of those action sequences and then pick something
[361.92s -> 366.36s]  called the elites. So the elites is a subset of your n samples, so you pick
[366.36s -> 371.08s]  m of those samples, where m is less than n, that have the highest value. One
[371.08s -> 375.08s]  common choice is to pick the 10% of your samples with the best value. And
[375.08s -> 379.36s]  then we're going to refit the distribution P of A just to the
[379.36s -> 384.48s]  elites. So for example if you choose your distribution to be a Gaussian
[384.56s -> 390.16s]  distribution, you would simply fit the Gaussian, find the max likelihood fit, to
[390.16s -> 394.84s]  the best m samples among the n samples that you generated. And then you
[394.84s -> 398.40s]  repeat the process, then you generate n more samples from that fitted
[398.40s -> 402.52s]  distribution, evaluate their return, pick their elites, and find a new
[402.52s -> 407.08s]  distribution that's hopefully better. Cross-entry method has a number of
[407.08s -> 411.00s]  very appealing guarantees. If you choose a large enough initial distribution and
[411.00s -> 414.08s]  you generate enough samples, cross-entry method will in general
[414.08s -> 418.32s]  actually find the global optimum. Of course for complex problems, that
[418.32s -> 421.04s]  number of samples and that number of iterations might be prohibitively large,
[421.04s -> 425.64s]  but in practice cross-entropy can work pretty well, and it has a number
[425.64s -> 429.08s]  of advantages. So because you're evaluating the return of all of your
[429.08s -> 434.56s]  action sequences in parallel, this is very friendly to modern kind of deep
[434.56s -> 438.24s]  learning frameworks that can accommodate a mini-batch. The method does not require
[438.24s -> 441.36s]  your actions, your model to be differential with respect to the
[441.36s -> 446.12s]  actions, and it can actually be extended to things like discrete actions by
[446.12s -> 451.40s]  using other distribution classes. So typically you would use a Gaussian
[451.40s -> 454.88s]  distribution for continuous actions, although other classes can also be used.
[454.88s -> 458.80s]  And you can make CEM quite a bit fancier, so if you want to check out
[458.80s -> 463.76s]  more sophisticated methods in this category, check out CMAES. CMAES, which
[463.76s -> 467.92s]  stands for covariance matrix adaptation evolution strategies, is a kind of
[467.92s -> 472.64s]  extension to CEM, which includes kind of momentum style terms, where if you're
[472.64s -> 478.12s]  going to take many iterations, then CMAES can produce better solutions with
[478.12s -> 483.92s]  smaller population sizes. Okay, so what are the benefits of these methods to
[483.92s -> 487.92s]  summarize? Well, they are very fast if they're parallelized. They are generally
[487.92s -> 493.04s]  extremely simple to implement. What's the problem? The problem is that they
[493.04s -> 495.76s]  typically have a pretty harsh dimensionality limit. You're really
[495.76s -> 499.28s]  relying on this random sampling procedure to give you good coverage or
[499.28s -> 503.52s]  potential actions, and while refitting your distribution like we
[503.52s -> 508.96s]  did in CEM can help the situation, it still poses a major challenge. And
[508.96s -> 515.16s]  these methods only produce open-loop planning. So the dimensionality limit, if
[515.16s -> 518.08s]  you want kind of a rule of thumb, obviously depends on the details of your
[518.08s -> 522.28s]  problem, but typically if you have more than about 30 to 60 dimensions, chances
[522.28s -> 526.00s]  are these methods are going to struggle. You can sometimes get away with
[526.00s -> 530.64s]  longer sequences, so if you have, let's say, a 10-dimensional problem and you
[530.64s -> 535.24s]  have 15 time steps, you technically have 150 dimensions, but the successive
[535.24s -> 538.24s]  time steps are strongly correlated with each other, so that might still
[538.24s -> 543.88s]  work. But generally, you know, 30 to 60 dimensions works really well. If you're
[543.88s -> 548.80s]  doing planning, rule of thumb, 10 dimensions, 15 time steps is a bad way
[548.80s -> 551.56s]  you're going to be able to do much more than that, and you'll start running
[551.56s -> 558.16s]  into problems. Okay, next we're going to talk about another way that we can do
[558.16s -> 562.88s]  planning, which actually does consider the closed, you know, closed-loop feedback,
[562.88s -> 567.64s]  which is Monte Carlo tree search. Monte Carlo tree search can accommodate
[567.64s -> 571.00s]  both discrete and continuous states, although it's a little bit more commonly
[571.00s -> 576.04s]  used used for discrete states, and it's particularly popular for board games. So
[576.04s -> 579.80s]  things like AlphaGo actually used a variant of Monte Carlo tree search. In
[580.00s -> 584.28s]  general, Monte Carlo tree search is a very good choice for kind of games of
[584.28s -> 589.20s]  chance, so poker, for example, is a common application for Monte Carlo tree
[589.20s -> 594.36s]  search. So here's how we can think about Monte Carlo tree search. Let's say
[594.36s -> 596.72s]  that you want to play an Atari game. Let's say you want to play this game
[596.72s -> 601.28s]  called SeaQuest, where you have to shoot these torpedoes at some fish. I
[601.28s -> 603.68s]  don't know why you want to shoot torpedoes at fish, that seems
[603.68s -> 608.52s]  ecologically irresponsible, but that's what the game requires you to do. And
[608.56s -> 612.16s]  the game requires selecting from a discrete set of actions to control your
[612.16s -> 616.44s]  little submarine. What you could imagine if you have access to a model is you
[616.44s -> 621.00s]  could take your starting state and see what happens if you take action A1
[621.00s -> 625.08s]  equals 0 and see what happens if you take action A1 equals 1. Maybe you have
[625.08s -> 628.80s]  just two actions. And both of them, you know, each of those actions will put
[628.80s -> 632.32s]  you in a different state. It might put you in a different state each time you
[632.32s -> 636.30s]  take that action, so the true dynamics might be stochastic, but that's okay. We
[636.30s -> 640.02s]  would just take the action multiple times and see what happens. And then
[640.02s -> 644.46s]  maybe for each of the possible states you land in, you can try every possible
[644.46s -> 650.26s]  value fraction A2 and so on and so on. Now if you can actually do this, you
[650.26s -> 652.98s]  will eventually find the optimal action to take, but this is
[652.98s -> 656.54s]  unfortunately an exponentially expensive process. So without some
[656.54s -> 660.90s]  additional tricks, this general unrestricted unconstrained tree search
[660.90s -> 665.02s]  requires an exponential number of expansions at every layer, which means
[665.06s -> 668.60s]  that if you want to control your system for capital T time steps, you need
[668.60s -> 672.46s]  a number of steps that's exponential capital T, and that's no good. We don't
[672.46s -> 678.02s]  want that. So how can we approximate the value of some state without expanding
[678.02s -> 682.30s]  the full tree? Well one thing you could imagine doing is when you land at
[682.30s -> 685.82s]  each, at some node, let's say you pick a depth, let's say you say my depth is
[685.82s -> 690.34s]  going to be 3, I'll expand the tree to depth 3, and after three steps what
[690.34s -> 695.30s]  I'll do is I'll just run some baseline policy. Maybe it's even
[695.30s -> 699.86s]  just a random policy. Now the value that I get when I run that baseline
[699.86s -> 704.54s]  policy is not really going to be exactly the true value of having
[704.54s -> 708.38s]  taken those actions, but if I've expanded enough actions, and especially if I have
[708.38s -> 713.42s]  something like a discount factor, that roll out with the random policy might
[713.42s -> 717.42s]  still give me a reasonable idea of how good those states are.
[717.46s -> 724.82s]  Essentially if I land in a really bad state, the random policy will probably do badly. If I land in a really good state, let's say I land in a state where I'm about to win the game,
[724.82s -> 730.62s]  pretty much any move will probably give me a decent value. So it's not an
[730.62s -> 734.26s]  optimal strategy, it's not going to give you exactly the right value, but it
[734.26s -> 738.54s]  might be pretty good if you expand the tree enough and use a sensible roll
[738.54s -> 742.34s]  out policy. In fact in practice Monte Carlo tree search is actually a very good
[742.34s -> 746.98s]  algorithm for these kinds of discrete stochastic settings where you really
[747.02s -> 753.06s]  want to account for the closed loop case. Okay, so this might at first seem a
[753.06s -> 756.54s]  little weird, a little contradictory, but it turns out the basic idea actually
[756.54s -> 762.38s]  works very well. Okay, now we can't of course search all the paths, so the
[762.38s -> 765.82s]  question we usually have to answer with Monte Carlo tree search is with
[765.82s -> 771.14s]  which path do we search first? So we start at the root, we have action a1
[771.14s -> 776.70s]  equals 0 and a1 equals 1, which one do we start with? Well let's
[776.70s -> 781.58s]  say that we picked a1 equals 0. We don't know anything about these actions
[781.58s -> 785.86s]  initially, so we have to make that choice arbitrarily or randomly. We
[785.86s -> 792.94s]  picked a1 equals 0 and we got a reward of a value of plus 10. Now plus 10
[792.94s -> 796.14s]  here refers to the full value of that roll out, so it refers to what we got
[796.14s -> 801.58s]  from taking action a1 equals 0 and then running our baseline policy. Now at this
[801.58s -> 806.88s]  point we don't know whether plus 10 is good or bad, it's just a number, so we
[806.88s -> 809.54s]  have to take the other action. We don't know anything about the other action, so
[809.54s -> 812.70s]  we can't really trade off which of these two paths is more promising to
[812.70s -> 817.14s]  explore. Let's say we take the other action and we get a return of plus 15,
[817.14s -> 821.70s]  and remember this plus 15 refers to the total reward you get from taking the
[821.70s -> 827.74s]  action a1 equals 1 and then running your baseline policy. Now we have to
[827.74s -> 831.74s]  remember something very important here. We are planning in a stochastic system,
[831.74s -> 836.46s]  which means that if we were to take a1 equals 0 again and run that random
[836.46s -> 840.66s]  policy again, we might not get plus 10 again, we might get something else. We
[840.66s -> 843.86s]  might get something else because our policy is random and because the
[843.86s -> 850.38s]  outcome of a1 equals 0 is also random. So these values should be
[850.38s -> 857.82s]  treated as sample-based estimates for the real value of taking that action.
[858.62s -> 863.98s]  Okay, so at this point if we look at these two outcomes, a reasonable
[863.98s -> 869.66s]  conclusion we might draw is that action 1 is a bit better than action
[869.66s -> 873.70s]  0. We don't know that for sure, we might be wrong, but we took both actions
[873.70s -> 877.74s]  once and one of them ended up being better, so if you really had to choose
[877.82s -> 881.98s]  which direction to explore, maybe we should explore the one that produced the better return.
[881.98s -> 887.06s]  So the intuition is you choose the nodes with the best return, but you
[887.06s -> 891.22s]  prefer rarely visited nodes, so if some node was not visited before at all, you
[891.22s -> 893.78s]  really need to try it because you have no way of knowing whether its return is
[893.78s -> 897.54s]  better or worse. But at this point we probably want to explore the right
[897.54s -> 902.94s]  subtree. Okay, so let's try to formalize this into an actual algorithm.
[902.94s -> 909.18s]  Here's a generic sketch of an MCTS method. First we're going to take our
[909.18s -> 915.98s]  current tree and we're going to find a leaf SL using some tree policy. The
[915.98s -> 918.70s]  term tree policy doesn't refer to an actual policy that you run in the
[918.70s -> 922.86s]  world, it refers to a strategy for looking at your tree and selecting which
[922.86s -> 931.26s]  leaf node to expand. Step 2, you expand that leaf node using your default
[931.30s -> 936.10s]  policy. Now default policy here is actually referring to a real policy like
[936.10s -> 940.70s]  that random policy that I had before. How do you expand the leaf? Well,
[940.70s -> 946.62s]  remember that the nodes in the tree correspond to actual sequences. The same
[946.62s -> 950.38s]  actual sequence executed multiple times might actually lead to different
[950.38s -> 954.66s]  states, so the way that you evaluate a leaf is you take, you start in the
[954.66s -> 958.10s]  initial state s1 and then you take all the actions on the path from that
[958.14s -> 964.82s]  leaf to the root and then follow the default policy, so you don't just
[964.82s -> 968.70s]  teleport to some arbitrary state. You could do the teleporting thing too and
[968.70s -> 972.14s]  that would also give you actually a well-defined algorithm, but typically
[972.14s -> 975.46s]  you would actually execute the same sequence of actions again to actually
[975.46s -> 978.22s]  give them the chance to lead to a different random outcome, because
[978.22s -> 982.94s]  remember you want the actions that are best in expectation. And then step
[982.94s -> 988.58s]  3, update all the values in the tree between s1 and sL and then repeat this
[988.58s -> 994.78s]  process. And then once you're done, you take the best action from the root s1
[994.78s -> 999.06s]  and typically in MCTS you would actually rerun the whole planning
[999.06s -> 1002.50s]  process each time each time steps you would take the best action from the
[1002.50s -> 1006.50s]  root and then the world would randomly present you with a different state and
[1006.54s -> 1013.38s]  then you would do all the planning all over again. Okay, so our tree policy
[1013.38s -> 1017.50s]  initially can't do anything smart. We haven't expanded any of the actions. You
[1017.50s -> 1023.58s]  just have to try action 0 and then you evaluate the using the default
[1023.58s -> 1028.86s]  policy and then you update all the values in the tree between s1 and sL, sL
[1028.86s -> 1034.28s]  here is s2, and notice here that we collected a return which is 10 and we
[1034.28s -> 1040.28s]  also record how many times we visited that node which is 1. Now we have to
[1040.28s -> 1043.96s]  expand the other action, we can't really say anything meaningful about it
[1043.96s -> 1048.80s]  without expanding it, so we go and expand the action 1 and there we get a
[1048.80s -> 1055.32s]  return of 12 and n equals 1 because we visited only once. So a very common
[1055.32s -> 1060.88s]  choice for the tree policy is the the UCT tree policy which basically
[1060.88s -> 1065.36s]  follows the following recipe. If some state has not been fully expanded,
[1065.36s -> 1071.02s]  choose a new action in that state. Otherwise, choose a child of that state
[1071.02s -> 1075.90s]  with the best score and the score will be defined shortly and then you
[1075.90s -> 1080.20s]  apply this recursively. So essentially this tree policy starts at the root s1,
[1080.20s -> 1084.32s]  if some action at the root is not expanded then you expand it, otherwise
[1084.32s -> 1089.98s]  you choose a child with the best score and then recurse. So here, you
[1090.02s -> 1093.98s]  know, for any reasonable value of score we would have chosen s2 because they both
[1093.98s -> 1098.98s]  have been visited the same number of times but the value at s2 is larger so
[1098.98s -> 1104.98s]  we would go and expand a new action for s2 and maybe we get a return of 10.
[1104.98s -> 1111.90s]  Now the n value at that leaf is 1 but remember step 3 in MCTS is to
[1111.90s -> 1116.34s]  propagate all the values back up to the root so we also update s2 to give
[1116.34s -> 1122.74s]  it n equals 2 and q equals 22. So essentially every time we update a node
[1122.74s -> 1127.46s]  we add the new value to its old value and we add 1 to its count. That
[1127.46s -> 1130.90s]  way we can always recover the average value at some node by dividing
[1130.90s -> 1135.94s]  q by n. By the way, one thing I might mention is when you see these indices
[1135.94s -> 1141.50s]  s1, s2, s3, these numbers are just referring to the time step.
[1141.54s -> 1146.50s]  Remember these nodes do not uniquely index states. If you take the same action
[1146.50s -> 1149.78s]  sequence two times you might get a different state but I'm still referring
[1149.78s -> 1154.18s]  that as s2 or s3 because it's the state at time step 2 or 3. So the
[1154.18s -> 1161.28s]  actual states are stochastic. All right, so now we have a choice to make. We
[1161.28s -> 1167.42s]  have two possible choices from the root. One leads to a node with q equals 10
[1167.50s -> 1174.18s]  and n equals 1. The other leads to q equals 22 and n equals 2. So the action 1
[1174.18s -> 1178.70s]  still leads to a higher average value which is 11 but the action 0 leads to
[1178.70s -> 1181.98s]  another that's been visited less often. So here the choice of score is
[1181.98s -> 1185.70s]  actually highly non-trivial. There are many possible choices for the
[1185.70s -> 1191.30s]  score in MCTS but one very common choice which is this UCT rule is to
[1191.30s -> 1196.52s]  basically choose a node based on its average value, so q over n, plus some
[1196.60s -> 1201.88s]  bonus for rarely visited nodes. So one commonly used bonus is written here.
[1201.88s -> 1207.44s]  It's two times some constant of the square root of two times the natural
[1207.44s -> 1212.12s]  log of the count at the current node divided by the count at the target node.
[1212.12s -> 1218.04s]  So the denominator basically refers to how many times each child of the
[1218.04s -> 1221.80s]  current node has been visited. The intuition is that the less often a
[1221.84s -> 1227.44s]  child has been visited, the more you want to take the corresponding action. So
[1227.44s -> 1233.00s]  here the node for action 0 has a denominator of 1, the node for action
[1233.00s -> 1238.62s]  1 has a denominator of 2. So a1 equals 0 has a bigger bonus. The
[1238.62s -> 1242.32s]  numerator is two times the natural log of the number of times the current
[1242.32s -> 1246.96s]  node has been visited and that's meant to basically account for the fact
[1247.04s -> 1253.80s]  that if you visited some node a very small number of times then you want to
[1253.80s -> 1258.40s]  prioritize novelty more. If you visit a node a very large number of times then
[1258.40s -> 1263.52s]  then you probably have a more confident estimate of the values. Okay so in this
[1263.52s -> 1268.00s]  case we would probably actually choose to visit the node a1 equals 0 because
[1268.00s -> 1272.28s]  even though its average value is lower, its n value is also lower so we get a
[1272.28s -> 1276.72s]  larger bonus which might exceed the difference in value if the
[1276.72s -> 1280.76s]  constant c is large enough. And then when we visit that node we have to just
[1280.76s -> 1283.92s]  expand an arbitrary new action because we don't know the value of
[1283.92s -> 1289.14s]  anything else. And then here we record q equals 12, n equals 1, we again
[1289.14s -> 1292.88s]  propagate it back up to the root so add the n to the n at the
[1292.88s -> 1297.16s]  parent node, add the q to the q at the parent node, and now we have two nodes with
[1297.16s -> 1300.80s]  equal values, they're both 11. So we have to break the tie somehow
[1300.80s -> 1306.20s]  arbitrarily and we go over here, we get a q equals 8 and n equals 1 and now
[1306.24s -> 1310.56s]  the value at this node becomes 30 and the denominator is 3. So now take a moment to
[1310.56s -> 1316.64s]  think about which way MCTS would go. Yep, it has to go to the right because then
[1316.64s -> 1320.04s]  the node for the right, the one corresponding to action a1 equals 1,
[1320.04s -> 1324.12s]  has both a larger value and a lower visitation count so that's what we're
[1324.12s -> 1329.00s]  going to do. And so on. So then this process will recurse for some number
[1329.00s -> 1333.16s]  of steps. You have to choose your step based on your computational budget and
[1333.20s -> 1337.16s]  once your computational budget is exhausted then you take the action
[1337.16s -> 1343.04s]  leading to the node with the best average return. Okay, if you want to
[1343.04s -> 1347.40s]  learn more about MCTS, I would highly recommend this paper, A Survey of
[1347.40s -> 1350.32s]  Monte Carlo Tree Search Methods, which provides kind of a high-level overview.
[1350.32s -> 1355.00s]  In general MCTS methods are very difficult to analyze theoretically and
[1355.00s -> 1358.00s]  they actually have surprisingly few guarantees but they do work very well
[1358.00s -> 1362.28s]  in practice and if you have a kind of a some kind of game of chance for
[1362.40s -> 1366.96s]  stochasticity, these kinds of algorithms tend to be a very good choice. And of
[1366.96s -> 1371.00s]  course there are many ways you can make MCTS more clever, for instance by
[1371.00s -> 1375.12s]  actually learning your default policy using the best policy you've got. You
[1375.12s -> 1378.76s]  could use value functions to evaluate terminal nodes and so on. If you take
[1378.76s -> 1382.68s]  this to the extreme you get something similar to, for example, what AlphaGo
[1382.68s -> 1386.56s]  actually did which was a combination of MCTS and reinforcement learning of
[1386.56s -> 1390.64s]  value functions and default policies.
