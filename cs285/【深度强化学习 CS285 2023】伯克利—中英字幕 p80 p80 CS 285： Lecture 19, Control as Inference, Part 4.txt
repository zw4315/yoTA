# Detected language: en (p=1.00)

[0.00s -> 4.48s]  All right, let's discuss a few reinforcement learning algorithms based
[4.48s -> 8.60s]  on the ideas that I presented in the previous section, as well as some maybe
[8.60s -> 13.64s]  more direct approaches. So first, we could imagine doing Q-learning with
[13.64s -> 20.44s]  soft optimality. Standard Q-learning uses this update, right, so you set your
[20.44s -> 24.60s]  new phi vector to be the old one plus some learning rate times grad phi Q
[24.60s -> 31.24s]  times the Bellman error, r plus next to v minus Q, where your target value is just
[31.24s -> 38.28s]  the max over the Q values of the next time step. You can implement a soft
[38.28s -> 43.00s]  Q-learning procedure in exactly the same way. In fact, the Q-function update is
[43.00s -> 47.56s]  identical. The only difference is when you calculate your target value, instead
[47.56s -> 52.16s]  of taking a hard max over the next action, you take a soft max, which is a
[52.16s -> 57.28s]  log integral of exponentiated Q-values or a log sum of exponentiated values if
[57.28s -> 61.64s]  you have discrete actions. So that's very straightforward, and you can derive it
[61.64s -> 65.88s]  in the same way that we derived Q-learning from value iteration when
[65.88s -> 69.04s]  we talked about value-based methods before, and I would encourage you to do
[69.04s -> 73.12s]  this as a as an exercise if you want to get more familiar with this
[73.12s -> 78.26s]  material. The policy that you recover in soft Q-learning is given by the
[78.26s -> 82.38s]  exponentiated advantage rather than being the greedy policy, and this policy
[82.38s -> 85.62s]  can be shown to be the solution to the corresponding variational inference
[85.62s -> 91.42s]  problem. So the resulting algorithm, the resulting soft Q-learning algorithm,
[91.42s -> 96.90s]  looks, you know, very similar to the algorithms that we saw before. We take
[96.90s -> 102.06s]  some action and observe s i a i s i prime r i added to our buffer, sample a
[102.06s -> 106.02s]  mini-batch from our buffer, and then we compute a target value, where the
[106.02s -> 109.50s]  only difference is that instead of taking a max over the Q-value at the next time
[109.50s -> 114.70s]  step, now we take a soft max. But then just like before, we update our Q
[114.70s -> 118.94s]  function to take a regression step to improve our fit to the target value, and
[118.94s -> 123.70s]  we lazily update our target network parameters. So everything is exactly the
[123.70s -> 129.66s]  same except for the use of the soft max. We could also not worry about
[129.66s -> 134.22s]  dynamic programming and instead go back to the original objective that I
[134.22s -> 138.58s]  obtained from variational inference, which is the sum of expected rewards
[138.58s -> 143.70s]  plus the sum of entropies. And it's actually very straightforward to
[143.70s -> 147.82s]  derive a policy gradient algorithm to optimize a subjective. Of course, the
[147.82s -> 151.60s]  expected reward portion of the objective, its gradient is exactly the
[151.60s -> 162.02s]  standard policy gradient, the only new thing is the entropy. So the intuition
[162.02s -> 168.50s]  behind this procedure is that the policy pi will be proportional to the
[168.50s -> 172.22s]  exponentiated Q-value or equal to the exponentiated advantage when pi
[172.22s -> 178.82s]  minimizes the KL divergence between pi and 1 over z times exponentiated Q.
[178.82s -> 185.98s]  And of course this KL divergence is equal up to a constant to the
[185.98s -> 191.50s]  expectation under pi of Q minus the entropy of pi. So this is actually
[191.50s -> 195.10s]  often referred to as an entropy-regularized policy gradient, and it
[195.10s -> 198.54s]  can be a good idea for policy gradients because it combats premature
[198.54s -> 202.62s]  collapse of the policy entropy. Remember that the on-policy policy
[202.62s -> 206.82s]  gradient algorithm explores entirely with the stochasticity that's present in the
[206.82s -> 210.70s]  policy. So if the policy becomes too deterministic too early, you'll get
[210.70s -> 214.18s]  very bad results. So this entropy regularizer that we're adding to the
[214.18s -> 219.92s]  policy objective can be very helpful in practice. It turns out to be also
[220.08s -> 224.08s]  very closely related to Q-learning algorithms, and this is discussed in a
[224.08s -> 227.36s]  few previous works which I will reference at the end of the last part
[227.36s -> 233.40s]  of this lecture. Okay, let's talk a little bit about how policy gradients
[233.40s -> 240.04s]  relate to Q-learning in this inference framework. So if we write out the
[240.04s -> 243.66s]  objective, here there's the objective again, and I called it J of theta, just
[243.66s -> 249.20s]  like in the policy gradient lecture, the entropy is just the expected value
[249.20s -> 256.32s]  under pi of negative log pi. So this is an equivalent way of writing this. When
[256.32s -> 259.76s]  we take the gradient of this expression, we can write that gradient
[259.76s -> 264.24s]  as the sum of two terms. The first one is the regular policy gradient
[264.24s -> 269.32s]  that treats r minus log pi as a reward, and the second term has the
[269.32s -> 277.56s]  gradient going through the log pi term in the objective. Now it turns out
[277.60s -> 284.68s]  that if you actually crunch the numbers on this, the expression is actually equal
[284.68s -> 291.62s]  to the regular policy gradient with r minus log pi as the reward, with the
[291.62s -> 297.04s]  additional term added by the gradient through log pi simply being minus one.
[297.04s -> 303.12s]  Why is it minus one? Well, it's minus one if we apply the same identity that
[303.16s -> 311.48s]  we used to derive policy gradient, but in reverse, because we have a grad log pi
[311.48s -> 318.00s]  outside the parentheses, so the derivative of minus log pi is just
[318.00s -> 321.96s]  grad log pi times negative one. So that's where the minus one comes from.
[321.96s -> 327.08s]  It's very, very simple. But remember that the policy gradient is equal in
[327.08s -> 331.72s]  expectation if you add or subtract any quantity to the reward. So that
[331.76s -> 336.92s]  minus one actually has no effect, which leads us to the surprising conclusion
[336.92s -> 340.36s]  that if you want to do entropy- regularized policy gradient, all you
[340.36s -> 344.00s]  have to do is subtract log pi from the reward, and the gradient expression
[344.00s -> 349.84s]  doesn't actually change. So that's where the one comes from, and you can
[349.84s -> 354.68s]  ignore it because of the baseline. All right, so this quantity inside the
[354.68s -> 358.76s]  parentheses, you can think of it as the Q-value at the next time step,
[358.76s -> 363.00s]  right? So this is the sum of all the rewards minus all the log pis from t
[363.00s -> 369.80s]  plus one until the end. And remember that the policy is given by the
[369.80s -> 374.56s]  exponential of Q minus v in soft optimality. So we can substitute this
[374.56s -> 382.28s]  in for log pi, and we can, basically replacing log pi everywhere with Q minus
[382.28s -> 386.04s]  v, we can write this expression as following. We can write it as grad Q
[386.04s -> 395.72s]  minus grad v times R plus next Q minus current Q plus current v. But remember,
[395.72s -> 400.28s]  because of the baseline property, any state-dependent function inside the
[400.28s -> 407.20s]  parentheses can be removed. So this v of s,t we can simply get rid of it. And now
[407.20s -> 412.36s]  remember what the Q-learning objective is. The Q-learning objective is grad Q
[412.52s -> 419.60s]  times R plus the softmax of the next Q minus the current Q. So here we can see
[419.60s -> 424.80s]  now that the policy gradient actually looks a lot like the Q-learning
[424.80s -> 428.40s]  objective under the soft optimality framework. The main difference being the
[428.40s -> 432.88s]  policy gradient subtracts the minus grad v, and the Q-learning objective has
[432.88s -> 439.56s]  a softmax. So the Q-learning objective has this off-policy correction. If you
[439.56s -> 443.52s]  get an on-policy Q-learning method, you could omit this, and the policy gradient
[443.52s -> 448.60s]  has the minus grad v term, but otherwise they're actually quite similar.
[448.76s -> 452.96s]  All right, so that's maybe a little bit of a tidbit involving the
[452.96s -> 456.28s]  connection between these two approaches in the soft optimality
[456.28s -> 460.24s]  framework. But maybe more practically, what are some of the benefits of all
[460.24s -> 464.00s]  this variational inference, control-less inference, and soft optimality stuff?
[464.00s -> 468.84s]  Well, one benefit, at least in the case of policy gradients, is that it
[468.84s -> 473.04s]  improves exploration and prevents premature entropy collapse, which for
[473.04s -> 478.00s]  policy gradients can greatly harm exploration. It can be somewhat easier to
[478.00s -> 481.52s]  specialize or fine-tune policies for more specific tasks. When you end up
[481.52s -> 485.72s]  with policies that are more random, it turns out they're better suited for
[485.72s -> 488.96s]  fine-tuning when the task changes slightly, and I'll show some examples
[488.96s -> 493.80s]  that illustrate this in the next part of the lecture. It provides a principled
[493.80s -> 497.56s]  approach to break ties, so if two actions really have the same exact
[497.56s -> 501.24s]  advantage, they will also have the same exact probability, without having to worry
[501.24s -> 506.00s]  about how to take an argmax. This approach also provides better
[506.00s -> 510.32s]  robustness, because you achieve better coverage of different states. You can
[510.32s -> 514.24s]  think of this intuitively as following. If you learn to solve the task in many
[514.24s -> 518.52s]  possible ways, then if one of those ways becomes invalid due to a change in
[518.52s -> 522.92s]  the environment, then you might still have a non-zero chance of succeeding.
[523.60s -> 527.68s]  And of course, this framework reduces to classical hard optimality as the reward
[527.68s -> 531.28s]  magnitude increases, or if you put a temperature and drive that temperature
[531.28s -> 537.80s]  to zero. It's also a good model for modeling human behavior, which is not in
[537.80s -> 540.96s]  general deterministic, and humans do tend to make mistakes. So this
[540.96s -> 544.52s]  basically says you can make mistakes, but the mistakes become exponentially
[544.52s -> 550.04s]  unlikely as their reward decreases. So we'll talk about this a lot more on
[550.12s -> 555.24s]  Wednesday. All right, so just to review, we talked about how
[555.24s -> 559.76s]  reinforcement learning can be viewed as inference in a graphical model. We talked
[559.76s -> 565.36s]  about how the value function is a kind of backward message, and we talked about
[565.36s -> 571.44s]  how we can maximize the reward and the entropy, and the bigger the reward,
[571.44s -> 575.60s]  the less the entropy matters, so we can recover hard optimality. We talked about
[575.68s -> 580.56s]  how variational inference removes the optimism problem, and we discussed how
[580.56s -> 584.08s]  to instantiate it as either soft Q-learning or an entropy-regularized
[584.08s -> 588.52s]  pulse gradient procedure, but also how these two approaches actually have
[588.52s -> 591.96s]  some pretty striking similarities.
