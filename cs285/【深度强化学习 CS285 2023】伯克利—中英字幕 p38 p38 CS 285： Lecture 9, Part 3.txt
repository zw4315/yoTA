# Detected language: en (p=1.00)

[0.00s -> 5.32s]  All right, let's talk about how we can actually implement policy gradients with
[5.32s -> 8.34s]  constraints to instantiate the algorithm that we derived in the
[8.34s -> 16.60s]  previous section. So the claim that we had was that p theta of s t is close
[16.60s -> 22.04s]  to p theta prime of s t when pi theta is close to pi theta prime, and close
[22.04s -> 26.40s]  here was defined in terms of total variation divergence. Now it would help
[26.40s -> 30.88s]  us to get a slightly more convenient bound, because for some policy classes
[30.88s -> 35.32s]  imposing total variation divergence constraints is actually pretty difficult.
[35.32s -> 41.96s]  So a more convenient bound that we could derive, you can use the fact that
[41.96s -> 46.56s]  total variation divergence is actually related to KL divergence via an
[46.56s -> 50.80s]  inequality. So if you bound the KL divergence between pi theta prime and
[50.80s -> 54.72s]  pi theta, that also bounds the total variation divergence as per this
[54.72s -> 61.24s]  equation. So that means that the KL divergence between pi theta prime and pi
[61.24s -> 65.36s]  theta bounds the state marginal difference. For those of you that
[65.36s -> 69.56s]  aren't familiar with the KL divergence, it's basically the most widely used
[69.56s -> 73.08s]  type of divergence measure between distributions, and it can be very
[73.08s -> 78.32s]  convenient because it has tractable expressions expressed as basically
[78.32s -> 84.44s]  expected values of log probabilities, and many continuous value distributions
[84.44s -> 89.12s]  have tractable closed form solutions for the KL divergence. So this makes the
[89.12s -> 95.88s]  KL divergence very convenient to use. So the expression for a KL divergence is
[95.88s -> 102.36s]  the expected value of the log of the ratio between the distributions, and one
[102.36s -> 105.50s]  way to get some intuition for why this class of divergences is a little bit
[105.50s -> 108.86s]  more manageable is that the total variation divergence is expressed in terms
[108.86s -> 112.14s]  of absolute value, which means that it's in general not differentiable
[112.14s -> 116.00s]  everywhere, whereas the KL divergence is differentiable so long as the two
[116.00s -> 121.22s]  distributions have the same support. So the KL divergence has some very
[121.22s -> 124.78s]  convenient properties that make it much easier to approximate and much
[124.78s -> 128.94s]  easier to use in an actual reinforcement learning algorithm, as we'll see in the
[128.94s -> 132.70s]  remainder of this lecture. So in practice, if we actually want to have a
[132.70s -> 137.18s]  constrained policy gradient method, we're going to express our constraint as a KL
[137.18s -> 140.86s]  divergence rather than as a total variation divergence, but since the KL
[140.86s -> 144.82s]  divergence bounds the total variation divergence, this is a legitimate thing to
[144.82s -> 150.98s]  do and it preserves our bound. Okay, so then the objective that we're going to
[150.98s -> 156.02s]  want to optimize is our usual importance sampled objective with the
[156.02s -> 160.50s]  constraint that the KL divergence between pi theta prime and pi theta is
[160.58s -> 166.26s]  bounded by epsilon, and if epsilon is small enough, this is guaranteed to
[166.26s -> 172.78s]  improve j theta prime minus j theta because the error term will be bounded.
[172.78s -> 177.30s]  So how can we enforce this constraint? Well, there are a number of different
[177.30s -> 182.30s]  methods to do this, and you know one very simple one is to actually write
[182.30s -> 186.82s]  out an objective in terms of the Lagrangian of this constraint
[186.86s -> 191.74s]  optimization problem. So the Lagrangian, for those of you that don't remember
[191.74s -> 195.94s]  the convex optimization, is formed by simply taking the constraint, taking the
[195.94s -> 198.26s]  left-hand side of the constraint minus the right-hand side, and
[198.26s -> 202.42s]  multiplying it by Lagrange multiplier, and we have one constraint and one
[202.42s -> 207.46s]  Lagrange multiplier, which I'm calling lambda. If you want to solve a
[207.46s -> 212.06s]  constrained optimization problem, one of the things you can do is you can
[212.06s -> 215.78s]  alternate between maximizing the Lagrangian with respect to the primal
[215.82s -> 221.30s]  variables, which are theta, and then taking a step on the dual variables, a
[221.30s -> 225.94s]  gradient descent step. So the intuition is that you raise lambda if the
[225.94s -> 229.58s]  constraint is violated too much, otherwise you lower it, right, because if
[229.58s -> 233.18s]  the KL divergence is much larger than epsilon, then you're going to add
[233.18s -> 237.58s]  dKL minus epsilon, which is a positive number, to lambda, so lambda will get
[237.58s -> 241.22s]  bigger. If the constraint is much lower than epsilon, you'll add a negative
[241.22s -> 244.62s]  number, and lambda will get smaller. So you essentially alternate between
[244.66s -> 249.34s]  solving an unconstrained problem and adjusting your Lagrange multiplier to
[249.34s -> 252.90s]  enforce the constraint more or less strictly, depending on whether it's being
[252.90s -> 259.50s]  violated or not. This procedure is called dual gradient descent, and we'll
[259.50s -> 263.98s]  actually talk about dual gradient descent in much more detail in a
[263.98s -> 267.94s]  subsequent lecture, but for now you can just sort of take my word for it that
[267.94s -> 272.30s]  this procedure will asymptotically find the right Lagrange multiplier, which
[272.30s -> 277.06s]  means that it will actually solve your constraint optimization problem. Now this
[277.06s -> 280.94s]  is sort of the theoretically principled solution. You could imagine an even more
[280.94s -> 285.10s]  heuristic solution. Maybe you could just select a value of lambda manually
[285.10s -> 290.98s]  and just treat that KL divergence as a kind of regularizer, and intuitively
[290.98s -> 294.42s]  that also makes a lot of sense. You're basically penalizing your policy for
[294.42s -> 300.34s]  deviating too much from the old policy, and you can do this
[300.38s -> 303.74s]  maximization incompletely for a few gradient steps, because of course in
[303.74s -> 307.70s]  practice running the optimization of convergence might be very expensive or
[307.70s -> 313.14s]  impractical. So this is a complete algorithm for doing constrained
[313.14s -> 318.38s]  optimization with your policy. You can calculate the gradient of the objective,
[318.38s -> 321.90s]  and you get something that looks basically like an important sample
[321.90s -> 325.50s]  policy gradient, and you can calculate the gradient of the KL divergence. That's
[325.50s -> 329.70s]  also very straightforward to do, and you will get a derivative and you can use
[329.70s -> 333.82s]  it to maximize this, and this will allow you to have a reinforcement learning algorithm
[333.82s -> 338.54s]  that actually enforces a constraint. And there are a number of algorithms
[338.54s -> 342.06s]  that use some variant on this idea, including actually the original guided
[342.06s -> 347.70s]  policy search method and PPO, and these methods tend to work quite well.
