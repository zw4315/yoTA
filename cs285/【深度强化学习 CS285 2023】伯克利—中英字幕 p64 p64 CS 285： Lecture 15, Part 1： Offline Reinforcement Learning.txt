# Detected language: en (p=1.00)

[0.00s -> 5.00s]  In today's lecture, we're going to discuss offline reinforcement learning.
[5.00s -> 11.00s]  This lecture is going to cover quite a few fairly recent methods,
[11.00s -> 18.00s]  so it'll be both an overview of the fundamentals and also just kind of a survey of various techniques.
[18.00s -> 23.00s]  We'll talk about very classic techniques for offline RL going back several decades,
[23.00s -> 26.00s]  as well as some of the more recent ones.
[26.00s -> 31.00s]  But first, let's start with a little bit of motivation to explain why we're covering this topic and what this is.
[31.00s -> 41.00s]  One bit of motivation that I find pretty interesting is this observation that the kinds of situations
[41.00s -> 46.00s]  where current reinforcement learning methods of the sort that we learned about in previous lectures work really well,
[46.00s -> 53.00s]  they just look kind of different than the settings where we've seen supervised deep learning techniques be very effective.
[53.00s -> 59.00s]  As we all know, reinforcement learning is fundamentally, as we learned about it so far, an active and online process,
[59.00s -> 65.00s]  whether you're doing on-policy or off-policy algorithms, they're all about iteratively interacting with the environment,
[65.00s -> 70.00s]  collecting some data, using that to improve your policy, and then collecting some more data.
[70.00s -> 76.00s]  And we know that deep reinforcement learning methods work well across a range of different problem settings,
[76.00s -> 78.00s]  including ones that are really difficult.
[78.00s -> 85.00s]  But if we just look at the kind of domains where supervised methods work well, we see a really big gap.
[85.00s -> 90.00s]  And it's not so much a gap in terms of how complex the task is, but it's a gap in terms of generalization.
[90.00s -> 95.00s]  So if you look at, for example, state-of-the-art techniques for image recognition,
[95.00s -> 101.00s]  generally these techniques are tested on recognizing objects in images taken from the internet
[101.00s -> 104.00s]  that could be all over the world in any situation and so on.
[104.00s -> 111.00s]  Whereas the policies that we train with DeepRL, they typically work best in more constrained closed-world environments.
[111.00s -> 115.00s]  So the AlphaGo system can beat the world champion at Go,
[115.00s -> 118.00s]  but it doesn't have to worry about somebody spilling coffee on the Go board.
[118.00s -> 122.00s]  It deals with a closed-world environment with known and well-understood rules,
[122.00s -> 132.00s]  or at least these systems deal with settings where the world is contained within a particular kind of environment.
[133.00s -> 140.00s]  Now, it's fairly easy for us to say that maybe the only difference between the stuff on the left and the stuff on the right
[140.00s -> 142.00s]  is just the variability of data.
[142.00s -> 148.00s]  So if you were to take one of these DeepRL algorithms that we learned about in the previous lectures
[148.00s -> 154.00s]  and just scale it up so that it's trained with a huge amount of data from many different situations,
[154.00s -> 157.00s]  maybe it will generalize just as effectively as the supervised techniques.
[157.00s -> 162.00s]  And in fact, we're going to analyze that with some examples in a few moments.
[162.00s -> 167.00s]  But the big challenge with doing this in a classic kind of on-policy framework,
[167.00s -> 171.00s]  or even off-policy algorithms that iteratively collect more data and use a replay buffer,
[171.00s -> 177.00s]  is that we're essentially asking the algorithm to collect these very large datasets in the loop actively.
[177.00s -> 181.00s]  So if you imagine that you need an ImageNet-sized dataset to generalize effectively,
[181.00s -> 186.00s]  then for an on-policy algorithm, you're essentially asking for an ImageNet-sized dataset every iteration,
[187.00s -> 193.00s]  and even for an off-policy algorithm, you're asking for an ImageNet-sized dataset for every training run.
[193.00s -> 196.00s]  And that can quickly get extremely unwieldy.
[196.00s -> 204.00s]  Ideally, with datasets that are as large and as complicated as the ones that we need for supervised techniques,
[204.00s -> 210.00s]  the ones that require millions of samples or maybe billions of sentences for state-of-the-art language models,
[210.00s -> 216.00s]  ideally we would collect those only once and then use them repeatedly, just like in the world of supervised learning.
[219.00s -> 224.00s]  And this is really key, because if we really ask ourselves a very basic question,
[224.00s -> 228.00s]  what is it that makes modern machine learning work, at a very high level,
[228.00s -> 234.00s]  we really need large datasets and large high-capacity models to get effective generalization.
[234.00s -> 241.00s]  So we can't really get the sort of generalization that we've seen in supervised learning land with our RL methods
[241.00s -> 243.00s]  if we don't have these two ingredients.
[243.00s -> 248.00s]  And using large models is not too difficult, you just need a big enough data center,
[248.00s -> 253.00s]  but using those large datasets in an active learning framework can be very tough.
[254.00s -> 260.00s]  So the basic principle behind offline reinforcement learning is to essentially develop reinforcement learning methods
[260.00s -> 266.00s]  that can reuse previously collected datasets to essentially create a kind of a data-driven RL framework.
[266.00s -> 270.00s]  So, so far in the course we learned about on-policy RL.
[270.00s -> 274.00s]  On-policy RL is when you have a policy that interacts with the world, collects some data,
[274.00s -> 278.00s]  uses that data to update, throws that data out, and then interacts with the world some more.
[278.00s -> 281.00s]  So, for example, policy gradients are a classic example of this.
[281.00s -> 285.00s]  Every iteration, they collect some trials, they use those trials to improve the policy,
[285.00s -> 287.00s]  and then they discard them and collect some more.
[287.00s -> 290.00s]  We learned about off-policy RL techniques like Q-learning,
[290.00s -> 294.00s]  which you can kind of think of it as a buffered version of on-policy RL.
[294.00s -> 298.00s]  So for off-policy methods, they buffer all their data into the replay buffer,
[298.00s -> 303.00s]  and then they replay from that buffer, which is great for improving data efficiency,
[303.00s -> 308.00s]  but the off-policy RL methods that we learned about so far in the course
[308.00s -> 311.00s]  actually do require additional online collection,
[311.00s -> 315.00s]  meaning that after you update the policy, you do need to use that policy to collect some more data,
[315.00s -> 318.00s]  perhaps with some exploration scheme like epsilon greedy.
[318.00s -> 323.00s]  It may not be completely obvious why this is the case, and we'll actually analyze this shortly.
[323.00s -> 328.00s]  We'll actually discuss what happens if you simply run an off-policy RL algorithm,
[328.00s -> 331.00s]  but then stop collecting more data.
[331.00s -> 338.00s]  So the main topic for today's lecture will be to figure out how to devise offline RL algorithms,
[338.00s -> 341.00s]  and these are also sometimes called batch reinforcement learning algorithms,
[341.00s -> 345.00s]  and also less often fully off-policy reinforcement learning algorithms.
[345.00s -> 349.00s]  Basically, the difference between these and the other two is that in offline reinforcement learning,
[349.00s -> 352.00s]  you don't actually get this act of interaction, you just get a buffer.
[352.00s -> 357.00s]  So you assume that somebody else collected that buffer for you by writing some other policy,
[357.00s -> 361.00s]  which we refer to as a behavior policy. It's called pi-beta.
[361.00s -> 365.00s]  By the way, it's very important to distinguish the behavior policy
[365.00s -> 368.00s]  from the expert policy that we had in imitation learning.
[368.00s -> 371.00s]  So in offline reinforcement learning, we don't assume that pi-beta is any sort of expert.
[371.00s -> 376.00s]  Pi-beta could even be fully random. It's just a mechanism that was used to collect a data set.
[376.00s -> 381.00s]  And then that data set will be used during a training phase to train up the best policy we can get.
[381.00s -> 385.00s]  And then that policy is deployed, but once it's deployed, there's no more training.
[385.00s -> 390.00s]  So in a sense, this is just like every other machine learning discipline,
[390.00s -> 394.00s]  where you're given a data set, you train the best model you can on that data set,
[394.00s -> 396.00s]  and then you go and use that model.
[396.00s -> 399.00s]  And there's no active or online phase, there's no exploration.
[400.00s -> 406.00s]  And we could imagine different ways that such offline RL methods could actually be utilized in reality.
[406.00s -> 413.00s]  For example, you might imagine that you have a robot that is tasked with doing a wide variety of chores around the house,
[413.00s -> 417.00s]  and repairing electronics, and sweeping the floor, and cooking.
[417.00s -> 423.00s]  And maybe over the course of its lifetime, it gathers a large and very diverse data set of everything that it has done before.
[423.00s -> 428.00s]  And then when you want it to learn a new skill, maybe you will load up all of that past data,
[428.00s -> 433.00s]  use it to train up the best skill you can get, and that's your offline training phase.
[433.00s -> 436.00s]  And that already gives you a good initialization.
[436.00s -> 439.00s]  And then of course in reality, there's no reason to do things fully offline.
[439.00s -> 446.00s]  So you could then deploy it and collect more data, and further refine with some online reinforcement learning.
[446.00s -> 451.00s]  But the key thing here is that you're not relying entirely on that online phase,
[451.00s -> 454.00s]  you're actually getting a lot of basic functionality from the offline data.
[454.00s -> 459.00s]  This seems like actually a very reasonable model of how people would do things too.
[459.00s -> 464.00s]  If I were to try to explain to you some new tasks that you're supposed to be doing,
[464.00s -> 467.00s]  you would probably draw on your past experience and everything you know about the world
[467.00s -> 470.00s]  to get an initial guess for how to do it, and that guess might actually be very good.
[470.00s -> 475.00s]  But then you could still refine your skill with some practice, with some trial and error.
[476.00s -> 482.00s]  And developing these kinds of methods could potentially open the door to a wide range of RL applications
[482.00s -> 486.00s]  that are very difficult to do with fully online active settings,
[486.00s -> 491.00s]  obviously including things like robotics, but also domains where it's very difficult to imagine
[491.00s -> 495.00s]  applying RL methods with active exploration, like for example medical diagnosis.
[495.00s -> 500.00s]  Can you have a policy that learns to recommend prescriptions and tests for a patient?
[500.00s -> 503.00s]  It would be ridiculous to explore actively.
[503.00s -> 510.00s]  We already had our experience with active exploration for drug prescription in our exploration lectures last week.
[510.00s -> 515.00s]  It didn't go so well, but if you take prior data and use it to distill out the best policy,
[515.00s -> 517.00s]  maybe you can do it quite well.
[517.00s -> 521.00s]  Scheduling scientific experiments, controlling power grids, logistics networks, and so on.
[521.00s -> 525.00s]  There are a lot of applications where we would like to use learning-based control,
[525.00s -> 530.00s]  we would like to use reinforcement learning, but it's very difficult to imagine doing online exploration.
[531.00s -> 535.00s]  Okay, so let's get into more of the details.
[535.00s -> 538.00s]  What does the term offline RL really mean?
[538.00s -> 542.00s]  Well, so these are the pictures we had on the previous slide,
[542.00s -> 546.00s]  but formally we're going to say that we have a data set D,
[546.00s -> 550.00s]  and just like in our discussion of Q-learning from before,
[550.00s -> 552.00s]  the data set is really a data set of transitions.
[552.00s -> 555.00s]  State, action, next state, and reward.
[555.00s -> 559.00s]  Now these transitions are likely to be arranged into trajectories,
[559.00s -> 565.00s]  meaning that for the first transition, the S prime is actually the S for the second transition,
[565.00s -> 568.00s]  so very likely it's arranged into trajectories,
[568.00s -> 572.00s]  but the algorithms that we're going to discuss usually don't assume that,
[572.00s -> 575.00s]  with the exception of important sample policy gradients.
[575.00s -> 579.00s]  But most dynamic programming algorithms, most algorithms that are based on Q functions,
[579.00s -> 582.00s]  just assume that your data set is arranged into transitions.
[584.00s -> 588.00s]  We will have states, and we will often refer to state distributions.
[588.00s -> 591.00s]  We've used different notation in this class at different points,
[591.00s -> 593.00s]  mostly because I want to make sure that you guys are familiar
[593.00s -> 596.00s]  with all of the notation that's used in the literature,
[596.00s -> 601.00s]  but we will use the symbol D with a superscript corresponding to a policy
[601.00s -> 604.00s]  to denote a state marginal state distribution induced by that policy.
[606.00s -> 611.00s]  We will use pi beta to denote the unknown policy that collected our data set.
[611.00s -> 613.00s]  So in reality, you usually don't know what pi beta is,
[613.00s -> 616.00s]  so if someone gave you a data set, maybe you're doing autonomous driving,
[616.00s -> 618.00s]  and your data set consists of all sorts of human driving,
[618.00s -> 621.00s]  good drivers, bad drivers, mediocre drivers.
[621.00s -> 624.00s]  You don't actually know what their policy is,
[624.00s -> 626.00s]  but for the purpose of mathematical analysis,
[626.00s -> 629.00s]  you can assume that there exists some policy pi beta,
[629.00s -> 631.00s]  you just don't necessarily know it.
[632.00s -> 634.00s]  There's some kind of transition function,
[634.00s -> 636.00s]  there's some kind of reward that's just like before,
[636.00s -> 640.00s]  and as I said, the behavior policy is generally not known.
[640.00s -> 642.00s]  The RL objective is just like before,
[642.00s -> 645.00s]  is to maximize the expected sum of discounted rewards,
[645.00s -> 647.00s]  so nothing really changed there.
[647.00s -> 651.00s]  And there are several types of offline RL problems that we could define.
[652.00s -> 654.00s]  One sort of auxiliary problem,
[654.00s -> 658.00s]  which is not the full offline RL problem but is closely related,
[658.00s -> 660.00s]  is what's called off-policy evaluation.
[660.00s -> 661.00s]  And we'll discuss this a little bit,
[661.00s -> 664.00s]  the main focus of the lecture is on off-policy RL,
[664.00s -> 666.00s]  but we'll discuss off-policy evaluation a little
[666.00s -> 669.00s]  because it forms kind of a common building block.
[670.00s -> 673.00s]  And off-policy evaluation basically refers to this problem.
[673.00s -> 677.00s]  Given a data set D, estimate the return of some policy.
[677.00s -> 679.00s]  Now it could be the behavior policy, that's actually very easy,
[679.00s -> 681.00s]  but there's a little typo on the slide,
[681.00s -> 682.00s]  this should actually be J of pi,
[682.00s -> 685.00s]  so it's estimate the return of some policy that you're given,
[685.00s -> 687.00s]  using a data set D.
[687.00s -> 689.00s]  Estimating J of pi beta is actually pretty easy.
[691.00s -> 695.00s]  But you could use the same data set to estimate the value of some other policy
[695.00s -> 697.00s]  that was not the one that collected the data,
[697.00s -> 700.00s]  and that's a lot harder and that's called off-policy evaluation.
[701.00s -> 703.00s]  Offline reinforcement learning,
[703.00s -> 707.00s]  which is also sometimes called batch RL and sometimes fully off-policy RL,
[707.00s -> 712.00s]  is given the data set D, learn the best possible policy.
[712.00s -> 715.00s]  Now, as we saw in previous lectures,
[715.00s -> 719.00s]  oftentimes learning a policy requires evaluating a policy, right?
[719.00s -> 721.00s]  So in actor-critic methods, for example,
[721.00s -> 724.00s]  we would evaluate our current actor and then we would improve it.
[724.00s -> 728.00s]  So some sort of off-policy evaluation is often in the inner loop
[728.00s -> 730.00s]  of an offline RL method.
[732.00s -> 735.00s]  But I will discuss a little bit later how, in some sense,
[735.00s -> 737.00s]  the general off-policy evaluation problem,
[737.00s -> 739.00s]  the problem of evaluating any policy,
[739.00s -> 742.00s]  can actually be a little bit harder than learning the best possible policy.
[742.00s -> 745.00s]  The intuition for why is that when you're learning the best possible policy,
[745.00s -> 748.00s]  you don't actually have to evaluate some other policies
[748.00s -> 750.00s]  that might never arise in the course of learning.
[752.00s -> 754.00s]  Now, there's a subtlety with this statement,
[754.00s -> 757.00s]  and I intentionally wrote it in kind of a slightly cryptic way,
[757.00s -> 760.00s]  given D, learn the best possible policy pi-theta.
[760.00s -> 763.00s]  That is not necessarily the best possible policy in the MDP.
[763.00s -> 768.00s]  It's the best possible policy that you can learn using your data set.
[768.00s -> 772.00s]  And it's, I think, pretty easy to see why, given certain data sets,
[772.00s -> 776.00s]  the optimal policy may simply never be learnable.
[776.00s -> 779.00s]  So, for example, if you have a very large MDP
[779.00s -> 782.00s]  with some really good states, like maybe Montezuma's Revenge,
[782.00s -> 784.00s]  when you win the game, there's a really high reward,
[784.00s -> 786.00s]  but all of your data is in the first room of Montezuma's Revenge.
[787.00s -> 789.00s]  Well, there's no way you could possibly use that data
[789.00s -> 791.00s]  to learn the true optimal policy,
[791.00s -> 795.00s]  because large parts of your MDP are just never seen in the data.
[795.00s -> 798.00s]  So we have to be a little bit careful with what best possible policy means,
[798.00s -> 802.00s]  and the formal statement is not here on the slide,
[802.00s -> 803.00s]  it's actually pretty complicated,
[803.00s -> 806.00s]  but the informal intuition is you want to learn the best policy
[806.00s -> 809.00s]  that is supported by your data,
[809.00s -> 813.00s]  the best possible policy for which the evidence that it is good
[813.00s -> 815.00s]  is present in your data set.
[817.00s -> 822.00s]  Okay, so now one question that some of you might ask is,
[822.00s -> 824.00s]  how is this even possible?
[824.00s -> 827.00s]  Like, can we even hope to learn good policies
[827.00s -> 832.00s]  by using only data sets without ever interacting with the world?
[832.00s -> 834.00s]  Some of you might be wondering the opposite,
[834.00s -> 836.00s]  it's like, why is this difficult?
[836.00s -> 839.00s]  And actually both of those questions are a little bit non-trivial.
[839.00s -> 842.00s]  So there are a few different perspectives
[842.00s -> 843.00s]  on what we should expect
[843.00s -> 846.00s]  offline reinforcement learning algorithms to do.
[846.00s -> 850.00s]  One perspective is we should expect them to find the good stuff
[850.00s -> 853.00s]  in a data set full of good and bad behaviors.
[853.00s -> 854.00s]  And they can definitely do that,
[854.00s -> 856.00s]  although that is a fairly naive view
[856.00s -> 860.00s]  of what good offline RL methods should do.
[860.00s -> 863.00s]  So by find the good stuff, what I mean is,
[863.00s -> 867.00s]  if I give you a data set of human drivers driving a vehicle,
[867.00s -> 868.00s]  and some of them are good,
[868.00s -> 869.00s]  and some of them are mediocre,
[869.00s -> 870.00s]  and some of them are bad,
[870.00s -> 872.00s]  if you were to run imitation learning on that,
[872.00s -> 875.00s]  you would kind of get the average human driver.
[875.00s -> 877.00s]  If your imitation learning algorithm succeeds.
[877.00s -> 879.00s]  If your imitation algorithm fails,
[879.00s -> 881.00s]  you might just get this accumulating errors
[881.00s -> 883.00s]  that we talked about in the beginning of the course,
[883.00s -> 885.00s]  but assuming that doesn't happen,
[885.00s -> 888.00s]  what imitation learning is trying to do
[888.00s -> 893.00s]  is give you the average driver in the data set.
[893.00s -> 896.00s]  But if you run offline RL, in principle,
[896.00s -> 899.00s]  it should get you the behavior of the best driver,
[899.00s -> 901.00s]  which should be better.
[901.00s -> 903.00s]  But this of course begs the question,
[903.00s -> 906.00s]  is offline RL always going to be only as good
[906.00s -> 908.00s]  as the best behavior in the data set?
[908.00s -> 910.00s]  And the answer is in general, no.
[910.00s -> 912.00s]  In general, it is possible to do better
[912.00s -> 914.00s]  than the best behavior in the data set.
[914.00s -> 916.00s]  And this might be a little non obvious,
[916.00s -> 919.00s]  but hopefully the intuition that will come next
[919.00s -> 922.00s]  will convince you of this.
[922.00s -> 925.00s]  So the basic idea is that
[925.00s -> 926.00s]  you're going to have generalization,
[926.00s -> 929.00s]  which means that good behavior in one place
[929.00s -> 931.00s]  may suggest good behavior in another place.
[931.00s -> 935.00s]  So if, for example, you have some human drivers
[935.00s -> 937.00s]  and one of them really knows
[937.00s -> 940.00s]  how to do a really fast merge onto a highway
[940.00s -> 942.00s]  and another one really knows
[942.00s -> 947.00s]  how to transit through an intersection very efficiently,
[947.00s -> 949.00s]  but neither of them is good at both,
[949.00s -> 950.00s]  well, you can sort of just sell out
[950.00s -> 951.00s]  the best parts of each of them,
[951.00s -> 954.00s]  and you can even generalize to a degree
[954.00s -> 958.00s]  to situations where you haven't seen them acting.
[958.00s -> 960.00s]  You can imagine such algorithms
[961.00s -> 964.00s]  as combining good behaviors in different places
[964.00s -> 965.00s]  to attain behaviors
[965.00s -> 967.00s]  that were never actually seen in the data set.
[967.00s -> 969.00s]  So one really simple example of this
[969.00s -> 971.00s]  is the stitching example.
[971.00s -> 972.00s]  So if in your data set,
[972.00s -> 976.00s]  you've seen some transitions that go from A to B,
[976.00s -> 979.00s]  and you've seen other transitions that go from B to C,
[979.00s -> 980.00s]  then you can figure out
[980.00s -> 982.00s]  that it's possible to go from A to C.
[982.00s -> 986.00s]  And figuring this out requires reinforcement learning.
[986.00s -> 989.00s]  But it actually goes a little bit further than that.
[989.00s -> 991.00s]  So of course, you could take the stitching intuition
[991.00s -> 993.00s]  and you could scale it up massively.
[993.00s -> 996.00s]  You could imagine that you have a maze navigation task
[996.00s -> 997.00s]  where you see transitions
[997.00s -> 1001.00s]  between different pairs of starting goal states in a maze,
[1001.00s -> 1003.00s]  and from that, you can figure out
[1003.00s -> 1005.00s]  how to get between any two points in the maze,
[1005.00s -> 1006.00s]  even pairs of points
[1006.00s -> 1009.00s]  between which you've never seen a transition.
[1009.00s -> 1011.00s]  And you can, of course, stitch these together
[1011.00s -> 1013.00s]  to get transitions through the maze
[1013.00s -> 1015.00s]  that are much, much longer
[1015.00s -> 1019.00s]  than any of the ones that were seen in the data set.
[1019.00s -> 1023.00s]  All right, let's get into this a little bit more.
[1023.00s -> 1026.00s]  So the kind of the bad intuition,
[1026.00s -> 1027.00s]  in my opinion, the bad intuition
[1027.00s -> 1029.00s]  for what offline RL methods do
[1029.00s -> 1030.00s]  is that they do something
[1030.00s -> 1032.00s]  that is a bit like imitation learning.
[1032.00s -> 1033.00s]  So you have some data,
[1033.00s -> 1035.00s]  it goes between a start and a goal,
[1035.00s -> 1039.00s]  and you're going to basically imitate that data.
[1039.00s -> 1041.00s]  It can be shown that offline RL
[1041.00s -> 1042.00s]  is theoretically actually better
[1042.00s -> 1044.00s]  than imitation learning even in this setting,
[1044.00s -> 1047.00s]  but this is not really the main point.
[1047.00s -> 1049.00s]  A better intuition is that an offline RL method
[1049.00s -> 1051.00s]  can kind of get order from chaos.
[1051.00s -> 1052.00s]  So let's say that your data set
[1052.00s -> 1054.00s]  consists of these trajectories.
[1054.00s -> 1056.00s]  Now, clearly, they are very suboptimal,
[1056.00s -> 1058.00s]  and most of them don't go anywhere near the goal,
[1058.00s -> 1060.00s]  but if you take kind of the best bits of both
[1060.00s -> 1062.00s]  with a little bit of generalization thrown in,
[1062.00s -> 1064.00s]  you can get a much better behavior
[1064.00s -> 1068.00s]  than any of the behavior you saw in the data set.
[1068.00s -> 1070.00s]  And the stitching that we discussed on the previous slide
[1070.00s -> 1072.00s]  is, of course, one instance of this,
[1072.00s -> 1075.00s]  but it's merely the clearest example of a more general trend.
[1075.00s -> 1078.00s]  So you could imagine sort of microscopic level stitching
[1078.00s -> 1079.00s]  where you have many trajectories
[1079.00s -> 1083.00s]  that are all kind of a little bit suboptimal in every place,
[1083.00s -> 1084.00s]  but if you have enough of them
[1084.00s -> 1086.00s]  and you recombine the best parts of all of them,
[1086.00s -> 1088.00s]  you can get a highly optimal behavior.
[1088.00s -> 1091.00s]  So you could take a bunch of human behaviors for driving,
[1091.00s -> 1093.00s]  for instance, that are each suboptimal
[1093.00s -> 1094.00s]  in slightly different ways
[1094.00s -> 1096.00s]  and get a superhuman behavior
[1096.00s -> 1099.00s]  that is essentially the best of the best human
[1099.00s -> 1101.00s]  in every single situation.
[1102.00s -> 1103.00s]  So if we have algorithms
[1103.00s -> 1105.00s]  that can probably perform dynamic programming,
[1105.00s -> 1107.00s]  we can take this idea much further
[1107.00s -> 1109.00s]  and really get near-optimal policies
[1109.00s -> 1112.00s]  from highly suboptimal data.
[1112.00s -> 1114.00s]  Okay, here's a vivid example
[1114.00s -> 1115.00s]  or what I think is a vivid example
[1115.00s -> 1117.00s]  from some past work in my group
[1117.00s -> 1119.00s]  that I think kind of shows this point
[1119.00s -> 1122.00s]  a little bit more concretely.
[1122.00s -> 1124.00s]  Let's say that you have a robotic arm
[1124.00s -> 1127.00s]  and that robotic arm is supposed to pick up an object
[1127.00s -> 1129.00s]  from an open drawer.
[1129.00s -> 1131.00s]  So the training data for it,
[1131.00s -> 1132.00s]  the experience that it has
[1132.00s -> 1134.00s]  is picking up the object from an open drawer.
[1134.00s -> 1136.00s]  And now maybe at test time,
[1136.00s -> 1138.00s]  the drawer is closed, right?
[1138.00s -> 1142.00s]  So generally, if your RL policy
[1142.00s -> 1143.00s]  is trained with an open drawer,
[1143.00s -> 1145.00s]  it's not gonna have any way
[1145.00s -> 1146.00s]  to know what to do
[1146.00s -> 1148.00s]  when the drawer is closed at test time.
[1149.00s -> 1154.00s]  So what if in addition to the drawer grasping,
[1154.00s -> 1156.00s]  you also have a bunch of experience
[1156.00s -> 1158.00s]  of picking up and moving objects,
[1158.00s -> 1160.00s]  opening drawers, closing drawers,
[1160.00s -> 1161.00s]  basically all sorts of other skills
[1161.00s -> 1163.00s]  that you could do in this environment,
[1163.00s -> 1164.00s]  but they're all separate
[1164.00s -> 1166.00s]  from the skill of picking up the object
[1166.00s -> 1167.00s]  from the open drawer.
[1167.00s -> 1168.00s]  So all this prior data
[1168.00s -> 1169.00s]  contains a variety of skills,
[1169.00s -> 1171.00s]  but none of it contains
[1171.00s -> 1173.00s]  the picking up from the open drawer.
[1173.00s -> 1175.00s]  Well, if you just take all this prior data
[1175.00s -> 1177.00s]  and you put it into the same data set
[1177.00s -> 1179.00s]  as the object grasping skill
[1179.00s -> 1181.00s]  with the open drawer,
[1181.00s -> 1183.00s]  then this same stitching property
[1183.00s -> 1184.00s]  that I discussed before
[1184.00s -> 1187.00s]  will actually allow you to acquire a policy
[1187.00s -> 1188.00s]  that can pick up the object
[1188.00s -> 1190.00s]  in different situations.
[1190.00s -> 1191.00s]  So of course, you can pick up the object
[1191.00s -> 1193.00s]  when the drawer is open,
[1193.00s -> 1194.00s]  but you could also handle a situation
[1194.00s -> 1196.00s]  where the drawer starts out closed
[1196.00s -> 1198.00s]  because you've seen prior data
[1198.00s -> 1199.00s]  that involves opening drawers
[1199.00s -> 1201.00s]  and via dynamic programming,
[1201.00s -> 1202.00s]  via techniques like Q-learning,
[1202.00s -> 1204.00s]  you can actually connect it up
[1204.00s -> 1206.00s]  to the skill of picking up the object
[1206.00s -> 1207.00s]  from the open drawer.
[1207.00s -> 1208.00s]  So in this case,
[1208.00s -> 1210.00s]  none of the prior data
[1210.00s -> 1212.00s]  has that full sequence.
[1212.00s -> 1214.00s]  None of it has the robot opening the drawer
[1214.00s -> 1215.00s]  and picking up the ball,
[1215.00s -> 1216.00s]  but it has some opening
[1216.00s -> 1217.00s]  and it has some picking
[1217.00s -> 1218.00s]  and the RL algorithm can figure out
[1218.00s -> 1219.00s]  how to put them together.
[1219.00s -> 1222.00s]  Another example is close the top drawer,
[1222.00s -> 1223.00s]  open the bottom drawer
[1223.00s -> 1225.00s]  and then pick up the ball.
[1225.00s -> 1226.00s]  So there it goes.
[1226.00s -> 1227.00s]  Close the top, open the bottom,
[1227.00s -> 1228.00s]  pick up the ball.
[1228.00s -> 1231.00s]  Another example is pick up the object
[1231.00s -> 1232.00s]  and move it out of the way,
[1232.00s -> 1234.00s]  then open the drawer,
[1234.00s -> 1235.00s]  then pick up the ball.
[1235.00s -> 1236.00s]  And in all these cases,
[1236.00s -> 1240.00s]  it's combining multiple trials in new ways
[1240.00s -> 1242.00s]  by using dynamic programming,
[1242.00s -> 1245.00s]  by using Q-learning.
[1245.00s -> 1248.00s]  All right, so why should we care about this?
[1248.00s -> 1250.00s]  Well, if we can get this kind of offline
[1250.00s -> 1251.00s]  RL recipe to work,
[1251.00s -> 1252.00s]  we don't have to collect
[1252.00s -> 1253.00s]  ImageNet-sized datasets
[1253.00s -> 1255.00s]  every time we wanna train a policy.
[1255.00s -> 1257.00s]  We can apply RL to domains
[1257.00s -> 1260.00s]  that don't conventionally look like
[1260.00s -> 1261.00s]  reinforcement learning problems,
[1261.00s -> 1264.00s]  ones where exploration and trial and error
[1264.00s -> 1266.00s]  would be prohibitively expensive or dangerous.
[1266.00s -> 1268.00s]  And we can hopefully move towards
[1268.00s -> 1271.00s]  a much greater level of generalization.
[1271.00s -> 1274.00s]  So now that I've kind of discussed
[1274.00s -> 1277.00s]  the why offline RL is good,
[1277.00s -> 1279.00s]  let me tell you some of the bad news.
[1279.00s -> 1281.00s]  Let me tell you what goes wrong
[1281.00s -> 1283.00s]  if we try to do offline RL naively
[1283.00s -> 1285.00s]  using the kinds of techniques
[1285.00s -> 1286.00s]  that we've learned about before.
[1286.00s -> 1289.00s]  So back in 2018,
[1289.00s -> 1293.00s]  when I was working on robotic grasping at Google,
[1293.00s -> 1296.00s]  we had a little experiment that we did,
[1296.00s -> 1297.00s]  or actually a very large experiment,
[1297.00s -> 1299.00s]  where we tried to implement an effective
[1299.00s -> 1301.00s]  offline reinforcement learning algorithm
[1301.00s -> 1303.00s]  with online fine-tuning.
[1303.00s -> 1306.00s]  And scale it up to see if we can get policies
[1306.00s -> 1307.00s]  that generalize.
[1307.00s -> 1309.00s]  Our goal was to get policies
[1309.00s -> 1310.00s]  that generalize as effectively
[1310.00s -> 1313.00s]  as these supervised models train
[1313.00s -> 1314.00s]  on things like ImageNet.
[1314.00s -> 1316.00s]  So we collected a very large dataset
[1316.00s -> 1317.00s]  of robotic grasping
[1317.00s -> 1319.00s]  with a combination of scripted policies
[1319.00s -> 1320.00s]  as well as learned policies.
[1320.00s -> 1323.00s]  And we had a kind of a hybrid system
[1323.00s -> 1325.00s]  that would use all prior stored data
[1325.00s -> 1326.00s]  as essentially offline data.
[1326.00s -> 1328.00s]  And then it would also do some online collection
[1328.00s -> 1330.00s]  and it was massively parallel and massively scaled up.
[1331.00s -> 1332.00s]  But at its core,
[1332.00s -> 1334.00s]  this system implemented an algorithm
[1334.00s -> 1336.00s]  that was very much like the Q-learning approach
[1336.00s -> 1338.00s]  that we talked about before.
[1338.00s -> 1339.00s]  It's for continuous action,
[1339.00s -> 1341.00s]  so it used the optimization-based
[1341.00s -> 1343.00s]  target value calculation.
[1343.00s -> 1344.00s]  But otherwise it was basically
[1344.00s -> 1346.00s]  a classic deep Q-learning method.
[1348.00s -> 1349.00s]  Now this worked very well
[1349.00s -> 1351.00s]  and it did in fact generalize.
[1352.00s -> 1353.00s]  But what I want to tell you about
[1353.00s -> 1355.00s]  in this lecture,
[1355.00s -> 1356.00s]  other than using this as an opportunity
[1356.00s -> 1358.00s]  to show you some cool robot videos,
[1358.00s -> 1362.00s]  is to compare the performance of this system
[1362.00s -> 1364.00s]  when it was used in offline mode
[1364.00s -> 1365.00s]  versus when it was fine-tuned
[1365.00s -> 1367.00s]  with some additional online data collection.
[1368.00s -> 1370.00s]  And this will hopefully help illustrate
[1370.00s -> 1371.00s]  some of the challenges
[1371.00s -> 1373.00s]  with offline reinforcement learning.
[1373.00s -> 1374.00s]  So offline mode here
[1374.00s -> 1377.00s]  actually used pretty good offline data.
[1377.00s -> 1378.00s]  So it was data from past RL runs
[1378.00s -> 1380.00s]  that was just reloaded
[1380.00s -> 1382.00s]  and then used to train up better policies.
[1382.00s -> 1383.00s]  Whereas with fine-tuning,
[1383.00s -> 1386.00s]  we had some additional online data collection.
[1386.00s -> 1388.00s]  The offline data set consisted of
[1388.00s -> 1391.00s]  580,000 trials, grasp attempts.
[1391.00s -> 1393.00s]  These were collected over several months.
[1393.00s -> 1396.00s]  The fine-tuned version further augmented it
[1396.00s -> 1398.00s]  with about 28,000 trials
[1398.00s -> 1400.00s]  of additional online fine-tuning.
[1400.00s -> 1401.00s]  So it's a lot of trials,
[1401.00s -> 1404.00s]  but the amount of additional online data
[1404.00s -> 1405.00s]  is comparatively very small.
[1405.00s -> 1407.00s]  So probably whatever difference you see
[1407.00s -> 1408.00s]  is not due to the difference
[1408.00s -> 1409.00s]  in the data set size.
[1409.00s -> 1411.00s]  It's due to something about the difference
[1411.00s -> 1413.00s]  between offline versus online training.
[1414.00s -> 1415.00s]  And by the way,
[1415.00s -> 1417.00s]  the online training here is still off policy.
[1417.00s -> 1418.00s]  So it's still using replay buffers.
[1418.00s -> 1421.00s]  It's still recycling all of that offline data.
[1421.00s -> 1422.00s]  It's just collecting additional data
[1422.00s -> 1423.00s]  with the latest policy.
[1424.00s -> 1425.00s]  So the success rate
[1425.00s -> 1427.00s]  for the offline method was 87%,
[1427.00s -> 1429.00s]  which means that 87% of the time,
[1429.00s -> 1430.00s]  you could successfully grasp
[1430.00s -> 1431.00s]  the objects in front of it.
[1432.00s -> 1433.00s]  Whereas the fine-tuned method
[1433.00s -> 1435.00s]  had a success rate of 96%.
[1436.00s -> 1437.00s]  Now you might say,
[1437.00s -> 1438.00s]  well, these are both pretty good,
[1438.00s -> 1440.00s]  but if you look at the failure rate,
[1440.00s -> 1441.00s]  it becomes a little more obvious
[1441.00s -> 1442.00s]  that the failure rate
[1442.00s -> 1443.00s]  for the fully offline method
[1443.00s -> 1445.00s]  is more than three times larger
[1445.00s -> 1446.00s]  than the fine-tuned method.
[1447.00s -> 1450.00s]  So more than threefold increase
[1450.00s -> 1451.00s]  in the failure rate
[1451.00s -> 1452.00s]  just from omitting
[1452.00s -> 1454.00s]  those 28,000 online transitions.
[1454.00s -> 1456.00s]  So something here is going on.
[1456.00s -> 1457.00s]  The system worked pretty well.
[1457.00s -> 1458.00s]  We were pretty happy with it,
[1458.00s -> 1460.00s]  but it seems a little frustrating
[1460.00s -> 1462.00s]  that just taking that data
[1462.00s -> 1464.00s]  and using it as an offline data set
[1464.00s -> 1466.00s]  doesn't yield the same kind of success rate
[1466.00s -> 1467.00s]  as if you then did
[1467.00s -> 1469.00s]  a little bit of online fine-tuning.
[1470.00s -> 1472.00s]  So to get a little bit of intuition
[1472.00s -> 1474.00s]  for what might be going on here,
[1474.00s -> 1478.00s]  here's a little experiment,
[1478.00s -> 1480.00s]  a very controlled experiment
[1480.00s -> 1482.00s]  that tries to get at this point.
[1482.00s -> 1484.00s]  So this was an experiment
[1484.00s -> 1485.00s]  that was conducted
[1485.00s -> 1487.00s]  using the Half-Cheetah benchmark task,
[1487.00s -> 1488.00s]  which hopefully all of you
[1488.00s -> 1489.00s]  are familiar with
[1489.00s -> 1490.00s]  from your homework.
[1490.00s -> 1492.00s]  And in this case,
[1492.00s -> 1493.00s]  data was collected
[1493.00s -> 1496.00s]  using basically from the replay buffer
[1497.00s -> 1499.00s]  of an RL training run.
[1499.00s -> 1500.00s]  This was a training run
[1500.00s -> 1502.00s]  with soft actor-critic with SAC.
[1502.00s -> 1503.00s]  And that training run
[1503.00s -> 1504.00s]  was allowed to go up
[1504.00s -> 1505.00s]  until the policy reached
[1505.00s -> 1507.00s]  some kind of mediocre performance.
[1507.00s -> 1508.00s]  I think the reward was several thousand,
[1508.00s -> 1510.00s]  maybe around five to six thousand.
[1510.00s -> 1511.00s]  And then that entire replay buffer
[1511.00s -> 1513.00s]  was used as an offline data set
[1513.00s -> 1515.00s]  to train a policy.
[1517.00s -> 1518.00s]  And it was trained
[1518.00s -> 1520.00s]  with basically a classic actor-critic algorithm
[1520.00s -> 1521.00s]  with Q functions,
[1521.00s -> 1522.00s]  much like the one
[1522.00s -> 1523.00s]  that we learned about
[1523.00s -> 1524.00s]  a few weeks ago.
[1526.00s -> 1528.00s]  And the different curves here
[1528.00s -> 1530.00s]  show different data set sizes,
[1530.00s -> 1531.00s]  ranging from one thousand
[1531.00s -> 1533.00s]  to one million transitions.
[1533.00s -> 1534.00s]  And you can see that
[1534.00s -> 1535.00s]  all of them do very badly.
[1535.00s -> 1536.00s]  So the data set average here
[1536.00s -> 1537.00s]  is several thousand.
[1537.00s -> 1538.00s]  So if you were to just imitate
[1538.00s -> 1539.00s]  the data set,
[1539.00s -> 1540.00s]  you would get a reward
[1540.00s -> 1541.00s]  of several thousand.
[1541.00s -> 1542.00s]  If you just run
[1542.00s -> 1543.00s]  off-policy actor-critic on it,
[1543.00s -> 1544.00s]  the rewards are generally
[1544.00s -> 1545.00s]  in the range of
[1545.00s -> 1547.00s]  negative 750 to negative 250.
[1547.00s -> 1549.00s]  And they're not getting better.
[1549.00s -> 1550.00s]  In fact, they actually get worse
[1550.00s -> 1552.00s]  right from the start.
[1552.00s -> 1553.00s]  The initial policy performance
[1553.00s -> 1555.00s]  is at zero and then it gets worse.
[1555.00s -> 1556.00s]  So something really terrible
[1556.00s -> 1558.00s]  is going on here.
[1558.00s -> 1559.00s]  Interestingly enough,
[1559.00s -> 1562.00s]  if we look at the actual Q values
[1562.00s -> 1563.00s]  that are learned
[1563.00s -> 1564.00s]  by this actor-critic algorithm
[1564.00s -> 1566.00s]  for each data set size,
[1566.00s -> 1567.00s]  the y-axis here
[1567.00s -> 1569.00s]  is a log scale.
[1569.00s -> 1570.00s]  So the estimated Q values
[1570.00s -> 1572.00s]  are extremely large.
[1572.00s -> 1574.00s]  For example, for that red curve,
[1574.00s -> 1575.00s]  the one that has
[1575.00s -> 1576.00s]  a million data points,
[1576.00s -> 1577.00s]  it's estimated it's going
[1577.00s -> 1578.00s]  to get a reward
[1578.00s -> 1579.00s]  of ten to the seventh power,
[1579.00s -> 1580.00s]  and it actually gets
[1580.00s -> 1582.00s]  minus 250.
[1582.00s -> 1584.00s]  So that's really weird.
[1584.00s -> 1585.00s]  Our Q function thinks
[1585.00s -> 1587.00s]  it's going to get huge values,
[1587.00s -> 1590.00s]  but then if we actually train
[1590.00s -> 1591.00s]  the corresponding actor
[1591.00s -> 1592.00s]  and run the actor,
[1592.00s -> 1594.00s]  we get really horrible values.
[1594.00s -> 1595.00s]  So that's strange,
[1595.00s -> 1599.00s]  and it's not an accident.
[1599.00s -> 1600.00s]  The fundamental problem
[1600.00s -> 1601.00s]  in offline RL is really,
[1601.00s -> 1602.00s]  you can think of it
[1602.00s -> 1603.00s]  as a problem
[1603.00s -> 1605.00s]  of counterfactual queries.
[1605.00s -> 1606.00s]  Let's say that we have
[1606.00s -> 1608.00s]  our autonomous driving example
[1608.00s -> 1609.00s]  from before,
[1609.00s -> 1610.00s]  and our training data
[1610.00s -> 1612.00s]  has a human driver
[1612.00s -> 1614.00s]  driving the car decently well.
[1614.00s -> 1615.00s]  The human driver
[1615.00s -> 1616.00s]  is not optimal.
[1616.00s -> 1617.00s]  Remember, we're not assuming
[1617.00s -> 1618.00s]  that they're an expert,
[1618.00s -> 1619.00s]  but there are certain things
[1619.00s -> 1620.00s]  that human drivers
[1620.00s -> 1621.00s]  just don't do.
[1621.00s -> 1622.00s]  So they're not going to
[1622.00s -> 1623.00s]  randomly swerve off the road
[1623.00s -> 1624.00s]  in the middle of nowhere.
[1624.00s -> 1625.00s]  They might do some things
[1625.00s -> 1626.00s]  that are unsafe.
[1626.00s -> 1627.00s]  They might occasionally
[1627.00s -> 1628.00s]  go into the opposing lane
[1628.00s -> 1629.00s]  or something,
[1629.00s -> 1630.00s]  or they might occasionally
[1630.00s -> 1632.00s]  make a turn on a red light,
[1632.00s -> 1633.00s]  but they're not going to
[1633.00s -> 1634.00s]  do really crazy stuff.
[1634.00s -> 1636.00s]  So their coverage
[1636.00s -> 1637.00s]  of possible behaviors
[1637.00s -> 1638.00s]  is incomplete.
[1638.00s -> 1639.00s]  There are some things
[1639.00s -> 1641.00s]  that they just don't do.
[1641.00s -> 1642.00s]  During training,
[1642.00s -> 1644.00s]  when we're updating the policy,
[1644.00s -> 1645.00s]  what we're really doing
[1645.00s -> 1647.00s]  is we're asking,
[1647.00s -> 1648.00s]  for all of the actions
[1648.00s -> 1649.00s]  available to us
[1649.00s -> 1650.00s]  in this state
[1650.00s -> 1651.00s]  that we've seen before,
[1651.00s -> 1653.00s]  which action is best?
[1653.00s -> 1655.00s]  And that implicitly requires
[1655.00s -> 1656.00s]  comparing the action
[1656.00s -> 1657.00s]  that was taken in the data set
[1657.00s -> 1658.00s]  to the other actions
[1658.00s -> 1660.00s]  that were available.
[1660.00s -> 1661.00s]  So it requires asking,
[1661.00s -> 1662.00s]  is this other action
[1662.00s -> 1663.00s]  that it didn't take,
[1663.00s -> 1665.00s]  is it good or is it bad?
[1665.00s -> 1666.00s]  How can we know that
[1666.00s -> 1667.00s]  if we didn't see it
[1667.00s -> 1668.00s]  in the data?
[1668.00s -> 1669.00s]  Well, for some actions
[1669.00s -> 1670.00s]  we might know
[1670.00s -> 1671.00s]  if they're good or bad
[1671.00s -> 1672.00s]  because of generalization.
[1672.00s -> 1673.00s]  So we might have seen
[1673.00s -> 1674.00s]  similar actions
[1674.00s -> 1676.00s]  in other similar states.
[1676.00s -> 1677.00s]  But because our data set
[1677.00s -> 1678.00s]  doesn't have perfect coverage,
[1678.00s -> 1679.00s]  there are certain behaviors
[1679.00s -> 1680.00s]  that are unlike anything
[1680.00s -> 1681.00s]  that we've seen
[1681.00s -> 1684.00s]  but that are available to us.
[1684.00s -> 1685.00s]  Online RL algorithms
[1685.00s -> 1686.00s]  don't have to worry
[1686.00s -> 1687.00s]  about this
[1687.00s -> 1688.00s]  because they can simply
[1688.00s -> 1689.00s]  try that action
[1689.00s -> 1690.00s]  and see what happens.
[1690.00s -> 1692.00s]  So a regular online algorithm,
[1692.00s -> 1693.00s]  even if it's
[1693.00s -> 1694.00s]  an off-policy algorithm,
[1694.00s -> 1695.00s]  it might decide erroneously
[1695.00s -> 1696.00s]  that the action
[1696.00s -> 1697.00s]  of swerving off the road
[1697.00s -> 1698.00s]  is a good one
[1698.00s -> 1699.00s]  and it might go and try it
[1699.00s -> 1700.00s]  and then it might learn
[1700.00s -> 1701.00s]  that it's bad
[1701.00s -> 1702.00s]  and not do it again.
[1702.00s -> 1703.00s]  That's actually exactly
[1703.00s -> 1704.00s]  why we don't want
[1704.00s -> 1705.00s]  to deploy these algorithms
[1705.00s -> 1706.00s]  in the real world
[1706.00s -> 1708.00s]  in safety-critical settings.
[1708.00s -> 1709.00s]  But offline RL algorithms
[1709.00s -> 1710.00s]  have to somehow account
[1710.00s -> 1712.00s]  for these unseen actions,
[1712.00s -> 1713.00s]  ideally in a safe way.
[1713.00s -> 1714.00s]  So they need
[1714.00s -> 1715.00s]  to basically understand
[1715.00s -> 1717.00s]  that just because you didn't
[1717.00s -> 1718.00s]  see that it's bad
[1718.00s -> 1719.00s]  doesn't mean
[1719.00s -> 1720.00s]  that it could be good.
[1720.00s -> 1721.00s]  They need to somehow figure out
[1721.00s -> 1722.00s]  that actions that are
[1722.00s -> 1723.00s]  completely unfamiliar
[1723.00s -> 1724.00s]  should not be assigned
[1724.00s -> 1725.00s]  high values
[1725.00s -> 1726.00s]  or should not be attempted
[1726.00s -> 1727.00s]  because there is not
[1727.00s -> 1728.00s]  enough confidence
[1728.00s -> 1729.00s]  for it.
[1729.00s -> 1730.00s]  We refer to these actions
[1730.00s -> 1731.00s]  as out-of-distribution actions
[1731.00s -> 1732.00s]  in the sense that
[1732.00s -> 1733.00s]  pi-beta induces
[1733.00s -> 1735.00s]  some distribution over actions
[1735.00s -> 1736.00s]  and some actions
[1736.00s -> 1737.00s]  are inside of that distribution
[1737.00s -> 1739.00s]  and some are outside of it.
[1739.00s -> 1740.00s]  Now it's important
[1740.00s -> 1741.00s]  not to confuse
[1741.00s -> 1742.00s]  out-of-distribution actions
[1742.00s -> 1744.00s]  with out-of-sample actions.
[1744.00s -> 1745.00s]  Your Q function
[1745.00s -> 1747.00s]  could be quite capable
[1747.00s -> 1748.00s]  of accurately estimating
[1748.00s -> 1749.00s]  the value of actions
[1749.00s -> 1751.00s]  that it never saw before
[1751.00s -> 1752.00s]  if those actions
[1752.00s -> 1755.00s]  come from the same distribution.
[1755.00s -> 1756.00s]  And this is actually
[1756.00s -> 1757.00s]  a really important point
[1757.00s -> 1758.00s]  because if you didn't do this,
[1758.00s -> 1759.00s]  then you wouldn't
[1759.00s -> 1760.00s]  get generalization
[1760.00s -> 1761.00s]  and you wouldn't be able
[1761.00s -> 1762.00s]  to build effective algorithms.
[1762.00s -> 1763.00s]  This is actually a mistake
[1763.00s -> 1764.00s]  that some early research
[1764.00s -> 1765.00s]  in offline RL made
[1765.00s -> 1766.00s]  where they didn't account
[1766.00s -> 1767.00s]  for the difference
[1767.00s -> 1768.00s]  between out-of-sample
[1768.00s -> 1769.00s]  and out-of-distribution,
[1769.00s -> 1770.00s]  but it's a really
[1770.00s -> 1771.00s]  important distinction.
[1771.00s -> 1772.00s]  So it's really
[1772.00s -> 1773.00s]  out-of-distribution
[1773.00s -> 1774.00s]  that we're worried about.
[1774.00s -> 1775.00s]  And at the same time,
[1775.00s -> 1776.00s]  you still have to make use
[1776.00s -> 1777.00s]  of generalization
[1777.00s -> 1778.00s]  to come up with behaviors
[1778.00s -> 1779.00s]  that are better
[1779.00s -> 1780.00s]  than the best thing
[1780.00s -> 1781.00s]  that you saw in the data.
[1781.00s -> 1782.00s]  So this is a subtle
[1782.00s -> 1783.00s]  and delicate
[1783.00s -> 1784.00s]  trade-off to strike.
[1784.00s -> 1785.00s]  You have to be better
[1785.00s -> 1786.00s]  than the best thing
[1786.00s -> 1787.00s]  that you have to generalize,
[1787.00s -> 1788.00s]  but at the same time,
[1788.00s -> 1789.00s]  you have to avoid
[1789.00s -> 1790.00s]  assuming that an
[1790.00s -> 1791.00s]  out-of-distribution action
[1791.00s -> 1792.00s]  could be good
[1792.00s -> 1793.00s]  when in fact,
[1793.00s -> 1794.00s]  you simply do not
[1794.00s -> 1795.00s]  have enough evidence
[1795.00s -> 1797.00s]  to determine its value.
[1797.00s -> 1798.00s]  Okay, so that's
[1798.00s -> 1799.00s]  kind of the intuition,
[1799.00s -> 1800.00s]  but let's talk about
[1800.00s -> 1801.00s]  some of the math
[1801.00s -> 1802.00s]  behind this problem.
[1802.00s -> 1803.00s]  So this problem
[1803.00s -> 1805.00s]  is often referred to
[1805.00s -> 1806.00s]  in statistics as
[1806.00s -> 1807.00s]  distribution shift.
[1807.00s -> 1808.00s]  Distribution shift means
[1808.00s -> 1809.00s]  that you have
[1809.00s -> 1810.00s]  one particular distribution
[1810.00s -> 1811.00s]  you use in training
[1811.00s -> 1812.00s]  and then you need
[1812.00s -> 1813.00s]  to perform well
[1813.00s -> 1814.00s]  under a different distribution
[1814.00s -> 1815.00s]  and because those
[1815.00s -> 1816.00s]  distributions don't match,
[1816.00s -> 1817.00s]  you end up performing
[1817.00s -> 1818.00s]  very poorly.
[1818.00s -> 1822.00s]  So, whenever we train
[1822.00s -> 1823.00s]  something with
[1823.00s -> 1824.00s]  supervised learning,
[1824.00s -> 1826.00s]  we typically use
[1826.00s -> 1827.00s]  a training set
[1827.00s -> 1828.00s]  and the training set
[1828.00s -> 1829.00s]  comes from some
[1829.00s -> 1830.00s]  training distribution.
[1830.00s -> 1831.00s]  So let's say that
[1831.00s -> 1832.00s]  you are learning
[1832.00s -> 1834.00s]  a function f theta of x
[1834.00s -> 1835.00s]  and you're regressing
[1835.00s -> 1837.00s]  onto ground truth values y
[1837.00s -> 1838.00s]  and your x's come
[1838.00s -> 1840.00s]  from a distribution p of x
[1840.00s -> 1841.00s]  and your y's come
[1841.00s -> 1842.00s]  from a ground truth
[1842.00s -> 1843.00s]  distribution p of y
[1843.00s -> 1844.00s]  given x.
[1844.00s -> 1845.00s]  So you would sample
[1845.00s -> 1846.00s]  a training set
[1846.00s -> 1847.00s]  from your training
[1847.00s -> 1849.00s]  distribution and minimize
[1849.00s -> 1850.00s]  the error on that.
[1850.00s -> 1852.00s]  And this is what's referred to
[1852.00s -> 1853.00s]  as empirical risk
[1853.00s -> 1854.00s]  minimization.
[1854.00s -> 1856.00s]  So your sampled training set
[1856.00s -> 1857.00s]  is giving you
[1857.00s -> 1858.00s]  an empirical estimate
[1858.00s -> 1859.00s]  of the risk
[1859.00s -> 1860.00s]  and that's what
[1860.00s -> 1861.00s]  you're minimizing
[1861.00s -> 1862.00s]  and as long as you
[1862.00s -> 1863.00s]  don't overfit,
[1863.00s -> 1864.00s]  minimizing the empirical risk
[1864.00s -> 1865.00s]  will also minimize
[1865.00s -> 1866.00s]  the actual risk,
[1866.00s -> 1867.00s]  which is what
[1867.00s -> 1868.00s]  I have written down here.
[1868.00s -> 1869.00s]  So this equation is actually
[1869.00s -> 1870.00s]  not the empirical risk.
[1870.00s -> 1871.00s]  This equation is the actual,
[1871.00s -> 1873.00s]  the real risk
[1873.00s -> 1875.00s]  under the full distribution.
[1875.00s -> 1876.00s]  But a training set
[1876.00s -> 1877.00s]  approximates this
[1877.00s -> 1880.00s]  with samples from p of x.
[1880.00s -> 1881.00s]  So then we could ask
[1881.00s -> 1882.00s]  some basic questions
[1882.00s -> 1883.00s]  like for example,
[1883.00s -> 1885.00s]  given some test point x star,
[1885.00s -> 1888.00s]  is f theta of x star correct?
[1888.00s -> 1889.00s]  That might seem like
[1889.00s -> 1890.00s]  the most basic question
[1890.00s -> 1891.00s]  you could ask
[1891.00s -> 1892.00s]  in a supervised
[1892.00s -> 1894.00s]  learning problem.
[1894.00s -> 1895.00s]  And it's actually
[1895.00s -> 1897.00s]  a surprisingly subtle question.
[1897.00s -> 1898.00s]  So as long as
[1898.00s -> 1899.00s]  you didn't overfit,
[1900.00s -> 1901.00s]  you know that the expected
[1901.00s -> 1902.00s]  value of your error
[1902.00s -> 1903.00s]  under the training
[1903.00s -> 1905.00s]  distribution is low.
[1905.00s -> 1906.00s]  In general,
[1906.00s -> 1907.00s]  the expected value
[1907.00s -> 1908.00s]  of your error
[1908.00s -> 1909.00s]  under some other distribution
[1909.00s -> 1911.00s]  p bar of x is not low
[1911.00s -> 1912.00s]  if p bar of x
[1912.00s -> 1914.00s]  is not equal to p of x.
[1914.00s -> 1915.00s]  I mean, the easiest way
[1915.00s -> 1916.00s]  to think about it
[1916.00s -> 1917.00s]  is to imagine that
[1917.00s -> 1918.00s]  p of x is an interval
[1918.00s -> 1920.00s]  on x from like 0 to 1
[1920.00s -> 1921.00s]  and p bar of x
[1921.00s -> 1923.00s]  is from like 1 to 2, right?
[1923.00s -> 1924.00s]  If you've just never seen
[1924.00s -> 1925.00s]  any y values
[1925.00s -> 1926.00s]  in that interval,
[1926.00s -> 1927.00s]  there's no reason
[1927.00s -> 1928.00s]  to expect
[1928.00s -> 1931.00s]  good answers.
[1931.00s -> 1932.00s]  Now, what if x star
[1932.00s -> 1934.00s]  is sampled from p of x?
[1934.00s -> 1935.00s]  Well, the thing is,
[1935.00s -> 1936.00s]  even then,
[1936.00s -> 1937.00s]  you're actually not guaranteed
[1937.00s -> 1939.00s]  to have low error
[1939.00s -> 1940.00s]  because you could have
[1940.00s -> 1942.00s]  low error in expectation
[1942.00s -> 1943.00s]  under p of x
[1943.00s -> 1944.00s]  and still have high error
[1944.00s -> 1945.00s]  on some individual points
[1945.00s -> 1946.00s]  that are within
[1946.00s -> 1948.00s]  the support of p of x.
[1948.00s -> 1949.00s]  So even that
[1949.00s -> 1952.00s]  is not guaranteed.
[1952.00s -> 1953.00s]  Now, if you are
[1953.00s -> 1955.00s]  a deep learning practitioner
[1955.00s -> 1956.00s]  at this point,
[1956.00s -> 1957.00s]  you might be saying,
[1957.00s -> 1958.00s]  well, yeah, this is true.
[1958.00s -> 1959.00s]  This is like a bunch
[1959.00s -> 1960.00s]  of kind of dusty,
[1960.00s -> 1962.00s]  boring statistics.
[1962.00s -> 1963.00s]  But usually we're not
[1963.00s -> 1964.00s]  worried about this
[1964.00s -> 1965.00s]  because neural networks
[1965.00s -> 1966.00s]  generalize really well.
[1966.00s -> 1967.00s]  And maybe neural networks
[1967.00s -> 1968.00s]  generalize so well
[1968.00s -> 1970.00s]  that even though,
[1970.00s -> 1971.00s]  in principle,
[1971.00s -> 1972.00s]  they shouldn't be handling
[1972.00s -> 1973.00s]  distributional shift,
[1973.00s -> 1974.00s]  in practice,
[1974.00s -> 1975.00s]  they'll learn
[1975.00s -> 1976.00s]  a really good function
[1976.00s -> 1977.00s]  and it will just
[1977.00s -> 1978.00s]  succeed anyway.
[1978.00s -> 1979.00s]  And this is actually
[1979.00s -> 1980.00s]  sometimes the case.
[1980.00s -> 1981.00s]  Like, you know,
[1981.00s -> 1982.00s]  you could say
[1982.00s -> 1983.00s]  that an image recognition system
[1983.00s -> 1984.00s]  trained on ImageNet
[1984.00s -> 1985.00s]  might be tested
[1985.00s -> 1986.00s]  on new images
[1986.00s -> 1987.00s]  from Flickr
[1987.00s -> 1988.00s]  or wherever ImageNet
[1988.00s -> 1989.00s]  was collected,
[1989.00s -> 1990.00s]  and yet they might still work.
[1990.00s -> 1991.00s]  Right?
[1991.00s -> 1992.00s]  So if you learn
[1992.00s -> 1993.00s]  a general enough function,
[1993.00s -> 1995.00s]  maybe it'll just generalize.
[1995.00s -> 1996.00s]  But there's a little bit
[1996.00s -> 1997.00s]  of a problem
[1997.00s -> 1999.00s]  if these x star test points
[1999.00s -> 2001.00s]  are not just chosen arbitrarily,
[2001.00s -> 2002.00s]  but if they're actually
[2002.00s -> 2004.00s]  explicitly chosen
[2004.00s -> 2005.00s]  so as to maximize
[2005.00s -> 2007.00s]  the value of f theta x.
[2007.00s -> 2009.00s]  So x star is not arbitrary.
[2009.00s -> 2010.00s]  It's actually
[2010.00s -> 2011.00s]  specifically chosen
[2011.00s -> 2013.00s]  with f theta in mind.
[2013.00s -> 2014.00s]  It actually knows
[2014.00s -> 2015.00s]  which function
[2015.00s -> 2016.00s]  to test it on,
[2016.00s -> 2018.00s]  and it maximizes
[2018.00s -> 2020.00s]  the value of that function.
[2020.00s -> 2021.00s]  Now we can get
[2021.00s -> 2022.00s]  into really big trouble
[2022.00s -> 2023.00s]  no matter how good
[2023.00s -> 2025.00s]  our function approximator is.
[2025.00s -> 2026.00s]  So imagine this green curve
[2026.00s -> 2028.00s]  is the true function,
[2028.00s -> 2029.00s]  and the blue curve
[2029.00s -> 2030.00s]  is our fit.
[2030.00s -> 2031.00s]  So the blue curve
[2031.00s -> 2032.00s]  is f theta of x.
[2032.00s -> 2033.00s]  The blue curve
[2033.00s -> 2034.00s]  is actually very good
[2034.00s -> 2035.00s]  in most places,
[2035.00s -> 2036.00s]  but if we choose
[2036.00s -> 2037.00s]  the point x
[2037.00s -> 2038.00s]  that maximizes
[2038.00s -> 2039.00s]  the blue curve,
[2039.00s -> 2041.00s]  we'll get exactly that point
[2041.00s -> 2042.00s]  that has the largest error
[2042.00s -> 2044.00s]  in the positive direction.
[2044.00s -> 2045.00s]  And this should actually
[2045.00s -> 2046.00s]  come as no surprise to us,
[2046.00s -> 2048.00s]  because in a sense,
[2048.00s -> 2049.00s]  the process of creating
[2049.00s -> 2050.00s]  adversarial examples
[2050.00s -> 2051.00s]  looks very much
[2051.00s -> 2052.00s]  like this problem.
[2052.00s -> 2053.00s]  Adversarial examples
[2053.00s -> 2054.00s]  are created by
[2054.00s -> 2055.00s]  optimizing an image
[2055.00s -> 2056.00s]  so as to maximize
[2056.00s -> 2057.00s]  the probability
[2057.00s -> 2059.00s]  of some desired class
[2059.00s -> 2060.00s]  or minimize the probability
[2060.00s -> 2061.00s]  of the true class.
[2061.00s -> 2062.00s]  So when you maximize
[2062.00s -> 2064.00s]  or optimize with
[2064.00s -> 2065.00s]  respect to inputs
[2065.00s -> 2066.00s]  into a neural net,
[2066.00s -> 2067.00s]  you can fool that neural net
[2067.00s -> 2068.00s]  into doing
[2068.00s -> 2070.00s]  just about anything.
[2070.00s -> 2072.00s]  So what does all of this
[2072.00s -> 2073.00s]  have to do
[2074.00s -> 2075.00s]  Well, let's go back
[2075.00s -> 2076.00s]  to the classic
[2076.00s -> 2077.00s]  Q-learning framework
[2077.00s -> 2078.00s]  that we talked about.
[2078.00s -> 2079.00s]  We discussed how Q-learning
[2079.00s -> 2080.00s]  is an off-policy algorithm,
[2080.00s -> 2082.00s]  so it's a very natural choice
[2082.00s -> 2083.00s]  as a starting point
[2083.00s -> 2084.00s]  for offline RL,
[2084.00s -> 2085.00s]  although I'll discuss
[2085.00s -> 2086.00s]  how it's not the only
[2086.00s -> 2087.00s]  starting point.
[2087.00s -> 2088.00s]  And in Q-learning,
[2088.00s -> 2089.00s]  you have to compute
[2089.00s -> 2090.00s]  target values.
[2090.00s -> 2091.00s]  And when you compute
[2091.00s -> 2092.00s]  target values,
[2092.00s -> 2093.00s]  you take the reward
[2093.00s -> 2094.00s]  at that transition
[2094.00s -> 2095.00s]  plus the maxim
[2095.00s -> 2096.00s]  over the next
[2096.00s -> 2097.00s]  time step actions
[2097.00s -> 2098.00s]  and plug those actions
[2098.00s -> 2099.00s]  into your
[2099.00s -> 2101.00s]  Q-function estimate.
[2101.00s -> 2102.00s]  And as we discussed
[2102.00s -> 2103.00s]  before when we talked
[2103.00s -> 2104.00s]  about Q-learning
[2104.00s -> 2105.00s]  and actor-critic,
[2105.00s -> 2106.00s]  you can equivalently
[2106.00s -> 2107.00s]  write this as the
[2107.00s -> 2108.00s]  expected value
[2108.00s -> 2109.00s]  of the Q-function
[2109.00s -> 2110.00s]  under some policy
[2110.00s -> 2111.00s]  pi-nu,
[2111.00s -> 2112.00s]  where if you're doing
[2112.00s -> 2113.00s]  classic Q-learning,
[2113.00s -> 2114.00s]  pi-nu is just
[2114.00s -> 2117.00s]  the greedy arc max policy.
[2117.00s -> 2118.00s]  And these are the
[2118.00s -> 2119.00s]  target values y
[2119.00s -> 2120.00s]  that we're regressing onto.
[2120.00s -> 2121.00s]  So the objective
[2121.00s -> 2122.00s]  when training
[2122.00s -> 2123.00s]  the Q-function
[2123.00s -> 2124.00s]  is to minimize the error
[2124.00s -> 2125.00s]  between Q and
[2125.00s -> 2126.00s]  the target values y.
[2126.00s -> 2127.00s]  And if we're doing
[2127.00s -> 2128.00s]  offline RL,
[2128.00s -> 2129.00s]  we're doing this
[2129.00s -> 2130.00s]  minimization
[2130.00s -> 2131.00s]  under some distribution,
[2131.00s -> 2132.00s]  which is the one
[2132.00s -> 2133.00s]  that produced our data set,
[2133.00s -> 2134.00s]  which is the behavior
[2134.00s -> 2135.00s]  policy pi-beta.
[2135.00s -> 2136.00s]  So we're minimizing
[2136.00s -> 2137.00s]  the difference
[2137.00s -> 2138.00s]  between Q and y
[2138.00s -> 2140.00s]  under the behavior policy.
[2140.00s -> 2141.00s]  Just like on the
[2141.00s -> 2142.00s]  previous slide,
[2142.00s -> 2143.00s]  we talked about how
[2143.00s -> 2144.00s]  we had a training
[2144.00s -> 2145.00s]  distribution p of x.
[2145.00s -> 2146.00s]  Now our training
[2146.00s -> 2148.00s]  distribution is pi-beta sa,
[2148.00s -> 2149.00s]  which means
[2149.00s -> 2150.00s]  that we expect
[2150.00s -> 2151.00s]  good accuracy.
[2151.00s -> 2152.00s]  We expect Q-values
[2152.00s -> 2153.00s]  to be accurate
[2153.00s -> 2154.00s]  in expectation
[2154.00s -> 2155.00s]  under pi-beta.
[2155.00s -> 2156.00s]  And we're evaluating
[2156.00s -> 2157.00s]  them in our target values
[2157.00s -> 2158.00s]  in expectation
[2158.00s -> 2159.00s]  under pi-nu.
[2159.00s -> 2160.00s]  So that means
[2160.00s -> 2161.00s]  that if pi-beta
[2161.00s -> 2162.00s]  is equal to pi-nu,
[2162.00s -> 2163.00s]  then we would expect
[2163.00s -> 2164.00s]  our target values
[2164.00s -> 2165.00s]  to be correct.
[2165.00s -> 2166.00s]  But of course,
[2166.00s -> 2167.00s]  that's not ever
[2167.00s -> 2168.00s]  going to happen
[2168.00s -> 2169.00s]  because the whole point
[2169.00s -> 2170.00s]  of running offline RL
[2170.00s -> 2172.00s]  is to find a better policy
[2172.00s -> 2173.00s]  to get pi-nu
[2173.00s -> 2175.00s]  to be better than pi-beta.
[2175.00s -> 2176.00s]  And even worse,
[2176.00s -> 2177.00s]  pi-nu is selected
[2177.00s -> 2178.00s]  to maximize
[2178.00s -> 2179.00s]  the expected Q-value.
[2179.00s -> 2180.00s]  That's what we
[2180.00s -> 2181.00s]  essentially do
[2181.00s -> 2182.00s]  when we take
[2182.00s -> 2183.00s]  the argmax policy,
[2183.00s -> 2184.00s]  and that's what we do
[2184.00s -> 2185.00s]  when we train an actor
[2185.00s -> 2186.00s]  and act a critic,
[2186.00s -> 2187.00s]  which is just like
[2187.00s -> 2188.00s]  this problem
[2188.00s -> 2189.00s]  on the previous slide.
[2189.00s -> 2190.00s]  Put another way,
[2190.00s -> 2191.00s]  pi-nu is explicitly trying
[2191.00s -> 2193.00s]  to find an adversarial example
[2193.00s -> 2195.00s]  to fool the Q-function
[2195.00s -> 2196.00s]  into outputting
[2196.00s -> 2197.00s]  a large value.
[2197.00s -> 2198.00s]  And that's why,
[2198.00s -> 2199.00s]  when we actually
[2199.00s -> 2200.00s]  look at the Q-values,
[2200.00s -> 2202.00s]  we see massive overestimation
[2202.00s -> 2204.00s]  because the policy
[2204.00s -> 2205.00s]  can always find some action
[2205.00s -> 2207.00s]  that fools the Q-function
[2207.00s -> 2208.00s]  if the Q-function
[2208.00s -> 2209.00s]  is not allowed
[2209.00s -> 2210.00s]  to collect more data
[2210.00s -> 2211.00s]  and update its
[2211.00s -> 2212.00s]  training distribution.
[2212.00s -> 2214.00s]  So that's the fundamental
[2214.00s -> 2216.00s]  problem in offline RL methods
[2216.00s -> 2217.00s]  that are based
[2217.00s -> 2218.00s]  on value functions
[2218.00s -> 2220.00s]  is that you're maximizing
[2220.00s -> 2221.00s]  the value function,
[2221.00s -> 2222.00s]  the value function is trained
[2222.00s -> 2223.00s]  under some distribution,
[2223.00s -> 2225.00s]  and when you maximize it,
[2225.00s -> 2226.00s]  you're going to query it
[2226.00s -> 2227.00s]  with a different distribution,
[2227.00s -> 2228.00s]  which means you can
[2228.00s -> 2229.00s]  always fool it
[2229.00s -> 2230.00s]  to output erroneously
[2230.00s -> 2233.00s]  large values.
[2233.00s -> 2234.00s]  And there are all sorts
[2234.00s -> 2235.00s]  of issues with generalization
[2235.00s -> 2237.00s]  that are basically
[2237.00s -> 2238.00s]  corrected by online
[2238.00s -> 2239.00s]  data collection
[2239.00s -> 2240.00s]  in the online setting
[2240.00s -> 2241.00s]  but not corrected
[2241.00s -> 2242.00s]  in the offline setting.
[2242.00s -> 2243.00s]  So if you imagine
[2243.00s -> 2244.00s]  that these are your actions
[2244.00s -> 2245.00s]  and you're fitting
[2245.00s -> 2246.00s]  their Q-value
[2246.00s -> 2247.00s]  in the online setting,
[2247.00s -> 2248.00s]  you conclude that this action
[2248.00s -> 2249.00s]  is the best,
[2249.00s -> 2250.00s]  but then you go
[2250.00s -> 2251.00s]  and try this action
[2251.00s -> 2252.00s]  to find that it's not the best
[2252.00s -> 2253.00s]  and then you update
[2253.00s -> 2254.00s]  your function.
[2254.00s -> 2255.00s]  In the offline setting,
[2255.00s -> 2256.00s]  you don't get to do that.
[2256.00s -> 2257.00s]  So if you extrapolate
[2257.00s -> 2258.00s]  erroneously,
[2258.00s -> 2259.00s]  if you generalize erroneously,
[2259.00s -> 2260.00s]  you're basically
[2260.00s -> 2261.00s]  stuck with that.
[2261.00s -> 2262.00s]  So that means that you get
[2262.00s -> 2263.00s]  this really nasty
[2263.00s -> 2264.00s]  out-of-distribution
[2264.00s -> 2265.00s]  action problem,
[2265.00s -> 2266.00s]  and it also means
[2266.00s -> 2267.00s]  that existing challenges
[2267.00s -> 2268.00s]  with sampling error
[2268.00s -> 2269.00s]  and function approximation error
[2269.00s -> 2271.00s]  that occur in standard RL
[2271.00s -> 2272.00s]  just become much more severe
[2272.00s -> 2274.00s]  in the offline RL setting
[2274.00s -> 2275.00s]  because you don't have
[2275.00s -> 2276.00s]  this feedback effect
[2276.00s -> 2277.00s]  where you collect more data
[2277.00s -> 2278.00s]  and fix up your mistakes.
