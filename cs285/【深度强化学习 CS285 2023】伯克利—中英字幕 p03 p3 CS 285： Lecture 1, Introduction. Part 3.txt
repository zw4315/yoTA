# Detected language: en (p=1.00)

[0.00s -> 5.00s]  Why should we study deep-reinforcement learning today?
[5.00s -> 13.20s]  Well, as I mentioned earlier, recent progress on data-driven, large-scale AI systems has
[13.20s -> 17.00s]  led to some pretty impressive results, but the methods that are trained to simply copy
[17.00s -> 21.32s]  data produced by humans, they're mainly impressive because they produce things that
[21.32s -> 22.80s]  look like human-generated results.
[22.80s -> 27.40s]  But in many cases, we actually want algorithms that will do better than the typical human
[27.40s -> 31.04s]  data, either because the human data is not good, or because it's hard to obtain, or because
[31.04s -> 35.72s]  we really do want the highest possible performance, like in the case of AlphaGo.
[35.72s -> 40.00s]  We want solutions that are impressive because the machine didn't need to be told how to
[40.00s -> 43.08s]  do something, because it discovered it on its own, because it discovered a solution
[43.08s -> 46.52s]  that was better, or because it discovered a solution in a situation where it didn't
[46.52s -> 52.76s]  have the benefit of human foresight to provide the kind of training data that it needed.
[52.76s -> 57.72s]  So recall that a lot of these very successful data-driven methods work on the basis of
[57.72s -> 61.44s]  density estimation, which has particular implications.
[61.44s -> 67.20s]  It means that these methods will produce the kinds of data that humans tend to produce,
[67.20s -> 72.32s]  but it also means that they in some sense won't go beyond good human behavior.
[72.32s -> 75.64s]  They might be much better at indexing into human data, as is certainly the case with
[75.64s -> 78.76s]  large language models, they have a lot more knowledge, but not necessarily better
[78.76s -> 82.24s]  at utilizing that knowledge to solve concrete problems.
[82.24s -> 86.72s]  If you tell a large language model, for example, to persuade somebody that it's, you know,
[86.72s -> 89.92s]  in their best interest to go see a doctor, the language model probably won't be able
[89.92s -> 94.24s]  to persuade them much better than a person would, despite the fact that it has this
[94.24s -> 100.24s]  huge repository of internet knowledge to draw.
[100.24s -> 101.60s]  So where does that leave us?
[101.60s -> 106.12s]  Well, we've got these data-driven AI systems that learn about the real world from data,
[106.12s -> 109.20s]  potentially huge amounts of data, but they don't really try to do better than the
[109.20s -> 111.20s]  data in any meaningful sense.
[111.20s -> 114.24s]  And we've got these reinforced learning systems, and they can optimize a goal with
[114.24s -> 117.36s]  a merging behavior, and that seems like something that should address one of the
[117.36s -> 120.40s]  major shortcomings of these data-driven AI methods.
[120.40s -> 123.68s]  But of course, we need to figure out how to use these reinforced learning methods at scale.
[123.68s -> 126.80s]  We need to combine them with the kinds of huge models and huge data sets that have
[126.80s -> 131.52s]  been so successful, and that's really where the deep part in deep reinforced learning comes in.
[131.52s -> 135.76s]  So data-driven AI is all about using data, reinforced learning is all about optimization.
[135.76s -> 139.00s]  Deep reinforced learning is about this kind of optimization at scale.
[139.04s -> 144.00s]  And data without optimization basically doesn't allow us to solve new problems in new ways.
[144.00s -> 149.40s]  It might allow us to be very good at indexing into large data sets to figure out solutions
[149.40s -> 155.00s]  that are human-like, but not necessarily solutions that are superhuman.
[155.00s -> 159.80s]  Something that I often like to bring up in the context of this discussion is an article
[159.80s -> 161.48s]  written by Richard Sutton.
[161.48s -> 166.00s]  So Richard Sutton is actually one of the pioneers of reinforced learning.
[166.00s -> 169.80s]  He was basically the person who popularized reinforced learning in computer science,
[169.80s -> 174.60s]  whereas previous to that, it was really a subject of study primarily in psychology.
[174.60s -> 180.00s]  So in many ways, he sort of founded the study of reinforced learning in CS.
[180.00s -> 184.32s]  Richard Sutton wrote an article in 2019 called The Bitter Lesson.
[184.32s -> 188.56s]  Those of you that haven't read it, I very strongly encourage you to read through it.
[188.56s -> 194.32s]  It provides a very concise and very clear explanation for why we've seen this revolution
[194.36s -> 197.60s]  in data-driven AI over the last few years.
[197.60s -> 203.24s]  And in that essay, he writes that we have to learn the bitter lesson that building in
[203.24s -> 205.76s]  how we think we think does not work in the long run.
[205.76s -> 209.32s]  The two methods that seem to scale arbitrarily are learning and search.
[209.32s -> 213.28s]  What he's arguing here is essentially that if we want very powerful learning machines,
[213.28s -> 218.52s]  we should build machines that are very good at using data and very good at being scaled up
[218.52s -> 223.12s]  and not necessarily worry so much about engineering these systems so that they solve problems
[223.12s -> 226.56s]  the way that we think that humans solve problems.
[226.56s -> 232.92s]  As an example, we might imagine building a system for detecting cars by somehow engineering
[232.92s -> 239.76s]  some detectors for like wheels and headlights and things like that, and then try to program
[239.76s -> 242.68s]  in that, well, a car is something that has four wheels and like two headlights in
[242.68s -> 243.68s]  the front and two in the back.
[243.68s -> 247.24s]  So if you see some wheels and some headlights, well, that's probably a car, and we can
[247.24s -> 250.24s]  basically program that in, and that's actually how people used to build computer vision
[250.24s -> 254.12s]  systems maybe about a decade ago.
[254.12s -> 257.28s]  But these days, we very rarely build perception systems that way.
[257.28s -> 261.40s]  Instead, what we do is we get lots of examples of cars, label them as cars, and let the
[261.40s -> 263.00s]  computer figure it out.
[263.00s -> 266.00s]  And that's basically what Richard Sutton is saying, that let's not worry so much
[266.00s -> 269.56s]  about building in how we think the problem should be solved, and let's instead focus
[269.56s -> 273.50s]  on scalable learning machines.
[273.50s -> 277.32s]  The machine learning community has had sort of a perpetual debate about the degree to
[277.32s -> 281.80s]  which we should be building in these kinds of components, and that's why this article
[281.80s -> 285.58s]  was so influential.
[285.58s -> 291.32s]  But a lot of people who read this article take away kind of a funny impression, maybe
[291.32s -> 296.76s]  that the emphasis is really on just scale and not really on the particular algorithm
[296.76s -> 297.88s]  that is being scaled up.
[297.88s -> 302.88s]  So maybe it's okay if we just take, let's say supervised learning methods, and as long
[302.88s -> 306.88s]  as we can figure out how to basically shovel more data into GPUs or build larger
[306.92s -> 310.00s]  server farms, that's really all that matters.
[310.00s -> 315.88s]  Data plus lots of machines, lots of computers, and not worry about how the problem is solved.
[315.88s -> 318.48s]  But that's not actually what the essay says.
[318.48s -> 320.32s]  Notice how it says learning and search.
[320.32s -> 325.16s]  It doesn't say learning and GPUs, it doesn't say learning and big data sets, it says
[325.16s -> 326.16s]  learning and search.
[326.16s -> 328.72s]  And there's a very important reason for that.
[328.72s -> 331.48s]  Learning is about extracting patterns from data.
[331.48s -> 335.32s]  You look at the world, you pull in some data, and you train some learning machine
[335.36s -> 338.76s]  on that, and it finds the patterns that are in there.
[338.76s -> 342.72s]  Search is about using computation to extract inferences.
[342.72s -> 347.48s]  Rich Sutton is using the term search in a very particular, very technical sense that
[347.48s -> 349.12s]  is commonly used in reinforcement learning.
[349.12s -> 352.12s]  Search doesn't mean like A-star search necessarily.
[352.12s -> 359.08s]  Search means some kind of computation or optimization that you use to extract inferences.
[359.08s -> 362.28s]  So search is not about getting more data.
[362.28s -> 371.28s]  Search is about using what you've got to reach more interesting and meaningful conclusions.
[371.28s -> 373.80s]  Search is essentially optimization.
[373.80s -> 378.08s]  Some kind of optimization process that uses typically iterative computation to make rational
[378.08s -> 379.08s]  decisions.
[379.08s -> 383.12s]  And it's important to have both of those things.
[383.12s -> 386.36s]  Because learning is what allows you to understand the world, and search is what
[386.36s -> 391.80s]  allows you to leverage that understanding for interesting emerging behavior.
[391.80s -> 397.12s]  And you really need both if you want to have flexible and rational and optimal decision-making
[397.12s -> 398.12s]  in real-world settings.
[398.12s -> 401.36s]  You need to understand how the world works, and then instead of just using your understanding
[401.36s -> 405.24s]  to regurgitate what you've seen before, use that understanding to find a better solution
[405.24s -> 407.44s]  than what you've seen before.
[407.44s -> 410.76s]  That's basically what deep reinforcement learning tries to do.
[410.76s -> 414.52s]  Data without optimization doesn't allow us to solve new problems in new ways.
[414.52s -> 418.36s]  Optimization without data, without experience, is hard to apply in the real world, outside
[418.36s -> 421.16s]  of things like simulators where you can write down equations of motion.
[421.16s -> 425.04s]  But if you have both of those things, then you can start to solve real-world problems
[425.04s -> 427.04s]  in more optimal ways.
[427.04s -> 431.88s]  I should add a little bit of philosophy here, where this view is not just about
[431.88s -> 436.40s]  how to control robots or how to play video games, specifically emphasized in the previous
[436.40s -> 440.44s]  section that deep reinforcement learning methods have been applied very fruitfully
[440.44s -> 444.52s]  to arrange for other domains too, and there's actually a deep reason for this.
[444.52s -> 447.64s]  To try to understand this reason, let's ask a very basic question.
[447.64s -> 450.84s]  Let's ask the question, why do we need machine learning?
[450.84s -> 454.52s]  And as an aside to help us answer that question, we can ask an even more basic question,
[454.52s -> 457.28s]  why do we need brains?
[457.28s -> 461.48s]  The neuroscientist Daniel Walpert, who knows quite a bit about brains, had this to say
[461.48s -> 462.48s]  on this topic.
[462.48s -> 466.36s]  He said, we have a brain for one reason and one reason only, and that's to produce
[466.36s -> 468.56s]  adaptable and complex movements.
[468.56s -> 472.16s]  Movement is the only way we have affecting the world around us, and I believe that to
[472.16s -> 474.56s]  understand movement is to understand the whole brain.
[474.56s -> 478.04s]  Now it won't surprise you to know that Daniel Walpert works on the neuroscience
[478.04s -> 482.92s]  of motor control, but I think this quote is very thought-provoking, and I think we can
[482.92s -> 486.64s]  apply the same intuition to machine learning and formulate this posture of it.
[486.64s -> 490.08s]  Perhaps we need machine learning for one reason and one reason only, and that's
[490.08s -> 493.56s]  to produce adaptable and complex decisions.
[493.56s -> 496.24s]  That makes a lot of sense, in the same way that your brain is only useful to you
[496.24s -> 499.00s]  insofar as it moves your body, because that's the only way that it affects the world
[499.00s -> 500.00s]  around it.
[500.00s -> 503.56s]  The machine learning system is only useful insofar as it makes good decisions, because
[503.56s -> 506.04s]  that's the only thing it's outputting.
[506.04s -> 510.12s]  And now we can start to view all machine learning problems through this lens, not as
[510.12s -> 513.04s]  problems of prediction, but as problems of decision making.
[513.04s -> 517.32s]  This is obvious if you're controlling a robot, your decision is how to move the joints,
[517.32s -> 520.68s]  it's obvious if you're driving a car, your decision is how to steer the car, but even
[520.68s -> 523.84s]  something like a computer vision system, in the end it's a decision-making system.
[523.84s -> 529.40s]  If I make a decision, which could be the image label, but really the decision has
[529.40s -> 532.20s]  implications of what happens downstream of that image label.
[532.20s -> 536.00s]  Maybe this perception system is detecting how many cars there are at an intersection,
[536.00s -> 541.24s]  and that label will be used to determine how to route traffic, so it has long-term implications.
[541.24s -> 544.60s]  Maybe the computer vision system is detecting people in a security camera, and it's going
[544.60s -> 547.24s]  to call security if it sees someone where they shouldn't be.
[547.24s -> 553.24s]  Well that's definitely a decision that could lead to some very complex and very difficult
[553.24s -> 554.72s]  to model outcomes.
[554.72s -> 558.36s]  If you view all of the outcomes of machine learning problems as decisions, then it becomes
[558.36s -> 561.64s]  clearer that all of machine learning problems are really reinforcement learning problems
[561.64s -> 565.16s]  in disguise, it's just that in some cases we have the privilege of supervised labeled
[565.20s -> 568.88s]  data that can aid us in solving them.
[568.88s -> 571.72s]  And while this perspective might be a little bit reductionist, I think it's important
[571.72s -> 576.36s]  to keep in mind, because it really tells us those building blocks, learning and search,
[576.36s -> 581.08s]  are not just special things that we want for robots and video game playing, but they're
[581.08s -> 586.40s]  really general building blocks of AI systems.
[586.40s -> 590.36s]  And that brings us to some big questions, like how do we build intelligent machines?
[590.36s -> 595.64s]  Very general intelligent machines, not just machines that can detect objects and images,
[595.64s -> 602.40s]  but you know, things like this, or this, or this, or if you are more nefarious, inclined,
[602.40s -> 603.56s]  things like this, right?
[603.56s -> 607.16s]  The kinds of intelligent machines that were popularized in science fiction that captured
[607.16s -> 612.08s]  the imagination, maybe they're quite a ways away, but how do we start taking steps
[612.08s -> 616.88s]  towards this kind of thing?
[616.88s -> 620.16s]  I think deep reinforcement learning forms a significant part of that, and I think if
[620.16s -> 623.76s]  we study it now, we might put ourselves on the path to eventually answer some pretty
[623.76s -> 626.36s]  fundamental questions.
[626.36s -> 628.96s]  So why should we study deep reinforcement learning today?
[628.96s -> 634.24s]  Well, part of the answer is that big end-to-end trained models seem to work quite well.
[634.24s -> 637.88s]  If we use large datasets and large models like transformers, we can solve some pretty
[637.88s -> 639.92s]  impressive problems.
[639.92s -> 643.56s]  And at the same time we have RL algorithms that we can feasibly combine with deep neural
[643.56s -> 647.56s]  networks, we've figured out a lot about how to implement RL algorithms so they can be used
[647.56s -> 651.00s]  to train these kind of big end-to-end models.
[651.00s -> 655.92s]  And yet learning-based control in truly open-world settings remains a major open challenge.
[655.92s -> 659.48s]  There are some initial results, including the robotics results I presented, the results
[659.48s -> 663.40s]  in other domains, that show the inkling of the capability of these systems, but
[663.40s -> 666.54s]  a lot of potential has yet to be realized.
[666.54s -> 671.08s]  And I'll talk about some of that potential in today's lecture and also over the course
[671.08s -> 676.80s]  of this class, and also discuss how some of these ideas can maybe bring us closer.
[676.80s -> 679.84s]  So it's a very exciting time, I think, to study this topic, because in some ways many
[679.84s -> 683.08s]  of the puzzle pieces are falling into place, and yet major questions remain, which could
[683.08s -> 688.88s]  be questions that you yourselves could answer in your own future work.
[688.88s -> 692.56s]  But before I get into that, I want to discuss a little bit about the broader picture
[692.56s -> 695.72s]  of the reinforcement learning field.
[695.72s -> 699.76s]  Besides the basic problem of maximizing reward functions, what are other problems that we
[699.76s -> 703.32s]  need to solve to enable real-world sequential decision-making, because this course is not
[703.32s -> 706.92s]  just about reward maximization, it's also about a variety of other problems that crop
[706.92s -> 713.20s]  up when we study decision-making and control in realistic data-driven settings, and the
[713.20s -> 716.84s]  kinds of methods that could address it.
[716.84s -> 720.84s]  For example, basic reinforcement learning deals with maximizing rewards, but this is
[720.84s -> 723.72s]  not the only problem that matters for sequential decision-making.
[723.72s -> 727.08s]  We'll cover more advanced topics like learning reward functions from examples, which is
[727.08s -> 731.04s]  referred to as inverse reinforcement learning, transferring knowledge between domains like
[731.04s -> 735.72s]  transfer learning and meta-learning, learning to predict and using prediction to act, and
[735.72s -> 736.72s]  so on.
[736.72s -> 739.96s]  Here's one question, where do rewards come from?
[739.96s -> 743.12s]  If you're playing a video game, it's pretty obvious, maybe the reward function is the
[743.12s -> 746.64s]  score in the video game, you kind of don't even have to think about it very hard.
[746.64s -> 748.12s]  But in other settings, you do.
[748.12s -> 752.60s]  What if you want to get a robot to pick up a jug and pour a glass of water?
[752.60s -> 756.44s]  Well, any child could do this, but just figuring out the reward function is the water
[756.44s -> 764.36s]  in the glass is itself a complex perception problem.
[764.36s -> 770.36s]  There's a paper that was published by some folks at UC Berkeley on exploration, actually,
[770.36s -> 773.40s]  about four or five years ago, and it had this nice quote.
[773.40s -> 777.36s]  As human agents, we are accustomed to operating with rewards that are so sparse that we
[777.36s -> 782.24s]  only experience them once or twice in a lifetime, if at all.
[782.24s -> 786.04s]  What this means is that a lot of the things that humans do that are very impressive,
[786.04s -> 789.24s]  where the reward might be so delayed, that it's very difficult to imagine learning just
[789.24s -> 790.24s]  from that reward signal.
[790.24s -> 796.88s]  For example, the reward that you'll receive for, let's say, completing a Ph.D. degree,
[796.88s -> 800.60s]  you only get that reward once, and you maybe experience some satisfaction, the real
[800.60s -> 803.88s]  outcome might be what you do afterwards with that degree, and yet you might set yourself
[803.88s -> 805.68s]  on the path to do that.
[805.68s -> 808.64s]  Clearly, it's not something that you learned through trial and error by attempting many,
[808.64s -> 812.02s]  many Ph.D. degrees in the past.
[812.02s -> 817.30s]  This is actually a quote that was posted on Reddit, where the commenter replied by writing
[817.30s -> 820.90s]  a Ph.D. author.
[820.90s -> 825.42s]  We know that there is actually a structure in the human brain, the basal ganglia, which
[825.42s -> 828.58s]  is actually responsible for the reward signal that the brain uses for reinforcement.
[828.58s -> 831.98s]  I mean, this is actually something that's been said quite a lot, and it's a non-trivial
[831.98s -> 832.98s]  structure.
[832.98s -> 837.50s]  You can see it takes up quite a bit of space, so clearly it's doing something sophisticated.
[837.50s -> 841.26s]  It's not hard to imagine that, for example, for a cheetah that needs to chase down
[841.30s -> 846.82s]  a gazelle, well, if the cheetah learned through trial and error, receiving the reward only
[846.82s -> 850.66s]  when it caught the gazelle, that's a pretty ridiculous image of a learning system.
[850.66s -> 854.62s]  If the cheetah just runs around in the savanna randomly, hoping to randomly stumble into
[854.62s -> 858.74s]  a gazelle, then randomly eat it, and only then realize that catching gazelles is a
[858.74s -> 861.54s]  good idea, well, that cheetah would probably die of starvation.
[861.54s -> 863.62s]  Of course cheetahs don't learn this way.
[863.62s -> 867.70s]  They might learn from observing other cheetahs, they might learn from their parents, they
[867.70s -> 870.58s]  might learn from all sorts of other signals, but clearly they're not learning from
[871.02s -> 875.54s]  rewards obtained only from eating the meat of the gazelle at the end of a successful hunt.
[875.54s -> 880.90s]  So there's a lot that goes into these reward signals, and you could imagine extracting
[880.90s -> 883.66s]  other, more useful forms of supervision.
[883.66s -> 887.26s]  You could learn from demonstrations, either by directly copying the observed behavior
[887.26s -> 891.22s]  or even inferring rewards from observed behavior via something called inverse reinforcement
[891.22s -> 892.22s]  learning.
[892.22s -> 895.22s]  You could learn from observing the world, learn to break what will happen next, even
[895.22s -> 898.14s]  if you're not sure what you're supposed to be doing, and then leverage that knowledge
[898.18s -> 900.86s]  later once you're more aware of what your task is.
[900.86s -> 905.22s]  You can employ unsupervised learning, unsupervised feature extraction, things like that.
[905.22s -> 908.66s]  You can also transfer knowledge from other tasks, and you can even use meta-learning
[908.66s -> 912.62s]  where you learn to adapt more quickly from your past experience of solving other tasks.
[912.62s -> 915.18s]  And these are all things that we could try to leverage, and these are all things that
[915.18s -> 917.98s]  we'll actually learn about in this course.
[917.98s -> 921.42s]  Here's an example of imitation learning, this is actually a fairly old example, at this
[921.42s -> 926.30s]  point from about 80 years ago, from some work from NVIDIA, showing a purely imitation-based
[926.30s -> 930.50s]  method for autonomous driving.
[930.50s -> 934.14s]  Now this method tries to directly copy the actions of the observed human driver, but
[934.14s -> 935.66s]  of course you could do a lot better.
[935.66s -> 937.58s]  You could, for example, infer their intent.
[937.58s -> 942.14s]  And we know this is something that humans do, this is a psychology study, here the
[942.14s -> 944.94s]  test subject is the child on the right-hand side.
[944.94s -> 948.14s]  Now you can see the child here is not going to try to imitate what the experimenter
[948.14s -> 951.74s]  is doing, because clearly the experimenter is not doing something very smart.
[951.74s -> 956.00s]  What the child will do instead is infer their intent, and then taking a very different
[956.00s -> 961.08s]  sequence of actions that is better for fulfilling their intent, rather than simply copying them.
[961.08s -> 964.20s]  This is really the hallmark of human imitation, when we say that a person imitates somebody
[964.20s -> 968.08s]  else, they're not literally observing someone's muscle activations and performing
[968.08s -> 971.52s]  the same muscle activations, at some level they're always inferring something about
[971.52s -> 976.76s]  what that other creature or person is attempting to do, and then doing it in their own way.
[976.76s -> 980.08s]  It might be very literal, where they still carry out the same motions, but figure
[980.08s -> 984.68s]  out the commands to their muscles that will create those motions, or it might be even
[984.72s -> 988.48s]  more abstract, like it is here, where they carry out entirely different actions, but that
[988.48s -> 992.12s]  lead to the seemingly desired outcome.
[992.12s -> 996.12s]  Inverse reinforcement learning algorithms can be actually used with robots, this is again
[996.12s -> 999.80s]  work that's at this point pretty old, it's about 8 years old, that shows an inverse
[999.80s -> 1003.56s]  reinforcement learning algorithm, where this robot infers the intent of the human
[1003.56s -> 1007.16s]  demonstrator, showing this pouring motion, figures out that the point is to really
[1007.16s -> 1011.28s]  seek out that yellow cup, and to pour the content of the orange cup into the yellow
[1011.28s -> 1015.28s]  cup, and once it inferred that intent, that it could perform the task in a variety of
[1015.28s -> 1019.48s]  settings.
[1019.48s -> 1025.32s]  Prediction is a really big part of control, prediction is separate from how we typically
[1025.32s -> 1030.32s]  think of model-free reinforcement learning, but there is ample evidence in neuroscience
[1030.32s -> 1034.40s]  and psychology that prediction is a very important part of how humans and animals
[1034.40s -> 1036.08s]  learn about their world.
[1036.08s -> 1039.28s]  We could imagine predictive models in a very literal sense, where you could actually predict
[1039.28s -> 1044.24s]  your future sensory readings, and you can implement real-world predictive models.
[1044.24s -> 1048.16s]  So here, a robot plays around with objects in its environment, collects some data, and
[1048.16s -> 1052.08s]  then learns to predict what it will see in response to different actions.
[1052.08s -> 1056.48s]  So the different columns here show predicted future images in response to different
[1056.48s -> 1057.48s]  motor commands.
[1057.48s -> 1061.04s]  This is quite a while back, this is 7 years ago, so you can see that the predictions
[1061.04s -> 1064.56s]  here are not of very high quality, but they capture the gist of what the robot is trying
[1064.56s -> 1069.76s]  to do, and they can be used to control objects, so you can tell it move this particular object
[1069.76s -> 1074.68s]  marked in red to the green location, it will imagine the movement, and then it will actually
[1074.68s -> 1077.80s]  actuate the arm to move the object in that way, so predictive models can allow
[1077.80s -> 1081.96s]  you to solve new tasks.
[1081.96s -> 1085.12s]  You can use this as a very powerful tool for emergent behavior.
[1085.12s -> 1089.12s]  You could, for example, command the robot to move some objects, and it might figure
[1089.12s -> 1093.64s]  out that it needs to pick up a tool to move those two objects together.
[1093.64s -> 1098.36s]  Here's another tool use example here, it figures out that that L-shaped tool can slide the
[1098.36s -> 1102.48s]  blue object, and here there's an emergent tool use scenario where it figures out that to
[1102.48s -> 1108.88s]  move these two pieces of trash, the water bottle makes for a nice improvised tool.
[1108.88s -> 1111.52s]  And predictive models have really come a long way, so in recent years they've gotten
[1111.52s -> 1115.40s]  a lot better with modern advances in generative modeling, and this is a diffusion-based video
[1115.40s -> 1120.02s]  prediction model that is being used to synthesize clips of driving videos.
[1120.02s -> 1124.18s]  The first three frames here are real, the remaining frames are actually synthesized, and you can
[1124.18s -> 1129.54s]  see that the model will actually produce realistic camera movement, it will introduce
[1129.54s -> 1136.62s]  new objects as the car turns, it will even predict the motion of the other cars
[1136.62s -> 1139.42s]  with some reasonable fidelity.
[1139.42s -> 1143.30s]  In these examples, by the way, the left video in each pair is the real one, the
[1143.30s -> 1145.50s]  right one is the synthetic one.
[1145.50s -> 1150.18s]  And here the same model is being run on robotic videos, similar to the ones that I showed
[1150.18s -> 1155.18s]  before, just so you can see the contrast from 2017 to 2022, you can see that now the
[1155.18s -> 1160.98s]  arm is clear and crisp, the objects move in realistic ways, and so forth.
[1160.98s -> 1164.90s]  There's also a lot of interesting progress, especially in the past year, on leveraging
[1164.90s -> 1166.80s]  advances in pre-trained models.
[1166.80s -> 1170.54s]  So when we do reinforcement learning, we typically don't have to do it from scratch, what
[1170.54s -> 1174.22s]  we could do is we could use a model pre-trained on large amounts of internet data, and
[1174.22s -> 1175.74s]  then use it for control.
[1175.74s -> 1178.86s]  This is actually an imitation learning example, it doesn't do RL, it actually does direct
[1178.86s -> 1183.70s]  imitation, but it is doing a learning-based control, this is the RT2 model, which uses
[1183.70s -> 1188.98s]  a, first a language model that is pre-trained on language, then a visual language model
[1188.98s -> 1193.54s]  that uses that language model to process internet images for things like question answering,
[1193.54s -> 1196.70s]  like what is happening in the image, let's say it's a great doggy walking down the street.
[1196.70s -> 1200.10s]  So now the model understands pictures, it understands text, and then that model is further
[1200.10s -> 1204.74s]  fine-tuned to output robot actions, so that when it's told, what should the robot do
[1204.74s -> 1208.74s]  to pick up the chips, it'll output the numerical values for the actions that will
[1208.74s -> 1210.02s]  actually pick up the chips.
[1210.02s -> 1213.38s]  So now it can bring in knowledge that it learned from the internet to perform this
[1213.38s -> 1215.62s]  task more effectively.
[1215.62s -> 1220.46s]  Here are some examples of the kind of intelligence tasks that this model can pass.
[1220.46s -> 1223.52s]  So it can be told to move the banana to the bottle, the robot data has examples
[1223.52s -> 1226.82s]  of moving bananas, but to understand what it means to move it to the bottle, it has
[1226.82s -> 1228.06s]  to leverage internet data.
[1228.10s -> 1232.86s]  Here it's asked to solve a math problem by putting the banana on the answer to the math problem.
[1232.86s -> 1235.78s]  Here it's told to put the strawberry into the correct bowl, to figure out what correct
[1235.78s -> 1239.06s]  bowl means, it needs to recognize the fruit scene through the bowls, and figure out that
[1239.06s -> 1244.14s]  the strawberry bowl is in fact the correct one.
[1244.14s -> 1247.14s]  And here's some more examples, pick up an object that is different from all the other
[1247.14s -> 1251.02s]  objects, now it knows how to pick up objects from the robot data, but it doesn't
[1251.02s -> 1253.62s]  know what different from all the others means from that, but that has to leverage
[1253.62s -> 1256.74s]  internet data, and it figures out that the bar is the different object because all
[1256.78s -> 1258.78s]  the other objects are bottles.
[1258.78s -> 1262.22s]  It can understand instructions in other languages, even though the robot data is only
[1262.22s -> 1265.30s]  annotated in one language and so on.
[1265.30s -> 1269.94s]  Okay, so these are some examples of the kinds of problems that we might study
[1269.94s -> 1274.26s]  in the context of learning-based decision-making, besides the 4RL problems.
[1274.26s -> 1278.30s]  But to conclude this lecture, I want to end on maybe a somewhat more grandiose point,
[1278.30s -> 1281.42s]  I want to come back to this question, how do we build intelligent machines,
[1281.42s -> 1285.38s]  and really argue that the basic building blocks of DeepRL might be very good
[1285.42s -> 1287.90s]  building blocks for answering this question.
[1287.90s -> 1290.30s]  This is of course a controversial statement, I don't expect everybody to
[1290.30s -> 1293.18s]  agree with me on this, but this is a big part of why I'm excited about this
[1293.18s -> 1296.38s]  topic, and I hope to convey some of that excitement to you.
[1296.38s -> 1299.22s]  So imagine that you have to build an intelligent machine, something as
[1299.22s -> 1302.74s]  intelligent as a person, where would you start?
[1302.74s -> 1305.30s]  Well, in the olden days, the way we would think about this is that maybe we
[1305.30s -> 1308.14s]  need to understand the brain, and the brain has a lot of parts, so let's
[1308.14s -> 1311.06s]  understand what those parts are, figure out how each of them work, and then
[1311.06s -> 1315.46s]  write computer programs to emulate the behavior of each of those parts.
[1315.46s -> 1318.34s]  Of course, our modern understanding of the brain is more advanced than
[1318.34s -> 1321.82s]  what it was in the 19th century, but parts of the brain more closely
[1321.82s -> 1324.62s]  reflect their actual function, but this is still a very difficult
[1324.62s -> 1328.26s]  problem, because each of the parts is very complex, and if we have
[1328.26s -> 1331.06s]  to do a bunch of programming to code up the behavior of each of the
[1331.06s -> 1334.82s]  parts, and do a bunch more coding to wire them together, we might be
[1334.82s -> 1337.06s]  at this for a very long time, that might just be a very, very
[1337.06s -> 1339.78s]  difficult way to implement an intelligent machine.
[1339.82s -> 1342.34s]  It might actually take a lot more intelligence on our part than we
[1342.34s -> 1344.58s]  actually have.
[1344.58s -> 1347.66s]  So, if we hypothesize that learning might be the basis of
[1347.66s -> 1350.42s]  intelligence, that might actually offer us a much easier way to
[1350.42s -> 1352.70s]  address this problem.
[1352.70s -> 1354.82s]  And here's an argument for why learning might be the basis of
[1354.82s -> 1356.22s]  intelligence.
[1356.22s -> 1358.82s]  There are some things that we can all do, like walking, so it
[1358.82s -> 1360.74s]  might be reasonably argued that maybe those things are sort of
[1360.74s -> 1362.82s]  built into our brains somehow.
[1362.82s -> 1364.74s]  But there are also some things that we can only learn, like
[1364.74s -> 1365.46s]  driving a car.
[1365.46s -> 1368.70s]  Clearly, driving a car is not built into our brains, because cars
[1368.74s -> 1370.86s]  weren't around when our brains evolved.
[1370.86s -> 1373.42s]  And we can learn a huge variety of things, including very
[1373.42s -> 1374.78s]  difficult things.
[1374.78s -> 1377.26s]  So therefore, our learning mechanisms are likely powerful
[1377.26s -> 1380.54s]  enough to do everything that we associate with intelligence.
[1380.54s -> 1382.46s]  It may be that in practice we don't actually use our
[1382.46s -> 1385.90s]  learning mechanisms for some things, like walking, but we
[1385.90s -> 1387.98s]  might hypothesize that maybe they're powerful enough that if
[1387.98s -> 1389.90s]  we didn't have those things built in, we could figure
[1389.90s -> 1391.30s]  it out anyway.
[1391.30s -> 1393.70s]  That may or may not be true, but I think there's a pretty
[1393.70s -> 1396.30s]  good reason to believe this might be true.
[1396.30s -> 1397.86s]  It might still be very convenient to hard code a
[1397.90s -> 1399.78s]  few really important bits, but let's not get distracted by
[1399.78s -> 1402.18s]  that part.
[1402.18s -> 1404.98s]  We can further hypothesize that not only is learning the
[1404.98s -> 1406.78s]  basis of intelligence, but in fact, maybe there's actually
[1406.78s -> 1409.66s]  a single learning procedure that underlies all that we
[1409.66s -> 1411.34s]  associate with intelligent behavior.
[1411.34s -> 1412.98s]  Now, that's a more radical statement.
[1412.98s -> 1414.86s]  It basically says that the way that we learn how to see
[1414.86s -> 1416.34s]  and the way that we learn how to talk and the way that
[1416.34s -> 1420.90s]  we learn how to hear is at some level the same.
[1420.90s -> 1423.14s]  Instead of having an algorithm for every module,
[1423.14s -> 1425.50s]  maybe we have a single flexible algorithm that, placed
[1425.50s -> 1427.22s]  in the right context, implements all of the
[1427.22s -> 1429.66s]  modules, everything that we need in the brain.
[1429.66s -> 1432.38s]  And there's some circumstantial evidence to indicate that
[1432.38s -> 1433.46s]  this might in fact be the case.
[1433.46s -> 1436.30s]  For example, these are some slides borrowed from
[1436.30s -> 1437.42s]  Andrew Ng.
[1437.42s -> 1439.26s]  You can build an electrode array that you can put on
[1439.26s -> 1442.02s]  your tongue, attach that array to a camera and learn
[1442.02s -> 1445.14s]  how to perceive visual percepts through your tongue.
[1445.14s -> 1448.54s]  You can take an animal, a ferret, you can disconnect
[1448.54s -> 1451.18s]  the optic nerve from the visual cortex and plug it into
[1451.18s -> 1453.46s]  the auditory cortex and after a while, the ferret will
[1453.46s -> 1455.70s]  regain some degree of visual acuity, which means that
[1455.74s -> 1458.02s]  this auditory cortex can essentially learn to process
[1458.02s -> 1459.38s]  visual signals.
[1459.38s -> 1461.10s]  So these things kind of indicate that perhaps there's
[1461.10s -> 1463.62s]  a degree of generality or homogeneity to the brain,
[1463.62s -> 1466.22s]  at least for the neocortex, such that it can adapt
[1466.22s -> 1468.30s]  to whatever sensory input is provided,
[1468.30s -> 1470.82s]  which might indicate that there's one algorithm.
[1470.82s -> 1472.82s]  And if there is one algorithm, what does this one
[1472.82s -> 1474.46s]  algorithm need to be able to do?
[1474.46s -> 1476.58s]  Well, it needs to interpret rich sensory inputs
[1476.58s -> 1478.66s]  and it needs to choose complex actions.
[1478.66s -> 1482.30s]  And to do both of those things, we need large
[1482.30s -> 1484.02s]  high capacity models because that's the only way
[1484.02s -> 1486.22s]  we know how to deal with rich sensory inputs.
[1486.22s -> 1488.26s]  And we need reinforcement learning because that's
[1488.26s -> 1491.26s]  the mathematical formulas we use to take actions.
[1491.26s -> 1493.14s]  So why deep reinforcement learning?
[1493.14s -> 1495.90s]  Well, the deep part provides us with scalable learning
[1495.90s -> 1498.46s]  from large complex data sets and the reinforced learning
[1498.46s -> 1500.76s]  gives us the optimization, the ability to take actions,
[1500.76s -> 1502.42s]  the combination of learning and search.
[1502.42s -> 1504.74s]  Deep is great for learning, reinforcement learning
[1504.74s -> 1506.46s]  is the way that we do the search.
[1507.74s -> 1510.18s]  And in fact, there's some evidence in neuroscience
[1510.18s -> 1511.02s]  for both of these things.
[1511.02s -> 1515.22s]  There's evidence that the kinds of representations
[1515.22s -> 1516.78s]  are acquired by deep neural networks
[1516.78s -> 1519.62s]  have some statistical similarity to representations
[1519.62s -> 1520.82s]  that are observed in the brain.
[1520.82s -> 1522.30s]  That doesn't mean that the brain works the same way
[1522.30s -> 1524.40s]  that deep nets do, it just means that at some level,
[1524.40s -> 1525.62s]  when you process lots of data
[1525.62s -> 1527.42s]  and extract suitable representations,
[1527.42s -> 1529.88s]  they end up looking similar, which could have more to do
[1529.88s -> 1531.66s]  with the fact that a large enough learning machine
[1531.66s -> 1533.10s]  just pulls out those patterns in the data
[1533.10s -> 1534.66s]  because that's what the data is made of.
[1534.66s -> 1536.02s]  Or it could say something about deep learning,
[1536.02s -> 1539.18s]  that's I think a much harder question to answer,
[1539.18s -> 1540.10s]  but the evidence suggests
[1540.10s -> 1543.02s]  that some kind of representational similarity exists.
[1543.02s -> 1546.34s]  For visual percepts, for auditory features
[1546.34s -> 1547.94s]  and even for the sense of touch,
[1548.94s -> 1551.02s]  the experiments done to ascertain this
[1551.02s -> 1552.54s]  are actually a little bit creative
[1552.54s -> 1557.54s]  where the brain signals indicating the kind of features
[1558.04s -> 1560.50s]  that in this case monkeys use for touch
[1560.50s -> 1563.28s]  are obtained from recordings from monkey neurons.
[1563.28s -> 1564.78s]  The deep learning experiments done
[1564.78s -> 1567.58s]  by actually taking a glove dusted with
[1569.66s -> 1572.86s]  white dust, getting a person to touch objects,
[1572.86s -> 1574.46s]  and then using a deep neural network
[1574.46s -> 1578.82s]  to discover patterns in the dust patterns on the glove.
[1578.82s -> 1581.54s]  So interesting experiments suggest
[1581.54s -> 1584.18s]  that maybe the statistical properties of features
[1584.18s -> 1586.58s]  extracted by sufficiently powerful learning machines
[1586.58s -> 1588.98s]  resemble the features in the brain.
[1588.98s -> 1590.98s]  And there's plenty of evidence in favor
[1590.98s -> 1593.22s]  of reinforcement learning as at least one of the mechanisms
[1593.22s -> 1595.56s]  underlying decision-making in humans and animals.
[1595.56s -> 1598.02s]  In fact, reinforcement learning actually emerged
[1598.06s -> 1600.10s]  as a study of animal intelligence,
[1600.10s -> 1601.38s]  but we know now from evidence
[1601.38s -> 1603.04s]  that percepts that dissipate reward
[1603.04s -> 1604.90s]  become associated with similar firing patterns
[1604.90s -> 1606.04s]  as the reward itself,
[1606.04s -> 1607.22s]  which is exactly what we would expect
[1607.22s -> 1609.94s]  from a temporal difference learning process.
[1609.94s -> 1612.66s]  The basal ganglia appears to be a kind of reward system
[1612.66s -> 1614.30s]  and that model-free RL-like adaptation
[1614.30s -> 1616.14s]  is often a good fit for experimental data
[1616.14s -> 1617.82s]  of animal adaptation.
[1617.82s -> 1618.94s]  Although not always.
[1620.58s -> 1622.44s]  But the picture is not complete, right?
[1622.44s -> 1625.86s]  So all of these kind of bits of circumstantial evidence
[1625.86s -> 1627.94s]  might suggest that the tools of deep learning
[1628.82s -> 1630.62s]  and reinforcement learning might be good tools
[1630.62s -> 1633.30s]  for tackling the problem of intelligence.
[1633.30s -> 1636.02s]  But the problem is clearly not solved.
[1636.02s -> 1637.94s]  We have great methods that can learn
[1637.94s -> 1640.56s]  from huge amounts of data by using deep learning.
[1640.56s -> 1642.90s]  We have great optimization methods for RL.
[1642.90s -> 1644.12s]  We don't yet have amazing methods
[1644.12s -> 1645.38s]  that both use data in RL.
[1645.38s -> 1648.58s]  RL has been made much more scalable in recent years.
[1648.58s -> 1651.16s]  It can tackle things like real world robotics problems,
[1651.16s -> 1654.02s]  but the kind of huge scale language model
[1654.02s -> 1655.94s]  and generative modeling applications
[1655.94s -> 1657.70s]  still primarily use supervised learning.
[1658.30s -> 1659.38s]  So there are still some algorithmic building blocks
[1659.38s -> 1660.62s]  that are necessary.
[1660.62s -> 1662.86s]  And furthermore, humans learn incredibly quickly,
[1662.86s -> 1664.50s]  whereas deep RL methods typically require
[1664.50s -> 1665.90s]  large amounts of data.
[1665.90s -> 1667.74s]  And humans reuse past knowledge,
[1667.74s -> 1670.54s]  whereas transfer learning in RL is still an open problem.
[1670.54s -> 1672.58s]  It's not always clear what the reward function should be.
[1672.58s -> 1673.42s]  And it's not always clear
[1673.42s -> 1674.38s]  what the role of prediction should be.
[1674.38s -> 1676.38s]  It seems like these methods can be very powerful,
[1676.38s -> 1678.26s]  but how do they fit in with model-free methods?
[1678.26s -> 1679.38s]  Are they just different things
[1679.38s -> 1681.36s]  or can they be reconciled in some way?
[1681.36s -> 1682.90s]  So all these question marks, I think,
[1682.90s -> 1685.58s]  give us ample space for additional research
[1685.58s -> 1687.06s]  that we can do in this area.
[1687.06s -> 1689.14s]  And perhaps if the tools of deep learning
[1689.14s -> 1691.38s]  and reinforcement learning are the right tools
[1691.38s -> 1693.14s]  for building enormously powerful
[1693.14s -> 1694.70s]  artificial intelligence systems,
[1694.70s -> 1695.80s]  then maybe studying these questions
[1695.80s -> 1698.50s]  can allow us to make some headway on that problem.
[1699.48s -> 1700.86s]  And ultimately, I think that we can get away
[1700.86s -> 1704.46s]  from this picture of thinking of intelligence systems
[1704.46s -> 1706.26s]  as a collection of modules to implement
[1706.26s -> 1708.92s]  and instead as a very elegant and simple framework
[1708.92s -> 1710.54s]  where we have a general learning algorithm
[1710.54s -> 1713.50s]  that can figure out whatever problems is posed to it.
[1713.50s -> 1715.62s]  In fact, this idea is not by any means new.
[1715.62s -> 1717.66s]  It's not something that was created in the 21st century.
[1717.66s -> 1719.14s]  It's not even something that was created
[1719.14s -> 1720.34s]  for deep learning
[1720.34s -> 1722.90s]  or even in the age of machine learning.
[1722.90s -> 1723.86s]  Here's a quote that I think
[1723.86s -> 1726.34s]  very nicely exemplifies this perspective.
[1726.34s -> 1728.48s]  Instead of trying to produce a program
[1728.48s -> 1729.92s]  to simulate the adult mind,
[1729.92s -> 1731.10s]  why not rather try to produce one
[1731.10s -> 1732.56s]  which simulates the child's?
[1732.56s -> 1733.80s]  If this were then subjected
[1733.80s -> 1735.54s]  to an appropriate course of education,
[1735.54s -> 1737.62s]  one would obtain the adult brain.
[1737.62s -> 1739.26s]  Who said this?
[1739.26s -> 1740.08s]  I'll turn.
