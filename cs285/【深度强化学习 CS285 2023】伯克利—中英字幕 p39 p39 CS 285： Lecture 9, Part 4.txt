# Detected language: en (p=1.00)

[0.00s -> 4.92s]  Alright, in the last portion of today's lecture we're going to go over another
[4.92s -> 10.94s]  way to impose the constraint, which is a little more approximate but leads to a
[10.94s -> 18.34s]  pretty simple algorithmic framework called natural gradient. Okay, so so far
[18.34s -> 23.80s]  we discussed this optimization problem, maximize the expected value under the
[23.80s -> 27.52s]  old policy state distribution of the expected value under the old policy's
[27.56s -> 32.08s]  action distribution of the importance sampled estimator, subject to a constraint
[32.08s -> 37.08s]  that the KL divergence between pi theta prime and pi theta is bounded by epsilon,
[37.08s -> 42.96s]  and we saw how if you can enforce this for a small enough epsilon, then this is
[42.96s -> 48.12s]  guaranteed to improve J theta prime minus J theta. In the previous section
[48.12s -> 51.68s]  we saw how we could impose this constraint by using something like a
[51.68s -> 55.78s]  dual gradient descent method, typical Lagrange multiplier, and then
[55.78s -> 61.22s]  essentially optimize the Lagrangian as a kind of penalized objective to find the
[61.22s -> 68.00s]  new policy parameters. Now we're going to talk about a more approximate way of
[68.00s -> 71.82s]  doing this, which at first will seem a little bit worse because we're going
[71.82s -> 75.74s]  to make a lot of approximations, but in fact is actually highly desirable
[75.74s -> 80.46s]  because it allows us to recover a very simple method that doesn't require any
[80.46s -> 85.26s]  additional penalties and simply applies a linear transformation to the standard
[85.26s -> 93.82s]  policy gradient that we saw back in lecture 3. Okay, so as before I'm going
[93.82s -> 100.98s]  to use a bar theta prime to denote this whole objective. When we calculate
[100.98s -> 105.06s]  the gradient of some objective and we use gradient ascent or gradient descent,
[105.06s -> 111.34s]  that can be interpreted as optimizing a first-order Taylor expansion of that
[111.34s -> 116.06s]  objective. This might seem like a slightly silly statement because a first-order
[116.06s -> 121.54s]  Taylor expansion is just a gradient, but when there's a constraint present it
[121.54s -> 126.98s]  actually has some meaning. So essentially if we want to optimize some
[126.98s -> 131.82s]  complicated function like this blue curve here, one way we could do it is
[131.82s -> 136.86s]  we could pick a region, compute a very simple approximation, a linear
[136.86s -> 141.06s]  approximation to that function shown here with this green line and that's
[141.06s -> 145.86s]  obtained by taking the gradient, and then instead of optimizing the blue
[145.86s -> 149.86s]  curve which is really complicated, we optimize the green line which is very
[149.86s -> 154.46s]  very simple. Of course if we don't impose any constraint, the green line
[154.46s -> 158.66s]  goes to positive and negative infinity, so this only makes sense if we impose a
[158.66s -> 163.68s]  constraint which is essentially the region within which we trust the
[163.68s -> 167.28s]  degree to which the green line approximates the blue line, a trust
[167.28s -> 171.88s]  region, and that's what this red box represents. So if you find the best
[171.88s -> 176.12s]  value of the green line within the red box, if the red box is chosen to be
[176.12s -> 181.32s]  small enough, you might hope that that point is also going to improve the
[181.32s -> 184.96s]  blue curve which is what you really want. So if we are minimizing, then we
[184.96s -> 188.24s]  would pick this point. We would pick a point on the edge of the red region
[188.24s -> 192.52s]  where the green function has the lowest value, which will hopefully also be
[192.56s -> 197.00s]  a point where the blue function has a lower value, and if we're maximizing, we
[197.00s -> 201.72s]  would go the other way. So what this means is that if you approximate your
[201.72s -> 205.32s]  complicated nonlinear function with a first-order Taylor expansion,
[205.32s -> 209.36s]  essentially a linear function, you could optimize that linear function so
[209.36s -> 213.28s]  long as you impose a trust region, so long as you limit your optimization to
[213.28s -> 218.00s]  just the region where that linear approximation is a good approximation to
[218.00s -> 222.36s]  the true nonlinear function. So we could do that with our optimization
[222.40s -> 228.00s]  problem, and we could change it to take the argmax of grad A transpose
[228.00s -> 232.76s]  theta prime minus theta, substitute this constraint, and the constraint is
[232.76s -> 236.84s]  what's imposing the trust region. And now this is very, very easy to do, of
[236.84s -> 239.96s]  course, in terms of the objective, because the objective is simply
[239.96s -> 244.08s]  linear in theta prime, which is our optimization variable. The constraint is
[244.08s -> 249.12s]  still kind of complicated, so that makes things a little more complex. Okay, so
[249.28s -> 253.20s]  the basic intuition behind what I'm going to describe next is that we're
[253.20s -> 258.12s]  going to approximate that constraint in some very tractable way, which will
[258.12s -> 261.44s]  actually give us a closed-form solution for theta prime, where we
[261.44s -> 265.92s]  won't actually have to do any complicated nonlinear optimization
[265.92s -> 270.00s]  procedure with multiple gradient steps, but in fact, a single step of policy
[270.00s -> 273.68s]  gradient will actually take care of this entire constraint optimization
[273.68s -> 277.24s]  problem. And the intuition for why that's possible is because once our
[277.28s -> 283.36s]  objective is linear, optimization becomes easy. So our objective turned
[283.36s -> 287.28s]  into a first-order Taylor approximation. By the way, just as an aside, why do we
[287.28s -> 290.12s]  want to do all this? Well, the reason we want to do all this is because
[290.12s -> 292.64s]  we're going to get a very, very simple algorithm. So we're going to
[292.64s -> 295.96s]  introduce these additional steps of approximation, but the payoff that we'll
[295.96s -> 303.84s]  get is a simpler algorithm that still works quite well. Okay, so there's our
[304.24s -> 311.20s]  new simplified constraint problem, and one really appealing thing about
[311.20s -> 317.48s]  simplifying it in this way is that our gradient grad theta a theta is
[317.48s -> 321.08s]  going to essentially become exactly the normal policy gradient. So here's
[321.08s -> 325.68s]  the equation for grad theta prime a theta prime, and since only the
[325.68s -> 328.52s]  importance weight depends on theta prime, we just get the important
[328.52s -> 334.00s]  sample policy gradient. But if we evaluate it at the point theta prime
[334.00s -> 338.76s]  equals theta, then the importance weights actually cancel out because we
[338.76s -> 341.40s]  get the same thing in the numerator and the denominator. So here I just
[341.40s -> 346.52s]  wrote it out for grad theta a bar theta. And when they cancel out, you're
[346.52s -> 351.36s]  just left with the standard policy gradient equation which we saw in the
[351.36s -> 354.88s]  previous lecture. And this is very simple. This is essentially exactly the
[354.88s -> 359.40s]  policy gradient that we had back in reinforce, except that we have, you know,
[359.40s -> 363.40s]  some choice of advantage estimator for a pi theta, and we can use anything we
[363.40s -> 367.24s]  want there. Okay, so that's nice, and that's just the thing that we're going
[367.24s -> 375.20s]  to plug into our objective. That's just the regular policy gradient. Now, could
[375.20s -> 379.00s]  we just use gradient descent then? If we're just calculating the
[379.00s -> 383.68s]  policy gradient, our objective is grad theta j theta transpose theta prime
[383.68s -> 387.00s]  does gradient ascent do something similar to this constraint optimization
[387.00s -> 392.52s]  problem. So gradient ascent would just take theta and set it to be the old
[392.52s -> 399.60s]  theta plus alpha times grad theta j theta. One problem with doing this is
[399.60s -> 405.96s]  that as we change theta, the probabilities of the new policy, the pi
[405.96s -> 410.92s]  theta a a given s, will change by different amounts because some
[410.92s -> 416.48s]  parameters, some entries in theta, affect the probabilities more than others. So in
[416.48s -> 421.18s]  general, taking a stuff like this will usually not respect the KL divergence
[421.18s -> 424.40s]  constraint because you might have some very very small change in some
[424.40s -> 428.36s]  parameter, but that parameter might have a very very large influence on the
[428.36s -> 433.80s]  probabilities. But what does gradient descent do? Well, my claim is that
[433.80s -> 438.88s]  gradient descent actually solves this constraint optimization problem, which is
[438.92s -> 442.28s]  very similar to our original one. So the difference is that our original
[442.28s -> 447.92s]  constraint optimization problem had grad j theta prime minus theta as its
[447.92s -> 452.92s]  objective, but the constraint was the KL divergence between pi theta prime and
[452.92s -> 459.60s]  pi theta. In gradient descent, the constraint is the square Euclidean
[459.60s -> 466.40s]  distance between theta prime and theta. So the original problem asks for theta
[466.44s -> 470.76s]  prime to result in a policy whose distribution is close to the policy pi
[470.76s -> 475.40s]  theta. Gradient descent instead produces parameters that are close to the
[475.40s -> 481.44s]  parameters theta. Geometrically, you can think of it like this, that in gradient
[481.44s -> 488.40s]  descent, we have our parameter space, which here is represented by this 2D
[488.40s -> 493.32s]  region, so there's two axes corresponding to two policy parameters. When we
[493.32s -> 497.16s]  linearize our objective, we can visualize our objective as this kind of
[497.16s -> 502.00s]  gradient field, as this colored gradient over the space, so lower right is better
[502.00s -> 507.68s]  values, upper left is worse values, and this constraint corresponds to a
[507.68s -> 512.40s]  circle in this space. So all the points that are epsilon away from the
[512.40s -> 517.40s]  dot in the center all satisfy that constraint. So because your objective is
[517.40s -> 521.40s]  linear and your constraint is a circle, then the solution will always lie on the
[521.40s -> 525.40s]  surface of that circle in the direction of the objective, so in the direction of
[525.40s -> 529.72s]  grad J theta. So that means that when you do gradient ascent, you will take a
[529.72s -> 534.16s]  step in the direction grad J theta, and the length of that step will be
[534.16s -> 539.32s]  determined by epsilon. In fact, the learning rate in gradient descent can
[539.32s -> 543.76s]  actually be obtained as the Lagrange multiplier for this constraint, and you
[543.76s -> 548.44s]  can also derive a closed-form equation for it. So if you perform gradient
[548.44s -> 551.76s]  ascent like this, where your learning rate is given by the square root of
[551.76s -> 557.92s]  epsilon divided by the squared length of grad J, you can actually prove that
[557.92s -> 561.24s]  you will exactly satisfy that constraint. And the way that you prove
[561.24s -> 564.84s]  this is you take that equation for theta prime, substitute that into the
[564.84s -> 568.52s]  constraint, the thetas will cancel, and you'll be left with the square root of
[568.52s -> 573.40s]  epsilon divided by grad J squared, and then that whole thing squared, so then
[573.60s -> 579.44s]  the square root goes away, and you're left with grad J squared at the top, grad J
[579.44s -> 582.92s]  squared at the bottom. Those cancel out, and you're just left with epsilon, and
[582.92s -> 587.44s]  epsilon is less than or equal to epsilon. If this is unclear to you, then
[587.44s -> 591.16s]  what I would encourage you to do is to take a piece of paper, take this
[591.16s -> 595.92s]  equation that I wrote for theta prime, plug that into the constraint, and do
[595.92s -> 599.76s]  the algebra, and you'll be able to show that the result is that epsilon is
[599.76s -> 604.76s]  less than or equal to epsilon. Okay, so where do we get to with all this? The
[604.76s -> 609.52s]  conclusion from all this is that gradient descent does actually solve a
[609.52s -> 613.76s]  constrained optimization problem, but it's the wrong constraint. The
[613.76s -> 618.52s]  constraint is in theta space instead of distribution space, and because
[618.52s -> 621.80s]  different entries in theta might affect the distribution to different
[621.80s -> 627.20s]  degrees, in general a constraint in theta space will not enforce the
[627.20s -> 631.36s]  constraint in KL divergence space because some parameters theta might
[631.36s -> 635.00s]  change a tiny bit but produce very big changes in the corresponding
[635.00s -> 641.40s]  distribution. So intuitively, the constrained shape is a circle for
[641.40s -> 644.88s]  gradient descent, but we want it to be a kind of ellipse, we want the ellipse
[644.88s -> 647.96s]  to be squished along the highly sensitive direction, so we want a
[647.96s -> 652.00s]  tighter constraint in the direction of theta that results in big changes in
[652.00s -> 655.56s]  probability, and we want the ellipse to be elongated in the directions
[655.60s -> 661.60s]  where large changes in theta result in small changes in probability. So that's
[661.60s -> 666.72s]  what we're going to try to get next. So what we're going to do, we're going to
[666.72s -> 669.60s]  recognize that these are not the same, and we're going to construct a
[669.60s -> 673.84s]  tractable approximation to the KL divergence constraint, which is the one
[673.84s -> 678.84s]  that we actually want. So the way that we're going to do this is by doing
[678.84s -> 682.08s]  the same thing to the constraint that we did to the objective. So for the
[682.08s -> 686.20s]  objective, we calculated a Taylor expansion, specifically a first-order
[686.20s -> 690.40s]  Taylor expansion. For the constraint, we're actually going to use a
[690.40s -> 696.28s]  second-order Taylor expansion around the point theta prime equals theta. We
[696.28s -> 700.36s]  don't want to use a first-order Taylor expansion because the KL
[700.36s -> 704.12s]  divergence has a derivative of 0 at theta prime equals theta, it's actually
[704.12s -> 709.08s]  flat, but its second derivative is not 0. The second-order Taylor expansion is a
[709.08s -> 714.60s]  quadratic form with some matrix F, and it turns out that the matrix that goes
[714.60s -> 718.68s]  in there is something called the Fisher information matrix. The calculation for
[718.68s -> 721.84s]  this is a bit involved, but you basically start with a formula for the
[721.84s -> 725.22s]  KL divergence, and if you take the second derivative, you can show that the
[725.22s -> 730.28s]  correct thing to use for that matrix is the Fisher information matrix, which
[730.28s -> 734.92s]  is given by this equation. So the Fisher information matrix is the expected
[734.92s -> 742.12s]  value under pi-theta of the outer product of the grad log pines. And one
[742.12s -> 745.76s]  very convenient thing about the Fisher information matrix is that you can
[745.76s -> 749.84s]  approximate it using samples. Because it's the expected value of some quantity,
[749.84s -> 754.08s]  you could simply use the same samples that you drew from pi-theta to estimate
[754.08s -> 761.36s]  your policy gradient to also estimate the Fisher information matrix. Now if
[761.40s -> 766.24s]  we're going to use this second-order Taylor expansion as an approximation for
[766.24s -> 770.32s]  our KL divergence constraint, something that we can note is that gradient
[770.32s -> 776.48s]  descent actually also has a quadratic constraint. Theta minus theta prime
[776.48s -> 780.76s]  squared can also be interpreted as theta prime minus theta transpose times
[780.76s -> 785.36s]  identity times theta prime minus theta. And now we have the same thing, only
[785.44s -> 791.72s]  instead of identity we have f. So we can estimate our Fisher information matrix
[791.72s -> 797.16s]  with samples, and then we can plug that in as a quadratic constraint just
[797.16s -> 801.96s]  using the matrix f instead of identity. And visually you can think of
[801.96s -> 805.68s]  it as turning that circle into some ellipse where the shape of the
[805.68s -> 810.92s]  ellipse is determined by the matrix f. The matrix f basically describes which
[810.96s -> 816.64s]  directions in theta space around the point theta lead to large changes in
[816.64s -> 822.76s]  probability and which ones lead to small changes in probability. And you
[822.76s -> 826.68s]  can actually show that if your constraint is quadratic like this and
[826.68s -> 831.52s]  you write out the Lagrangian, if you know the Lagrange multiplier, then the
[831.52s -> 834.96s]  solution will be given by this equation. The reason for that of course
[834.96s -> 839.20s]  is that the Lagrangian will have a linear term grad J transpose theta
[839.20s -> 845.00s]  prime and a quadratic term given by the quadratic KL divergence expansion, and a
[845.00s -> 849.20s]  second-order polynomial function like this has a closed-form solution which
[849.20s -> 854.92s]  uses the inverse of the quadratic term times the linear term. So this is
[854.92s -> 858.24s]  the closed-form solution when the constraint is given by one-half theta
[858.24s -> 864.16s]  prime minus theta transpose times f times theta prime minus theta. And
[864.16s -> 868.28s]  furthermore, if we want to enforce the constraint that this second-order
[868.36s -> 872.76s]  expansion be less than or equal to epsilon, we can use the same general
[872.76s -> 879.04s]  idea as we used in gradient descent and figure out the step size that
[879.04s -> 883.92s]  satisfies that constraint. And again, you can check that the step size is
[883.92s -> 888.12s]  correct by plugging in this equation for alpha, then plugging in the
[888.12s -> 892.24s]  resulting equation for theta prime into our KL divergence approximation, and
[892.24s -> 896.08s]  then you will be able to show that you end up with the constraint being
[896.08s -> 902.40s]  satisfied. So this expression is called the natural gradient. So you take the
[902.40s -> 905.84s]  regular gradient and you multiply it by the inverse of the Fisher information
[905.84s -> 913.20s]  matrix, and you get the natural gradient. Okay, so take a moment to look
[913.20s -> 917.04s]  over the slide. If something here is unclear, please make sure to write a
[917.04s -> 924.96s]  question in the comments. All right, now here what I want to do is I want to
[925.00s -> 929.12s]  back up a little bit and just ask, well, is this even a problem in practice? Like,
[929.12s -> 932.88s]  it kind of seemed like regular gradient ascent, regular vanilla policy
[932.88s -> 936.96s]  gradient, already enforces some kind of constraint. Maybe that constraint is
[936.96s -> 940.80s]  just good enough. I mean, it does keep theta prime close to theta, maybe not
[940.80s -> 945.84s]  quite in the right way, but maybe that's a reasonable approximation. Well,
[945.84s -> 949.24s]  let's think back to that example that we had in the policy gradient
[949.24s -> 952.96s]  lecture, where we had a one-dimensional state space and a
[953.04s -> 957.76s]  one-dimensional action space, where you get to choose to go left or right. Your
[957.76s -> 962.20s]  reward was given by negative s squared minus a squared, and you have two policy
[962.20s -> 967.60s]  parameters, k and sigma, where the probability of the actions is normally
[967.60s -> 973.60s]  distributed, where the mean is k times s, and sigma is the variance. So here
[973.60s -> 978.32s]  your parameters are k and sigma. In that setting, you're basically adding some
[978.32s -> 982.26s]  Gaussian noise and walking left and right, in that setting we saw that the
[982.26s -> 987.94s]  regular policy gradient actually looks very unfortunate. The regular policy
[987.94s -> 991.62s]  gradient does have to point in the right direction, it always does, but it's
[991.62s -> 996.02s]  extremely ill-conditioned. Essentially, as sigma decreases, the gradient with
[996.02s -> 999.82s]  respect to sigma becomes much, much larger than the gradient with respect to
[999.82s -> 1004.02s]  k. So even though the optimal solution is that k equals negative 1, you might
[1004.02s -> 1008.02s]  never reach that solution because you'll be too busy just decreasing sigma.
[1008.02s -> 1012.66s]  Intuitively, the reason this is happening is that k and sigma affect the
[1012.66s -> 1017.26s]  probabilities to different degrees, and because sigma affects the probabilities
[1017.26s -> 1022.22s]  so much more when small, then the corresponding gradient component is much
[1022.22s -> 1030.10s]  larger, and simply staying within a fixed ball in theta space, even in this
[1030.10s -> 1035.30s]  very simple two-dimensional space, is just not enough. So it's essentially the
[1035.30s -> 1040.82s]  same as a very ill-conditioned optimization problem. The natural policy
[1040.82s -> 1045.50s]  gradient largely resolves this issue, so if you just multiply the gradient by f
[1045.50s -> 1050.58s]  inverse, then that correctly accounts for the fact that sigma affects the
[1050.58s -> 1054.10s]  parameters a lot more, and it actually prioritizes k large, it actually
[1054.10s -> 1057.70s]  increases the gradient component with respect to k, and gets you to point toward in
[1057.70s -> 1061.54s]  the right direction. So this really is a problem in practice, especially for
[1061.58s -> 1065.82s]  continuous distributions like Gaussians. So in practice, natural gradient really
[1065.82s -> 1070.02s]  does give you a much more well-conditioned gradient direction, and
[1070.02s -> 1076.34s]  it makes it a lot easier to adjust learning rates. All right, a few practical
[1076.34s -> 1080.58s]  notes. So natural policy gradient, as classically described in the
[1080.58s -> 1084.66s]  literature, is an algorithm where you calculate the Fisher information matrix
[1084.66s -> 1090.02s]  f approximately using samples, and then manually select a step size alpha.
[1090.02s -> 1093.94s]  It's generally a good choice to stabilize policy gradient training, and
[1093.94s -> 1097.46s]  if you want to learn more about this, check out this paper by Peters and
[1097.46s -> 1102.70s]  Schall called Reinforcement Learning of Motor Skills with Policy Gradients.
[1102.70s -> 1108.74s]  Practical implementations require computing this product between f inverse
[1108.74s -> 1113.26s]  and grad J, which is a little bit non-trivial to do without computing a
[1113.26s -> 1117.70s]  full matrix. But if you want to implement this in practice, check out
[1117.70s -> 1121.14s]  this paper called Trust Fusion Policy Optimization. There are a few tricks that
[1121.14s -> 1127.82s]  are very useful. So one trick is that you can actually solve for the step
[1127.82s -> 1132.18s]  size alpha, which actually respects a particular value of epsilon. So if
[1132.18s -> 1136.42s]  instead of picking alpha, you want to pick epsilon, here's the formula. But
[1136.42s -> 1141.02s]  even more importantly, one of the things you could do, which makes this
[1141.02s -> 1144.54s]  much more efficient to calculate, is you can use the conjugate gradient method
[1144.54s -> 1150.10s]  to calculate f inverse grad J, and in the process of running conjugate gradient,
[1150.10s -> 1153.54s]  you're actually going to get the step size alpha given by this equation
[1153.54s -> 1158.06s]  automatically as a byproduct. I'm not going to go into details about this,
[1158.06s -> 1161.46s]  and I would encourage you to check out the Trust Fusion Policy Optimization
[1161.46s -> 1165.36s]  paper for details, but it's a fairly straightforward application of CG if
[1165.36s -> 1169.50s]  conjugate gradient. Now of course you could just use the important sampled
[1169.50s -> 1173.92s]  objective directly with something like dual gradient descent, or even a
[1173.96s -> 1178.32s]  heuristic selection of the Lagrange multiplier, and that works fairly well.
[1178.32s -> 1182.52s]  So that can be interpreted as a kind of regularization to stay close to the old
[1182.52s -> 1186.76s]  policy, and that's what I covered in the previous section. If you want to
[1186.76s -> 1191.80s]  learn more about this, check out Proximal Policy Optimization. Okay, so
[1191.80s -> 1196.04s]  to review, we talked about how policy gradient can be interpreted as policy
[1196.04s -> 1201.10s]  iteration, how you can optimize the advantage of the old policy under the
[1201.10s -> 1205.22s]  new policy state distribution, and that is exactly policy iteration, but
[1205.22s -> 1208.86s]  furthermore, if you use the old policy state distribution, then you're
[1208.86s -> 1214.06s]  optimizing a bound if the policies are close enough, and this results in a
[1214.06s -> 1218.46s]  constraint optimization problem. You can enforce this constraint
[1218.46s -> 1225.26s]  explicitly, or you can impose the constraint approximately by doing a
[1225.26s -> 1228.18s]  first-order approximation of the objective, which gives you gradient
[1228.26s -> 1232.58s]  ascent, but regular gradient ascent has the wrong constraint, so you use natural
[1232.58s -> 1236.86s]  gradient, which gives you an approximation to the KL divergence constraint. So to
[1236.86s -> 1241.42s]  summarize, regular policy gradient is a constraint procedure, but it has the
[1241.42s -> 1245.88s]  wrong constraint. Natural policy gradient replaces it with an approximation to the
[1245.88s -> 1250.94s]  KL divergence, or you could actually optimize the original important sampled
[1250.94s -> 1255.34s]  objective for many gradient steps with an additional regularizer to stay
[1255.38s -> 1259.86s]  close to the policy, the previous policy, multiplied by Lagrange multiplier.
[1259.86s -> 1264.10s]  Practical algorithms that use this, natural policy gradient, trust region
[1264.10s -> 1269.18s]  policy optimization, and proximal policy optimization.
