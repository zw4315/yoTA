# Detected language: en (p=1.00)

[0.00s -> 11.84s]  Welcome, everyone, to CS231N.
[11.84s -> 15.22s]  I'm super excited to offer this class again for the third time.
[15.22s -> 19.46s]  It seems that every time we offer this class, it's just growing exponentially, unlike
[19.46s -> 21.48s]  most things in the world.
[21.48s -> 26.32s]  This is the third time we're teaching this class, the first time we had 150 students.
[26.32s -> 28.82s]  Last year, we had 350 students, so it doubled.
[28.82s -> 35.70s]  This year, we've doubled again to about 730 students when I checked this morning.
[35.70s -> 41.02s]  Anyone who was not able to fit into the lecture hall, I apologize, but the videos will be
[41.02s -> 44.88s]  up on the SCPD website within about two hours.
[44.88s -> 48.54s]  If you weren't able to come today, then you can still check it out within a couple
[48.54s -> 51.16s]  hours.
[51.16s -> 55.78s]  This class, CS231N, is really about computer vision.
[55.78s -> 57.28s]  What is computer vision?
[57.28s -> 60.38s]  Computer vision is really the study of visual data.
[60.38s -> 63.90s]  Since there's so many people enrolled in this class, I think I probably don't need to convince
[63.90s -> 69.92s]  you that this is an important problem, but I'm still going to try to do that anyway.
[69.92s -> 73.60s]  The amount of visual data in our world has really exploded just to a ridiculous degree
[73.60s -> 75.50s]  in the last couple of years.
[75.50s -> 80.32s]  This is largely a result of the large number of sensors in the world.
[80.32s -> 83.40s]  Probably most of us in this room are carrying around smartphones, and each smartphone
[83.40s -> 86.90s]  has one, two, or maybe even three cameras on it.
[86.90s -> 91.08s]  I think on average there's even more cameras in the world than there are people.
[91.08s -> 95.14s]  As a result of all these sensors, there's just a crazy large massive amount of visual
[95.14s -> 98.86s]  data being produced out there in the world each day.
[98.86s -> 104.74s]  One statistic that I really like, to put this in perspective, is a 2015 study from
[104.74s -> 111.58s]  Cisco that estimated that by 2017, which is where we are now, that roughly 80 percent
[111.58s -> 114.74s]  of all traffic on the internet would be video.
[115.36s -> 120.38s]  This is not even counting all the images and other types of visual data on the web,
[120.38s -> 125.62s]  but just from a pure number of bits perspective, the majority of bits flying around the internet
[125.62s -> 127.46s]  are actually visual data.
[127.46s -> 132.42s]  It's really critical that we develop algorithms that can utilize and understand this data.
[132.42s -> 138.36s]  However, there's a problem with visual data, and that's that it's really hard to understand.
[138.36s -> 142.90s]  Sometimes we call visual data the dark matter of the internet, in analogy with dark
[142.90s -> 144.70s]  matter in physics.
[144.70s -> 149.74s]  For those of you who have heard of this in physics before, dark matter accounts for some
[149.74s -> 153.22s]  astonishingly large fraction of the mass in the universe.
[153.22s -> 157.54s]  We know about it due to the existence of gravitational pulls on various celestial
[157.54s -> 160.82s]  bodies and whatnot, but we can't directly observe it.
[160.82s -> 164.68s]  Visual data on the internet is much the same, where it comprises the majority of
[164.68s -> 169.94s]  bits flying around the internet, but it's very difficult for algorithms to actually
[169.94s -> 175.86s]  go in and understand and see what exactly is comprising all the visual data on the web.
[175.86s -> 179.66s]  Another statistic that I like is that of YouTube.
[179.66s -> 184.14s]  Roughly every second of clock time that happens in the world, there's something like
[184.14s -> 187.50s]  five hours of video being uploaded to YouTube.
[187.50s -> 194.30s]  If we just sit here and count, one, two, three, now there's 15 more hours of video
[194.30s -> 199.06s]  on YouTube, and Google has a lot of employees, but there's no way that they could ever
[199.06s -> 204.02s]  have an employee sit down and watch and understand and annotate every video.
[204.02s -> 208.90s]  So if they want to catalog and serve you relevant videos and maybe monetize by putting
[208.90s -> 213.02s]  ads in those videos, it's really crucial that we develop technologies that can dive
[213.02s -> 218.50s]  in and automatically understand the content of visual data.
[218.50s -> 223.74s]  So this field of computer vision is truly an interdisciplinary field, and it touches
[223.74s -> 227.30s]  on many different areas of science and engineering and technology.
[227.30s -> 233.22s]  So obviously computer vision is the center of the universe, but as a constellation of
[233.22s -> 237.70s]  fields around computer vision, we touch on areas like physics, because we need to understand
[237.70s -> 241.62s]  optics and image formation and how images are actually physically formed.
[241.62s -> 247.32s]  We need to understand biology and psychology, to understand how animal brains physically
[247.32s -> 249.74s]  see and process visual information.
[249.74s -> 253.78s]  We of course draw a lot on computer science, mathematics, and engineering as we actually
[253.78s -> 259.90s]  strive to build computer systems that implement our computer vision algorithms.
[259.90s -> 263.70s]  So a little bit more about where I'm coming from and about where the teaching staff
[263.70s -> 266.74s]  of this course is coming from.
[266.74s -> 272.54s]  Me and my co-instructor Serena are both PhD students in the Stanford Vision Lab,
[272.54s -> 278.58s]  which is headed by Professor Fei-Fei Li, and our lab really focuses on machine learning
[278.58s -> 280.58s]  and the computer science side of things.
[280.58s -> 282.82s]  I work a little bit more on language and vision.
[282.86s -> 284.62s]  I've done some projects in that.
[284.62s -> 287.86s]  And other folks in our group have worked a little bit on the neuroscience and cognitive
[287.86s -> 292.26s]  science side of things.
[292.26s -> 295.70s]  So as a bit of introduction, you might be curious about how this course relates to
[295.70s -> 297.60s]  other courses at Stanford.
[297.60s -> 302.98s]  So we kind of assume a basic introductory level understanding of computer vision.
[302.98s -> 307.02s]  So if you're kind of an undergrad and you've never seen computer vision before, maybe
[307.02s -> 312.10s]  you should have taken CS 131, which was offered earlier this year by Fei-Fei and Juan
[312.10s -> 314.30s]  Carlos Nieblaes.
[314.30s -> 320.18s]  There was a course taught last quarter by Professor Chris Manning and Richard Soacher
[320.18s -> 325.02s]  about the intersection of deep learning and natural language processing.
[325.02s -> 329.08s]  And I imagine a number of you may have taken that course last quarter.
[329.08s -> 333.60s]  But we will cover some of the overlap between this course and that.
[333.60s -> 337.62s]  But we're really focusing on the computer vision side of things and really focusing
[337.62s -> 341.18s]  all of our motivation in computer vision.
[341.18s -> 347.50s]  Also concurrently taught this quarter is CS 231A taught by Professor Silvio Savarossi.
[347.50s -> 353.66s]  And CS 231A really focuses on, is a more all-encompassing computer vision course.
[353.66s -> 359.96s]  It's focusing on things like 3D reconstruction, on matching and robotic vision, and is a bit
[359.96s -> 363.72s]  more all-encompassing with regards to vision than our course.
[363.72s -> 369.18s]  And this course, CS 231N, really focuses on a particular class of algorithms revolving
[369.18s -> 373.34s]  around neural networks, and especially convolutional neural networks, and their applications
[373.34s -> 376.40s]  to various visual recognition tasks.
[376.40s -> 378.76s]  Of course, there's also a number of seminar courses that are taught.
[378.76s -> 384.06s]  And you'll have to check the syllabus and course schedule for more details on those
[384.06s -> 387.58s]  because they vary a bit each year.
[387.58s -> 391.78s]  So this lecture is normally given by Professor Fei-Fei Li.
[391.78s -> 394.30s]  Unfortunately she wasn't able to be here today.
[394.30s -> 398.34s]  So instead, for the majority of the lecture, we're going to tag team a little bit.
[398.34s -> 403.62s]  So she actually recorded a bit of pre-recorded audio describing to you the history of computer
[403.62s -> 409.38s]  vision because this class is a computer vision course, and it's very critical and important
[409.38s -> 413.72s]  that you understand the history and the context of all the existing work that led
[413.72s -> 417.74s]  us to these developments of convolutional neural networks as we know them today.
[417.74s -> 424.04s]  I'll let virtual Fei-Fei take over and give you a brief introduction to the history
[424.04s -> 426.76s]  of computer vision.
[426.76s -> 432.52s]  OK, let's start with today's agenda.
[432.52s -> 434.80s]  So we have two topics to cover.
[434.80s -> 439.92s]  One is a brief history of computer vision, and the other one is the overview of our
[439.92s -> 443.36s]  course, CS 231N.
[443.36s -> 450.20s]  So we'll start with a very brief history of where vision comes from, when did computer
[450.20s -> 455.72s]  vision start, and where we are today.
[455.72s -> 460.32s]  The history of vision can go back many, many years ago.
[460.32s -> 465.80s]  In fact, about 543 million years ago.
[465.80s -> 468.28s]  What was life like during that time?
[468.28s -> 471.44s]  Well, the earth was mostly water.
[471.44s -> 477.64s]  There were few species of animals floating around in the ocean.
[477.64s -> 479.72s]  And life was very chill.
[479.72s -> 482.32s]  Animals didn't move around much.
[482.32s -> 484.54s]  They don't have eyes or anything.
[484.54s -> 487.24s]  When food swims by, they grab them.
[487.24s -> 491.74s]  If the food didn't swim by, they just float around.
[491.74s -> 499.08s]  But something really remarkable happened around 540 million years ago.
[499.08s -> 507.12s]  From fossil studies, zoologists found out within a very short period of time, 10 million
[507.12s -> 513.12s]  years, the number of animal species just exploded.
[513.12s -> 517.68s]  It went from a few of them to hundreds of thousands.
[517.68s -> 519.84s]  And that was strange.
[519.84s -> 521.68s]  What caused this?
[521.68s -> 527.60s]  There were many theories, but for many years, it was a mystery.
[527.60s -> 531.84s]  Evolutionary biologists call this evolution's big bang.
[531.84s -> 538.34s]  A few years ago, an Australian zoologist called Andrew Parker proposed one of the most
[538.34s -> 540.44s]  convincing theories.
[540.44s -> 549.54s]  From the studies of fossils, he discovered around 540 million years ago, the first animals
[549.54s -> 552.68s]  developed eyes.
[552.68s -> 562.84s]  And the onset of vision started this explosive speciation phase.
[562.84s -> 565.08s]  Animals can suddenly see.
[565.08s -> 569.30s]  Once you can see, life becomes much more proactive.
[569.30s -> 575.40s]  Some predators went after preys, and preys have to escape from predators.
[575.40s -> 583.20s]  So the evolution or onset of vision started an evolutionary arms race, and animals had
[583.20s -> 589.16s]  to evolve quickly in order to survive as a species.
[589.16s -> 593.26s]  So that was the beginning of vision in animals.
[593.26s -> 600.96s]  After 540 million years, vision has developed into the biggest sensory system of almost
[600.96s -> 604.20s]  all animals, especially intelligent animals.
[604.20s -> 613.88s]  In humans, we have almost 50% of the neurons in our cortex involved in visual processing.
[613.88s -> 622.28s]  It is the biggest sensory system that enables us to survive, work, move around, manipulate
[622.28s -> 626.64s]  things, communicate, entertain, and many things.
[627.08s -> 634.32s]  So vision is really important for animals and especially intelligent animals.
[634.32s -> 640.72s]  So that was a quick story of biological vision.
[640.72s -> 650.48s]  What about humans, the history of humans making mechanical vision or cameras?
[650.68s -> 657.96s]  Well, one of the early cameras that we know today is from the 1600s, the Renaissance
[657.96s -> 661.96s]  period of time, camera obscura.
[661.96s -> 667.46s]  And this is a camera based on pinhole camera theories.
[667.46s -> 678.76s]  It's very similar to, it's very similar to the, to the early eyes that animals developed
[678.76s -> 686.60s]  with a hole that collects lights and then a plane in the back of the camera that collects
[686.60s -> 690.12s]  the information and projects the imagery.
[690.12s -> 697.40s]  So as cameras evolved, today we have cameras everywhere.
[697.40s -> 705.40s]  This is one of the most popular sensors people use from smartphones to other sensors.
[705.40s -> 712.00s]  In the meantime, biologists started studying the mechanism of vision.
[712.00s -> 718.48s]  One of the most influential work in both human vision or animal vision, as well as
[718.48s -> 726.68s]  that inspired computer vision is the work done by Hubel and Wiesel in the 50s and
[726.68s -> 730.58s]  60s using electrophysiology.
[730.58s -> 738.54s]  What they were asking the question is, what was the visual processing mechanism like in
[738.54s -> 742.02s]  primates, in mammals?
[742.02s -> 748.42s]  So they chose to study cat brain, which is more or less similar to human brain from
[748.42s -> 751.34s]  a visual processing point of view.
[751.34s -> 757.00s]  What they did is to stick some electrodes in the back of the cat brain, which is where
[757.00s -> 766.56s]  the primary visual cortex area is, and then look at what stimuli makes the neurons in
[766.56s -> 773.68s]  the, in the back, in the primary visual cortex of cat brain respond excitedly.
[773.68s -> 780.38s]  What they learned is that there are many types of cells in the, in the primary visual
[780.38s -> 783.72s]  cortex part of the cat brain.
[783.76s -> 787.00s]  One of the most important cell is the simple cells.
[787.00s -> 793.84s]  They respond to oriented edges when they move in certain directions.
[793.84s -> 798.88s]  Of course, there are also more complex cells, but by and large, what they discovered
[798.88s -> 808.96s]  is visual processing starts with simple structure of the visual world, oriented edges, and
[809.00s -> 816.92s]  As information moves along the visual processing pathway, the brain builds up the complexity
[816.92s -> 824.76s]  of the visual information until it can recognize the complex visual world.
[824.76s -> 831.60s]  So the history of computer vision also starts around early 60s.
[831.60s -> 839.20s]  Block World is a set of work published by Larry Roberts, which is widely known as
[839.20s -> 846.68s]  one of the first, probably the first PhD thesis of computer vision, where the visual
[846.68s -> 851.00s]  world was simplified into simple geometric shapes.
[851.00s -> 858.98s]  And the goal is to be able to recognize and reconstruct what these shapes are.
[858.98s -> 870.42s]  In 1966, there was a now famous MIT summer project called the Summer Vision Project.
[870.42s -> 877.54s]  The goal of the Summer Vision Project, as I read, is an attempt to use our summer
[877.54s -> 883.94s]  workers effectively in the construction of a significant part of a visual system.
[883.94s -> 890.74s]  So the goal is in one summer, we're going to work out the bulk of the visual system.
[890.74s -> 893.22s]  That was an ambitious goal.
[893.22s -> 895.42s]  Fifty years have passed.
[895.42s -> 902.50s]  The field of computer vision has blossomed from one summer project into a field of
[902.50s -> 908.70s]  thousands of researchers worldwide still working on some of the most fundamental problems
[908.70s -> 909.86s]  of vision.
[909.86s -> 917.94s]  We still have not yet solved vision, but it has grown into one of the most important
[917.94s -> 923.98s]  and fastest growing areas of artificial intelligence.
[923.98s -> 929.38s]  Another person that we should pay tribute to is David Marr.
[929.38s -> 935.88s]  David Marr was a MIT vision scientist, and he has written an influential book in the
[935.88s -> 945.68s]  late 70s about what he thinks vision is and how we should go about computer vision and
[945.68s -> 952.32s]  developing algorithms that can enable computers to recognize the visual world.
[952.32s -> 963.52s]  The thought process in David Marr's book is that in order to take an image and arrive
[963.52s -> 971.08s]  at a final, holistic, full 3D representation of the visual world, we have to go through
[971.08s -> 972.96s]  several processes.
[972.96s -> 976.52s]  The first process is what he calls primal sketch.
[976.52s -> 983.56s]  This is where mostly the edges, the bars, the ends, the virtual lines, the curves,
[983.56s -> 986.60s]  the boundaries are represented.
[986.60s -> 990.76s]  And this is very much inspired by what neuroscientists have seen.
[990.76s -> 996.44s]  Hubel and Wiesel told us the early stage of visual processing has a lot to do with
[996.44s -> 1000.40s]  simple structures like edges.
[1000.40s -> 1006.20s]  Then the next step after the edges and the curves is what David Marr calls two and
[1006.20s -> 1008.04s]  a half D sketch.
[1008.04s -> 1014.68s]  This is where we start to piece together the surfaces, the depth information, the layers
[1014.68s -> 1019.12s]  or the discontinuities of the visual scene.
[1019.12s -> 1026.92s]  And then eventually we put everything together and have a 3D model hierarchically organized
[1026.92s -> 1031.42s]  in terms of surface, volumetric primitives and so on.
[1031.42s -> 1039.92s]  So that was a very idealized thought process of what vision is.
[1039.92s -> 1047.20s]  And this way of thinking actually has dominated computer vision for several decades and is
[1047.20s -> 1052.84s]  also a very intuitive way for students to enter the field of vision and think about
[1052.84s -> 1060.04s]  how we can deconstruct the visual information.
[1060.04s -> 1069.24s]  Another very important seminal group of work happened in the 70s where people began to
[1069.24s -> 1076.68s]  ask the question, how can we move beyond the simple block world and start recognizing
[1076.68s -> 1080.80s]  or representing real world object?
[1080.80s -> 1086.56s]  And I think about the 70s, it's a time that there's very little data available.
[1086.56s -> 1092.36s]  Computers are extremely slow, PCs are not even around, but computer scientists are
[1092.36s -> 1098.04s]  starting to think about how we can recognize and represent objects.
[1098.04s -> 1106.46s]  So in Palo Alto, both at Stanford as well as SRI, two groups of scientists that propose
[1106.46s -> 1112.62s]  similar ideas, one is called generalized cylinder, one is called pictorial structure.
[1112.62s -> 1120.98s]  The basic idea is that every object is composed of simple geometric primitives.
[1120.98s -> 1128.38s]  For example, a person can be pieced together by generalized cylindrical shapes or a person
[1128.38s -> 1136.30s]  can be pieced together by critical parts and their elastic distance between these parts.
[1136.30s -> 1144.94s]  So either representation is a way to reduce the complex structure of the object into
[1144.94s -> 1152.34s]  a collection of simpler shapes and their geometric configuration.
[1152.34s -> 1159.88s]  These work have been influential for quite a few years.
[1159.92s -> 1168.60s]  And then in the 80s, David Lowe here is another example of thinking how to reconstruct
[1168.60s -> 1173.40s]  or recognize the visual world from simple world structures.
[1173.40s -> 1184.36s]  This work is by David Lowe, which he tries to recognize razors by constructing lines
[1184.36s -> 1192.16s]  and edges and mostly straight lines and their combination.
[1192.16s -> 1202.14s]  So there was a lot of effort in trying to think what is the tasks in computer vision
[1202.14s -> 1205.48s]  in the 60s, 70s, and 80s.
[1205.48s -> 1212.32s]  And frankly, it was very hard to solve the problem of object recognition.
[1212.32s -> 1220.58s]  Everything I've shown you so far are very audacious, ambitious attempts, but they remain
[1220.58s -> 1226.40s]  at the level of toy examples or just a few examples.
[1226.40s -> 1233.32s]  Not a lot of progress has been made in terms of delivering something that can work
[1233.32s -> 1235.56s]  in real world.
[1235.56s -> 1242.40s]  So as people think about what are the problems to solve in vision, one important question
[1242.40s -> 1250.22s]  came around is if object recognition is too hard, maybe we should first do object segmentation.
[1250.22s -> 1258.92s]  That is the task of taking an image and group the pixels into meaningful areas.
[1258.92s -> 1265.40s]  We might not know the pixels that group together is called a person, but we can extract
[1265.40s -> 1270.32s]  out all the pixels that belong to the person from its background.
[1270.32s -> 1272.72s]  That is called image segmentation.
[1272.72s -> 1279.96s]  So here's one very early seminal work by Jitendra Malik and his student Zhenbo Shi
[1279.96s -> 1291.28s]  from Berkeley from using a graph theory algorithm for the problem of image segmentation.
[1291.28s -> 1299.76s]  There's another problem that made some headway ahead of many other problems in computer
[1299.76s -> 1303.14s]  vision, which is face detection.
[1303.14s -> 1308.08s]  Face was one of the most important objects to humans, probably the most important objects
[1308.08s -> 1309.76s]  to humans.
[1309.76s -> 1318.56s]  Around the time of 1999 to 2000, machine learning techniques, especially statistical
[1318.56s -> 1323.08s]  machine learning techniques, start to gain momentum.
[1323.08s -> 1330.92s]  These are techniques such as support vector machines, boosting graphical models, including
[1330.92s -> 1334.04s]  the first wave of a neural network.
[1334.04s -> 1341.16s]  One particular work that made a lot of contribution was using AdaBoost algorithm
[1341.16s -> 1346.76s]  to do real time face detection by Paul Viola and Michael Jones.
[1346.76s -> 1349.84s]  And there's a lot to admire in this work.
[1349.84s -> 1355.88s]  It was done in 2001 when computer chips are still very, very slow, but they're able to
[1355.88s -> 1360.52s]  do face detection images in near real time.
[1360.52s -> 1369.92s]  And after the publication of this paper, in five years time, 2006, Fujifilm rolled out
[1369.92s -> 1378.40s]  the first digital camera that has a real time face detector in the camera.
[1378.40s -> 1387.52s]  So it was a very rapid transfer from basic science research to real world application.
[1387.52s -> 1395.24s]  So as a field, we continue to explore how we can do object recognition better.
[1395.24s -> 1404.48s]  So one of the very influential way of thinking in the late 90s till the first 10 years of
[1404.48s -> 1409.44s]  2000 is feature-based object recognition.
[1409.44s -> 1414.68s]  And here is a seminal work by David Lowe called SIFT feature.
[1414.68s -> 1422.44s]  The idea is that to match an entire object, for example, here is a stop sign, to another
[1422.44s -> 1423.44s]  stop sign.
[1423.44s -> 1429.84s]  It's very difficult because there might be all kinds of changes due to camera angles,
[1429.84s -> 1437.96s]  occlusion, viewpoint, lighting, and just the intrinsic variation of the object itself.
[1437.96s -> 1446.00s]  But it's inspired to observe that there are some parts of the object, some features
[1446.00s -> 1453.32s]  that tend to remain diagnostic and invariant to changes.
[1453.32s -> 1460.44s]  So the task of object recognition began with identifying these critical features
[1460.44s -> 1465.88s]  on the object and then match the features to a similar object.
[1465.88s -> 1470.80s]  That's an easier task than pattern matching the entire object.
[1470.80s -> 1480.24s]  So here is a figure from his paper where it shows that several dozen SIFT features
[1480.24s -> 1491.76s]  from one stop sign are identified and matched to the SIFT features of another stop sign.
[1491.76s -> 1499.96s]  Using the same building block, which is features, diagnostic features in images, we
[1499.96s -> 1506.68s]  have as a field has made another step forward and start to recognizing holistic scenes.
[1506.68s -> 1512.36s]  Here's an example algorithm called spatial pyramid matching.
[1512.36s -> 1519.60s]  The idea is that there are features in the images that can give us clues about which
[1519.60s -> 1526.76s]  type of scene it is, whether it's a landscape or a kitchen or a highway and so on.
[1527.28s -> 1533.56s]  This particular work takes these features from different parts of the image and in different
[1533.56s -> 1539.28s]  resolutions and put them together in a feature descriptor.
[1539.28s -> 1545.24s]  And then we do support vector machine algorithm on top of that.
[1545.24s -> 1554.12s]  Similarly, a very similar work has gained momentum in human recognition.
[1554.20s -> 1564.12s]  So putting together these features, we have a number of work that looks at how we can
[1564.12s -> 1570.52s]  compose human bodies in more realistic images and recognize them.
[1570.52s -> 1576.40s]  So one work is called the histogram of gradients, another work is called deformable
[1576.40s -> 1579.28s]  body part models.
[1579.28s -> 1589.44s]  So as you can see, as we move from the 60s, 70s, 80s towards the first decade of the
[1589.44s -> 1596.32s]  21st century, one thing is changing and that's the quality of the pictures.
[1596.32s -> 1604.12s]  We're no longer with the internet, the growth of the internet, the digital cameras,
[1604.12s -> 1608.80s]  we're having better and better data to study computer vision.
[1608.80s -> 1618.56s]  So one of the outcome in the early 2000s is that the field of computer vision has defined
[1618.56s -> 1622.84s]  a very important building block problem to solve.
[1622.84s -> 1629.36s]  It's not the only problem to solve, but in terms of recognition, this is a very important
[1629.36s -> 1632.80s]  problem to solve, which is object recognition.
[1632.80s -> 1641.68s]  I talked about object recognition all along, but in the early 2000s, we began to have
[1641.68s -> 1649.64s]  benchmark data set that can enable us to measure the progress of object recognition.
[1649.64s -> 1656.88s]  One of the most influential benchmark data set is called Pascal Visual Object Challenge.
[1656.88s -> 1663.88s]  And it's a data set composed of 20 object classes.
[1663.88s -> 1667.80s]  Three of them are shown here, train, airplane, person.
[1667.80s -> 1673.44s]  I think it also has cows, bottles, cats, and so on.
[1673.44s -> 1681.64s]  And the data set is composed of several thousand to 10,000 images per category.
[1681.64s -> 1690.76s]  And then the field, different groups develop algorithm to test against the testing set
[1690.76s -> 1693.28s]  and see how we have made progress.
[1693.28s -> 1704.32s]  So here is a figure that shows from year 2007 to year 2012 the performance on detecting
[1704.32s -> 1715.64s]  object, the 20 object in the image, in the benchmark data set has steadily increased.
[1715.64s -> 1718.44s]  So that was a lot of progress made.
[1718.44s -> 1727.08s]  Around that time, a group of us from Princeton to Stanford also began to ask a harder question
[1727.08s -> 1735.60s]  to ourselves as well as our field, which is, are we ready to recognize every object
[1735.60s -> 1738.72s]  or most of the object in the world?
[1738.72s -> 1746.12s]  It's also motivated by an observation that is rooted in machine learning, which is
[1746.12s -> 1752.04s]  that most of the machine learning algorithms, it doesn't matter if it's graphical model
[1752.04s -> 1761.88s]  or support vector machine or AdaBoost, it's very likely to overfit in the training process.
[1761.88s -> 1766.04s]  And part of the problem is visual data is very complex.
[1766.04s -> 1773.76s]  Because it's complex, our models tend to have a high dimension of input and have to
[1773.76s -> 1776.54s]  have a lot of parameters to fit.
[1776.54s -> 1782.62s]  And when we don't have enough training data, overfitting happens very fast.
[1782.62s -> 1785.58s]  And then we cannot generalize very well.
[1785.58s -> 1793.86s]  So motivated by this dual reason, one is just want to recognize the world of all
[1793.86s -> 1794.86s]  the objects.
[1794.86s -> 1801.14s]  The other one is to come back the machine learning, overcome the machine learning bottleneck
[1801.14s -> 1803.08s]  of overfitting.
[1803.08s -> 1806.46s]  We began this project called ImageNet.
[1806.46s -> 1813.40s]  We wanted to put together the largest possible data set of all the pictures we can find
[1813.40s -> 1820.76s]  in the world of objects and use that for training as well as for benchmarking.
[1820.76s -> 1827.10s]  So it was a project that took us about three years, lots of hard work.
[1827.10s -> 1835.26s]  It basically began with downloading billions of images from the internet, organized by
[1835.26s -> 1843.34s]  the dictionary we called WordNet, which is tens of thousands of object classes.
[1843.34s -> 1851.30s]  And then we have to use some clever crowd engineering trick and method using Amazon
[1851.30s -> 1858.68s]  Mechanical Turk platform to sort, clean, label each of the images.
[1858.68s -> 1868.70s]  The end result is a ImageNet of almost 15 million or 40 million plus images organized
[1868.70s -> 1873.74s]  in 22,000 categories of objects and scenes.
[1873.74s -> 1885.06s]  And this is the gigantic, probably the biggest data set produced in the field of AI at that
[1885.06s -> 1886.34s]  time.
[1886.34s -> 1894.82s]  And it began to push forward the algorithm development of object recognition into another
[1894.82s -> 1895.82s]  phase.
[1895.82s -> 1901.38s]  Especially important is how to benchmark the progress.
[1901.42s -> 1910.50s]  So starting 2009, the ImageNet team rolled out an international challenge called ImageNet
[1910.50s -> 1914.66s]  Large Scale Visual Recognition Challenge.
[1914.66s -> 1922.38s]  And for this challenge, we put together a more stringent test set of 1.4 million
[1922.38s -> 1927.06s]  objects across 1,000 object classes.
[1927.06s -> 1934.18s]  And this is to test the image classification recognition results for the computer vision
[1934.18s -> 1935.74s]  algorithms.
[1935.74s -> 1938.98s]  So here's the example picture.
[1938.98s -> 1950.86s]  And if an algorithm can output five labels and the top five labels includes the correct
[1950.86s -> 1957.38s]  object in this picture, then we call this a success.
[1957.38s -> 1968.06s]  So here is a result summary of the ImageNet challenge, of the image classification result
[1968.06s -> 1972.06s]  from 2010 to 2015.
[1972.06s -> 1975.98s]  So on the x-axis, you see the years.
[1975.98s -> 1980.80s]  On the y-axis, you see the error rate.
[1980.80s -> 1985.56s]  So the good news is the error rate is steadily decreasing.
[1985.56s -> 1993.08s]  To the point by 2012, the error rate is so low, it's on par with what humans can
[1993.08s -> 1994.08s]  do.
[1994.08s -> 2004.60s]  And here, by human, I mean a single Stanford PhD student who spent weeks doing this task
[2004.60s -> 2009.90s]  as if he were a computer participating in the ImageNet challenge.
[2009.90s -> 2014.36s]  So that's a lot of progress made.
[2014.36s -> 2021.28s]  Even though we have not solved all the problems of object recognition, which you'll learn
[2021.28s -> 2030.34s]  about in this class, but to go from an error rate that's unacceptable for real world application
[2030.34s -> 2036.74s]  all the way to being on par with humans in ImageNet challenge, the field took only
[2036.74s -> 2038.98s]  a few years.
[2038.98s -> 2050.90s]  And one particular moment you should notice on this graph is the year 2012.
[2050.90s -> 2056.82s]  In the first two years, our error rate hovered around 25 percent.
[2056.82s -> 2066.22s]  But in 2012, the error rate was dropped almost 10 percent to 16 percent.
[2066.22s -> 2072.24s]  Even though now it's better, but that drop was very significant.
[2072.24s -> 2082.34s]  And the winning algorithm of that year is a convolutional neural network model that
[2082.34s -> 2089.92s]  beat all other algorithms around that time to win the ImageNet challenge.
[2089.92s -> 2098.44s]  And this is the focus of our whole course this quarter, is to look at, to have a deep
[2098.44s -> 2103.84s]  dive into what convolutional neural network models are.
[2103.84s -> 2113.46s]  And another name for this is deep learning by popular name now is called deep learning.
[2113.46s -> 2119.88s]  And to look at what these models are, what are the principles, what are the good practices,
[2119.88s -> 2122.64s]  what are the recent progress of this model.
[2122.64s -> 2131.04s]  So, but here is where the history was made is that we, around 2012, convolutional
[2131.04s -> 2137.68s]  neural network model or deep learning models showed the tremendous capacity and ability
[2137.68s -> 2145.56s]  in making good progress in the field of computer vision along with several other sister
[2145.56s -> 2149.92s]  fields like natural language processing and speech recognition.
[2149.92s -> 2158.48s]  So without further ado, I'm going to hand the rest of the lecture to Justin to talk
[2158.48s -> 2163.12s]  about the overview of CS231n.
[2163.12s -> 2164.12s]  All right.
[2164.12s -> 2165.26s]  Thanks so much Fei-Fei.
[2165.26s -> 2168.36s]  I think that I'll take it over from here.
[2168.36s -> 2172.50s]  So now I want to shift gears a little bit and talk a little bit more about this class
[2172.50s -> 2175.82s]  CS231n.
[2175.82s -> 2181.70s]  So this class focuses on one of these most, so the primary focus of this class is this
[2181.70s -> 2186.30s]  image classification problem, which we previewed a little bit in the context of the ImageNet
[2186.30s -> 2187.44s]  challenge.
[2187.44s -> 2191.82s]  So in image classification, again, the setup is that your algorithm looks at an image
[2191.82s -> 2197.26s]  and then picks from among some fixed set of categories to classify that image.
[2197.26s -> 2201.46s]  And this seems like, this might seem like somewhat of a restrictive or artificial setup,
[2201.46s -> 2206.34s]  but it's actually quite general and this problem can be applied in many different settings,
[2206.34s -> 2209.90s]  both in industry and academia and many different places.
[2209.90s -> 2214.34s]  So for example, you could apply this to recognizing food or recognizing calories
[2214.34s -> 2218.50s]  in food or recognizing different artworks, different products out in the world.
[2218.50s -> 2224.42s]  So this relatively basic tool of image classification is super useful on its own and could be applied
[2224.42s -> 2229.18s]  all over the place for many different applications.
[2229.18s -> 2234.44s]  But in this course, we're also going to talk about several other visual recognition problems
[2234.44s -> 2240.14s]  that build upon many of the tools that we develop for the purpose of image classification.
[2240.14s -> 2245.10s]  We'll talk about other problems such as object detection or image captioning.
[2245.10s -> 2248.96s]  So the setup in object detection is a little bit different.
[2248.96s -> 2254.02s]  Rather than classifying an entire image as a cat or a dog or a horse or whatnot,
[2254.02s -> 2257.82s]  instead we want to go in and draw bounding boxes and say that there is a dog here and
[2257.82s -> 2264.54s]  a cat here and a carb over in the background and draw these boxes describing where objects are in the image.
[2264.54s -> 2269.10s]  We'll also talk about image captioning, where given an image, the system now needs to produce
[2269.10s -> 2271.82s]  a natural language sentence describing the image.
[2271.82s -> 2276.66s]  It sounds like a really hard, complicated, and different problem, but we'll see that many of the tools
[2276.66s -> 2286.62s]  we develop in service of image classification will be reused in these other problems as well.
[2287.14s -> 2290.82s]  We mentioned this before in the context of the ImageNet challenge, but one of the things
[2290.82s -> 2294.78s]  that's really driven the progress of the field in recent years has been this adoption
[2294.78s -> 2300.90s]  of convolutional neural networks, or CNNs, or sometimes called convnets.
[2300.90s -> 2306.34s]  If we look at the algorithms that have won the ImageNet challenge for the last several
[2306.34s -> 2313.04s]  years, in 2011, we see this method from Lin et al., which is still hierarchical.
[2313.08s -> 2317.76s]  It consists of multiple layers, so first we compute some features, next we compute some
[2317.76s -> 2322.86s]  local invariances, some pooling, and go through several layers of processing, and then finally
[2322.86s -> 2327.38s]  feed this resulting descriptor to a linear SVM.
[2327.38s -> 2329.56s]  What you'll notice here is that this is still hierarchical.
[2329.56s -> 2333.44s]  We're still detecting edges, we're still having notions of invariance, and many of
[2333.44s -> 2336.64s]  these intuitions will carry over into convnets.
[2336.64s -> 2344.42s]  But the breakthrough moment was really in 2012 when Jeff Hinton's group in Toronto, together
[2344.42s -> 2349.64s]  with Alex Kuchevsky and Ilya Sotskovar, who were his PhD students at that time, created
[2349.64s -> 2355.52s]  this seven-layer convolutional neural network, now known as AlexNet, then called SuperVision,
[2355.52s -> 2360.40s]  which just did very, very well in the ImageNet competition in 2012.
[2360.40s -> 2364.64s]  Since then, every year, the winner of ImageNet has been a neural network.
[2364.64s -> 2368.44s]  The trend has been that these networks are getting deeper and deeper each year.
[2368.44s -> 2373.08s]  So AlexNet was a seven- or eight-layer neural network, depending on how exactly you count
[2373.08s -> 2374.08s]  things.
[2374.08s -> 2379.64s]  In 2015, we had these much deeper networks, GoogleNet from Google, and the VGG network
[2379.64s -> 2383.40s]  from Oxford, which was about 19 layers at that time.
[2383.40s -> 2388.30s]  Then in 2015, it got really crazy, and this paper came out from Microsoft Research
[2388.30s -> 2393.56s]  Asia called Residual Networks, which were 152 layers at that time.
[2393.56s -> 2397.20s]  Since then, it turns out you can get a little bit better if you go up to 200, but you run
[2397.20s -> 2400.68s]  out of memory on your GPUs, so we'll get into all of that later.
[2400.68s -> 2404.52s]  But the main takeaway here is that convolutional neural networks really had this breakthrough
[2404.52s -> 2408.70s]  moment in 2012, and since then, there's been a lot of effort focused on tuning and
[2408.70s -> 2412.20s]  tweaking these algorithms to make them perform better and better on this problem of
[2412.20s -> 2413.96s]  image classification.
[2413.96s -> 2417.48s]  Throughout the rest of the quarter, we're going to really dive in deep, and you'll
[2417.48s -> 2422.96s]  understand exactly how these different models work.
[2422.96s -> 2427.80s]  But one point that's really important is that it's true that the breakthrough moment
[2427.80s -> 2432.24s]  for convolutional neural networks was in 2012, when these networks performed very
[2432.24s -> 2436.82s]  well on the ImageNet challenge, but they certainly weren't invented in 2012.
[2436.82s -> 2442.80s]  These algorithms had actually been around for quite a long time before that.
[2442.80s -> 2446.92s]  One of the foundational works in this area of convolutional neural networks was actually
[2446.92s -> 2454.32s]  in the 90s from Yann LeCun and collaborators, who at that time were at Bell Labs.
[2454.32s -> 2459.78s]  In 1998, they built this convolutional neural network for recognizing digits.
[2459.78s -> 2465.64s]  They wanted to deploy this and be able to automatically recognize handwritten checks
[2465.64s -> 2469.60s]  or addresses for the post office, and they built this convolutional neural network
[2469.60s -> 2475.00s]  which could take in the pixels of an image and then classify either what digit it was
[2475.00s -> 2478.32s]  or what letter it was or whatnot.
[2478.32s -> 2482.20s]  The structure of this network actually looks pretty similar to the AlexNet architecture
[2482.20s -> 2484.04s]  that was used in 2012.
[2484.04s -> 2488.06s]  Here we see that we're taking in these raw pixels, we have many layers of convolution
[2488.06s -> 2492.32s]  and subsampling together with these so-called fully connected layers, all of which will
[2492.32s -> 2495.04s]  be explained in much more detail later in the course.
[2495.04s -> 2499.12s]  But if you just kind of look at these two pictures, they look pretty similar.
[2499.12s -> 2504.64s]  And this architecture in 2012 sort of has a lot of these architectural similarities
[2504.64s -> 2509.60s]  that are shared with this network going back to the 90s.
[2509.60s -> 2513.80s]  So then the question you might ask is, okay, if these algorithms were around since the
[2513.80s -> 2517.82s]  90s, why have they only suddenly become popular in the last couple of years?
[2517.82s -> 2522.24s]  And there's a couple really key innovations that happened that have changed since the
[2522.24s -> 2523.64s]  90s.
[2523.64s -> 2525.76s]  One is computation.
[2525.76s -> 2529.64s]  Thanks to Moore's Law, we've gotten faster and faster computers every year.
[2529.64s -> 2533.36s]  And this is kind of a course measure, but if you just look at the number of transistors
[2533.36s -> 2537.80s]  that are on chips, then that has grown by several orders of magnitude between the 90s
[2537.80s -> 2539.44s]  and today.
[2539.44s -> 2545.48s]  We've also had this advent of graphics processing units, or GPUs, which are super
[2545.48s -> 2550.48s]  paralyzable and ended up being a perfect tool for really crunching these computationally
[2550.48s -> 2553.28s]  intensive convolutional neural network models.
[2553.28s -> 2558.76s]  So just by having more compute available, it allowed researchers to explore with larger
[2558.76s -> 2564.08s]  architectures and larger models, and in some cases just increasing the model size but still
[2564.08s -> 2568.04s]  using these kind of classical approaches and classical algorithms tends to work quite
[2568.04s -> 2569.16s]  well.
[2569.16s -> 2576.36s]  So this idea of increasing computation is super important in the history of deep learning.
[2576.36s -> 2581.68s]  I think the second key innovation that changed between now and the 90s was data.
[2581.68s -> 2584.60s]  So these algorithms are very hungry for data.
[2584.60s -> 2588.32s]  You need to feed them a lot of labeled images and labeled pixels for them to eventually
[2588.32s -> 2590.40s]  work quite well.
[2590.40s -> 2594.72s]  And in the 90s, there just wasn't that much labeled data available.
[2594.72s -> 2599.44s]  This was, again, before tools like Mechanical Turk, before the internet was super, super
[2599.44s -> 2604.64s]  widely used, and it was very difficult to collect large, varied data sets.
[2604.64s -> 2610.82s]  But now, in the 2010s, with data sets like Pascal and ImageNet, there existed these
[2610.82s -> 2616.12s]  relatively large, high-quality labeled data sets that were, again, orders of magnitude
[2616.12s -> 2619.04s]  bigger than the data sets available in the 90s.
[2619.04s -> 2623.52s]  And these much larger data sets, again, allowed us to work with higher-capacity models and
[2623.52s -> 2627.76s]  train these models to actually work quite well on real-world problems.
[2627.76s -> 2632.60s]  But the critical takeaway here is that convolutional neural networks, although they seem like
[2632.60s -> 2636.56s]  this fancy new thing that's only popped up in the last couple of years, that's really
[2636.56s -> 2637.96s]  not the case.
[2637.96s -> 2641.48s]  And these class of algorithms have existed for quite a long time in their own right
[2641.48s -> 2645.68s]  as well.
[2645.68s -> 2649.96s]  Another thing I'd like to point out is that, in computer vision, we're in the business
[2649.96s -> 2654.48s]  of trying to build machines that can see people, and people can actually do a lot
[2654.48s -> 2657.92s]  of amazing things with their visual systems.
[2657.92s -> 2661.68s]  When you go around the world, you do a lot more than just drawing boxes around
[2661.68s -> 2665.40s]  the objects and classifying things as cats or dogs.
[2665.40s -> 2668.08s]  Your visual system is much more powerful than that.
[2668.08s -> 2671.72s]  And as we move forward in the field, I think there's still a ton of open challenges
[2671.72s -> 2677.12s]  and open problems that we need to address, and we need to continue to develop our algorithms
[2677.12s -> 2680.64s]  to do even better and tackle more ambitious problems.
[2680.64s -> 2684.48s]  Some examples of this are going back to these older ideas, in fact.
[2684.48s -> 2688.72s]  Things like semantic segmentation or perceptual grouping, where rather than labeling the
[2688.72s -> 2692.76s]  entire image, we want to understand, for every pixel in the image, what is it doing?
[2692.76s -> 2694.12s]  What does it mean?
[2694.12s -> 2697.20s]  And we'll revisit that idea a little bit later in the course.
[2697.20s -> 2701.36s]  There's definitely work going back to this idea of 3D understanding, of reconstructing
[2701.36s -> 2707.72s]  the entire world, and that's still an unsolved problem, I think.
[2707.72s -> 2709.84s]  There are just tons and tons of other tasks you can imagine.
[2709.84s -> 2711.92s]  For example, activity recognition.
[2711.92s -> 2715.88s]  If I'm given a video of some person doing some activity, what's the best way to recognize
[2715.88s -> 2717.08s]  that activity?
[2717.08s -> 2719.68s]  And that's quite a challenging problem as well.
[2719.68s -> 2723.56s]  And then, as we move forward with things like augmented reality and virtual reality,
[2723.56s -> 2727.56s]  and as new technologies and new types of sensors become available, I think we'll come
[2727.56s -> 2734.72s]  up with a lot of new, interesting, hard, and challenging problems to tackle as a field.
[2734.72s -> 2740.92s]  This is an example that is from some of my own work in the vision lab on this dataset
[2740.92s -> 2742.84s]  called Visual Genome.
[2742.84s -> 2747.84s]  Here the idea is that we're trying to capture some of these intricacies in the real world,
[2747.84s -> 2752.78s]  and rather than maybe describing just boxes, maybe we should be describing images as
[2752.78s -> 2758.14s]  these whole large graphs of semantically-related concepts that encompass not just object identities,
[2758.14s -> 2763.70s]  but also object relationships, object attributes, actions that are occurring in the scene, and
[2763.70s -> 2768.86s]  this type of representation might allow us to capture some of this richness of the
[2768.86s -> 2773.30s]  visual world that's left on the table when we're using simple classification.
[2773.30s -> 2776.90s]  This is by no means a standard approach at this point, but just kind of giving you
[2776.90s -> 2781.38s]  this sense that there's so much more that your visual system can do that is maybe not
[2781.42s -> 2785.46s]  captured in this vanilla image classification setup.
[2785.46s -> 2792.54s]  One, I think, another really interesting work that points in this direction actually comes
[2792.54s -> 2797.78s]  from Fei Fei's grad school days when she was doing her PhD at Caltech with her
[2797.78s -> 2799.06s]  advisors there.
[2799.06s -> 2803.34s]  In this setup, they had people, they stuck people, and they showed people this image
[2803.34s -> 2806.86s]  for just half a second, so they flashed this image in front of them for just a very
[2806.86s -> 2808.58s]  short period of time.
[2808.62s -> 2812.66s]  Even in this very, very rapid exposure to an image, people were able to write these
[2812.66s -> 2817.50s]  long, descriptive paragraphs, giving a whole story of the image.
[2817.50s -> 2823.24s]  This is quite remarkable if you think about it, that after just half a second of looking
[2823.24s -> 2827.90s]  at this image, a person was able to say that this is some kind of a game or fight,
[2827.90s -> 2831.44s]  two groups of men, the man on the left is throwing something outdoors because it
[2831.44s -> 2834.82s]  seems like I have an impression of grass, and so on and so on.
[2834.82s -> 2838.02s]  You can imagine that if a person were to look even longer at this image, they could
[2838.02s -> 2841.62s]  write probably a whole novel about who these people are and why are they in this field
[2841.62s -> 2844.94s]  playing this game, and they could go on and on and on, roping in things from their
[2844.94s -> 2847.82s]  external knowledge and their prior experience.
[2847.82s -> 2852.30s]  This is, in some sense, the holy grail of computer vision, to understand the story
[2852.30s -> 2855.02s]  of an image in a very rich and deep way.
[2855.02s -> 2858.86s]  I think that despite the massive progress in the field that we've had over the past
[2858.86s -> 2864.86s]  several years, we're still quite a long way from achieving this holy grail.
[2864.86s -> 2868.78s]  Another image that I think really exemplifies this idea actually comes, again, from
[2868.78s -> 2873.16s]  Andrej Karpathy's blog, is this amazing image.
[2873.16s -> 2874.38s]  Many of you smiled, many of you laughed.
[2874.38s -> 2876.30s]  I think this is a pretty funny image.
[2876.30s -> 2877.62s]  But why is it a funny image?
[2877.62s -> 2881.98s]  Well, we've got a man standing on a scale, and we know that people are kind of self-conscious
[2881.98s -> 2886.30s]  about their weight sometimes, and scales measure weight, and then we've got this other
[2886.30s -> 2890.30s]  guy behind him pushing his foot down on the scale, and we know that because of the
[2890.30s -> 2894.38s]  way scales work, that will cause him to have an inflated reading on the scale,
[2894.38s -> 2895.38s]  but there's more.
[2895.38s -> 2897.38s]  We know that this person is not just any person.
[2897.38s -> 2901.06s]  This is actually Barack Obama, who was at the time President of the United States, and
[2901.06s -> 2904.58s]  we know that Presidents of the United States are supposed to be respectable politicians
[2904.58s -> 2911.72s]  that are probably not supposed to be playing jokes on their compatriots in this way.
[2911.72s -> 2914.78s]  We know that there's these people in the background that are laughing and smiling,
[2914.78s -> 2918.14s]  and we know that that means that they're understanding something about the scene.
[2918.14s -> 2921.46s]  We have some understanding that they know that President Obama is this respectable
[2921.46s -> 2923.12s]  guy who's looking at this other guy.
[2923.12s -> 2924.30s]  This is crazy.
[2924.30s -> 2928.82s]  There's so much going on in this image, and our computer vision algorithms today are actually
[2928.82s -> 2935.26s]  a long way, I think, from this true, deep understanding of images, so I think that despite
[2935.26s -> 2939.48s]  the massive progress in the field, we really have a long way to go.
[2939.48s -> 2943.18s]  To me, that's really exciting as a researcher because I think that we'll have just a lot
[2943.18s -> 2948.38s]  of really exciting, cool problems to tackle moving forward.
[2948.38s -> 2952.22s]  I hope at this point I've done a relatively good job to convince you that computer vision
[2952.22s -> 2957.50s]  is really interesting, it's really exciting, it can be very useful, it can go out and
[2957.50s -> 2960.50s]  make the world a better place in various ways.
[2960.50s -> 2965.10s]  Computer vision could be applied in places like medical diagnosis and self-driving cars
[2965.10s -> 2970.66s]  and robotics and all these different places, in addition to tying back to this core
[2970.66s -> 2973.64s]  idea of understanding human intelligence.
[2973.64s -> 2977.26s]  To me, I think that computer vision is this fantastically amazing, interesting field,
[2977.26s -> 2980.66s]  and I'm really glad that over the course of the quarter we'll get to really dive in
[2980.66s -> 2986.50s]  and dig into all these different details about how these algorithms are working these days.
[2986.50s -> 2990.22s]  So that's sort of my pitch about computer vision and about the history of computer
[2990.22s -> 2991.22s]  vision.
[2991.22s -> 2994.94s]  I don't know if there's any questions about this at this time?
[2994.94s -> 2997.22s]  Okay.
[2997.22s -> 3001.06s]  So then I want to talk a little bit more about the logistics of this class for the
[3001.06s -> 3002.60s]  rest of the quarter.
[3002.60s -> 3004.66s]  So you might ask, who are we?
[3004.66s -> 3011.42s]  So this class is taught by Fei-Fei Li, who's a professor of computer science here at Stanford,
[3011.42s -> 3017.78s]  who's my advisor, and director of the Stanford Vision Lab and also the Stanford AI Lab.
[3017.78s -> 3021.74s]  The other two instructors are me, Justin Johnson, and Serena Young, who's up here
[3021.74s -> 3023.14s]  in the front.
[3023.14s -> 3028.34s]  We're both PhD students working under Fei-Fei on various computer vision problems.
[3028.34s -> 3033.22s]  We have an amazing teaching staff this year of 18 TAs so far, many of whom are sitting
[3033.22s -> 3034.74s]  over here in the front.
[3034.74s -> 3038.90s]  These guys are really the unsung heroes behind the scenes, making the course run smoothly,
[3038.90s -> 3040.66s]  making sure everything happens well.
[3040.66s -> 3044.42s]  So be nice to them.
[3044.42s -> 3048.80s]  I think I also should mention that this is the third time we've taught this course,
[3048.80s -> 3053.34s]  and it's the first time that Andrej Karpathy has not been an instructor in this course.
[3053.34s -> 3056.54s]  He was a very close friend of mine.
[3056.54s -> 3057.54s]  He's still alive.
[3057.54s -> 3058.54s]  He's okay.
[3058.54s -> 3059.74s]  Don't worry.
[3059.90s -> 3065.70s]  He graduated, so he's actually here, I think, hanging around in lecture hall.
[3065.70s -> 3069.42s]  A lot of the development and the history of this course is really due to him working
[3069.42s -> 3075.70s]  on it with me over the last couple of years, so I think you should be aware of that.
[3075.70s -> 3080.62s]  Also about logistics, probably the best way for keeping in touch with the course
[3080.62s -> 3082.50s]  staff is through Piazza.
[3082.50s -> 3085.34s]  You should all go and sign up right now.
[3085.38s -> 3091.06s]  Piazza is really our preferred method of communication with the class, with the teaching staff.
[3091.06s -> 3094.50s]  If you have questions that you're afraid of being embarrassed about asking in front
[3094.50s -> 3099.58s]  of your classmates, go ahead and ask anonymously, or even post private questions directly to
[3099.58s -> 3101.58s]  the teaching staff.
[3101.58s -> 3104.86s]  Basically anything that you need should ideally go through Piazza.
[3104.86s -> 3111.18s]  We also have a staff mailing list, but we ask that this is mostly for personal, confidential
[3111.18s -> 3114.08s]  things that you don't want going on Piazza.
[3114.08s -> 3119.12s]  Or if you have something that's super confidential, super personal, then feel free to directly
[3119.12s -> 3122.36s]  email me or Fei-Fei or Serena about that.
[3122.36s -> 3126.60s]  But for the most part, most of your communication with the staff should be through Piazza.
[3126.60s -> 3129.12s]  We also have an optional textbook this year.
[3129.12s -> 3130.80s]  This is by no means required.
[3130.80s -> 3132.88s]  You can go through the course totally fine without it.
[3132.88s -> 3134.84s]  Everything will be self-contained.
[3134.84s -> 3139.40s]  This is sort of exciting because it's maybe the first textbook about deep learning that
[3139.40s -> 3144.72s]  got published earlier this year by Ian Goodfellow, Yoshua Bengio, and Aaron Corville.
[3144.72s -> 3146.88s]  I put the Amazon link here in the slides.
[3146.88s -> 3148.68s]  You can go get it if you want to.
[3148.68s -> 3151.80s]  But also the whole content of the book is free online, so you don't even have to
[3151.80s -> 3153.16s]  buy it if you don't want to.
[3153.16s -> 3155.92s]  So again, this is totally optional, but we'll probably be posting some readings throughout
[3155.92s -> 3162.24s]  the quarter that give you an additional perspective on some of the material.
[3162.24s -> 3167.60s]  Our philosophy about this class is that you should really understand the deep mechanics
[3167.60s -> 3169.00s]  of all of these algorithms.
[3169.00s -> 3173.04s]  You should understand at a very deep level exactly how these algorithms are working, like
[3173.04s -> 3177.16s]  what exactly is going on when you're stitching together these neural networks, how do these
[3177.16s -> 3181.56s]  architectural decisions influence how the network is trained and tested and whatnot
[3181.56s -> 3182.64s]  and all that.
[3182.64s -> 3187.40s]  And throughout the course, through the assignments, you'll be implementing your own convolutional
[3187.40s -> 3189.88s]  neural networks from scratch in Python.
[3189.88s -> 3193.08s]  You'll be implementing the full forward and backward passes through these things, and
[3193.08s -> 3195.84s]  by the end you'll have implemented a whole convolutional neural network totally on your
[3195.84s -> 3196.84s]  own.
[3196.84s -> 3198.58s]  I think that's really cool.
[3198.58s -> 3203.22s]  But we're also kind of practical, and we know that in most cases, people are probably
[3203.22s -> 3206.58s]  not writing these things from scratch, so we also want to give you a good introduction
[3206.58s -> 3210.54s]  to some of the state-of-the-art software tools that are used in practice for these
[3210.54s -> 3211.54s]  things.
[3211.54s -> 3214.78s]  So we're going to talk about some of the state-of-the-art software packages, like
[3214.78s -> 3220.66s]  TensorFlow, Torch, PyTorch, all these other things, and I think you'll get some
[3220.66s -> 3225.46s]  exposure to those on the homeworks and definitely through the course project as well.
[3225.46s -> 3227.96s]  Another note about this course is that it's very state-of-the-art.
[3227.96s -> 3228.96s]  I think it's super exciting.
[3228.96s -> 3231.08s]  This is a very fast-moving field.
[3231.08s -> 3235.54s]  As you saw, even these plots in the ImageNet Challenge, basically there's been a ton of
[3235.54s -> 3239.96s]  progress since 2012, and while I've been in grad school, the whole field is sort
[3239.96s -> 3244.44s]  of transforming every year, and that's super exciting and super encouraging.
[3244.44s -> 3248.92s]  What that means is that there's probably content that we'll cover this year that did
[3248.92s -> 3252.88s]  not exist the last time this course was taught last year.
[3252.88s -> 3255.76s]  I think that's super exciting, and that's one of my favorite parts about teaching
[3255.76s -> 3260.76s]  this course, is just roping in all these new, scientific, hot-off-the-presses stuff
[3260.76s -> 3264.80s]  and then being able to present it to you guys.
[3264.80s -> 3268.36s]  We're also sort of about fun, so we're going to talk about some interesting, maybe
[3268.36s -> 3273.44s]  not-so-serious topics as well this quarter, including image captioning is pretty fun,
[3273.44s -> 3276.80s]  where we can write descriptions about images, but we'll also cover some of these more
[3276.80s -> 3281.88s]  artistic things, like Deep Dream here on the left, where we can use neural networks
[3281.88s -> 3285.80s]  to hallucinate these crazy, psychedelic images, and by the end of the course, you'll know
[3285.80s -> 3290.14s]  how that works, or on the right, this idea of style transfer, where we can take an image
[3290.14s -> 3295.36s]  and render it in the style of famous artists like Picasso or Van Gogh or whatnot,
[3295.36s -> 3301.08s]  and again, by the end of the quarter, you'll see how this stuff works.
[3301.08s -> 3304.36s]  The way the course works is that we're going to have three problem sets.
[3304.36s -> 3308.76s]  The first problem set will hopefully be out by the end of the week.
[3308.76s -> 3312.84s]  You'll have an in-class, written midterm exam, and a large portion of your grade will
[3312.84s -> 3317.48s]  be the final course project, where you'll work in teams of one to three and produce
[3317.48s -> 3320.88s]  some amazing project that will blow everyone's minds.
[3320.88s -> 3325.08s]  We have a late policy, so you have seven late days that you're free to allocate
[3325.08s -> 3327.56s]  among your different homeworks.
[3327.56s -> 3333.64s]  These are meant to cover things like minor illnesses or traveling or conferences or anything
[3333.64s -> 3334.64s]  like that.
[3334.64s -> 3337.88s]  If you come to us at the end of the quarter and say that, oh, I suddenly have
[3337.88s -> 3341.20s]  to go give a presentation at this conference, that's not going to be okay, that's what your
[3341.20s -> 3342.88s]  late days are for.
[3342.88s -> 3346.72s]  That being said, if you have some very extenuating circumstances, then do feel free
[3346.72s -> 3350.48s]  to email the course staff if you have some extreme circumstances about that.
[3350.48s -> 3354.54s]  Finally, I want to make a note about the collaboration policy.
[3354.54s -> 3358.52s]  As Stanford students, you should all be aware of the honor code that governs the way
[3358.52s -> 3361.96s]  that you should be collaborating and working together.
[3361.96s -> 3364.12s]  We take this very seriously.
[3364.12s -> 3367.80s]  We encourage you to think very carefully about how you're collaborating and making sure it's
[3367.80s -> 3372.76s]  within the bounds of the honor code.
[3372.76s -> 3377.92s]  In terms of prerequisites, I think the most important is probably a deep familiarity with
[3377.92s -> 3383.36s]  Python because all of the programming assignments will be in Python.
[3383.36s -> 3386.70s]  Some familiarity with C or C++ would be useful.
[3386.70s -> 3391.28s]  You will probably not be writing any C or C++ in this course, but as you're browsing
[3391.28s -> 3395.64s]  through the source code of these various software packages, being able to read C++
[3395.64s -> 3399.48s]  is very useful for understanding how these packages work.
[3399.48s -> 3402.20s]  We also assume that you know what calculus is.
[3402.20s -> 3404.60s]  You know how to take derivatives, all that sort of stuff.
[3404.60s -> 3408.00s]  We assume some linear algebra that you know what matrices are and how to multiply
[3408.00s -> 3410.84s]  them and stuff like that.
[3410.84s -> 3414.92s]  We can't be teaching you how to take derivatives and stuff.
[3414.92s -> 3419.04s]  We also assume a little bit of knowledge coming in of computer vision, maybe at the
[3419.04s -> 3421.60s]  level of CS 131 or 231A.
[3421.60s -> 3424.56s]  If you have taken those courses before, you'll be fine.
[3424.60s -> 3427.52s]  If you haven't, I think you'll be okay in this class.
[3427.52s -> 3430.92s]  You might have a tiny bit of catching up to do, but I think you'll probably be okay.
[3430.92s -> 3434.24s]  Those are not super strict prerequisites.
[3434.24s -> 3437.68s]  We also assume a little bit of background knowledge about machine learning,
[3437.68s -> 3440.52s]  maybe at the level of CS 229.
[3440.52s -> 3444.56s]  But again, I think really important key fundamental machine learning concepts
[3444.56s -> 3447.20s]  will reintroduce as they come up and become important.
[3447.20s -> 3451.20s]  But that being said, a familiarity with these things will be helpful going forward.
[3452.04s -> 3453.36s]  We have a course website.
[3453.36s -> 3454.20s]  Go check it out.
[3454.20s -> 3457.96s]  There's a lot of information and links and syllabus and all that.
[3457.96s -> 3461.40s]  I think that's all that I really want to cover today.
[3461.40s -> 3466.16s]  Then later this week, on Thursday, we'll really dive into our first learning algorithm
[3466.16s -> 3468.16s]  and start diving into the details of these things.
