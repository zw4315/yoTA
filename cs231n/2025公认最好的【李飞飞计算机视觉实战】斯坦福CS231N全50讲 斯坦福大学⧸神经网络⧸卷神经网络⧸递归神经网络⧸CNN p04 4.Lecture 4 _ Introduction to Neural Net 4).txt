# Detected language: en (p=1.00)

[0.00s -> 1.74s]  Stanford University.
[10.38s -> 12.48s]  Okay, so good afternoon, everyone.
[12.48s -> 13.98s]  Let's get started.
[15.18s -> 18.04s]  So, hi, so for those of you who I haven't met yet,
[18.04s -> 19.84s]  my name is Serena Young,
[19.84s -> 24.68s]  and I'm the third and final instructor for this class.
[24.68s -> 28.12s]  And I'm also a PhD student in FEFE's group.
[28.16s -> 31.16s]  Okay, so today we're going to talk about backpropagation
[31.16s -> 32.74s]  and neural networks.
[32.74s -> 34.40s]  And so now we're really starting to get to some
[34.40s -> 37.12s]  of the core material in this class.
[37.12s -> 39.70s]  Before we begin, let's see, oh.
[41.92s -> 44.00s]  So a few administrative details.
[44.00s -> 47.44s]  So assignment one is due Thursday, April 20th.
[47.44s -> 51.32s]  So reminder, we've shifted the date back by a little bit.
[51.32s -> 55.48s]  And it's going to be due 11.59 p.m. on Canvas.
[56.46s -> 59.08s]  So you should start thinking about your projects.
[59.08s -> 62.10s]  There are TA specialties listed on the Piazza website,
[62.10s -> 65.18s]  so if you have questions about a specific project topic
[65.18s -> 68.80s]  you're thinking about, you can go and try and find
[68.80s -> 72.38s]  the TAs that might be most relevant.
[72.38s -> 74.10s]  And then also for Google Cloud,
[74.10s -> 77.34s]  so all students are going to get $100 in credits
[77.34s -> 81.02s]  to use for Google Cloud for their assignments and project.
[81.02s -> 83.20s]  So you should be receiving an email for that this week.
[83.20s -> 85.06s]  I think a lot of you may have already,
[85.48s -> 87.00s]  and then for those of you who haven't,
[87.00s -> 89.68s]  they're going to come, should be by the end of this week.
[92.40s -> 95.48s]  Okay, so where we are, so far we've talked about
[95.48s -> 99.00s]  how to define a classifier using a function f
[99.00s -> 100.76s]  parametrized by weights w.
[100.76s -> 103.72s]  And this function f is going to take data x as input
[103.72s -> 108.72s]  and output a vector of scores for each of the classes
[109.72s -> 111.24s]  that you want to classify.
[111.42s -> 115.42s]  So from here we can also define a loss function,
[115.42s -> 117.14s]  so for example the SVM loss function
[117.14s -> 120.18s]  that we've talked about, which basically quantifies
[120.18s -> 121.78s]  how happy or unhappy we are
[121.78s -> 123.98s]  with the scores that we've produced.
[123.98s -> 127.50s]  And then we can use that to define a total loss term,
[127.50s -> 130.70s]  so L here, which is a combination of this data term
[130.70s -> 132.34s]  combined with a regularization term
[132.34s -> 135.74s]  that expresses how simple our model is,
[135.74s -> 138.10s]  and we have a preference for simpler models
[138.10s -> 139.98s]  for better generalization.
[140.00s -> 143.08s]  And so now we want to find the parameters w
[143.08s -> 145.00s]  that correspond to our lowest loss, right,
[145.00s -> 146.92s]  we want to minimize the loss function.
[146.92s -> 149.04s]  And so to do that, we want to find the gradient
[149.04s -> 150.92s]  of L with respect to w.
[153.04s -> 155.64s]  So last lecture we talked about how we can do this
[155.64s -> 158.28s]  using optimization, where we want to iteratively
[158.28s -> 160.96s]  take steps in the direction of steepest descent,
[160.96s -> 162.60s]  which is the negative of the gradient,
[162.60s -> 164.60s]  in order to walk down this loss landscape
[164.60s -> 166.84s]  and get to the point of lowest loss.
[167.02s -> 170.54s]  And we saw how this gradient descent
[170.54s -> 172.46s]  can basically take this trajectory
[172.46s -> 174.54s]  looking like this image on the right,
[174.54s -> 177.02s]  getting to the bottom of your loss landscape.
[182.06s -> 184.26s]  Okay, and so we also talked about different ways
[184.26s -> 185.94s]  for computing the gradient.
[185.94s -> 188.34s]  We can compute this numerically
[188.34s -> 190.50s]  using finite difference approximation,
[190.50s -> 192.46s]  which is slow and approximate,
[192.46s -> 194.50s]  but at the same time it's really easy to write out,
[194.52s -> 197.36s]  we know you can always get the gradient this way.
[197.36s -> 200.72s]  We also talked about how to use the analytic gradient,
[200.72s -> 203.52s]  right, and computing this is, it's fast and exact
[203.52s -> 204.96s]  once you've gotten the expression
[204.96s -> 207.12s]  for the analytic gradient, but at the same time
[207.12s -> 208.68s]  you have to do all the math and the calculus
[208.68s -> 212.32s]  to derive this, so it's also easy to make mistakes.
[212.32s -> 213.88s]  Right, so in practice what we want to do
[213.88s -> 217.36s]  is we want to derive the analytic gradient and use this,
[217.36s -> 219.52s]  but at the same time check our implementation
[219.52s -> 221.16s]  using the numerical gradient to make sure
[221.16s -> 223.16s]  that we've gotten all of our math right.
[225.48s -> 228.32s]  So today we're going to talk about how to compute
[228.32s -> 232.04s]  the analytic gradient for arbitrarily complex functions
[232.04s -> 233.88s]  using a framework that I'm going to call
[233.88s -> 236.92s]  computational graphs, and so basically
[236.92s -> 239.04s]  what a computational graph is is that we can use
[239.04s -> 242.28s]  this kind of graph in order to represent any function
[242.28s -> 245.08s]  where the nodes of the graph are steps of computation
[245.08s -> 246.36s]  that we go through.
[246.36s -> 248.84s]  So for example, in this example of a linear classifier
[248.84s -> 253.84s]  that we've talked about, the inputs here are X and W,
[254.26s -> 256.90s]  right, and then this multiplication node
[256.90s -> 259.38s]  represents the matrix multiplier,
[259.38s -> 262.70s]  the multiplication of the parameters W
[262.70s -> 264.78s]  with our data X that we have,
[264.78s -> 267.26s]  outputting our vector of scores,
[267.26s -> 269.22s]  and then we have another computational node
[269.22s -> 271.70s]  which represents our hinge loss, right,
[271.70s -> 274.70s]  computing our data loss term, LI,
[274.70s -> 278.38s]  and we also have this regularization term at the bottom,
[278.38s -> 282.74s]  right, so this node which computes our regularization term
[282.74s -> 284.72s]  and then our total loss here at the end, L,
[284.72s -> 288.88s]  is the sum of the regularization term and the data term.
[290.24s -> 292.68s]  And the advantage is that once we can express a function
[292.68s -> 295.44s]  using a computational graph, then we can use a technique
[295.44s -> 297.92s]  that we call backpropagation,
[297.92s -> 300.04s]  which is going to recursively use the chain rule
[300.04s -> 302.68s]  in order to compute the gradient with respect
[302.68s -> 305.36s]  to every variable in the computational graph,
[305.36s -> 309.60s]  and so we're going to see how this is done.
[309.60s -> 311.56s]  And this becomes very useful when we start working
[311.58s -> 313.14s]  with really complex functions,
[313.14s -> 315.22s]  so for example, convolutional neural networks
[315.22s -> 317.54s]  that we're going to talk about later in this class,
[317.54s -> 320.62s]  we have here the input, right, image at the top,
[320.62s -> 322.10s]  we have our loss at the bottom,
[322.10s -> 323.82s]  and the input has to go through many layers
[323.82s -> 326.48s]  of transformations in order to get all the way down
[326.48s -> 327.48s]  to the loss function.
[330.66s -> 333.46s]  And this can get even crazier with things like the,
[333.46s -> 335.18s]  you know, like a neural Turing machine,
[335.18s -> 337.62s]  which is another kind of deep learning model,
[337.62s -> 339.70s]  and in this case, you can see that the computational graph
[339.70s -> 343.16s]  for this is really insane, and especially,
[343.16s -> 345.64s]  we end up, you know, unrolling this over time.
[345.64s -> 347.28s]  It's basically completely impractical
[347.28s -> 349.72s]  if you want to compute the gradients
[349.72s -> 351.76s]  for any of these intermediate variables.
[355.32s -> 357.44s]  Okay, so how does backpropagation work?
[358.84s -> 361.56s]  So we're going to start off with a simple example,
[361.56s -> 363.36s]  where again, our goal is that we have a function,
[363.36s -> 366.04s]  so in this case, f of x, y, z
[366.04s -> 368.40s]  equals x plus y times z,
[368.40s -> 370.42s]  and we want to find the gradients of the output
[370.42s -> 373.30s]  of the function with respect to any of the variables.
[373.30s -> 376.02s]  So the first step always is we want to take our function,
[376.02s -> 377.78s]  f, and we want to represent it
[377.78s -> 380.50s]  using a computational graph, right?
[380.50s -> 383.22s]  So here, our computational graph is on the right,
[383.22s -> 385.70s]  and you can see that we have our,
[385.70s -> 387.58s]  first we have the plus node, so x plus y,
[387.58s -> 389.90s]  and then we have this multiplication node, right,
[389.90s -> 392.10s]  for the second computation that we're doing.
[393.82s -> 396.02s]  And then, now we're going to do a forward pass
[396.02s -> 397.06s]  through this network.
[397.06s -> 399.64s]  So given the values of the variables that we have,
[399.64s -> 402.76s]  so here, x equals negative two, y equals five,
[402.76s -> 404.88s]  and z equals negative four,
[404.88s -> 406.00s]  I'm going to fill these all in
[406.00s -> 408.00s]  in our computational graph,
[408.00s -> 410.28s]  and then here we can compute an intermediate value,
[410.28s -> 412.76s]  so x plus y gives three,
[412.76s -> 414.80s]  and then finally, we pass it through again
[414.80s -> 415.92s]  through the last node,
[415.92s -> 418.40s]  the multiplication to get our final node
[418.40s -> 419.80s]  of f equals negative 12.
[419.86s -> 424.86s]  So here, we want to give every intermediate variable a name.
[424.86s -> 426.86s]  Right, so here I've called this intermediate variable
[426.86s -> 428.86s]  after the plus node q,
[428.86s -> 430.86s]  and we have q equals x plus y,
[430.86s -> 434.86s]  and then f equals q times z using this intermediate node.
[434.86s -> 436.86s]  And I've also written out here
[436.86s -> 439.86s]  the gradients of q with respect to x and y,
[439.86s -> 442.86s]  which are just one because of the addition,
[442.86s -> 445.86s]  and then the gradients of f with respect to q and z,
[445.86s -> 448.86s]  which are just one because of the addition,
[448.92s -> 451.92s]  and then the z, which is z and q respectively,
[451.92s -> 453.92s]  because of the multiplication rule.
[453.92s -> 455.92s]  And so what we want to find
[455.92s -> 457.92s]  is we want to find the gradients of f
[457.92s -> 459.92s]  with respect to x, y, and z.
[463.92s -> 465.92s]  So what backprop is,
[465.92s -> 467.92s]  it's a recursive application of the chain rule,
[467.92s -> 469.92s]  so we're going to start at the back,
[469.92s -> 471.92s]  the very end of the computational graph,
[471.92s -> 473.92s]  and then we're going to work our way backwards
[473.92s -> 475.92s]  and compute all the gradients along the way.
[475.92s -> 477.92s]  So here, if we start at the very end,
[477.98s -> 479.98s]  we want to compute the gradient of the output
[479.98s -> 481.98s]  with respect to the last variable,
[481.98s -> 483.98s]  which is just f.
[483.98s -> 486.98s]  And so this gradient is just one, it's trivial.
[488.98s -> 490.98s]  So now moving backwards,
[490.98s -> 494.98s]  we want the gradient with respect to z.
[494.98s -> 498.98s]  And we know that df over dz is equal to q.
[498.98s -> 501.98s]  So the value of q is just three,
[501.98s -> 506.98s]  and so we have here df over dz equals three.
[507.98s -> 510.98s]  And so next, if we want to do df over dq,
[510.98s -> 512.98s]  what is the value of that?
[515.98s -> 517.98s]  What is df over dq?
[517.98s -> 521.98s]  So we have here df over dq is equal to z,
[524.98s -> 528.98s]  right, and the value of z is negative four.
[528.98s -> 532.98s]  So here we have df over dq is equal to negative four.
[533.04s -> 535.04s]  Okay.
[537.04s -> 540.04s]  Okay, so now continuing to move backwards through the graph,
[540.04s -> 542.04s]  we want to find df over dy.
[542.04s -> 544.04s]  Right, but here in this case,
[544.04s -> 546.04s]  the gradient with respect to y,
[546.04s -> 548.04s]  y is not connected directly to f.
[548.04s -> 552.04s]  Right, it's connected through an intermediate node of z.
[552.04s -> 554.04s]  And so the way we're going to do this
[554.04s -> 557.04s]  is we can leverage the chain rule,
[557.04s -> 560.04s]  which says that df over dy can be written as
[561.04s -> 563.54s]  df over dq times dq over dy.
[564.54s -> 566.54s]  And so the intuition of this is that
[566.54s -> 570.04s]  in order to find the effect of y on f,
[570.04s -> 572.04s]  this is actually equivalent to
[572.04s -> 575.54s]  if we take the effect of q times q on f,
[575.54s -> 577.04s]  which we already know, right,
[577.04s -> 580.54s]  df over dq is equal to negative four,
[580.54s -> 584.54s]  and we compound it with the effect of y on q,
[584.54s -> 586.04s]  dq over dy.
[586.54s -> 590.04s]  So what's dq over dy equal to in this case?
[591.54s -> 593.04s]  One, right, exactly.
[593.04s -> 595.04s]  So dq over dy is equal to one,
[595.04s -> 598.04s]  which means if we change y by a little bit,
[598.04s -> 600.04s]  q is going to change by approximately the same amount,
[600.04s -> 601.54s]  right, this is the effect.
[601.54s -> 605.54s]  And so what this is doing is this is saying,
[605.54s -> 607.54s]  well, if I change y by a little bit,
[607.54s -> 609.04s]  the effect of dq,
[609.04s -> 613.04s]  the effect of y on q is going to be one,
[613.04s -> 615.04s]  and then the effect of q on f
[615.04s -> 618.04s]  is going to be approximately a factor of negative four,
[618.04s -> 620.04s]  right, so then we multiply these together,
[620.04s -> 623.54s]  and we get that the effect of y on f
[623.54s -> 626.04s]  is going to be negative four.
[630.54s -> 632.54s]  Okay, so now if we want to do the same thing
[632.54s -> 635.04s]  for the gradient with respect to x,
[635.04s -> 637.54s]  right, we can follow the same procedure,
[637.54s -> 640.54s]  and so what is this going to be?
[642.54s -> 644.04s]  I heard the same.
[644.04s -> 646.04s]  Yeah, exactly.
[646.04s -> 650.04s]  So in this case, we want to again apply the chain rule,
[650.04s -> 653.04s]  right, we know the effect of q on f
[653.04s -> 656.04s]  is negative four, and here again,
[656.04s -> 658.54s]  since we have also the same addition node,
[658.54s -> 661.04s]  dq over dx is equal to one,
[661.04s -> 663.54s]  again we have negative four times one, right,
[663.54s -> 665.54s]  and the gradient with respect to x
[665.54s -> 668.04s]  is going to be negative four.
[670.54s -> 672.54s]  Okay, so what we're doing in backprop
[672.54s -> 674.54s]  is we basically have all of these nodes
[674.54s -> 676.54s]  in our computational graph,
[676.54s -> 679.54s]  but each node is only aware of its immediate surroundings,
[679.54s -> 681.54s]  right, so we have at each node,
[681.54s -> 684.54s]  we have the local inputs that are connected to this node,
[684.54s -> 686.54s]  the values that are flowing into the node,
[686.54s -> 688.54s]  and then we also have the output
[688.54s -> 691.54s]  that is directly outputted from this node, right,
[691.54s -> 694.54s]  so here our local inputs are x and y,
[694.54s -> 696.54s]  and the output is z.
[697.54s -> 701.54s]  And at this node, we also know the local gradient, right,
[701.54s -> 704.54s]  we can compute the gradient of z with respect to x
[704.54s -> 706.54s]  and the gradient of z with respect to y,
[706.54s -> 709.54s]  and these are usually really simple operations, right,
[709.54s -> 711.54s]  each node is going to be something like the addition
[711.54s -> 714.54s]  or the multiplication that we had in that earlier example,
[714.54s -> 716.54s]  which is something where we can just write down
[716.54s -> 718.54s]  the gradient, and we don't have to, you know,
[718.54s -> 721.54s]  go through very complex calculus in order to find this.
[726.54s -> 729.54s]  Yeah, so basically if we go back,
[741.54s -> 744.54s]  so if we go back here, we could exactly find
[744.54s -> 746.54s]  all of these using just calculus, so we could say,
[746.54s -> 749.54s]  you know, we want df over dx, right,
[749.54s -> 751.54s]  and we can probably expand out this expression
[751.54s -> 754.54s]  and see that it's just going to be z,
[754.54s -> 757.54s]  but we can do this in this case because it's simple,
[757.54s -> 759.54s]  but we'll see examples later on where once
[759.54s -> 761.54s]  this becomes a really complicated expression,
[761.54s -> 765.54s]  you don't want to have to use calculus to derive, right,
[765.54s -> 766.54s]  the gradient for something,
[766.54s -> 768.54s]  for a super complicated expression,
[768.54s -> 771.54s]  and instead if you use this formalism
[771.54s -> 774.54s]  and you break it down into these computational nodes,
[774.54s -> 777.54s]  then you can only ever work with gradients
[777.54s -> 780.54s]  of very simple computations, right,
[780.54s -> 783.54s]  at the level of, you know, additions, multiplications,
[783.54s -> 785.54s]  exponentials, things as simple as you want them,
[785.54s -> 787.54s]  and then you just use the chain rule
[787.54s -> 789.54s]  to multiply all of these together
[789.54s -> 791.54s]  and get the value of your gradient
[791.54s -> 795.54s]  without having to ever derive the entire expression.
[797.54s -> 799.54s]  Does that make sense?
[800.54s -> 804.54s]  Okay, so we'll see an example of this later.
[807.54s -> 809.54s]  And so was there another question?
[809.54s -> 810.54s]  Yeah.
[811.54s -> 814.54s]  Negative, oh, okay, yeah, so the negative four,
[814.54s -> 816.54s]  these were the green values on top
[816.54s -> 819.54s]  where all the values of the function
[819.54s -> 822.54s]  as we passed it forward through the computational graph,
[822.54s -> 825.54s]  right, so we set up here that x is equal to negative two,
[825.54s -> 828.54s]  y is equal to five, and z equals negative four,
[828.54s -> 830.54s]  so we filled in all of these values,
[830.54s -> 832.54s]  and then we just wanted to compute
[832.54s -> 835.54s]  the value of this function, right,
[836.54s -> 840.54s]  so we said this value of q is going to be x plus y,
[840.54s -> 842.54s]  it's going to be negative two plus five,
[842.54s -> 844.54s]  and it's going to be three,
[844.54s -> 846.54s]  and we have z is equal to negative four,
[846.54s -> 847.54s]  so we fill that in here,
[847.54s -> 850.54s]  and then we multiplied q and z together,
[850.54s -> 851.54s]  negative four times three
[851.54s -> 854.54s]  in order to get the final value of f, right,
[854.54s -> 855.54s]  and then the red values underneath
[855.54s -> 857.54s]  were as we were filling in the gradients
[857.54s -> 860.54s]  as we were working backwards.
[862.54s -> 863.54s]  Okay.
[866.54s -> 870.54s]  Okay, so, right, so we said that, you know,
[870.54s -> 872.54s]  we have these local, these nodes,
[872.54s -> 876.54s]  and each node basically gets its local inputs coming in
[876.54s -> 878.54s]  and the output that it sees directly passing on
[878.54s -> 880.54s]  to the next node,
[880.54s -> 881.54s]  and we also have these local gradients
[881.54s -> 883.54s]  that we computed, right,
[883.54s -> 886.54s]  the gradient of the immediate output of the node
[886.54s -> 888.54s]  with respect to the inputs coming in,
[888.54s -> 892.54s]  and so what happens during backprop is we have these,
[893.54s -> 895.54s]  we start from the back of the graph, right,
[895.54s -> 897.54s]  and then we work our way from the end
[897.54s -> 899.54s]  all the way back to the beginning,
[899.54s -> 901.54s]  and when we reach each node,
[901.54s -> 904.54s]  at each node we have the upstream gradients coming back,
[904.54s -> 907.54s]  right, with respect to the immediate output of the node,
[907.54s -> 910.54s]  so by the time we reach this node in backprop,
[910.54s -> 912.54s]  we've already computed the gradient
[912.54s -> 916.54s]  of our final loss L with respect to z, right,
[916.54s -> 919.54s]  and so now what we want to find next
[919.54s -> 920.54s]  is we want to find the gradients
[920.54s -> 922.54s]  with respect to just before the node,
[922.54s -> 924.54s]  to the values of x and y,
[926.54s -> 927.54s]  and so as we saw earlier,
[927.54s -> 930.54s]  we do this using the chain rule, right,
[930.54s -> 931.54s]  we have from the chain rule
[931.54s -> 933.54s]  that the gradient of this loss function
[933.54s -> 935.54s]  with respect to x is going to be
[935.54s -> 937.54s]  the gradient with respect to z
[937.54s -> 940.54s]  times compounded by this gradient,
[940.54s -> 943.54s]  local gradient of z with respect to x,
[943.54s -> 944.54s]  right, so in the chain rule
[944.54s -> 947.54s]  we always take this upstream gradient coming down
[947.54s -> 949.54s]  and we multiply it by the local gradient
[949.54s -> 954.54s]  in order to get the gradient with respect to the input.
[971.54s -> 974.54s]  So the question is whether this only works
[974.54s -> 978.54s]  because we're working with the current values
[978.54s -> 982.54s]  of the function, and so it works, right,
[982.54s -> 984.54s]  given the current values of the function that we plug in,
[984.54s -> 986.54s]  but we can write an expression for this
[986.54s -> 988.54s]  still in terms of the variables, right,
[988.54s -> 992.54s]  so we'll see that gradient of L with respect to z
[992.54s -> 994.54s]  is going to be some expression,
[994.54s -> 996.54s]  and gradient of z with respect to x
[996.54s -> 998.54s]  is going to be another expression, right,
[998.54s -> 1000.54s]  but we plug in these,
[1000.54s -> 1003.54s]  we plug in the values of these numbers at the time
[1003.54s -> 1006.54s]  in order to get the value of the gradient
[1006.54s -> 1007.54s]  with respect to x.
[1007.54s -> 1009.54s]  So what you could do is you could recursively
[1009.54s -> 1013.54s]  plug in all of these expressions, right,
[1013.54s -> 1016.54s]  gradient with respect, z with respect to x
[1016.54s -> 1020.54s]  is going to be a simple, simple expression, right,
[1020.54s -> 1022.54s]  so in this case if we have a multiplication node,
[1022.54s -> 1024.54s]  gradient of z with respect to x
[1024.54s -> 1027.54s]  is just going to be y, right, we know that,
[1027.54s -> 1029.54s]  but the gradient of L with respect to z,
[1029.54s -> 1033.54s]  this is probably a complex part of the graph in itself,
[1033.54s -> 1036.54s]  right, so here's where we want to just,
[1036.54s -> 1039.54s]  in this case, have this numerical, right,
[1039.54s -> 1042.54s]  so as you said, basically this is going to be
[1042.54s -> 1044.54s]  just a number coming down, right, a value,
[1044.54s -> 1046.54s]  and then we just multiply it with the expression
[1046.54s -> 1049.54s]  that we have for the local gradient.
[1049.54s -> 1051.54s]  And I think this will be more clear
[1051.54s -> 1053.54s]  when we go through a more complicated example
[1053.54s -> 1055.54s]  in a few slides.
[1057.54s -> 1059.54s]  Okay, so now the gradient of L with respect to y,
[1059.54s -> 1062.54s]  we have exactly the same idea,
[1062.54s -> 1064.54s]  where again we use the chain rule,
[1064.54s -> 1066.54s]  we have gradient of L with respect to z
[1066.54s -> 1069.54s]  times the gradient of z with respect to y, right,
[1069.54s -> 1071.54s]  we use the chain rule, multiply these together,
[1071.54s -> 1073.54s]  and get our gradient.
[1075.54s -> 1076.54s]  And then once we have these,
[1076.54s -> 1078.54s]  we'll pass these onto the node directly before
[1078.54s -> 1080.54s]  connected to this node.
[1080.54s -> 1082.54s]  And so the main thing to take away from this
[1082.54s -> 1084.54s]  is that at each node,
[1084.54s -> 1086.54s]  we just want to have our local gradient
[1086.54s -> 1088.54s]  that we compute, just keep track of this,
[1088.54s -> 1091.54s]  and then during backprop as we're receiving,
[1091.54s -> 1094.54s]  you know, numerical values of gradients
[1094.54s -> 1096.54s]  coming from upstream, we just take what that is,
[1096.54s -> 1098.54s]  multiply it by the local gradient,
[1098.54s -> 1100.54s]  and then this is what we then send back
[1100.54s -> 1104.54s]  to the connected nodes, the next nodes going backwards
[1104.54s -> 1107.54s]  without having to care about anything else
[1107.54s -> 1110.54s]  besides these immediate surroundings.
[1110.54s -> 1113.54s]  So now we're going to go through another example,
[1113.54s -> 1114.54s]  this time a little bit more complex,
[1114.54s -> 1118.54s]  so we can see more why backprop is so useful.
[1118.54s -> 1120.54s]  So in this case, our function is
[1120.54s -> 1125.54s]  f of w and x, which is equal to one over one plus e
[1125.54s -> 1128.54s]  to the negative of w zero times x zero
[1128.54s -> 1131.54s]  plus w one x one plus w two.
[1131.54s -> 1133.54s]  Right, so again, the first step always
[1133.54s -> 1136.54s]  is we want to write this out as a computational graph.
[1136.54s -> 1138.54s]  So in this case, we can see that in this graph,
[1138.54s -> 1140.54s]  right, first we multiply together
[1140.54s -> 1142.54s]  the w and x terms that we have,
[1142.54s -> 1145.54s]  w zero with x zero, w one with x one,
[1145.54s -> 1149.54s]  and w two, then we add all of these together,
[1149.54s -> 1153.54s]  right, then we do, scale it by negative one,
[1153.54s -> 1156.54s]  we take the exponential, we add one,
[1156.54s -> 1160.54s]  and then finally we do one over this whole term.
[1161.54s -> 1164.54s]  And then here I've also filled in values of these,
[1164.54s -> 1166.54s]  so let's say given values that we have
[1166.54s -> 1169.54s]  for the ws and xs, right, we can make a forward pass
[1169.54s -> 1171.54s]  and basically compute what the value is
[1171.54s -> 1174.54s]  at every stage of the computation.
[1175.54s -> 1178.54s]  And here, I've also written down here at the bottom
[1178.54s -> 1181.54s]  the values, the expressions for some derivatives
[1181.54s -> 1183.54s]  that are going to be helpful later on,
[1183.54s -> 1187.54s]  so same as we did before with a simple example.
[1187.54s -> 1190.54s]  Okay, so now when we're going to do backprop through here,
[1190.54s -> 1192.54s]  right, so again, we're going to start
[1192.54s -> 1195.54s]  at the very end of the graph, and so here again,
[1195.54s -> 1198.54s]  the gradient of the output with respect
[1198.54s -> 1202.54s]  to the last variable is just one, it's just trivial.
[1203.54s -> 1206.54s]  And so now moving backwards one step, right,
[1206.54s -> 1209.54s]  so what's the gradient with respect
[1209.54s -> 1212.54s]  to the input just before one over x?
[1212.54s -> 1214.54s]  Well, so in this case,
[1215.54s -> 1217.54s]  we know that the upstream gradient
[1217.54s -> 1220.54s]  that we have coming down, right, is this red one, right,
[1220.54s -> 1223.54s]  this is the upstream gradient that we have flowing down.
[1223.54s -> 1226.54s]  And then now we need to find the local gradient, right,
[1226.54s -> 1227.54s]  and the local gradient of this node,
[1227.54s -> 1229.54s]  this node is one over x, right,
[1229.54s -> 1231.54s]  so we have f of x equals one over x here in red,
[1231.54s -> 1234.54s]  and the local gradient of this, d f over d x,
[1234.54s -> 1238.54s]  is equal to negative one over x squared, right.
[1238.54s -> 1242.54s]  So here we're going to take negative one over x squared
[1242.54s -> 1244.54s]  and plug in the value of x that we had
[1244.54s -> 1247.54s]  during this forward pass, 1.37,
[1247.54s -> 1251.54s]  and so our final gradient with respect to this variable
[1251.54s -> 1255.54s]  is going to be negative one over 1.37 squared
[1255.54s -> 1258.54s]  times one equals negative 0.53, right.
[1262.54s -> 1265.54s]  So moving back to the next node,
[1265.54s -> 1267.54s]  we're going to go through the exact same process, right,
[1267.54s -> 1271.54s]  so here the gradient flowing from upstream
[1271.54s -> 1274.54s]  is going to be negative 0.53, right,
[1274.54s -> 1276.54s]  and here the local gradient,
[1276.54s -> 1278.54s]  the local, the node here is a plus one,
[1278.54s -> 1281.54s]  and so now looking at our reference
[1281.54s -> 1284.54s]  of derivatives at the bottom,
[1284.54s -> 1287.54s]  we have that for a constant plus x,
[1287.54s -> 1290.54s]  the local gradient is just one, right,
[1290.54s -> 1293.54s]  so what's the gradient with respect to this variable
[1293.54s -> 1295.54s]  using the chain rule?
[1301.54s -> 1303.54s]  So it's going to be the upstream gradient
[1303.54s -> 1308.54s]  of negative 0.53 times our local gradient of one,
[1308.54s -> 1311.54s]  which is equal to negative 0.53.
[1314.54s -> 1318.54s]  So let's keep moving backwards one more step.
[1318.54s -> 1321.54s]  So here we have the exponential, right,
[1321.54s -> 1325.54s]  so what's the upstream gradient coming down?
[1327.54s -> 1330.54s]  Right, so the upstream gradient is negative 0.53.
[1330.54s -> 1334.54s]  What's the local gradient here?
[1334.54s -> 1337.54s]  It's going to be the local gradient of e to the x, right,
[1337.54s -> 1339.54s]  this is an exponential node,
[1339.54s -> 1343.54s]  and so our chain rule is going to tell us
[1343.54s -> 1346.54s]  that our gradient's going to be negative 0.53
[1346.54s -> 1350.54s]  times e to the power of x,
[1350.54s -> 1353.54s]  which in this case is negative one from our forward pass,
[1353.54s -> 1354.54s]  and this is going to give us
[1354.54s -> 1357.54s]  our final gradient of negative 0.2.
[1359.54s -> 1363.54s]  Okay, so now one more node here,
[1363.54s -> 1365.54s]  the next node that we reach
[1365.54s -> 1368.54s]  is going to be a multiplication with negative one, right,
[1368.54s -> 1372.54s]  so here, what's the upstream gradient coming down?
[1373.54s -> 1374.54s]  Negative 0.2, right,
[1374.54s -> 1377.54s]  and what's going to be the local gradient?
[1377.54s -> 1380.54s]  You can look at the reference sheet.
[1380.54s -> 1382.54s]  It's going to be, what was it?
[1382.54s -> 1384.54s]  I think I heard it.
[1384.54s -> 1388.54s]  It's going to be minus one, exactly, yeah,
[1388.54s -> 1391.54s]  because our local gradient says
[1391.54s -> 1394.54s]  it's going to be df over dx is a, right,
[1394.54s -> 1396.54s]  and the value of a that we scaled x by
[1396.54s -> 1398.54s]  is negative one here,
[1398.54s -> 1401.54s]  so we have here that the gradient is negative one
[1401.54s -> 1405.54s]  times negative 0.2, and so our gradient is 0.2.
[1407.54s -> 1411.54s]  Okay, so now we've reached an addition node,
[1411.54s -> 1413.54s]  and so in this case we have
[1413.54s -> 1415.54s]  these two branches both connected to it, right,
[1415.54s -> 1417.54s]  so what's the upstream gradient here?
[1417.54s -> 1419.54s]  It's going to be 0.2, right,
[1419.54s -> 1421.54s]  just says everything else,
[1421.54s -> 1424.54s]  and here now the gradient with respect
[1424.54s -> 1427.54s]  to each of these branches,
[1427.54s -> 1428.54s]  it's an addition, right,
[1428.54s -> 1430.54s]  and we saw from before in our simple example
[1430.54s -> 1432.54s]  that when we have an addition node,
[1432.54s -> 1434.54s]  the gradient with respect to each of the inputs
[1434.54s -> 1437.54s]  to the addition is just going to be one, right,
[1437.54s -> 1440.54s]  so here our local gradient for our,
[1440.54s -> 1443.54s]  looking at our top stream is going to be one
[1443.54s -> 1446.54s]  times the upstream gradient of 0.2,
[1446.54s -> 1450.54s]  which is going to give a total gradient of 0.2, right,
[1450.54s -> 1452.54s]  and then we, for our bottom branch,
[1452.54s -> 1454.54s]  we do the same thing, right,
[1454.54s -> 1456.54s]  our upstream gradient is 0.2,
[1457.54s -> 1459.54s]  our local gradient is one again,
[1459.54s -> 1462.54s]  and the total gradient is 0.2.
[1462.54s -> 1465.54s]  So is everything clear about this?
[1466.54s -> 1467.54s]  Okay.
[1469.54s -> 1472.54s]  So we have a few more gradients to fill out,
[1472.54s -> 1476.54s]  so moving back, now we've reached w0 and x0,
[1476.54s -> 1480.54s]  and so here we have a multiplication node, right,
[1480.54s -> 1483.54s]  so we saw the multiplication node from before,
[1483.54s -> 1485.54s]  it just, the gradient with respect to one
[1485.54s -> 1488.54s]  of the inputs just is the value of the other input,
[1488.54s -> 1490.54s]  and so in this case, what's the gradient
[1490.54s -> 1492.54s]  with respect to w0?
[1498.54s -> 1501.54s]  Minus, I'm hearing minus 0.2, exactly.
[1501.54s -> 1503.54s]  Yeah, so with respect to w0,
[1503.54s -> 1507.54s]  we have our upstream gradient 0.2, right,
[1508.54s -> 1510.54s]  times our, this is the bottom one,
[1510.54s -> 1512.54s]  times our value of x, which is negative one,
[1512.54s -> 1515.54s]  we get negative 0.2, and we can do the same thing
[1515.54s -> 1517.54s]  for our gradient with respect to x0,
[1517.54s -> 1520.54s]  it's going to be 0.2 times the value of w0,
[1520.54s -> 1523.54s]  which is two, and we get 0.4.
[1525.54s -> 1530.54s]  Okay, so here we've filled out most of these gradients,
[1530.54s -> 1533.54s]  and so there was a question earlier
[1534.54s -> 1538.54s]  about why this is simpler than just computing,
[1538.54s -> 1541.54s]  deriving the analytic gradient, the expression,
[1541.54s -> 1543.54s]  with respect to any of these variables, right,
[1543.54s -> 1546.54s]  and so you can see here, all we ever dealt with
[1546.54s -> 1548.54s]  was expressions for local gradients
[1548.54s -> 1550.54s]  that we had to write out,
[1550.54s -> 1552.54s]  so once we had these expressions for local gradients,
[1552.54s -> 1554.54s]  all we did was plug in the values
[1554.54s -> 1556.54s]  for each of these that we have,
[1556.54s -> 1558.54s]  and use a chain rule to numerically multiply this
[1558.54s -> 1560.54s]  all the way backwards and get the gradients
[1560.54s -> 1563.54s]  with respect to all of the variables.
[1567.54s -> 1570.54s]  And so, you know, we can also fill out
[1570.54s -> 1573.54s]  the gradients with respect to w1 and x1 here
[1573.54s -> 1575.54s]  in exactly the same way,
[1575.54s -> 1577.54s]  and so one thing that I want to note
[1577.54s -> 1580.54s]  is that when we're creating these computational graphs,
[1580.54s -> 1582.54s]  we can define the computational nodes
[1582.54s -> 1584.54s]  at any granularity that we want to,
[1584.54s -> 1586.54s]  so in this case, we broke it down
[1586.54s -> 1589.54s]  into the absolute simplest that we could, right,
[1589.54s -> 1592.54s]  we broke it down into additions and multiplications,
[1592.54s -> 1595.54s]  you know, it basically can't get any simpler than that,
[1595.54s -> 1597.54s]  but in practice, right, we can group
[1597.54s -> 1599.54s]  some of these nodes together
[1599.54s -> 1601.54s]  into more complex nodes if we want,
[1601.54s -> 1603.54s]  as long as we're able to write down
[1603.54s -> 1606.54s]  the local gradient for that node, right,
[1606.54s -> 1608.54s]  and so as an example,
[1610.54s -> 1612.54s]  if we look at a sigmoid function,
[1612.54s -> 1614.54s]  so I've defined the sigmoid function
[1614.54s -> 1616.54s]  in the upper right here,
[1616.54s -> 1618.54s]  of a sigmoid of x is equal to
[1618.54s -> 1620.54s]  one over one plus e to the negative x,
[1620.54s -> 1622.54s]  and this is something that's a really common function
[1622.54s -> 1625.54s]  that you'll see a lot in the rest of this class,
[1625.54s -> 1628.54s]  and we can compute the gradient for this
[1628.54s -> 1630.54s]  but we can write it out,
[1630.54s -> 1633.54s]  and if we do actually go through the math
[1633.54s -> 1635.54s]  of doing this analytically,
[1635.54s -> 1637.54s]  we can get a nice expression at the end,
[1637.54s -> 1639.54s]  so in this case, it's equal to
[1639.54s -> 1643.54s]  one minus sigma of x, so the output of this function,
[1643.54s -> 1645.54s]  times sigma of x, right,
[1645.54s -> 1648.54s]  and so in cases where we have something like this,
[1648.54s -> 1652.54s]  we could just take all the computations
[1652.54s -> 1655.54s]  that we had in our graph that made up the sigmoid,
[1655.54s -> 1657.54s]  and we could just replace it with
[1657.54s -> 1659.54s]  a big node that's a sigmoid, right,
[1659.54s -> 1663.54s]  because we do know the local gradient for this gate,
[1663.54s -> 1668.54s]  it's this expression of the sigmoid of x over dx, right,
[1668.54s -> 1670.54s]  so basically the important thing here
[1670.54s -> 1673.54s]  is that you can group any nodes that you want
[1673.54s -> 1678.54s]  to make any sorts of a little bit more complex nodes,
[1678.54s -> 1681.54s]  as long as you can write down the local gradient for this,
[1681.54s -> 1683.54s]  and so all this is is basically a trade-off
[1683.54s -> 1686.54s]  between how much math that you want to do
[1686.54s -> 1690.54s]  to get a more kind of concise and simpler graph, right,
[1690.54s -> 1695.54s]  versus how simple you want each of your gradients to be,
[1695.54s -> 1697.54s]  right, and then you can write out
[1697.54s -> 1700.54s]  as complex of a computational graph that you want.
[1700.54s -> 1701.54s]  Yeah, question.
[1711.54s -> 1714.54s]  So they could also be connected into a single addition node,
[1714.54s -> 1718.54s]  so the question was, is there a reason why w0 and x0
[1718.54s -> 1720.54s]  are not connected with w2,
[1720.54s -> 1722.54s]  all of these additions just connected together,
[1722.54s -> 1725.54s]  and yeah, so the answer is that you can do that
[1725.54s -> 1727.54s]  if you want, and in practice,
[1727.54s -> 1729.54s]  maybe you would actually want to do that,
[1729.54s -> 1731.54s]  because this is still a very simple node, right,
[1731.54s -> 1734.54s]  so in this case, I just wrote this out into
[1734.54s -> 1738.54s]  as simple as possible, where each node
[1738.54s -> 1740.54s]  only had up to two inputs,
[1740.54s -> 1743.54s]  but yeah, you could definitely do that.
[1743.54s -> 1745.54s]  Any other questions about this?
[1747.54s -> 1750.54s]  Okay, so the one thing that I really like
[1750.54s -> 1752.54s]  about thinking about this like a computational graph
[1752.54s -> 1754.54s]  is that I feel very comforted, right,
[1754.54s -> 1756.54s]  like any time I have to take a gradient,
[1756.54s -> 1758.54s]  find gradients of something,
[1758.54s -> 1760.54s]  even if the expression that I want
[1760.54s -> 1762.54s]  to compute gradients of is really hairy
[1762.54s -> 1763.54s]  and really scary, you know,
[1763.54s -> 1764.54s]  whether it's something like the sigmoid
[1764.54s -> 1767.54s]  or something worse, I know that, you know,
[1767.54s -> 1769.54s]  I could derive this if I want to,
[1769.54s -> 1772.54s]  but really, if I just sit down and write it out
[1772.54s -> 1774.54s]  in terms of a computational graph,
[1774.54s -> 1776.54s]  I can go as simple as I need to
[1776.54s -> 1778.54s]  to always be able to apply backprop
[1778.54s -> 1780.54s]  and the chain rule and be able to
[1780.54s -> 1783.54s]  compute all the gradients that I need,
[1783.54s -> 1784.54s]  and so this is something that you guys
[1784.54s -> 1788.54s]  should think about when you're doing your homeworks
[1788.54s -> 1790.54s]  as basically, you know,
[1790.54s -> 1792.54s]  any time you're having trouble finding gradients
[1792.54s -> 1793.54s]  of something, just think about it
[1793.54s -> 1794.54s]  as a computational graph,
[1794.54s -> 1796.54s]  break it down into all of these parts,
[1796.54s -> 1798.54s]  and then use the chain rule.
[1799.54s -> 1801.54s]  Okay, and so, you know,
[1802.54s -> 1804.54s]  so we talked about how we could group
[1804.54s -> 1807.54s]  these set of nodes together into a sigmoid gate,
[1807.54s -> 1809.54s]  and just to confirm, like,
[1809.54s -> 1811.54s]  that this is actually exactly equivalent,
[1811.54s -> 1813.54s]  we can plug this in, right,
[1813.54s -> 1816.54s]  so we have that our input here
[1816.54s -> 1820.54s]  to the sigmoid gate is going to be one in green,
[1820.54s -> 1822.54s]  and then we have that the output
[1822.54s -> 1825.54s]  is going to be here 0.73, right,
[1825.54s -> 1827.54s]  and this will work out if you plug it
[1827.54s -> 1829.54s]  into the sigmoid function.
[1829.54s -> 1831.54s]  And so now if we want to do,
[1831.54s -> 1833.54s]  if we want to take the gradient
[1833.54s -> 1835.54s]  and we want to treat this entire sigmoid
[1835.54s -> 1838.54s]  as one node, now what we should do
[1838.54s -> 1840.54s]  is we need to use this local gradient
[1840.54s -> 1842.54s]  that we've derived up here, right,
[1842.54s -> 1845.54s]  one minus sigmoid of x times the sigmoid of x.
[1845.54s -> 1847.54s]  So if we plug this in,
[1847.54s -> 1849.54s]  and here we know that the value of sigmoid of x
[1849.54s -> 1852.54s]  was 0.73, so if we plug this value in,
[1852.54s -> 1854.54s]  we'll see that the value of this gradient
[1854.54s -> 1856.54s]  is equal to 0.2, right,
[1856.54s -> 1859.54s]  and so the value of this local gradient is 0.2,
[1859.54s -> 1862.54s]  we multiply it by the upstream gradient which is one,
[1862.54s -> 1864.54s]  and we're going to get out
[1864.54s -> 1866.54s]  exactly the same value of the gradient
[1866.54s -> 1868.54s]  with respect to before the sigmoid gate
[1868.54s -> 1869.54s]  as if we broke it down
[1869.54s -> 1872.54s]  into all of the smaller computations.
[1876.54s -> 1878.54s]  Okay, and so as we're looking at what's happening,
[1878.54s -> 1881.54s]  right, as we're taking these gradients
[1881.54s -> 1884.54s]  going backwards through our computational graph,
[1884.54s -> 1887.54s]  there's some patterns that you'll notice
[1887.54s -> 1890.54s]  where there's some intuitive interpretation
[1890.54s -> 1892.54s]  that we can give these, right,
[1892.54s -> 1894.54s]  so we saw that the add gate
[1894.54s -> 1896.54s]  is a gradient distributor, right,
[1896.54s -> 1900.54s]  when we pass through this addition gate here
[1900.54s -> 1902.54s]  which had two branches coming out of it,
[1902.54s -> 1905.54s]  it took the gradient, the upstream gradient,
[1905.54s -> 1907.54s]  and it just distributed it, passed the exact same thing
[1907.54s -> 1911.54s]  to both of the branches that were connected.
[1911.54s -> 1914.54s]  So here's a couple more that we can think about.
[1914.54s -> 1917.54s]  So what's a max gate look like?
[1918.54s -> 1920.54s]  So we have a max gate here at the bottom, right,
[1920.54s -> 1923.54s]  where the inputs coming in are z and w,
[1923.54s -> 1927.54s]  z has a value of two, w has a value of negative one,
[1927.54s -> 1930.54s]  and then we took the max of this which is two, right,
[1930.54s -> 1932.54s]  and so we passed this down into
[1932.54s -> 1935.54s]  the remainder of our computational graph.
[1935.54s -> 1937.54s]  So now if we're taking the gradients
[1937.54s -> 1939.54s]  with respect to this,
[1939.54s -> 1942.54s]  the upstream gradient is, let's say, two coming back, right,
[1942.54s -> 1946.54s]  and what does this local gradient look like?
[1949.54s -> 1950.54s]  Yes.
[1954.54s -> 1955.54s]  Right.
[1958.54s -> 1961.54s]  Exactly, so the answer that was given
[1961.54s -> 1964.54s]  is that z will have a gradient of two,
[1964.54s -> 1967.54s]  w will have a gradient of zero,
[1968.54s -> 1970.54s]  and so one of these is going to get the full value
[1970.54s -> 1972.54s]  of the gradient just passed back
[1972.54s -> 1975.54s]  and routed to that variable,
[1975.54s -> 1978.54s]  and then the other one will have a gradient of zero.
[1978.54s -> 1982.54s]  And so we can think of this as kind of a gradient router,
[1982.54s -> 1985.54s]  right, so whereas the addition node passed back
[1985.54s -> 1987.54s]  the same gradient to both branches coming in,
[1987.54s -> 1989.54s]  the max gate will just take the gradient
[1989.54s -> 1991.54s]  and route it to one of the branches.
[1991.54s -> 1994.54s]  And this makes sense because if we look at our forward pass,
[1994.54s -> 1996.54s]  what's happening is that only the value
[1996.54s -> 1998.54s]  that was the maximum got passed down
[1998.54s -> 2002.54s]  to the rest of the computational graph, right,
[2002.54s -> 2004.54s]  so it's the only value that actually affected
[2004.54s -> 2006.54s]  our function computation at the end,
[2006.54s -> 2008.54s]  and so it makes sense that when we're passing
[2008.54s -> 2011.54s]  our gradients back, we just want to adjust what,
[2011.54s -> 2015.54s]  you know, flow it through that branch of the computation.
[2017.54s -> 2018.54s]  Okay, and so another one,
[2018.54s -> 2022.54s]  what's a multiplication gate, which we saw earlier?
[2022.54s -> 2025.54s]  Is there any interpretation of this?
[2027.54s -> 2028.54s]  Okay, so.
[2032.54s -> 2034.54s]  Okay, so the answer that was given
[2034.54s -> 2036.54s]  is that the local gradient is basically
[2036.54s -> 2038.54s]  just the value of the other variable.
[2038.54s -> 2040.54s]  Yeah, so that's exactly right.
[2040.54s -> 2043.54s]  So we can think of this as a gradient switcher, right,
[2043.54s -> 2045.54s]  a switcher and I guess a scalar,
[2045.54s -> 2047.54s]  where we take the upstream gradient
[2047.54s -> 2051.54s]  and we scale it by the value of the other branch.
[2052.54s -> 2054.54s]  Okay, and so one other thing to note
[2054.54s -> 2057.54s]  is that when we have a place where one node
[2057.54s -> 2059.54s]  is connected to multiple nodes,
[2059.54s -> 2063.54s]  the gradients add up at this node, right,
[2063.54s -> 2066.54s]  so at these branches, using the multivariate chain rule,
[2066.54s -> 2068.54s]  we're just going to take the value
[2068.54s -> 2070.54s]  of the upstream gradient coming back
[2070.54s -> 2073.54s]  from each of these nodes and we'll add these together
[2073.54s -> 2075.54s]  to get the total upstream gradient
[2075.54s -> 2077.54s]  that's flowing back into this node.
[2077.54s -> 2079.54s]  And you can see this from the,
[2079.54s -> 2082.54s]  and you can see this from the multivariate chain rule
[2082.54s -> 2084.54s]  and also thinking about this,
[2084.54s -> 2086.54s]  you can think about this,
[2086.54s -> 2089.54s]  that if you're going to change this node a little bit,
[2089.54s -> 2092.54s]  it's going to affect both of these connected nodes
[2092.54s -> 2093.54s]  in the forward pass, right,
[2093.54s -> 2096.54s]  when you're making your forward pass through the graph.
[2096.54s -> 2098.54s]  And so then when you're doing backprop, right,
[2098.54s -> 2102.54s]  then now the both of these gradients coming back
[2102.54s -> 2105.54s]  are going to affect this node, right,
[2105.54s -> 2107.54s]  and so that's why we're going to sum these up
[2107.54s -> 2109.54s]  to be the total upstream gradient
[2109.54s -> 2112.54s]  flowing back into this node.
[2112.54s -> 2116.54s]  Okay, so any questions about backprop
[2116.54s -> 2120.54s]  going through these forward and backward passes?
[2128.54s -> 2130.54s]  Right, so the question is,
[2130.54s -> 2131.54s]  we haven't done anything yet
[2131.54s -> 2133.54s]  to update the values of these weights,
[2133.54s -> 2135.54s]  we've only found the gradients
[2135.54s -> 2137.54s]  with respect to the variables, that's exactly right.
[2137.54s -> 2140.54s]  So what we've talked about so far in this lecture
[2140.54s -> 2142.54s]  is how to compute gradients
[2142.54s -> 2145.54s]  with respect to any variables in our function, right,
[2145.54s -> 2147.54s]  and then once we have these,
[2147.54s -> 2150.54s]  we can just apply everything we learned
[2150.54s -> 2153.54s]  in the optimization lecture last lecture, right,
[2153.54s -> 2155.54s]  so given the gradient,
[2155.54s -> 2158.54s]  we now take a step in the direction of the gradient
[2158.54s -> 2161.54s]  in order to update our parameters, right,
[2161.54s -> 2163.54s]  so we can just take this entire framework
[2163.54s -> 2166.54s]  that we learned about last lecture for optimization,
[2166.54s -> 2168.54s]  and what we've done here
[2168.54s -> 2171.54s]  is just learn how to compute the gradients we need
[2171.54s -> 2174.54s]  for arbitrarily complex functions, right,
[2174.54s -> 2175.54s]  and so this is going to be useful
[2175.54s -> 2177.54s]  when we talk about complex functions
[2177.54s -> 2180.54s]  like neural networks later on, yeah.
[2186.54s -> 2190.54s]  Yeah, so I can write this maybe on the board.
[2191.54s -> 2195.54s]  Right, so basically if we're going to have,
[2195.54s -> 2198.54s]  let's see, if we want to have the gradient of F
[2198.54s -> 2201.54s]  with respect to some variable X, right,
[2201.54s -> 2205.54s]  and let's say it's connected through variables,
[2208.54s -> 2211.54s]  let's see, I, we can basically,
[2220.54s -> 2223.54s]  right, so this is basically saying that
[2223.54s -> 2226.54s]  if X is connected to these multiple elements, right,
[2226.54s -> 2229.54s]  let's say in this case different QIs,
[2229.54s -> 2232.54s]  then the chain rule is taking all,
[2233.54s -> 2235.54s]  it's going to take the effect
[2235.54s -> 2237.54s]  of each of these intermediate variables, right,
[2237.54s -> 2241.54s]  on our final output F, and then compound each one
[2241.54s -> 2245.54s]  with the local effect of our variable X
[2245.54s -> 2248.54s]  on that intermediate value, right,
[2248.54s -> 2253.54s]  so yeah, it's basically just summing all these up together.
[2255.54s -> 2258.54s]  Okay, so now that we've, you know,
[2258.54s -> 2260.54s]  done all these examples in the scalar case,
[2260.54s -> 2262.54s]  we're going to look at what happens
[2262.54s -> 2264.54s]  when we have vectors, right,
[2264.54s -> 2266.54s]  so now for variables X, Y, and Z,
[2266.54s -> 2269.54s]  instead of just being numbers, we have vectors for these,
[2269.54s -> 2273.54s]  and so everything stays exactly the same, the entire flow,
[2273.54s -> 2275.54s]  the only difference is that now our gradients
[2275.54s -> 2278.54s]  are going to be Jacobian matrices, right,
[2278.54s -> 2282.54s]  so these are now going to be matrices
[2282.54s -> 2285.54s]  containing the derivative of each element
[2285.54s -> 2289.54s]  of, for example, Z, with respect to each element of X.
[2293.54s -> 2296.54s]  Okay, and so to, you know,
[2296.54s -> 2299.54s]  so give an example of something where this is happening,
[2299.54s -> 2301.54s]  right, let's say that we have,
[2301.54s -> 2303.54s]  our input is going to now be a vector,
[2303.54s -> 2306.54s]  so let's say we have a 4,096-dimensional input vector,
[2306.54s -> 2308.54s]  and this is kind of a common size that you might see
[2308.54s -> 2312.54s]  in convolutional neural networks later on,
[2312.54s -> 2316.54s]  and our node is going to be an element-wise maximum,
[2316.54s -> 2320.54s]  right, so we have F of X is equal to the maximum
[2320.54s -> 2324.54s]  of X compared with zero element-wise,
[2324.54s -> 2326.54s]  and then our output is going to be
[2326.54s -> 2329.54s]  also a 4,096-dimensional vector.
[2330.54s -> 2332.54s]  Okay, so in this case,
[2332.54s -> 2334.54s]  what's the size of our Jacobian matrix?
[2334.54s -> 2336.54s]  Remember I said earlier the Jacobian matrix
[2336.54s -> 2339.54s]  is going to be, like each row is,
[2339.54s -> 2341.54s]  it's going to be partial derivatives,
[2341.54s -> 2343.54s]  a matrix of partial derivatives
[2343.54s -> 2344.54s]  of each dimension of the output
[2344.54s -> 2348.54s]  with respect to each dimension of the input.
[2349.54s -> 2353.54s]  Okay, so the answer I heard was 4,096-squared,
[2353.54s -> 2355.54s]  and that's, yeah, that's correct.
[2355.54s -> 2358.54s]  So this is pretty large, right, 4,096-squared,
[2358.54s -> 2360.54s]  right, 4,096 by 4,096,
[2361.54s -> 2364.54s]  and in practice this is going to be even larger
[2364.54s -> 2367.54s]  because we're going to work with many batches of,
[2367.54s -> 2370.54s]  you know, of, for example, 100 inputs at the same time,
[2370.54s -> 2372.54s]  right, and we'll put all of these
[2372.54s -> 2375.54s]  through our node at the same time to be more efficient,
[2375.54s -> 2378.54s]  and so this is going to scale this by 100,
[2378.54s -> 2380.54s]  and in practice our Jacobian's actually going to turn out
[2380.54s -> 2384.54s]  to be something like 409,000 by 409,000, right,
[2384.54s -> 2386.54s]  so this is really huge
[2386.54s -> 2390.54s]  and basically completely impractical to work with.
[2390.54s -> 2392.54s]  So in practice, though,
[2392.54s -> 2396.54s]  we don't actually need to compute this huge Jacobian
[2396.54s -> 2398.54s]  most of the time, and so why is that?
[2398.54s -> 2400.54s]  Like, what does this Jacobian matrix look like?
[2400.54s -> 2403.54s]  If we think about what's happening here
[2403.54s -> 2406.54s]  where we're taking this element-wise maximum
[2406.54s -> 2408.54s]  and we think about what are each of the
[2408.54s -> 2409.54s]  partial derivatives, right,
[2409.54s -> 2412.54s]  which dimension of the inputs
[2412.54s -> 2415.54s]  affect which dimensions of the output?
[2415.54s -> 2419.54s]  What sort of structure can we see in our Jacobian matrix?
[2420.54s -> 2422.54s]  Okay, so I heard that it's diagonal.
[2422.54s -> 2423.54s]  Right, exactly.
[2423.54s -> 2425.54s]  So because this is element-wise, right,
[2425.54s -> 2429.54s]  each element of the input, say the first dimension,
[2429.54s -> 2433.54s]  only affects that corresponding element in the output,
[2433.54s -> 2437.54s]  right, and so because of that, our Jacobian matrix,
[2437.54s -> 2441.54s]  which is just going to be a diagonal matrix, right,
[2441.54s -> 2442.54s]  and so in practice, then,
[2442.54s -> 2444.54s]  we don't actually have to write out
[2444.54s -> 2446.54s]  and formulate this entire Jacobian.
[2446.54s -> 2450.54s]  We can just know the effect of X on the output, right,
[2454.54s -> 2457.54s]  and then we can just use these values, right,
[2457.54s -> 2461.54s]  and fill it in as we're computing the gradient.
[2463.54s -> 2465.54s]  Okay, so now we're going to go through
[2465.54s -> 2468.54s]  a more concrete vectorized example
[2468.54s -> 2469.54s]  of a computational graph.
[2469.54s -> 2471.54s]  Right, so let's look at a case
[2471.54s -> 2474.54s]  where we have the function f of X and W
[2474.54s -> 2478.54s]  is equal to basically the L2 of W multiplied by X,
[2481.54s -> 2483.54s]  and so in this case, we're going to say
[2483.54s -> 2486.54s]  X is n-dimensional and W is n by n.
[2488.54s -> 2489.54s]  Right, so again, our first step,
[2489.54s -> 2491.54s]  writing out the computational graph,
[2491.54s -> 2493.54s]  right, we have W multiplied by X
[2493.54s -> 2497.54s]  and then followed by, I'm just going to call this L2.
[2498.54s -> 2501.54s]  And so now let's also fill out some values for this,
[2501.54s -> 2503.54s]  so we can see that, you know,
[2503.54s -> 2506.54s]  let's say I have W be this two by two matrix
[2506.54s -> 2509.54s]  and X is going to be this two-dimensional vector.
[2509.54s -> 2511.54s]  Right, and so we can say,
[2511.54s -> 2513.54s]  label again our intermediate nodes,
[2513.54s -> 2515.54s]  our intermediate node after the multiplication
[2515.54s -> 2517.54s]  is going to be Q.
[2517.54s -> 2519.54s]  We have Q equals W times X,
[2519.54s -> 2521.54s]  which we can write out element-wise this way.
[2521.54s -> 2524.54s]  Right, the first element is just W one one times X one
[2524.54s -> 2526.54s]  plus W one two times X two and so on.
[2527.54s -> 2531.54s]  And then we can now express F in relation to Q.
[2531.54s -> 2533.54s]  Right, so looking at the second node,
[2533.54s -> 2537.54s]  we have F of Q is equal to the L2 norm of Q,
[2538.54s -> 2542.54s]  which is equal to Q one squared plus Q two squared.
[2543.54s -> 2544.54s]  Okay, so we fill this in,
[2544.54s -> 2548.54s]  right, we get Q and then we get our final output.
[2549.54s -> 2552.54s]  Okay, so now let's do back-pop through this.
[2552.54s -> 2555.54s]  Right, so again, this is always the first step.
[2555.54s -> 2558.54s]  We have the gradient with respect to,
[2558.54s -> 2560.54s]  our output is just one, right.
[2563.54s -> 2566.54s]  Okay, so now let's move back one node.
[2566.54s -> 2569.54s]  So now we want to find the gradient with respect to Q.
[2569.54s -> 2573.54s]  Right, our intermediate variable before the L2.
[2574.54s -> 2577.54s]  And so Q is a two-dimensional vector.
[2577.54s -> 2580.54s]  And what we want to do is we want to
[2580.54s -> 2583.54s]  find how each element of Q
[2584.54s -> 2587.54s]  affects our final value of F.
[2587.54s -> 2589.54s]  Right, and so if we look at this expression
[2589.54s -> 2591.54s]  that we've written out for F here at the bottom,
[2591.54s -> 2594.54s]  we can see that the gradient of F
[2594.54s -> 2597.54s]  with respect to a specific Q I,
[2597.54s -> 2601.54s]  let's say Q one, is just going to be two times Q I.
[2602.54s -> 2605.54s]  Right, this is just taking this derivative here.
[2605.54s -> 2608.54s]  And so we have this expression for,
[2609.54s -> 2611.54s]  with respect to each element of Q I,
[2611.54s -> 2615.54s]  we could also write this out in vector form if we want to.
[2615.54s -> 2618.54s]  It's just going to be two times our vector of Q, right,
[2618.54s -> 2620.54s]  if we want to write this out in vector form.
[2620.54s -> 2623.54s]  And so what we get is that our gradient is
[2623.54s -> 2626.54s]  0.44 and 0.52, this vector.
[2626.54s -> 2628.54s]  Right, and so you can see that it just
[2628.54s -> 2630.54s]  took Q and it scaled it by two.
[2630.54s -> 2633.54s]  Right, each element is just multiplied by two.
[2633.54s -> 2637.54s]  So the gradient of a vector is always going to be
[2637.54s -> 2639.54s]  the same size as the original vector.
[2639.54s -> 2642.54s]  And each element of this gradient
[2642.54s -> 2647.54s]  is going to, it means how much this particular element
[2647.54s -> 2651.54s]  affects our final output of the function.
[2654.54s -> 2657.54s]  Okay, so now let's move one step backwards.
[2657.54s -> 2660.54s]  Right, what's the gradient with respect to W?
[2660.54s -> 2664.54s]  And so here again, we want to use the same concept
[2664.54s -> 2665.54s]  of trying to apply the chain rule, right?
[2665.54s -> 2667.54s]  So we want to compute our local gradient
[2668.54s -> 2670.54s]  of Q with respect to W.
[2670.54s -> 2672.54s]  And so let's look at this again, element rise.
[2672.54s -> 2675.54s]  And if we do that, let's see what's the effect
[2675.54s -> 2678.54s]  of each Q, right, each element of Q
[2678.54s -> 2680.54s]  with respect to each element of W.
[2680.54s -> 2682.54s]  Right, so this is kind of the Jacobian
[2682.54s -> 2684.54s]  that we talked about earlier.
[2684.54s -> 2688.54s]  And if we look at this in this multiplication,
[2688.54s -> 2690.54s]  Q is equal to W times X.
[2690.54s -> 2694.54s]  Right, what's the, let's see,
[2694.54s -> 2696.54s]  what's the derivative or the gradient
[2696.54s -> 2701.54s]  of the first element of Q, so our first element up top,
[2701.54s -> 2703.54s]  with respect to W one one?
[2703.54s -> 2706.54s]  So Q one with respect to W one one.
[2706.54s -> 2708.54s]  What's that value?
[2708.54s -> 2710.54s]  X one, exactly.
[2710.54s -> 2713.54s]  Yeah, so we know that this is X one
[2713.54s -> 2715.54s]  and we can write this out more generally
[2715.54s -> 2718.54s]  of the gradient of Q K with respect to,
[2720.54s -> 2723.54s]  of W I J is equal to X J.
[2723.54s -> 2725.54s]  Right, and then now if we want to find the gradient
[2725.54s -> 2729.54s]  with respect to, of F with respect to each W I J,
[2730.54s -> 2733.54s]  so looking at these derivatives now,
[2734.54s -> 2737.54s]  we can use this chain rule that we talked to earlier
[2737.54s -> 2741.54s]  where we basically compound D F over D Q K
[2743.54s -> 2747.54s]  for each element of Q with D Q K over W I J
[2748.54s -> 2750.54s]  for each element of W I J.
[2750.54s -> 2752.54s]  Right, so we find the effect of each element
[2752.54s -> 2757.54s]  of W on each element of Q and sum this across all Q.
[2757.54s -> 2758.54s]  And so if you write this out,
[2758.54s -> 2759.54s]  this is going to give this expression
[2759.54s -> 2762.04s]  of two times Q I times X J.
[2765.54s -> 2767.54s]  Okay, and so filling this out,
[2767.54s -> 2771.54s]  then we get this gradient with respect to W.
[2771.54s -> 2773.54s]  And so again, we can compute this each element wise
[2773.54s -> 2776.54s]  or we can also look at this expression
[2776.54s -> 2780.54s]  that we've derived and write it out in vectorized form.
[2780.54s -> 2784.54s]  Right, so, okay, and remember the important thing
[2784.54s -> 2787.54s]  is always to check the gradient with respect to a variable
[2787.54s -> 2789.54s]  should have the same shape as the variable.
[2789.54s -> 2792.54s]  And so this is something really useful in practice
[2792.54s -> 2794.54s]  to sanity check, right?
[2794.54s -> 2796.54s]  Like once you've computed what your gradient should be,
[2796.54s -> 2800.54s]  check that this is the same shape as your variable.
[2801.54s -> 2805.54s]  Because again, each element of your gradient
[2805.54s -> 2808.54s]  is quantifying how much that element
[2808.54s -> 2813.54s]  is contributing to your, is affecting your final output.
[2813.54s -> 2814.54s]  Yeah.
[2816.54s -> 2818.54s]  The bold size, oh, the bold size one
[2818.54s -> 2820.54s]  is an indicator function, so this is saying
[2820.54s -> 2823.54s]  that it's just one if K equals I.
[2827.54s -> 2830.54s]  Okay, so, see, so we've done that.
[2830.54s -> 2834.54s]  And so now just, let's see, one more example.
[2835.54s -> 2837.54s]  Now our last thing we need to find
[2837.54s -> 2840.54s]  is the gradient with respect to qi.
[2840.54s -> 2843.54s]  So here if we compute the partial derivatives,
[2843.54s -> 2847.54s]  we can see that dqk over dxi is equal to wki,
[2848.54s -> 2852.54s]  right, using the same way as we did it for w.
[2854.54s -> 2857.54s]  And then again, we can just use the chain rule
[2857.54s -> 2860.54s]  and get the total expression for that.
[2862.54s -> 2863.54s]  Right, and so this is going to be
[2863.54s -> 2865.54s]  the gradient with respect to x,
[2865.54s -> 2868.54s]  again, of the same shape as x,
[2868.54s -> 2870.54s]  and we can also write this out in vectorized form
[2870.54s -> 2871.54s]  if we want.
[2873.54s -> 2875.54s]  Okay, so any questions about this?
[2875.54s -> 2876.54s]  Yeah.
[2879.54s -> 2882.54s]  So we are computing the Jacobian.
[2883.54s -> 2885.54s]  So let me go back here.
[2887.54s -> 2889.54s]  Right, so if we're doing, so right,
[2889.54s -> 2892.54s]  so we have these partial derivatives of qk
[2892.54s -> 2894.54s]  with respect to xi, right?
[2894.54s -> 2897.54s]  And these can, these are forming your,
[2897.54s -> 2900.54s]  the entries of your Jacobian, right?
[2900.54s -> 2902.54s]  And so in practice what we're going to do
[2902.54s -> 2905.54s]  is we basically take that and you're going to see it
[2905.54s -> 2907.54s]  up there in the chain rule.
[2907.54s -> 2909.54s]  So the vectorized expression of gradient
[2909.54s -> 2911.54s]  with respect to x, right, this is going to have
[2911.54s -> 2915.54s]  the Jacobian here, which is this transposed value here.
[2915.54s -> 2919.54s]  So you can write it out in vectorized form.
[2920.54s -> 2922.54s]  So, well, so in this case the matrix
[2922.54s -> 2925.54s]  is going to be the same size as w, right?
[2925.54s -> 2930.54s]  So it's not actually a large matrix in this case, right?
[2933.54s -> 2936.54s]  Okay, so the way that we've been thinking about this
[2936.54s -> 2939.54s]  is like a really modularized implementation, right,
[2939.54s -> 2942.54s]  where in our computational graph, right,
[2942.54s -> 2944.54s]  we look at each node locally
[2944.54s -> 2946.54s]  and we compute the local gradients
[2946.54s -> 2947.54s]  and chain them with upstream gradients
[2947.54s -> 2949.54s]  coming down, and so you can think of this
[2949.54s -> 2952.54s]  as basically a forward and a backwards API, right?
[2952.54s -> 2956.54s]  In the forward pass we implement the, you know,
[2956.54s -> 2959.54s]  a function computing the output of this node
[2959.54s -> 2962.54s]  and then in the backwards pass we compute the gradient,
[2962.54s -> 2964.54s]  right, and so when we actually implement this in code
[2964.54s -> 2967.54s]  we're going to do this in exactly the same way.
[2967.54s -> 2972.54s]  So we can basically think about for each gate, right,
[2972.54s -> 2974.54s]  if we implement a forward function
[2974.54s -> 2976.54s]  and a backward function,
[2976.54s -> 2978.54s]  where the backward function is computing the chain rule,
[2978.54s -> 2981.54s]  then if we have our entire graph
[2981.54s -> 2985.54s]  we can just make a forward pass through the entire graph
[2985.54s -> 2988.54s]  by iterating through all the nodes in the graph,
[2988.54s -> 2989.54s]  all the gates.
[2989.54s -> 2990.54s]  Here I'm going to use the word gate
[2990.54s -> 2991.54s]  and node kind of interchangeably.
[2991.54s -> 2994.54s]  We can iterate through all of these gates
[2994.54s -> 2997.54s]  and just call forward on each of the gates, right,
[2997.54s -> 2998.54s]  and we just want to do this
[2998.54s -> 2999.54s]  in topologically sorted order
[2999.54s -> 3002.54s]  so we process all the inputs coming into a node
[3002.54s -> 3004.54s]  before we process that node.
[3005.54s -> 3006.54s]  And then going backwards,
[3006.54s -> 3008.54s]  we're just going to then go through all of the gates
[3008.54s -> 3010.54s]  in this reverse sorted order
[3010.54s -> 3014.54s]  and then call backwards on each of these gates.
[3015.54s -> 3020.54s]  Okay, and so if we look at then the implementation
[3020.54s -> 3021.54s]  for a particular gate,
[3021.54s -> 3023.54s]  so for example this multiply gate here,
[3023.54s -> 3026.54s]  we want to implement the forward pass, right,
[3026.54s -> 3028.54s]  so it gets x and y as inputs
[3028.54s -> 3031.54s]  and returns the value of z.
[3031.54s -> 3033.54s]  And then when we go backwards, right,
[3033.54s -> 3036.54s]  we get as input dz which is our upstream gradient
[3036.54s -> 3039.54s]  and we want to output the gradients
[3039.54s -> 3041.54s]  on the inputs x and y to pass down, right,
[3041.54s -> 3045.54s]  so we're going to output dx and dy.
[3045.54s -> 3048.54s]  And so in this case, in this example,
[3048.54s -> 3050.54s]  everything is back to the scalar case here.
[3050.54s -> 3054.54s]  And so if we look at this in the forward pass,
[3054.54s -> 3055.54s]  one thing that's important
[3055.54s -> 3056.54s]  is that we need to,
[3056.54s -> 3058.54s]  we should catch the values of the forward pass, right,
[3058.54s -> 3060.54s]  because we end up using this in the backward pass
[3060.54s -> 3062.54s]  a lot of the time.
[3062.54s -> 3064.54s]  So here in the forward pass,
[3064.54s -> 3067.54s]  we want to catch the values of x and y, right,
[3067.54s -> 3070.54s]  and in the backward pass, using the chain rule,
[3070.54s -> 3072.54s]  we're going to, remember,
[3072.54s -> 3073.54s]  take the value of the upstream gradient
[3073.54s -> 3076.54s]  and scale it by the value of the other branch, right,
[3076.54s -> 3078.54s]  and so we'll keep, for dx,
[3078.54s -> 3081.54s]  we'll take our value of self.y that we kept
[3081.54s -> 3084.54s]  and multiply it by dz coming down.
[3085.54s -> 3087.54s]  And same for dy.
[3087.54s -> 3091.54s]  Okay, so if you look at a lot of deep learning,
[3091.54s -> 3093.54s]  you know, frameworks and libraries,
[3093.54s -> 3095.54s]  you'll see that they exactly follow
[3095.54s -> 3097.54s]  this kind of modularization.
[3097.54s -> 3098.54s]  Right, so for example,
[3098.54s -> 3102.54s]  CAFE is a popular deep learning framework,
[3102.54s -> 3103.54s]  and you'll see,
[3103.54s -> 3105.54s]  if you go look through the CAFE source code,
[3105.54s -> 3107.54s]  you'll get to some directory that says layers,
[3107.54s -> 3111.54s]  and in layers, which are basically computational nodes,
[3111.54s -> 3113.54s]  usually layers might be slightly more,
[3113.54s -> 3115.54s]  you know, some of these more complex computational nodes
[3115.54s -> 3118.54s]  like the sigmoid that we talked about earlier,
[3118.54s -> 3121.54s]  you'll see basically just a whole list
[3121.54s -> 3124.54s]  of all different kinds of computational nodes, right?
[3124.54s -> 3125.54s]  So you might have the sigmoid,
[3125.54s -> 3126.54s]  I don't know, there might be here,
[3126.54s -> 3128.54s]  there's like a convolution is one,
[3128.54s -> 3131.54s]  there's an argmax is another layer,
[3131.54s -> 3132.54s]  you'll have all of these layers,
[3132.54s -> 3134.54s]  and if you dig into each of them,
[3134.54s -> 3136.54s]  they're just exactly implementing a forward pass
[3136.54s -> 3137.54s]  and a backward pass.
[3137.54s -> 3140.54s]  And then all of these are called
[3140.54s -> 3142.54s]  when we do forward and backward pass
[3142.54s -> 3144.54s]  through the entire network that we form.
[3144.54s -> 3147.54s]  And so our network is just basically going to be
[3147.54s -> 3149.54s]  stacking up all of these,
[3149.54s -> 3153.54s]  the different layers that we choose to use in the network.
[3153.54s -> 3156.54s]  So for example, if we look at a specific one,
[3156.54s -> 3159.54s]  in this case, a sigmoid layer,
[3159.54s -> 3161.54s]  you'll see that in the sigmoid layer, right,
[3161.54s -> 3163.54s]  we've talked about the sigmoid function,
[3163.54s -> 3166.54s]  you'll see that there's a forward pass,
[3166.54s -> 3170.54s]  which basically computes exactly the sigmoid expression,
[3170.54s -> 3172.54s]  and then a backward pass, right,
[3172.54s -> 3175.54s]  where it is taking as input something,
[3178.54s -> 3179.54s]  basically a top diff,
[3179.54s -> 3181.54s]  which is our upstream gradient in this case,
[3181.54s -> 3185.54s]  and multiplying it by a local gradient that we compute.
[3187.54s -> 3188.54s]  So in assignment one,
[3188.54s -> 3192.54s]  you'll get practice with this kind of,
[3192.54s -> 3194.54s]  this computational graph way of thinking
[3194.54s -> 3197.54s]  where you're going to be writing your SVM
[3197.54s -> 3200.54s]  and softmax losses and taking the gradients of these.
[3200.54s -> 3201.54s]  And so again, remember always,
[3201.54s -> 3203.54s]  you want to first step,
[3203.54s -> 3206.54s]  represent it as a computational graph, right,
[3206.54s -> 3208.54s]  figure out what are all the computations
[3208.54s -> 3210.54s]  that you did leading up to the output,
[3210.54s -> 3213.54s]  and then when it's time to do your backward pass,
[3213.54s -> 3215.54s]  just take the gradient with respect
[3215.54s -> 3217.54s]  to each of these intermediate variables
[3217.54s -> 3221.54s]  that you've defined in your computational graph,
[3221.54s -> 3225.54s]  and use the chain rule to link them all together.
[3226.54s -> 3229.54s]  Okay, so summary of what we've talked about so far.
[3230.54s -> 3232.54s]  When we get down to working with neural networks,
[3232.54s -> 3234.54s]  these are going to be really large and complex,
[3234.54s -> 3235.54s]  so it's going to be impractical
[3235.54s -> 3238.54s]  to write down the gradient formula by hand
[3238.54s -> 3240.54s]  for all your parameters.
[3240.54s -> 3244.54s]  So in order to get these gradients, right,
[3244.54s -> 3246.54s]  we talked about how what we should use
[3246.54s -> 3248.54s]  is backpropagation, right,
[3248.54s -> 3251.54s]  and this is kind of one of the core techniques
[3251.54s -> 3255.54s]  of neural networks is basically using backpropagation
[3257.54s -> 3258.54s]  to get your gradients, right,
[3258.54s -> 3261.54s]  and so this is a recursive application of the chain rule
[3261.54s -> 3263.54s]  where we have this computational graph,
[3263.54s -> 3265.54s]  and we start at the back and we go backwards through it
[3265.54s -> 3267.54s]  to compute the gradients with respect
[3267.54s -> 3269.54s]  to all of the intermediate variables,
[3269.54s -> 3271.54s]  which are your inputs, your parameters,
[3271.54s -> 3274.54s]  and everything else in the middle.
[3274.54s -> 3276.54s]  And we've also talked about how really
[3276.54s -> 3279.54s]  this implementation in this graph structure,
[3279.54s -> 3281.54s]  each of these nodes is really,
[3281.54s -> 3282.54s]  you can see this as implementing
[3282.54s -> 3284.54s]  a forward and backwards API, right,
[3284.54s -> 3286.54s]  and so in the forward pass,
[3286.54s -> 3288.54s]  we want to compute the results of the operation,
[3288.54s -> 3291.54s]  and we want to save any intermediate values
[3291.54s -> 3292.54s]  that we might want to use later
[3292.54s -> 3294.54s]  in our gradient computation,
[3294.54s -> 3295.54s]  and then in the backwards pass,
[3295.54s -> 3297.54s]  we apply this chain rule
[3297.54s -> 3299.54s]  and we take this upstream gradient,
[3299.54s -> 3302.54s]  we chain it, multiply it with our local gradient
[3302.54s -> 3304.54s]  to compute the gradient with respect
[3304.54s -> 3305.54s]  to the inputs of the node,
[3305.54s -> 3308.54s]  and we pass this down to the nodes
[3308.54s -> 3310.54s]  that are connected next.
[3312.54s -> 3314.54s]  Okay, so now, finally,
[3314.54s -> 3317.54s]  we're going to talk about neural networks.
[3317.54s -> 3319.54s]  So, really, you know,
[3321.54s -> 3324.54s]  neural networks, people draw a lot of analogies
[3324.54s -> 3326.54s]  between neural networks and the brain
[3326.54s -> 3328.54s]  and different types of biological inspirations,
[3328.54s -> 3330.54s]  and we'll get to that in a little bit,
[3330.54s -> 3332.54s]  but first, let's talk about it,
[3332.54s -> 3334.54s]  you know, just looking at it as a function,
[3334.54s -> 3338.54s]  as a class of functions without all the brain stuff.
[3338.54s -> 3340.54s]  So, so far, we've talked about,
[3340.54s -> 3341.54s]  you know, we've worked a lot
[3341.54s -> 3343.54s]  with this linear score function, right,
[3343.54s -> 3345.54s]  f equals w times x,
[3345.54s -> 3347.54s]  and so we've been using this as a running example
[3347.54s -> 3351.54s]  of a function that we want to optimize.
[3351.54s -> 3354.54s]  So, instead of using this single linear transformation,
[3354.54s -> 3356.54s]  if we want a neural network,
[3356.54s -> 3358.54s]  we can just, as the simplest form,
[3358.54s -> 3360.54s]  just stack two of these together, right,
[3360.54s -> 3363.54s]  just a linear transformation on top of another one
[3363.54s -> 3367.54s]  in order to get a two-layer neural network, right,
[3367.54s -> 3369.54s]  and so what this looks like is,
[3369.54s -> 3371.54s]  first, we have our, you know,
[3371.54s -> 3374.54s]  a matrix multiply of w1 with x,
[3374.54s -> 3378.54s]  and then we get this intermediate variable,
[3378.54s -> 3381.54s]  and we have this nonlinear function
[3381.54s -> 3384.54s]  of a max of zero with w,
[3384.54s -> 3387.54s]  max with this output of this linear layer,
[3387.54s -> 3389.54s]  and it's really important to have
[3389.54s -> 3391.54s]  these nonlinearities in place,
[3391.54s -> 3393.54s]  which we'll talk about more later,
[3393.54s -> 3395.54s]  because otherwise, if you just stack linear layers
[3395.54s -> 3396.54s]  on top of each other,
[3396.54s -> 3397.54s]  they're just going to collapse to,
[3397.54s -> 3399.54s]  like, a single linear function.
[3399.54s -> 3401.54s]  Okay, so we have our first linear layer,
[3401.54s -> 3404.54s]  and then we have this nonlinearity, right,
[3404.54s -> 3407.54s]  and then on top of this, we'll add another linear layer,
[3407.54s -> 3409.54s]  and then from here, finally,
[3409.54s -> 3411.54s]  we can get our score function,
[3411.54s -> 3413.54s]  our output vector of scores.
[3413.54s -> 3416.54s]  So, basically, like, more broadly speaking,
[3416.54s -> 3419.54s]  neural networks are a class of functions
[3419.54s -> 3421.54s]  where we have simpler functions, right,
[3421.54s -> 3423.54s]  that are stacked on top of each other,
[3423.54s -> 3425.54s]  and we stack them in a hierarchical way
[3425.54s -> 3428.54s]  in order to make up a more complex nonlinear function,
[3428.54s -> 3431.54s]  and so this is the idea of having, basically,
[3431.54s -> 3434.54s]  multiple stages of hierarchical computation.
[3434.54s -> 3437.54s]  Right, and so, you know,
[3437.54s -> 3440.54s]  so this is kind of the main way that we do this
[3440.54s -> 3443.54s]  is by taking something like this matrix multiply,
[3443.54s -> 3444.54s]  this linear layer,
[3444.54s -> 3446.54s]  and we just stack multiple of these
[3446.54s -> 3448.54s]  on top of each other
[3448.54s -> 3453.54s]  with the nonlinear functions in between, right.
[3454.54s -> 3456.54s]  And so one thing that this can help solve
[3456.54s -> 3458.54s]  is if we look, if we remember back
[3458.54s -> 3460.54s]  to this linear score function that we were talking about,
[3460.54s -> 3463.54s]  right, remember we discussed earlier
[3463.54s -> 3466.54s]  how each row of our weight matrix W
[3466.54s -> 3469.54s]  was something like a template, right,
[3469.54s -> 3471.54s]  it was a template that sort of expressed,
[3471.54s -> 3473.54s]  you know, what we're looking for
[3473.54s -> 3476.54s]  in the input for a specific class, right,
[3476.54s -> 3478.54s]  so for example, you know, the card template
[3478.54s -> 3480.54s]  looks something like this kind of fuzzy red card
[3480.54s -> 3482.54s]  and we were looking for this in the input
[3482.54s -> 3485.54s]  to compute the score for the card class, right,
[3485.54s -> 3487.54s]  and we talked about one of the problems with this
[3487.54s -> 3489.54s]  is that there's only one template, right,
[3489.54s -> 3490.54s]  there's this red card,
[3490.54s -> 3492.54s]  whereas in practice, we actually have multiple modes,
[3492.54s -> 3493.54s]  right, we might want,
[3493.54s -> 3495.54s]  we're looking for, you know, a red card,
[3495.54s -> 3496.54s]  there's also a yellow card,
[3496.54s -> 3499.54s]  like all of these are different kinds of cards.
[3499.54s -> 3502.54s]  And so what this kind of
[3502.54s -> 3505.54s]  multiple layer network lets you do
[3505.54s -> 3507.54s]  is now, you know,
[3507.54s -> 3510.54s]  each of this intermediate variable H, right,
[3510.54s -> 3513.54s]  W1 can still be these kinds of templates,
[3513.54s -> 3515.54s]  but now you have all of these scores
[3515.54s -> 3517.54s]  for these templates in H
[3517.54s -> 3519.54s]  and we can have another layer on top
[3519.54s -> 3522.54s]  that's combining these together, right,
[3522.54s -> 3525.54s]  so we can say that actually my card class should be,
[3525.54s -> 3527.54s]  you know, connected to,
[3527.54s -> 3530.54s]  we're looking for both red cards as well as yellow cards,
[3530.54s -> 3533.54s]  right, because we have this matrix W2,
[3533.54s -> 3535.54s]  which is now awaiting,
[3535.54s -> 3538.54s]  of all of our, of our vector in H.
[3542.54s -> 3545.54s]  Okay, any questions about this?
[3545.54s -> 3546.54s]  Yeah.
[3550.54s -> 3552.54s]  Yeah, so there's a lot of ways,
[3552.54s -> 3554.54s]  so there's a lot of different non-linear functions
[3554.54s -> 3556.54s]  that you can choose from
[3556.54s -> 3558.54s]  and we'll talk later on in a later lecture
[3558.54s -> 3560.54s]  about all the different kinds of non-linearities
[3560.54s -> 3563.54s]  that you might want to use.
[3563.54s -> 3564.54s]  Yeah.
[3585.54s -> 3590.54s]  So, so W1, because it's directly connected to the input X,
[3590.54s -> 3592.54s]  this is what's like really interpretable, right,
[3592.54s -> 3596.54s]  because you're, you can formulate all of these templates.
[3596.54s -> 3599.54s]  W2, so H is going to be a score
[3599.54s -> 3603.54s]  of how much of each template you saw, for example, right,
[3603.54s -> 3605.54s]  so it might be like you have, you know,
[3605.54s -> 3609.54s]  like a, I don't know, two for the red card
[3609.54s -> 3613.54s]  and like one for the yellow card or something like that.
[3613.54s -> 3614.54s]  Yeah.
[3621.54s -> 3625.54s]  Exactly, so the question is basically whether,
[3625.54s -> 3627.54s]  in W1 you could have both left-facing course
[3627.54s -> 3629.54s]  and right-facing course, right,
[3629.54s -> 3632.54s]  and so, yeah, exactly, so now W1 can be
[3632.54s -> 3634.54s]  many different kinds of templates, right,
[3634.54s -> 3637.54s]  there are not, and then W2 now,
[3637.54s -> 3640.54s]  like basically is a weighted sum of all of these templates.
[3640.54s -> 3642.54s]  So now it allows you to weight together
[3642.54s -> 3645.54s]  multiple templates in order to get the final score
[3645.54s -> 3647.54s]  for a particular class.
[3662.54s -> 3664.54s]  Right, so, okay, so the question is,
[3664.54s -> 3668.54s]  if our image X is like a left-facing course
[3668.54s -> 3671.54s]  and in W1 we have a template of a left-facing course
[3671.54s -> 3674.54s]  and a right-facing course, then what's happening, right?
[3674.54s -> 3677.54s]  So what happens is, yeah, so in H you might have
[3677.54s -> 3680.54s]  a really high score for your left-facing course,
[3680.54s -> 3684.54s]  kind of a lower score for your right-facing course,
[3684.54s -> 3687.54s]  and W2 is, it's a weighted sum, so it's not a maximum.
[3687.54s -> 3690.54s]  It's a weighted sum of these templates,
[3690.54s -> 3693.54s]  but if you have either a really high score
[3693.54s -> 3695.54s]  for one of these templates, or let's say you have
[3695.54s -> 3698.54s]  kind of a lower, a medium score for both of these templates,
[3698.54s -> 3700.54s]  all of these kinds of combinations
[3700.54s -> 3701.54s]  have high scores, right?
[3701.54s -> 3703.54s]  And so in the end, what you're going to get
[3703.54s -> 3705.54s]  is something that generally scores high
[3705.54s -> 3708.54s]  when you have a course of any kind.
[3708.54s -> 3710.54s]  Right, so let's say you had a front-facing course.
[3710.54s -> 3712.54s]  You might have medium values
[3712.54s -> 3716.54s]  for both the left and the right templates.
[3716.54s -> 3718.54s]  Yeah, question?
[3721.54s -> 3722.54s]  W2 is doing the weighting.
[3722.54s -> 3724.54s]  So the question is, is W2 doing the weighting
[3724.54s -> 3726.54s]  or is H doing the weighting?
[3726.54s -> 3729.54s]  H is the value, like in this example,
[3729.54s -> 3733.54s]  H is the value of scores for each of your templates
[3733.54s -> 3735.54s]  that you have in W1.
[3736.54s -> 3740.54s]  Right, so H is like the score function, right?
[3740.54s -> 3744.54s]  It's how much of each template in W1 is present,
[3744.54s -> 3748.54s]  and then W2 is going to weight all of these,
[3748.54s -> 3750.54s]  weight all of these intermediate scores
[3750.54s -> 3753.54s]  to get your final score for the class.
[3755.54s -> 3757.54s]  So the question is, which is the nonlinear thing?
[3757.54s -> 3762.54s]  So the nonlinearity usually happens right before H.
[3762.54s -> 3766.54s]  So H is the value right after the nonlinearity.
[3766.54s -> 3769.54s]  So we're talking about this intuitively
[3769.54s -> 3772.54s]  as this example of W1 is looking for,
[3772.54s -> 3774.54s]  has these same templates as before
[3774.54s -> 3776.54s]  and W2 is weighting for these.
[3776.54s -> 3778.54s]  In practice, it's not exactly like this, right?
[3778.54s -> 3780.54s]  Because as you said,
[3780.54s -> 3782.54s]  there's all these nonlinearities thrown in and so on,
[3782.54s -> 3787.54s]  but it has this approximate type of interpretation to it.
[3793.54s -> 3795.54s]  Yeah, so the question is H just W1 X?
[3795.54s -> 3800.54s]  So H is just W1 times X with the max function on top.
[3808.54s -> 3810.54s]  Okay, so we talked about this as an example
[3810.54s -> 3811.54s]  of a two-layer neural network,
[3811.54s -> 3813.54s]  and we can stack more layers of these
[3813.54s -> 3815.54s]  to get deeper networks of arbitrary depth, right?
[3815.54s -> 3817.54s]  So we can just do this one more time
[3817.54s -> 3820.54s]  at another max nonlinearity
[3820.54s -> 3823.54s]  and matrix multiply now by W3,
[3823.54s -> 3826.54s]  and now we have a three-layer neural network, right?
[3826.54s -> 3828.54s]  And so this is where the term deep neural network
[3828.54s -> 3830.54s]  is basically coming from, right?
[3830.54s -> 3833.54s]  This idea that you can stack multiple of these layers,
[3833.54s -> 3836.54s]  you know, for very deep networks.
[3836.54s -> 3841.54s]  And so in homework, you'll get a practice of writing
[3842.54s -> 3844.54s]  and, you know, training one of these neural networks,
[3844.54s -> 3846.54s]  I think, in assignment two,
[3846.54s -> 3849.54s]  but basically a full implementation of this
[3849.54s -> 3851.54s]  using this idea of forward pass, right,
[3851.54s -> 3854.54s]  and backward passes and using chain rule
[3854.54s -> 3856.54s]  to compute gradients that we've already seen.
[3856.54s -> 3859.54s]  The entire implementation of a two-layer neural network
[3859.54s -> 3860.54s]  is actually really simple.
[3860.54s -> 3863.54s]  It can just be done in 20 lines.
[3863.54s -> 3865.54s]  And so you'll get some practice with this
[3865.54s -> 3869.54s]  in assignment two, writing out all of these parts.
[3869.54s -> 3872.54s]  And, okay, so now that we've sort of seen
[3872.54s -> 3874.54s]  what neural networks are as a function, right,
[3874.54s -> 3876.54s]  like, you know, we hear people talking a lot
[3876.54s -> 3879.54s]  about how there's biological inspirations
[3879.54s -> 3880.54s]  for neural networks,
[3880.54s -> 3883.54s]  and so even though it's important to emphasize
[3883.54s -> 3886.54s]  that these analogies are really loose,
[3886.54s -> 3889.54s]  it's really just very loose ties.
[3889.54s -> 3891.54s]  It's, by and large,
[3891.54s -> 3895.54s]  very loose ties, but it's still interesting to understand
[3895.54s -> 3898.54s]  where some of these connections and inspirations come from.
[3898.54s -> 3902.54s]  And so now I'm going to talk briefly about that.
[3902.54s -> 3904.54s]  So if we think about a neuron
[3904.54s -> 3907.54s]  in kind of a very simple way,
[3907.54s -> 3910.54s]  this neuron is, here's a diagram of a neuron.
[3910.54s -> 3912.54s]  We have the impulses that are carried
[3912.54s -> 3914.54s]  towards each neuron, right?
[3914.54s -> 3916.54s]  So we have a lot of neurons connected together.
[3916.54s -> 3918.54s]  And each neuron has dendrites,
[3918.54s -> 3919.54s]  and these are sort of,
[3919.54s -> 3921.54s]  these are what receives the impulses
[3921.54s -> 3923.54s]  that come into the neuron.
[3923.54s -> 3925.54s]  And then we have a cell body, right,
[3925.54s -> 3928.54s]  that basically integrates these signals coming in,
[3928.54s -> 3932.54s]  and then there's a kind of, then it takes this,
[3932.54s -> 3934.54s]  and after integrating all of these signals,
[3934.54s -> 3936.54s]  it passes on, you know,
[3936.54s -> 3938.54s]  the impulse carries away from the cell body
[3938.54s -> 3941.54s]  to downstream neurons that it's connected to, right,
[3941.54s -> 3945.54s]  and it carries this away through axons.
[3946.54s -> 3950.54s]  So now if we look at what we've been doing so far, right,
[3950.54s -> 3952.54s]  with each computational node,
[3952.54s -> 3955.54s]  you can see that this actually has,
[3955.54s -> 3958.54s]  you can see it in kind of a similar way, right,
[3958.54s -> 3960.54s]  where nodes are connected to each other
[3960.54s -> 3962.54s]  in a computational graph,
[3962.54s -> 3966.54s]  and we have inputs or signals, x, x, right,
[3966.54s -> 3968.54s]  coming into a neuron.
[3968.54s -> 3972.54s]  And then all of these x, right, x0, x1, x2,
[3972.54s -> 3974.54s]  these are combined and integrated together,
[3975.54s -> 3978.54s]  right, using, for example, our weights, w, right,
[3978.54s -> 3981.54s]  so we do some sort of computation, right,
[3981.54s -> 3984.54s]  in some of the computations we've been doing so far,
[3984.54s -> 3988.54s]  something like w times x plus b, right,
[3988.54s -> 3990.54s]  integrating all of these together,
[3990.54s -> 3992.54s]  and then we have an activation function
[3992.54s -> 3994.54s]  that we apply on top, we get this value,
[3994.54s -> 3996.54s]  this output, and we pass it down
[3996.54s -> 3998.54s]  to the connecting neurons.
[4000.54s -> 4002.54s]  So if you look at this, this is actually,
[4003.54s -> 4005.54s]  you can think about this in a very similar way, right,
[4005.54s -> 4008.54s]  like, you know, these are, what's the signals coming in
[4008.54s -> 4012.54s]  are kind of the connected at synapses, right,
[4012.54s -> 4015.54s]  the synapse connecting the multiple neurons,
[4015.54s -> 4018.54s]  the dendrites are integrating all of these,
[4018.54s -> 4021.54s]  they're integrating all of this information together
[4021.54s -> 4023.54s]  in the cell body, and then we have the output
[4023.54s -> 4026.54s]  carried on the output later on.
[4027.54s -> 4029.54s]  And so this is kind of the analogy
[4029.54s -> 4030.54s]  that you can draw between them,
[4030.54s -> 4033.54s]  and if you look at these activation functions,
[4033.54s -> 4036.54s]  right, this is what basically takes
[4036.54s -> 4038.54s]  all the inputs coming in when output's one number
[4038.54s -> 4040.54s]  that's going out later on,
[4040.54s -> 4042.54s]  and we've talked about examples like
[4042.54s -> 4045.54s]  sigmoid activation function, right,
[4045.54s -> 4047.54s]  and different kinds of non-linearities,
[4047.54s -> 4051.54s]  and so sort of one kind of loose analogy
[4052.54s -> 4055.54s]  that you can draw is that these non-linearities
[4055.54s -> 4057.54s]  can represent something sort of like
[4057.54s -> 4061.54s]  the firing or spiking rate of the neurons, right,
[4061.54s -> 4065.54s]  where neurons transmit signals to connected neurons
[4065.54s -> 4068.54s]  using kind of these discrete spikes, right,
[4068.54s -> 4070.54s]  and so we can think of, you know,
[4070.54s -> 4072.54s]  if they're spiking very fast,
[4072.54s -> 4074.54s]  then there's kind of a strong signal
[4074.54s -> 4075.54s]  that's passed later on,
[4075.54s -> 4077.54s]  and so we can think of this value
[4077.54s -> 4080.54s]  after our activation function as sort of,
[4080.54s -> 4082.54s]  in a sense, sort of this firing rate
[4082.54s -> 4084.54s]  that we're going to pass on.
[4084.54s -> 4086.54s]  And, you know, in practice,
[4086.54s -> 4088.54s]  I think neuroscientists who are actually studying this
[4088.54s -> 4090.54s]  say that kind of one of the non-linearities
[4090.54s -> 4093.54s]  that are most similar to the way
[4093.54s -> 4095.54s]  that neurons are actually behaving
[4095.54s -> 4098.54s]  is a ReLU function, which is a ReLU non-linearity,
[4098.54s -> 4100.54s]  which is something that we're going to
[4100.54s -> 4101.54s]  look at more later on,
[4101.54s -> 4104.54s]  but it's a function that's a zero
[4104.54s -> 4107.54s]  for all negative values of input,
[4107.54s -> 4109.54s]  and then it's a linear function
[4109.54s -> 4113.54s]  for everything that's in kind of the positive regime.
[4113.54s -> 4114.54s]  And so, you know,
[4114.54s -> 4115.54s]  we'll talk more about this activation function later on,
[4116.54s -> 4118.54s]  but that's kind of, in practice,
[4118.54s -> 4120.54s]  maybe the one that's most similar
[4120.54s -> 4123.54s]  to how neurons are actually behaving.
[4125.54s -> 4128.54s]  But it's really important to be extremely careful
[4128.54s -> 4131.54s]  with making any of these sorts of brain analogies,
[4131.54s -> 4133.54s]  because in practice, biological neurons
[4133.54s -> 4135.54s]  are way more complex than this.
[4135.54s -> 4139.54s]  There's many different kinds of biological neurons.
[4139.54s -> 4141.54s]  The dendrites can perform really complex
[4141.54s -> 4144.54s]  non-linear computations.
[4144.54s -> 4147.54s]  Our synapses, right, the W zeros that we had earlier,
[4147.54s -> 4149.54s]  where we drew this analogy,
[4149.54s -> 4151.54s]  are not single weights like we had.
[4151.54s -> 4153.54s]  They're actually really complex
[4153.54s -> 4156.54s]  non-linear dynamical systems in practice,
[4156.54s -> 4159.54s]  and also this idea of interpreting
[4159.54s -> 4162.54s]  our activation function as a sort of rate code
[4162.54s -> 4167.54s]  or firing rate is also, is insufficient in practice.
[4167.54s -> 4170.54s]  It's just this kind of firing rate
[4170.54s -> 4172.54s]  is probably not a sufficient model
[4172.54s -> 4175.54s]  of how neurons will actually communicate
[4175.54s -> 4176.54s]  to downstream neurons, right?
[4176.54s -> 4179.54s]  Like, even as a very simple way,
[4179.54s -> 4181.54s]  there's a very, the neurons will fire
[4181.54s -> 4183.54s]  at a variable rate,
[4183.54s -> 4187.54s]  and this variability probably should be taken into account.
[4187.54s -> 4189.54s]  And so, there's all these, you know,
[4189.54s -> 4192.54s]  it's kind of a much more complex thing
[4192.54s -> 4195.54s]  than what we're dealing with.
[4195.54s -> 4197.54s]  There's references, for example,
[4197.54s -> 4200.54s]  this dendritic computation that you can look at
[4200.54s -> 4203.54s]  if you're interested in this topic.
[4203.54s -> 4205.54s]  But, yeah, so, but in practice, you know,
[4205.54s -> 4208.54s]  we can sort of see how it may resemble a neuron
[4208.54s -> 4210.54s]  at this very high level,
[4210.54s -> 4215.54s]  but neurons are in practice much more complicated than that.
[4215.54s -> 4217.54s]  Okay, so, we talked about how there's, you know,
[4217.54s -> 4219.54s]  many different kinds of activation functions
[4219.54s -> 4220.54s]  that could be used.
[4220.54s -> 4223.54s]  There's the ReLU that I mentioned earlier,
[4223.54s -> 4225.54s]  and we'll talk about all of these different kinds
[4225.54s -> 4229.54s]  of activation functions in much more detail later on,
[4229.54s -> 4231.54s]  and the choices of these activation functions
[4231.54s -> 4233.54s]  that you might want to use.
[4233.54s -> 4235.54s]  And so, we'll also talk about different kinds
[4235.54s -> 4238.54s]  of neural network architectures.
[4238.54s -> 4240.54s]  So we gave the example
[4241.54s -> 4244.54s]  of these fully connected neural networks, right,
[4244.54s -> 4247.54s]  where each layer is this matrix multiply.
[4247.54s -> 4249.54s]  And so, so the way we,
[4250.54s -> 4252.54s]  so the way we actually want to call these
[4252.54s -> 4255.54s]  is we set two-layer neural network before,
[4255.54s -> 4257.54s]  and that corresponded to the fact that we have
[4257.54s -> 4260.54s]  two of these linear layers, right,
[4260.54s -> 4262.54s]  where we're doing a matrix multiply.
[4262.54s -> 4265.54s]  Two fully connected layers is what we call these.
[4265.54s -> 4268.54s]  We could also call this a one-hidden layer neural network.
[4268.54s -> 4270.54s]  So instead of counting the number
[4270.54s -> 4272.54s]  of matrix multiplies we're doing,
[4272.54s -> 4274.54s]  counting the number of hidden layers that we have.
[4274.54s -> 4276.54s]  I think it's, you can use either.
[4276.54s -> 4278.54s]  I think maybe two-layer neural network
[4278.54s -> 4280.54s]  is something that's a little more commonly used.
[4280.54s -> 4282.54s]  And then also here,
[4282.54s -> 4284.54s]  for our three-layer neural network that we have,
[4284.54s -> 4288.54s]  this can also be called a two-hidden layer neural network.
[4292.54s -> 4295.54s]  And so, we saw that, you know,
[4295.54s -> 4297.54s]  when we're doing this type of feed-forward, right,
[4297.54s -> 4300.54s]  forward pass through a neural network,
[4300.54s -> 4303.54s]  each of these, each of these
[4304.54s -> 4307.54s]  nodes in this network is basically
[4308.54s -> 4311.54s]  doing the kind of operation of the neuron
[4311.54s -> 4315.54s]  that I, you know, showed earlier, right?
[4315.54s -> 4317.54s]  And so, what's actually happening is
[4317.54s -> 4319.54s]  it's basically each hidden layer
[4319.54s -> 4321.54s]  you can think of as a whole vector, right,
[4321.54s -> 4323.54s]  a set of these neurons.
[4323.54s -> 4325.54s]  And so, by writing it out this way
[4325.54s -> 4330.54s]  with these matrix multiplies to compute our neuron values,
[4330.54s -> 4332.54s]  it's a way that we can officially evaluate
[4332.54s -> 4334.54s]  this entire layer of neurons, right?
[4334.54s -> 4336.54s]  So with one matrix multiply,
[4336.54s -> 4339.54s]  we get output values of, you know,
[4339.54s -> 4344.54s]  of a layer of, let's say, 10 or 50 or 100 of neurons.
[4345.54s -> 4348.54s]  Right, and so, so looking at this,
[4348.54s -> 4352.54s]  again, writing this all out in matrix form,
[4352.54s -> 4355.54s]  matrix vector form, we have our, you know,
[4355.54s -> 4358.54s]  nonlinearity here, f, that we're using,
[4358.54s -> 4360.54s]  in this case, the sigmoid function, right?
[4360.54s -> 4365.54s]  And we can take our data x, some input vector of values,
[4365.54s -> 4367.54s]  and we can apply our first matrix multiply,
[4367.54s -> 4372.54s]  w1 on top of this, then our nonlinearity,
[4372.54s -> 4373.54s]  then a second matrix multiply
[4373.54s -> 4375.54s]  to get a second hidden layer, h2,
[4375.54s -> 4378.54s]  and then we have our final output, right?
[4378.54s -> 4381.54s]  And so, you know, this is basically all you need
[4381.54s -> 4384.54s]  to be able to write a neural network.
[4384.54s -> 4387.54s]  And as we saw earlier, the backward pass,
[4387.54s -> 4390.54s]  you then just use backprop to compute all of those.
[4390.54s -> 4394.54s]  And so, that's basically all there is
[4394.54s -> 4398.54s]  to kind of the main idea of what's a neural network.
[4400.54s -> 4402.54s]  Okay, so just to summarize,
[4402.54s -> 4406.54s]  we talked about how we can arrange neurons
[4406.54s -> 4407.54s]  into these computations, right,
[4407.54s -> 4410.54s]  of fully connected or linear layers.
[4410.54s -> 4413.54s]  This abstraction of a layer has a nice property
[4413.54s -> 4415.54s]  that we can use very efficient vectorized code
[4415.54s -> 4417.54s]  to compute all of these.
[4417.54s -> 4420.54s]  We also talked about how it's important to keep in mind
[4420.54s -> 4423.54s]  that neural networks do have some, you know,
[4423.54s -> 4426.54s]  analogy and loose inspiration from biology,
[4426.54s -> 4428.54s]  but they're not really neural.
[4428.54s -> 4430.54s]  I mean, this is a pretty loose analogy
[4430.54s -> 4431.54s]  that we're making.
[4431.54s -> 4433.54s]  And next time, we'll talk about
[4433.54s -> 4435.54s]  convolutional neural networks.
[4435.54s -> 4436.54s]  Okay, thanks.
