# Detected language: en (p=1.00)

[0.00s -> 2.84s]  Welcome to lecture two of CS231n.
[2.84s -> 5.40s]  On Tuesday, we, just recall, we sort of gave you
[5.40s -> 7.88s]  the big picture view of what is computer vision,
[7.88s -> 9.44s]  what is the history, and a little bit
[9.44s -> 11.08s]  of the overview of the class.
[11.08s -> 12.48s]  And today, we're really gonna dive in
[12.48s -> 14.28s]  for the first time into the details.
[14.28s -> 16.24s]  And we'll start to see in much more depth
[16.24s -> 18.52s]  exactly how some of these learning algorithms
[18.52s -> 20.64s]  actually work in practice.
[20.64s -> 22.08s]  So the first lecture of the class
[22.08s -> 24.88s]  is going to be about the history of computer vision.
[24.88s -> 26.24s]  And we're going to be talking about
[26.24s -> 28.00s]  the history of computer vision.
[28.88s -> 30.08s]  So the first lecture of the class
[30.08s -> 32.92s]  is probably the sort of the largest big picture vision.
[32.92s -> 34.84s]  And the majority of the lectures in this class
[34.84s -> 36.60s]  will be much more detail-oriented,
[36.60s -> 38.36s]  much more focused on the specific mechanics
[38.36s -> 40.60s]  of these different algorithms.
[40.60s -> 42.32s]  So today, we'll see our first learning algorithm,
[42.32s -> 44.36s]  and that'll be really exciting, I think.
[44.36s -> 45.84s]  But before we get to that,
[45.84s -> 48.44s]  I wanted to talk about a couple administrative issues.
[48.44s -> 50.64s]  One is Piazza.
[50.64s -> 52.60s]  So I saw, when I checked yesterday,
[52.60s -> 55.20s]  it seemed like we had maybe 500 students
[55.20s -> 56.36s]  signed up on Piazza,
[56.36s -> 57.84s]  which means that there are several hundred of you
[57.84s -> 59.52s]  who are not yet there.
[59.52s -> 62.56s]  So we really want Piazza to be the main source
[62.56s -> 65.32s]  of communication between the students and the core staff.
[65.32s -> 68.44s]  So we've gotten a lot of questions to the staff list
[68.44s -> 70.52s]  about project ideas,
[70.52s -> 72.28s]  or questions about midterm attendance,
[72.28s -> 73.96s]  or poster session attendance.
[73.96s -> 75.44s]  And any sort of questions like that
[75.44s -> 77.16s]  should really go to Piazza.
[77.16s -> 78.88s]  You'll probably get answers to your questions
[78.88s -> 79.84s]  faster on Piazza,
[79.84s -> 82.32s]  because all the TAs are knowing to check that.
[82.32s -> 84.40s]  And it's sort of easy for emails to get lost
[84.40s -> 87.36s]  in the shuffle if you just send to the course list.
[87.40s -> 90.00s]  It's also come to my attention that some SCPD students
[90.00s -> 92.64s]  are having a bit of a hard time signing up for Piazza.
[94.80s -> 96.56s]  SCPD students are supposed to receive
[96.56s -> 99.80s]  a .stanford, at stanford.edu email address.
[99.80s -> 101.72s]  So once you get that email address,
[101.72s -> 105.16s]  then you can use the Stanford email to sign into Piazza.
[105.16s -> 106.76s]  Probably that doesn't affect those of you
[106.76s -> 107.92s]  who are sitting in the room right now,
[107.92s -> 110.50s]  but for those students listening on SCPD.
[113.60s -> 116.60s]  The next administrative issue is about assignment one.
[116.60s -> 119.16s]  Assignment one will be up later today,
[119.16s -> 120.56s]  probably sometime this afternoon,
[120.56s -> 124.20s]  but I promise before I go to sleep tonight, it'll be up.
[124.20s -> 125.80s]  But if you're getting a little bit antsy
[125.80s -> 128.04s]  and really wanna start working on it right now,
[128.04s -> 130.32s]  then you can look at last year's version
[130.32s -> 131.46s]  of assignment one.
[131.46s -> 133.64s]  It'll be pretty much the same content.
[133.64s -> 135.32s]  We're just reshuffling it a little bit
[135.32s -> 137.36s]  to make it, for example, upgrading to work
[137.36s -> 139.86s]  with Python 3 rather than Python 2.7,
[139.86s -> 142.00s]  and some of these minor cosmetic changes.
[142.00s -> 143.44s]  But the content of the assignment
[143.44s -> 145.74s]  will still be the same as last year.
[145.90s -> 147.78s]  In this assignment, you'll be implementing
[147.78s -> 149.62s]  your own k-nearest neighbor classifier,
[149.62s -> 151.68s]  which we're gonna talk about in this lecture.
[151.68s -> 154.46s]  You'll also implement several different linear classifiers,
[154.46s -> 156.62s]  including the SVM and Softmax,
[156.62s -> 159.18s]  as well as a simple two-layer neural network.
[159.18s -> 160.20s]  We'll cover all of this content
[160.20s -> 161.90s]  over the next couple of lectures.
[164.38s -> 167.34s]  All of our assignments are using Python and NumPy.
[167.34s -> 170.22s]  If you aren't familiar with Python or NumPy,
[170.22s -> 171.94s]  then we have written a tutorial
[171.94s -> 173.58s]  that you can find on the course website
[173.58s -> 175.50s]  to try and get you up to speed.
[176.10s -> 177.98s]  This is actually pretty important.
[177.98s -> 179.98s]  NumPy lets you write these very efficient
[179.98s -> 182.18s]  vectorized operations that let you do
[182.18s -> 185.38s]  quite a lot of computation in just a couple lines of code.
[185.38s -> 188.10s]  This is super important for pretty much all aspects
[188.10s -> 190.22s]  of numerical computing and machine learning
[190.22s -> 191.42s]  and everything like that,
[191.42s -> 195.10s]  is efficiently implementing these vectorized operations.
[195.10s -> 196.90s]  You'll get a lot of practice with this
[196.90s -> 198.70s]  on the first assignment.
[198.70s -> 200.82s]  For those of you who don't have a lot of experience
[200.82s -> 203.66s]  with MATLAB or NumPy or other types
[203.66s -> 205.98s]  of vectorized tensor computation,
[205.98s -> 207.54s]  I recommend that you start looking
[207.54s -> 208.66s]  at this assignment pretty early
[208.66s -> 210.86s]  and also read carefully through the tutorial.
[213.22s -> 215.22s]  The other thing I wanted to talk about
[215.22s -> 217.14s]  is that we're happy to announce
[217.14s -> 219.14s]  that we're officially supported
[219.14s -> 221.54s]  through Google Cloud for this class.
[221.54s -> 224.62s]  Google Cloud is somewhat similar to Amazon AWS.
[224.62s -> 227.74s]  You can go and start virtual machines up in the cloud.
[227.74s -> 229.82s]  These virtual machines can have GPUs.
[231.54s -> 233.14s]  We're working on the tutorial
[233.14s -> 234.94s]  for exactly how to use Google Cloud
[234.94s -> 236.58s]  and get it to work for the assignments.
[236.58s -> 238.90s]  But our intention is that you'll be able
[238.90s -> 240.90s]  to just download some image
[240.90s -> 242.14s]  and it'll be very seamless for you
[242.14s -> 243.42s]  to work on the assignment
[243.42s -> 245.86s]  on one of these instances on the cloud.
[245.86s -> 248.22s]  And because Google has very generously
[248.22s -> 249.52s]  supported this course,
[249.52s -> 251.62s]  we'll be able to distribute to each of you
[251.62s -> 254.58s]  coupons that let you use Google Cloud credits
[254.58s -> 256.54s]  for free for the class.
[256.54s -> 259.78s]  So you can feel free to use these for the assignments
[259.78s -> 261.10s]  and also for the course projects
[261.10s -> 262.78s]  when you want to start using GPUs
[263.38s -> 265.02s]  and larger machines and whatnot.
[265.02s -> 266.98s]  So we'll post more details about that
[266.98s -> 268.86s]  probably on Piazza later today.
[268.86s -> 270.14s]  But I just wanted to mention,
[270.14s -> 272.38s]  because I know there had been a couple of questions
[272.38s -> 274.46s]  about can I use my laptop,
[274.46s -> 275.46s]  do I have to run on corn,
[275.46s -> 276.86s]  do I have to, whatever.
[276.86s -> 278.30s]  And the answer is that you'll be able
[278.30s -> 279.38s]  to run on Google Cloud
[279.38s -> 282.26s]  and we'll provide you some coupons for that.
[284.86s -> 288.12s]  Yeah, so those are kind of the major
[288.12s -> 290.44s]  administrative issues I wanted to talk about today.
[290.80s -> 293.28s]  And then let's dive into the content.
[294.64s -> 296.52s]  So the last lecture we talked a little bit
[296.52s -> 299.08s]  about this task of image classification,
[299.08s -> 301.46s]  which is really a core task in computer vision.
[301.46s -> 303.60s]  And this is something that we'll really focus on
[303.60s -> 305.00s]  throughout the course of the class,
[305.00s -> 306.64s]  is exactly how do we work
[306.64s -> 308.84s]  on this image classification task.
[308.84s -> 310.76s]  So a little bit more concretely,
[310.76s -> 313.24s]  when you're doing image classification,
[313.24s -> 315.44s]  your system receives some input image,
[315.44s -> 317.50s]  which is this cute cat in this example,
[317.70s -> 320.98s]  and the system is aware of some predetermined set
[320.98s -> 323.38s]  of categories or labels.
[323.38s -> 325.94s]  So these might be like a dog or a cat
[325.94s -> 327.30s]  or a truck or a plane,
[327.30s -> 330.02s]  and there's some fixed set of category labels,
[330.02s -> 332.30s]  and the job of the computer is to look at the picture
[332.30s -> 335.70s]  and assign it one of these fixed category labels.
[335.70s -> 337.34s]  This seems like a really easy problem
[337.34s -> 340.62s]  because so much of your own visual system
[340.62s -> 342.60s]  and your brain is hardwired
[342.60s -> 345.52s]  for doing these sort of visual recognition tasks.
[345.60s -> 347.56s]  But this is actually a really, really hard problem
[347.56s -> 349.20s]  for a machine.
[349.20s -> 351.36s]  So if you dig in and think about actually
[351.36s -> 354.20s]  what does a computer see when it looks at this image,
[354.20s -> 356.80s]  it definitely doesn't get this holistic idea of a cat
[356.80s -> 358.42s]  that you see when you look at it.
[358.42s -> 360.48s]  And the computer really is representing the image
[360.48s -> 362.24s]  as this gigantic grid of numbers.
[363.84s -> 368.18s]  So the image might be something like 800 by 600 pixels,
[368.18s -> 371.00s]  and each pixel is represented by three numbers,
[371.00s -> 374.12s]  giving the red, green, and blue values for that pixel.
[374.12s -> 375.12s]  So to the computer,
[375.12s -> 376.76s]  this is just a gigantic grid of numbers,
[376.76s -> 379.56s]  and it's very difficult to distill the catness
[379.56s -> 383.76s]  out of this giant array of thousands or whatever,
[383.76s -> 385.32s]  very many different numbers.
[387.84s -> 390.32s]  So we refer to this problem as the semantic gap,
[390.32s -> 393.72s]  that this idea of a cat or this label of a cat
[393.72s -> 396.52s]  is a semantic label that we're assigning to this image,
[396.52s -> 397.80s]  and there's this huge gap
[397.80s -> 399.78s]  between the semantic idea of a cat
[399.78s -> 401.28s]  and these pixel values
[401.28s -> 403.84s]  that the computer is actually seeing.
[404.40s -> 405.24s]  And this is a really hard problem
[405.24s -> 407.68s]  because you can change the picture
[407.68s -> 409.82s]  in very small, subtle ways
[409.82s -> 412.80s]  that will cause this pixel grid to change entirely.
[412.80s -> 414.88s]  So for example, if we took this same cat,
[414.88s -> 416.56s]  and if the cat happened to sit still
[416.56s -> 418.24s]  and not even twitch, not move a muscle,
[418.24s -> 419.92s]  which is never gonna happen,
[419.92s -> 421.96s]  but we moved the camera to the other side,
[421.96s -> 423.72s]  then every single grid,
[423.72s -> 426.52s]  every single pixel in this giant grid of numbers
[426.52s -> 427.80s]  would be completely different.
[427.80s -> 430.32s]  But somehow, it's still representing the same cat,
[430.32s -> 432.64s]  and our algorithms need to be robust to this.
[433.96s -> 436.20s]  But not only viewpoint is one problem,
[436.20s -> 437.24s]  another is illumination.
[437.24s -> 438.76s]  There can be different lighting conditions
[438.76s -> 440.32s]  going on in the scene,
[440.32s -> 441.44s]  whether the cat is appearing
[441.44s -> 443.48s]  in this very dark, moody scene
[443.48s -> 445.60s]  or in this very bright, sunlit scene,
[445.60s -> 446.48s]  it's still a cat,
[446.48s -> 449.54s]  and our algorithms need to be robust to that.
[449.54s -> 451.44s]  Objects can also deform.
[451.44s -> 453.52s]  I think cats are maybe among the more deformable
[453.52s -> 455.56s]  of animals that you might see out there.
[455.56s -> 456.96s]  And cats can really assume
[456.96s -> 459.88s]  a lot of different varied poses and positions,
[459.88s -> 461.32s]  and our algorithms should be robust
[461.32s -> 463.28s]  to these different kinds of transforms.
[464.30s -> 466.72s]  There can also be problems of occlusion,
[466.72s -> 469.50s]  where you might only see part of a cat,
[469.50s -> 470.64s]  like just the face,
[470.64s -> 471.84s]  or in this extreme example,
[471.84s -> 474.84s]  just a tail peeking out from under the couch cushion.
[474.84s -> 475.68s]  But in these cases,
[475.68s -> 477.68s]  it's pretty easy for you as a person
[477.68s -> 479.80s]  to realize that this is probably a cat,
[479.80s -> 482.40s]  and you still recognize these images as cats.
[482.40s -> 484.96s]  And this is something that our algorithms
[484.96s -> 486.52s]  also must be robust to,
[486.52s -> 488.32s]  which is quite difficult, I think.
[489.40s -> 491.68s]  There can also be problems of background clutter,
[491.68s -> 493.68s]  where maybe the foreground object,
[493.68s -> 496.20s]  the cat, could actually look quite similar
[496.20s -> 497.64s]  in appearance to the background,
[497.64s -> 500.14s]  and this is another thing that we need to handle.
[501.24s -> 504.52s]  There's also this problem of intra-class variation,
[504.52s -> 506.76s]  that this one notion of catness
[506.76s -> 509.34s]  actually spans a lot of different visual appearances,
[509.34s -> 511.48s]  and cats can come in different shapes and sizes
[511.48s -> 513.30s]  and colors and ages,
[513.30s -> 514.36s]  and our algorithm, again,
[514.36s -> 517.32s]  needs to work and handle all these different variations.
[517.32s -> 520.96s]  So this is actually a really, really challenging problem,
[520.96s -> 524.48s]  and it's sort of easy to forget how easy this is,
[524.48s -> 527.28s]  because so much of your brain is specifically tuned
[527.28s -> 528.84s]  for dealing with these things.
[528.84s -> 530.60s]  But now, if we want our computer programs
[530.60s -> 533.56s]  to deal with all of these problems all simultaneously,
[533.56s -> 535.20s]  and not just for cats, by the way,
[535.20s -> 537.80s]  but for just about any object category you could imagine,
[537.80s -> 539.96s]  this is a fantastically challenging problem,
[539.96s -> 541.64s]  and it's actually somewhat miraculous
[541.64s -> 543.92s]  that this works at all, in my opinion.
[543.92s -> 545.72s]  But actually, not only does it work,
[545.72s -> 548.24s]  but these things work very close to human accuracy
[548.24s -> 550.20s]  in some limited situations,
[550.28s -> 553.40s]  and take maybe only hundreds of milliseconds to do so.
[553.40s -> 554.92s]  So this is some pretty amazing,
[554.92s -> 556.80s]  incredible technology, in my opinion,
[556.80s -> 559.36s]  and over the course of the rest of the class,
[559.36s -> 561.48s]  we'll really see what kinds of advancements
[561.48s -> 562.78s]  have made this possible.
[564.62s -> 565.92s]  So now, if you kind of think about
[565.92s -> 568.52s]  what is the API for writing an image classifier,
[568.52s -> 570.20s]  you might sit down and try to write
[570.20s -> 572.08s]  a method in Python like this,
[572.08s -> 573.72s]  where you wanna take in an image
[573.72s -> 575.36s]  and then do some crazy magic,
[575.36s -> 577.16s]  and then eventually spit out this class label
[577.16s -> 579.16s]  to say cat or dog or whatnot.
[579.20s -> 582.44s]  And there's really no obvious way to do this, right?
[582.44s -> 584.44s]  Like, if you're taking an algorithms class
[584.44s -> 586.08s]  and your task is to sort numbers
[586.08s -> 587.80s]  or compute a convex hull
[587.80s -> 590.68s]  or even do something like RSA encryption,
[590.68s -> 592.54s]  you sort of can write down an algorithm
[592.54s -> 594.84s]  and enumerate all the steps that need to happen
[594.84s -> 596.96s]  in order for these things to work.
[596.96s -> 599.36s]  But when we're trying to recognize objects
[599.36s -> 601.30s]  or recognize cats or images,
[601.30s -> 603.92s]  there's no really clear, explicit algorithm
[603.92s -> 605.90s]  that makes intuitive sense
[605.90s -> 608.84s]  for how you might go about recognizing these objects.
[609.36s -> 610.92s]  This is, again, quite challenging,
[610.92s -> 612.36s]  if you think about,
[612.36s -> 614.60s]  if it was your first day programming,
[614.60s -> 616.48s]  you had to sit down and write this function,
[616.48s -> 618.52s]  I think most people would be in trouble.
[620.00s -> 620.84s]  That being said,
[620.84s -> 622.88s]  people have definitely made explicit attempts
[622.88s -> 625.96s]  to try to write sort of hand-coded rules
[625.96s -> 627.84s]  for recognizing different animals.
[627.84s -> 630.60s]  So we touched on this a little bit in the last lecture.
[630.60s -> 633.20s]  But maybe one idea for cats is that
[633.20s -> 635.20s]  we know that cats have ears and eyes
[635.20s -> 636.60s]  and mouths and noses,
[636.60s -> 637.84s]  and we know that edges are,
[637.84s -> 638.96s]  from Hubel and Wiesel,
[638.96s -> 640.64s]  we know that edges are pretty important
[640.64s -> 642.54s]  when it comes to visual recognition.
[642.54s -> 643.88s]  So one thing we might try to do
[643.88s -> 646.38s]  is compute the edges of this image
[646.38s -> 648.08s]  and then go in and try to categorize
[648.08s -> 649.88s]  all the different corners and boundaries
[649.88s -> 652.52s]  and say that if we have maybe three lines
[652.52s -> 653.36s]  meeting this way,
[653.36s -> 655.60s]  then it might be a corner and an ear has one corner here
[655.60s -> 657.48s]  and one corner there and one corner there,
[657.48s -> 660.18s]  and then kind of write down this explicit set of rules
[660.18s -> 661.52s]  for recognizing cats.
[662.52s -> 665.38s]  But this turns out not to work very well.
[665.42s -> 666.86s]  One, it's super brittle,
[666.86s -> 669.76s]  and two, say if you want to start over
[669.76s -> 671.30s]  for another object category
[671.30s -> 672.88s]  and maybe not worry about cats,
[672.88s -> 675.10s]  but talk about trucks or dogs or fishes
[675.10s -> 676.12s]  or something else,
[676.12s -> 677.98s]  then you need to start all over again.
[677.98s -> 680.74s]  So this is really not a very scalable approach.
[680.74s -> 682.62s]  We want to come up with some algorithm
[682.62s -> 686.22s]  or some method for these recognition tasks,
[686.22s -> 687.60s]  which scales much more naturally
[687.60s -> 689.86s]  to all the variety of objects in the world.
[689.86s -> 693.46s]  So the insight that sort of makes this all work
[693.46s -> 696.14s]  is this idea of the data-driven approach,
[696.14s -> 698.42s]  is that rather than sitting down
[698.42s -> 700.62s]  and writing these hand-specified rules
[700.62s -> 703.26s]  to try to craft exactly what is a cat or a fish
[703.26s -> 704.66s]  or what have you,
[704.66s -> 706.82s]  instead we'll go out onto the internet
[706.82s -> 710.06s]  and collect a large data set of many, many cats
[710.06s -> 712.34s]  and many, many airplanes and many, many deer
[712.34s -> 714.26s]  and different things like this.
[714.26s -> 716.90s]  And we can actually use tools like Google Image Search
[716.90s -> 717.94s]  or something like that
[717.94s -> 720.50s]  to go out and collect a very large number of examples
[720.50s -> 722.50s]  of these different categories.
[722.50s -> 724.98s]  By the way, this actually takes quite a lot of effort
[724.98s -> 727.10s]  to go out and actually collect these data sets,
[727.10s -> 729.54s]  but people, luckily there's a lot of really good
[729.54s -> 732.90s]  high-quality data sets out there already for you to use.
[732.90s -> 734.78s]  Then once we get this data set,
[734.78s -> 737.38s]  we train this machine learning classifier
[737.38s -> 739.82s]  that is going to ingest all of the data,
[739.82s -> 741.22s]  summarize it in some way,
[741.22s -> 744.42s]  and then spit out a model that summarizes the knowledge
[744.42s -> 746.50s]  of how to recognize these different objects
[746.50s -> 749.38s]  of how to recognize these different object categories.
[749.38s -> 751.06s]  Then finally, we'll use this train model
[751.06s -> 752.82s]  and apply it on new images
[752.82s -> 755.42s]  that will then be able to recognize cats and dogs
[755.42s -> 756.90s]  and whatnot.
[756.90s -> 759.34s]  So here, our API has changed a little bit.
[759.34s -> 760.64s]  Rather than a single function
[760.64s -> 762.90s]  that just inputs an image and recognizes a cat,
[762.90s -> 764.50s]  we have these two functions.
[764.50s -> 766.34s]  One that's called train
[766.34s -> 769.02s]  that's gonna input images and labels
[769.02s -> 770.22s]  and then output a model,
[770.22s -> 772.94s]  and then separately, another function called predict
[772.94s -> 774.02s]  which will input the model
[774.02s -> 776.10s]  and then make predictions for images.
[776.58s -> 777.86s]  This is kind of the key insight
[777.86s -> 780.62s]  that allowed all these things to start working really well
[780.62s -> 782.42s]  over the last 10, 20 years or so.
[787.30s -> 789.30s]  This class is primarily about neural networks
[789.30s -> 790.70s]  and convolutional neural networks
[790.70s -> 792.14s]  and deep learning and all that,
[792.14s -> 794.74s]  but this idea of a data-driven approach
[794.74s -> 797.02s]  is much more general than just deep learning.
[797.02s -> 800.14s]  I think it's useful to sort of step through this process
[800.14s -> 801.90s]  for a very simple classifier first
[801.90s -> 804.10s]  before we get to these big, complex ones.
[804.10s -> 807.94s]  So probably the simplest classifier you can imagine
[807.94s -> 809.86s]  is something we call nearest neighbor.
[809.86s -> 812.34s]  The algorithm is pretty dumb, honestly.
[812.34s -> 815.26s]  So during the training step, we won't do anything.
[815.26s -> 817.86s]  We'll just memorize all of the training data.
[817.86s -> 820.02s]  So this is very simple.
[820.02s -> 822.14s]  And now during the prediction step,
[822.14s -> 824.14s]  we're gonna take some new image
[824.14s -> 826.70s]  and go and try to find the most similar image
[826.70s -> 828.90s]  in the training data to that new image
[828.90s -> 832.38s]  and now predict the label of that most similar image.
[832.38s -> 834.10s]  Very simple algorithm.
[834.10s -> 836.18s]  But it sort of has a lot of these nice properties
[836.18s -> 838.42s]  with respect to data-drivenness and whatnot.
[840.90s -> 842.62s]  So to be a little bit more concrete,
[842.62s -> 845.90s]  you might imagine working on this dataset called CIFAR-10,
[845.90s -> 848.50s]  which is very commonly used in machine learning
[848.50s -> 850.22s]  as kind of a small test case.
[850.22s -> 852.50s]  And you'll be working with this dataset on your homework.
[852.50s -> 856.02s]  So the CIFAR-10 dataset gives you 10 different classes,
[856.02s -> 858.62s]  airplanes and automobiles and birds and cats
[858.62s -> 860.86s]  and different things like that.
[860.94s -> 863.26s]  And for each of those 10 categories,
[863.26s -> 868.22s]  it provides 50,000 training images,
[868.22s -> 871.46s]  roughly evenly distributed across these 10 categories,
[871.46s -> 874.70s]  and then 10,000 additional testing images
[874.70s -> 877.34s]  that you are supposed to test your algorithm on.
[878.62s -> 879.90s]  So now if you think about,
[879.90s -> 881.74s]  so here's an example of applying
[881.74s -> 883.74s]  this simple nearest neighbor classifier
[883.74s -> 886.82s]  to some of these test images on CIFAR-10.
[886.82s -> 889.10s]  So on this grid on the right,
[889.14s -> 891.06s]  for the leftmost column,
[891.06s -> 893.86s]  gives a test image in the CIFAR-10 dataset.
[893.86s -> 897.06s]  And now on the right, we see,
[897.06s -> 898.70s]  we've sorted the training images
[898.70s -> 901.58s]  and show the most similar training images
[901.58s -> 903.66s]  to each of these test examples.
[903.66s -> 906.10s]  And you can see that they look kind of visually similar
[906.10s -> 908.38s]  to the training images,
[908.38s -> 911.22s]  although they are not always correct.
[911.22s -> 913.98s]  So maybe on this second row, we see that the testing,
[913.98s -> 915.02s]  this is kind of hard to see
[915.02s -> 917.62s]  because these images are 32 by 32 pixels.
[917.62s -> 919.02s]  You need to really dive in there
[919.02s -> 921.38s]  and try to make your best guess.
[921.38s -> 922.98s]  But this image is a dog,
[922.98s -> 925.62s]  and its nearest neighbor is also a dog.
[925.62s -> 927.34s]  But this next one, I think,
[927.34s -> 930.26s]  is actually a deer or a horse or something else.
[930.26s -> 933.26s]  But you can see that it looks quite visually similar
[933.26s -> 934.66s]  because there's kind of a white blob
[934.66s -> 936.54s]  in the middle and whatnot.
[936.54s -> 939.02s]  So if we're applying the nearest neighbor algorithm
[939.02s -> 941.78s]  to this image, we'll find the closest example
[941.78s -> 942.86s]  in the training set.
[942.86s -> 945.30s]  And now the closest example, we know it's label
[945.30s -> 947.06s]  because it comes from the training set,
[947.10s -> 949.22s]  and now we'll simply say that this testing image
[949.22s -> 951.06s]  is also a dog.
[951.06s -> 953.18s]  You can see kind of from these examples
[953.18s -> 955.82s]  that this is probably not gonna work very well,
[955.82s -> 958.50s]  but it's still kind of a nice example to work through.
[960.82s -> 963.14s]  But then one detail that we need to know
[963.14s -> 964.94s]  is given a pair of images,
[964.94s -> 966.86s]  how can we actually compare them?
[966.86s -> 968.66s]  Because if we're gonna take our test image
[968.66s -> 970.54s]  and compare it to all the training images,
[970.54s -> 972.10s]  we actually have many different choices
[972.10s -> 975.62s]  for exactly what that comparison function should look like.
[975.66s -> 977.70s]  So in the example in the previous slide,
[977.70s -> 980.18s]  we've used what's called the L1 distance,
[980.18s -> 982.54s]  also sometimes called the Manhattan distance.
[982.54s -> 984.74s]  So this is a really, really sort of simple,
[984.74s -> 987.42s]  easy idea for comparing images.
[987.42s -> 989.90s]  And that's that we're gonna take the,
[989.90s -> 992.82s]  just compare individual pixels in these images.
[992.82s -> 995.10s]  So supposing that our test image
[995.10s -> 999.18s]  is maybe just a tiny four by four image of pixel values,
[999.18s -> 1001.90s]  then we're gonna take this upper left-hand pixel
[1001.90s -> 1004.18s]  of the test image, subtract off the value
[1004.18s -> 1006.34s]  in the training image, take the absolute value,
[1006.34s -> 1007.78s]  and get the difference in that pixel
[1007.78s -> 1010.10s]  between the two images, and then sum all these up
[1010.10s -> 1011.90s]  across all the pixels in the image.
[1011.90s -> 1014.22s]  So this is kind of a stupid way to compare images,
[1014.22s -> 1017.86s]  but it does some reasonable things sometimes.
[1017.86s -> 1019.58s]  But this gives us a very concrete way
[1019.58s -> 1021.94s]  to measure the difference between two images.
[1021.94s -> 1024.98s]  And in this case, we have this difference of 456
[1024.98s -> 1026.34s]  between these two images.
[1028.38s -> 1030.90s]  So here's some full Python code
[1030.90s -> 1033.34s]  for implementing this nearest neighbor classifier.
[1033.38s -> 1035.50s]  And you can see it's actually pretty short
[1035.50s -> 1037.62s]  and pretty concise, because we've made use
[1037.62s -> 1039.78s]  of many of these vectorized operations
[1039.78s -> 1041.42s]  offered by NumPy.
[1041.42s -> 1043.94s]  So here we can see that the training,
[1043.94s -> 1046.42s]  this train function that we talked about earlier
[1046.42s -> 1049.06s]  is, again, very simple in the case of nearest neighbor.
[1049.06s -> 1050.66s]  You just memorize the training data.
[1050.66s -> 1052.46s]  There's not really much to do here.
[1053.46s -> 1055.98s]  And now at test time, we're gonna take in our image
[1055.98s -> 1057.66s]  and then go in and compare,
[1057.66s -> 1060.34s]  using this L1 distance function, our test image,
[1060.34s -> 1062.10s]  to each of these training examples
[1062.14s -> 1065.46s]  and find the most similar example in the training set.
[1065.46s -> 1067.70s]  And you can see that we're actually able to do this
[1067.70s -> 1070.86s]  in just one or two lines of Python code
[1070.86s -> 1073.98s]  by utilizing these vectorized operations in NumPy.
[1073.98s -> 1075.98s]  So this is something that you'll get practice with
[1075.98s -> 1077.14s]  on the first assignment.
[1078.70s -> 1082.30s]  So now a couple questions about this simple classifier.
[1082.30s -> 1084.94s]  First, if we have n examples in our training set,
[1084.94s -> 1087.82s]  then how fast can we expect training and testing to be?
[1088.82s -> 1090.82s]  Well, training is probably constant,
[1090.82s -> 1092.66s]  because we don't really need to do anything.
[1092.66s -> 1094.94s]  We just need to memorize the data.
[1094.94s -> 1096.30s]  And if you're just copying a pointer,
[1096.30s -> 1097.62s]  that's gonna be constant time,
[1097.62s -> 1099.62s]  no matter how big your data set is.
[1099.62s -> 1102.22s]  But now at test time, we need to do this
[1102.22s -> 1104.78s]  comparison step and compare our test image
[1104.78s -> 1108.14s]  to each of the n training examples in the data set.
[1108.14s -> 1110.30s]  And this is actually quite slow.
[1111.90s -> 1114.54s]  So this is actually somewhat interesting.
[1115.06s -> 1117.62s]  So this is actually somewhat backwards,
[1117.62s -> 1120.46s]  if you think about it, because in practice,
[1120.46s -> 1123.58s]  we want our classifiers to be slow at training time
[1123.58s -> 1125.46s]  and then fast at testing time,
[1125.46s -> 1127.46s]  because you might imagine that a classifier
[1127.46s -> 1130.06s]  might go and be trained in a data center somewhere,
[1130.06s -> 1132.22s]  and you can afford to spend a lot of computation
[1132.22s -> 1135.02s]  at training time to make the classifier really good.
[1135.02s -> 1136.86s]  But then when you go and deploy the classifier
[1136.86s -> 1139.66s]  at test time, you want it to run on your mobile phone
[1139.66s -> 1142.38s]  or in the browser or some other low-power device,
[1142.38s -> 1144.78s]  and you really want the testing time performance
[1144.78s -> 1147.10s]  of your classifier to be quite fast.
[1147.10s -> 1150.06s]  So from this perspective, this nearest neighbor algorithm
[1150.06s -> 1151.90s]  is actually a little bit backwards,
[1151.90s -> 1153.34s]  and we'll see that once we move
[1153.34s -> 1154.86s]  to convolutional neural networks
[1154.86s -> 1156.86s]  and other types of parametric models,
[1156.86s -> 1158.38s]  there'll be the reverse of this,
[1158.38s -> 1160.46s]  where you'll spend a lot of compute at training time,
[1160.46s -> 1162.78s]  but then they'll be quite fast at testing time.
[1164.94s -> 1167.42s]  So then the question is, what exactly does this
[1167.42s -> 1169.10s]  nearest neighbor algorithm look like
[1169.10s -> 1170.78s]  when you apply it in practice?
[1170.86s -> 1174.14s]  So here we've drawn what we call the decision regions
[1174.14s -> 1176.22s]  of a nearest neighbor classifier.
[1176.22s -> 1179.74s]  So here our training set consists of these points
[1179.74s -> 1182.14s]  in the two-dimensional plane,
[1182.14s -> 1185.50s]  where the color of the point represents the category
[1185.50s -> 1187.66s]  or the class label of that point.
[1187.66s -> 1189.42s]  So here we see we have five classes
[1189.42s -> 1191.66s]  and some blue ones up in the corner here,
[1191.66s -> 1193.90s]  some purple ones in the upper right-hand corner,
[1193.90s -> 1196.70s]  and now for each pixel in this entire plane,
[1196.70s -> 1199.66s]  we've gone and computed what is the nearest training
[1200.54s -> 1203.34s]  what is the nearest example in these training data,
[1203.34s -> 1205.18s]  and then colored the point of the background
[1205.18s -> 1207.66s]  corresponding to what is the class label.
[1207.66s -> 1209.74s]  So you can see that this nearest neighbor classifier
[1209.74s -> 1211.66s]  is just sort of carving up the space
[1211.66s -> 1214.62s]  and coloring the space according to the nearby points.
[1215.98s -> 1219.02s]  But this classifier is maybe not so great,
[1219.02s -> 1220.62s]  and by looking at this picture,
[1220.62s -> 1222.38s]  we can start to see some of the problems
[1222.38s -> 1225.34s]  that might come out with a nearest neighbor classifier.
[1225.34s -> 1228.30s]  For one, this central region actually contains
[1228.30s -> 1229.74s]  mostly green points,
[1229.74s -> 1232.54s]  but one little yellow point in the middle.
[1232.54s -> 1235.02s]  But because we're just looking at the nearest neighbor,
[1235.02s -> 1237.10s]  this causes a little yellow island to appear
[1237.10s -> 1239.10s]  in this middle of the green cluster,
[1239.10s -> 1241.02s]  and that's maybe not so great.
[1241.02s -> 1244.38s]  Maybe those points actually should have been green.
[1244.38s -> 1247.26s]  And then similarly, we also see these sort of fingers
[1247.26s -> 1250.78s]  of the green region pushing into the blue region,
[1250.78s -> 1253.18s]  again, due to the presence of one point,
[1253.18s -> 1255.74s]  which may have been noisy or spurious.
[1256.06s -> 1259.18s]  So this kind of motivates a slight generalization
[1259.18s -> 1261.42s]  of this algorithm called k-nearest neighbors.
[1262.46s -> 1265.98s]  So rather than just looking for the single nearest neighbor,
[1265.98s -> 1268.94s]  instead, we'll do something a little bit fancier
[1268.94s -> 1271.50s]  and find k of our nearest neighbors
[1271.50s -> 1273.10s]  according to our distance metric,
[1273.10s -> 1275.90s]  and then take a vote among each of our neighbors,
[1275.90s -> 1278.54s]  and then predict the majority vote among our neighbors.
[1279.50s -> 1281.90s]  You can imagine slightly more complex ways of doing this.
[1281.90s -> 1283.82s]  Maybe you vote, waited on the distance,
[1283.82s -> 1284.62s]  or something like that.
[1284.62s -> 1288.14s]  But the simplest thing that tends to work pretty well
[1288.14s -> 1289.98s]  is just taking a majority vote.
[1289.98s -> 1293.10s]  So here we've shown the exact same set of points
[1293.10s -> 1296.30s]  using this k equals one nearest neighbor classifier,
[1296.30s -> 1298.62s]  as well as k equals three and k equals five
[1298.62s -> 1300.14s]  in the middle and on the right.
[1300.14s -> 1302.86s]  And once we move to k equals three,
[1302.86s -> 1304.94s]  you can see that that spurious yellow point
[1304.94s -> 1306.46s]  in the middle of the green cluster
[1306.46s -> 1309.66s]  is no longer causing the points near that region
[1309.66s -> 1311.18s]  to be classified as yellow.
[1311.18s -> 1313.98s]  Now this entire green portion in the middle
[1314.14s -> 1316.14s]  is all being classified as green.
[1316.14s -> 1317.98s]  You can also see that these fingers
[1317.98s -> 1319.58s]  of the red and blue regions
[1319.58s -> 1321.10s]  are starting to get smoothed out
[1321.10s -> 1322.70s]  due to this majority voting.
[1322.70s -> 1325.66s]  And then once we move to the k equals five case,
[1325.66s -> 1327.18s]  then these decision boundaries
[1327.18s -> 1328.86s]  between the blue and red regions
[1328.86s -> 1331.02s]  have become quite smooth and quite nice.
[1331.02s -> 1332.46s]  So this is generally something,
[1332.46s -> 1335.02s]  so generally when you're using nearest neighbor classifiers,
[1335.02s -> 1338.86s]  you almost always want to use some value of k,
[1338.86s -> 1341.02s]  which is larger than one,
[1341.02s -> 1342.38s]  because this tends to smooth out
[1342.54s -> 1345.10s]  your decision boundaries and lead to better results.
[1347.10s -> 1350.06s]  So if we, to kind of, oh, yeah, question?
[1354.54s -> 1355.58s]  Yeah, so the question is,
[1355.58s -> 1357.58s]  what is the deal with these white regions?
[1357.58s -> 1359.18s]  And those are, the white regions
[1359.18s -> 1361.50s]  are where there was no majority
[1361.50s -> 1363.42s]  among the k nearest neighbors.
[1363.42s -> 1365.82s]  You could imagine maybe doing something slightly fancier
[1365.82s -> 1367.26s]  and maybe taking a guess
[1367.26s -> 1370.86s]  or randomly selecting among the majority winners,
[1370.86s -> 1372.06s]  but for this simple example,
[1372.06s -> 1373.02s]  we're just coloring it white
[1373.02s -> 1374.62s]  to indicate that there was no nearest neighbor
[1374.62s -> 1375.46s]  in those points.
[1378.22s -> 1379.34s]  So we already kind of saw,
[1379.34s -> 1381.82s]  so I like to, whenever we're thinking
[1381.82s -> 1382.70s]  about computer vision,
[1382.70s -> 1384.30s]  I think it's really useful to kind of
[1384.30s -> 1386.86s]  flip back and forth between several different viewpoints.
[1386.86s -> 1389.98s]  One is this idea of high-dimensional points in the plane,
[1389.98s -> 1393.18s]  and then the other is actually looking at concrete images,
[1393.18s -> 1394.54s]  because the pixels of the image
[1394.54s -> 1397.90s]  actually allow us to think of these images
[1397.90s -> 1399.34s]  as high-dimensional vectors.
[1399.34s -> 1401.50s]  And it's sort of useful to ping-pong back and forth
[1401.50s -> 1403.50s]  between these two different viewpoints.
[1403.50s -> 1406.46s]  So then, when sort of taking this k nearest neighbor
[1406.46s -> 1407.98s]  and going back to the images,
[1407.98s -> 1409.66s]  you can see that it's actually not very good.
[1409.66s -> 1411.34s]  Here I've colored in red and green
[1411.34s -> 1413.58s]  which images would actually be classified correctly
[1413.58s -> 1415.74s]  or incorrectly according to their nearest neighbor,
[1415.74s -> 1418.46s]  and you can see that it's really not very good.
[1418.46s -> 1421.66s]  But maybe if we used a larger value of k,
[1421.66s -> 1423.98s]  then this would involve actually voting among
[1423.98s -> 1425.90s]  maybe the top three or the top five,
[1425.90s -> 1427.42s]  or maybe even the whole row,
[1427.42s -> 1429.06s]  and you could imagine that that would actually
[1429.06s -> 1432.10s]  end up being a lot more robust to some of this noise
[1432.10s -> 1436.26s]  that we see when retrieving neighbors in this way.
[1437.50s -> 1439.18s]  So another choice we have
[1439.18s -> 1442.18s]  when we're working with the k nearest neighbor algorithm
[1442.18s -> 1444.82s]  is determining exactly how we should be comparing
[1444.82s -> 1446.42s]  our different points.
[1446.42s -> 1450.30s]  For the example so far, we've just shown this,
[1450.30s -> 1452.18s]  we've talked about this L1 distance,
[1452.18s -> 1454.10s]  which takes the sum of the absolute values
[1454.10s -> 1455.78s]  between the pixels.
[1455.78s -> 1458.78s]  But another common choice is the L2 or Euclidean distance,
[1459.42s -> 1462.30s]  where you take the square root of the sum of the squares
[1462.30s -> 1465.42s]  and take this as your distance.
[1465.42s -> 1467.22s]  Choosing different distance metrics
[1467.22s -> 1469.78s]  actually is a pretty interesting topic
[1469.78s -> 1471.18s]  because different distance metrics
[1471.18s -> 1473.82s]  make different assumptions about the underlying geometry
[1473.82s -> 1476.42s]  or topology that you expect in the space.
[1476.42s -> 1479.74s]  So this L1 distance, underneath this,
[1479.74s -> 1482.70s]  this is actually a circle according to the L1 distance,
[1482.70s -> 1486.10s]  and it forms this square shape thing around the origin
[1486.10s -> 1488.66s]  where each of the points on the square
[1489.42s -> 1492.14s]  is equidistant from the origin according to L1.
[1492.14s -> 1494.26s]  Whereas with the L2 or Euclidean distance,
[1494.26s -> 1496.42s]  then this circle is a familiar circle,
[1496.42s -> 1498.38s]  it looks like what you would expect.
[1498.38s -> 1499.94s]  So one interesting thing to point out
[1499.94s -> 1502.02s]  between these two metrics in particular
[1502.02s -> 1504.30s]  is that the L1 distance depends
[1504.30s -> 1506.22s]  on your choice of coordinate system.
[1506.22s -> 1508.42s]  So if you were to rotate the coordinate frame,
[1508.42s -> 1510.14s]  that would actually change the L1 distance
[1510.14s -> 1511.30s]  between the points.
[1511.30s -> 1514.62s]  Whereas changing the coordinate frame in the L2 distance
[1514.62s -> 1516.26s]  doesn't matter, it's the same thing
[1516.26s -> 1518.30s]  no matter what your coordinate frame is.
[1518.98s -> 1520.86s]  Maybe if your input features,
[1520.86s -> 1523.34s]  if the individual entries in your vector
[1523.34s -> 1525.54s]  have some important meaning for your task,
[1525.54s -> 1528.94s]  then maybe somehow L1 might be a more natural fit.
[1528.94s -> 1531.14s]  But if it's just a generic vector in some space
[1531.14s -> 1533.30s]  and you don't know which of the different elements,
[1533.30s -> 1534.74s]  you don't know what they actually mean,
[1534.74s -> 1536.66s]  then maybe L2 is slightly more natural.
[1537.98s -> 1540.62s]  And another point here is that we can actually,
[1540.62s -> 1542.38s]  by using different distance metrics,
[1542.38s -> 1545.10s]  we can actually generalize the k-nearest neighbor classifier
[1545.10s -> 1546.90s]  to many, many different types of data,
[1546.90s -> 1548.94s]  not just vectors, not just images.
[1548.94s -> 1551.06s]  So for example, imagine you wanted
[1551.06s -> 1552.86s]  to classify pieces of text.
[1552.86s -> 1554.62s]  Then the only thing you need to do
[1554.62s -> 1557.06s]  to use k-nearest neighbors is to specify
[1557.06s -> 1560.82s]  some distance function that can measure distances
[1560.82s -> 1563.06s]  between maybe two paragraphs or two sentences
[1563.06s -> 1564.34s]  or something like that.
[1564.34s -> 1567.38s]  So simply by specifying different distance metrics,
[1567.38s -> 1570.14s]  we can actually apply this algorithm very generally
[1570.14s -> 1572.10s]  to basically any type of data.
[1572.10s -> 1573.34s]  So it's actually a very,
[1573.34s -> 1575.26s]  even though it's a kind of simple algorithm,
[1575.26s -> 1577.74s]  in general, it's a very good thing to try first
[1577.74s -> 1579.70s]  when you're looking at a new problem.
[1582.30s -> 1584.50s]  So then it's also kind of interesting to think about
[1584.50s -> 1586.38s]  what is actually happening geometrically
[1586.38s -> 1588.94s]  if we choose different distance metrics.
[1588.94s -> 1591.86s]  So here we see the same set of points on the left
[1591.86s -> 1594.30s]  using the L1 or Manhattan distance,
[1594.30s -> 1595.58s]  and then on the right,
[1595.58s -> 1598.70s]  using the familiar L2 or Euclidean distance.
[1598.70s -> 1600.30s]  And you can see that the shapes
[1600.30s -> 1602.98s]  of these decision boundaries actually change quite a bit
[1602.98s -> 1604.54s]  between the two metrics.
[1605.02s -> 1606.94s]  So when you're looking at L1,
[1606.94s -> 1608.86s]  these decision boundaries tend to follow
[1608.86s -> 1611.10s]  the coordinate axes, and this is, again,
[1611.10s -> 1612.70s]  because the L1 actually depends
[1612.70s -> 1614.38s]  on our choice of coordinate system,
[1614.38s -> 1616.82s]  where the L2 sort of doesn't really care
[1616.82s -> 1618.74s]  about the coordinate axes, it just puts the boundaries
[1618.74s -> 1621.02s]  where they sort of should fall naturally.
[1623.12s -> 1627.52s]  So actually, my confession is that each of these examples
[1627.52s -> 1628.86s]  that I've shown you is actually
[1628.86s -> 1631.24s]  from this interactive web demo that I built,
[1631.24s -> 1632.46s]  where you can go and play
[1632.50s -> 1635.10s]  with this k-nearest neighbor classifier on your own.
[1635.10s -> 1638.34s]  And this is really hard to work on a projector screen.
[1638.34s -> 1642.10s]  So maybe we'll do that on your own time.
[1647.14s -> 1648.74s]  So let's just go back to here.
[1653.30s -> 1654.94s]  Man, this is kind of embarrassing.
[1662.46s -> 1667.46s]  Okay, that was way more trouble than it was worth.
[1667.58s -> 1669.06s]  So let's skip this,
[1669.06s -> 1671.12s]  but I encourage you to go play with this in your browser.
[1671.12s -> 1673.86s]  It's actually pretty fun and kind of nice
[1673.86s -> 1675.68s]  to be able to roll this out,
[1675.68s -> 1696.90s]  so let's all give them a cool driving
[1696.90s -> 1699.74s]  to build intuition about how the decision boundary changes
[1699.74s -> 1703.90s]  as you change the k and change your distance metric
[1703.90s -> 1706.58s]  and all those sorts of things.
[1711.02s -> 1712.88s]  Okay, so then the question is,
[1712.88s -> 1714.64s]  once you're actually trying to use this algorithm
[1714.64s -> 1717.62s]  in practice, there are several choices you need to make.
[1717.62s -> 1719.86s]  We talked about choosing different values of k,
[1719.86s -> 1722.02s]  we talked about choosing different distance metrics,
[1722.02s -> 1724.56s]  and the question becomes, how do you actually make
[1725.10s -> 1728.28s]  these choices for your problem and for your data?
[1728.28s -> 1732.70s]  So these choices of things like k and the distance metric,
[1732.70s -> 1736.52s]  we call hyperparameters because they are not necessarily
[1736.52s -> 1738.26s]  learned from the training data.
[1738.26s -> 1740.48s]  Instead, these are choices about your algorithm
[1740.48s -> 1744.26s]  that you make ahead of time and there's no way
[1744.26s -> 1746.58s]  to learn them directly from the data.
[1746.58s -> 1749.94s]  So the question is, how do you set these things
[1749.94s -> 1751.12s]  in practice?
[1751.12s -> 1753.46s]  And they turn out to be very problem dependent
[1753.48s -> 1756.36s]  and the simple thing that most people do is simply
[1756.36s -> 1758.96s]  try different values of hyperparameters for your data
[1758.96s -> 1761.92s]  and for your problem and figure out which one works best.
[1761.92s -> 1762.88s]  There's a question?
[1770.40s -> 1773.48s]  So the question is where L1 distance might be preferable
[1773.48s -> 1775.48s]  to using L2 distance.
[1775.48s -> 1777.80s]  I think it's mainly problem dependent.
[1777.80s -> 1780.28s]  It's sort of difficult to say in which cases
[1780.28s -> 1782.60s]  you think one might be better than the other.
[1782.62s -> 1785.38s]  But I think that because L1 has this sort of
[1785.38s -> 1789.38s]  coordinate dependency, it actually depends on
[1789.38s -> 1791.54s]  the coordinate system of your data.
[1791.54s -> 1794.02s]  If you sort of know that you have a vector
[1794.02s -> 1795.62s]  and maybe the individual elements of the vector
[1795.62s -> 1799.10s]  have meaning, like maybe you're classifying employees
[1799.10s -> 1800.82s]  for some reason, and then the different elements
[1800.82s -> 1803.62s]  of that vector correspond to different features
[1803.62s -> 1806.50s]  or aspects of an employee, like their salary
[1806.50s -> 1807.74s]  or the number of years they've been working
[1807.74s -> 1809.78s]  at the company or something like that.
[1809.78s -> 1812.10s]  So I think when your individual elements actually have
[1812.64s -> 1814.52s]  some meaning is where I think maybe using L1
[1814.52s -> 1816.80s]  might make a little bit more sense.
[1816.80s -> 1818.68s]  But in general, again, this is a hyperparameter
[1818.68s -> 1820.96s]  and it really depends on your problem and your data.
[1820.96s -> 1823.30s]  So the best answer is just to try them both
[1823.30s -> 1824.60s]  and see what works better.
[1827.60s -> 1830.64s]  So then the question, so even this idea of trying out
[1830.64s -> 1832.48s]  different values of hyperparameters and seeing
[1832.48s -> 1835.24s]  what works best, there are many different choices here.
[1835.24s -> 1837.52s]  What exactly does it mean to try hyperparameters
[1837.52s -> 1839.24s]  and see what works best?
[1839.24s -> 1841.54s]  Well, the first idea you might think of
[1841.56s -> 1844.04s]  is simply choosing the hyperparameters that give you
[1844.04s -> 1846.32s]  the best accuracy or best performance
[1846.32s -> 1847.60s]  on your training data.
[1848.60s -> 1850.56s]  This is actually a really terrible idea,
[1850.56s -> 1852.96s]  like you should never do this.
[1852.96s -> 1855.20s]  And in the concrete case of the nearest neighbor
[1855.20s -> 1858.40s]  classifier, for example, if we set k equals one,
[1858.40s -> 1861.18s]  we will always classify the training data perfectly.
[1862.08s -> 1864.40s]  So if we use this strategy, we'll always pick
[1864.40s -> 1867.48s]  k equals one, but as we saw from the examples earlier,
[1867.48s -> 1870.24s]  and in practice, it seems that the setting k equals
[1870.30s -> 1873.06s]  to larger values might cause us to misclassify
[1873.06s -> 1875.02s]  some of the training data, but in fact lead
[1875.02s -> 1877.18s]  to better performance on points that were not
[1877.18s -> 1878.70s]  in the training data.
[1878.70s -> 1880.94s]  And ultimately in machine learning, we don't care
[1880.94s -> 1883.10s]  about fitting the training data, we really care
[1883.10s -> 1885.62s]  about how our classifier or how our method
[1885.62s -> 1888.10s]  will perform on unseen data after training.
[1888.10s -> 1891.46s]  So this is a terrible idea, don't do this.
[1891.46s -> 1894.38s]  So another idea that you might think of is maybe
[1894.38s -> 1896.58s]  we'll take our full data set and we'll split it
[1896.58s -> 1899.38s]  into some training data and some test data.
[1899.40s -> 1903.16s]  And now I'll try training my algorithm
[1903.16s -> 1904.60s]  with different choices of hyperparameters
[1904.60s -> 1907.12s]  on the training data, and then I'll go and apply
[1907.12s -> 1909.72s]  that trained classifier on the test data,
[1909.72s -> 1912.12s]  and now I will pick the set of hyperparameters
[1912.12s -> 1914.60s]  that cause me to perform best on the test data.
[1915.64s -> 1918.56s]  This seems like maybe a more reasonable strategy,
[1918.56s -> 1920.64s]  but in fact, this is also a terrible idea
[1920.64s -> 1922.16s]  and you should never do this.
[1922.16s -> 1924.84s]  Because again, the point of machine learning systems
[1924.84s -> 1927.64s]  is that we want to know how our algorithm will,
[1927.66s -> 1930.46s]  so the point of the test set is to give us some estimate
[1930.46s -> 1933.22s]  of how our method will do on unseen data
[1933.22s -> 1935.54s]  that's sort of coming out from the wild.
[1935.54s -> 1937.58s]  And if we use this strategy of training
[1937.58s -> 1940.94s]  many different algorithms with different hyperparameters
[1940.94s -> 1943.46s]  and then selecting the one which does the best
[1943.46s -> 1946.94s]  on the test data, then it's possible that we may
[1946.94s -> 1949.22s]  have just picked the right set of hyperparameters
[1949.22s -> 1951.46s]  that caused our algorithm to work quite well
[1951.46s -> 1954.02s]  on this testing set, but now our performance
[1954.02s -> 1956.58s]  on this test set will no longer be representative
[1956.60s -> 1959.24s]  of our performance on new unseen data.
[1959.24s -> 1961.60s]  So again, you should not do this.
[1961.60s -> 1964.28s]  This is a bad idea, you'll get in trouble if you do this.
[1965.64s -> 1968.08s]  What is much more common is to actually split your data
[1968.08s -> 1970.04s]  into three different sets.
[1970.04s -> 1974.12s]  You'll choose, you'll partition most of your data
[1974.12s -> 1976.04s]  into a training set and then you'll create
[1976.04s -> 1978.28s]  a validation set and a test set.
[1978.28s -> 1981.76s]  And now what we typically do is go and train algorithm,
[1981.76s -> 1983.72s]  train our algorithm with many different choices
[1983.72s -> 1986.24s]  of hyperparameters on the training set,
[1986.94s -> 1989.14s]  evaluate on the validation set, and now pick the set
[1989.14s -> 1991.10s]  of hyperparameters which performs best
[1991.10s -> 1992.62s]  on the validation set.
[1992.62s -> 1994.66s]  And now after you've done all your developments,
[1994.66s -> 1995.94s]  after you've done all your debugging,
[1995.94s -> 1998.30s]  after you've done everything, then you take
[1998.30s -> 2002.78s]  that best performing classifier on the validation set
[2002.78s -> 2004.38s]  and run it once on the test set.
[2004.38s -> 2006.02s]  And now that's the number that goes into your paper,
[2006.02s -> 2008.26s]  that's the number that goes into your report,
[2008.26s -> 2010.42s]  that's the number that actually is telling you
[2010.42s -> 2013.22s]  how your algorithm is doing on unseen data.
[2013.22s -> 2015.62s]  And this is actually really, really important
[2015.76s -> 2017.36s]  that you keep a very strict separation
[2017.36s -> 2019.92s]  between the validation data and the test data.
[2019.92s -> 2022.48s]  So for example, when we're working on research papers,
[2022.48s -> 2024.52s]  we typically only touch the test set
[2024.52s -> 2026.72s]  at the very last minute.
[2026.72s -> 2029.10s]  So when I'm writing papers, I tend to only touch
[2029.10s -> 2031.52s]  the test set for my problem in maybe the week
[2031.52s -> 2033.64s]  before the deadline or so to really ensure
[2033.64s -> 2036.44s]  that we're not being dishonest here
[2036.44s -> 2038.96s]  and we're not reporting a number which is unfair.
[2038.96s -> 2041.16s]  So this is actually super, super important
[2041.16s -> 2043.36s]  and you wanna make sure to keep your test data
[2043.36s -> 2044.44s]  quite under control.
[2046.60s -> 2048.96s]  So another strategy for setting hyperparameters
[2048.96s -> 2051.08s]  is called cross-validation.
[2051.08s -> 2053.96s]  And this is used a little bit more commonly
[2053.96s -> 2057.60s]  for small data sets, not used so much in deep learning.
[2057.60s -> 2060.48s]  So here the idea is that we're gonna take our test data,
[2060.48s -> 2062.80s]  or we're gonna take our data set, as usual,
[2062.80s -> 2065.80s]  hold out some test set to use at the very end.
[2065.80s -> 2067.28s]  And now for the rest of the data,
[2067.28s -> 2069.60s]  rather than splitting it into a single training
[2069.60s -> 2073.20s]  and validation partition, instead we can split
[2073.98s -> 2076.50s]  our training data into many different folds.
[2076.50s -> 2079.06s]  And now in this way, we kind of cycle through
[2079.06s -> 2082.38s]  choosing which fold is going to be the validation set.
[2082.38s -> 2084.86s]  So now in this example, we're using
[2084.86s -> 2086.62s]  five-fold cross-validation.
[2086.62s -> 2088.06s]  So you would train your algorithm
[2088.06s -> 2090.98s]  with one set of hyperparameters on the first four folds,
[2090.98s -> 2093.00s]  evaluate the performance on fold four,
[2093.00s -> 2094.66s]  and now go and retrain your algorithm
[2094.66s -> 2096.72s]  on folds one, two, three, and five,
[2096.72s -> 2099.58s]  evaluate on fold four, and sort of cycle
[2099.58s -> 2101.24s]  through all the different folds.
[2101.24s -> 2102.74s]  And when you do it this way,
[2103.40s -> 2106.08s]  you get much higher confidence about which hyperparameters
[2106.08s -> 2108.04s]  are going to perform more robustly.
[2108.04s -> 2110.32s]  So this is kind of the gold standard to use,
[2110.32s -> 2112.32s]  but in practice, in deep learning,
[2112.32s -> 2113.88s]  when we're training large models
[2113.88s -> 2116.60s]  and training is very computationally expensive,
[2116.60s -> 2119.16s]  this doesn't get used too much in practice.
[2119.16s -> 2120.00s]  Question?
[2120.00s -> 2120.84s]  Yeah.
[2130.94s -> 2133.42s]  So the question is a little bit more concretely,
[2133.42s -> 2134.94s]  what's the difference between the training
[2134.94s -> 2136.50s]  and the validation set?
[2136.50s -> 2141.50s]  So if you think about the k-nearest neighbor classifier,
[2141.90s -> 2146.14s]  then the training set is this set of images with labels
[2146.14s -> 2147.66s]  where we memorize the labels.
[2147.66s -> 2149.34s]  And now to classify an image,
[2149.64s -> 2151.20s]  we're gonna take the image and compare it
[2151.20s -> 2153.68s]  to each element in the training data,
[2153.68s -> 2156.08s]  and then transfer the label from the nearest training,
[2156.08s -> 2159.24s]  from the nearest training point.
[2159.24s -> 2161.28s]  So now what we're gonna do with the validation,
[2161.28s -> 2163.24s]  so now our algorithm will memorize everything
[2163.24s -> 2165.28s]  in the training set, and now it will take
[2165.28s -> 2167.08s]  each element of the validation set
[2167.08s -> 2169.36s]  and compare it to each element in the training data,
[2169.36s -> 2173.16s]  and then use this to determine what is the accuracy
[2173.16s -> 2175.16s]  of our classifier when it's applied
[2175.16s -> 2176.32s]  on the validation set.
[2177.02s -> 2178.42s]  So this is kind of the distinction
[2178.42s -> 2180.14s]  between training and validation,
[2180.14s -> 2183.26s]  where your algorithm is able to see the labels
[2183.26s -> 2186.02s]  of the training set, but for the validation set,
[2186.02s -> 2188.78s]  your algorithm doesn't have direct access to the labels.
[2188.78s -> 2191.22s]  We only use the labels of the validation set
[2191.22s -> 2194.26s]  to check how well our algorithm is doing.
[2194.26s -> 2195.10s]  A question?
[2195.10s -> 2195.94s]  A question?
[2205.36s -> 2207.36s]  The question is whether the test set,
[2208.72s -> 2210.04s]  is it possible that the test set
[2210.04s -> 2213.80s]  might not be representative of data out there in the wild?
[2213.80s -> 2216.96s]  And this definitely can be a problem in practice.
[2216.96s -> 2219.08s]  The underlying statistical assumption here
[2219.08s -> 2221.36s]  is that your data are all independently
[2221.36s -> 2222.84s]  and identically distributed,
[2222.86s -> 2226.70s]  so that all of your data points
[2226.70s -> 2229.22s]  should be drawn from the same underlying
[2229.22s -> 2231.26s]  probability distribution.
[2231.26s -> 2233.94s]  Of course, in practice, this might not always be the case
[2233.94s -> 2235.86s]  and you definitely can run into cases
[2235.86s -> 2239.78s]  where the test set might not be super representative
[2239.78s -> 2241.92s]  of what you see in the wild.
[2241.92s -> 2244.66s]  So this is kind of a problem that dataset creators
[2244.66s -> 2246.62s]  and dataset curators need to think about,
[2246.62s -> 2248.70s]  but when I'm creating datasets, for example,
[2248.70s -> 2250.74s]  one thing I do is I'll go and collect
[2250.74s -> 2252.66s]  a whole bunch of data all at once
[2253.36s -> 2255.36s]  using the exact same methodology for collecting the data,
[2255.36s -> 2257.76s]  and then afterwards, you go and partition it randomly
[2257.76s -> 2259.80s]  between train and test.
[2259.80s -> 2261.32s]  One thing that can screw you up here
[2261.32s -> 2264.10s]  is maybe if you're collecting data over time
[2264.10s -> 2266.48s]  and you make the earlier data that you collect first
[2266.48s -> 2268.68s]  be the training data, and the later data that you collect
[2268.68s -> 2270.44s]  be the test data, then you actually might run
[2270.44s -> 2272.86s]  into this kind of shift that could cause problems.
[2272.86s -> 2274.94s]  But as long as this partition is random
[2274.94s -> 2277.00s]  among your entire set of data points,
[2277.00s -> 2278.76s]  then that's how we sort of try
[2278.76s -> 2280.76s]  to alleviate this problem in practice.
[2282.66s -> 2287.66s]  Then once you've gone through this cross-validation procedure
[2287.66s -> 2290.66s]  then you end up with graphs that look something like this.
[2290.66s -> 2294.00s]  So here on the x-axis, we are showing the value of k
[2294.00s -> 2297.08s]  for a k-nearest neighbor classifier on some problem.
[2297.08s -> 2300.84s]  Now on the y-axis, we are showing what is the accuracy
[2300.84s -> 2305.00s]  of our classifier on some dataset for different values
[2305.00s -> 2307.56s]  of k, and you can see that in this case,
[2307.56s -> 2311.00s]  we've done five-fold cross-validation over the data,
[2311.98s -> 2314.78s]  so for each value of k, we have five different examples
[2314.78s -> 2317.86s]  of how well this algorithm is doing.
[2318.86s -> 2320.58s]  Actually, going back to the question
[2320.58s -> 2322.86s]  about having some test sets that are better
[2322.86s -> 2327.66s]  or worse for your algorithm, using k-fold cross-validation
[2327.66s -> 2330.46s]  is maybe one way to help quantify that a little bit,
[2330.46s -> 2333.62s]  and in that, we can see the variance
[2333.62s -> 2336.34s]  of how this algorithm performs on different
[2336.34s -> 2338.90s]  of the validation folds, and that gives you some sense
[2338.92s -> 2341.20s]  of not just what is the best, but also what is the
[2341.20s -> 2344.32s]  distribution of that performance.
[2344.32s -> 2346.52s]  So whenever you're training machine learning models,
[2346.52s -> 2348.84s]  you end up making plots like this where they show you
[2348.84s -> 2350.72s]  what is your accuracy or your performance
[2350.72s -> 2352.56s]  as a function of your hyperparameters,
[2352.56s -> 2354.84s]  and then you want to go and pick the model
[2354.84s -> 2357.20s]  or the set of hyperparameters at the end of the day
[2357.20s -> 2359.96s]  that performs the best on the validation set.
[2359.96s -> 2362.60s]  So here, we see that maybe about k equals seven
[2362.60s -> 2366.84s]  probably works about best for this problem.
[2367.70s -> 2372.66s]  So one thing that is, so k-nearest neighbor classifiers
[2372.66s -> 2375.62s]  on images are actually almost never used in practice,
[2375.62s -> 2378.10s]  because they're just, with all of these problems
[2378.10s -> 2379.34s]  that we've talked about.
[2379.34s -> 2382.30s]  So one problem is that it's very slow at test time,
[2382.30s -> 2383.94s]  which is kind of the reverse of what we want,
[2383.94s -> 2385.46s]  which we talked about earlier.
[2385.46s -> 2388.58s]  Another problem is that these things like Euclidean distance
[2388.58s -> 2392.62s]  or L1 distance are really not a very good way
[2392.62s -> 2395.50s]  to measure distances between images.
[2395.68s -> 2398.24s]  These sort of vectorial distance functions
[2398.24s -> 2400.92s]  do not correspond very well to perceptual similarity
[2400.92s -> 2403.40s]  between images, how you perceive differences
[2403.40s -> 2405.12s]  between images.
[2405.12s -> 2407.88s]  So in this example, we've constructed,
[2407.88s -> 2409.84s]  there's this image on the left of a girl,
[2409.84s -> 2412.32s]  and then three different distorted images on the right,
[2412.32s -> 2414.04s]  where we've blocked out her mouth,
[2414.04s -> 2417.20s]  we've actually shifted down by a couple pixels,
[2417.20s -> 2419.40s]  or tinted the entire image blue.
[2419.40s -> 2422.00s]  And actually, if you compute the Euclidean distance
[2422.00s -> 2423.80s]  between the original and the boxed,
[2423.80s -> 2425.16s]  the original and the shuffled,
[2425.70s -> 2426.54s]  and the original and the tinted,
[2426.54s -> 2428.82s]  they all have the same L2 distance,
[2428.82s -> 2430.50s]  which is maybe not so good,
[2430.50s -> 2432.82s]  because it sort of gives you the sense
[2432.82s -> 2436.06s]  that the L2 distance is really not doing a very good job
[2436.06s -> 2438.82s]  at capturing these perceptual differences between images.
[2441.66s -> 2442.98s]  Another sort of problem
[2442.98s -> 2444.74s]  with the k-nearest neighbor classifier
[2444.74s -> 2446.22s]  has to do with something we call
[2446.22s -> 2447.86s]  the curse of dimensionality.
[2447.86s -> 2450.70s]  So if you recall back this viewpoint we had
[2450.70s -> 2452.38s]  of the k-nearest neighbor classifier,
[2452.38s -> 2454.18s]  it's sort of dropping paint around each
[2454.24s -> 2455.76s]  of the training data points,
[2455.76s -> 2458.40s]  and using that to sort of partition the space.
[2458.40s -> 2459.88s]  So that means that if we expect
[2459.88s -> 2462.16s]  the k-nearest neighbor classifier to work well,
[2462.16s -> 2463.84s]  we kind of need our training examples
[2463.84s -> 2466.76s]  to cover the space quite densely.
[2466.76s -> 2470.98s]  Otherwise, our nearest neighbors
[2470.98s -> 2472.28s]  could actually be quite far away,
[2472.28s -> 2474.00s]  and might not actually be very similar
[2474.00s -> 2476.40s]  to our testing points.
[2476.40s -> 2478.60s]  So, and the problem is that actually
[2478.60s -> 2480.20s]  densely covering the space
[2480.20s -> 2482.48s]  means that we need a number of training examples,
[2482.50s -> 2485.82s]  which is exponential in the dimension of the problem.
[2485.82s -> 2488.46s]  So this is very bad, exponential growth is always bad,
[2488.46s -> 2490.18s]  and you're never gonna get enough,
[2490.18s -> 2492.38s]  basically you're never gonna get enough images
[2492.38s -> 2494.46s]  to densely cover this space of pixels
[2494.46s -> 2496.58s]  in this high-dimensional space.
[2496.58s -> 2498.70s]  So that's maybe another thing to keep in mind
[2498.70s -> 2501.10s]  when you're thinking about using k-nearest neighbor.
[2502.18s -> 2504.30s]  So kind of the summary is that we're using k-nearest
[2504.30s -> 2506.78s]  neighbor to introduce this idea of image classification.
[2506.78s -> 2508.62s]  We have a training set of images and labels,
[2508.62s -> 2511.26s]  and then we use that to predict these labels
[2511.64s -> 2512.92s]  on the test set, question?
[2515.32s -> 2517.68s]  Oh, sorry, the question is what was going on
[2517.68s -> 2520.44s]  with this picture, what are the green and blue dots?
[2520.44s -> 2524.44s]  So here, this was maybe, we have some training samples
[2524.44s -> 2526.76s]  which are represented by points,
[2526.76s -> 2528.60s]  and the color of the dot maybe represents
[2528.60s -> 2532.08s]  the category of the point of this training sample.
[2532.08s -> 2533.78s]  So if we're in one dimension,
[2533.78s -> 2536.18s]  then you maybe only need four training samples
[2536.18s -> 2538.72s]  to kind of densely cover the space.
[2538.72s -> 2540.44s]  But if we move to two dimensions,
[2540.46s -> 2543.82s]  then we now need four times four is 16 training examples
[2543.82s -> 2545.78s]  to densely cover this space.
[2546.78s -> 2548.54s]  And if we move to three, four, five,
[2548.54s -> 2551.06s]  many more dimensions, the number of training examples
[2551.06s -> 2553.42s]  that we need to densely cover the space
[2553.42s -> 2556.18s]  grows exponentially with the dimension.
[2556.18s -> 2557.54s]  So this is kind of giving you this sense
[2557.54s -> 2559.18s]  that maybe in two dimensions we might have
[2559.18s -> 2561.10s]  this kind of funny curved shape,
[2561.10s -> 2563.86s]  or you might have sort of arbitrary manifolds
[2563.86s -> 2567.58s]  of labels in different dimensional spaces.
[2567.58s -> 2568.74s]  And the only way that the,
[2568.74s -> 2570.52s]  because the k-nearest neighbor algorithm
[2570.52s -> 2572.12s]  doesn't really make any assumptions
[2572.12s -> 2573.98s]  about these underlying manifolds,
[2573.98s -> 2575.72s]  the only way it can perform properly
[2575.72s -> 2577.56s]  is if it has quite a dense sample
[2577.56s -> 2579.44s]  of training points to work with.
[2582.68s -> 2585.94s]  So this is kind of the overview of k-nearest neighbors,
[2585.94s -> 2588.06s]  and you'll get a chance to actually implement this
[2588.06s -> 2590.56s]  and try it out on images in the first assignment.
[2592.04s -> 2594.12s]  So if there's any last-minute questions about k and n,
[2594.12s -> 2596.96s]  I'm going to move on to the next topic.
[2596.96s -> 2597.80s]  Question?
[2598.74s -> 2599.58s]  Sorry, say that again.
[2605.20s -> 2607.00s]  Yeah, so the question is why do these images
[2607.00s -> 2608.88s]  have the same L2 distance?
[2608.88s -> 2611.24s]  And the answer is that I carefully constructed them
[2611.24s -> 2613.08s]  to have the same L2 distance.
[2615.08s -> 2617.36s]  But it's just giving you this sense
[2617.36s -> 2620.28s]  that the L2 distance is not a very good measure
[2620.28s -> 2622.12s]  of similarity between images,
[2622.12s -> 2624.76s]  and these images are actually all different from each other,
[2624.76s -> 2626.20s]  and they're all different from each other
[2626.26s -> 2628.90s]  and these images are actually all different from each other
[2628.90s -> 2631.50s]  in quite disparate ways.
[2632.58s -> 2635.66s]  But if you're using k and n,
[2635.66s -> 2637.82s]  then the only thing you have to measure distance
[2637.82s -> 2641.34s]  between images is this single distance metric.
[2641.34s -> 2642.96s]  And this kind of gives you an example
[2642.96s -> 2646.38s]  where that distance metric is actually not capturing
[2646.38s -> 2647.92s]  the full description of distance
[2647.92s -> 2649.94s]  or difference between images.
[2649.94s -> 2650.90s]  But yes, in this case,
[2650.90s -> 2653.74s]  I just sort of carefully constructed these translations
[2653.74s -> 2655.54s]  and these offsets to match exactly.
[2656.34s -> 2657.18s]  Question?
[2669.54s -> 2672.50s]  So the question is maybe this is actually good
[2672.50s -> 2674.82s]  because all of these things are actually
[2674.82s -> 2677.54s]  having the same distance to the image.
[2677.54s -> 2679.02s]  That's maybe true for this example,
[2679.02s -> 2680.86s]  but I think you could also construct examples
[2680.86s -> 2682.82s]  where maybe we have two original images
[2682.82s -> 2684.48s]  and then by putting the boxes in the right places
[2684.48s -> 2686.94s]  or tinting them, we could cause it to be nearer
[2686.94s -> 2689.30s]  to pretty much anything that you want.
[2689.30s -> 2692.30s]  Because in this example, we can do arbitrary shifting
[2692.30s -> 2695.66s]  and tinting to change these distances nearly arbitrarily
[2695.66s -> 2698.60s]  without changing the perceptual nature of these images.
[2698.60s -> 2700.22s]  So I think that this could actually screw you up
[2700.22s -> 2702.78s]  if you have many different original images.
[2704.14s -> 2704.98s]  Question?
[2715.34s -> 2716.90s]  The question is whether or not it's common
[2716.90s -> 2719.38s]  in real-world cases to go back and retrain
[2719.38s -> 2722.58s]  on the entire data set once you've found
[2722.58s -> 2724.34s]  those best hyperparameters.
[2724.34s -> 2728.82s]  So people do sometimes do this in practice,
[2728.82s -> 2731.14s]  but it's somewhat a matter of taste.
[2731.14s -> 2732.86s]  If you're really rushing for that deadline
[2732.86s -> 2734.58s]  and you really gotta get this model out the door,
[2734.58s -> 2736.98s]  then if it takes a long time to retrain the model
[2736.98s -> 2740.06s]  on the whole data set, then maybe you won't do it.
[2740.06s -> 2741.98s]  But if you have a little bit more time to spare
[2741.98s -> 2743.70s]  and a little bit more compute to spare,
[2744.56s -> 2747.96s]  and you wanna squeeze out maybe that extra 1% of performance,
[2747.96s -> 2749.60s]  then that is a trick you can use.
[2754.48s -> 2756.52s]  So we kinda saw the k-nearest neighbor
[2756.52s -> 2757.84s]  has a lot of the nice properties
[2757.84s -> 2759.68s]  of machine learning algorithms.
[2759.68s -> 2761.64s]  But in practice, it's not so great
[2761.64s -> 2764.24s]  and really not used very much in images.
[2766.12s -> 2767.92s]  So the next thing I'd like to talk about
[2767.92s -> 2769.60s]  is linear classification.
[2769.60s -> 2771.76s]  And linear classification is, again,
[2771.76s -> 2773.60s]  quite a simple learning algorithm,
[2774.46s -> 2776.06s]  but this will become super important
[2776.06s -> 2778.32s]  and help us build up to whole neural networks
[2778.32s -> 2780.66s]  and whole convolutional networks.
[2780.66s -> 2782.98s]  So one analogy people often talk about
[2782.98s -> 2784.46s]  when working with neural networks
[2784.46s -> 2787.30s]  is we think of them as being kind of like Lego blocks,
[2787.30s -> 2789.24s]  that you can have different kinds of components
[2789.24s -> 2790.86s]  of neural networks, and you can stick
[2790.86s -> 2792.58s]  these components together to build
[2792.58s -> 2796.42s]  these large different towers of convolutional networks.
[2796.42s -> 2798.80s]  And one of the most basic building blocks
[2798.80s -> 2801.02s]  that we'll see in different types
[2801.04s -> 2804.24s]  of deep learning applications is this linear classifier.
[2804.24s -> 2805.76s]  So I think it's actually really important
[2805.76s -> 2807.96s]  to have a good understanding of what's happening
[2807.96s -> 2809.32s]  with linear classification,
[2809.32s -> 2811.22s]  because this will end up generalizing quite nicely
[2811.22s -> 2813.60s]  to whole neural networks.
[2813.60s -> 2816.38s]  So another example of kind of this modular nature
[2816.38s -> 2818.12s]  of neural networks comes from some research
[2818.12s -> 2819.76s]  in our own lab on image captioning,
[2819.76s -> 2821.32s]  just as a little bit of a preview.
[2821.32s -> 2823.92s]  So here, the setup is that we wanna input an image
[2823.92s -> 2825.80s]  and then output a descriptive sentence
[2825.80s -> 2827.14s]  describing the image.
[2827.14s -> 2828.56s]  And the way this kind of works
[2828.58s -> 2831.46s]  is that we have one convolutional neural network
[2831.46s -> 2832.66s]  that's looking at the image,
[2832.66s -> 2835.58s]  and a recurrent neural network that knows about language.
[2835.58s -> 2837.66s]  And we can kind of just stick these two pieces together,
[2837.66s -> 2839.94s]  like Lego blocks, and train the whole thing together,
[2839.94s -> 2841.98s]  and end up with a pretty cool system
[2841.98s -> 2843.86s]  that can do some non-trivial things.
[2843.86s -> 2845.78s]  And we'll work through the details of this model
[2845.78s -> 2847.30s]  as we go forward in the class,
[2847.30s -> 2848.54s]  but this just gives you the sense
[2848.54s -> 2851.30s]  that these deep neural networks are kind of like Legos,
[2851.30s -> 2853.70s]  and this linear classifier is kind of like
[2853.70s -> 2856.50s]  the most basic building block of these giant networks.
[2857.36s -> 2859.56s]  But that's a little bit too exciting for lecture two,
[2859.56s -> 2861.80s]  so we have to go back to CIFAR-10 for the moment.
[2861.80s -> 2866.00s]  So recall that CIFAR-10 has these 50,000 training examples.
[2866.00s -> 2869.64s]  Each image is 32 by 32 pixels and three color channels.
[2870.56s -> 2872.36s]  So now, we're gonna take a bit of,
[2872.36s -> 2874.12s]  in linear classification, we're going to take
[2874.12s -> 2876.96s]  a bit of a different approach from k-nearest neighbor.
[2876.96s -> 2881.96s]  So the linear classifier is one of the simplest examples
[2882.20s -> 2884.92s]  of what we call a parametric model.
[2885.54s -> 2888.02s]  So now, our parametric model actually
[2888.02s -> 2889.66s]  has two different components.
[2889.66s -> 2893.26s]  It's gonna take in this image, maybe of a cat on the left,
[2893.26s -> 2898.26s]  and this, that we usually write as x for our input data,
[2898.58s -> 2901.42s]  and also a set of parameters or weights,
[2901.42s -> 2904.50s]  which is usually called w, also sometimes theta,
[2904.50s -> 2906.06s]  depending on the literature.
[2906.06s -> 2908.02s]  And now we're gonna write down some function
[2908.02s -> 2911.94s]  which takes in both the data x and the parameters w,
[2911.94s -> 2914.66s]  and this will spit out now 10 numbers
[2915.36s -> 2918.20s]  describing what are the scores corresponding
[2918.20s -> 2921.04s]  to each of those 10 categories in CIFAR-10,
[2921.04s -> 2923.32s]  with the interpretation that larger scores,
[2923.32s -> 2925.28s]  like the larger score for cat,
[2925.28s -> 2929.60s]  indicates a larger probability of that input x being cat.
[2929.60s -> 2931.34s]  And now, a question?
[2939.80s -> 2943.08s]  Oh, so the question is, what is the three?
[2943.14s -> 2945.50s]  So the three in this example corresponds
[2945.50s -> 2947.98s]  to the three color channels, red, green, and blue,
[2947.98s -> 2949.58s]  because we typically work on color images.
[2949.58s -> 2953.78s]  That's nice information that you don't want to throw away.
[2953.78s -> 2956.34s]  So what's kind of different about this parametric,
[2956.34s -> 2958.66s]  so in the k-nearest neighbor setup,
[2958.66s -> 2959.88s]  there was no parameters.
[2959.88s -> 2961.54s]  Instead, we just sort of keep around
[2961.54s -> 2963.70s]  the whole training data, the whole training set,
[2963.70s -> 2965.30s]  and use that at test time.
[2965.30s -> 2966.98s]  But now, in a parametric approach,
[2966.98s -> 2969.46s]  we're going to summarize our knowledge of the training data
[2969.46s -> 2972.22s]  and stick all that knowledge into these parameters w.
[2972.24s -> 2974.44s]  And now, at test time, we no longer need
[2974.44s -> 2976.52s]  the actual training data, we can throw it away.
[2976.52s -> 2979.12s]  We only need these parameters w at test time.
[2979.12s -> 2981.68s]  So this allows our models to now be more efficient
[2981.68s -> 2984.52s]  and actually run on maybe small devices like phones.
[2985.76s -> 2988.36s]  So kind of the whole story in deep learning
[2988.36s -> 2992.00s]  is coming up with the right structure for this function f.
[2992.00s -> 2994.52s]  You can imagine writing down different functional forms
[2994.52s -> 2996.20s]  for how to combine weights and data
[2996.20s -> 2997.76s]  in different complex ways,
[2997.76s -> 3001.16s]  and these could correspond to different network architectures.
[3002.28s -> 3003.92s]  But the simplest possible example
[3003.92s -> 3005.36s]  of combining these two things
[3005.36s -> 3007.00s]  is just maybe to multiply them.
[3007.00s -> 3009.92s]  And this is a linear classifier.
[3009.92s -> 3014.40s]  So here, our f of x, w is just equal to the w times x.
[3014.40s -> 3016.88s]  Probably the simplest equation you can imagine.
[3016.88s -> 3018.80s]  So here, if you kind of unpack the dimensions
[3018.80s -> 3021.40s]  of these things, we recall that our image
[3021.40s -> 3025.96s]  was maybe 32 by 32 by three values.
[3025.96s -> 3028.84s]  So then this becomes, we're going to take those values
[3028.98s -> 3032.70s]  and stretch them out into a long column vector
[3032.70s -> 3035.82s]  that has 3,072 by one entries.
[3035.82s -> 3037.50s]  And now we need to end up,
[3037.50s -> 3040.78s]  we want to end up with 10 class scores.
[3040.78s -> 3042.90s]  We want to end up with 10 numbers for this image,
[3042.90s -> 3045.42s]  giving us the scores for each of the 10 categories,
[3045.42s -> 3047.58s]  which means that now our matrix w
[3047.58s -> 3050.14s]  needs to be 10 by 3072,
[3050.14s -> 3052.46s]  so that once we multiply these two things out,
[3052.46s -> 3055.42s]  then we'll end up with a single column vector 10 by one
[3055.42s -> 3057.08s]  giving us our 10 class scores.
[3057.38s -> 3060.06s]  Also sometimes, you'll typically see this,
[3060.06s -> 3061.98s]  is we'll often add a bias term,
[3061.98s -> 3064.78s]  which will be a constant vector of 10 elements
[3064.78s -> 3066.90s]  that does not interact with the training data,
[3066.90s -> 3068.70s]  and instead just gives us some sort of
[3068.70s -> 3072.26s]  data-independent preferences for some classes over another.
[3072.26s -> 3074.74s]  So you might imagine that if your data set was unbalanced
[3074.74s -> 3077.26s]  and had many more cats than dogs, for example,
[3077.26s -> 3080.06s]  then the bias elements corresponding to cat
[3080.06s -> 3081.86s]  would be higher than the other ones.
[3083.66s -> 3085.82s]  So if you kind of think about pictorially
[3085.88s -> 3087.48s]  what this function is doing,
[3087.48s -> 3089.92s]  we're taking, we have, in this figure,
[3089.92s -> 3091.48s]  we have an example on the left
[3091.48s -> 3095.68s]  of a simple image with just a two-by-two image,
[3095.68s -> 3097.44s]  so it has four pixels total.
[3097.44s -> 3100.08s]  So the way that the linear classifier works
[3100.08s -> 3102.28s]  is that we take this two-by-two image,
[3102.28s -> 3105.24s]  we stretch it out into a column vector
[3105.24s -> 3108.04s]  with four elements, and now in this example,
[3108.04s -> 3110.44s]  we are just restricting to three classes,
[3110.44s -> 3111.80s]  cat, dog, and ship,
[3111.80s -> 3114.20s]  because you can't fit 10 on a slide.
[3114.26s -> 3118.42s]  And now our weight matrix is going to be four by three,
[3119.94s -> 3122.26s]  so we have four pixels and three classes.
[3122.26s -> 3125.62s]  And now, again, we have a three-element bias vector
[3125.62s -> 3129.06s]  that gives us data-independent bias terms
[3129.06s -> 3130.26s]  for each category.
[3130.26s -> 3132.82s]  So now we see that the cat score
[3132.82s -> 3135.26s]  is gonna be the inner product between
[3135.26s -> 3140.26s]  the pixels of our image and this row in the weight matrix
[3140.66s -> 3142.90s]  added together with this bias term.
[3142.96s -> 3145.20s]  So when you look at it this way,
[3145.20s -> 3148.24s]  you can kind of understand linear classification
[3148.24s -> 3150.40s]  as almost a template-matching approach,
[3150.40s -> 3152.52s]  where each of the rows in this matrix
[3152.52s -> 3155.72s]  correspond to some template of the image,
[3155.72s -> 3158.24s]  and now the inner product or dot product
[3158.24s -> 3159.88s]  between the row of the matrix
[3159.88s -> 3163.20s]  and the column giving the pixels of the image,
[3163.20s -> 3166.20s]  computing this dot product kind of gives us a similarity
[3166.20s -> 3168.08s]  between this template for the class
[3168.08s -> 3170.32s]  and the pixels of our image.
[3170.86s -> 3172.38s]  And then this bias just, again,
[3172.38s -> 3175.38s]  gives you this data-independent scaling offset
[3175.38s -> 3176.82s]  to each of the classes.
[3178.50s -> 3180.82s]  So now, from this template-matching,
[3180.82s -> 3182.54s]  if we think about linear classification
[3182.54s -> 3184.94s]  from this viewpoint of template-matching,
[3184.94s -> 3187.70s]  we can actually take the rows of that weight matrix
[3187.70s -> 3190.02s]  and unravel them back into images
[3190.02s -> 3192.86s]  and actually visualize those templates as images.
[3192.86s -> 3195.82s]  And this gives us some sense of what a linear classifier
[3195.82s -> 3199.02s]  might actually be doing to try to understand our data.
[3199.04s -> 3200.68s]  So in this example, we've gone ahead
[3200.68s -> 3203.36s]  and trained a linear classifier on our images,
[3203.36s -> 3205.96s]  and now on the bottom, we're visualizing
[3205.96s -> 3209.12s]  what are those rows in that learned weight matrix
[3209.12s -> 3212.40s]  corresponding to each of the 10 categories in CIFAR-10.
[3212.40s -> 3213.92s]  And in this way, we kind of get a sense
[3213.92s -> 3215.92s]  for what's going on in these images.
[3215.92s -> 3218.88s]  So for example, in the left, on the bottom left,
[3218.88s -> 3221.08s]  we see the template for the plane class
[3221.08s -> 3223.20s]  kind of consists of this blue blob,
[3223.20s -> 3224.96s]  this kind of blobby thing in the middle
[3224.96s -> 3226.80s]  and maybe blue in the background,
[3226.80s -> 3228.96s]  which gives you the sense that this template
[3229.78s -> 3231.14s]  for this linear classifier for plane
[3231.14s -> 3233.62s]  is maybe looking for blue stuff and blobby stuff,
[3233.62s -> 3236.26s]  and those features are going to cause the classifier
[3236.26s -> 3238.46s]  to like planes more.
[3238.46s -> 3240.34s]  Or if we look at this car example,
[3240.34s -> 3243.26s]  we kind of see that there's a red blobby thing
[3243.26s -> 3245.66s]  through the middle and a blue blobby thing at the top
[3245.66s -> 3247.86s]  that maybe is kind of a blurry windshield.
[3249.22s -> 3250.54s]  But this is a little bit weird.
[3250.54s -> 3252.14s]  This doesn't really look like a car.
[3252.14s -> 3254.54s]  No individual car actually looks like this.
[3254.54s -> 3256.66s]  So the problem is that the linear classifier
[3256.76s -> 3259.48s]  is only learning one template for each class.
[3259.48s -> 3261.16s]  So if there's sort of variations
[3261.16s -> 3262.88s]  in how that class might appear,
[3262.88s -> 3265.40s]  it's trying to average out all those different variations,
[3265.40s -> 3266.76s]  all those different appearances,
[3266.76s -> 3268.52s]  and use just one single template
[3268.52s -> 3270.62s]  to recognize each of those categories.
[3270.62s -> 3272.60s]  We can also see this pretty explicitly
[3272.60s -> 3274.12s]  in the horse classifier.
[3274.12s -> 3275.44s]  So in the horse classifier,
[3275.44s -> 3276.76s]  we see green stuff on the bottom
[3276.76s -> 3278.48s]  because horses are usually on grass.
[3278.48s -> 3279.76s]  And then if you look carefully,
[3279.76s -> 3282.04s]  the horse actually seems to have maybe two heads,
[3282.04s -> 3284.20s]  one head on each side.
[3284.26s -> 3287.14s]  And I've never seen a horse with two heads,
[3287.14s -> 3289.30s]  but the linear classifier is just doing the best
[3289.30s -> 3291.14s]  that it can because it's only allowed
[3291.14s -> 3293.82s]  to learn one template per category.
[3293.82s -> 3296.18s]  And as we see, as we move forward into neural networks
[3296.18s -> 3297.58s]  and more complex models,
[3297.58s -> 3300.64s]  we'll be able to achieve much better accuracy
[3300.64s -> 3302.32s]  because they no longer have this restriction
[3302.32s -> 3304.88s]  of just learning a single template per category.
[3308.06s -> 3309.90s]  So if you think about what,
[3309.90s -> 3312.30s]  another viewpoint of the linear classifier
[3312.30s -> 3314.64s]  is to go back to this idea of images
[3314.64s -> 3316.80s]  as points in a high-dimensional space.
[3316.80s -> 3321.28s]  And you can imagine that each of our images
[3321.28s -> 3324.40s]  is something like a point in this high-dimensional space.
[3324.40s -> 3325.84s]  And now the linear classifier
[3325.84s -> 3329.28s]  is putting in these linear decision boundaries
[3329.28s -> 3331.12s]  to try to draw linear separation
[3331.12s -> 3334.32s]  between one category and the rest of the categories.
[3334.32s -> 3336.92s]  So maybe up on the upper left-hand side,
[3336.92s -> 3340.00s]  we see these training examples of airplanes.
[3340.06s -> 3342.58s]  And throughout the process of training,
[3342.58s -> 3344.02s]  the linear classifier will go
[3344.02s -> 3345.94s]  and try to draw this blue line
[3345.94s -> 3347.94s]  to separate out with a single line
[3347.94s -> 3350.54s]  the airplane class from all the rest of the classes.
[3350.54s -> 3351.90s]  And it's actually kind of fun
[3351.90s -> 3353.66s]  if you watch during the training process.
[3353.66s -> 3355.02s]  These lines will start out randomly
[3355.02s -> 3356.38s]  and then go and snap into place
[3356.38s -> 3358.22s]  to try to separate the data properly.
[3359.86s -> 3362.78s]  But when you think about linear classification in this way
[3362.78s -> 3365.28s]  from this high-dimensional vector point of view,
[3365.28s -> 3366.78s]  you can start to see, again,
[3366.78s -> 3367.90s]  what are some of the problems
[3367.92s -> 3370.08s]  that might come up with linear classification.
[3370.08s -> 3373.00s]  And it's not too hard to construct examples of data sets
[3373.00s -> 3375.56s]  where a linear classifier will totally fail.
[3375.56s -> 3377.96s]  So one example on the left here is that,
[3377.96s -> 3380.64s]  suppose we have a data set of two categories,
[3380.64s -> 3382.36s]  and these are all maybe somewhat artificial,
[3382.36s -> 3386.52s]  but maybe our data set has two categories, blue and red.
[3386.52s -> 3389.40s]  And the blue ones, the blue categories,
[3389.40s -> 3391.36s]  are the number of pixels in the image
[3391.36s -> 3393.44s]  which are greater than zero is odd.
[3393.44s -> 3395.16s]  And anything where the number of pixels
[3395.16s -> 3396.36s]  greater than zero is even,
[3396.38s -> 3399.02s]  we want to classify as the red category.
[3399.02s -> 3400.70s]  So if you actually go and draw
[3400.70s -> 3405.38s]  what these different decision regions
[3405.38s -> 3406.74s]  look like in the plane,
[3406.74s -> 3408.58s]  you can see that our blue class
[3408.58s -> 3410.38s]  with an odd number of pixels
[3410.38s -> 3413.74s]  is going to be these two quadrants in the plane,
[3413.74s -> 3416.38s]  and even will be the opposite two quadrants.
[3416.38s -> 3418.54s]  So now, there's no way that we can draw
[3418.54s -> 3421.66s]  a single linear line to separate the blue from the red.
[3421.66s -> 3422.74s]  So this would be an example
[3422.74s -> 3425.78s]  where a linear classifier would really struggle.
[3426.40s -> 3429.28s]  And this is maybe not such an artificial thing after all.
[3430.36s -> 3431.80s]  Instead of counting pixels,
[3431.80s -> 3433.20s]  maybe we're actually trying to count
[3433.20s -> 3435.72s]  whether the number of animals or people
[3435.72s -> 3437.38s]  in an image is odd or even.
[3437.38s -> 3439.08s]  So this kind of a parity problem
[3439.92s -> 3442.04s]  of separating odds from evens
[3442.04s -> 3443.64s]  is something that linear classification
[3443.64s -> 3445.44s]  really struggles with traditionally.
[3446.48s -> 3449.24s]  Another, other problems that you might,
[3449.24s -> 3451.32s]  other situations where a linear classifier
[3451.32s -> 3454.84s]  really struggles are multimodal situations.
[3454.90s -> 3458.90s]  So here on the right, maybe our blue category
[3458.90s -> 3460.54s]  has these three different islands
[3460.54s -> 3462.90s]  of where the blue category lives,
[3462.90s -> 3465.70s]  and then everything else is some other category.
[3465.70s -> 3467.50s]  So for something like horses,
[3467.50s -> 3468.94s]  we saw on the previous example,
[3468.94s -> 3470.50s]  is something where this actually
[3470.50s -> 3472.10s]  might be happening in practice,
[3472.10s -> 3473.50s]  where there's maybe one island
[3473.50s -> 3475.98s]  in the pixel space of horses looking to the left,
[3475.98s -> 3478.42s]  and another island of horses looking to the right.
[3478.42s -> 3480.18s]  And now, there's no good way
[3480.18s -> 3481.86s]  to draw a single linear boundary
[3481.86s -> 3484.70s]  between these two isolated islands of data.
[3485.04s -> 3487.64s]  Any time where you have multimodal data,
[3487.64s -> 3489.68s]  data like one class that can appear
[3489.68s -> 3491.32s]  in different regions of space,
[3491.32s -> 3494.32s]  is another place where linear classifiers might struggle.
[3494.32s -> 3496.24s]  So there's kind of a lot of problems
[3496.24s -> 3497.44s]  with linear classifiers,
[3497.44s -> 3498.92s]  but it is a super simple algorithm,
[3498.92s -> 3500.92s]  super nice and easy to interpret
[3500.92s -> 3502.60s]  and easier to understand.
[3502.60s -> 3504.84s]  So you'll actually be implementing these things
[3504.84s -> 3506.60s]  on your first homework assignment.
[3508.60s -> 3511.60s]  So at this point, we've kind of talked about
[3511.60s -> 3513.00s]  what is the functional form
[3513.14s -> 3514.94s]  corresponding to a linear classifier,
[3514.94s -> 3517.10s]  and we've seen that this functional form
[3517.10s -> 3519.66s]  of a matrix vector multiply
[3519.66s -> 3521.90s]  corresponds to this idea of template matching,
[3521.90s -> 3523.18s]  and learning a single template
[3523.18s -> 3525.18s]  for each category in your data.
[3525.18s -> 3528.22s]  And then once we have this weight vector,
[3528.22s -> 3529.90s]  or this trained matrix,
[3529.90s -> 3532.86s]  you can use it to actually go and get your scores
[3532.86s -> 3535.38s]  for any new training example.
[3536.34s -> 3538.46s]  But what we have not told you is,
[3538.46s -> 3540.06s]  how do you actually go about choosing
[3540.06s -> 3542.30s]  the right W for your data set?
[3542.84s -> 3544.84s]  We've just talked about what is the functional form
[3544.84s -> 3547.80s]  and what is going on with this thing.
[3547.80s -> 3552.00s]  So that's something we'll really focus on next time.
[3552.00s -> 3553.84s]  And next lecture, we'll talk about
[3553.84s -> 3555.64s]  what are the strategies and algorithms
[3555.64s -> 3557.12s]  for choosing the right W?
[3557.12s -> 3559.60s]  And this will lead us to questions of loss functions
[3559.60s -> 3562.20s]  and optimization and eventually contents.
[3562.20s -> 3565.92s]  So that's a bit of the preview for next week,
[3565.92s -> 3567.76s]  and that's all we have for today.
