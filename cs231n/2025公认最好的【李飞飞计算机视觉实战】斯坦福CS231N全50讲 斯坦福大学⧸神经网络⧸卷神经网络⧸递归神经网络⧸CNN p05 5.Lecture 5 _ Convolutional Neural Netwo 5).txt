# Detected language: en (p=1.00)

[0.00s -> 5.00s]  Stanford University.
[8.00s -> 13.00s]  Okay, let's get started.
[13.00s -> 15.00s]  All right, so welcome to Lecture 5.
[15.00s -> 18.00s]  Today we're going to be getting to the title of the class,
[18.00s -> 22.00s]  Convolutional Neural Networks.
[22.00s -> 25.00s]  Okay, so a couple of administrative details before we get started.
[25.00s -> 29.00s]  Assignment one is due Thursday, April 20th,
[29.00s -> 31.00s]  11.59 p.m. on Canvas.
[31.00s -> 38.00s]  We're also going to be releasing assignment two on Thursday.
[38.00s -> 40.00s]  Okay, so a quick review of last time.
[40.00s -> 42.00s]  We talked about neural networks
[42.00s -> 44.00s]  and how we had the running example
[44.00s -> 46.00s]  of the linear score function that we talked about
[46.00s -> 48.00s]  through the first few lectures,
[48.00s -> 50.00s]  and then we turned this into a neural network
[50.00s -> 53.00s]  by stacking these linear layers on top of each other
[53.00s -> 56.00s]  with non-linearities in between.
[56.00s -> 58.00s]  And we also saw that this could help address
[58.00s -> 61.00s]  the mode problem where we are able to learn
[61.00s -> 63.00s]  intermediate templates that are looking for,
[63.00s -> 66.00s]  for example, you know, different types of cards, right,
[66.00s -> 68.00s]  a red card versus a yellow card and so on,
[68.00s -> 70.00s]  and to combine these together to come up
[70.00s -> 74.00s]  with a final score function for a class.
[74.00s -> 76.00s]  Okay, so today we're going to talk about
[76.00s -> 78.00s]  convolutional neural networks,
[78.00s -> 80.00s]  which is basically the same sort of idea,
[80.00s -> 82.00s]  but now we're going to learn about convolutional layers
[82.00s -> 85.00s]  that reason on top of basically explicitly
[85.00s -> 89.00s]  trying to maintain spatial structure.
[91.00s -> 93.00s]  So let's first talk a little bit about
[93.00s -> 95.00s]  the history of neural networks
[95.00s -> 97.00s]  and then also how convolutional neural networks
[97.00s -> 98.00s]  were developed.
[98.00s -> 101.00s]  So we can go all the way back to 1957
[101.00s -> 103.00s]  with Frank Rosenblatt,
[103.00s -> 106.00s]  who developed the Mark I Perceptron Machine,
[106.00s -> 108.00s]  which was the first implementation of an algorithm
[108.00s -> 110.00s]  called the Perceptron,
[110.00s -> 112.00s]  which had sort of the similar idea
[112.00s -> 114.00s]  of getting score functions, right,
[114.00s -> 118.00s]  using some, you know, w times x plus a bias,
[118.00s -> 121.00s]  but here the outputs are going to be either one or a zero,
[121.00s -> 124.00s]  and then in this case we have an update rule,
[124.00s -> 126.00s]  so an update rule for our weights w,
[126.00s -> 128.00s]  which also look kind of similar
[128.00s -> 129.00s]  to the type of update rule
[129.00s -> 131.00s]  that we're also seeing in backprop,
[131.00s -> 134.00s]  but in this case there was no principled
[134.00s -> 136.00s]  backpropagation technique yet.
[136.00s -> 138.00s]  We just sort of took the weights and adjusted them
[138.00s -> 142.00s]  in the direction towards the target that we wanted.
[143.00s -> 146.00s]  So in 1960, we had Widrow and Hoff,
[146.00s -> 148.00s]  who developed Adaline and Madaline,
[148.00s -> 152.00s]  which was the first time that we were able to get,
[152.00s -> 155.00s]  to start to stack these linear layers
[155.00s -> 158.00s]  into multilayer perceptron networks.
[158.00s -> 160.00s]  And so, you know, this is starting to now look
[160.00s -> 164.00s]  kind of like this idea of neural network layers,
[164.00s -> 166.00s]  but we still didn't have backprop
[166.00s -> 170.00s]  or any sort of principled way to train this.
[170.00s -> 173.00s]  And so the first time backprop was really introduced
[173.00s -> 175.00s]  was in 1986 with Rummelhart,
[175.00s -> 177.00s]  and so here we can start seeing, you know,
[177.00s -> 179.00s]  these kinds of equations with the chain rule
[179.00s -> 181.00s]  and the update rules that we're starting
[181.00s -> 183.00s]  to get familiar with, right,
[183.00s -> 184.00s]  and so this is the first time
[184.00s -> 186.00s]  we started to have a principled way
[186.00s -> 190.00s]  to train these kinds of network architectures.
[191.00s -> 193.00s]  And so after that, you know,
[193.00s -> 195.00s]  this still wasn't able to scale
[195.00s -> 197.00s]  to very large neural networks,
[197.00s -> 199.00s]  and so there's sort of a period in which
[199.00s -> 202.00s]  there wasn't a whole lot of new things happening here
[202.00s -> 206.00s]  or a lot of popular use of these kinds of networks,
[206.00s -> 208.00s]  and so this really started being reinvigorated
[208.00s -> 210.00s]  around the 2000s.
[210.00s -> 214.00s]  So in 2006, there was this paper by Jeff Hinton
[214.00s -> 217.00s]  and Ruslan Solukudinov,
[217.00s -> 219.00s]  which basically showed that we could train
[219.00s -> 220.00s]  a deep neural network
[220.00s -> 222.00s]  and show that we could do this effectively.
[222.00s -> 225.00s]  But it was still not quite the sort of modern iteration
[225.00s -> 227.00s]  of neural networks.
[227.00s -> 229.00s]  It required really careful initialization
[229.00s -> 232.00s]  in order to be able to do backprop,
[232.00s -> 233.00s]  and so what they had here was
[233.00s -> 236.00s]  they would have this first pre-training stage
[236.00s -> 238.00s]  where you model each hidden layer
[238.00s -> 239.00s]  through this kind of,
[239.00s -> 241.00s]  through a restricted Boltzmann machine,
[241.00s -> 244.00s]  and so you're going to get some initialized weights
[244.00s -> 247.00s]  by training each of these layers iteratively,
[247.00s -> 249.00s]  and so once you get all of these hidden layers,
[249.00s -> 251.00s]  you then use that to initialize
[251.00s -> 254.00s]  your full neural network,
[254.00s -> 256.00s]  and then from there,
[256.00s -> 259.00s]  you do backprop and fine-tuning of that.
[262.00s -> 264.00s]  And so when we really started to get
[264.00s -> 266.00s]  the first really strong results
[266.00s -> 267.00s]  using neural networks
[267.00s -> 270.00s]  and what sort of really sparked
[270.00s -> 274.00s]  the whole craze of starting to use
[274.00s -> 276.00s]  these kinds of networks really widely
[276.00s -> 279.00s]  was at around 2000, 2012,
[279.00s -> 282.00s]  where we had first the strongest results
[282.00s -> 284.00s]  using, for speech recognition,
[284.00s -> 287.00s]  and so this is work out of Jeff Hinton's lab
[287.00s -> 290.00s]  for acoustic modeling and speech recognition,
[290.00s -> 292.00s]  and then for image recognition,
[292.00s -> 294.00s]  2012 was the landmark paper
[294.00s -> 298.00s]  from Alex Krashevsky in Jeff Hinton's lab,
[298.00s -> 300.00s]  which introduced the first
[300.00s -> 302.00s]  convolutional neural network architecture
[302.00s -> 304.00s]  that was able to do,
[304.00s -> 306.00s]  get really strong results on ImageNet classification,
[306.00s -> 308.00s]  and so it took the ImageNet
[308.00s -> 310.00s]  image classification benchmark
[310.00s -> 312.00s]  and was able to dramatically reduce
[312.00s -> 315.00s]  the error on that benchmark.
[315.00s -> 317.00s]  And so since then,
[317.00s -> 318.00s]  you know,
[318.00s -> 320.00s]  ConvNets have gotten really widely used
[320.00s -> 323.00s]  in all kinds of applications.
[323.00s -> 324.00s]  So now let's step back
[324.00s -> 326.00s]  and take a look at
[326.00s -> 328.00s]  what gave rise to
[328.00s -> 331.00s]  convolutional neural networks specifically.
[331.00s -> 333.00s]  And so we can go back to the 1950s,
[333.00s -> 335.00s]  where Hubel and Wiesel
[335.00s -> 337.00s]  did a series of experiments
[337.00s -> 340.00s]  trying to understand how
[340.00s -> 342.00s]  neurons in the visual cortex worked,
[342.00s -> 345.00s]  and they studied this specifically for cats.
[345.00s -> 348.00s]  And so we talked a little bit about this in lecture one,
[348.00s -> 350.00s]  but basically in these experiments,
[350.00s -> 352.00s]  they put electrodes in the cat,
[352.00s -> 353.00s]  into the cat brain,
[353.00s -> 356.00s]  and they gave the cat different visual stimulus, right?
[356.00s -> 358.00s]  And so things like, you know,
[358.00s -> 361.00s]  different kinds of edges, oriented edges,
[361.00s -> 362.00s]  different sorts of shapes,
[362.00s -> 365.00s]  and they measured the response of the neurons
[365.00s -> 367.00s]  to these stimuli.
[369.00s -> 372.00s]  And so there are a couple of important
[372.00s -> 375.00s]  conclusions that they were able to make in observations.
[375.00s -> 377.00s]  And so the first thing they found that,
[377.00s -> 379.00s]  you know, there's sort of this topographical mapping
[379.00s -> 380.00s]  in the cortex.
[380.00s -> 382.00s]  So nearby cells in the cortex
[382.00s -> 385.00s]  also represent nearby regions in the visual field.
[385.00s -> 387.00s]  And so you can see, for example,
[387.00s -> 388.00s]  on the right here,
[388.00s -> 391.00s]  where if you take kind of the spatial mapping
[391.00s -> 394.00s]  and map this onto a visual cortex,
[394.00s -> 397.00s]  there's more peripheral regions are these blue areas,
[397.00s -> 400.00s]  you know, farther away from the center.
[402.00s -> 404.00s]  And so they also discovered that these neurons
[404.00s -> 407.00s]  had a hierarchical organization.
[407.00s -> 411.00s]  And so if you look at different types of visual stimuli,
[411.00s -> 414.00s]  they were able to find that at the earliest layers,
[414.00s -> 417.00s]  retinal ganglion cells were responsive to things
[417.00s -> 419.00s]  that looked kind of like, you know,
[419.00s -> 421.00s]  circular regions of spots.
[421.00s -> 424.00s]  And then on top of that, there are simple cells,
[424.00s -> 428.00s]  and these simple cells are responsive to oriented edges,
[428.00s -> 431.00s]  so different orientation of the light stimulus.
[431.00s -> 432.00s]  And then going further,
[432.00s -> 434.00s]  they discovered that these were then connected
[434.00s -> 435.00s]  to more complex cells,
[435.00s -> 438.00s]  which were responsive to both light orientation
[438.00s -> 440.00s]  as well as movement and so on.
[440.00s -> 442.00s]  And you get, you know, increasing complexity,
[442.00s -> 444.00s]  for example, hyper-complex cells
[444.00s -> 447.00s]  are now responsive to movement
[447.00s -> 449.00s]  with kind of an end point, right?
[449.00s -> 452.00s]  And so now you're starting to get the idea of corners
[452.00s -> 454.00s]  and then blobs and so on.
[458.00s -> 462.00s]  And so then in 1980, the neo-cognitron was
[463.00s -> 466.00s]  the first example of a network architecture,
[466.00s -> 469.00s]  a model that had this idea of, you know,
[469.00s -> 470.00s]  simple and complex cells
[470.00s -> 472.00s]  that Hubel and Wiesel had discovered,
[472.00s -> 474.00s]  and in this case, Fukushima put these
[474.00s -> 478.00s]  into these alternating layers of simple and complex cells,
[478.00s -> 480.00s]  where you had these simple cells
[480.00s -> 481.00s]  that had modifiable parameters
[481.00s -> 483.00s]  and then complex cells on top of these
[483.00s -> 486.00s]  that performed this sort of pooling
[486.00s -> 488.00s]  so that it was invariant to, you know,
[488.00s -> 492.00s]  different minor modifications from the simple cells.
[494.00s -> 497.00s]  And so this is work that was in the 1980s, right?
[497.00s -> 499.00s]  And so by 1998,
[499.00s -> 503.00s]  Yann LeCun basically showed the first example
[503.00s -> 507.00s]  of applying backpropagation and radiant-based learning
[507.00s -> 509.00s]  to train convolutional neural networks
[509.00s -> 511.00s]  that did really well on document recognition,
[511.00s -> 515.00s]  and specifically, they were able to do a good job
[515.00s -> 517.00s]  of recognizing digits of zip codes,
[517.00s -> 519.00s]  and so these were then used, you know,
[519.00s -> 522.00s]  pretty widely for zip code recognition
[522.00s -> 524.00s]  in the postal service.
[524.00s -> 527.00s]  But beyond that, it wasn't able to scale yet
[527.00s -> 531.00s]  to more challenging and complex data, right?
[531.00s -> 533.00s]  Digits are still fairly simple
[533.00s -> 535.00s]  and a limited set to recognize.
[536.00s -> 540.00s]  And so this is where Alex Krashevsky in 2012
[540.00s -> 542.00s]  gave the modern incarnation
[542.00s -> 544.00s]  of convolutional neural networks,
[544.00s -> 548.00s]  and his network, we sort of colloquially call AlexNet,
[548.00s -> 551.00s]  but this network really didn't look so much different
[551.00s -> 554.00s]  than the convolutional neural networks
[554.00s -> 556.00s]  that Yann LeCun was dealing with.
[556.00s -> 557.00s]  They're now, you know,
[557.00s -> 560.00s]  they were scaled now to be larger and deeper
[560.00s -> 563.00s]  and able to, the most important parts were that
[563.00s -> 564.00s]  they were now able to take advantage
[564.00s -> 567.00s]  of the large amount of data that's now available,
[567.00s -> 571.00s]  you know, in web images, in ImageNet dataset,
[571.00s -> 573.00s]  as well as take advantage
[573.00s -> 577.00s]  of the parallel computing power in GPUs.
[577.00s -> 580.00s]  And so we'll talk more about that later.
[580.00s -> 582.00s]  But fast-forwarding today,
[582.00s -> 584.00s]  so now, you know, ConvNets are used everywhere,
[584.00s -> 587.00s]  and so we have the initial, you know,
[587.00s -> 591.00s]  classification results on ImageNet from Alex Krashevsky.
[592.00s -> 595.00s]  This is able to do a really good job of image retrieval.
[595.00s -> 596.00s]  You can see that, you know,
[596.00s -> 598.00s]  when we're trying to retrieve a flower, for example,
[598.00s -> 601.00s]  the features that are learned are really powerful
[601.00s -> 604.00s]  for doing similarity matching.
[604.00s -> 607.00s]  We also have ConvNets that are used for detection,
[607.00s -> 610.00s]  so we're able to do a really good job of localizing,
[610.00s -> 612.00s]  you know, where in an image is,
[612.00s -> 614.00s]  for example, a bus or a boat and so on,
[614.00s -> 617.00s]  and draw precise bounding boxes around that.
[617.00s -> 620.00s]  We're able to go even deeper beyond that
[620.00s -> 621.00s]  to do segmentation, right?
[621.00s -> 623.00s]  So these are now richer tasks where we don't,
[623.00s -> 625.00s]  we're not looking for just the bounding box,
[625.00s -> 627.00s]  but we're actually going to label every pixel
[627.00s -> 631.00s]  on the outline of, you know, trees and people and so on.
[633.00s -> 636.00s]  And these kind of algorithms are used in,
[636.00s -> 638.00s]  you know, for example, self-driving cars,
[638.00s -> 640.00s]  and a lot of this is powered by GPUs,
[640.00s -> 641.00s]  as I mentioned earlier,
[641.00s -> 644.00s]  that's able to do parallel processing
[644.00s -> 648.00s]  and able to efficiently train and run these ConvNets.
[648.00s -> 651.00s]  And so, you know, we have modern powerful GPUs,
[651.00s -> 655.00s]  as well as ones that, you know, work in embedded systems,
[655.00s -> 659.00s]  for example, that you would use in a self-driving car.
[659.00s -> 660.00s]  So we can also look at many,
[660.00s -> 661.00s]  some of the other applications
[661.00s -> 663.00s]  that ConvNets are used for.
[663.00s -> 665.00s]  So face recognition, right?
[665.00s -> 666.00s]  We can put an input image of a face
[666.00s -> 670.00s]  and get out a likelihood of who this person is.
[672.00s -> 674.00s]  ConvNets are applied to video,
[674.00s -> 677.00s]  and so this is an example of a video network
[677.00s -> 681.00s]  that looks at both images as well as temporal information,
[681.00s -> 685.00s]  and from there is able to classify videos.
[685.00s -> 688.00s]  We're also able to do pose recognition,
[688.00s -> 690.00s]  being able to recognize, you know, shoulders,
[690.00s -> 692.00s]  elbows, and different joints.
[692.00s -> 697.00s]  And so here are some images of our fabulous TA, Lane,
[697.00s -> 702.00s]  in various kinds of pretty non-standard human poses.
[702.00s -> 705.00s]  But ConvNets are able to do a pretty good job
[705.00s -> 708.00s]  of pose recognition these days.
[708.00s -> 711.00s]  They're also used in game playing,
[711.00s -> 714.00s]  so some of the work in reinforcement learning,
[714.00s -> 716.00s]  deep reinforcement learning that you may have seen,
[716.00s -> 718.00s]  playing Atari games and Go and so on,
[718.00s -> 722.00s]  and ConvNets are an important part of all of these.
[722.00s -> 724.00s]  Some other applications,
[724.00s -> 727.00s]  so they're being used for interpretation
[727.00s -> 730.00s]  and diagnosis of medical images,
[730.00s -> 733.00s]  for classification of galaxies,
[733.00s -> 736.00s]  for street sign recognition.
[738.00s -> 739.00s]  There's also whale recognition,
[739.00s -> 742.00s]  this is from a recent Kaggle challenge.
[742.00s -> 746.00s]  We also have examples of looking at aerial maps
[746.00s -> 748.00s]  and being able to draw out where are the streets
[748.00s -> 749.00s]  on these maps, where are buildings,
[749.00s -> 753.00s]  and being able to segment all of these.
[755.00s -> 758.00s]  And then beyond recognition of classification,
[758.00s -> 760.00s]  detection, these types of tasks,
[760.00s -> 762.00s]  we also have tasks like image captioning,
[762.00s -> 764.00s]  where given an image, we want to write
[764.00s -> 767.00s]  a sentence description about what's in the image.
[767.00s -> 769.00s]  And so this is something that we'll go into
[769.00s -> 772.00s]  a little bit more later in the class.
[772.00s -> 776.00s]  And we also have really fancy and cool artwork
[778.00s -> 780.00s]  that we can do using neural networks.
[780.00s -> 783.00s]  And so on the left is an example of DeepDream,
[783.00s -> 784.00s]  where we're able to take images
[784.00s -> 788.00s]  and kind of hallucinate different kinds of objects
[789.00s -> 791.00s]  and concepts in the image.
[792.00s -> 794.00s]  There's also a neural style type work,
[794.00s -> 797.00s]  where we take an image and we're able to
[797.00s -> 800.00s]  re-render this image using a style
[801.00s -> 803.00s]  of a particular artist and artwork.
[803.00s -> 806.00s]  And so here we can take, for example, Van Gogh
[806.00s -> 808.00s]  on the right, Starry Night,
[808.00s -> 811.00s]  and use that to redraw our original image
[811.00s -> 813.00s]  using that style.
[813.00s -> 816.00s]  And Justin has done a lot of work in this,
[816.00s -> 818.00s]  and so if you guys are interested,
[818.00s -> 822.00s]  these are images produced by some of his code,
[822.00s -> 826.00s]  and you guys should talk to him more about it.
[826.00s -> 830.00s]  Okay, so basically, this is just a small sample
[830.00s -> 832.00s]  of where comnets are being used today.
[832.00s -> 834.00s]  But there's really a huge amount
[834.00s -> 835.00s]  that can be done with this, right?
[835.00s -> 838.00s]  And so for your guys' projects,
[838.00s -> 840.00s]  sort of let your imagination go wild,
[840.00s -> 843.00s]  and we're excited to see what sorts
[843.00s -> 846.00s]  of applications you can come up with.
[846.00s -> 848.00s]  So today we're going to talk about
[848.00s -> 850.00s]  how convolutional neural networks work.
[850.00s -> 852.00s]  And again, same as with neural networks,
[852.00s -> 854.00s]  we're going to first talk about
[854.00s -> 856.00s]  how they work from a functional perspective,
[856.00s -> 858.00s]  without any of the brain analogies,
[858.00s -> 860.00s]  and then we'll talk briefly about
[860.00s -> 862.00s]  some of these connections.
[865.00s -> 868.00s]  Okay, so last lecture, we talked about
[868.00s -> 871.00s]  this idea of a fully connected layer.
[872.00s -> 876.00s]  And how, you know, for a fully connected layer,
[876.00s -> 878.00s]  what we're doing is we operate on top
[878.00s -> 879.00s]  of these vectors, right?
[879.00s -> 883.00s]  And so let's say we have, you know, an image,
[883.00s -> 885.00s]  a 3D image, 32 by 32 by three,
[885.00s -> 888.00s]  so some of the images that we were looking at previously,
[888.00s -> 891.00s]  we'll take that, we'll stretch all the pixels out,
[891.00s -> 895.00s]  right, and then we have this 3,072 dimensional vector,
[895.00s -> 896.00s]  for example, in this case,
[896.00s -> 898.00s]  and then we have these weights, right?
[898.00s -> 901.00s]  So we're going to multiply this by a weight matrix,
[901.00s -> 903.00s]  and so here, for example, our W,
[903.00s -> 906.00s]  we're going to say is 10 by 3,072,
[906.00s -> 910.00s]  and then we're going to get the activations,
[910.00s -> 912.00s]  the output of this layer, right?
[912.00s -> 916.00s]  And so in this case, we take each of our 10 rows
[916.00s -> 918.00s]  and we do this dot product
[918.00s -> 921.00s]  with the 3,072 dimensional input,
[921.00s -> 923.00s]  and from there we get this one number
[923.00s -> 927.00s]  that's kind of the value of that neuron.
[927.00s -> 928.00s]  And so in this case,
[928.00s -> 932.00s]  we're going to have 10 of these neuron outputs.
[935.00s -> 936.00s]  And so a convolutional layer,
[936.00s -> 938.00s]  so the main difference between this
[938.00s -> 940.00s]  and the fully connected layer that we've been talking about
[940.00s -> 943.00s]  is that here we want to preserve spatial structure.
[943.00s -> 946.00s]  And so taking this 32 by 32 by three image
[946.00s -> 947.00s]  that we had earlier,
[947.00s -> 950.00s]  instead of stretching this all out into one long vector,
[950.00s -> 953.00s]  we're now going to keep the structure of this image,
[953.00s -> 956.00s]  right, this three dimensional input.
[957.00s -> 959.00s]  And then what we're going to do is,
[959.00s -> 961.00s]  our weights are going to be these small filters,
[961.00s -> 963.00s]  so in this case, for example,
[963.00s -> 965.00s]  a five by five by three filter,
[965.00s -> 967.00s]  and we're going to take this filter
[967.00s -> 969.00s]  and we're going to slide it over the image spatially
[969.00s -> 973.00s]  and compute dot products at every spatial location.
[973.00s -> 974.00s]  And so we're going to go into detail
[974.00s -> 977.00s]  of how exactly this works.
[978.00s -> 980.00s]  So our filters, first of all,
[980.00s -> 983.00s]  always extend the full depth of the input volume.
[983.00s -> 988.00s]  And so they're going to be just a smaller spatial area,
[988.00s -> 990.00s]  so in this case, five by five, right,
[990.00s -> 993.00s]  instead of our full 32 by 32 spatial input,
[993.00s -> 997.00s]  but they're always going to go through the full depth.
[997.00s -> 1001.00s]  So here we're going to take five by five by three.
[1002.00s -> 1004.00s]  And then we're going to take this filter
[1004.00s -> 1006.00s]  and at a given spatial location,
[1006.00s -> 1009.00s]  we're going to do a dot product between this filter
[1009.00s -> 1012.00s]  and then a chunk of the image.
[1012.00s -> 1014.00s]  So we're just going to overlay this filter
[1014.00s -> 1017.00s]  on top of a spatial location in the image, right,
[1017.00s -> 1018.00s]  and then do the dot product,
[1018.00s -> 1022.00s]  the multiplication of each element of that filter
[1022.00s -> 1025.00s]  with each corresponding element in that spatial location
[1025.00s -> 1027.00s]  that we've just plopped it on top of.
[1027.00s -> 1029.00s]  And then this is going to give us a dot product.
[1029.00s -> 1034.00s]  So in this case, we have five times five times three,
[1034.00s -> 1036.00s]  this is the number of multiplications
[1036.00s -> 1037.00s]  that we're going to do, right,
[1037.00s -> 1038.00s]  plus the bias term.
[1038.00s -> 1040.00s]  And so this is basically
[1040.00s -> 1045.00s]  taking our filter w and basically doing w transpose
[1045.00s -> 1047.00s]  times x and plus bias.
[1047.00s -> 1050.00s]  So is that clear how this works?
[1055.00s -> 1057.00s]  Yeah, so the question is when we do the dot product,
[1057.00s -> 1060.00s]  do we turn the five by five by three into one vector?
[1060.00s -> 1062.00s]  Yeah, and in essence, that's what you're doing.
[1062.00s -> 1065.00s]  I mean, you can think of it as just popping it on
[1065.00s -> 1068.00s]  and doing the element-wise multiplication at each location,
[1068.00s -> 1070.00s]  but this is going to give you the same result
[1070.00s -> 1073.00s]  as if you stretched out the filter at that point,
[1073.00s -> 1075.00s]  stretched out the input volume that it's laid over
[1075.00s -> 1077.00s]  and then took the dot product.
[1077.00s -> 1078.00s]  And that's what's written here.
[1078.00s -> 1080.00s]  Yeah, question?
[1083.00s -> 1085.00s]  Oh, this is, so the question is any intuition
[1085.00s -> 1087.00s]  for why this is a w transpose?
[1087.00s -> 1090.00s]  And this is just, not really,
[1090.00s -> 1092.00s]  this is just the notation that we have here
[1092.00s -> 1095.00s]  to make the math work out as a dot product.
[1095.00s -> 1097.00s]  So it just depends on whether,
[1097.00s -> 1101.00s]  how you're representing w and whether,
[1101.00s -> 1104.00s]  in this case, if we look at the w matrix,
[1104.00s -> 1105.00s]  this happens to be each column
[1105.00s -> 1107.00s]  and so we're just taking the transpose
[1107.00s -> 1109.00s]  to get a row out of it.
[1109.00s -> 1112.00s]  But there's no intuition here.
[1112.00s -> 1114.00s]  We're just taking the filters of w
[1114.00s -> 1117.00s]  and we're stretching it out into a 1D vector
[1117.00s -> 1119.00s]  and in order for it to be a dot product,
[1119.00s -> 1123.00s]  it has to be like a one by, one by n vector.
[1123.00s -> 1124.00s]  Yes?
[1128.00s -> 1130.00s]  Okay, so the question is,
[1130.00s -> 1132.00s]  is w here not five by five by three,
[1132.00s -> 1134.00s]  it's one by 75?
[1134.00s -> 1136.00s]  So that's the case, right,
[1136.00s -> 1138.00s]  if we're going to do this dot product
[1138.00s -> 1140.00s]  of w transpose times x,
[1140.00s -> 1141.00s]  we have to stretch it out first
[1141.00s -> 1142.00s]  before we do the dot product.
[1142.00s -> 1145.00s]  So we take the five by five by three
[1145.00s -> 1146.00s]  and we just take all these values
[1146.00s -> 1150.00s]  and stretch it out into a long vector.
[1150.00s -> 1154.00s]  And so, again, similar to the other question,
[1154.00s -> 1156.00s]  the actual operation that we're doing here
[1156.00s -> 1158.00s]  is plopping our filter on top
[1158.00s -> 1160.00s]  of a spatial location in the image
[1160.00s -> 1163.00s]  and multiplying all the corresponding values together,
[1163.00s -> 1165.00s]  but in order just to make it kind of an easy expression
[1165.00s -> 1167.00s]  similar to what we've seen before,
[1167.00s -> 1169.00s]  we can also just stretch each of these out,
[1169.00s -> 1172.00s]  make sure that dimensions are transposed correctly
[1172.00s -> 1175.00s]  so that it works out as a dot product.
[1180.00s -> 1181.00s]  Okay, the question is,
[1181.00s -> 1183.00s]  how do we slide the filter over the image?
[1183.00s -> 1186.00s]  We'll go into that next, yes.
[1192.00s -> 1193.00s]  Okay, so the question is,
[1193.00s -> 1196.00s]  should we rotate the kernel by 180 degrees
[1196.00s -> 1199.00s]  to better match the definition of a convolution?
[1199.00s -> 1201.00s]  And so the answer is that
[1201.00s -> 1203.00s]  we'll also show the equation for this later,
[1203.00s -> 1207.00s]  but we're using convolution as kind of a
[1207.00s -> 1209.00s]  looser definition of what's happening.
[1209.00s -> 1211.00s]  So for people from signal processing,
[1211.00s -> 1213.00s]  what we are actually technically doing,
[1213.00s -> 1215.00s]  if you want to call this a convolution,
[1215.00s -> 1217.00s]  is we're convolving with the flipped version
[1217.00s -> 1219.00s]  of the filter, but for the most part,
[1219.00s -> 1221.00s]  we just don't worry about this
[1221.00s -> 1224.00s]  and we just, yeah, do this operation
[1224.00s -> 1228.00s]  and it's like a convolution in spirit.
[1228.00s -> 1233.00s]  Okay, so, okay, so we had a question earlier,
[1233.00s -> 1235.00s]  how do we, you know,
[1235.00s -> 1237.00s]  slide this over all the spatial locations?
[1237.00s -> 1238.00s]  Right, so what we're going to do
[1238.00s -> 1240.00s]  is we're going to take this filter,
[1240.00s -> 1241.00s]  we're going to start, you know,
[1241.00s -> 1243.00s]  at the upper left-hand corner
[1243.00s -> 1245.00s]  and basically center our filter
[1245.00s -> 1249.00s]  on top of every pixel in this input volume
[1249.00s -> 1251.00s]  and at every position,
[1251.00s -> 1253.00s]  we're going to do this dot product
[1253.00s -> 1255.00s]  and this will produce one value
[1255.00s -> 1257.00s]  in our output activation map.
[1257.00s -> 1260.00s]  And so then we're going to just slide this around.
[1260.00s -> 1262.00s]  The simplest version is just that every pixel
[1262.00s -> 1264.00s]  we're going to do this operation
[1264.00s -> 1267.00s]  and fill in the corresponding point
[1268.00s -> 1270.00s]  in our output activation.
[1270.00s -> 1273.00s]  You can see here that the dimensions
[1273.00s -> 1275.00s]  are not exactly what would happen, right,
[1275.00s -> 1276.00s]  if you're going to do this.
[1276.00s -> 1277.00s]  I had 32 by 32 in the input
[1277.00s -> 1280.00s]  and I'm having 28 by 28 in the output
[1280.00s -> 1282.00s]  and so we'll go into examples later
[1282.00s -> 1285.00s]  of the math of exactly how this is going to work out
[1285.00s -> 1287.00s]  dimension-wise, but basically,
[1287.00s -> 1291.00s]  you have a choice of how you're going to slide this,
[1291.00s -> 1293.00s]  whether you go at every pixel
[1293.00s -> 1296.00s]  or whether you slide, let's say, you know,
[1296.00s -> 1299.00s]  two input values over at a time,
[1299.00s -> 1301.00s]  two pixels over at a time,
[1301.00s -> 1303.00s]  and so you can get different sized outputs
[1303.00s -> 1305.00s]  depending on how you choose to slide,
[1305.00s -> 1306.00s]  but you're basically doing this operation
[1306.00s -> 1308.00s]  in a grid fashion.
[1310.00s -> 1312.00s]  Okay, so what we just saw earlier,
[1312.00s -> 1314.00s]  this is taking one filter,
[1314.00s -> 1318.00s]  sliding it over all of the spatial locations in the image
[1318.00s -> 1320.00s]  and then we're going to get this activation map out,
[1320.00s -> 1322.00s]  which is the value of that filter
[1322.00s -> 1324.00s]  at every spatial location.
[1324.00s -> 1327.00s]  And so when we're dealing with a convolutional layer,
[1327.00s -> 1329.00s]  we want to work with multiple filters, right,
[1329.00s -> 1331.00s]  because each filter is kind of looking
[1331.00s -> 1334.00s]  for a specific type of template or concept
[1334.00s -> 1336.00s]  in the input volume.
[1336.00s -> 1340.00s]  And so we're going to have a set of multiple filters
[1340.00s -> 1342.00s]  and so here I'm going to take a second filter,
[1342.00s -> 1346.00s]  this green filter, which is again five by five by three.
[1346.00s -> 1347.00s]  I'm going to slide this over
[1347.00s -> 1351.00s]  all of the spatial locations in my input volume,
[1351.00s -> 1353.00s]  and then I'm going to get out
[1353.00s -> 1357.00s]  this second green activation map also of the same size.
[1359.00s -> 1361.00s]  And we can do this for as many filters
[1361.00s -> 1363.00s]  as we want to have in this layer.
[1363.00s -> 1365.00s]  So for example, if we have six filters,
[1365.00s -> 1367.00s]  six of these five by five filters,
[1367.00s -> 1369.00s]  then we're going to get in total
[1369.00s -> 1371.00s]  six activation maps out.
[1371.00s -> 1374.00s]  All of, so we're going to get this output volume
[1374.00s -> 1378.00s]  that's going to be basically six by 28 by 28.
[1381.00s -> 1383.00s]  Right, and so a preview of how we're going to use
[1383.00s -> 1386.00s]  these convolutional layers in our convolutional network
[1386.00s -> 1388.00s]  is that our comment is basically going to be
[1388.00s -> 1390.00s]  a sequence of these convolutional layers
[1390.00s -> 1392.00s]  stacked on top of each other,
[1392.00s -> 1395.00s]  same way as what we had with the simple linear layers
[1395.00s -> 1396.00s]  in our neural network,
[1396.00s -> 1398.00s]  and then we're going to intersperse these
[1398.00s -> 1399.00s]  with activation functions,
[1399.00s -> 1403.00s]  so for example, a ReLU activation function.
[1404.00s -> 1406.00s]  Right, and so you're going to get something like
[1406.00s -> 1409.00s]  convolutional, conv ReLU,
[1409.00s -> 1411.00s]  and usually also some pooling layers,
[1411.00s -> 1412.00s]  and then you're just going to get
[1412.00s -> 1414.00s]  a sequence of these, each putting,
[1414.00s -> 1416.00s]  each creating an output that's now going to be
[1416.00s -> 1420.00s]  the input to the next convolutional layer.
[1423.00s -> 1425.00s]  Okay, and so each of these layers,
[1425.00s -> 1428.00s]  as I said earlier, has multiple filters, right,
[1428.00s -> 1430.00s]  many filters, and each of the filters
[1430.00s -> 1432.00s]  is producing an activation map.
[1433.00s -> 1435.00s]  And so when you look at multiple of these layers
[1435.00s -> 1437.00s]  stacked together in a ConvNet,
[1437.00s -> 1439.00s]  what ends up happening is you end up learning
[1439.00s -> 1441.00s]  this hierarchy of filters,
[1441.00s -> 1443.00s]  where the filters at the earlier layers
[1443.00s -> 1445.00s]  usually represent low-level features
[1445.00s -> 1446.00s]  that you're looking for,
[1446.00s -> 1449.00s]  so things kind of like edges, right,
[1449.00s -> 1450.00s]  and then at the mid-level,
[1450.00s -> 1452.00s]  you're going to get more complex,
[1452.00s -> 1454.00s]  more complex kinds of features,
[1454.00s -> 1457.00s]  so maybe it's looking more for things like
[1457.00s -> 1459.00s]  corners and blobs and so on,
[1459.00s -> 1460.00s]  and then at higher-level features,
[1460.00s -> 1462.00s]  you're going to get things that are starting
[1462.00s -> 1465.00s]  to more resemble concepts and blobs.
[1465.00s -> 1467.00s]  And we'll go into more detail later in the class
[1467.00s -> 1470.00s]  on how you can actually visualize all these features
[1470.00s -> 1473.00s]  and try and interpret what your network,
[1473.00s -> 1475.00s]  what kinds of features your network is learning,
[1475.00s -> 1476.00s]  but the important thing for now
[1476.00s -> 1479.00s]  is just to understand that what these features
[1479.00s -> 1482.00s]  end up being when you have a whole stack of these
[1482.00s -> 1486.00s]  is these types of simple to more complex features.
[1488.00s -> 1489.00s]  Yeah.
[1489.00s -> 1490.00s]  Yeah.
[1499.00s -> 1501.00s]  Okay, so the question is what's the intuition
[1501.00s -> 1503.00s]  for increasing the depth each time,
[1503.00s -> 1506.00s]  so here I had three filters in the original layer
[1506.00s -> 1509.00s]  and then six filters in the next layer, right,
[1509.00s -> 1512.00s]  and so this is mostly a design choice.
[1512.00s -> 1514.00s]  You know, people in practice have found
[1514.00s -> 1517.00s]  certain types of these configurations to work better,
[1517.00s -> 1519.00s]  and so later on we'll go into case studies
[1519.00s -> 1521.00s]  of different kinds of convolutional neural network
[1521.00s -> 1525.00s]  architectures and design choices for these
[1525.00s -> 1528.00s]  and why certain ones work better than others.
[1528.00s -> 1530.00s]  But yeah, basically the choice of,
[1530.00s -> 1531.00s]  you're going to have many design choices
[1531.00s -> 1533.00s]  in a convolutional neural network,
[1533.00s -> 1534.00s]  the size of your filter, the stride,
[1534.00s -> 1536.00s]  how many filters you have,
[1536.00s -> 1539.00s]  and so we'll talk about this all more later.
[1539.00s -> 1540.00s]  Questions?
[1548.00s -> 1552.00s]  Yeah, so the question is, as we're sliding this filter
[1552.00s -> 1554.00s]  over the image spatially, it looks like
[1554.00s -> 1557.00s]  we're sampling the edges and corners less
[1557.00s -> 1558.00s]  than the other locations.
[1558.00s -> 1560.00s]  Yeah, that's a really good point,
[1560.00s -> 1563.00s]  and we'll talk, I think, in a few slides
[1563.00s -> 1567.00s]  about how we try and compensate for that.
[1570.00s -> 1574.00s]  Okay, so, you know, so each of these layers,
[1575.00s -> 1578.00s]  convolutional layers that we have stacked together,
[1578.00s -> 1582.00s]  we saw how we're starting with more simpler features
[1582.00s -> 1584.00s]  and then aggregating these into more
[1584.00s -> 1586.00s]  complex features later on.
[1586.00s -> 1589.00s]  And so in practice, this is compatible with
[1589.00s -> 1592.00s]  what Tubal and Wiesel noticed in their experiments,
[1592.00s -> 1595.00s]  right, that we had these simple cells
[1595.00s -> 1597.00s]  at the earlier stages of processing
[1597.00s -> 1599.00s]  followed by more complex cells later on.
[1599.00s -> 1601.00s]  And so even though we didn't try,
[1601.00s -> 1605.00s]  we didn't explicitly, you know, force our comment
[1605.00s -> 1607.00s]  to learn these kinds of features in practice
[1607.00s -> 1609.00s]  when you give it this type of hierarchical structure
[1609.00s -> 1611.00s]  and train it using back propagation,
[1611.00s -> 1615.00s]  these are the kinds of filters that end up being learned.
[1625.00s -> 1627.00s]  Okay, so yeah, so the question is,
[1627.00s -> 1630.00s]  what are we seeing in these visualizations?
[1631.00s -> 1633.00s]  And so, right, so in these visualizations,
[1633.00s -> 1635.00s]  like if we look at this conv1,
[1635.00s -> 1637.00s]  the first convolutional layer,
[1637.00s -> 1640.00s]  each of these grid, each part of this grid
[1640.00s -> 1643.00s]  is a one neuron, and so what we visualized here
[1643.00s -> 1647.00s]  is what the input looks like that maximizes
[1648.00s -> 1650.00s]  the activation of that particular neuron.
[1650.00s -> 1652.00s]  So what sort of image you would get
[1652.00s -> 1654.00s]  that would give you the largest,
[1654.00s -> 1656.00s]  make that neuron fire and have the largest value.
[1656.00s -> 1659.00s]  And so the way we do this is basically
[1659.00s -> 1662.00s]  by doing kind of back propagation
[1662.00s -> 1664.00s]  from a particular neuron activation
[1664.00s -> 1666.00s]  and seeing what in the input will trigger,
[1666.00s -> 1668.00s]  will give you the highest values of this neuron.
[1668.00s -> 1670.00s]  And this is something that we'll talk about
[1670.00s -> 1673.00s]  in much more depth in a later lecture
[1673.00s -> 1676.00s]  about how we create all of these visualizations.
[1676.00s -> 1679.00s]  But basically, each element of these grids
[1679.00s -> 1683.00s]  is showing what in the input would look like
[1683.00s -> 1686.00s]  that basically maximizes the activation of the neuron.
[1686.00s -> 1690.00s]  So in a sense, what is the neuron looking for?
[1693.00s -> 1698.00s]  Okay, so here is an example of some of the activation maps
[1698.00s -> 1699.00s]  produced by each filter, right?
[1699.00s -> 1702.00s]  So we can visualize up here on the top,
[1702.00s -> 1706.00s]  we have this whole row of example five by five filters,
[1706.00s -> 1708.00s]  and so this is basically a real case
[1708.00s -> 1712.00s]  from a trained ConvNet where each of these
[1712.00s -> 1715.00s]  is what a five by five filter looks like
[1716.00s -> 1718.00s]  and then as we convolve this over an image,
[1718.00s -> 1721.00s]  so in this case, I think it's like a quarter of a car,
[1721.00s -> 1724.00s]  the car light, what the activation looks like, right?
[1724.00s -> 1728.00s]  And so here, for example, if we look at this first one,
[1728.00s -> 1731.00s]  this red filter, a filter with a red box around it,
[1731.00s -> 1733.00s]  we'll see that it's looking for,
[1733.00s -> 1736.00s]  the template looks like an edge, right, an oriented edge.
[1736.00s -> 1738.00s]  And so if you slide it over the image,
[1738.00s -> 1741.00s]  it'll have a high value, a more white value
[1741.00s -> 1746.00s]  where there are edges in this type of orientation.
[1746.00s -> 1749.00s]  And so each of these activation maps
[1749.00s -> 1752.00s]  is the output of sliding one of these filters over
[1752.00s -> 1755.00s]  and where these filters are causing,
[1755.00s -> 1757.00s]  you know, where this sort of template
[1757.00s -> 1760.00s]  is more present in the image.
[1760.00s -> 1763.00s]  And so the reason we call these convolutional
[1763.00s -> 1765.00s]  is because this is related
[1765.00s -> 1766.00s]  to the convolution of two signals
[1766.00s -> 1768.00s]  and so someone pointed out earlier
[1768.00s -> 1770.00s]  that, you know, this is basically
[1770.00s -> 1772.00s]  this convolution equation over here
[1772.00s -> 1774.00s]  for people who have seen convolutions before
[1774.00s -> 1776.00s]  in signal processing,
[1776.00s -> 1778.00s]  and in practice it's actually more like a correlation
[1778.00s -> 1780.00s]  where we're convolving with the, you know,
[1780.00s -> 1782.00s]  flipped version of the filter.
[1782.00s -> 1785.00s]  But this is, yeah, this is kind of a subtlety,
[1785.00s -> 1789.00s]  it's not really important for the purposes of this class.
[1789.00s -> 1791.00s]  But basically if you're writing out what you're doing,
[1791.00s -> 1795.00s]  it has an expression that looks something like this,
[1795.00s -> 1797.00s]  which is the standard definition of a convolution.
[1798.00s -> 1800.00s]  But this is basically just taking a filter,
[1800.00s -> 1802.00s]  sliding it spatially over the image
[1802.00s -> 1805.00s]  and computing the dot product at every location.
[1809.00s -> 1812.00s]  Okay, so, you know, as I mentioned earlier,
[1812.00s -> 1814.00s]  like, what our total convolutional neural network
[1814.00s -> 1815.00s]  is going to look like
[1815.00s -> 1817.00s]  is we're going to have an input image
[1817.00s -> 1819.00s]  and then we're going to pass it through
[1819.00s -> 1821.00s]  these sequence of layers, right,
[1821.00s -> 1823.00s]  where we're going to have a convolutional layer first,
[1823.00s -> 1828.00s]  and we usually have our non-linear layer after that,
[1828.00s -> 1830.00s]  so ReLU is something that's very commonly used
[1830.00s -> 1833.00s]  that we're going to talk about more later,
[1833.00s -> 1836.00s]  and then we have these conv ReLU, conv ReLU layers,
[1836.00s -> 1839.00s]  and then once in a while we'll use a pooling layer
[1839.00s -> 1841.00s]  that we'll talk about later as well,
[1841.00s -> 1846.00s]  that basically downsamples the size of our activation maps.
[1847.00s -> 1848.00s]  And then finally at the end of this
[1848.00s -> 1852.00s]  we'll have, take our last convolutional layer output,
[1852.00s -> 1856.00s]  and then we're going to use a fully connected layer
[1856.00s -> 1857.00s]  that we've seen before,
[1857.00s -> 1860.00s]  connected to all of these convolutional outputs
[1860.00s -> 1862.00s]  and use that to get a final score function,
[1862.00s -> 1866.00s]  basically like what we've already been working with.
[1868.00s -> 1871.00s]  Okay, so now let's work out some examples
[1871.00s -> 1874.00s]  of how the spatial dimensions work out.
[1875.00s -> 1879.00s]  So let's take our 32 by 32 by three image as before,
[1882.00s -> 1885.00s]  right, and we have our five by five by three filter
[1885.00s -> 1887.00s]  that we're going to slide over this image,
[1887.00s -> 1889.00s]  and we're going to see how we're going to use that
[1889.00s -> 1893.00s]  to produce exactly this 28 by 28 activation map.
[1893.00s -> 1895.00s]  So let's assume that we actually have
[1895.00s -> 1898.00s]  a seven by seven input, just to be simpler,
[1898.00s -> 1901.00s]  and let's assume we have a three by three filter.
[1901.00s -> 1902.00s]  So what we're going to do is
[1902.00s -> 1904.00s]  we're going to take this filter,
[1904.00s -> 1907.00s]  plop it out, plop it down in our upper left-hand corner,
[1907.00s -> 1909.00s]  right, and we're going to multiply,
[1909.00s -> 1911.00s]  do the dot product, multiply all these values together
[1911.00s -> 1912.00s]  to get our first value,
[1912.00s -> 1914.00s]  and this is going to go into the upper left-hand value
[1914.00s -> 1916.00s]  of our activation map.
[1916.00s -> 1918.00s]  Right, and then what we're going to do next
[1918.00s -> 1920.00s]  is we're just going to take this filter,
[1920.00s -> 1922.00s]  slide it one position to the right,
[1922.00s -> 1925.00s]  and then we're going to get another value out from here.
[1925.00s -> 1927.00s]  And so we can continue with this
[1927.00s -> 1930.00s]  to have another value, another,
[1930.00s -> 1932.00s]  and in the end, what we're going to get
[1932.00s -> 1934.00s]  is a five by five output, right,
[1934.00s -> 1937.00s]  because what fit was basically sliding this filter
[1937.00s -> 1939.00s]  a total of, you know,
[1939.00s -> 1942.00s]  at five spatial locations horizontally
[1942.00s -> 1945.00s]  and five spatial locations vertically.
[1947.00s -> 1949.00s]  Okay, so as I said before,
[1949.00s -> 1950.00s]  there's different kinds of design choices
[1950.00s -> 1952.00s]  that we can make, right?
[1952.00s -> 1955.00s]  So previously, I slid it at every single spatial location,
[1955.00s -> 1958.00s]  and the interval at which I slide,
[1958.00s -> 1960.00s]  I'm going to call the stride.
[1960.00s -> 1963.00s]  And so previously, we used a stride of one.
[1963.00s -> 1964.00s]  And so now let's see what happens
[1964.00s -> 1966.00s]  if we have a stride of two.
[1966.00s -> 1968.00s]  Right, so now we're going to take our first location,
[1968.00s -> 1970.00s]  the same as before,
[1970.00s -> 1973.00s]  and then we're going to skip, this time,
[1973.00s -> 1974.00s]  two pixels over,
[1974.00s -> 1977.00s]  and we're going to get our next value
[1977.00s -> 1979.00s]  centered at this location.
[1980.00s -> 1983.00s]  Right, and so now if we use a stride of two,
[1983.00s -> 1987.00s]  we have in total three of these that can fit,
[1987.00s -> 1991.00s]  and so we're going to get a three by three output.
[1993.00s -> 1995.00s]  Okay, and so what happens when we have a stride of three?
[1995.00s -> 1998.00s]  What's the output size of this?
[1998.00s -> 2000.00s]  And so in this case, right,
[2000.00s -> 2004.00s]  we have three, we slide it over by three again,
[2004.00s -> 2006.00s]  and the problem is that here,
[2006.00s -> 2008.00s]  it actually doesn't fit, right?
[2008.00s -> 2009.00s]  So we slide it over by three,
[2009.00s -> 2012.00s]  and now it doesn't fit nicely within the image.
[2012.00s -> 2013.00s]  And so what we, in practice,
[2013.00s -> 2015.00s]  we just, it just doesn't work.
[2015.00s -> 2017.00s]  We don't do convolutions like this
[2017.00s -> 2018.00s]  because it's going to lead
[2018.00s -> 2021.00s]  to asymmetric outputs happening.
[2026.00s -> 2029.00s]  Right, and so just kind of looking at the way
[2029.00s -> 2030.00s]  that we computed how many,
[2030.00s -> 2032.00s]  what the output size is going to be,
[2032.00s -> 2034.00s]  this actually can work into a nice formula
[2034.00s -> 2037.00s]  where we take our dimension of our input n,
[2037.00s -> 2039.00s]  we have our filter size f,
[2039.00s -> 2043.00s]  we have our stride at which we're sliding along,
[2044.00s -> 2046.00s]  and our final output size,
[2046.00s -> 2049.00s]  each, the dimension, spatial dimension of each output size
[2049.00s -> 2053.00s]  is going to be n minus f divided by the stride plus one.
[2053.00s -> 2056.00s]  Right, and you can kind of see this as,
[2056.00s -> 2058.00s]  you know, if I'm going to take my filter,
[2058.00s -> 2059.00s]  let's say I fill it in
[2059.00s -> 2061.00s]  at the very last possible position that it can be in,
[2061.00s -> 2063.00s]  and then take all the pixels before that,
[2063.00s -> 2067.00s]  how many instances of moving by the stride can I fit in?
[2069.00s -> 2072.00s]  Right, and so that's how this equation kind of works out.
[2072.00s -> 2074.00s]  And so as we saw before, right,
[2074.00s -> 2077.00s]  if we have n equals seven and f equals three,
[2077.00s -> 2079.00s]  if we want a stride of one, we plug it into this formula,
[2079.00s -> 2081.00s]  we get five by five as we had before
[2081.00s -> 2083.00s]  and the same thing for two.
[2083.00s -> 2087.00s]  And with a stride of three, this doesn't really work out.
[2090.00s -> 2092.00s]  And so in practice, it's actually common
[2092.00s -> 2094.00s]  to zero pad the borders
[2094.00s -> 2099.00s]  in order to make the size work out to what we want it to.
[2099.00s -> 2101.00s]  And so this is kind of related to a question earlier,
[2101.00s -> 2104.00s]  which is what do we do right at the corners?
[2104.00s -> 2106.00s]  And so what in practice happens is
[2106.00s -> 2109.00s]  we're going to actually pad our input image with zeros
[2109.00s -> 2112.00s]  and so now you're going to be able to place a filter
[2112.00s -> 2116.00s]  centered at the upper right-hand pixel location
[2116.00s -> 2119.00s]  of your actual input image.
[2119.00s -> 2121.00s]  Okay, so here's a question.
[2121.00s -> 2124.00s]  So who can tell me if I have my same input,
[2124.00s -> 2127.00s]  seven by seven, three by three filters, slide one,
[2127.00s -> 2129.00s]  but now I pad with a one-pixel border,
[2130.00s -> 2133.00s]  what's the size of my output going to be?
[2136.00s -> 2139.00s]  So I'm hearing, I heard some sixes,
[2141.00s -> 2145.00s]  so remember we have this formula that we have before.
[2145.00s -> 2147.00s]  So if we plug in n is equal to seven,
[2147.00s -> 2150.00s]  f is equal to three, right,
[2150.00s -> 2153.00s]  and then our stride is equal to one.
[2153.00s -> 2155.00s]  So what we actually get,
[2155.00s -> 2159.00s]  so actually this is giving us seven by four,
[2159.00s -> 2161.00s]  so seven minus three is four,
[2161.00s -> 2163.00s]  divided by one, plus one is five,
[2163.00s -> 2165.00s]  and so this is what we had before.
[2165.00s -> 2167.00s]  So we actually need to adjust this formula a little bit.
[2167.00s -> 2169.00s]  So this was actually, this formula is the case
[2169.00s -> 2172.00s]  where we don't have zero-padded pixels.
[2172.00s -> 2174.00s]  But if we do pad it,
[2174.00s -> 2177.00s]  then if you now take your new output
[2177.00s -> 2179.00s]  and you slide it along,
[2179.00s -> 2182.00s]  you'll see that actually seven of the filters fit.
[2182.00s -> 2184.00s]  So you get a seven by seven output
[2184.00s -> 2186.00s]  and plugging in our original formula, right,
[2186.00s -> 2190.00s]  so our n now is not seven, it's nine.
[2190.00s -> 2191.00s]  So if we go back here,
[2191.00s -> 2195.00s]  we have n equals nine minus the filter size of three,
[2195.00s -> 2197.00s]  which gives six, right,
[2197.00s -> 2199.00s]  divided by our stride, which is one,
[2199.00s -> 2200.00s]  so it's still six,
[2200.00s -> 2202.00s]  and then plus one, we get seven.
[2202.00s -> 2203.00s]  Right, and so once you've padded it,
[2203.00s -> 2207.00s]  you want to incorporate this padding into your formula.
[2209.00s -> 2211.00s]  Yes, question.
[2211.00s -> 2212.00s]  Yeah.
[2221.00s -> 2223.00s]  Seven, okay, so the question,
[2223.00s -> 2226.00s]  so the question is what's the actual output of the size?
[2226.00s -> 2229.00s]  Is it seven by seven or seven by seven by three?
[2229.00s -> 2232.00s]  The output is going to be seven by seven
[2232.00s -> 2234.00s]  by the number of filters that you have.
[2234.00s -> 2237.00s]  So remember, each filter is going to
[2237.00s -> 2240.00s]  go dot product through the entire depth
[2240.00s -> 2241.00s]  of your input volume,
[2241.00s -> 2244.00s]  but then that's going to produce one number, right,
[2244.00s -> 2248.00s]  so each filter is, let's see if we can,
[2248.00s -> 2250.00s]  well, go back here,
[2250.00s -> 2252.00s]  each filter is producing a one by,
[2252.00s -> 2254.00s]  you know, seven by seven, in this case,
[2254.00s -> 2256.00s]  activation map output,
[2256.00s -> 2258.00s]  and so the depth is going to be
[2258.00s -> 2261.00s]  the number of filters that we have.
[2268.00s -> 2272.00s]  Sorry, let me just, one second, go back.
[2272.00s -> 2275.00s]  Okay, can you repeat your question again?
[2291.00s -> 2293.00s]  Okay, so the question is,
[2293.00s -> 2295.00s]  how does this connect to before
[2295.00s -> 2299.00s]  when we had a 32 by 32 by three input, right,
[2299.00s -> 2300.00s]  so our input had depth,
[2300.00s -> 2301.00s]  and here in this example,
[2301.00s -> 2304.00s]  I'm showing a 2D example with no depth,
[2304.00s -> 2306.00s]  and so, yeah, so I'm showing this for simplicity,
[2306.00s -> 2309.00s]  but in practice, you're going to take your,
[2309.00s -> 2311.00s]  you're going to multiply throughout the entire depth
[2311.00s -> 2312.00s]  as we had before,
[2312.00s -> 2313.00s]  so you're going to,
[2313.00s -> 2315.00s]  your filter is going to be, in this case,
[2315.00s -> 2317.00s]  a three by three spatial filter
[2317.00s -> 2319.00s]  by whatever input depth that you had,
[2319.00s -> 2323.00s]  so three by three by three in this case.
[2324.00s -> 2327.00s]  Yeah, everything else stays the same.
[2327.00s -> 2328.00s]  Yes, question?
[2328.00s -> 2330.00s]  Yeah, so the question is,
[2334.00s -> 2336.00s]  does the zero padding add some sort of
[2336.00s -> 2338.00s]  extraneous features at the corners,
[2338.00s -> 2340.00s]  and yeah, so I mean, we're doing our best
[2340.00s -> 2343.00s]  to still, you know, get some value
[2343.00s -> 2346.00s]  and do, like, process that region of the image,
[2346.00s -> 2350.00s]  and so zero padding is kind of one way to do this,
[2350.00s -> 2352.00s]  where, you know, I guess we can,
[2352.00s -> 2356.00s]  we are detecting part of this template in this region.
[2356.00s -> 2358.00s]  There's also other ways to do this that, you know,
[2358.00s -> 2360.00s]  you can try and, like, mirror the values here
[2360.00s -> 2363.00s]  or extend them, and so it doesn't have to be
[2363.00s -> 2364.00s]  zero padding, but in practice,
[2364.00s -> 2366.00s]  this is one thing that works reasonably,
[2366.00s -> 2368.00s]  and so, yeah, so there is a little bit of,
[2368.00s -> 2370.00s]  kind of, artifacts at the edge,
[2370.00s -> 2371.00s]  and we sort of just,
[2371.00s -> 2373.00s]  you do your best to deal with it,
[2373.00s -> 2376.00s]  and in practice, this works reasonably.
[2376.00s -> 2379.00s]  I think there was another question.
[2379.00s -> 2380.00s]  Yeah, question?
[2380.00s -> 2381.00s]  Sorry.
[2388.00s -> 2390.00s]  So, if we have non-square images,
[2390.00s -> 2392.00s]  do we ever use a stride that's different
[2392.00s -> 2394.00s]  horizontally and vertically?
[2394.00s -> 2396.00s]  So, I mean, there's nothing stopping you
[2396.00s -> 2397.00s]  from doing that.
[2397.00s -> 2399.00s]  You could, but in practice,
[2399.00s -> 2401.00s]  we just usually take the same stride.
[2401.00s -> 2403.00s]  We usually operate over square regions,
[2403.00s -> 2405.00s]  and we just, yeah, we usually just take
[2405.00s -> 2406.00s]  the same stride everywhere,
[2406.00s -> 2408.00s]  and it's sort of like,
[2408.00s -> 2410.00s]  in a sense, it's a little bit like,
[2410.00s -> 2412.00s]  it's a little bit like the resolution
[2412.00s -> 2414.00s]  at which you're, you know,
[2414.00s -> 2415.00s]  looking at this image,
[2415.00s -> 2417.00s]  and so, usually, there's kind of,
[2417.00s -> 2419.00s]  you might want to match sort of your horizontal
[2419.00s -> 2421.00s]  and vertical resolutions, but,
[2421.00s -> 2423.00s]  yeah, so in practice, you could,
[2423.00s -> 2426.00s]  but really, people don't do that.
[2426.00s -> 2428.00s]  Okay, another question?
[2431.00s -> 2432.00s]  Why, so the question is,
[2432.00s -> 2434.00s]  why do we do zero padding?
[2434.00s -> 2436.00s]  So the way we do zero padding is
[2436.00s -> 2438.00s]  to maintain the same input size
[2438.00s -> 2439.00s]  as we had before, right?
[2439.00s -> 2441.00s]  So we started with seven by seven,
[2441.00s -> 2444.00s]  and if we looked at just starting your filter
[2444.00s -> 2445.00s]  from the upper left-hand corner,
[2445.00s -> 2447.00s]  filling everything in, right,
[2447.00s -> 2449.00s]  then we get a smaller size output,
[2449.00s -> 2453.00s]  but we would like to maintain our full size output.
[2456.00s -> 2460.00s]  Okay, so, yeah, so we saw how padding
[2460.00s -> 2463.00s]  can basically help you maintain
[2463.00s -> 2465.00s]  the size of the output that you want,
[2465.00s -> 2468.00s]  as well as apply your filter
[2468.00s -> 2470.00s]  at these, like, corner regions and edge regions,
[2470.00s -> 2473.00s]  and so, in general, in terms of choosing,
[2473.00s -> 2475.00s]  you know, your stride, your filter,
[2475.00s -> 2477.00s]  your filter size, your stride size, zero padding,
[2477.00s -> 2481.00s]  what's common to see is filters of size three by three,
[2481.00s -> 2483.00s]  five by five, seven by seven,
[2483.00s -> 2485.00s]  these are pretty common filter sizes,
[2485.00s -> 2488.00s]  and so each of these, for three by three,
[2488.00s -> 2490.00s]  you will want to zero pad with one
[2490.00s -> 2493.00s]  in order to maintain the same spatial size.
[2493.00s -> 2495.00s]  If you're going to do five by five,
[2495.00s -> 2496.00s]  you can work out the math,
[2496.00s -> 2497.00s]  but it's going to come out to,
[2497.00s -> 2499.00s]  you want to zero pad by two,
[2499.00s -> 2503.00s]  and then for seven, you want to zero pad by three.
[2504.00s -> 2506.00s]  Okay, and so again, you know,
[2506.00s -> 2510.00s]  the motivation for doing this type of zero padding
[2510.00s -> 2512.00s]  and trying to maintain the input size, right,
[2512.00s -> 2514.00s]  so we kind of alluded to this before,
[2514.00s -> 2519.00s]  but if you have multiple of these layers stacked together,
[2523.00s -> 2526.00s]  so if you have multiple of these layers stacked together,
[2526.00s -> 2528.00s]  you'll see that, you know,
[2528.00s -> 2530.00s]  if we don't do this kind of zero padding
[2530.00s -> 2531.00s]  or any kind of padding,
[2531.00s -> 2533.00s]  we're going to really quickly shrink
[2533.00s -> 2535.00s]  the size of the outputs that we have, right,
[2535.00s -> 2537.00s]  and so this is not something that we want,
[2537.00s -> 2539.00s]  like you can imagine if you have a pretty deep network,
[2539.00s -> 2543.00s]  then very quickly, the size of your activation map
[2543.00s -> 2546.00s]  is going to shrink to something very small,
[2546.00s -> 2549.00s]  and this is bad both because now we're kind of losing out
[2549.00s -> 2550.00s]  on some of this information, right,
[2550.00s -> 2551.00s]  now you're using a very,
[2551.00s -> 2554.00s]  a much smaller number of values
[2554.00s -> 2556.00s]  in order to represent your original image,
[2556.00s -> 2558.00s]  so you don't want that,
[2558.00s -> 2561.00s]  and then at the same time, also,
[2561.00s -> 2563.00s]  as, you know, we talked about earlier,
[2563.00s -> 2566.00s]  you're also kind of losing
[2566.00s -> 2568.00s]  sort of some of this like edge information,
[2568.00s -> 2570.00s]  corner information that each time we're losing out
[2570.00s -> 2572.00s]  and shrinking that further.
[2575.00s -> 2577.00s]  Okay, so let's go through a couple more examples
[2577.00s -> 2580.00s]  of computing some of these sizes.
[2581.00s -> 2583.00s]  So let's say that we have an input volume,
[2583.00s -> 2585.00s]  which is 32 by 32 by three,
[2585.00s -> 2589.00s]  and here we have 10 five by five filters,
[2589.00s -> 2592.00s]  let's use stride one and pad two,
[2592.00s -> 2593.00s]  and so who can tell me
[2593.00s -> 2596.00s]  what's the output volume size of this?
[2598.00s -> 2600.00s]  So you can think about the formulary.
[2600.00s -> 2601.00s]  Sorry, what was it?
[2601.00s -> 2603.00s]  32 by 32 by 10.
[2603.00s -> 2607.00s]  32 by 32 by 10, yes, that's correct,
[2607.00s -> 2609.00s]  and so the way we can see this
[2610.00s -> 2613.00s]  is so we have our input size, f is 32,
[2613.00s -> 2616.00s]  then in this case, we want to augment it
[2616.00s -> 2618.00s]  by the padding that we added onto this,
[2618.00s -> 2621.00s]  so we padded two in each dimension,
[2621.00s -> 2623.00s]  so we're actually going to get total width
[2623.00s -> 2627.00s]  and total height's going to be 32 plus four on each side,
[2627.00s -> 2629.00s]  and then minus our filter size, five,
[2629.00s -> 2631.00s]  divided by one plus one, and we get 32,
[2631.00s -> 2636.00s]  so our output is going to be 32 by 32 for each filter,
[2637.00s -> 2640.00s]  and then we have 10 filters total,
[2640.00s -> 2642.00s]  so we have 10 of these activation maps,
[2642.00s -> 2643.00s]  and our total output volume
[2643.00s -> 2646.00s]  is going to be 32 by 32 by 10.
[2648.00s -> 2650.00s]  Okay, next question.
[2650.00s -> 2654.00s]  So what's the number of parameters in this layer?
[2654.00s -> 2658.00s]  So remember, we have 10 five by five filters.
[2658.00s -> 2662.00s]  I kind of heard something, but it was quiet.
[2673.00s -> 2676.50s]  250, okay, so I heard 250, which is close,
[2678.00s -> 2680.00s]  but remember that we're also, our input volume,
[2680.00s -> 2682.00s]  each of these filters goes through by depth,
[2682.00s -> 2684.00s]  so maybe this wasn't clearly written here
[2684.00s -> 2687.00s]  because each of the filters is five by five spatially,
[2687.00s -> 2690.00s]  but implicitly we also have the depth in here, right?
[2690.00s -> 2693.00s]  It's going to go through the whole volume.
[2693.00s -> 2696.00s]  So I heard, yeah, 750 I heard.
[2696.00s -> 2697.00s]  Almost there, this is kind of a trick question,
[2697.00s -> 2699.00s]  because also remember we usually
[2699.00s -> 2702.00s]  always have a bias term, right?
[2702.00s -> 2706.00s]  So in practice, each filter has five by five by three
[2706.00s -> 2708.00s]  weights plus our one bias term.
[2708.00s -> 2710.00s]  We have 76 parameters per filter,
[2710.00s -> 2712.00s]  and then we have 10 of these total,
[2712.00s -> 2715.00s]  and so there's 760 total parameters.
[2718.00s -> 2720.00s]  Okay, and so here's just a summary
[2720.00s -> 2723.00s]  of the convolutional layer that, you know,
[2723.00s -> 2725.00s]  you guys can read a little bit more carefully later on,
[2725.00s -> 2728.00s]  but we have our input volume of a certain dimension,
[2728.00s -> 2731.00s]  we have all these, we have our filters, right,
[2731.00s -> 2733.00s]  where we have number of filters, the filter size,
[2733.00s -> 2736.00s]  the stride of the size, the amount of zero padding,
[2736.00s -> 2738.00s]  and you basically can use all of these,
[2738.00s -> 2741.00s]  go through the computations that we talked about earlier
[2741.00s -> 2743.00s]  in order to find out what your output volume
[2743.00s -> 2744.00s]  is actually going to be,
[2744.00s -> 2746.00s]  and how many total parameters that you have.
[2749.00s -> 2751.00s]  And so some common settings of this,
[2751.00s -> 2755.00s]  you know, we talked earlier about common filter sizes
[2755.00s -> 2758.00s]  of three by three, five by five,
[2758.00s -> 2760.00s]  stride is usually, you know, one and two
[2760.00s -> 2762.00s]  is pretty common, and then your padding P
[2762.00s -> 2764.00s]  is going to be whatever fits,
[2764.00s -> 2766.00s]  like whatever, you know, will preserve
[2766.00s -> 2770.00s]  your spatial extent is what's common.
[2770.00s -> 2773.00s]  And then the total number of filters K
[2773.00s -> 2776.00s]  usually we use powers of two just to be nice,
[2776.00s -> 2780.00s]  so, you know, 32, 64, 128, and so on, 512,
[2780.00s -> 2782.00s]  these are pretty common numbers that you'll see.
[2784.00s -> 2786.00s]  And just as an aside,
[2786.00s -> 2789.00s]  we can also do a one by one convolution,
[2789.00s -> 2790.00s]  this still makes perfect sense,
[2790.00s -> 2793.00s]  where given a one by one convolution,
[2793.00s -> 2795.00s]  we still slide it over each spatial extent,
[2795.00s -> 2797.00s]  but now, you know, the spatial region
[2797.00s -> 2798.00s]  is not really five by five,
[2798.00s -> 2802.00s]  it's just kind of the trivial case of one by one,
[2802.00s -> 2804.00s]  but we are still, you know,
[2804.00s -> 2806.00s]  having this filter go through the entire depth, right,
[2806.00s -> 2808.00s]  so this is going to be a dot product
[2808.00s -> 2811.00s]  through the entire depth of your input volume.
[2811.00s -> 2813.00s]  And so the output here, right,
[2813.00s -> 2818.00s]  if we have an input volume of 56 by 56 by 64 depth,
[2818.00s -> 2820.00s]  and we're going to do one by one convolution
[2820.00s -> 2823.00s]  with 32 filters, then our output is going to be
[2823.00s -> 2826.00s]  56 by 56 by our number of filters, 32.
[2827.00s -> 2830.00s]  Okay, and so here's an example of a convolutional layer
[2830.00s -> 2833.00s]  in, you know, TORCH, deep learning framework,
[2833.00s -> 2836.00s]  and so you'll see that, you know, last lecture,
[2836.00s -> 2838.00s]  we talked about how you can go into
[2838.00s -> 2839.00s]  these deep learning frameworks,
[2839.00s -> 2842.00s]  you can see these definitions of each layer, right,
[2842.00s -> 2843.00s]  where they had kind of the forward pass
[2843.00s -> 2846.00s]  and the backward pass implemented for each layer,
[2846.00s -> 2848.00s]  and so you'll see, you know, convolution,
[2848.00s -> 2851.00s]  spatial convolution is going to be just one of these,
[2851.00s -> 2853.00s]  and then the arguments that it's going to take
[2853.00s -> 2855.00s]  are going to be all the way through,
[2855.00s -> 2856.00s]  and the arguments that it's going to take
[2856.00s -> 2859.00s]  are going to be all of these design choices of,
[2859.00s -> 2863.00s]  you know, I guess your input and output sizes,
[2863.00s -> 2866.00s]  but also your choices of like your kernel width,
[2866.00s -> 2870.00s]  your kernel size, padding, and these kinds of things.
[2870.00s -> 2873.00s]  Right, and so if we look at another framework, CAFE,
[2873.00s -> 2875.00s]  you'll see something very similar,
[2875.00s -> 2877.00s]  where again, now when you're defining your network,
[2877.00s -> 2881.00s]  you define networks in CAFE using this kind of,
[2881.00s -> 2883.00s]  you know, proto text file where you're,
[2883.00s -> 2887.00s]  specifying each of your design choices for your layer,
[2887.00s -> 2889.00s]  and you can see for a convolutional layer,
[2889.00s -> 2890.00s]  we'll say things like, you know,
[2890.00s -> 2892.00s]  the number of outputs that we have,
[2892.00s -> 2894.00s]  this is going to be the number of filters for CAFE,
[2894.00s -> 2898.00s]  as well as the kernel size and stride and so on.
[2901.00s -> 2904.00s]  Okay, and so, I guess before I go on,
[2904.00s -> 2906.00s]  any questions about convolution,
[2906.00s -> 2909.00s]  how the convolution operation works?
[2910.00s -> 2911.00s]  Yes.
[2914.00s -> 2918.00s]  Yeah, so the question is, what's the intuition
[2934.00s -> 2936.00s]  behind how you choose your stride?
[2936.00s -> 2940.00s]  And so, at one sense, it's kind of the resolution
[2940.00s -> 2941.00s]  at which you slide it on,
[2941.00s -> 2942.00s]  and usually the reason behind this
[2942.00s -> 2945.00s]  is because when we have a larger stride,
[2945.00s -> 2947.00s]  what we end up getting as the output
[2947.00s -> 2949.00s]  is a down-sampled image, right?
[2949.00s -> 2953.00s]  And so, what this down-sampled image lets us have is,
[2953.00s -> 2955.00s]  you know, both, it's a way,
[2955.00s -> 2957.00s]  it's kind of like pooling in a sense,
[2957.00s -> 2958.00s]  but it's just a different
[2958.00s -> 2961.00s]  and sometimes works better way of doing pooling
[2961.00s -> 2963.00s]  is one of the intuitions behind this, right?
[2963.00s -> 2964.00s]  Because you get the same effect
[2964.00s -> 2967.00s]  of down-sampling your image.
[2968.00s -> 2970.00s]  And then also, as you're doing this,
[2970.00s -> 2974.00s]  you're reducing the size of the activation maps
[2975.00s -> 2977.00s]  that you're dealing with at each layer, right?
[2977.00s -> 2979.00s]  And so, this also affects later on
[2979.00s -> 2981.00s]  the total number of parameters that you have
[2981.00s -> 2985.00s]  because, for example, at the end of all your conv layers,
[2985.00s -> 2989.00s]  now you might put fully connected layers on top,
[2989.00s -> 2991.00s]  for example, and now the fully connected layer
[2991.00s -> 2993.00s]  is going to be connected to every value
[2993.00s -> 2996.00s]  of your convolutional output, right?
[2996.00s -> 2998.00s]  And so, a smaller one will give you
[2998.00s -> 3000.00s]  smaller number of parameters,
[3000.00s -> 3002.00s]  and so now you can get into, like,
[3002.00s -> 3004.00s]  basically thinking about trade-offs of, you know,
[3004.00s -> 3007.00s]  number of parameters you have, the size of your model,
[3007.00s -> 3009.00s]  overfitting, things like that.
[3009.00s -> 3011.00s]  And so, yeah, these are kind of some of the things
[3011.00s -> 3015.00s]  that you want to think about with choosing your stride.
[3018.00s -> 3020.00s]  Okay, so now if we look a little bit
[3020.00s -> 3022.00s]  at kind of the, you know,
[3022.00s -> 3025.00s]  brain neuron view of a convolutional layer,
[3025.00s -> 3027.00s]  similar to what we looked at for,
[3027.00s -> 3031.00s]  you know, the neurons in the last lecture,
[3031.00s -> 3035.00s]  so what we have is that at every spatial location,
[3035.00s -> 3037.00s]  we take a dot product between a filter
[3037.00s -> 3039.00s]  and a specific part of the image, right,
[3039.00s -> 3042.00s]  and we get one number out from here.
[3042.00s -> 3043.00s]  And so, this is the same idea
[3043.00s -> 3046.00s]  of doing these types of dot products, right,
[3046.00s -> 3049.00s]  taking your input, weighting it by these Ws,
[3049.00s -> 3052.00s]  right, values of your filter, these weights
[3052.00s -> 3055.00s]  that are the synapses and getting a value out,
[3055.00s -> 3056.00s]  but the main difference here
[3056.00s -> 3059.00s]  is just that now your neuron has local connectivity.
[3059.00s -> 3062.00s]  So instead of being connected to the entire input,
[3062.00s -> 3064.00s]  it's just looking at a local region,
[3064.00s -> 3066.00s]  spatially, of your image.
[3066.00s -> 3068.00s]  And so, this looks at a local region,
[3068.00s -> 3071.00s]  and then now you're going to get kind of,
[3071.00s -> 3075.00s]  you know, this, how much this neuron is being triggered
[3075.00s -> 3077.00s]  at every spatial location in your image.
[3077.00s -> 3079.00s]  Right, so now you preserve the spatial structure
[3079.00s -> 3081.00s]  and you can say, you know,
[3081.00s -> 3084.00s]  be able to reason on top of,
[3084.00s -> 3088.00s]  on top of these kinds of activation maps in later layers.
[3090.00s -> 3093.00s]  And just a little bit of terminology,
[3093.00s -> 3097.00s]  again, for, you know, we have this five by five filter,
[3097.00s -> 3100.00s]  we can also call this a five by five receptive field
[3100.00s -> 3102.00s]  for the neuron, because this is,
[3102.00s -> 3104.00s]  the receptive field is basically the, you know,
[3104.00s -> 3106.00s]  input field that this,
[3106.00s -> 3108.00s]  field of vision that this neuron is receiving, right,
[3108.00s -> 3112.00s]  and so, yeah, so that's just another common term
[3112.00s -> 3113.00s]  that you'll hear for this.
[3113.00s -> 3114.00s]  And then, again, remember,
[3114.00s -> 3115.00s]  each of these five by five filters,
[3115.00s -> 3118.00s]  we're sliding them over to the spatial locations,
[3118.00s -> 3120.00s]  but they're the same set of weights,
[3120.00s -> 3122.00s]  they share the same parameters.
[3125.00s -> 3127.00s]  Okay, and so, you know, as we talked about,
[3127.00s -> 3129.00s]  like, what we're going to get at this output
[3129.00s -> 3131.00s]  is going to be this volume, right,
[3131.00s -> 3132.00s]  where spatially we have, you know,
[3132.00s -> 3133.00s]  let's say 20 by 28,
[3133.00s -> 3136.00s]  and then our number of filters is the depth.
[3136.00s -> 3138.00s]  And so, for example, with five filters,
[3138.00s -> 3140.00s]  what we're going to get out is this 3D grid
[3140.00s -> 3143.00s]  that's 28 by 28 by five,
[3143.00s -> 3146.00s]  and so if you look at the filters across,
[3146.00s -> 3149.00s]  in, you know, one spatial location
[3149.00s -> 3150.00s]  of the activation volume
[3150.00s -> 3153.00s]  and going through depth, these five neurons,
[3153.00s -> 3155.00s]  all of these neurons,
[3155.00s -> 3157.00s]  basically the way you can interpret this
[3157.00s -> 3159.00s]  is they're all looking at the same region
[3159.00s -> 3160.00s]  in the input volume,
[3160.00s -> 3162.00s]  but they're just looking for different things, right,
[3162.00s -> 3163.00s]  so they're different filters
[3163.00s -> 3167.00s]  applied to the same spatial location in the image.
[3168.00s -> 3170.00s]  And so just a reminder again,
[3170.00s -> 3173.00s]  kind of comparing with the fully connected layer
[3173.00s -> 3175.00s]  that we talked about earlier,
[3175.00s -> 3176.00s]  in that case, right,
[3176.00s -> 3177.00s]  if we look at each of the neurons
[3177.00s -> 3180.00s]  in our activation or output,
[3180.00s -> 3182.00s]  each of the neurons was connected
[3182.00s -> 3183.00s]  to the entire stretched out input,
[3183.00s -> 3186.00s]  so it looked at the entire full input volume
[3186.00s -> 3187.00s]  compared to, you know,
[3187.00s -> 3189.00s]  now where each one just looks
[3189.00s -> 3192.00s]  at this local spatial region.
[3192.00s -> 3193.00s]  Question.
[3198.00s -> 3200.00s]  Okay, so the question is,
[3200.00s -> 3201.00s]  within a given layer,
[3201.00s -> 3204.00s]  are the filters completely symmetric?
[3204.00s -> 3208.00s]  So what do you mean by symmetric exactly, I guess?
[3219.00s -> 3220.00s]  Right, so okay,
[3220.00s -> 3222.00s]  so the filters, you know,
[3222.00s -> 3223.00s]  are the filters doing,
[3223.00s -> 3224.00s]  they're doing the same dimension
[3224.00s -> 3225.00s]  the same calculation?
[3225.00s -> 3226.00s]  Yes.
[3226.00s -> 3228.00s]  Okay, so is there anything different
[3228.00s -> 3231.00s]  other than they have the same parameter values?
[3231.00s -> 3233.00s]  No, so you're exactly right, right,
[3233.00s -> 3234.00s]  we're just taking a filter
[3234.00s -> 3236.00s]  with a given set of, you know,
[3236.00s -> 3238.00s]  five by five by three parameter values
[3238.00s -> 3241.00s]  and we just slide this in exactly the same way
[3241.00s -> 3243.00s]  over the entire input volume
[3243.00s -> 3246.00s]  to get an activation map.
[3247.00s -> 3248.00s]  Okay, so, you know,
[3248.00s -> 3250.00s]  we've gone into a lot of detail
[3250.00s -> 3253.00s]  on what these convolutional layers look like
[3253.00s -> 3255.00s]  and so now I'm just going to go briefly through
[3255.00s -> 3257.00s]  the other layers that we have
[3257.00s -> 3259.00s]  that form this, you know,
[3259.00s -> 3261.00s]  entire convolutional network, right?
[3261.00s -> 3262.00s]  So remember again,
[3262.00s -> 3263.00s]  we have convolutional layers
[3263.00s -> 3265.00s]  interspersed with pooling layers
[3265.00s -> 3266.00s]  once a while,
[3266.00s -> 3269.00s]  as well as these non-linearities.
[3269.00s -> 3271.00s]  Okay, so what the pooling layers do
[3271.00s -> 3273.00s]  is that they make the representations
[3273.00s -> 3275.00s]  smaller and more manageable, right?
[3275.00s -> 3277.00s]  So we talked about this earlier
[3277.00s -> 3278.00s]  when someone asked a question
[3278.00s -> 3280.00s]  of why we would want to make
[3280.00s -> 3283.00s]  the representation smaller
[3283.00s -> 3284.00s]  and so this is, again,
[3284.00s -> 3286.00s]  for it to have, you know,
[3286.00s -> 3287.00s]  fewer,
[3287.00s -> 3288.00s]  it affects the number of parameters
[3288.00s -> 3290.00s]  that you have at the end,
[3290.00s -> 3292.00s]  as well as the number of parameters
[3292.00s -> 3294.00s]  that you have at the end.
[3294.00s -> 3295.00s]  And so this is, again,
[3295.00s -> 3296.00s]  for it to have, you know,
[3296.00s -> 3297.00s]  fewer,
[3297.00s -> 3298.00s]  it affects the number of parameters
[3298.00s -> 3301.00s]  as well as basically does some,
[3301.00s -> 3302.00s]  you know,
[3302.00s -> 3304.00s]  invariance over a given region.
[3304.00s -> 3306.00s]  And so what the pooling layer does
[3306.00s -> 3309.00s]  is it does exactly just
[3309.00s -> 3310.00s]  down samples
[3310.00s -> 3313.00s]  and it takes your input volume,
[3313.00s -> 3314.00s]  so for example,
[3314.00s -> 3316.00s]  224 by 224 by 64
[3316.00s -> 3318.00s]  and spatially down samples this.
[3318.00s -> 3319.00s]  So in the end,
[3319.00s -> 3321.00s]  you'll get out 112 by 112
[3321.00s -> 3322.00s]  and it's important to note,
[3322.00s -> 3325.00s]  this doesn't do anything in the depth, right?
[3325.00s -> 3327.00s]  We're only pooling spatially.
[3327.00s -> 3330.00s]  The number of your input depth
[3330.00s -> 3333.00s]  is going to be the same as your output depth.
[3333.00s -> 3334.00s]  And so for example,
[3334.00s -> 3337.00s]  a common way to do this is max pooling.
[3337.00s -> 3338.00s]  So in this case,
[3338.00s -> 3341.00s]  we are pooling layer also has a filter size
[3341.00s -> 3342.00s]  and this is,
[3342.00s -> 3344.00s]  this filter size is going to be the region
[3344.00s -> 3346.00s]  at which we pool over, right?
[3346.00s -> 3347.00s]  So in this case,
[3347.00s -> 3348.00s]  if we have two by two filters,
[3348.00s -> 3350.00s]  we're going to slide this,
[3350.00s -> 3351.00s]  and so here,
[3351.00s -> 3353.00s]  we also have stride two in this case.
[3353.00s -> 3354.00s]  So we're going to take this filter
[3354.00s -> 3356.00s]  and we're going to
[3356.00s -> 3359.00s]  slide it along our input volume
[3359.00s -> 3361.00s]  in exactly the same way as we did for convolution.
[3361.00s -> 3362.00s]  But here,
[3362.00s -> 3363.00s]  instead of doing these dot products,
[3363.00s -> 3365.00s]  we just take the maximum value
[3365.00s -> 3368.00s]  of the input volume in that region, right?
[3368.00s -> 3369.00s]  So here,
[3369.00s -> 3371.00s]  if we look at the red values,
[3371.00s -> 3373.00s]  the value of that will be six is the largest.
[3373.00s -> 3374.00s]  If we look at the greens,
[3374.00s -> 3375.00s]  it's going to give an eight
[3375.00s -> 3378.00s]  and then we have a three and a four.
[3383.00s -> 3384.00s]  Yes, question.
[3387.00s -> 3388.00s]  Yeah, so the question is,
[3388.00s -> 3389.00s]  is it typical to set up the stride
[3389.00s -> 3392.00s]  so that there isn't an overlap?
[3392.00s -> 3393.00s]  And yeah,
[3393.00s -> 3394.00s]  so for the pooling layers,
[3394.00s -> 3395.00s]  it is,
[3395.00s -> 3396.00s]  I think the more common thing to do
[3396.00s -> 3398.00s]  is to have them
[3398.00s -> 3399.00s]  not have any overlap
[3399.00s -> 3402.00s]  and I guess the way you can
[3402.00s -> 3404.00s]  think about this is,
[3404.00s -> 3405.00s]  you know,
[3405.00s -> 3406.00s]  basically,
[3406.00s -> 3407.00s]  we just want to down sample
[3407.00s -> 3408.00s]  and so it makes sense
[3408.00s -> 3409.00s]  to kind of look at this region
[3409.00s -> 3410.00s]  and just get one value
[3410.00s -> 3411.00s]  to represent this region
[3411.00s -> 3413.00s]  and then just look at the next region
[3413.00s -> 3414.00s]  and so on.
[3415.00s -> 3416.00s]  Yeah, question.
[3422.00s -> 3423.00s]  So the question is,
[3423.00s -> 3424.00s]  why is max pooling better
[3424.00s -> 3425.00s]  than just taking the,
[3425.00s -> 3427.00s]  doing something like average pooling?
[3427.00s -> 3428.00s]  Yeah, so that's a good point.
[3428.00s -> 3429.00s]  Like, average pooling
[3429.00s -> 3431.00s]  is also something that you can do
[3431.00s -> 3432.00s]  and intuition behind
[3432.00s -> 3434.00s]  why max pooling is commonly used
[3434.00s -> 3437.00s]  is that you can have this interpretation of,
[3437.00s -> 3438.00s]  you know,
[3438.00s -> 3439.00s]  if this is,
[3439.00s -> 3441.00s]  these are activations of my neurons, right?
[3441.00s -> 3443.00s]  And so each value is kind of
[3443.00s -> 3446.00s]  how much this neuron fired in this location,
[3446.00s -> 3448.00s]  how much this filter fired in this location
[3448.00s -> 3450.00s]  and so you can think of max pooling
[3450.00s -> 3451.00s]  as saying,
[3451.00s -> 3452.00s]  you know,
[3452.00s -> 3454.00s]  giving a signal of how much
[3454.00s -> 3456.00s]  did this filter fire
[3456.00s -> 3458.00s]  at any location in this image, right?
[3458.00s -> 3460.00s]  And if we're thinking about detecting,
[3460.00s -> 3461.00s]  you know,
[3461.00s -> 3463.00s]  doing recognition,
[3463.00s -> 3465.00s]  this might make some intuitive sense
[3465.00s -> 3466.00s]  where you're saying,
[3466.00s -> 3467.00s]  well, you know,
[3467.00s -> 3468.00s]  whether a light
[3468.00s -> 3470.00s]  or whether some aspect of your image
[3470.00s -> 3471.00s]  that you're looking for,
[3471.00s -> 3473.00s]  whether it happens anywhere in this region,
[3473.00s -> 3474.00s]  we want to fire it
[3474.00s -> 3476.00s]  with a high value.
[3476.00s -> 3477.00s]  Questions?
[3485.00s -> 3486.00s]  So the question is,
[3486.00s -> 3487.00s]  since pooling and stride
[3487.00s -> 3490.00s]  both have the same effect of downsampling,
[3490.00s -> 3491.00s]  can you just use stride
[3491.00s -> 3493.00s]  instead of pooling and so on?
[3493.00s -> 3494.00s]  Yeah, and so,
[3494.00s -> 3495.00s]  in practice,
[3495.00s -> 3497.00s]  like, I think looking at more recent
[3497.00s -> 3498.00s]  neural network architectures,
[3498.00s -> 3500.00s]  people have begun to use
[3501.00s -> 3502.00s]  stride more
[3503.00s -> 3505.00s]  in order to do the downsampling
[3505.00s -> 3507.00s]  instead of just pooling
[3507.00s -> 3510.00s]  and I think this gets into things like,
[3510.00s -> 3511.00s]  you know,
[3511.00s -> 3512.00s]  also like fractional strides
[3512.00s -> 3513.00s]  and things that you can do,
[3513.00s -> 3514.00s]  but in practice,
[3514.00s -> 3516.00s]  this, in a sense,
[3516.00s -> 3517.00s]  maybe, you know,
[3517.00s -> 3519.00s]  has been a little bit better way
[3519.00s -> 3521.00s]  to get better results using that.
[3521.00s -> 3522.00s]  So yeah,
[3522.00s -> 3524.00s]  so I think using stride is definitely,
[3524.00s -> 3525.00s]  you can do it
[3525.00s -> 3527.00s]  and people are doing it.
[3528.00s -> 3529.00s]  Okay, so,
[3529.00s -> 3530.00s]  let's see,
[3530.00s -> 3531.00s]  where were we?
[3531.00s -> 3532.00s]  Okay, so,
[3533.00s -> 3535.00s]  yeah, so with these pooling layers,
[3535.00s -> 3536.00s]  so again,
[3536.00s -> 3537.00s]  there's, right,
[3537.00s -> 3539.00s]  some design choices that you make.
[3539.00s -> 3540.00s]  You take this input volume
[3540.00s -> 3542.00s]  of w by h by d
[3543.00s -> 3544.00s]  and then you're going to
[3544.00s -> 3546.00s]  set your hyperparameters
[3546.00s -> 3549.00s]  for design choices of your filter size
[3549.00s -> 3550.00s]  with the spatial extent
[3550.00s -> 3551.00s]  over which you're pooling
[3551.00s -> 3552.00s]  as well as your stride
[3552.00s -> 3553.00s]  and then you can again
[3553.00s -> 3555.00s]  compute your output volume
[3556.00s -> 3558.00s]  using the same equation
[3558.00s -> 3559.00s]  that you used earlier for convolution.
[3559.00s -> 3561.00s]  It still applies here, right?
[3561.00s -> 3564.00s]  So we still have our w total extent
[3564.00s -> 3565.00s]  minus filter size
[3565.00s -> 3567.00s]  divided by stride plus one.
[3570.00s -> 3571.00s]  Okay,
[3571.00s -> 3573.00s]  and so just one other thing to note,
[3573.00s -> 3574.00s]  it's also,
[3574.00s -> 3575.00s]  typically,
[3575.00s -> 3577.00s]  people don't really use zero padding
[3577.00s -> 3578.00s]  for the pooling layers
[3578.00s -> 3579.00s]  because you're just trying to do
[3579.00s -> 3581.00s]  a direct downsampling, right?
[3581.00s -> 3582.00s]  So there isn't this problem
[3582.00s -> 3584.00s]  of like applying a filter at the quarter
[3584.00s -> 3585.00s]  and having some part of the filter
[3585.00s -> 3586.00s]  go off your input volume
[3586.00s -> 3587.00s]  and so for pooling,
[3587.00s -> 3589.00s]  we don't usually have to worry about this
[3589.00s -> 3590.00s]  and we just,
[3590.00s -> 3591.00s]  yeah,
[3591.00s -> 3592.00s]  directly downsample
[3592.00s -> 3594.00s]  and so some common settings
[3594.00s -> 3596.00s]  for the pooling layer
[3596.00s -> 3597.00s]  is the filter size
[3597.00s -> 3598.00s]  of two by two
[3598.00s -> 3599.00s]  or three by three
[3599.00s -> 3600.00s]  strides,
[3600.00s -> 3601.00s]  two by two,
[3601.00s -> 3602.00s]  you know,
[3602.00s -> 3603.00s]  you can have,
[3603.00s -> 3604.00s]  also,
[3604.00s -> 3605.00s]  you can still have pooling
[3605.00s -> 3606.00s]  of two by two
[3606.00s -> 3607.00s]  even with a stride of,
[3607.00s -> 3608.00s]  even with a filter size
[3608.00s -> 3609.00s]  of three by three.
[3609.00s -> 3610.00s]  I think someone asked that earlier
[3610.00s -> 3611.00s]  but in practice,
[3611.00s -> 3612.00s]  it's pretty common
[3612.00s -> 3613.00s]  just to have
[3614.00s -> 3615.00s]  yeah,
[3615.00s -> 3616.00s]  two by two.
[3618.00s -> 3619.00s]  Okay,
[3619.00s -> 3620.00s]  so now we've talked about
[3620.00s -> 3621.00s]  these convolutional layers.
[3621.00s -> 3623.00s]  The ReLU layers were the same
[3623.00s -> 3624.00s]  as what we had before
[3624.00s -> 3625.00s]  with the,
[3626.00s -> 3627.00s]  with the,
[3627.00s -> 3628.00s]  you know,
[3628.00s -> 3629.00s]  just the base neural network
[3629.00s -> 3631.00s]  that we talked about last lecture.
[3631.00s -> 3632.00s]  So we intersperse these
[3632.00s -> 3634.00s]  and then we have a pooling layer
[3634.00s -> 3635.00s]  every,
[3635.00s -> 3636.00s]  you know,
[3636.00s -> 3637.00s]  once in a while
[3637.00s -> 3638.00s]  when we feel like downsampling, right?
[3638.00s -> 3639.00s]  And then the last thing
[3639.00s -> 3641.00s]  is that at the end,
[3641.00s -> 3642.00s]  we want to have
[3642.00s -> 3644.00s]  a fully connected layer.
[3644.00s -> 3645.00s]  And so this will be just
[3645.00s -> 3646.00s]  exactly the same
[3646.00s -> 3648.00s]  as the fully connected layers
[3648.00s -> 3649.00s]  that you've seen before.
[3649.00s -> 3650.00s]  So in this case now,
[3650.00s -> 3651.00s]  what we do
[3651.00s -> 3653.00s]  is we take the convolutional,
[3653.00s -> 3655.00s]  convolutional network output.
[3655.00s -> 3656.00s]  We,
[3656.00s -> 3657.00s]  the last layer,
[3657.00s -> 3658.00s]  we have some volume
[3658.00s -> 3659.00s]  so we're going to have
[3659.00s -> 3660.00s]  width by height
[3660.00s -> 3661.00s]  by some depth
[3661.00s -> 3662.00s]  and we just take all of these
[3662.00s -> 3663.00s]  and we essentially
[3663.00s -> 3664.00s]  just stretch these out, right?
[3664.00s -> 3665.00s]  And so now we're going to get
[3665.00s -> 3666.00s]  the same kind of,
[3666.00s -> 3667.00s]  you know,
[3667.00s -> 3668.00s]  basically 1D input
[3668.00s -> 3669.00s]  that we're used to
[3669.00s -> 3670.00s]  for a
[3670.00s -> 3671.00s]  vanilla neural network.
[3671.00s -> 3672.00s]  And then we're going to
[3672.00s -> 3675.00s]  apply this fully connected
[3675.00s -> 3676.00s]  layer on top.
[3676.00s -> 3677.00s]  So now we're going to have
[3677.00s -> 3679.00s]  connections to every one of these,
[3679.00s -> 3680.00s]  of these
[3680.00s -> 3682.00s]  convolutional map outputs.
[3682.00s -> 3683.00s]  And so what you can think of this
[3683.00s -> 3684.00s]  is basically
[3684.00s -> 3685.00s]  now instead of preserving,
[3685.00s -> 3686.00s]  you know,
[3686.00s -> 3687.00s]  before we were preserving
[3687.00s -> 3688.00s]  spatial structure, right?
[3688.00s -> 3689.00s]  And so,
[3689.00s -> 3690.00s]  but at the last layer at the end,
[3690.00s -> 3691.00s]  we want to aggregate
[3691.00s -> 3692.00s]  all of this together
[3692.00s -> 3693.00s]  and we want to reason
[3693.00s -> 3694.00s]  basically on top of
[3694.00s -> 3696.00s]  all of this as we had before.
[3696.00s -> 3697.00s]  And so
[3697.00s -> 3698.00s]  what you get from that
[3698.00s -> 3699.00s]  is just our
[3699.00s -> 3700.00s]  you know,
[3700.00s -> 3701.00s]  score outputs
[3701.00s -> 3702.00s]  as we had earlier.
[3705.00s -> 3706.00s]  Okay.
[3720.00s -> 3721.00s]  Okay, so the question is
[3721.00s -> 3722.00s]  what are the 16 pixels
[3722.00s -> 3723.00s]  that are on the far right?
[3723.00s -> 3724.00s]  Do you mean the
[3726.00s -> 3728.00s]  Oh, each column.
[3730.00s -> 3731.00s]  The green ones
[3731.00s -> 3732.00s]  or the black ones?
[3732.00s -> 3735.00s]  The one with pool.
[3735.00s -> 3736.00s]  Oh, okay.
[3736.00s -> 3737.00s]  Yeah, so the question is
[3737.00s -> 3739.00s]  how do we interpret this column,
[3739.00s -> 3741.00s]  right, for example at pool?
[3741.00s -> 3742.00s]  And so
[3742.00s -> 3744.00s]  what we're showing here is
[3744.00s -> 3745.00s]  each of these columns
[3745.00s -> 3747.00s]  is the output
[3747.00s -> 3749.00s]  activation maps, right?
[3749.00s -> 3750.00s]  The output from one of these layers.
[3750.00s -> 3751.00s]  And so
[3751.00s -> 3752.00s]  starting from the beginning,
[3752.00s -> 3753.00s]  the beginning,
[3753.00s -> 3755.00s]  we have our car.
[3755.00s -> 3756.00s]  After the convolutional layer,
[3756.00s -> 3757.00s]  we now have these
[3757.00s -> 3758.00s]  activation maps
[3758.00s -> 3759.00s]  of each of the filters
[3759.00s -> 3760.00s]  slid off spatially
[3760.00s -> 3761.00s]  over the input image.
[3761.00s -> 3763.00s]  Then we pass that through a ReLU
[3763.00s -> 3764.00s]  so you can see
[3764.00s -> 3766.00s]  the values coming out from there
[3766.00s -> 3768.00s]  and then going all the way over.
[3768.00s -> 3769.00s]  And so what you get
[3769.00s -> 3770.00s]  for the pooling layer
[3770.00s -> 3771.00s]  is that
[3771.00s -> 3772.00s]  it's really just
[3772.00s -> 3773.00s]  taking the
[3775.00s -> 3777.00s]  output of the ReLU layer
[3777.00s -> 3778.00s]  that came just before it
[3778.00s -> 3779.00s]  and then it's pooling it.
[3779.00s -> 3781.00s]  So it's going to down sample it,
[3781.00s -> 3782.00s]  right, and then
[3782.00s -> 3783.00s]  it's going to take
[3783.00s -> 3784.00s]  the max value
[3784.00s -> 3785.00s]  in each filter location.
[3785.00s -> 3786.00s]  And so now
[3786.00s -> 3787.00s]  if you look at this
[3787.00s -> 3788.00s]  pool layer output like,
[3788.00s -> 3789.00s]  for example,
[3789.00s -> 3790.00s]  the last one that you were mentioning,
[3790.00s -> 3791.00s]  it looks the same
[3791.00s -> 3793.00s]  as this ReLU output
[3793.00s -> 3794.00s]  except
[3794.00s -> 3795.00s]  that it's
[3795.00s -> 3796.00s]  down sampled
[3796.00s -> 3797.00s]  and that it has
[3797.00s -> 3798.00s]  this kind of
[3798.00s -> 3799.00s]  max value
[3799.00s -> 3800.00s]  at every spatial location
[3800.00s -> 3801.00s]  and so that's
[3801.00s -> 3802.00s]  the minor difference
[3802.00s -> 3803.00s]  that you'll see
[3803.00s -> 3812.00s]  between those two.
[3812.00s -> 3813.00s]  So the question is now like,
[3813.00s -> 3814.00s]  this looks like
[3814.00s -> 3815.00s]  just a very small
[3815.00s -> 3816.00s]  amount of information, right?
[3816.00s -> 3817.00s]  So how can it know
[3817.00s -> 3818.00s]  to classify it from here?
[3818.00s -> 3819.00s]  And so
[3819.00s -> 3820.00s]  the way that you
[3820.00s -> 3821.00s]  should think about this
[3821.00s -> 3822.00s]  is that
[3822.00s -> 3823.00s]  each of these
[3823.00s -> 3824.00s]  each of these values
[3824.00s -> 3826.00s]  inside one of these pool outputs
[3826.00s -> 3827.00s]  is actually
[3827.00s -> 3828.00s]  it's the
[3828.00s -> 3829.00s]  accumulation of
[3829.00s -> 3830.00s]  all the processing
[3830.00s -> 3831.00s]  that you've done
[3831.00s -> 3832.00s]  throughout this entire network, right?
[3832.00s -> 3833.00s]  So
[3833.00s -> 3834.00s]  it's at the very top
[3834.00s -> 3835.00s]  of your hierarchy
[3835.00s -> 3836.00s]  and so
[3836.00s -> 3837.00s]  each actually represents
[3837.00s -> 3838.00s]  kind of a
[3838.00s -> 3839.00s]  higher level concept.
[3839.00s -> 3840.00s]  So we saw before
[3840.00s -> 3841.00s]  you know,
[3841.00s -> 3842.00s]  for example,
[3842.00s -> 3843.00s]  Hubel and Wiesel
[3843.00s -> 3844.00s]  and building up
[3844.00s -> 3845.00s]  these hierarchical filters
[3845.00s -> 3846.00s]  where at the
[3846.00s -> 3847.00s]  bottom level
[3847.00s -> 3848.00s]  we're looking for
[3848.00s -> 3849.00s]  edges, right?
[3849.00s -> 3850.00s]  Or things like
[3850.00s -> 3851.00s]  very simple structures
[3851.00s -> 3852.00s]  like edges
[3852.00s -> 3853.00s]  and so
[3853.00s -> 3855.00s]  after your convolutional layer
[3855.00s -> 3856.00s]  these
[3856.00s -> 3857.00s]  the outputs that you see here
[3857.00s -> 3858.00s]  in this first column
[3858.00s -> 3859.00s]  is basically
[3859.00s -> 3860.00s]  how much do specific
[3860.00s -> 3862.00s]  for example edges
[3862.00s -> 3863.00s]  fire at different locations
[3863.00s -> 3864.00s]  in the image?
[3864.00s -> 3865.00s]  But then as you go through
[3865.00s -> 3866.00s]  you're going to get
[3866.00s -> 3867.00s]  more complex
[3867.00s -> 3868.00s]  it's looking for
[3868.00s -> 3869.00s]  more complex things, right?
[3869.00s -> 3870.00s]  And so
[3870.00s -> 3871.00s]  the next convolutional layer
[3871.00s -> 3872.00s]  is going to fire
[3872.00s -> 3873.00s]  at how much you know
[3873.00s -> 3874.00s]  let's say
[3874.00s -> 3875.00s]  corners
[3875.00s -> 3876.00s]  show up
[3876.00s -> 3877.00s]  in the image, right?
[3877.00s -> 3878.00s]  Because it's reasoning
[3878.00s -> 3879.00s]  its input
[3879.00s -> 3880.00s]  is not the original image
[3880.00s -> 3881.00s]  its input is
[3881.00s -> 3882.00s]  the output
[3882.00s -> 3883.00s]  it's already
[3883.00s -> 3884.00s]  you know
[3884.00s -> 3885.00s]  the edge maps, right?
[3885.00s -> 3886.00s]  So it's reasoning
[3886.00s -> 3887.00s]  on top of edge maps
[3887.00s -> 3888.00s]  and so that allows it
[3888.00s -> 3889.00s]  to get
[3889.00s -> 3890.00s]  more complex
[3890.00s -> 3891.00s]  detect more complex things
[3891.00s -> 3892.00s]  and so
[3892.00s -> 3893.00s]  by the time you get
[3893.00s -> 3894.00s]  all the way up
[3894.00s -> 3895.00s]  to this last pooling layer
[3895.00s -> 3896.00s]  each value is representing
[3896.00s -> 3897.00s]  how much a relatively
[3897.00s -> 3898.00s]  complex
[3898.00s -> 3899.00s]  you know sort of
[3899.00s -> 3900.00s]  template is
[3900.00s -> 3901.00s]  firing, right?
[3901.00s -> 3902.00s]  And so
[3902.00s -> 3903.00s]  because of that
[3903.00s -> 3904.00s]  you just have
[3904.00s -> 3905.00s]  a fully connected layer
[3905.00s -> 3906.00s]  you're just aggregating
[3906.00s -> 3907.00s]  all of this information
[3907.00s -> 3908.00s]  together
[3908.00s -> 3909.00s]  to get
[3909.00s -> 3910.00s]  you know
[3910.00s -> 3911.00s]  a score for
[3911.00s -> 3912.00s]  your class.
[3912.00s -> 3913.00s]  So each of these values
[3913.00s -> 3914.00s]  is
[3914.00s -> 3915.00s]  how much
[3915.00s -> 3916.00s]  a pretty
[3916.00s -> 3917.00s]  complicated
[3917.00s -> 3918.00s]  complex concept
[3918.00s -> 3919.00s]  is
[3919.00s -> 3920.00s]  is firing.
[3920.00s -> 3921.00s]  A question.
[3925.00s -> 3926.00s]  So the question is
[3926.00s -> 3927.00s]  when do you know
[3927.00s -> 3928.00s]  you've done enough
[3928.00s -> 3929.00s]  pooling to do
[3929.00s -> 3930.00s]  the classification?
[3930.00s -> 3931.00s]  And the answer is
[3931.00s -> 3932.00s]  you just try
[3933.00s -> 3934.00s]  so in practice
[3934.00s -> 3935.00s]  you know these are
[3935.00s -> 3936.00s]  these are all
[3936.00s -> 3937.00s]  design choices
[3937.00s -> 3938.00s]  and you know
[3938.00s -> 3939.00s]  you can think about this
[3939.00s -> 3940.00s]  a little bit intuitively, right?
[3940.00s -> 3941.00s]  Like you want to pool
[3941.00s -> 3942.00s]  but if you like
[3942.00s -> 3943.00s]  if you pool
[3943.00s -> 3944.00s]  you know too much
[3944.00s -> 3945.00s]  you're going to have
[3945.00s -> 3946.00s]  very few values
[3946.00s -> 3947.00s]  like representing
[3947.00s -> 3948.00s]  your entire image
[3948.00s -> 3949.00s]  and so on so
[3949.00s -> 3950.00s]  so it's just kind of
[3950.00s -> 3951.00s]  a trade off
[3951.00s -> 3952.00s]  something reasonable
[3952.00s -> 3953.00s]  versus people have tried
[3953.00s -> 3954.00s]  a lot of
[3954.00s -> 3955.00s]  different configurations
[3955.00s -> 3956.00s]  so you'll
[3956.00s -> 3957.00s]  you'll probably
[3957.00s -> 3958.00s]  cross validate, right?
[3958.00s -> 3959.00s]  And try over
[3959.00s -> 3960.00s]  different
[3960.00s -> 3961.00s]  pooling sizes
[3961.00s -> 3962.00s]  and see what works best
[3962.00s -> 3963.00s]  for your problem
[3963.00s -> 3964.00s]  because
[3964.00s -> 3965.00s]  yeah like
[3965.00s -> 3966.00s]  every
[3966.00s -> 3967.00s]  every problem
[3967.00s -> 3968.00s]  with different data
[3968.00s -> 3969.00s]  is going to
[3969.00s -> 3970.00s]  you know
[3970.00s -> 3971.00s]  different set of
[3971.00s -> 3972.00s]  these sorts of
[3972.00s -> 3973.00s]  hyperparameters
[3973.00s -> 3974.00s]  might work best.
[3974.00s -> 3975.00s]  Okay so
[3975.00s -> 3976.00s]  last thing
[3976.00s -> 3977.00s]  just wanted to
[3977.00s -> 3978.00s]  point you guys
[3978.00s -> 3979.00s]  to this demo
[3979.00s -> 3980.00s]  of training
[3980.00s -> 3981.00s]  a
[3981.00s -> 3982.00s]  a com net
[3982.00s -> 3983.00s]  which was
[3983.00s -> 3984.00s]  created by
[3984.00s -> 3985.00s]  Andre Karpathy
[3985.00s -> 3986.00s]  the originator
[3986.00s -> 3987.00s]  of this class
[3987.00s -> 3988.00s]  and so he
[3988.00s -> 3989.00s]  wrote up this
[3989.00s -> 3990.00s]  this demo
[3990.00s -> 3991.00s]  where you can
[3991.00s -> 3992.00s]  basically train
[3992.00s -> 3993.00s]  a com net
[3993.00s -> 3994.00s]  on CIFAR-10
[3994.00s -> 3995.00s]  the data set
[3995.00s -> 3996.00s]  that we've seen before
[3996.00s -> 3997.00s]  right with 10 classes
[3997.00s -> 3998.00s]  and
[3998.00s -> 3999.00s]  what's nice about this demo
[3999.00s -> 4000.00s]  is
[4000.00s -> 4001.00s]  you can
[4001.00s -> 4002.00s]  it basically plots for you
[4002.00s -> 4003.00s]  what
[4003.00s -> 4004.00s]  what each of these filters
[4004.00s -> 4005.00s]  look like
[4005.00s -> 4006.00s]  what the activation maps
[4006.00s -> 4007.00s]  look like
[4007.00s -> 4008.00s]  so some of the
[4008.00s -> 4009.00s]  images I showed earlier
[4009.00s -> 4010.00s]  were taken from this demo
[4010.00s -> 4011.00s]  and so you can go
[4011.00s -> 4012.00s]  try it out
[4012.00s -> 4013.00s]  play around with it
[4013.00s -> 4014.00s]  and
[4014.00s -> 4015.00s]  you know
[4015.00s -> 4016.00s]  just go through
[4016.00s -> 4017.00s]  and try and get a sense
[4017.00s -> 4018.00s]  for
[4018.00s -> 4019.00s]  what these activation maps
[4019.00s -> 4020.00s]  actually
[4020.00s -> 4021.00s]  you know
[4021.00s -> 4022.00s]  the first layer
[4022.00s -> 4023.00s]  activation maps are
[4023.00s -> 4024.00s]  you can interpret them right
[4024.00s -> 4025.00s]  because they're operating
[4025.00s -> 4026.00s]  directly on the input image
[4026.00s -> 4027.00s]  so you can see
[4027.00s -> 4028.00s]  what these templates mean
[4028.00s -> 4029.00s]  as you get to
[4029.00s -> 4030.00s]  higher level layers
[4030.00s -> 4031.00s]  it starts getting really hard
[4031.00s -> 4032.00s]  like how do you actually
[4032.00s -> 4033.00s]  interpret
[4033.00s -> 4034.00s]  what do these mean
[4034.00s -> 4035.00s]  so
[4035.00s -> 4036.00s]  for the most part
[4036.00s -> 4037.00s]  it's just
[4037.00s -> 4038.00s]  hard to interpret
[4038.00s -> 4039.00s]  so you shouldn't
[4039.00s -> 4040.00s]  you know
[4040.00s -> 4041.00s]  don't worry if you
[4041.00s -> 4042.00s]  can't really make sense
[4042.00s -> 4043.00s]  of what's going on
[4043.00s -> 4044.00s]  but it's still nice
[4044.00s -> 4045.00s]  just to see
[4045.00s -> 4046.00s]  the entire flow
[4046.00s -> 4047.00s]  and
[4047.00s -> 4048.00s]  what outputs
[4048.00s -> 4049.00s]  so today we talked about
[4049.00s -> 4051.00s]  how convolutional neural networks work
[4051.00s -> 4052.00s]  how they're basically
[4052.00s -> 4053.00s]  stacks of these
[4053.00s -> 4054.00s]  convolutional
[4054.00s -> 4055.00s]  and pooling layers
[4055.00s -> 4056.00s]  followed by
[4056.00s -> 4057.00s]  fully connected layers
[4057.00s -> 4058.00s]  at the end
[4059.00s -> 4060.00s]  there's been a trend
[4060.00s -> 4061.00s]  towards having
[4061.00s -> 4062.00s]  smaller filters
[4062.00s -> 4063.00s]  and
[4063.00s -> 4064.00s]  deeper architectures
[4064.00s -> 4065.00s]  so we'll talk more about
[4065.00s -> 4066.00s]  case studies
[4066.00s -> 4067.00s]  for some of these
[4067.00s -> 4068.00s]  later on
[4068.00s -> 4069.00s]  there's also been a trend
[4069.00s -> 4070.00s]  towards
[4070.00s -> 4071.00s]  getting rid of these
[4071.00s -> 4072.00s]  pooling
[4072.00s -> 4073.00s]  and fully connected layers
[4073.00s -> 4074.00s]  entirely
[4074.00s -> 4075.00s]  so just keeping these
[4075.00s -> 4076.00s]  just having
[4076.00s -> 4077.00s]  you know
[4077.00s -> 4078.00s]  very deep networks
[4078.00s -> 4079.00s]  of conv layers
[4079.00s -> 4080.00s]  so again
[4080.00s -> 4081.00s]  we'll discuss
[4081.00s -> 4082.00s]  all this
[4082.00s -> 4083.00s]  later on
[4083.00s -> 4084.00s]  and then
[4084.00s -> 4085.00s]  typical architectures
[4085.00s -> 4086.00s]  again look like this
[4086.00s -> 4087.00s]  you know
[4087.00s -> 4088.00s]  as we had earlier
[4088.00s -> 4089.00s]  conv ReLU
[4089.00s -> 4090.00s]  for some n
[4090.00s -> 4091.00s]  number of steps
[4091.00s -> 4092.00s]  followed by a pool
[4092.00s -> 4093.00s]  every once in a while
[4093.00s -> 4094.00s]  this whole thing
[4094.00s -> 4095.00s]  repeated
[4095.00s -> 4096.00s]  some number of times
[4096.00s -> 4097.00s]  and then followed by
[4097.00s -> 4098.00s]  fully connected
[4098.00s -> 4099.00s]  ReLU layers
[4099.00s -> 4100.00s]  that we saw earlier
[4100.00s -> 4101.00s]  you know
[4101.00s -> 4102.00s]  one or two
[4102.00s -> 4103.00s]  or just a few of these
[4103.00s -> 4104.00s]  and then a softmax
[4104.00s -> 4105.00s]  at the end
[4105.00s -> 4106.00s]  for your
[4106.00s -> 4107.00s]  so
[4107.00s -> 4108.00s]  you know
[4108.00s -> 4109.00s]  some typical values
[4109.00s -> 4110.00s]  you might have
[4110.00s -> 4111.00s]  n up to
[4111.00s -> 4112.00s]  you know
[4112.00s -> 4113.00s]  five of these
[4113.00s -> 4114.00s]  you're gonna
[4114.00s -> 4115.00s]  to have
[4115.00s -> 4116.00s]  pretty deep
[4116.00s -> 4117.00s]  layers of
[4117.00s -> 4118.00s]  conv ReLU
[4118.00s -> 4119.00s]  pool sequences
[4119.00s -> 4120.00s]  and then usually
[4120.00s -> 4121.00s]  just a couple
[4121.00s -> 4122.00s]  of these fully
[4122.00s -> 4123.00s]  connected layers
[4123.00s -> 4124.00s]  at the end
[4124.00s -> 4125.00s]  I will also go into
[4125.00s -> 4126.00s]  some
[4126.00s -> 4127.00s]  newer architectures
[4127.00s -> 4128.00s]  like ResNet
[4128.00s -> 4129.00s]  and GoogleNet
[4129.00s -> 4130.00s]  which
[4130.00s -> 4131.00s]  challenged this
[4131.00s -> 4132.00s]  and will give
[4132.00s -> 4133.00s]  pretty different types
[4133.00s -> 4134.00s]  of architectures
[4134.00s -> 4135.00s]  okay thank you
