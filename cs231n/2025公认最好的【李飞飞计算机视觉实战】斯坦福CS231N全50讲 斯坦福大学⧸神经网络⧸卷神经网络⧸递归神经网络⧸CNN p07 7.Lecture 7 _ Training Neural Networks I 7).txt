# Detected language: en (p=1.00)

[0.00s -> 15.12s]  Okay, it's after 12, so I think we should get started.
[15.12s -> 17.92s]  Today we're going to pick up where we left off last time.
[17.92s -> 22.04s]  Last time we talked about a lot of tips and tricks involved in the nitty-gritty details
[22.04s -> 23.86s]  of training neural networks.
[23.86s -> 27.90s]  Today we'll pick up where we left off and talk about a lot more of these nitty-gritty
[27.90s -> 30.80s]  details about training these things.
[30.80s -> 35.10s]  As usual, a couple administrative notes before we get into the material.
[35.10s -> 38.08s]  As you all know, assignment one is already due.
[38.08s -> 39.08s]  Hopefully you all turned it in.
[39.08s -> 40.88s]  Did it go okay?
[40.88s -> 42.96s]  Was it not okay?
[42.96s -> 45.82s]  Rough sentiment?
[45.82s -> 46.82s]  Mostly okay?
[46.82s -> 48.82s]  Okay, that's good.
[48.82s -> 50.56s]  Awesome.
[50.56s -> 53.54s]  So we're in the process of grading those, so stay tuned.
[53.54s -> 57.88s]  We're hoping to get grades back for those before A2 is due.
[57.88s -> 65.98s]  Another reminder that your project proposals are due today at 11.59, so make sure you send
[65.98s -> 66.98s]  those in.
[66.98s -> 69.56s]  Details are on the website and on Piazza.
[69.56s -> 72.62s]  Also a reminder, assignment two is already out.
[72.62s -> 74.80s]  That'll be due a week from Thursday.
[74.80s -> 80.24s]  Historically, assignment two has been the longest one in the class, so if you haven't
[80.24s -> 87.72s]  started already on assignment two, I recommend you take a look at that pretty soon.
[87.72s -> 91.38s]  Another reminder is that for assignment two, I think a lot of you will be using Google
[91.38s -> 96.30s]  Cloud, and big reminder, make sure to stop your instances when you're not using them
[96.30s -> 100.24s]  because whenever your instance is on, you get charged, and we only have so many coupons
[100.24s -> 103.28s]  to distribute to you guys.
[103.28s -> 106.86s]  Any time your instance is on, even if you're not SSH to it, even if you're not
[106.86s -> 111.20s]  running things immediately in your Jupyter notebook, any time that instance is on, you're
[111.20s -> 112.80s]  gonna be charged.
[112.80s -> 117.40s]  Just make sure that you explicitly stop your instances when you're not using them.
[117.40s -> 121.80s]  In this example, I've got a little screenshot of my dashboard on Google Cloud, and you
[121.80s -> 125.28s]  need to go in there and explicitly go to the dropdown and click stop.
[125.28s -> 130.04s]  So just make sure that you do this when you're done working each day.
[130.04s -> 134.20s]  Another thing to remember is it's kinda up to you guys to keep track of your spending
[134.20s -> 135.76s]  on Google Cloud.
[135.76s -> 141.40s]  In particular, instances that use GPUs are a lot more expensive than those with CPUs.
[141.40s -> 146.48s]  Rough order of magnitude, those GPU instances are around 90 cents to a dollar an hour,
[146.48s -> 148.60s]  so those are actually quite pricey.
[148.60s -> 150.92s]  And the CPU instances are much cheaper.
[150.92s -> 154.86s]  The general strategy is that you probably wanna make two instances, one with a GPU
[154.86s -> 160.16s]  and one without, and then only use that GPU instance when you really need the GPU.
[160.16s -> 164.84s]  For example, on assignment two, most of the assignment, you should only need the
[164.84s -> 167.80s]  CPU, so you should only use your CPU instance for that.
[167.80s -> 173.40s]  But then the final question about TensorFlow or PyTorch, that will need a GPU.
[173.40s -> 176.12s]  This will give you a little bit of practice with switching between multiple instances
[176.64s -> 179.48s]  and only using that GPU when it's really necessary.
[179.48s -> 184.64s]  And again, just kinda watch your spending, try not to go too crazy on these things.
[184.64s -> 188.80s]  So any questions on the administrative stuff before we move on?
[191.80s -> 192.64s]  Question?
[194.44s -> 196.64s]  Question is how much RAM should we use?
[196.64s -> 202.00s]  I think eight or 16 gigs is probably good for everything that you need in this class.
[202.00s -> 204.72s]  But yeah, as you scale up the number of CPUs and the number of RAM,
[204.76s -> 206.28s]  you'll also end up spending more money.
[206.28s -> 211.12s]  So again, if you stick with two or four CPUs and eight or 16 gigs of RAM,
[211.12s -> 215.28s]  that should be plenty for all the homework-related stuff that you need to do.
[217.00s -> 220.88s]  So as a quick recap, last time we talked about activation functions.
[220.88s -> 223.56s]  We talked about this whole zoo of different activation functions
[223.56s -> 225.48s]  and some of their different properties.
[225.48s -> 230.12s]  We saw that the sigmoid, which used to be quite popular when training neural networks
[230.12s -> 233.72s]  maybe 10 years ago or so, has this problem with vanishing gradients
[233.72s -> 237.48s]  where the two ends of the activation function.
[237.48s -> 239.96s]  Tanh has this similar sort of problem.
[239.96s -> 243.24s]  And kind of the general recommendation is that you probably wanna stick with ReLU
[243.24s -> 246.00s]  for most cases as sort of a default choice
[246.00s -> 249.76s]  because it tends to work well for a lot of different architectures.
[249.76s -> 251.96s]  We also talked about weight initialization.
[251.96s -> 255.16s]  And remember that up on the top, we have this idea that
[255.16s -> 258.48s]  when you initialize your weights at the start of training,
[258.48s -> 260.96s]  if those weights are initialized to be too small,
[261.04s -> 265.36s]  then the activations will vanish as you go through the network.
[265.36s -> 268.20s]  Because as you multiply by these small numbers over and over again,
[268.20s -> 269.92s]  they'll all sort of decay to zero.
[269.92s -> 272.76s]  And then everything will be zero, learning won't happen, you'll be sad.
[272.76s -> 275.84s]  On the other hand, if you initialize your weights too big,
[275.84s -> 279.80s]  then as you go through the network and multiply by your weight matrix over and over again,
[279.80s -> 284.20s]  eventually they'll explode, you'll be unhappy, there'll be no learning, it will be very bad.
[284.20s -> 287.80s]  But if you get that initialization just right,
[287.80s -> 292.80s]  for example using the Xavier initialization or the MSRA initialization,
[292.80s -> 297.20s]  then you kind of keep a nice distribution of activations as you go through the network.
[297.20s -> 302.56s]  And remember that this kind of gets more and more important and more and more critical
[302.56s -> 304.56s]  as your networks get deeper and deeper,
[304.56s -> 308.56s]  because as your network gets deeper, you're multiplying by those weight matrices over and over again
[308.56s -> 310.56s]  with these more multiplicative terms.
[310.56s -> 314.56s]  We also talked last time about data pre-processing.
[314.56s -> 320.56s]  We talked about how it's pretty typical in ConvNets to zero-center and normalize your data,
[320.56s -> 323.56s]  so it has zero mean and unit variance.
[323.56s -> 329.56s]  I wanted to provide a little bit of extra intuition about why you might actually want to do this.
[329.56s -> 334.56s]  Imagine a simple setup where we have a binary classification problem,
[334.56s -> 339.56s]  where we want to draw a line to separate these red points from these blue points.
[339.56s -> 345.56s]  On the left, you have this idea where if those data points are not normalized and not centered
[345.56s -> 349.56s]  and far away from the origin, then we can still use a line to separate them,
[349.56s -> 355.56s]  but now if that line wiggles just a little bit, then our classification is going to get totally destroyed.
[355.56s -> 361.56s]  That means that in the example on the left, the loss function is now extremely sensitive
[361.56s -> 365.56s]  to small perturbations in that linear classifier in our weight matrix.
[366.56s -> 371.56s]  We can still represent the same functions, but that might make learning quite difficult
[371.56s -> 376.56s]  because our loss is very sensitive to our parameter vector.
[376.56s -> 380.56s]  Whereas in the situation on the right, if you take that data cloud
[380.56s -> 383.56s]  and you move it into the origin and you make it unit variance,
[383.56s -> 387.56s]  then again we can still classify that data quite well,
[387.56s -> 390.56s]  but now as we wiggle that line a little bit,
[390.56s -> 395.56s]  then our loss function is less sensitive to small perturbations in the parameter values.
[395.56s -> 400.56s]  That maybe makes optimization a little bit easier, as we'll see a little bit going forward.
[400.56s -> 406.56s]  By the way, this situation is not only in the linear classification case.
[406.56s -> 413.56s]  Inside a neural network, remember we have these interleavings of these linear matrix multipliers
[413.56s -> 417.56s]  or convolutions, followed by non-linear activation functions.
[417.56s -> 423.56s]  If the input to some layer in your neural network is not centered,
[423.56s -> 425.56s]  or not zero-mean, not unit variance,
[425.56s -> 429.56s]  then again small perturbations in the weight matrix of that layer of the network
[429.56s -> 433.56s]  could cause large perturbations in the output of that layer,
[433.56s -> 435.56s]  which again might make learning difficult.
[435.56s -> 441.56s]  This is a little bit of extra intuition about why normalization might be important.
[441.56s -> 445.56s]  Because we have this intuition that normalization is so important,
[445.56s -> 447.56s]  we talked about batch normalization,
[447.56s -> 450.56s]  which is where we just add this additional layer inside our networks
[450.56s -> 455.56s]  to just force all of the intermediate activations to be zero-mean and unit variance.
[455.56s -> 459.56s]  I've sort of re-summarized the batch normalization equations here
[459.56s -> 461.56s]  with the shapes a little bit more explicitly.
[461.56s -> 465.56s]  Hopefully this can help you out when you're implementing this thing on assignment 2.
[465.56s -> 469.56s]  But again, in batch normalization we have this idea that in the forward pass
[469.56s -> 473.56s]  we use the statistics of the mini-batch to compute a mean and a standard deviation
[473.56s -> 478.56s]  and then use those estimates to normalize our data on the forward pass.
[478.56s -> 481.56s]  Then we also reintroduce these scale and shift parameters
[481.56s -> 484.56s]  to increase the expressivity of the layer.
[484.56s -> 488.56s]  You might want to refer back to this when working on assignment 2.
[488.56s -> 493.56s]  We also talked last time a little bit about babysitting the learning process,
[493.56s -> 497.56s]  how you should probably be looking at your loss curves during training.
[497.56s -> 501.56s]  Here's an example of some networks I was actually training over the weekend.
[502.56s -> 506.56s]  This is usually my setup when I'm working on these things.
[506.56s -> 510.56s]  On the left I have some plot showing the training loss over time.
[510.56s -> 514.56s]  You can see it's kind of going down, which means my network is reducing the loss.
[514.56s -> 516.56s]  It's doing well.
[516.56s -> 522.56s]  On the right there's this plot where the x-axis is, again, time, or the aeration number.
[522.56s -> 528.56s]  The y-axis is my performance measure both on my training set and on my validation set.
[528.56s -> 530.56s]  You can see that as we go over time
[530.56s -> 533.56s]  then my training set performance goes up and up and up and up
[533.56s -> 535.56s]  as my loss function goes down.
[535.56s -> 538.56s]  But at some point my validation set performance kind of plateaus.
[538.56s -> 541.56s]  This suggests that maybe I'm overfitting in this situation.
[541.56s -> 544.56s]  Maybe I should have been trying to add additional regularization.
[546.56s -> 549.56s]  We also talked a bit last time about hyperparameter search.
[549.56s -> 552.56s]  All these networks have a large zoo of hyperparameters
[552.56s -> 554.56s]  and it's pretty important to set them correctly.
[554.56s -> 557.56s]  We talked a little bit about grid search versus random search
[557.56s -> 560.56s]  and how random search is maybe a little bit nicer in theory
[560.56s -> 565.56s]  because in the situation where your performance might be more sensitive
[565.56s -> 567.56s]  with respect to one hyperparameter than another
[567.56s -> 570.56s]  and random search lets you cover that space a little bit better.
[570.56s -> 573.56s]  We also talked about the idea of coarse-to-fine search
[573.56s -> 576.56s]  where when you're doing this hyperparameter optimization
[576.56s -> 580.56s]  probably you want to start with very wide ranges for your hyperparameters,
[580.56s -> 582.56s]  only train for a couple iterations,
[582.56s -> 584.56s]  and then based on those results
[584.56s -> 587.56s]  you kind of narrow in on the range of hyperparameters that are good
[587.56s -> 591.56s]  and now again redo your search in a smaller range for more iterations.
[591.56s -> 593.56s]  You can kind of iterate this process
[593.56s -> 596.56s]  to hone in on the right region for hyperparameters.
[596.56s -> 598.56s]  Again, it's really important to, at the start,
[598.56s -> 601.56s]  have this very coarse range to start with
[601.56s -> 604.56s]  where you want very, very wide ranges for all your hyperparameters.
[604.56s -> 607.56s]  Ideally, those ranges should be so wide
[607.56s -> 610.56s]  that your network is kind of blowing up at either end of the range
[610.56s -> 613.56s]  so that you know that you've searched a wide enough range for those things.
[615.56s -> 617.56s]  So today...
[617.56s -> 619.56s]  Oh, question?
[631.56s -> 634.56s]  The question is how many hyperparameters do you typically search at a time?
[634.56s -> 637.56s]  Here is two, but there's a lot more than two in these typical things.
[637.56s -> 640.56s]  It kind of depends on the exact model and the exact architecture,
[640.56s -> 643.56s]  but because the number of possibilities is exponential
[643.56s -> 645.56s]  and the number of hyperparameters,
[645.56s -> 647.56s]  you can't really test too many at a time.
[647.56s -> 650.56s]  It also kind of depends on how many machines you have available,
[650.56s -> 653.56s]  so it kind of varies from person to person
[653.56s -> 655.56s]  and from experiment to experiment,
[655.56s -> 658.56s]  but generally I try not to do this
[658.56s -> 661.56s]  over more than maybe two or three or four at a time at most
[661.56s -> 664.56s]  because again, this exponential search just gets out of control.
[664.56s -> 667.56s]  So typically, learning rate is the really important one
[667.56s -> 669.56s]  that you need to nail first.
[669.56s -> 672.56s]  And then other things like regularization,
[672.56s -> 675.56s]  like learning rate decay, model size,
[675.56s -> 678.56s]  these other types of things tend to be a little bit less sensitive
[678.56s -> 679.56s]  than learning rate.
[679.56s -> 681.56s]  So sometimes you might do kind of a block coordinate descent
[681.56s -> 683.56s]  where you go and find the good learning rate,
[683.56s -> 687.56s]  then you go back and try to look at different model sizes,
[687.56s -> 690.56s]  and this can help you cut down on the exponential search a little bit,
[690.56s -> 692.56s]  but it's a little bit problem dependent
[692.56s -> 695.56s]  on exactly which ones you should be searching over in which order.
[695.56s -> 697.56s]  More questions?
[699.56s -> 717.56s]  Yeah, a question is,
[717.56s -> 720.56s]  how often does it happen where when you change one hyperparameter
[720.56s -> 723.56s]  then the optimal values of the other hyperparameters change?
[723.56s -> 726.56s]  That does happen sometimes,
[726.56s -> 728.56s]  although for learning rates,
[728.56s -> 731.56s]  that's typically less of a problem.
[731.56s -> 732.56s]  So for learning rates,
[732.56s -> 734.56s]  typically you want to get in a good range
[734.56s -> 736.56s]  and then set it maybe even a little bit lower than optimal
[736.56s -> 738.56s]  and let it go for a long time.
[738.56s -> 739.56s]  And then if you do that,
[739.56s -> 741.56s]  combined with some of the fancier optimization strategies
[741.56s -> 743.56s]  that we'll talk about today,
[743.56s -> 746.56s]  then a lot of models tend to be a little bit less sensitive
[746.56s -> 751.56s]  to learning rate once you get them in a good range.
[751.56s -> 757.56s]  Sorry, did you have a question in front as well?
[757.56s -> 759.56s]  The question is, what's wrong with having a small learning rate
[759.56s -> 761.56s]  and increasing the number of epochs?
[761.56s -> 767.56s]  The answer is that it might take a very long time.
[767.56s -> 770.56s]  Intuitively, if you set the learning rate very low
[770.56s -> 772.56s]  and let it go for a very long time,
[772.56s -> 774.56s]  then this should, in theory, always work.
[774.56s -> 777.56s]  But in practice, those factors of 10 or 100
[777.56s -> 780.56s]  actually matter a lot when you're training these things.
[780.56s -> 782.56s]  Maybe if you got the right learning rate,
[782.56s -> 784.56s]  you could train it in 6 hours, 12 hours, or a day,
[784.56s -> 786.56s]  but then if you just were super safe
[786.56s -> 789.56s]  and dropped it by a factor of 10 or by a factor of 100,
[789.56s -> 791.56s]  now that one-day training becomes 100 days of training,
[791.56s -> 795.56s]  that's three months, that's not going to be good.
[795.56s -> 798.56s]  When you're taking these intro computer science classes,
[798.56s -> 800.56s]  they always kind of sweep the constants under the rug,
[800.56s -> 802.56s]  but when you're actually thinking about training things,
[802.56s -> 805.56s]  those constants end up mattering a lot.
[805.56s -> 813.56s]  Another question?
[813.56s -> 815.56s]  The question is, for low learning rate,
[815.56s -> 818.56s]  is it more likely to be stuck in local optima?
[818.56s -> 820.56s]  I think that makes in some intuitive sense,
[820.56s -> 823.56s]  but in practice, that seems not to be much of a problem,
[823.56s -> 827.56s]  and I think we'll talk a bit more about that later today.
[827.56s -> 829.56s]  Today, I wanted to talk about a couple other
[829.56s -> 831.56s]  really interesting and important topics
[831.56s -> 833.56s]  when we're training neural networks.
[833.56s -> 835.56s]  In particular, I wanted to talk...
[835.56s -> 837.56s]  We've kind of alluded to this fact of fancier,
[837.56s -> 839.56s]  more powerful optimization algorithms a couple times.
[839.56s -> 841.56s]  I wanted to spend some time today
[841.56s -> 843.56s]  and really dig into those and talk about
[843.56s -> 845.56s]  the actual optimization algorithms
[845.56s -> 847.56s]  that most people are using these days.
[847.56s -> 850.56s]  We also touched on regularization in earlier lectures,
[850.56s -> 853.56s]  this concept of making your network do additional things
[853.56s -> 856.56s]  to reduce the gap between train and test error.
[856.56s -> 858.56s]  I wanted to talk about some more strategies
[858.56s -> 860.56s]  that people are using in practice of regularization
[860.56s -> 862.56s]  with respect to neural networks.
[862.56s -> 865.56s]  Finally, I also wanted to talk a bit about transfer learning,
[865.56s -> 867.56s]  where you can sometimes get away
[867.56s -> 869.56s]  with using less data than you think
[869.56s -> 872.56s]  by transferring from one problem to another.
[872.56s -> 874.56s]  If you recall from a few lectures ago,
[874.56s -> 877.56s]  the kind of core strategy in training neural networks
[877.56s -> 879.56s]  is an optimization problem,
[879.56s -> 881.56s]  where we write down some loss function,
[881.56s -> 885.56s]  which defines for each value of the network weights,
[885.56s -> 887.56s]  the loss function tells us how good or bad
[887.56s -> 890.56s]  is that value of the weights doing on our problem.
[890.56s -> 893.56s]  Then we imagine that this loss function
[893.56s -> 896.56s]  gives us some nice landscape over the weights,
[896.56s -> 898.56s]  where on the right, I've shown this
[898.56s -> 900.56s]  maybe small two-dimensional problem,
[900.56s -> 903.56s]  where the x and y axes are two values of the weights,
[903.56s -> 905.56s]  and then the color of the plot
[905.56s -> 907.56s]  kind of represents the value of the loss.
[907.56s -> 909.56s]  In this kind of cartoon picture
[909.56s -> 911.56s]  of a two-dimensional problem,
[911.56s -> 914.56s]  we're only optimizing over these two values, w1, w2,
[914.56s -> 919.56s]  and the goal is to find the most red region in this case,
[919.56s -> 921.56s]  which corresponds to the setting of the weights
[921.56s -> 922.56s]  with the lowest loss.
[922.56s -> 924.56s]  Remember, we've been working so far
[924.56s -> 926.56s]  with this extremely simple optimization algorithm,
[926.56s -> 928.56s]  stochastic gradient descent,
[928.56s -> 932.56s]  where we have this, it's super simple, it's three lines.
[932.56s -> 935.56s]  While true, we first evaluate the loss
[935.56s -> 937.56s]  and the gradient on some mini-batch of data,
[937.56s -> 941.56s]  and then we step updating our parameter value,
[941.56s -> 942.56s]  our parameter vector,
[942.56s -> 944.56s]  in the negative direction of the gradient,
[944.56s -> 946.56s]  because this gives, again, the direction
[946.56s -> 948.56s]  of greatest decrease of the loss function.
[948.56s -> 950.56s]  Then we repeat this over and over again,
[950.56s -> 952.56s]  and hopefully we converge to the red region,
[952.56s -> 955.56s]  and we get great errors, and we're very happy.
[956.56s -> 959.56s]  Unfortunately, this relatively simple optimization algorithm
[959.56s -> 961.56s]  has quite a lot of problems
[961.56s -> 965.56s]  that actually could come up in practice.
[965.56s -> 968.56s]  One problem with stochastic gradient descent,
[968.56s -> 971.56s]  imagine what happens if our objective function
[971.56s -> 973.56s]  looks something like this,
[973.56s -> 978.56s]  where, again, we're plotting two values, w1 and w2,
[978.56s -> 980.56s]  and as we change one of those values,
[980.56s -> 983.56s]  the loss function changes very slowly.
[983.56s -> 984.56s]  As we change the horizontal value,
[984.56s -> 986.56s]  then our loss changes slowly.
[986.56s -> 989.56s]  As we go up and down in this landscape,
[989.56s -> 991.56s]  now our loss is very sensitive
[991.56s -> 994.56s]  to changes in the vertical direction.
[994.56s -> 997.56s]  By the way, this is referred to as the loss
[997.56s -> 1000.56s]  having a bad condition number at this point,
[1000.56s -> 1002.56s]  which is the ratio between the largest
[1002.56s -> 1003.56s]  and smallest singular values
[1003.56s -> 1005.56s]  of the Hessian matrix at that point.
[1005.56s -> 1008.56s]  But the intuitive idea is that the loss landscape
[1008.56s -> 1009.56s]  kind of looks like a taco shell,
[1009.56s -> 1012.56s]  and it's sort of very sensitive in one direction,
[1012.56s -> 1013.56s]  not sensitive in the other direction.
[1013.56s -> 1016.56s]  The question is, what might SGD,
[1016.56s -> 1017.56s]  stochastic gradient descent,
[1017.56s -> 1020.56s]  do on a function that looks like this?
[1024.56s -> 1026.56s]  If you run stochastic gradient descent
[1026.56s -> 1028.56s]  on this type of function,
[1028.56s -> 1031.56s]  you might get this characteristic zig-zagging behavior,
[1031.56s -> 1035.56s]  because for this type of objective function,
[1035.56s -> 1039.56s]  the direction of the gradient does not align
[1039.56s -> 1041.56s]  with the direction towards the minima.
[1041.56s -> 1042.56s]  When you compute the gradient
[1042.56s -> 1043.56s]  and take a step,
[1043.56s -> 1046.56s]  you might step over this line
[1046.56s -> 1048.56s]  and zig-zag back and forth.
[1048.56s -> 1051.56s]  In effect, you get very slow progress
[1051.56s -> 1052.56s]  along the horizontal dimension,
[1052.56s -> 1054.56s]  which is the less sensitive dimension,
[1054.56s -> 1056.56s]  and you get this zig-zagging,
[1056.56s -> 1058.56s]  nasty zig-zagging behavior
[1058.56s -> 1060.56s]  across the fast-changing dimension.
[1060.56s -> 1063.56s]  This is undesirable behavior.
[1063.56s -> 1066.56s]  By the way, this problem actually becomes
[1066.56s -> 1069.56s]  much more common in high dimensions.
[1069.56s -> 1071.56s]  In this kind of cartoon picture,
[1071.56s -> 1073.56s]  we're only showing a two-dimensional
[1073.56s -> 1074.56s]  optimization landscape,
[1074.56s -> 1076.56s]  but in practice, our neural networks
[1076.56s -> 1078.56s]  might have millions, tens of millions,
[1078.56s -> 1080.56s]  hundreds of millions of parameters.
[1080.56s -> 1082.56s]  That's hundreds of millions of directions
[1082.56s -> 1084.56s]  along which this thing can move.
[1084.56s -> 1086.56s]  Among those hundreds of millions
[1086.56s -> 1088.56s]  of different directions to move,
[1088.56s -> 1090.56s]  if the ratio between the largest one
[1090.56s -> 1091.56s]  and the smallest one is bad,
[1091.56s -> 1093.56s]  then SGD will not perform so nicely.
[1093.56s -> 1095.56s]  You can imagine that if we have
[1095.56s -> 1096.56s]  a hundred million parameters,
[1096.56s -> 1098.56s]  probably the maximum ratio between those two
[1098.56s -> 1100.56s]  will be quite large.
[1100.56s -> 1102.56s]  I think this is actually quite a big problem
[1102.56s -> 1105.56s]  in practice for many high-dimensional problems.
[1107.56s -> 1109.56s]  Another problem with SGD has to do
[1109.56s -> 1111.56s]  with this idea of local minima,
[1111.56s -> 1113.56s]  or saddle points.
[1113.56s -> 1116.56s]  Here, I've swapped the graph a little bit,
[1116.56s -> 1118.56s]  and now the x-axis is showing
[1118.56s -> 1120.56s]  the value of one parameter,
[1120.56s -> 1122.56s]  and then the y-axis is showing
[1122.56s -> 1124.56s]  the value of a loss.
[1124.56s -> 1125.56s]  In this top example,
[1125.56s -> 1128.56s]  we have this curvy objective function
[1128.56s -> 1131.56s]  where there's a valley in the middle.
[1131.56s -> 1134.56s]  What happens to SGD in this situation?
[1136.56s -> 1139.56s]  In this situation, SGD will get stuck
[1139.56s -> 1141.56s]  because at this local minima,
[1141.56s -> 1144.56s]  the gradient is zero because it's locally flat.
[1144.56s -> 1145.56s]  Now remember, with SGD,
[1145.56s -> 1147.56s]  we compute the gradient and step
[1147.56s -> 1149.56s]  in the direction of opposite gradient.
[1149.56s -> 1150.56s]  If at our current point,
[1150.56s -> 1152.56s]  the opposite gradient is zero,
[1152.56s -> 1154.56s]  then we're not going to make any progress
[1154.56s -> 1156.56s]  and we'll get stuck at this point.
[1156.56s -> 1159.56s]  Another problem with this idea of saddle points,
[1159.56s -> 1161.56s]  rather than being a local minima,
[1161.56s -> 1163.56s]  you can imagine a point where
[1163.56s -> 1164.56s]  in one direction we go up
[1164.56s -> 1166.56s]  and in the other direction we go down,
[1166.56s -> 1167.56s]  and then at our current point,
[1167.56s -> 1169.56s]  the gradient is zero.
[1169.56s -> 1171.56s]  Again, in this situation,
[1171.56s -> 1173.56s]  the function will get stuck at the saddle point
[1173.56s -> 1176.56s]  because the gradient is zero.
[1176.56s -> 1178.56s]  Although, one thing I'd like to point out
[1178.56s -> 1182.56s]  is that in a one-dimensional problem like this,
[1182.56s -> 1184.56s]  local minima seem like a big problem
[1184.56s -> 1185.56s]  and saddle points seem like
[1185.56s -> 1187.56s]  not something to worry about.
[1187.56s -> 1188.56s]  In fact, it's the opposite
[1188.56s -> 1191.56s]  once you move to very high-dimensional problems.
[1191.56s -> 1193.56s]  Again, if you think about
[1193.56s -> 1195.56s]  you're in this 100 million dimensional space,
[1195.56s -> 1197.56s]  what does a saddle point mean?
[1197.56s -> 1199.56s]  That means that at my current point,
[1199.56s -> 1201.56s]  some directions the loss goes up,
[1201.56s -> 1203.56s]  and some directions the loss goes down.
[1203.56s -> 1205.56s]  If you have 100 million dimensions,
[1205.56s -> 1206.56s]  that's probably going to happen
[1206.56s -> 1209.56s]  almost everywhere, basically.
[1209.56s -> 1211.56s]  Whereas a local minima says that
[1211.56s -> 1214.56s]  of all those 100 million directions that I can move,
[1214.56s -> 1216.56s]  every one of them causes the loss to go up.
[1216.56s -> 1218.56s]  In fact, that seems pretty rare
[1218.56s -> 1219.56s]  when you're thinking about, again,
[1219.56s -> 1222.56s]  these very high-dimensional problems.
[1222.56s -> 1225.56s]  Really, the idea that has come to light
[1225.56s -> 1226.56s]  in the last few years is that
[1226.56s -> 1228.56s]  when you're training these very large neural networks,
[1228.56s -> 1230.56s]  the problem is more about saddle points
[1230.56s -> 1232.56s]  and less about local minima.
[1232.56s -> 1235.56s]  By the way, this also is a problem
[1235.56s -> 1237.56s]  not just exactly at the saddle point,
[1237.56s -> 1239.56s]  but also near the saddle point.
[1239.56s -> 1241.56s]  If you look at the example on the bottom,
[1241.56s -> 1243.56s]  you see that in the regions
[1243.56s -> 1245.56s]  around the saddle point,
[1245.56s -> 1246.56s]  the gradient isn't zero,
[1246.56s -> 1248.56s]  but the slope is very small.
[1248.56s -> 1249.56s]  That means that if we're, again,
[1249.56s -> 1251.56s]  just stepping in the direction of the gradient,
[1251.56s -> 1253.56s]  and that gradient is very small,
[1253.56s -> 1255.56s]  we're going to make very, very slow progress
[1255.56s -> 1258.56s]  whenever our current parameter value
[1258.56s -> 1261.56s]  is near a saddle point in the objective landscape.
[1261.56s -> 1264.56s]  This is actually a big problem.
[1266.56s -> 1270.56s]  Another problem with SGD comes from the S.
[1270.56s -> 1273.56s]  Remember that SGD is stochastic gradient descent.
[1273.56s -> 1276.56s]  Recall that our loss function is typically defined
[1276.56s -> 1280.56s]  by computing the loss over many, many different examples.
[1280.56s -> 1284.56s]  In this case, if N is your whole training set,
[1284.56s -> 1286.56s]  then that could be something like a million.
[1286.56s -> 1289.56s]  Each time computing the loss would be very, very expensive.
[1289.56s -> 1293.56s]  In practice, remember that we often estimate the loss
[1293.56s -> 1294.56s]  and estimate the gradient
[1294.56s -> 1297.56s]  using a small mini-batch of examples.
[1297.56s -> 1299.56s]  What this means is that we're not actually getting
[1299.56s -> 1302.56s]  true information about the gradient at every time step.
[1302.56s -> 1304.56s]  Instead, we're just getting some noisy estimate
[1304.56s -> 1306.56s]  of the gradient at our current point.
[1306.56s -> 1307.56s]  Here on the right,
[1307.56s -> 1310.56s]  I've kind of faked this plot a little bit.
[1310.56s -> 1313.56s]  I've just added random uniform noise
[1313.56s -> 1315.56s]  to the gradient at every point,
[1315.56s -> 1319.56s]  and then run SGD with these noisy, messed up gradients.
[1319.56s -> 1321.56s]  This is maybe not exactly what happens
[1321.56s -> 1322.56s]  with the SGD process,
[1322.56s -> 1324.56s]  but it still gives you the sense that
[1324.56s -> 1326.56s]  if there's noise in your gradient estimates,
[1326.56s -> 1329.56s]  then vanilla SGD kind of meanders around the space,
[1329.56s -> 1331.56s]  and it might actually take a long time
[1331.56s -> 1333.56s]  to get towards the minima.
[1335.56s -> 1338.56s]  Now that we've talked about a lot of these problems...
[1338.56s -> 1340.56s]  Sorry, was there a question?
[1348.56s -> 1349.56s]  The question is,
[1349.56s -> 1351.56s]  do all of these just go away
[1351.56s -> 1354.56s]  if we use normal gradient descent?
[1355.56s -> 1356.56s]  Let's see.
[1356.56s -> 1360.56s]  I think that the taco shell problem
[1360.56s -> 1361.56s]  of high condition numbers
[1361.56s -> 1364.56s]  is still a problem with full batch gradient descent.
[1364.56s -> 1366.56s]  The noise, as we'll see,
[1366.56s -> 1368.56s]  we might sometimes introduce additional noise
[1368.56s -> 1369.56s]  into the network,
[1369.56s -> 1371.56s]  not only due to sampling mini-batches,
[1371.56s -> 1374.56s]  but also due to explicit stochasticity in the network,
[1374.56s -> 1375.56s]  so we'll see that later.
[1375.56s -> 1377.56s]  That can still be a problem.
[1377.56s -> 1379.56s]  Saddle points,
[1379.56s -> 1381.56s]  that's still a problem for full batch gradient descent
[1381.56s -> 1383.56s]  because there can still be saddle points
[1383.56s -> 1385.56s]  in the full objective landscape.
[1385.56s -> 1387.56s]  Basically, even if we go to full batch gradient descent,
[1387.56s -> 1389.56s]  it doesn't really solve these problems.
[1389.56s -> 1391.56s]  We kind of need to think about
[1391.56s -> 1393.56s]  a slightly fancier optimization algorithm
[1393.56s -> 1396.56s]  that can try to address these concerns.
[1396.56s -> 1398.56s]  Thankfully, there's a really, really simple strategy
[1398.56s -> 1399.56s]  that works pretty well
[1399.56s -> 1401.56s]  at addressing many of these problems,
[1401.56s -> 1404.56s]  and that's this idea of adding a momentum term
[1404.56s -> 1406.56s]  to our stochastic gradient descent.
[1406.56s -> 1407.56s]  Here on the left,
[1407.56s -> 1410.56s]  we have our classic old friend SGD,
[1410.56s -> 1412.56s]  where we just always step in the direction of the gradient.
[1412.56s -> 1413.56s]  But now on the right,
[1413.56s -> 1415.56s]  we have this minor, minor variant
[1415.56s -> 1417.56s]  called SGD plus momentum,
[1417.56s -> 1420.56s]  which is now two equations and five lines of code,
[1420.56s -> 1422.56s]  so it's twice as complicated.
[1422.56s -> 1424.56s]  But it's very simple.
[1424.56s -> 1427.56s]  The idea is that we maintain a velocity over time,
[1427.56s -> 1430.56s]  and we add our gradient estimates to the velocity,
[1430.56s -> 1433.56s]  and then we step in the direction of the velocity
[1433.56s -> 1436.56s]  rather than stepping in the direction of the gradient.
[1436.56s -> 1437.56s]  This is relatively,
[1437.56s -> 1439.56s]  this is very, very simple.
[1440.56s -> 1443.56s]  Oh, and we also have this hyperparameter rho now,
[1443.56s -> 1445.56s]  which corresponds to friction.
[1445.56s -> 1446.56s]  Now at every time step,
[1446.56s -> 1448.56s]  we take our current velocity,
[1448.56s -> 1451.56s]  we decay the current velocity by the friction constant, rho,
[1451.56s -> 1453.56s]  which is often something high,
[1453.56s -> 1456.56s]  like 0.9 is a common choice.
[1456.56s -> 1458.56s]  We take our current velocity,
[1458.56s -> 1459.56s]  we decay it by friction,
[1459.56s -> 1461.56s]  and we add in our gradient.
[1461.56s -> 1463.56s]  Now we step in the direction of our velocity vector
[1463.56s -> 1467.56s]  rather than the direction of our raw gradient vector.
[1467.56s -> 1470.56s]  This super, super simple strategy
[1470.56s -> 1472.56s]  actually helps for all of these problems
[1472.56s -> 1474.56s]  that we just talked about.
[1474.56s -> 1476.56s]  If you think about what happens
[1476.56s -> 1478.56s]  at local minima or saddle points,
[1478.56s -> 1481.56s]  then if we're imagining velocity in this system,
[1481.56s -> 1483.56s]  then you kind of have this physical interpretation
[1483.56s -> 1486.56s]  of this ball kind of rolling down the hill,
[1486.56s -> 1488.56s]  picking up speed as it comes down.
[1488.56s -> 1490.56s]  Now once we have velocity,
[1490.56s -> 1493.56s]  then even when we pass that point of local minima,
[1493.56s -> 1495.56s]  the point will still have velocity
[1495.56s -> 1496.56s]  even if it doesn't have gradient.
[1496.56s -> 1498.56s]  So then we can hopefully get over
[1498.56s -> 1500.56s]  this local minima and continue downward.
[1500.56s -> 1503.56s]  There's a similar intuition near saddle points
[1503.56s -> 1505.56s]  where even though the gradient
[1505.56s -> 1507.56s]  around the saddle point is very small,
[1507.56s -> 1509.56s]  we have this velocity vector that we've built up
[1509.56s -> 1510.56s]  as we roll downhill,
[1510.56s -> 1512.56s]  and that can hopefully carry us through the saddle point
[1512.56s -> 1515.56s]  and let us continue rolling all the way down.
[1515.56s -> 1518.56s]  If you think about what happens in poor conditioning,
[1518.56s -> 1521.56s]  now if we were to have
[1521.56s -> 1523.56s]  these kind of zig-zagging approximations
[1523.56s -> 1524.56s]  to the gradient,
[1524.56s -> 1528.56s]  then those zig-zags will hopefully cancel each other out
[1528.56s -> 1530.56s]  pretty fast once we're using momentum,
[1530.56s -> 1532.56s]  and this will effectively reduce
[1532.56s -> 1535.56s]  the amount by which we step in the sensitive direction,
[1535.56s -> 1538.56s]  whereas in the horizontal direction,
[1538.56s -> 1540.56s]  our velocity will just keep building up
[1540.56s -> 1542.56s]  and will actually accelerate our descent
[1542.56s -> 1545.56s]  across that less sensitive dimension.
[1545.56s -> 1547.56s]  So adding momentum here can actually help us
[1547.56s -> 1550.56s]  with this high condition number problem as well.
[1550.56s -> 1552.56s]  Finally, on the right,
[1552.56s -> 1554.56s]  we've repeated the same visualization
[1554.56s -> 1557.56s]  of gradient descent with noise,
[1557.56s -> 1560.56s]  and here the black is this vanilla SGD,
[1560.56s -> 1562.56s]  which is sort of zig-zagging all over the place,
[1562.56s -> 1565.56s]  where the blue line is showing now SGD with momentum.
[1565.56s -> 1567.56s]  So you can see that because we're building up
[1567.56s -> 1569.56s]  this velocity over time,
[1569.56s -> 1571.56s]  the noise kind of gets averaged out
[1571.56s -> 1572.56s]  in our gradient estimates,
[1572.56s -> 1575.56s]  and now SGD ends up taking a much smoother path
[1575.56s -> 1577.56s]  towards the minima compared with the SGD,
[1577.56s -> 1580.56s]  which is kind of meandering due to noise.
[1580.56s -> 1581.56s]  Question?
[1594.56s -> 1597.56s]  The question is, how does SGD momentum help
[1597.56s -> 1600.56s]  with the poorly conditioned coordinate?
[1600.56s -> 1602.56s]  The idea is that if you go back
[1602.56s -> 1605.56s]  and look at this velocity computation,
[1605.56s -> 1608.56s]  we're adding in the gradient at every time step.
[1608.56s -> 1611.56s]  So it kind of depends on your setting of rho,
[1611.56s -> 1612.56s]  that hyperparameter,
[1612.56s -> 1615.56s]  but you can imagine that if the gradient
[1615.56s -> 1616.56s]  is relatively small,
[1616.56s -> 1619.56s]  and if rho is well-behaved in this situation,
[1619.56s -> 1621.56s]  then our velocity could actually monotonically increase
[1621.56s -> 1623.56s]  up to a point where the velocity
[1623.56s -> 1625.56s]  could now be larger than the actual gradient.
[1625.56s -> 1627.56s]  So then we might actually make faster progress
[1627.56s -> 1630.56s]  along the poorly conditioned dimension.
[1632.56s -> 1635.56s]  So kind of one picture that you can have in mind
[1635.56s -> 1637.56s]  when we're doing SGD plus momentum
[1637.56s -> 1639.56s]  is that the red here is our current point.
[1639.56s -> 1642.56s]  At our current point, we have some red vector,
[1642.56s -> 1644.56s]  which is the direction of the gradient,
[1644.56s -> 1646.56s]  or rather our estimate of the gradient
[1646.56s -> 1647.56s]  at the current point.
[1647.56s -> 1649.56s]  And green is now the direction of our velocity vector.
[1649.56s -> 1651.56s]  And now when we do the momentum update,
[1651.56s -> 1653.56s]  we're actually stepping according
[1653.56s -> 1655.56s]  to a weighted average of these two.
[1655.56s -> 1657.56s]  And this helps overcome some noise
[1657.56s -> 1659.56s]  in our gradient estimate.
[1659.56s -> 1661.56s]  There's a slight variation of momentum
[1661.56s -> 1664.56s]  that you sometimes see called Nesterov accelerated gradient,
[1664.56s -> 1666.56s]  also sometimes called Nesterov momentum.
[1667.56s -> 1670.56s]  That switches up this order of things a little bit.
[1670.56s -> 1673.56s]  So in sort of normal SGD momentum,
[1673.56s -> 1676.56s]  we imagine that we estimate the gradient
[1676.56s -> 1678.56s]  at our current point and then take a mix
[1678.56s -> 1680.56s]  of our velocity and our gradient.
[1680.56s -> 1681.56s]  With Nesterov accelerated gradient,
[1681.56s -> 1683.56s]  you do something a little bit different.
[1683.56s -> 1686.56s]  Here you start at the red point,
[1686.56s -> 1688.56s]  you step in the direction
[1688.56s -> 1690.56s]  of where the velocity would take you,
[1690.56s -> 1692.56s]  you evaluate the gradient at that point,
[1692.56s -> 1695.56s]  and then you go back to your original point
[1695.56s -> 1697.56s]  and kind of mix together those two.
[1697.56s -> 1700.56s]  So this is kind of a funny interpretation,
[1700.56s -> 1702.56s]  but you can imagine that you're kind of mixing
[1702.56s -> 1704.56s]  together information a little bit more.
[1704.56s -> 1706.56s]  That maybe if your velocity direction
[1706.56s -> 1708.56s]  was actually a little bit wrong,
[1708.56s -> 1710.56s]  it lets you incorporate gradient information
[1710.56s -> 1711.56s]  from a little bit larger parts
[1711.56s -> 1714.56s]  of the objective landscape.
[1714.56s -> 1716.56s]  This also has some really nice theoretical properties
[1716.56s -> 1719.56s]  when it comes to convex optimization.
[1719.56s -> 1722.56s]  But those guarantees go a little bit out the window
[1722.56s -> 1725.56s]  once it comes to non-convex problems like neural networks.
[1725.56s -> 1727.56s]  Writing it down in equations,
[1727.56s -> 1729.56s]  Nesterov momentum looks something like this,
[1729.56s -> 1732.56s]  where now to update our velocity,
[1732.56s -> 1735.56s]  we take a step according to our previous velocity
[1735.56s -> 1737.56s]  and evaluate the gradient there.
[1737.56s -> 1739.56s]  Now when we take our next step,
[1739.56s -> 1742.56s]  we mix together, we actually step in the direction
[1742.56s -> 1744.56s]  of our velocity that's incorporating information
[1744.56s -> 1746.56s]  from these multiple points.
[1746.56s -> 1747.56s]  Question?
[1748.56s -> 1750.56s]  Oh, sorry, so the question is,
[1750.56s -> 1752.56s]  what's a good initialization for the velocity?
[1752.56s -> 1754.56s]  And this is almost always zero.
[1754.56s -> 1756.56s]  So it's not even a hyperparameter,
[1756.56s -> 1758.56s]  just set it to zero and don't worry.
[1758.56s -> 1760.56s]  Another question?
[1770.56s -> 1772.56s]  Yeah, so intuitively, the velocity is kind of
[1772.56s -> 1774.56s]  a weighted sum of your gradients
[1774.56s -> 1776.56s]  that you've seen over the years.
[1776.56s -> 1780.56s]  Your gradients that you've seen over time.
[1780.56s -> 1783.56s]  Yes, with more recent gradients being weighted heavier.
[1783.56s -> 1786.56s]  So at every time step, we take our old velocity,
[1786.56s -> 1789.56s]  we decay by friction, and we add in our current gradient.
[1789.56s -> 1791.56s]  So you can kind of think of this
[1791.56s -> 1794.56s]  as a smooth moving average of your recent gradients
[1794.56s -> 1797.56s]  with kind of an exponentially decaying weight
[1797.56s -> 1800.56s]  on your gradients going back in time.
[1802.56s -> 1805.56s]  So this Nesterov formulation is a little bit annoying
[1805.56s -> 1807.56s]  because if you look at this,
[1807.56s -> 1809.56s]  normally when you have your loss function,
[1809.56s -> 1811.56s]  you want to evaluate your loss and your gradient
[1811.56s -> 1814.56s]  at the same point, and Nesterov breaks this a little bit.
[1814.56s -> 1816.56s]  So it's a little bit annoying to work with.
[1816.56s -> 1819.56s]  Thankfully, there's a cute change of variables you can do.
[1819.56s -> 1821.56s]  So if you do the change of variables
[1821.56s -> 1823.56s]  and reshuffle a little bit,
[1823.56s -> 1825.56s]  then you can write Nesterov momentum
[1825.56s -> 1827.56s]  in a slightly different way that now, again,
[1827.56s -> 1829.56s]  lets you evaluate the loss and the gradient
[1829.56s -> 1831.56s]  at the same point always.
[1831.56s -> 1833.56s]  And once you make this change of variables,
[1833.56s -> 1835.56s]  you get the result from Nesterov,
[1835.56s -> 1837.56s]  which is that here in the first step,
[1837.56s -> 1840.56s]  this looks exactly like updating the velocity
[1840.56s -> 1842.56s]  in the vanilla SGD momentum case
[1842.56s -> 1844.56s]  where we have our current velocity,
[1844.56s -> 1846.56s]  we evaluate gradient at the current point
[1846.56s -> 1849.56s]  and mix these two together in a decaying way.
[1849.56s -> 1851.56s]  And now in the second update,
[1851.56s -> 1853.56s]  and now when we're actually updating our parameter vector,
[1853.56s -> 1855.56s]  if you look at the second equation,
[1855.56s -> 1857.56s]  we have our current point plus our current velocity
[1857.56s -> 1860.56s]  plus a weighted difference between our current velocity
[1860.56s -> 1862.56s]  and our previous velocity.
[1862.56s -> 1864.56s]  So Nesterov momentum is kind of incorporating
[1864.56s -> 1867.56s]  some kind of error correcting term
[1867.56s -> 1871.56s]  between your current velocity and your previous velocity.
[1871.56s -> 1872.56s]  And if you look at what these,
[1872.56s -> 1875.56s]  so if we look at SGD, SGD momentum,
[1875.56s -> 1878.56s]  and Nesterov momentum on this kind of simple problem
[1878.56s -> 1881.56s]  compared with SGD, we notice that Nesterov and S,
[1881.56s -> 1884.56s]  so SGD kind of takes this, SGD is in the black,
[1884.56s -> 1886.56s]  kind of taking this slow progress toward the minima.
[1886.56s -> 1889.56s]  The blue and the green show momentum and Nesterov,
[1889.56s -> 1891.56s]  and these have this behavior
[1891.56s -> 1893.56s]  of overshooting the minimum,
[1893.56s -> 1894.56s]  because they're building up velocity
[1894.56s -> 1896.56s]  going past the minimum,
[1896.56s -> 1898.56s]  and then kind of correcting themselves
[1898.56s -> 1900.56s]  and coming back towards the minima.
[1900.56s -> 1901.56s]  Question?
[1911.56s -> 1914.56s]  Yeah, so the question is, this list picture looks good,
[1914.56s -> 1916.56s]  but what happens if your minima
[1916.56s -> 1918.56s]  lies in this very narrow basin?
[1918.56s -> 1920.56s]  Will the velocity just cause you
[1920.56s -> 1921.56s]  to over that minima?
[1921.56s -> 1923.56s]  That's actually a really interesting point
[1923.56s -> 1925.56s]  and the subject of some recent theoretical work,
[1925.56s -> 1928.56s]  but the idea is that maybe those really sharp minima
[1928.56s -> 1929.56s]  are actually bad minima.
[1929.56s -> 1931.56s]  We don't want to even land in those,
[1931.56s -> 1933.56s]  because the idea is that maybe
[1933.56s -> 1934.56s]  if you have a very sharp minima,
[1934.56s -> 1937.56s]  that actually could be a minima that overfits more,
[1937.56s -> 1940.56s]  that if you imagine that we doubled our training set,
[1940.56s -> 1942.56s]  the whole optimization landscape would change,
[1942.56s -> 1944.56s]  and maybe that very sensitive minima
[1944.56s -> 1945.56s]  would actually disappear
[1945.56s -> 1947.56s]  if we were to collect more training data.
[1947.56s -> 1949.56s]  So we kind of have this intuition
[1949.56s -> 1951.56s]  that we want to land in very flat minima,
[1951.56s -> 1953.56s]  because those very flat minima
[1953.56s -> 1956.56s]  are probably more robust as we change the training data,
[1956.56s -> 1958.56s]  so those flat minima might actually generalize better
[1958.56s -> 1960.56s]  to testing data.
[1960.56s -> 1963.56s]  This is, again, sort of very recent theoretical work,
[1963.56s -> 1965.56s]  but that's actually a really good point
[1965.56s -> 1966.56s]  that you bring it up.
[1966.56s -> 1968.56s]  In some sense, it's actually a feature
[1968.56s -> 1971.56s]  and not a bug that SGD momentum
[1971.56s -> 1975.56s]  actually skips over those very sharp minima.
[1975.56s -> 1978.56s]  That's actually a good thing, believe it or not.
[1980.56s -> 1981.56s]  Another thing you can see is,
[1981.56s -> 1982.56s]  if you look at the difference
[1982.56s -> 1984.56s]  between momentum and Nesterov here,
[1984.56s -> 1985.56s]  you can see that,
[1985.56s -> 1987.56s]  because of the correction factor in Nesterov,
[1987.56s -> 1990.56s]  maybe it's not overshooting quite as drastically
[1990.56s -> 1992.56s]  compared to vanilla momentum.
[1994.56s -> 1997.56s]  There's another kind of common optimization strategy
