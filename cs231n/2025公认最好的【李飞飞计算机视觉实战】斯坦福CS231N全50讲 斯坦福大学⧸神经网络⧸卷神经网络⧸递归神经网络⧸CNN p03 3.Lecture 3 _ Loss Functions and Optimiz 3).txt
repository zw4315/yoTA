# Detected language: en (p=1.00)

[0.00s -> 20.00s]  Welcome to CS231n Lecture 3. Today we're going to talk about loss functions and optimization, but as usual, before we get to the main content of the lecture, there's a couple of administrative things to talk about.
[21.00s -> 27.00s]  The first thing is that Assignment 1 has been released. You can find the link up on the website.
[27.00s -> 36.00s]  Since we were a little bit late in getting this assignment out to you guys, we've decided to change the due date to Thursday, April 20th at 11.59 PM.
[36.00s -> 43.00s]  This will give you a full two weeks from the assignment release date to go and actually finish and work on it.
[43.00s -> 48.00s]  We'll update the syllabus for this new due date in a little bit later today.
[48.00s -> 58.00s]  As a reminder, when you complete the assignment, you should go turn in the final zip file on Canvas so we can grade it and get you grades back as quickly as possible.
[58.00s -> 64.00s]  The next thing is always check out Piazza for interesting administrative stuff.
[64.00s -> 71.00s]  This week I wanted to highlight that we have several example project ideas as a pin post on Piazza.
[71.00s -> 78.00s]  We went out and solicited example of project ideas from various people in the Stanford community or affiliated to Stanford.
[78.00s -> 84.00s]  They came up with some interesting suggestions for projects that they might want students in the class to work on.
[84.00s -> 93.00s]  Check out this pinned post on Piazza. If you want to work on any of these projects, then feel free to contact the project mentors directly about these things.
[93.00s -> 103.00s]  Additionally, we've posted office hours on the course website. This is a Google calendar. This is something that people have been asking about and now it's up there.
[103.00s -> 112.00s]  The final administrative note is about Google Cloud. As a reminder, because we're supported by Google Cloud in this class,
[112.00s -> 118.00s]  we're able to give each of you an additional $100 credit for Google Cloud to work on your assignments and projects.
[118.00s -> 124.00s]  The exact details of how to redeem that credit will go out later today, most likely on Piazza.
[124.00s -> 130.00s]  I guess if there's no questions about administrative stuff, then we'll move on to course content.
[134.00s -> 144.00s]  Recall from last time, in lecture 2, we were really talking about the challenges of recognition and trying to hone in on this idea of a data-driven approach.
[144.00s -> 155.00s]  We talked about this idea of image classification, talked about why it's hard, there's this semantic gap between the giant grid of numbers that the computer sees and the actual image that you see.
[155.00s -> 160.00s]  We talked about various challenges regarding this, around illumination, deformation, etc.
[160.00s -> 167.00s]  And why this is actually a really, really hard problem, even though it's super easy for people to do with their human eyes and human visual system.
[168.00s -> 175.00s]  Then also recall last time, we talked about the k-Nearest Neighbor classifier as kind of a simple introduction to this whole data-driven mindset.
[175.00s -> 181.00s]  We talked about the CIFAR-10 dataset, where you can see an example of these images on the upper left here,
[181.00s -> 185.00s]  where CIFAR-10 gives you these 10 different categories, airplane, automobile, whatnot.
[185.00s -> 195.00s]  And we talked about how the k-Nearest Neighbor classifier can be used to learn decision boundaries to separate these data points into classes based on the training data.
[195.00s -> 203.00s]  This also led us to a discussion of the idea of cross-validation and setting hyperparameters by dividing your data into train, validation, and test sets.
[205.00s -> 212.00s]  Then also recall last time, we talked about linear classification as the first sort of building block as we move toward neural networks.
[212.00s -> 224.00s]  Recall that the linear classifier is an example of a parametric classifier, where all of our knowledge about the training data gets summarized into this parameter matrix, W, that is set during the process of training.
[225.00s -> 231.00s]  And this linear classifier, recall, is super simple, where we're going to take the image and stretch it out into a long vector.
[231.00s -> 242.00s]  So here the image is X, and then we take that image, which might be 32 by 32 by 3 pixels, stretch it out into a long column vector of 32 times 32 times 3 entries,
[242.00s -> 249.00s]  where the 32 and 32 are the height and width, and the 3 give you the three color channels, red, green, blue.
[250.00s -> 257.00s]  Then there exists some parameter matrix, W, which will take this long column vector, representing the image pixels,
[257.00s -> 264.00s]  and convert this and give you 10 numbers, giving scores for each of the 10 classes in the case of CIFAR-10,
[264.00s -> 270.00s]  where we kind of had this interpretation where larger values of those scores,
[270.00s -> 275.00s]  so a larger value for the cat class, means the classifier thinks that the cat is more likely for that image,
[275.00s -> 281.00s]  and lower values for maybe the dog or car class indicate lower probabilities of those classes being present in the image.
[283.00s -> 291.00s]  I think this point was a little bit unclear last time, that linear classification has this interpretation as learning templates per class,
[291.00s -> 297.00s]  where if you look at the diagram on the lower left, you think that for every pixel in the image,
[297.00s -> 302.00s]  and for every one of our 10 classes, there exists some entry in this matrix, W,
[302.00s -> 306.00s]  telling us how much does that pixel influence that class.
[306.00s -> 312.00s]  So that means that each of these rows in the matrix W ends up corresponding to a template for the class.
[312.00s -> 314.00s]  And if we take those rows and unravel...
[314.00s -> 322.00s]  So each of those rows, again, corresponds to a weighting between the pixel values of the image and that class.
[322.00s -> 329.00s]  So if we take that row and unravel it back into an image, then we can visualize the learned template for each of these classes.
[330.00s -> 337.00s]  We also had this interpretation of linear classification as learning linear decision boundaries between pixels in some high dimensional space,
[337.00s -> 343.00s]  where the dimensions of the space correspond to the pixel intensity values of the image.
[343.00s -> 346.00s]  So this is kind of where we left off last time.
[349.00s -> 354.00s]  Where we kind of ended up last time is we've got this idea of a linear classifier,
[354.00s -> 357.00s]  and we didn't talk about how to actually choose the W,
[357.00s -> 362.00s]  how to actually use the training data to determine which value of W should be best.
[362.00s -> 367.00s]  So kind of where we stopped off at is that for some setting of W,
[367.00s -> 372.00s]  we can use this W to come up with 10 with our class scores for any image.
[372.00s -> 375.00s]  And some of these class scores might be better or worse.
[375.00s -> 380.00s]  So here in this simple example, we've shown maybe just a training data set of three images,
[380.00s -> 385.00s]  along with the 10 class scores predicted for some value of W for those images.
[385.00s -> 389.00s]  And you can see that some of these scores are better or worse than others.
[389.00s -> 393.00s]  So for example, in the image on the left, if you look up, it's actually a cat,
[393.00s -> 395.00s]  because you're a human and you can tell these things.
[395.00s -> 398.00s]  But if we look at the assigned probabilities,
[398.00s -> 403.00s]  well not probabilities, but scores, then the classifier maybe for this setting of W
[403.00s -> 407.00s]  gave the cat class a score of 2.9 for this image,
[407.00s -> 410.00s]  whereas the frog class gave 3.78.
[410.00s -> 414.00s]  So maybe the classifier is doing not so good on this image, that's bad.
[414.00s -> 417.00s]  We wanted the true class to be actually the highest class score.
[417.00s -> 421.00s]  Whereas for some of these other examples, like the car for example,
[421.00s -> 425.00s]  you see that the automobile class has a score of 6, which is much higher than any of the others,
[425.00s -> 426.00s]  so that's good.
[426.00s -> 432.00s]  And the predicted scores are maybe negative 4, which is much lower than all the other ones.
[432.00s -> 434.00s]  So that's actually bad.
[434.00s -> 438.00s]  So this is kind of a hand-wavy approach, just kind of looking at the scores
[438.00s -> 440.00s]  and eyeballing which ones are good and which ones are bad.
[440.00s -> 443.00s]  But to actually write algorithms about these things
[443.00s -> 446.00s]  and to actually determine automatically which W will be best,
[446.00s -> 450.00s]  we need some way to quantify the badness of any particular W.
[450.00s -> 456.00s]  And this function that takes in a W, looks at the scores,
[456.00s -> 459.00s]  and then tells us how bad quantitatively is that W
[459.00s -> 461.00s]  is something that we'll call a loss function.
[461.00s -> 465.00s]  And in this lecture we'll see a couple examples of different loss functions
[465.00s -> 468.00s]  that you can use for this image classification problem.
[468.00s -> 472.00s]  So then once we've got this idea of a loss function,
[472.00s -> 476.00s]  this allows us to quantify, for any given value of W,
[476.00s -> 478.00s]  how good or bad is it.
[478.00s -> 481.00s]  But then we actually need to come up with an efficient procedure
[481.00s -> 484.00s]  for searching through the space of all possible Ws
[484.00s -> 488.00s]  and actually come up with what is the correct value of W
[488.00s -> 490.00s]  that is the least bad.
[490.00s -> 492.00s]  And this process will be an optimization procedure,
[492.00s -> 495.00s]  and we'll talk more about that in this lecture.
[495.00s -> 498.00s]  So I'm going to shrink this example a little bit
[498.00s -> 500.00s]  because 10 classes is a little bit unwieldy.
[500.00s -> 503.00s]  So we'll kind of work with this tiny toy data set
[503.00s -> 508.00s]  of three examples and three classes going forward in this lecture.
[508.00s -> 513.00s]  So again, in this example, the cat is maybe not so correctly classified,
[513.00s -> 515.00s]  the car is correctly classified,
[515.00s -> 520.00s]  and this setting of W got this frog image totally wrong
[520.00s -> 524.00s]  because the frog score is much lower than the others.
[524.00s -> 526.00s]  So to formalize this a little bit,
[526.00s -> 528.00s]  usually when we talk about a loss function,
[528.00s -> 531.00s]  we imagine that we have some training data set
[531.00s -> 534.00s]  of Xs and Ys, usually N examples of these,
[534.00s -> 537.00s]  where the Xs are the inputs to the algorithm.
[537.00s -> 539.00s]  In the image classification case,
[539.00s -> 543.00s]  the Xs would be the actual pixel values of your images,
[543.00s -> 546.00s]  and the Ys will be the things you want your algorithm to predict.
[546.00s -> 549.00s]  We usually call these the labels or the targets.
[549.00s -> 551.00s]  So in the case of image classification,
[551.00s -> 554.00s]  remember we're trying to categorize each image
[554.00s -> 557.00s]  for CIFAR-10 into one of 10 categories.
[557.00s -> 559.00s]  So the label Y here will be an integer
[559.00s -> 562.00s]  between 1 and 10 or maybe between 0 and 9
[562.00s -> 564.00s]  depending on what programming language you're using,
[564.00s -> 566.00s]  but it'll be an integer telling you
[566.00s -> 570.00s]  what is the correct category for each one of those images X.
[570.00s -> 574.00s]  And now our loss function will denote Li to denote the,
[574.00s -> 577.00s]  so then we have this prediction function X
[577.00s -> 581.00s]  which takes in our example X and our weight matrix W
[581.00s -> 583.00s]  and makes some prediction for Y.
[583.00s -> 584.00s]  In the case of image classification,
[584.00s -> 586.00s]  these will be our 10 numbers.
[586.00s -> 590.00s]  Then we'll define some loss function Li
[590.00s -> 592.00s]  which will take in the predicted scores
[592.00s -> 594.00s]  coming out of the function F
[594.00s -> 597.00s]  together with the true target or label Y
[597.00s -> 599.00s]  and give us some quantitative value
[599.00s -> 602.00s]  for how bad those predictions are for that training example.
[602.00s -> 604.00s]  And now the final loss, L,
[604.00s -> 606.00s]  will be the average of these losses
[606.00s -> 608.00s]  summed over the entire data set
[608.00s -> 610.00s]  over each of the N examples in our data set.
[610.00s -> 613.00s]  So this is actually a very general formulation
[613.00s -> 616.00s]  and actually extends even beyond image classification.
[616.00s -> 618.00s]  Kind of as we move forward
[618.00s -> 621.00s]  and see other examples of tasks in deep learning,
[621.00s -> 623.00s]  the kind of generic setup is that
[623.00s -> 626.00s]  for any task you have some Xs and Ys
[626.00s -> 628.00s]  and you want to write down some loss function
[628.00s -> 631.00s]  that quantifies exactly how happy you are
[631.00s -> 633.00s]  with your particular parameter settings W
[633.00s -> 636.00s]  and then you'll eventually search over the space of W
[636.00s -> 638.00s]  to find the W that minimizes the loss
[638.00s -> 640.00s]  on your training data.
[640.00s -> 645.00s]  So as a first example of a concrete loss function
[645.00s -> 649.00s]  that is a nice thing to work with in image classification,
[649.00s -> 652.00s]  we'll talk about the multi-class SVM loss.
[652.00s -> 655.00s]  You may have seen the binary SVM
[655.00s -> 658.00s]  or support vector machine in CS229
[658.00s -> 662.00s]  and the multi-class SVM is a generalization of that
[662.00s -> 665.00s]  to handle multiple classes.
[665.00s -> 669.00s]  In the binary SVM case, as you may have seen in 229,
[669.00s -> 671.00s]  you only had two classes.
[671.00s -> 673.00s]  Each example X was gonna be classified
[673.00s -> 675.00s]  as either a positive or negative example
[675.00s -> 677.00s]  but now we have 10 categories
[677.00s -> 679.00s]  so we need to generalize this notion
[679.00s -> 681.00s]  to handle multiple classes.
[681.00s -> 684.00s]  So this loss function has kind of a funny functional form
[685.00s -> 688.00s]  so we'll walk through it in quite a bit of detail
[688.00s -> 690.00s]  over the next couple of slides.
[690.00s -> 693.00s]  But what this is saying is that the loss Li
[693.00s -> 696.00s]  for any individual example, the way we'll compute it
[696.00s -> 700.00s]  is we're gonna perform a sum over all of the categories Y
[700.00s -> 703.00s]  except for the true category Yi.
[703.00s -> 706.00s]  So we're gonna sum over all the incorrect categories
[706.00s -> 708.00s]  and then we're gonna compare the score
[708.00s -> 710.00s]  of the correct category
[710.00s -> 712.00s]  and the score of the incorrect category.
[712.00s -> 715.00s]  And now if the score for the correct category
[715.00s -> 719.00s]  is greater than the score of the incorrect category,
[719.00s -> 722.00s]  greater than the incorrect score
[722.00s -> 725.00s]  by some safety margin that we set to one,
[725.00s -> 728.00s]  if that's the case, that means that the true score
[728.00s -> 731.00s]  is much, or the score for the true category
[731.00s -> 734.00s]  is if it's much larger than any of the false categories
[734.00s -> 737.00s]  then we'll get a loss of zero.
[737.00s -> 740.00s]  And we'll sum this up over all of the
[741.00s -> 743.00s]  incorrect categories for our image
[743.00s -> 745.00s]  and this will give us our final loss
[745.00s -> 747.00s]  for this one example in the data set.
[747.00s -> 750.00s]  And again, we'll take the average of this loss
[750.00s -> 752.00s]  over the whole training data set.
[752.00s -> 756.00s]  So this kind of like if-then statement,
[756.00s -> 760.00s]  like if the true class score is much larger
[760.00s -> 764.00s]  than the others, this kind of if-then formulation,
[764.00s -> 767.00s]  we often compactify into this single max
[767.00s -> 770.00s]  of zero sj minus syij plus one thing,
[770.00s -> 773.00s]  but I always find that notation a little bit confusing
[773.00s -> 775.00s]  and it always helps me to write it out
[775.00s -> 777.00s]  in this sort of case-based notation
[777.00s -> 779.00s]  to figure out exactly what the two cases are
[779.00s -> 781.00s]  and what's going on.
[781.00s -> 784.00s]  And by the way, this style of loss function
[784.00s -> 787.00s]  where we take max of zero and some other quantity
[787.00s -> 790.00s]  is often referred to as some type of a hinge loss
[790.00s -> 793.00s]  and this name comes from the shape of the graph
[793.00s -> 795.00s]  when you go and plot it.
[795.00s -> 799.00s]  So here the x-axis corresponds to the syi,
[799.00s -> 801.00s]  that is the score of the true class
[801.00s -> 803.00s]  for some training example
[803.00s -> 805.00s]  and now the y-axis is the loss.
[805.00s -> 810.00s]  And you can see that as the score for the true category
[810.00s -> 812.00s]  for this example increases,
[812.00s -> 814.00s]  then the loss will go down linearly
[814.00s -> 818.00s]  until we get to above this safety margin
[818.00s -> 820.00s]  and after which the loss will be zero
[820.00s -> 824.00s]  because we've already correctly classified this example.
[825.00s -> 828.00s]  So let's, oh, question?
[835.00s -> 837.00s]  Yeah, so the question is in terms of notation,
[837.00s -> 841.00s]  what is s and what is syi in particular?
[841.00s -> 844.00s]  So the s's are the predicted scores
[844.00s -> 847.00s]  for the classes that are coming out of the classifier.
[847.00s -> 849.00s]  So like if one is the cat class
[849.00s -> 850.00s]  and two is the dog class,
[850.00s -> 852.00s]  then s1 and s2 would be the cat
[852.00s -> 854.00s]  and dog scores respectively.
[854.00s -> 857.00s]  And remember we said that yi was the category
[857.00s -> 860.00s]  of the ground truth label for the example,
[860.00s -> 861.00s]  which is some integer.
[861.00s -> 865.00s]  So then syi, sorry for the double subscript,
[865.00s -> 869.00s]  that corresponds to the score of the true class
[869.00s -> 872.00s]  for the ith example in the training set.
[872.00s -> 874.00s]  Question?
[875.00s -> 878.00s]  Yeah, the question is what exactly is this computing here?
[878.00s -> 880.00s]  It's a little bit funny.
[880.00s -> 882.00s]  I think it'll become more clear
[882.00s -> 884.00s]  when we walk through an explicit example,
[884.00s -> 886.00s]  but in some sense what this loss is saying
[886.00s -> 889.00s]  is that we are happy if the true score
[889.00s -> 892.00s]  is much higher than all the other scores.
[892.00s -> 894.00s]  It needs to be higher than all the other scores
[894.00s -> 896.00s]  by some safety margin.
[896.00s -> 900.00s]  And if the true score is not high enough,
[900.00s -> 902.00s]  greater than any of the other scores,
[902.00s -> 906.00s]  then we will incur some loss and that will be bad.
[906.00s -> 909.00s]  So this might make a little bit more sense
[909.00s -> 911.00s]  if you walk through an explicit example
[911.00s -> 913.00s]  for this tiny three example data set.
[913.00s -> 916.00s]  So here remember I've sort of removed
[916.00s -> 918.00s]  the case-based notation and just switching back
[918.00s -> 920.00s]  to the zero one notation.
[920.00s -> 922.00s]  And now if we look at, if we think about
[922.00s -> 925.00s]  computing this multiclass SVM loss
[925.00s -> 927.00s]  for just this first training example on the left,
[927.00s -> 929.00s]  then remember we're going to loop over
[929.00s -> 931.00s]  all of the incorrect classes.
[931.00s -> 934.00s]  So for this example, cat is the correct class.
[934.00s -> 937.00s]  So we're going to loop over the car and frog classes.
[937.00s -> 941.00s]  And now for car, we're going to compare the,
[941.00s -> 943.00s]  we're going to look at the car score, 5.1,
[943.00s -> 947.00s]  minus the cat score, 3.2 plus one.
[947.00s -> 950.00s]  When we're comparing cat and car,
[950.00s -> 952.00s]  we expect to incur some loss here
[952.00s -> 954.00s]  because the car score is greater than the cat score,
[954.00s -> 955.00s]  which is bad.
[955.00s -> 959.00s]  So for this one class, for this one example,
[959.00s -> 961.00s]  we'll incur a loss of 2.9.
[961.00s -> 963.00s]  And then when we go and compare
[963.00s -> 965.00s]  the cat score and the frog score,
[965.00s -> 968.00s]  we see that cat is 3.2, frog is minus 1.7.
[968.00s -> 972.00s]  So cat is more than one greater than frog,
[972.00s -> 975.00s]  which means that between these two classes
[975.00s -> 976.00s]  we incur zero loss.
[976.00s -> 979.00s]  So then the training, so then the multiclass SVM loss
[979.00s -> 982.00s]  for this training example will be the sum of the losses
[982.00s -> 984.00s]  across each of these pairs of classes,
[984.00s -> 987.00s]  which will be 2.9 plus zero, which is 2.9.
[987.00s -> 990.00s]  Which is sort of saying that 2.9 is a quantitative measure
[990.00s -> 992.00s]  of how much our classifier screwed up
[992.00s -> 994.00s]  on this one training example.
[995.00s -> 997.00s]  And then if we repeat this procedure
[997.00s -> 999.00s]  for this next car image,
[999.00s -> 1001.00s]  then again the true class is car,
[1001.00s -> 1004.00s]  so we're gonna iterate over all of the other categories.
[1004.00s -> 1007.00s]  When we compare the car and the cat score,
[1007.00s -> 1010.00s]  we see that car is more than one greater than cat,
[1010.00s -> 1011.00s]  so we get no loss here.
[1011.00s -> 1013.00s]  When we compare car and frog,
[1013.00s -> 1015.00s]  we again see that the car score
[1015.00s -> 1017.00s]  is more than one greater than frog,
[1017.00s -> 1019.00s]  so we get again no loss here.
[1019.00s -> 1022.00s]  And our total loss for this training example is zero.
[1023.00s -> 1025.00s]  And now I think you hopefully get the picture by now,
[1025.00s -> 1027.00s]  but if you go look at frog,
[1027.00s -> 1030.00s]  now frog, we again compare frog and cat,
[1030.00s -> 1031.00s]  incur quite a lot of loss
[1031.00s -> 1033.00s]  because the frog score is very low.
[1033.00s -> 1035.00s]  Compare frog and car, incur a lot of loss
[1035.00s -> 1037.00s]  because the score is very low.
[1037.00s -> 1040.00s]  And then our loss for this example is 12.9.
[1041.00s -> 1044.00s]  And then our final loss for the entire data set
[1044.00s -> 1045.00s]  is the average of these losses
[1045.00s -> 1046.00s]  across the different examples.
[1046.00s -> 1049.00s]  So when you sum those out, it comes to about 5.3.
[1049.00s -> 1050.00s]  So then it's sort of,
[1050.00s -> 1051.00s]  this is our quantitative measure
[1051.00s -> 1054.00s]  that our classifier is 5.3 bad on this data set.
[1054.00s -> 1055.00s]  Was there a question?
[1058.00s -> 1059.00s]  Yeah, the question is,
[1059.00s -> 1061.00s]  how do you choose the plus one?
[1061.00s -> 1063.00s]  That's actually a really great question.
[1063.00s -> 1065.00s]  It seems like kind of an arbitrary choice here.
[1065.00s -> 1067.00s]  Like, it's the only constant
[1067.00s -> 1068.00s]  that appears in the loss function
[1068.00s -> 1069.00s]  and that seems to offend
[1069.00s -> 1072.00s]  your aesthetic sensibilities a bit, maybe.
[1072.00s -> 1073.00s]  But it turns out that
[1073.00s -> 1075.00s]  this is somewhat of an arbitrary choice.
[1075.00s -> 1076.00s]  You can kind of set the,
[1076.00s -> 1077.00s]  because we don't actually care
[1077.00s -> 1080.00s]  about the absolute values of the scores
[1081.00s -> 1082.00s]  in this loss function,
[1082.00s -> 1084.00s]  we only care about the relative differences
[1084.00s -> 1085.00s]  between the scores.
[1085.00s -> 1087.00s]  We only care that the correct score
[1087.00s -> 1089.00s]  is much greater than the incorrect scores.
[1089.00s -> 1091.00s]  So in fact, if you imagine scaling up
[1091.00s -> 1092.00s]  your whole W up or down,
[1092.00s -> 1094.00s]  then it kind of rescales
[1094.00s -> 1096.00s]  all the scores correspondingly.
[1096.00s -> 1098.00s]  And if you kind of work through the details
[1098.00s -> 1100.00s]  and there's a detailed derivation of this
[1100.00s -> 1101.00s]  in the course notes online,
[1101.00s -> 1103.00s]  you find that this choice of one
[1103.00s -> 1104.00s]  actually doesn't matter.
[1104.00s -> 1107.00s]  That this free parameter of one
[1107.00s -> 1108.00s]  kind of washes out
[1108.00s -> 1109.00s]  and is canceled with the scale,
[1109.00s -> 1112.00s]  like the overall setting of the scale in W.
[1112.00s -> 1114.00s]  And again, check the course notes
[1114.00s -> 1116.00s]  for a bit more detail on that.
[1118.00s -> 1120.00s]  So then, I think it's kind of useful
[1120.00s -> 1122.00s]  to think about a couple different questions
[1122.00s -> 1123.00s]  to try to understand intuitively
[1123.00s -> 1125.00s]  what this loss is doing.
[1125.00s -> 1127.00s]  So the first question is,
[1127.00s -> 1129.00s]  what's going to happen to the loss
[1129.00s -> 1130.00s]  if we change the scores
[1130.00s -> 1133.00s]  of the car image just a little bit?
[1133.00s -> 1134.00s]  Any ideas?
[1135.00s -> 1138.00s]  Everyone's too scared to ask a question?
[1138.00s -> 1139.00s]  Answer?
[1145.00s -> 1147.00s]  Yeah, so the answer is that
[1147.00s -> 1149.00s]  if we jiggle the scores
[1149.00s -> 1151.00s]  for this car image a little bit,
[1151.00s -> 1152.00s]  the loss will not change.
[1152.00s -> 1154.00s]  So the SVM loss, remember,
[1154.00s -> 1155.00s]  the only thing it cares about
[1155.00s -> 1157.00s]  is getting the correct score
[1157.00s -> 1159.00s]  to be greater than one more
[1159.00s -> 1160.00s]  than the incorrect scores.
[1160.00s -> 1161.00s]  But in this case,
[1161.00s -> 1163.00s]  the car score is already
[1163.00s -> 1165.00s]  quite a bit larger than the others.
[1165.00s -> 1168.00s]  So if the scores for this example
[1168.00s -> 1169.00s]  change just a little bit,
[1169.00s -> 1172.00s]  this margin of one will still be retained,
[1172.00s -> 1173.00s]  and the loss will not change.
[1173.00s -> 1175.00s]  We'll still get zero loss.
[1176.00s -> 1177.00s]  The next question,
[1177.00s -> 1181.00s]  what's the min and max possible loss for SVM?
[1184.00s -> 1186.00s]  Well, I hear some murmurs.
[1186.00s -> 1188.00s]  So the minimum loss is zero,
[1188.00s -> 1189.00s]  because if you can imagine
[1189.00s -> 1191.00s]  that across all the classes,
[1191.00s -> 1192.00s]  if our correct score was much larger,
[1192.00s -> 1194.00s]  then we'll incur zero loss
[1194.00s -> 1195.00s]  across all the classes,
[1195.00s -> 1197.00s]  and it'll be zero.
[1197.00s -> 1198.00s]  And if you think back to this
[1198.00s -> 1200.00s]  hinge loss plot that we had,
[1200.00s -> 1201.00s]  then you can see that
[1201.00s -> 1203.00s]  if the correct score
[1203.00s -> 1204.00s]  goes very, very negative,
[1204.00s -> 1206.00s]  then we could incur
[1206.00s -> 1208.00s]  potentially infinite loss.
[1208.00s -> 1209.00s]  So the min is zero,
[1209.00s -> 1210.00s]  and the max is infinity.
[1211.00s -> 1212.00s]  Another question.
[1212.00s -> 1214.00s]  Sort of when you initialize these things
[1214.00s -> 1216.00s]  and start training from scratch,
[1216.00s -> 1218.00s]  usually you kind of initialize W
[1218.00s -> 1220.00s]  with some small random values.
[1220.00s -> 1221.00s]  As a result,
[1221.00s -> 1223.00s]  your scores tend to be sort of small,
[1223.00s -> 1224.00s]  uniform, random values
[1224.00s -> 1226.00s]  at the beginning of training.
[1226.00s -> 1227.00s]  And then the question is that
[1227.00s -> 1228.00s]  if all of your Ss,
[1228.00s -> 1229.00s]  if all of the scores
[1229.00s -> 1230.00s]  are approximately zero
[1230.00s -> 1231.00s]  and approximately equal,
[1231.00s -> 1233.00s]  then what kind of loss do you expect
[1233.00s -> 1237.00s]  when you're using the multi-class SVM?
[1238.00s -> 1240.00s]  Yeah, so the answer is
[1240.00s -> 1242.00s]  number of classes minus one,
[1242.00s -> 1244.00s]  because remember that
[1244.00s -> 1246.00s]  if we're looping over
[1246.00s -> 1248.00s]  all of the incorrect classes,
[1248.00s -> 1249.00s]  so we're looping over
[1249.00s -> 1250.00s]  C minus one classes
[1250.00s -> 1251.00s]  within each of those classes,
[1251.00s -> 1253.00s]  the two Ss will be about the same,
[1253.00s -> 1255.00s]  so we'll get a loss of one
[1255.00s -> 1256.00s]  because of the margin,
[1256.00s -> 1257.00s]  and we'll get C minus one.
[1257.00s -> 1259.00s]  So this is actually kind of useful
[1259.00s -> 1262.00s]  because this is a useful debugging strategy
[1262.00s -> 1263.00s]  when you're using these things,
[1263.00s -> 1265.00s]  that when you start off training,
[1265.00s -> 1266.00s]  you should think about
[1266.00s -> 1268.00s]  what you expect your loss to be,
[1268.00s -> 1270.00s]  and if the loss you actually see
[1270.00s -> 1271.00s]  at the start of training
[1271.00s -> 1272.00s]  at that first iteration
[1272.00s -> 1274.00s]  is not equal to C minus one in this case,
[1274.00s -> 1275.00s]  that means you probably have a bug
[1275.00s -> 1277.00s]  and you should go check your code.
[1277.00s -> 1279.00s]  So this is actually kind of a useful thing
[1279.00s -> 1281.00s]  to be checking in practice.
[1282.00s -> 1283.00s]  Another question,
[1283.00s -> 1284.00s]  what happens if,
[1284.00s -> 1286.00s]  so I said we're summing an SVM
[1286.00s -> 1289.00s]  all over the incorrect classes.
[1289.00s -> 1290.00s]  What happens if this sum
[1290.00s -> 1291.00s]  is also over the correct class,
[1291.00s -> 1293.00s]  if we just go over everything?
[1296.00s -> 1297.00s]  Yeah, so the answer is
[1297.00s -> 1299.00s]  that the loss increases by one,
[1299.00s -> 1301.00s]  and I think the reason
[1301.00s -> 1302.00s]  that we do this in practice
[1302.00s -> 1304.00s]  is because normally loss of zero
[1304.00s -> 1306.00s]  has this nice interpretation
[1306.00s -> 1307.00s]  that you're not losing at all,
[1307.00s -> 1308.00s]  so that's nice.
[1309.00s -> 1312.00s]  So I think your answers wouldn't really change.
[1312.00s -> 1314.00s]  You would end up finding the same classifier
[1314.00s -> 1316.00s]  if you actually looped over all the categories,
[1316.00s -> 1318.00s]  but just by conventions,
[1318.00s -> 1320.00s]  it'd omit the correct class
[1320.00s -> 1323.00s]  so that our minimum loss is zero.
[1325.00s -> 1326.00s]  So another question,
[1326.00s -> 1328.00s]  what if we used mean instead of sum here?
[1331.00s -> 1333.00s]  Yeah, the answer is that it doesn't change.
[1333.00s -> 1334.00s]  So the number of classes
[1334.00s -> 1336.00s]  is gonna be fixed ahead of time
[1336.00s -> 1337.00s]  when we select our data set,
[1337.00s -> 1338.00s]  so that's just rescaling
[1338.00s -> 1340.00s]  the whole loss function by a constant,
[1340.00s -> 1341.00s]  so it doesn't really matter.
[1341.00s -> 1342.00s]  It'll sort of wash out
[1342.00s -> 1344.00s]  with all the other scale things
[1344.00s -> 1345.00s]  because we don't actually care
[1345.00s -> 1347.00s]  about the true values of the scores,
[1347.00s -> 1350.00s]  or the true value of the loss, for that matter.
[1350.00s -> 1352.00s]  So now here's another example.
[1352.00s -> 1354.00s]  What if we changed this loss formulation
[1354.00s -> 1356.00s]  and we actually added a square term
[1356.00s -> 1358.00s]  on top of this max?
[1358.00s -> 1360.00s]  Would this end up being the same problem
[1360.00s -> 1362.00s]  or would this be a different classification algorithm?
[1364.00s -> 1365.00s]  Yeah, so this would be different.
[1365.00s -> 1366.00s]  So here the idea is
[1366.00s -> 1368.00s]  that we're kind of changing the trade-offs
[1368.00s -> 1369.00s]  between good and badness
[1369.00s -> 1371.00s]  in kind of a non-linear way.
[1371.00s -> 1372.00s]  So this would end up actually
[1372.00s -> 1374.00s]  computing a different loss function.
[1374.00s -> 1376.00s]  This idea of a squared hinge loss
[1376.00s -> 1379.00s]  actually does get used sometimes in practice,
[1379.00s -> 1380.00s]  so that's kind of another trick
[1380.00s -> 1381.00s]  to have in your bag
[1381.00s -> 1382.00s]  when you're making up
[1382.00s -> 1383.00s]  your own loss functions
[1383.00s -> 1385.00s]  for your own problems.
[1385.00s -> 1386.00s]  So now you'll end up,
[1386.00s -> 1388.00s]  oh, was there a question?
[1392.00s -> 1393.00s]  Yeah, so the question is
[1393.00s -> 1394.00s]  why would you ever consider
[1394.00s -> 1395.00s]  using a squared loss
[1395.00s -> 1397.00s]  instead of a non-squared loss?
[1397.00s -> 1399.00s]  The whole point of a loss function
[1399.00s -> 1400.00s]  is to kind of quantify
[1400.00s -> 1403.00s]  how bad are different mistakes.
[1403.00s -> 1404.00s]  If the classifier is making
[1404.00s -> 1405.00s]  different sorts of mistakes,
[1405.00s -> 1406.00s]  how do we weigh off
[1406.00s -> 1407.00s]  the different trade-offs
[1407.00s -> 1409.00s]  between different types of mistakes
[1409.00s -> 1410.00s]  the classifier might make?
[1410.00s -> 1412.00s]  So if you're using a squared loss,
[1412.00s -> 1413.00s]  that sort of says
[1413.00s -> 1416.00s]  that things that are very, very bad
[1416.00s -> 1418.00s]  are now going to be squared bad,
[1418.00s -> 1419.00s]  so that's really, really bad.
[1419.00s -> 1420.00s]  We don't want anything
[1420.00s -> 1424.00s]  that's totally catastrophically misclassified.
[1424.00s -> 1426.00s]  Whereas if you're using this hinge loss,
[1426.00s -> 1428.00s]  then we don't actually care
[1428.00s -> 1430.00s]  between being a little bit wrong
[1430.00s -> 1433.00s]  and being a lot wrong.
[1433.00s -> 1435.00s]  If an example is a lot wrong
[1435.00s -> 1436.00s]  and we increase it
[1436.00s -> 1437.00s]  and make it a little bit less wrong,
[1437.00s -> 1439.00s]  that's kind of the same goodness
[1439.00s -> 1441.00s]  as an example which was
[1441.00s -> 1442.00s]  only a little bit wrong
[1442.00s -> 1443.00s]  and then increasing it
[1443.00s -> 1445.00s]  to be a little bit more right.
[1445.00s -> 1447.00s]  So that's a little bit hand-wavy,
[1447.00s -> 1448.00s]  but this idea of using
[1448.00s -> 1449.00s]  a linear versus a square
[1449.00s -> 1451.00s]  is a way to quantify
[1451.00s -> 1452.00s]  how much we care
[1452.00s -> 1454.00s]  about different categories of errors.
[1454.00s -> 1455.00s]  And this is definitely something
[1455.00s -> 1456.00s]  that you should think about
[1456.00s -> 1457.00s]  when you're actually
[1457.00s -> 1458.00s]  applying these things in practice,
[1458.00s -> 1459.00s]  because the loss function
[1459.00s -> 1461.00s]  is the way that you tell your algorithm
[1461.00s -> 1463.00s]  what types of errors you care about
[1463.00s -> 1464.00s]  and what types of errors
[1464.00s -> 1466.00s]  it should trade off against.
[1466.00s -> 1467.00s]  So that's actually
[1467.00s -> 1468.00s]  super important in practice
[1468.00s -> 1472.00s]  depending on your application.
[1472.00s -> 1474.00s]  So here's just a little snippet
[1474.00s -> 1476.00s]  of sort of vectorized code in NumPy,
[1476.00s -> 1477.00s]  and you'll end up
[1477.00s -> 1479.00s]  implementing something like this
[1479.00s -> 1481.00s]  for the first assignment,
[1481.00s -> 1482.00s]  but this kind of gives you
[1482.00s -> 1484.00s]  the sense that this sum
[1484.00s -> 1485.00s]  is actually pretty easy
[1485.00s -> 1486.00s]  to implement in NumPy.
[1486.00s -> 1487.00s]  It only takes a couple lines
[1487.00s -> 1489.00s]  of vectorized code.
[1489.00s -> 1490.00s]  And you can see in practice
[1490.00s -> 1492.00s]  one nice trick is that
[1492.00s -> 1494.00s]  we can actually go in here
[1494.00s -> 1496.00s]  and zero out the margins
[1496.00s -> 1498.00s]  corresponding to the correct class,
[1498.00s -> 1499.00s]  and that makes it easy
[1499.00s -> 1501.00s]  to then just...
[1501.00s -> 1503.00s]  That's sort of one nice vectorized trick
[1503.00s -> 1504.00s]  to skip, like,
[1504.00s -> 1506.00s]  iterate over all but one class.
[1506.00s -> 1507.00s]  You just kind of zero out
[1507.00s -> 1508.00s]  the one that you want to skip
[1508.00s -> 1509.00s]  and then compute the sum anyway.
[1509.00s -> 1510.00s]  So that's a nice trick
[1510.00s -> 1511.00s]  you might consider using
[1511.00s -> 1513.00s]  on the assignment.
[1513.00s -> 1515.00s]  So now, another kind of question
[1515.00s -> 1517.00s]  about this loss function.
[1517.00s -> 1518.00s]  Suppose that you were lucky enough
[1518.00s -> 1520.00s]  to find a W that has loss of zero.
[1520.00s -> 1521.00s]  Like, you're not losing at all,
[1521.00s -> 1522.00s]  you're totally winning,
[1522.00s -> 1524.00s]  this loss function is crushing it,
[1524.00s -> 1526.00s]  but then there's a question.
[1526.00s -> 1527.00s]  Is this W unique,
[1527.00s -> 1529.00s]  or were there other Ws
[1529.00s -> 1534.00s]  that could also have achieved zero loss?
[1534.00s -> 1535.00s]  Answer?
[1535.00s -> 1536.00s]  Yeah, so there are definitely
[1536.00s -> 1537.00s]  other Ws.
[1537.00s -> 1539.00s]  And in particular,
[1539.00s -> 1540.00s]  because we talked a little bit
[1540.00s -> 1542.00s]  about this thing
[1542.00s -> 1543.00s]  of scaling the whole problem
[1543.00s -> 1545.00s]  up or down, depending on W.
[1545.00s -> 1546.00s]  So you could actually
[1546.00s -> 1548.00s]  take W multiplied by two,
[1548.00s -> 1551.00s]  and this doubled W,
[1551.00s -> 1552.00s]  is it a quad U then?
[1552.00s -> 1553.00s]  I don't know.
[1553.00s -> 1556.00s]  This would also achieve zero loss.
[1556.00s -> 1558.00s]  So as a kind of concrete example
[1558.00s -> 1559.00s]  of this, you can go back
[1559.00s -> 1560.00s]  to your favorite example
[1560.00s -> 1561.00s]  and maybe work through the numbers
[1561.00s -> 1562.00s]  a little bit later,
[1562.00s -> 1564.00s]  but if you're taking W
[1564.00s -> 1565.00s]  and we double W,
[1565.00s -> 1567.00s]  then the margins between
[1567.00s -> 1568.00s]  the correct and incorrect scores
[1568.00s -> 1570.00s]  will also double.
[1570.00s -> 1571.00s]  So that means that
[1571.00s -> 1572.00s]  if all these margins
[1572.00s -> 1574.00s]  were already greater than one,
[1574.00s -> 1575.00s]  and we doubled them,
[1575.00s -> 1576.00s]  they're still going to be
[1576.00s -> 1577.00s]  greater than one,
[1577.00s -> 1580.00s]  so you'll still have zero loss.
[1580.00s -> 1582.00s]  And this is kind of interesting, right?
[1582.00s -> 1584.00s]  Because if our loss function
[1584.00s -> 1585.00s]  is the way that we tell
[1585.00s -> 1587.00s]  our classifier which W we want
[1587.00s -> 1589.00s]  and which W we care about,
[1589.00s -> 1590.00s]  this is a little bit weird.
[1590.00s -> 1592.00s]  Now there's this inconsistency,
[1592.00s -> 1594.00s]  and how is the classifier
[1594.00s -> 1595.00s]  to choose between these
[1595.00s -> 1597.00s]  different versions of W
[1597.00s -> 1600.00s]  that all achieve zero loss?
[1600.00s -> 1601.00s]  And that's because
[1601.00s -> 1602.00s]  what we've done here
[1602.00s -> 1604.00s]  is written down only a loss
[1604.00s -> 1605.00s]  in terms of the data,
[1605.00s -> 1609.00s]  and we've only told our classifier
[1609.00s -> 1610.00s]  that it should try to find a W
[1610.00s -> 1612.00s]  that fits the training data.
[1612.00s -> 1613.00s]  But really in practice,
[1613.00s -> 1615.00s]  we don't actually care that much
[1615.00s -> 1616.00s]  about fitting the training data.
[1616.00s -> 1618.00s]  The whole point of machine learning
[1618.00s -> 1620.00s]  is that we use the training data
[1620.00s -> 1621.00s]  to find some classifier,
[1621.00s -> 1623.00s]  and then we'll apply that thing
[1623.00s -> 1624.00s]  on test data.
[1624.00s -> 1625.00s]  So we don't really care
[1625.00s -> 1627.00s]  about the training data performance.
[1627.00s -> 1628.00s]  We really care about the performance
[1628.00s -> 1631.00s]  of this classifier on test data.
[1631.00s -> 1632.00s]  So as a result,
[1632.00s -> 1633.00s]  if the only thing
[1633.00s -> 1635.00s]  we're telling our classifier to do
[1635.00s -> 1636.00s]  is fit the training data,
[1636.00s -> 1638.00s]  then we can kind of lead ourselves
[1638.00s -> 1640.00s]  into some of these weird situations sometimes
[1640.00s -> 1641.00s]  where the classifier
[1641.00s -> 1644.00s]  might have unintuitive behavior.
[1644.00s -> 1646.00s]  So kind of a concrete canonical example
[1646.00s -> 1648.00s]  of this sort of thing.
[1648.00s -> 1649.00s]  By the way,
[1649.00s -> 1650.00s]  this is not linear classification anymore.
[1650.00s -> 1651.00s]  This is a little bit
[1651.00s -> 1652.00s]  of a more general
[1652.00s -> 1653.00s]  machine learning concept
[1653.00s -> 1654.00s]  is that suppose we have
[1654.00s -> 1656.00s]  this data set of blue points,
[1656.00s -> 1657.00s]  and we're gonna fit some curve
[1657.00s -> 1659.00s]  to the training data,
[1659.00s -> 1660.00s]  the blue points.
[1660.00s -> 1661.00s]  Then if the only thing
[1661.00s -> 1663.00s]  we've told our classifier to do
[1663.00s -> 1665.00s]  is to try and fit the training data,
[1665.00s -> 1666.00s]  it might go in and have
[1666.00s -> 1667.00s]  very wiggly curves
[1667.00s -> 1668.00s]  to try to perfectly classify
[1668.00s -> 1670.00s]  all of the training data points.
[1670.00s -> 1671.00s]  But this is bad
[1671.00s -> 1672.00s]  because we don't actually care
[1672.00s -> 1674.00s]  about this performance.
[1674.00s -> 1675.00s]  We care about the performance
[1675.00s -> 1677.00s]  on the test data.
[1677.00s -> 1679.00s]  So now if we have some new data come in
[1679.00s -> 1681.00s]  that sort of follows the same trend,
[1681.00s -> 1682.00s]  then this very wiggly blue line
[1682.00s -> 1684.00s]  is gonna be totally wrong.
[1684.00s -> 1685.00s]  And in fact,
[1685.00s -> 1686.00s]  what we probably would have
[1686.00s -> 1687.00s]  preferred the classifier to do
[1687.00s -> 1688.00s]  was maybe predict
[1688.00s -> 1689.00s]  this straight green line
[1689.00s -> 1691.00s]  rather than this very complex wiggly line
[1691.00s -> 1695.00s]  to perfectly fit all the training data.
[1695.00s -> 1696.00s]  And this is kind of
[1696.00s -> 1698.00s]  a core fundamental problem
[1698.00s -> 1699.00s]  in machine learning.
[1699.00s -> 1701.00s]  And the way we usually solve it
[1701.00s -> 1703.00s]  is this concept of regularization.
[1703.00s -> 1704.00s]  So here we're gonna add
[1704.00s -> 1705.00s]  an additional term
[1705.00s -> 1706.00s]  to the loss function.
[1706.00s -> 1708.00s]  In addition to the data loss,
[1708.00s -> 1709.00s]  which will tell our classifier
[1709.00s -> 1711.00s]  that it should fit the training data,
[1711.00s -> 1712.00s]  we'll also typically add
[1712.00s -> 1714.00s]  another term to the loss function
[1714.00s -> 1716.00s]  called a regularization term,
[1716.00s -> 1718.00s]  which encourages the model
[1718.00s -> 1720.00s]  to somehow pick a simpler w,
[1720.00s -> 1722.00s]  whereas the concept of simple
[1722.00s -> 1723.00s]  kind of depends on the task
[1723.00s -> 1724.00s]  and the model.
[1724.00s -> 1727.00s]  But the whole idea is that
[1727.00s -> 1728.00s]  there's this whole idea
[1728.00s -> 1729.00s]  of Occam's Razor,
[1729.00s -> 1731.00s]  which is kind of this fundamental idea
[1731.00s -> 1733.00s]  in scientific discovery more broadly,
[1733.00s -> 1734.00s]  which is that
[1734.00s -> 1735.00s]  if you have many different
[1735.00s -> 1736.00s]  competing hypotheses
[1736.00s -> 1738.00s]  that could explain your observations,
[1738.00s -> 1739.00s]  you should generally prefer
[1739.00s -> 1740.00s]  the simpler one
[1740.00s -> 1741.00s]  because that's the explanation
[1741.00s -> 1743.00s]  that is more likely to generalize
[1743.00s -> 1745.00s]  to new observations in the future.
[1745.00s -> 1747.00s]  And the way that we operationalize
[1747.00s -> 1749.00s]  this intuition in machine learning
[1749.00s -> 1750.00s]  is typically through
[1750.00s -> 1752.00s]  some explicit regularization penalty
[1752.00s -> 1755.00s]  that's often written down as r.
[1755.00s -> 1758.00s]  So then your standard loss function
[1758.00s -> 1760.00s]  usually has these two terms,
[1760.00s -> 1762.00s]  a data loss and a regularization loss,
[1762.00s -> 1764.00s]  and there's some hyperparameter here, lambda,
[1764.00s -> 1766.00s]  that trades off between the two.
[1766.00s -> 1767.00s]  And we talked about
[1767.00s -> 1769.00s]  hyperparameters and cross-validation
[1769.00s -> 1770.00s]  in the last lecture,
[1770.00s -> 1773.00s]  so this regularization hyperparameter lambda
[1773.00s -> 1775.00s]  will be one of the more important ones
[1775.00s -> 1776.00s]  that you'll need to tune
[1776.00s -> 1779.00s]  when training these models in practice.
[1779.00s -> 1780.00s]  Question?
[1790.00s -> 1791.00s]  Yeah, so the question is
[1791.00s -> 1792.00s]  what's the connection
[1792.00s -> 1793.00s]  between this lambda rw term
[1793.00s -> 1795.00s]  and actually forcing this wiggly line
[1795.00s -> 1798.00s]  to become a straight green line?
[1798.00s -> 1799.00s]  So you can sort of,
[1799.00s -> 1800.00s]  I didn't want to go through
[1800.00s -> 1801.00s]  the derivation on this
[1801.00s -> 1802.00s]  because I thought it would
[1802.00s -> 1803.00s]  sort of lead us too far astray,
[1803.00s -> 1804.00s]  but you can imagine
[1804.00s -> 1806.00s]  maybe you're doing a regression problem
[1806.00s -> 1808.00s]  in terms of different basis functions,
[1808.00s -> 1809.00s]  like polynomial basis functions,
[1809.00s -> 1811.00s]  and if you're adding this regression penalty,
[1811.00s -> 1813.00s]  you can encourage the model,
[1813.00s -> 1815.00s]  like maybe the model has access
[1815.00s -> 1817.00s]  to polynomials of very high degree,
[1817.00s -> 1818.00s]  but through this regression term,
[1818.00s -> 1819.00s]  you could encourage the model
[1819.00s -> 1821.00s]  to prefer polynomials of lower degree
[1821.00s -> 1823.00s]  if they fit the data properly
[1823.00s -> 1824.00s]  or if they fit the data
[1824.00s -> 1825.00s]  relatively well.
[1825.00s -> 1827.00s]  So you could imagine
[1827.00s -> 1829.00s]  there's sort of two ways to do this.
[1829.00s -> 1830.00s]  Either you can constrain
[1830.00s -> 1831.00s]  your model class
[1831.00s -> 1832.00s]  to just not contain
[1832.00s -> 1833.00s]  the more powerful,
[1833.00s -> 1834.00s]  more complex models,
[1834.00s -> 1835.00s]  or you can kind of
[1835.00s -> 1836.00s]  add this soft penalty
[1836.00s -> 1839.00s]  where the model still has access
[1839.00s -> 1841.00s]  to more complex models,
[1841.00s -> 1842.00s]  maybe high degree polynomials
[1842.00s -> 1843.00s]  in this case,
[1843.00s -> 1844.00s]  but you add this soft constraint
[1844.00s -> 1845.00s]  saying that
[1845.00s -> 1846.00s]  if you want to use
[1846.00s -> 1848.00s]  these more complex models,
[1848.00s -> 1849.00s]  you need to overcome
[1849.00s -> 1851.00s]  this penalty for using their complexity.
[1851.00s -> 1853.00s]  So that's kind of the connection here,
[1853.00s -> 1855.00s]  that is not quite linear classification,
[1855.00s -> 1856.00s]  but this is the kind of picture
[1856.00s -> 1858.00s]  that many people have in mind
[1858.00s -> 1859.00s]  when they think about
[1859.00s -> 1861.00s]  regularization at least.
[1863.00s -> 1864.00s]  So there's actually
[1864.00s -> 1865.00s]  a lot of different types
[1865.00s -> 1866.00s]  of regularization
[1866.00s -> 1867.00s]  that get used in practice.
[1867.00s -> 1868.00s]  The most common one
[1868.00s -> 1870.00s]  is probably L2 regularization
[1870.00s -> 1871.00s]  or weight decay,
[1871.00s -> 1872.00s]  but there's a lot of other ones
[1872.00s -> 1874.00s]  that you might see.
[1874.00s -> 1876.00s]  So this L2 regularization
[1876.00s -> 1878.00s]  is just the Euclidean norm
[1878.00s -> 1880.00s]  of this weight vector w,
[1880.00s -> 1882.00s]  or sometimes the squared norm,
[1882.00s -> 1884.00s]  or sometimes half the squared norm
[1884.00s -> 1885.00s]  because it makes your derivatives
[1885.00s -> 1887.00s]  work out a little bit nicer,
[1887.00s -> 1889.00s]  but the idea of L2 regularization
[1889.00s -> 1890.00s]  is you're just penalizing
[1890.00s -> 1891.00s]  the Euclidean norm
[1891.00s -> 1893.00s]  of this weight vector.
[1893.00s -> 1894.00s]  You might also sometimes see
[1894.00s -> 1896.00s]  L1 regularization
[1896.00s -> 1897.00s]  where we're penalizing
[1897.00s -> 1899.00s]  the L1 norm of the weight vector,
[1899.00s -> 1901.00s]  and the L1 regularization
[1901.00s -> 1903.00s]  has some nice properties
[1903.00s -> 1904.00s]  like encouraging sparsity
[1904.00s -> 1906.00s]  in this matrix w.
[1906.00s -> 1907.00s]  Some other things
[1907.00s -> 1908.00s]  you might see
[1908.00s -> 1909.00s]  would be this elastic
[1909.00s -> 1910.00s]  net regularization
[1910.00s -> 1911.00s]  which is some combination
[1911.00s -> 1913.00s]  of L1 and L2.
[1913.00s -> 1914.00s]  You sometimes see
[1914.00s -> 1916.00s]  max norm regularization,
[1916.00s -> 1917.00s]  like penalizing the max norm
[1917.00s -> 1921.00s]  rather than the L1 or L2 norm.
[1921.00s -> 1922.00s]  But these sorts
[1922.00s -> 1923.00s]  of regularizations
[1923.00s -> 1925.00s]  are things that you kind of see
[1925.00s -> 1926.00s]  not just in deep learning
[1926.00s -> 1927.00s]  but across many areas
[1927.00s -> 1928.00s]  of machine learning
[1928.00s -> 1929.00s]  and even optimization
[1929.00s -> 1931.00s]  more broadly.
[1931.00s -> 1933.00s]  But in some later lectures,
[1933.00s -> 1934.00s]  we'll also see
[1934.00s -> 1935.00s]  some types of regularization
[1935.00s -> 1936.00s]  that are more specific
[1936.00s -> 1938.00s]  to deep learning.
[1938.00s -> 1939.00s]  For example, dropout
[1939.00s -> 1940.00s]  would be in a couple lectures
[1940.00s -> 1941.00s]  or batch normalization,
[1941.00s -> 1943.00s]  stochastic depth.
[1943.00s -> 1944.00s]  These things get kind of crazy
[1944.00s -> 1945.00s]  in recent years.
[1945.00s -> 1946.00s]  But the whole idea
[1946.00s -> 1947.00s]  of regularization
[1947.00s -> 1948.00s]  is just anything
[1948.00s -> 1949.00s]  that you do to your model
[1949.00s -> 1951.00s]  that sort of penalizes
[1951.00s -> 1952.00s]  somehow the complexity
[1952.00s -> 1953.00s]  of the model
[1953.00s -> 1954.00s]  rather than explicitly trying
[1954.00s -> 1957.00s]  to fit the training data.
[1957.00s -> 1958.00s]  Question?
[1965.00s -> 1966.00s]  Yeah, so the question is
[1966.00s -> 1968.00s]  how does the L2 regularization
[1968.00s -> 1969.00s]  measure the complexity
[1969.00s -> 1970.00s]  of a model?
[1970.00s -> 1971.00s]  Thankfully, we have an example
[1971.00s -> 1972.00s]  of that right here.
[1972.00s -> 1975.00s]  Maybe we can walk through.
[1975.00s -> 1976.00s]  So here, we maybe have
[1976.00s -> 1978.00s]  some training example X,
[1978.00s -> 1979.00s]  and there's two different Ws
[1979.00s -> 1980.00s]  that we're considering.
[1980.00s -> 1981.00s]  So X is just this vector
[1981.00s -> 1982.00s]  of four ones,
[1982.00s -> 1983.00s]  and we're considering
[1983.00s -> 1984.00s]  these two different
[1984.00s -> 1987.00s]  possibilities for W.
[1987.00s -> 1988.00s]  One is a one in the first.
[1988.00s -> 1989.00s]  One has a single one
[1989.00s -> 1990.00s]  and three zeros,
[1990.00s -> 1992.00s]  and the other has this 0.25
[1992.00s -> 1993.00s]  spread across
[1993.00s -> 1994.00s]  the four different entries.
[1994.00s -> 1995.00s]  And now, when we're doing
[1995.00s -> 1996.00s]  linear classification,
[1996.00s -> 1997.00s]  we're really taking
[1997.00s -> 1998.00s]  dot products between
[1998.00s -> 2000.00s]  our X and our W.
[2000.00s -> 2002.00s]  So in terms of linear
[2002.00s -> 2003.00s]  classification,
[2003.00s -> 2004.00s]  these two Ws are the same
[2004.00s -> 2005.00s]  because they give
[2005.00s -> 2006.00s]  the same result
[2006.00s -> 2008.00s]  when dot producted with X.
[2008.00s -> 2009.00s]  But now, the question is,
[2009.00s -> 2010.00s]  if you look at these
[2010.00s -> 2011.00s]  two examples,
[2011.00s -> 2012.00s]  then which one
[2012.00s -> 2015.00s]  would L2 regression prefer?
[2015.00s -> 2018.00s]  Yeah, so L2 regression
[2018.00s -> 2019.00s]  would prefer W2
[2019.00s -> 2021.00s]  because it has a smaller norm.
[2021.00s -> 2023.00s]  So the answer is that
[2023.00s -> 2025.00s]  the L2 regression kind of
[2025.00s -> 2026.00s]  measures complexity
[2026.00s -> 2027.00s]  of the classifier
[2027.00s -> 2028.00s]  in this kind of relatively
[2028.00s -> 2029.00s]  coarse way,
[2029.00s -> 2031.00s]  where the idea is that,
[2031.00s -> 2033.00s]  remember the Ws
[2033.00s -> 2034.00s]  in linear classification
[2034.00s -> 2035.00s]  had this interpretation
[2035.00s -> 2037.00s]  of how much does this
[2037.00s -> 2039.00s]  value of the vector X
[2039.00s -> 2040.00s]  correspond to this
[2040.00s -> 2042.00s]  output class?
[2042.00s -> 2043.00s]  So L2 regularization
[2043.00s -> 2044.00s]  is saying that it
[2044.00s -> 2045.00s]  kind of prefers
[2045.00s -> 2046.00s]  to spread that influence
[2046.00s -> 2047.00s]  across all the different
[2047.00s -> 2049.00s]  values in X.
[2049.00s -> 2050.00s]  Maybe this might be
[2050.00s -> 2051.00s]  more robust
[2051.00s -> 2052.00s]  in case you come up
[2052.00s -> 2054.00s]  with Xs that sort of vary,
[2054.00s -> 2055.00s]  then our decisions
[2055.00s -> 2056.00s]  are kind of spread out
[2056.00s -> 2057.00s]  and depend on the entire
[2057.00s -> 2058.00s]  X vector,
[2058.00s -> 2059.00s]  rather than depending
[2059.00s -> 2060.00s]  only on certain elements
[2060.00s -> 2062.00s]  of the X vector.
[2062.00s -> 2063.00s]  And by the way,
[2063.00s -> 2064.00s]  L1 regularization
[2064.00s -> 2065.00s]  kind of has this
[2065.00s -> 2066.00s]  opposite interpretation.
[2066.00s -> 2067.00s]  So actually,
[2067.00s -> 2068.00s]  if we were using
[2068.00s -> 2069.00s]  L1 regularization,
[2069.00s -> 2070.00s]  then we would actually
[2070.00s -> 2072.00s]  prefer W1 over W2
[2072.00s -> 2074.00s]  because L1 regularization
[2074.00s -> 2075.00s]  has this different
[2075.00s -> 2076.00s]  notion of complexity,
[2076.00s -> 2077.00s]  saying that maybe
[2077.00s -> 2079.00s]  the model is less complex
[2079.00s -> 2080.00s]  if it only has,
[2080.00s -> 2082.00s]  maybe we measure
[2082.00s -> 2083.00s]  model complexity
[2083.00s -> 2084.00s]  by the number of zeros
[2084.00s -> 2085.00s]  in the weight vector.
[2085.00s -> 2087.00s]  So the question of
[2087.00s -> 2088.00s]  how do we measure complexity
[2088.00s -> 2089.00s]  and how does L2
[2089.00s -> 2091.00s]  measure complexity,
[2091.00s -> 2092.00s]  they're kind of
[2092.00s -> 2093.00s]  problem dependent,
[2093.00s -> 2094.00s]  and you kind of
[2094.00s -> 2095.00s]  have to think about
[2095.00s -> 2096.00s]  for your particular setup,
[2096.00s -> 2097.00s]  for your particular
[2097.00s -> 2098.00s]  model and data,
[2098.00s -> 2099.00s]  how do you think
[2099.00s -> 2100.00s]  that complexity
[2100.00s -> 2101.00s]  should be measured
[2101.00s -> 2102.00s]  on this task?
[2102.00s -> 2103.00s]  Question?
[2108.00s -> 2109.00s]  Oh yes, you're right.
[2109.00s -> 2110.00s]  So in this case,
[2110.00s -> 2111.00s]  L1 is actually
[2111.00s -> 2113.00s]  the same between these two.
[2113.00s -> 2114.00s]  But maybe if we set
[2114.00s -> 2115.00s]  the W,
[2115.00s -> 2116.00s]  you could construct
[2116.00s -> 2118.00s]  a similar example to this
[2118.00s -> 2119.00s]  where W1 would be
[2119.00s -> 2121.00s]  preferred by L1 regularization.
[2121.00s -> 2122.00s]  I guess the general
[2122.00s -> 2124.00s]  intuition behind L1
[2124.00s -> 2125.00s]  is that it generally
[2125.00s -> 2126.00s]  prefers sparse solutions,
[2126.00s -> 2128.00s]  that it kind of drives
[2128.00s -> 2129.00s]  all your entries
[2129.00s -> 2130.00s]  of W to zero
[2130.00s -> 2131.00s]  for most of the entries,
[2131.00s -> 2132.00s]  except for a couple
[2132.00s -> 2133.00s]  where it's allowed
[2133.00s -> 2134.00s]  to deviate from zero.
[2134.00s -> 2135.00s]  So kind of the way
[2135.00s -> 2137.00s]  of measuring complexity
[2137.00s -> 2138.00s]  for L1 is maybe
[2138.00s -> 2139.00s]  the number of
[2139.00s -> 2140.00s]  non-zero entries,
[2140.00s -> 2141.00s]  and then for L2,
[2141.00s -> 2142.00s]  it thinks that
[2142.00s -> 2143.00s]  things that spread
[2143.00s -> 2144.00s]  the W across all the values
[2144.00s -> 2145.00s]  are less complex.
[2145.00s -> 2146.00s]  So it kind of depends
[2146.00s -> 2147.00s]  on your data,
[2147.00s -> 2148.00s]  depends on your problem.
[2148.00s -> 2149.00s]  Oh, and by the way,
[2149.00s -> 2151.00s]  if you're a hardcore Bayesian,
[2151.00s -> 2153.00s]  then using L2 regularization
[2153.00s -> 2154.00s]  has kind of this
[2154.00s -> 2155.00s]  nice interpretation
[2155.00s -> 2156.00s]  of map inference
[2156.00s -> 2157.00s]  under a Gaussian prior
[2157.00s -> 2158.00s]  on the parameter vector.
[2158.00s -> 2159.00s]  I think there was
[2159.00s -> 2160.00s]  a homework problem
[2160.00s -> 2161.00s]  about that in 229,
[2161.00s -> 2162.00s]  but we won't talk about
[2162.00s -> 2163.00s]  that for the rest
[2163.00s -> 2164.00s]  of the quarter.
[2164.00s -> 2166.00s]  Okay, so that's sort of
[2166.00s -> 2168.00s]  my long deep dive
[2168.00s -> 2169.00s]  into the multiclass
[2169.00s -> 2171.00s]  SVM loss.
[2171.00s -> 2172.00s]  Question?
[2194.00s -> 2195.00s]  Yeah, so the question
[2195.00s -> 2197.00s]  is that this is still not,
[2197.00s -> 2198.00s]  adding regularization
[2198.00s -> 2199.00s]  is not going to change
[2199.00s -> 2200.00s]  the hypothesis class.
[2200.00s -> 2201.00s]  This is not going to change
[2201.00s -> 2202.00s]  us away from
[2202.00s -> 2203.00s]  a linear classifier.
[2203.00s -> 2205.00s]  So the idea is that
[2205.00s -> 2206.00s]  maybe this example
[2206.00s -> 2208.00s]  of this polynomial regression
[2208.00s -> 2209.00s]  is definitely not
[2209.00s -> 2210.00s]  linear regression.
[2210.00s -> 2211.00s]  That could be seen
[2211.00s -> 2212.00s]  as linear regression
[2212.00s -> 2213.00s]  on top of
[2213.00s -> 2214.00s]  a polynomial expansion
[2214.00s -> 2216.00s]  of the input,
[2216.00s -> 2218.00s]  and in which case,
[2218.00s -> 2219.00s]  then this regression
[2219.00s -> 2220.00s]  sort of says
[2220.00s -> 2222.00s]  that you're not allowed
[2222.00s -> 2223.00s]  to use as many
[2223.00s -> 2224.00s]  polynomial coefficients
[2224.00s -> 2226.00s]  as maybe you should have.
[2226.00s -> 2227.00s]  So you can imagine this
[2227.00s -> 2228.00s]  as when you're doing
[2228.00s -> 2229.00s]  polynomial regression,
[2229.00s -> 2230.00s]  you can write out
[2230.00s -> 2231.00s]  a polynomial as like
[2231.00s -> 2232.00s]  f of x equals
[2232.00s -> 2235.00s]  a zero plus a one x
[2235.00s -> 2236.00s]  plus a two x squared
[2236.00s -> 2237.00s]  plus a three x,
[2237.00s -> 2238.00s]  whatever.
[2238.00s -> 2239.00s]  In that case,
[2239.00s -> 2240.00s]  your parameters,
[2240.00s -> 2241.00s]  your w's,
[2241.00s -> 2242.00s]  would be these a's,
[2242.00s -> 2243.00s]  in which case,
[2243.00s -> 2245.00s]  this penalizing the w
[2245.00s -> 2246.00s]  could force it
[2246.00s -> 2247.00s]  towards lower degree
[2247.00s -> 2248.00s]  polynomials.
[2248.00s -> 2249.00s]  Except in the case
[2249.00s -> 2250.00s]  of polynomial regression,
[2250.00s -> 2251.00s]  you don't actually
[2251.00s -> 2252.00s]  want to parameterize
[2252.00s -> 2253.00s]  in terms of a's.
[2253.00s -> 2254.00s]  There's some other
[2254.00s -> 2255.00s]  parameterization
[2255.00s -> 2256.00s]  that you want to use,
[2256.00s -> 2257.00s]  but that's kind of
[2257.00s -> 2258.00s]  the general idea,
[2258.00s -> 2259.00s]  is that you're using
[2259.00s -> 2260.00s]  the parameters of the model
[2260.00s -> 2261.00s]  to force it towards
[2261.00s -> 2263.00s]  the simpler hypotheses
[2263.00s -> 2266.00s]  within your hypothesis class.
[2266.00s -> 2268.00s]  Maybe we can take this offline
[2268.00s -> 2269.00s]  if that's still
[2269.00s -> 2271.00s]  a bit confusing.
[2271.00s -> 2272.00s]  Okay, so then
[2272.00s -> 2273.00s]  we've sort of seen
[2273.00s -> 2275.00s]  this multi-class SVM loss.
[2275.00s -> 2276.00s]  And just by the way,
[2276.00s -> 2277.00s]  as a side note,
[2277.00s -> 2279.00s]  this is sort of one extension
[2279.00s -> 2281.00s]  or generalization
[2281.00s -> 2282.00s]  of the SVM loss
[2282.00s -> 2284.00s]  to multiple classes.
[2284.00s -> 2285.00s]  There's actually a couple
[2285.00s -> 2286.00s]  different formulations
[2286.00s -> 2287.00s]  that you can see
[2287.00s -> 2289.00s]  but kind of my intuition
[2289.00s -> 2290.00s]  is that they all tend
[2290.00s -> 2291.00s]  to work kind of similarly
[2291.00s -> 2292.00s]  in practice,
[2292.00s -> 2293.00s]  at least in the context
[2293.00s -> 2294.00s]  of deep learning.
[2294.00s -> 2295.00s]  So we'll kind of stick
[2295.00s -> 2296.00s]  with this one particular
[2296.00s -> 2297.00s]  formulation of the
[2297.00s -> 2298.00s]  multi-class SVM loss
[2298.00s -> 2301.00s]  in this class.
[2301.00s -> 2302.00s]  But of course,
[2302.00s -> 2303.00s]  there's many different
[2303.00s -> 2304.00s]  loss functions
[2304.00s -> 2305.00s]  you might imagine.
[2305.00s -> 2306.00s]  So another really
[2306.00s -> 2307.00s]  popular choice
[2307.00s -> 2310.00s]  in addition to the
[2310.00s -> 2312.00s]  multi-class SVM loss,
[2312.00s -> 2313.00s]  another really popular
[2313.00s -> 2314.00s]  choice in deep learning
[2314.00s -> 2315.00s]  is this multinomial
[2315.00s -> 2316.00s]  logistic regression
[2316.00s -> 2319.00s]  or a softmax loss.
[2319.00s -> 2320.00s]  And this one is probably
[2320.00s -> 2321.00s]  actually a bit more common
[2321.00s -> 2322.00s]  in the context
[2322.00s -> 2323.00s]  of deep learning
[2323.00s -> 2326.00s]  but I decided to present it
[2326.00s -> 2328.00s]  second for some reason.
[2328.00s -> 2329.00s]  So remember in the context
[2329.00s -> 2331.00s]  of the multi-class SVM loss,
[2331.00s -> 2332.00s]  we didn't actually have
[2332.00s -> 2333.00s]  an interpretation
[2333.00s -> 2335.00s]  for those scores.
[2335.00s -> 2336.00s]  Remember when we're doing
[2336.00s -> 2337.00s]  some classification,
[2337.00s -> 2339.00s]  our model F spits out
[2339.00s -> 2340.00s]  these 10 numbers
[2340.00s -> 2341.00s]  which are our scores
[2341.00s -> 2342.00s]  for the classes.
[2342.00s -> 2344.00s]  And for the multi-class SVM,
[2345.00s -> 2346.00s]  we didn't actually give
[2346.00s -> 2347.00s]  much interpretation
[2347.00s -> 2348.00s]  to those scores.
[2348.00s -> 2349.00s]  We just said that we want
[2349.00s -> 2350.00s]  the true score,
[2350.00s -> 2351.00s]  the score of the correct class
[2351.00s -> 2352.00s]  to be greater
[2352.00s -> 2354.00s]  than the incorrect classes
[2354.00s -> 2355.00s]  and beyond that,
[2355.00s -> 2356.00s]  we don't really say
[2356.00s -> 2358.00s]  what those scores mean.
[2358.00s -> 2359.00s]  But now for the
[2359.00s -> 2363.00s]  multinomial logistic
[2363.00s -> 2364.00s]  regression loss function,
[2364.00s -> 2365.00s]  we actually will
[2365.00s -> 2366.00s]  endow those scores
[2366.00s -> 2368.00s]  with some additional meaning.
[2368.00s -> 2369.00s]  And in particular,
[2369.00s -> 2370.00s]  we're gonna use those scores
[2370.00s -> 2371.00s]  to compute
[2371.00s -> 2372.00s]  a probability distribution
[2372.00s -> 2373.00s]  over our classes.
[2373.00s -> 2375.00s]  So we use this
[2375.00s -> 2377.00s]  so-called softmax function
[2377.00s -> 2379.00s]  where we take all of our scores,
[2379.00s -> 2380.00s]  we exponentiate them
[2380.00s -> 2381.00s]  so that now they become
[2381.00s -> 2382.00s]  positive,
[2382.00s -> 2384.00s]  then we renormalize them
[2384.00s -> 2386.00s]  by the sum of those exponents.
[2386.00s -> 2388.00s]  So now after we send our scores
[2388.00s -> 2390.00s]  through this softmax function,
[2390.00s -> 2391.00s]  now we end up with
[2391.00s -> 2392.00s]  this probability distribution
[2392.00s -> 2393.00s]  where now we have
[2393.00s -> 2395.00s]  probabilities over our classes
[2395.00s -> 2396.00s]  where each probability
[2396.00s -> 2397.00s]  is between zero and one
[2397.00s -> 2399.00s]  and the sum of probabilities
[2399.00s -> 2400.00s]  across all classes
[2400.00s -> 2401.00s]  sum to one.
[2402.00s -> 2404.00s]  And now kind of the interpretation
[2404.00s -> 2406.00s]  is that we kinda wanna,
[2406.00s -> 2407.00s]  there's this computed
[2407.00s -> 2408.00s]  probability distribution
[2408.00s -> 2410.00s]  that's implied by our scores
[2410.00s -> 2411.00s]  and we wanna compare this
[2411.00s -> 2413.00s]  with the target
[2413.00s -> 2415.00s]  or true probability distribution.
[2415.00s -> 2416.00s]  So if we know
[2416.00s -> 2417.00s]  that the thing is a cat,
[2417.00s -> 2419.00s]  then the target
[2419.00s -> 2420.00s]  probability distribution
[2420.00s -> 2421.00s]  would put all of the
[2421.00s -> 2423.00s]  probability mass on cat
[2423.00s -> 2424.00s]  so we'd have
[2424.00s -> 2425.00s]  probability of cat equals one
[2425.00s -> 2426.00s]  and zero probability
[2426.00s -> 2428.00s]  for all the other classes.
[2429.00s -> 2430.00s]  So now what we want to do
[2430.00s -> 2431.00s]  is encourage our computed
[2431.00s -> 2433.00s]  probability distribution
[2433.00s -> 2434.00s]  that's coming out
[2434.00s -> 2435.00s]  of this softmax function
[2435.00s -> 2436.00s]  to match this target
[2436.00s -> 2437.00s]  probability distribution
[2437.00s -> 2438.00s]  that has all the mass
[2438.00s -> 2440.00s]  on the correct class.
[2440.00s -> 2441.00s]  And the way that we do this,
[2441.00s -> 2442.00s]  I mean, you can view
[2442.00s -> 2444.00s]  this equation in many ways.
[2444.00s -> 2445.00s]  You can view this
[2445.00s -> 2446.00s]  as a KL divergence
[2446.00s -> 2447.00s]  between the target
[2447.00s -> 2449.00s]  and the computed
[2449.00s -> 2450.00s]  probability distribution.
[2450.00s -> 2451.00s]  You can view this
[2451.00s -> 2452.00s]  as a maximum likelihood
[2452.00s -> 2453.00s]  estimate.
[2453.00s -> 2454.00s]  But at the end of the day,
[2454.00s -> 2455.00s]  what we really want
[2455.00s -> 2456.00s]  is that the probability
[2456.00s -> 2459.00s]  of the true class is high
[2459.00s -> 2461.00s]  and is close to one.
[2461.00s -> 2462.00s]  So then our loss
[2462.00s -> 2464.00s]  will now be the negative log
[2464.00s -> 2465.00s]  of the probability
[2465.00s -> 2466.00s]  of the true class.
[2466.00s -> 2467.00s]  This is kind of confusing
[2467.00s -> 2468.00s]  because we're kind of
[2468.00s -> 2469.00s]  putting this through
[2469.00s -> 2470.00s]  multiple different things.
[2470.00s -> 2471.00s]  But remember,
[2471.00s -> 2472.00s]  we wanted the probability
[2472.00s -> 2473.00s]  to be close to one.
[2473.00s -> 2474.00s]  So now log
[2474.00s -> 2475.00s]  is a monotonic function
[2475.00s -> 2477.00s]  that kind of goes like this.
[2477.00s -> 2478.00s]  And it turns out mathematically
[2478.00s -> 2479.00s]  it's easier to optimize,
[2479.00s -> 2481.00s]  it's easier to maximize log
[2481.00s -> 2482.00s]  than it is to maximize
[2482.00s -> 2483.00s]  the raw probability
[2483.00s -> 2485.00s]  so we stick with log.
[2485.00s -> 2487.00s]  And now log is monotonic.
[2487.00s -> 2488.00s]  So if we maximize
[2488.00s -> 2490.00s]  log P of correct class,
[2490.00s -> 2491.00s]  that means we want
[2491.00s -> 2492.00s]  that to be high.
[2492.00s -> 2494.00s]  But loss functions
[2494.00s -> 2495.00s]  measure badness,
[2495.00s -> 2496.00s]  not goodness.
[2496.00s -> 2497.00s]  So we need to put in
[2497.00s -> 2498.00s]  the minus one
[2498.00s -> 2500.00s]  to make it go the right way.
[2500.00s -> 2502.00s]  So now our loss function
[2502.00s -> 2503.00s]  for SVM
[2503.00s -> 2504.00s]  is gonna be the minus log
[2504.00s -> 2505.00s]  of the probability
[2505.00s -> 2507.00s]  of the true class.
[2509.00s -> 2510.00s]  Yeah, so that's kind of
[2510.00s -> 2511.00s]  the summary here
[2511.00s -> 2513.00s]  is that we take our scores,
[2513.00s -> 2514.00s]  we run to the softmax,
[2514.00s -> 2515.00s]  and now our loss
[2515.00s -> 2516.00s]  is this minus log
[2516.00s -> 2517.00s]  of the probability
[2517.00s -> 2518.50s]  of the true class.
[2522.00s -> 2523.00s]  Okay, so then
[2523.00s -> 2524.00s]  if you kind of look
[2524.00s -> 2525.00s]  at what this looks like
[2525.00s -> 2526.00s]  on a concrete example,
[2526.00s -> 2527.00s]  then we go back
[2527.00s -> 2528.00s]  to our favorite cat,
[2528.00s -> 2529.00s]  our favorite beautiful cat
[2529.00s -> 2530.00s]  with our three examples,
[2530.00s -> 2531.00s]  and we've got these
[2531.00s -> 2532.00s]  three scores
[2532.00s -> 2533.00s]  that are coming out
[2533.00s -> 2535.00s]  of our linear classifier.
[2535.00s -> 2536.00s]  And these scores
[2536.00s -> 2537.00s]  are exactly the way
[2537.00s -> 2538.00s]  that they were
[2538.00s -> 2539.00s]  in the context
[2539.00s -> 2540.00s]  of the SVM loss.
[2540.00s -> 2541.00s]  But now rather than
[2541.00s -> 2542.00s]  taking these scores
[2542.00s -> 2543.00s]  and putting them directly
[2543.00s -> 2545.00s]  in, we're gonna take them all
[2545.00s -> 2546.00s]  and exponentiate them
[2546.00s -> 2547.00s]  so that they're all positive,
[2547.00s -> 2549.00s]  and then we'll normalize them
[2549.00s -> 2550.00s]  to make sure
[2550.00s -> 2551.00s]  that they all sum to one.
[2551.00s -> 2552.00s]  And now our loss
[2552.00s -> 2554.00s]  will be the minus log
[2554.00s -> 2557.00s]  of the true class score.
[2557.00s -> 2558.00s]  So that's kind of
[2558.00s -> 2560.00s]  the softmax loss,
[2560.00s -> 2561.00s]  or also called
[2561.00s -> 2564.00s]  multinomial logistic regression.
[2566.00s -> 2567.00s]  So now we asked
[2567.00s -> 2568.00s]  several questions
[2568.00s -> 2569.00s]  to try to gain intuition
[2569.00s -> 2571.00s]  about the multiclass SVM loss,
[2571.00s -> 2572.00s]  and it's kind of useful
[2572.00s -> 2573.00s]  to think about some
[2573.00s -> 2574.00s]  of the same questions
[2574.00s -> 2575.00s]  to contrast
[2575.00s -> 2578.00s]  with the softmax loss.
[2578.00s -> 2579.00s]  So then the question is,
[2579.00s -> 2580.00s]  what's the min and max value
[2580.00s -> 2582.00s]  of the softmax loss?
[2585.00s -> 2587.00s]  Okay, maybe not so sure.
[2587.00s -> 2588.00s]  There's too many logs
[2588.00s -> 2589.00s]  and sums and stuff
[2589.00s -> 2590.00s]  going on in here.
[2590.00s -> 2591.00s]  But remember our,
[2591.00s -> 2592.00s]  so the answer is
[2592.00s -> 2594.00s]  that the min loss is zero
[2594.00s -> 2596.00s]  and the max loss is infinity.
[2596.00s -> 2597.00s]  And the way
[2597.00s -> 2598.00s]  that you can see this
[2598.00s -> 2600.00s]  is that if the kind
[2600.00s -> 2601.00s]  of the probability distribution
[2601.00s -> 2602.00s]  that we want
[2602.00s -> 2603.00s]  is one on the correct class,
[2603.00s -> 2605.00s]  zero on the incorrect classes,
[2605.00s -> 2606.00s]  the way that we do that,
[2606.00s -> 2608.00s]  so if that were the case,
[2608.00s -> 2610.00s]  then this e to the,
[2610.00s -> 2611.00s]  this term would end up,
[2611.00s -> 2613.00s]  this thing inside the log
[2613.00s -> 2614.00s]  would end up being one
[2614.00s -> 2615.00s]  because it's the probability,
[2615.00s -> 2616.00s]  well, log probability
[2616.00s -> 2617.00s]  of the true class.
[2617.00s -> 2619.00s]  Then log of one is zero.
[2619.00s -> 2620.00s]  Minus log of zero is,
[2620.00s -> 2621.00s]  or minus log of one
[2621.00s -> 2622.00s]  is still zero.
[2622.00s -> 2623.00s]  So that means that
[2623.00s -> 2624.00s]  if we got the thing
[2624.00s -> 2625.00s]  totally right,
[2625.00s -> 2627.00s]  then our loss would be zero.
[2627.00s -> 2628.00s]  But by the way,
[2628.00s -> 2629.00s]  in order to get
[2629.00s -> 2630.00s]  the thing totally right,
[2630.00s -> 2631.00s]  what would our scores
[2631.00s -> 2632.00s]  have to look like?
[2636.00s -> 2637.00s]  Murmuring, murmuring.
[2637.00s -> 2638.00s]  So the scores would actually
[2638.00s -> 2640.00s]  have to go quite extreme,
[2640.00s -> 2641.00s]  like towards infinity.
[2641.00s -> 2643.00s]  So because we actually have
[2643.00s -> 2644.00s]  this exponentiation,
[2644.00s -> 2645.00s]  this normalization,
[2645.00s -> 2646.00s]  the only way
[2646.00s -> 2647.00s]  we can actually get
[2647.00s -> 2648.00s]  a probability distribution
[2648.00s -> 2649.00s]  of one and zero
[2649.00s -> 2651.00s]  is actually putting
[2651.00s -> 2652.00s]  an infinite score
[2652.00s -> 2653.00s]  for the correct class
[2653.00s -> 2655.00s]  and minus infinity score
[2655.00s -> 2657.00s]  for all the incorrect classes.
[2657.00s -> 2659.00s]  And computers don't do
[2659.00s -> 2660.00s]  so well with infinities.
[2660.00s -> 2661.00s]  So in practice,
[2661.00s -> 2662.00s]  you'll never get the zero loss
[2662.00s -> 2663.00s]  on this thing
[2663.00s -> 2664.00s]  with finite precision.
[2664.00s -> 2665.00s]  But you still have
[2665.00s -> 2666.00s]  this interpretation
[2666.00s -> 2667.00s]  that zero is
[2667.00s -> 2668.00s]  the theoretical minimum
[2668.00s -> 2669.00s]  loss here.
[2669.00s -> 2670.00s]  And the maximum loss
[2670.00s -> 2671.00s]  is unbounded.
[2671.00s -> 2673.00s]  So suppose that if we had
[2673.00s -> 2675.00s]  zero probability mass
[2675.00s -> 2677.00s]  on the correct class,
[2677.00s -> 2678.00s]  then you would have
[2678.00s -> 2680.00s]  minus log of zero.
[2680.00s -> 2681.00s]  Log of zero
[2681.00s -> 2682.00s]  is like minus infinity,
[2682.00s -> 2684.00s]  so minus log of zero
[2684.00s -> 2686.00s]  would be plus infinity.
[2686.00s -> 2687.00s]  So that's really bad.
[2687.00s -> 2688.00s]  But again,
[2688.00s -> 2689.00s]  you'll never really get here,
[2689.00s -> 2690.00s]  because the only way
[2690.00s -> 2692.00s]  you can actually get
[2692.00s -> 2694.00s]  this probability to be zero
[2694.00s -> 2696.00s]  is if e to the correct
[2696.00s -> 2698.00s]  class score is zero.
[2698.00s -> 2699.00s]  And that can only happen
[2699.00s -> 2700.00s]  if that correct class score
[2700.00s -> 2701.00s]  is minus infinity.
[2701.00s -> 2702.00s]  So again,
[2702.00s -> 2703.00s]  you'll never actually
[2703.00s -> 2704.00s]  get to these minimum
[2704.00s -> 2705.00s]  and maximum values
[2705.00s -> 2708.00s]  with finite precision.
[2708.00s -> 2709.00s]  So then,
[2709.00s -> 2710.00s]  remember we had this
[2710.00s -> 2711.00s]  sort of debugging
[2711.00s -> 2712.00s]  sanity check question
[2712.00s -> 2713.00s]  in the context
[2713.00s -> 2714.00s]  of the multi-class SVM,
[2714.00s -> 2715.00s]  and we can ask
[2715.00s -> 2716.00s]  the same for the softmax.
[2716.00s -> 2717.00s]  If all the S's
[2717.00s -> 2719.00s]  are small and about zero,
[2719.00s -> 2721.00s]  then what is the loss here?
[2721.00s -> 2722.00s]  Yeah, answer?
[2724.00s -> 2727.00s]  So minus log of one over C.
[2727.00s -> 2729.00s]  I think that's...
[2729.00s -> 2730.00s]  Yeah, yeah.
[2730.00s -> 2731.00s]  So then it will be
[2731.00s -> 2732.00s]  minus log of one over C,
[2732.00s -> 2733.00s]  which is,
[2733.00s -> 2734.00s]  because log,
[2734.00s -> 2735.00s]  you can flip the thing,
[2735.00s -> 2736.00s]  and then it's just log of C.
[2736.00s -> 2738.00s]  Yeah, so it's just log of C.
[2738.00s -> 2739.00s]  And again,
[2739.00s -> 2740.00s]  this is kind of
[2740.00s -> 2741.00s]  a nice debugging thing
[2741.00s -> 2742.00s]  if you're training a model
[2742.00s -> 2743.00s]  with this softmax loss.
[2743.00s -> 2744.00s]  You should check
[2744.00s -> 2745.00s]  the first iteration.
[2745.00s -> 2746.00s]  If it's not log C,
[2746.00s -> 2749.00s]  then something's gone wrong.
[2749.00s -> 2750.00s]  So then we can kind of
[2750.00s -> 2752.00s]  compare and contrast
[2752.00s -> 2754.00s]  these two loss functions a bit.
[2754.00s -> 2756.00s]  In terms of linear classification,
[2756.00s -> 2757.00s]  the setup looks the same.
[2757.00s -> 2758.00s]  We've got this W matrix
[2758.00s -> 2759.00s]  that gets multiplied
[2759.00s -> 2760.00s]  against our input
[2760.00s -> 2762.00s]  to produce this vector of scores.
[2762.00s -> 2763.00s]  And now the difference
[2763.00s -> 2764.00s]  between the two loss functions
[2764.00s -> 2765.00s]  is how we choose
[2765.00s -> 2766.00s]  to interpret those scores
[2766.00s -> 2767.00s]  to quantitatively measure
[2767.00s -> 2769.00s]  the badness afterwards.
[2769.00s -> 2770.00s]  So for SVM,
[2770.00s -> 2771.00s]  we were gonna go in
[2771.00s -> 2772.00s]  and look at the margins
[2772.00s -> 2773.00s]  between the true
[2773.00s -> 2774.00s]  and the scores
[2774.00s -> 2775.00s]  of the correct class
[2775.00s -> 2776.00s]  and the scores
[2776.00s -> 2777.00s]  of the incorrect class,
[2777.00s -> 2779.00s]  whereas for this softmax
[2779.00s -> 2780.00s]  or cross-entropy loss,
[2780.00s -> 2781.00s]  we're gonna go
[2781.00s -> 2782.00s]  and compute
[2782.00s -> 2783.00s]  a probability distribution
[2783.00s -> 2784.00s]  and then look at
[2784.00s -> 2785.00s]  the minus log probability
[2785.00s -> 2787.00s]  of the correct class.
[2787.00s -> 2788.00s]  So sometimes,
[2788.00s -> 2790.00s]  if you look at like,
[2790.00s -> 2791.00s]  in the terms of,
[2791.00s -> 2792.00s]  nevermind,
[2792.00s -> 2793.00s]  I'll skip that point.
[2793.00s -> 2795.00s]  So another question
[2795.00s -> 2796.00s]  that's kind of interesting
[2796.00s -> 2797.00s]  when contrasting
[2797.00s -> 2799.00s]  these two loss functions
[2799.00s -> 2800.00s]  is thinking,
[2800.00s -> 2801.00s]  suppose that I've got
[2801.00s -> 2803.00s]  this example point.
[2804.00s -> 2805.00s]  And if you change its scores,
[2805.00s -> 2807.00s]  so assume that we've got
[2807.00s -> 2809.00s]  three scores for this,
[2812.00s -> 2813.00s]  ignore the part on the bottom.
[2813.00s -> 2814.00s]  But what happens,
[2814.00s -> 2815.00s]  remember if we go back
[2815.00s -> 2816.00s]  to this example
[2816.00s -> 2819.00s]  where in the multiclass SVM loss
[2819.00s -> 2820.00s]  when we had the car
[2820.00s -> 2822.00s]  and the car was much better
[2822.00s -> 2823.00s]  than all the other class,
[2823.00s -> 2824.00s]  the car score was much better
[2824.00s -> 2826.00s]  than all the incorrect classes,
[2826.00s -> 2827.00s]  then jiggling the scores
[2827.00s -> 2828.00s]  for that car image
[2828.00s -> 2829.00s]  didn't change
[2829.00s -> 2831.00s]  the multiclass SVM loss at all
[2831.00s -> 2832.00s]  because the only thing
[2832.00s -> 2833.00s]  that the SVM loss cared about
[2833.00s -> 2835.00s]  was getting that correct score
[2835.00s -> 2836.00s]  to be greater than a margin
[2836.00s -> 2838.00s]  above the incorrect scores.
[2838.00s -> 2839.00s]  But now,
[2839.00s -> 2840.00s]  the softmax loss
[2840.00s -> 2841.00s]  is actually quite different
[2841.00s -> 2842.00s]  in this respect.
[2842.00s -> 2843.00s]  The softmax loss
[2843.00s -> 2844.00s]  actually always wants
[2844.00s -> 2845.00s]  to drive that probability mass
[2845.00s -> 2846.00s]  all the way to one.
[2846.00s -> 2850.00s]  So even if you're giving
[2850.00s -> 2851.00s]  very high score
[2851.00s -> 2852.00s]  to the correct class
[2852.00s -> 2853.00s]  and very low score
[2853.00s -> 2854.00s]  to all the incorrect classes,
[2854.00s -> 2855.00s]  softmax will want you
[2855.00s -> 2856.00s]  to pile more and more
[2856.00s -> 2857.00s]  probability mass
[2857.00s -> 2858.00s]  on the correct class
[2858.00s -> 2860.00s]  and continue to push the score
[2860.00s -> 2861.00s]  of that correct class
[2861.00s -> 2862.00s]  up towards infinity,
[2862.00s -> 2863.00s]  and the score
[2863.00s -> 2864.00s]  of the incorrect class
[2864.00s -> 2866.00s]  is down towards minus infinity.
[2866.00s -> 2867.00s]  So that's kind of
[2867.00s -> 2868.00s]  an interesting difference
[2868.00s -> 2869.00s]  between these two loss
[2869.00s -> 2870.00s]  functions in practice,
[2870.00s -> 2871.00s]  that SVM,
[2871.00s -> 2873.00s]  it'll get this data point
[2873.00s -> 2874.00s]  over the bar
[2874.00s -> 2875.00s]  to be correctly classified
[2875.00s -> 2876.00s]  and then just give up.
[2876.00s -> 2877.00s]  It doesn't care
[2877.00s -> 2878.00s]  about that data point anymore.
[2878.00s -> 2879.00s]  Whereas softmax
[2879.00s -> 2880.00s]  will just always try
[2880.00s -> 2881.00s]  to continually improve
[2881.00s -> 2882.00s]  every single data point
[2882.00s -> 2883.00s]  to get better and better
[2883.00s -> 2884.00s]  and better and better.
[2884.00s -> 2885.00s]  So that's kind of
[2885.00s -> 2886.00s]  an interesting difference
[2886.00s -> 2888.00s]  between these two functions.
[2888.00s -> 2889.00s]  In practice,
[2889.00s -> 2890.00s]  I think it tends not
[2890.00s -> 2891.00s]  to make a huge difference
[2891.00s -> 2892.00s]  which one you choose,
[2892.00s -> 2893.00s]  they tend to perform
[2893.00s -> 2894.00s]  pretty similarly
[2894.00s -> 2895.00s]  across at least
[2895.00s -> 2896.00s]  a lot of deep
[2896.00s -> 2897.00s]  learning applications.
[2897.00s -> 2898.00s]  But it is very useful
[2898.00s -> 2899.00s]  to keep some of these
[2899.00s -> 2901.00s]  differences in mind.
[2904.00s -> 2905.00s]  Yeah, so to kind of recap
[2905.00s -> 2906.00s]  where we've come to
[2906.00s -> 2907.00s]  from here
[2907.00s -> 2908.00s]  is that we've got
[2908.00s -> 2910.00s]  some data set of x's and y's.
[2910.00s -> 2912.00s]  We use our linear classifier
[2912.00s -> 2914.00s]  to get some score function
[2914.00s -> 2915.00s]  to compute our scores S
[2915.00s -> 2917.00s]  from our inputs X,
[2917.00s -> 2918.00s]  and then we'll use
[2918.00s -> 2919.00s]  a loss function,
[2919.00s -> 2920.00s]  maybe softmax or SVM
[2920.00s -> 2922.00s]  or some other loss function
[2922.00s -> 2923.00s]  to compute
[2923.00s -> 2925.00s]  how quantitatively bad
[2925.00s -> 2926.00s]  were our predictions
[2926.00s -> 2927.00s]  compared to this
[2927.00s -> 2929.00s]  ground truth targets Y.
[2929.00s -> 2931.00s]  And then we'll often augment
[2931.00s -> 2932.00s]  this loss function
[2932.00s -> 2934.00s]  with a regularization term
[2934.00s -> 2935.00s]  that tries to trade off
[2935.00s -> 2936.00s]  between fitting
[2936.00s -> 2937.00s]  the training data
[2937.00s -> 2938.00s]  and preferring
[2938.00s -> 2939.00s]  simpler models.
[2939.00s -> 2940.00s]  So this is kind of
[2940.00s -> 2941.00s]  a pretty generic overview
[2941.00s -> 2943.00s]  of a lot of what we call
[2943.00s -> 2944.00s]  supervised learning.
[2944.00s -> 2945.00s]  And what we'll see
[2945.00s -> 2946.00s]  in deep learning
[2946.00s -> 2947.00s]  as we sort of move forward
[2947.00s -> 2949.00s]  is that generally you'll want
[2949.00s -> 2951.00s]  to specify some function F
[2951.00s -> 2952.00s]  that could be very complex
[2952.00s -> 2953.00s]  in structure,
[2953.00s -> 2954.00s]  specify some loss function
[2954.00s -> 2955.00s]  that determines
[2955.00s -> 2957.00s]  how well your algorithm
[2957.00s -> 2958.00s]  is doing given any value
[2958.00s -> 2959.00s]  of the parameters,
[2959.00s -> 2961.00s]  some regularization term
[2961.00s -> 2963.00s]  for how to penalize
[2963.00s -> 2964.00s]  model complexity,
[2964.00s -> 2965.00s]  and then you combine
[2965.00s -> 2966.00s]  these things together
[2966.00s -> 2967.00s]  and try to find the W
[2967.00s -> 2968.00s]  that minimizes
[2968.00s -> 2971.00s]  this final loss function.
[2971.00s -> 2972.00s]  But then the question is
[2972.00s -> 2973.00s]  how do we actually
[2973.00s -> 2974.00s]  go about doing that?
[2974.00s -> 2975.00s]  How do we actually find
[2975.00s -> 2976.00s]  this W that minimizes
[2976.00s -> 2977.00s]  the loss?
[2977.00s -> 2978.00s]  And that leads us
[2978.00s -> 2981.00s]  to the topic of optimization.
[2981.00s -> 2982.00s]  So when we're doing
[2982.00s -> 2983.00s]  optimization,
[2983.00s -> 2984.00s]  I usually think of things
[2984.00s -> 2985.00s]  in terms of some,
[2985.00s -> 2986.00s]  like walking around
[2986.00s -> 2988.00s]  some large valley.
[2988.00s -> 2989.00s]  So the idea is that
[2989.00s -> 2990.00s]  you're walking around
[2990.00s -> 2992.00s]  this large valley
[2992.00s -> 2993.00s]  with different mountains
[2993.00s -> 2994.00s]  and valleys
[2994.00s -> 2995.00s]  and streams and stuff,
[2995.00s -> 2996.00s]  and every point
[2996.00s -> 2997.00s]  on this landscape
[2997.00s -> 2999.00s]  corresponds to some setting
[2999.00s -> 3001.00s]  of the parameters W.
[3001.00s -> 3002.00s]  And you're this little guy
[3002.00s -> 3003.00s]  who's walking around
[3003.00s -> 3004.00s]  this valley,
[3004.00s -> 3005.00s]  and you're trying to find,
[3005.00s -> 3006.00s]  and the height of each
[3006.00s -> 3007.00s]  of these points, sorry,
[3007.00s -> 3008.00s]  is equal to the loss
[3008.00s -> 3010.00s]  incurred by that setting
[3010.00s -> 3011.00s]  of W.
[3011.00s -> 3012.00s]  And now your job
[3012.00s -> 3013.00s]  is as this little man
[3013.00s -> 3015.00s]  walking around this landscape,
[3015.00s -> 3016.00s]  you need to somehow find
[3016.00s -> 3018.00s]  the bottom of this valley.
[3018.00s -> 3019.00s]  And this is kind of
[3019.00s -> 3021.00s]  a hard problem in general.
[3021.00s -> 3022.00s]  You might think like
[3022.00s -> 3023.00s]  maybe I'm really smart
[3023.00s -> 3024.00s]  and I can think really hard
[3024.00s -> 3025.00s]  about the analytic properties
[3025.00s -> 3026.00s]  of my loss function,
[3026.00s -> 3027.00s]  my regularization,
[3027.00s -> 3028.00s]  all that.
[3028.00s -> 3029.00s]  Maybe I can just write down
[3029.00s -> 3031.00s]  the minimizer,
[3031.00s -> 3032.00s]  and that would sort of
[3032.00s -> 3033.00s]  correspond to magically
[3033.00s -> 3034.00s]  teleporting all the way
[3034.00s -> 3036.00s]  to the bottom of this valley.
[3036.00s -> 3037.00s]  But in practice,
[3037.00s -> 3039.00s]  once your prediction function F
[3039.00s -> 3040.00s]  and your loss function
[3040.00s -> 3041.00s]  and your regularizer,
[3041.00s -> 3042.00s]  once these things
[3042.00s -> 3043.00s]  get big and complex
[3043.00s -> 3044.00s]  and using neural networks,
[3044.00s -> 3046.00s]  then you can't really hope,
[3046.00s -> 3047.00s]  there's really not much hope
[3047.00s -> 3048.00s]  in trying to write down
[3048.00s -> 3050.00s]  an explicit analytic solution
[3050.00s -> 3051.00s]  that takes you
[3051.00s -> 3052.00s]  directly to the minimum.
[3052.00s -> 3053.00s]  So in practice,
[3053.00s -> 3054.00s]  we tend to use
[3054.00s -> 3055.00s]  various types of
[3055.00s -> 3056.00s]  iterative methods
[3056.00s -> 3057.00s]  where we start with
[3057.00s -> 3058.00s]  some solution
[3058.00s -> 3059.00s]  and then gradually
[3059.00s -> 3061.00s]  improve it over time.
[3061.00s -> 3062.00s]  So kind of the very first
[3062.00s -> 3064.00s]  sort of stupidest thing
[3064.00s -> 3065.00s]  that you might imagine
[3065.00s -> 3067.00s]  is random search.
[3067.00s -> 3068.00s]  That will just take
[3068.00s -> 3069.00s]  a bunch of W's
[3069.00s -> 3070.00s]  sampled randomly
[3070.00s -> 3071.00s]  and throw them
[3071.00s -> 3072.00s]  into our loss function
[3072.00s -> 3075.00s]  and see how well they do.
[3075.00s -> 3076.00s]  So spoiler alert,
[3076.00s -> 3077.00s]  this is a really bad algorithm.
[3077.00s -> 3079.00s]  You probably shouldn't use this.
[3079.00s -> 3080.00s]  But at least it's kind of
[3080.00s -> 3081.00s]  one thing you might
[3081.00s -> 3083.00s]  imagine trying.
[3083.00s -> 3085.00s]  And we can actually do this.
[3085.00s -> 3086.00s]  We can actually try
[3086.00s -> 3088.00s]  to train a linear classifier
[3088.00s -> 3089.00s]  via random search
[3089.00s -> 3091.00s]  for CIFAR-10
[3092.00s -> 3093.00s]  and for this,
[3093.00s -> 3094.00s]  there's 10 classes
[3094.00s -> 3097.00s]  so random chance is 10%.
[3097.00s -> 3098.00s]  And if we did
[3098.00s -> 3100.00s]  some number of random trials,
[3100.00s -> 3101.00s]  we eventually found
[3101.00s -> 3102.00s]  just through sheer dumb luck
[3102.00s -> 3103.00s]  some setting of W
[3103.00s -> 3106.00s]  that got maybe 15% accuracy.
[3106.00s -> 3108.00s]  So that's better than random
[3108.00s -> 3109.00s]  but state of the art
[3109.00s -> 3110.00s]  is maybe 95%
[3110.00s -> 3111.00s]  so we've got a little bit
[3111.00s -> 3114.00s]  of gap to close here.
[3114.00s -> 3115.00s]  So again,
[3115.00s -> 3116.00s]  probably don't use this
[3116.00s -> 3117.00s]  in practice
[3117.00s -> 3118.00s]  but you might imagine
[3118.00s -> 3119.00s]  that this is something
[3119.00s -> 3120.00s]  you could potentially do.
[3120.00s -> 3121.00s]  So in practice,
[3121.00s -> 3122.00s]  maybe a better strategy
[3122.00s -> 3123.00s]  is actually using
[3123.00s -> 3124.00s]  some of the local geometry
[3124.00s -> 3126.00s]  of this landscape.
[3126.00s -> 3127.00s]  So if you're this little guy
[3127.00s -> 3128.00s]  who's walking around
[3128.00s -> 3129.00s]  this landscape,
[3129.00s -> 3131.00s]  maybe you can't see
[3131.00s -> 3132.00s]  directly the path
[3132.00s -> 3133.00s]  down to the bottom
[3133.00s -> 3134.00s]  of the valley
[3134.00s -> 3135.00s]  but what you can do
[3135.00s -> 3136.00s]  is kind of feel
[3136.00s -> 3137.00s]  with your foot
[3137.00s -> 3138.00s]  and figure out
[3138.00s -> 3139.00s]  what is the local geometry?
[3139.00s -> 3140.00s]  Like which way,
[3140.00s -> 3142.00s]  if I'm standing right here,
[3142.00s -> 3143.00s]  which way will take me
[3143.00s -> 3144.00s]  a little bit downhill?
[3144.00s -> 3145.00s]  So then you can kind of
[3145.00s -> 3146.00s]  feel with your feet
[3146.00s -> 3147.00s]  and feel where is the slope
[3147.00s -> 3148.00s]  of the ground
[3148.00s -> 3149.00s]  taking me down
[3149.00s -> 3150.00s]  a little bit
[3150.00s -> 3151.00s]  in this direction
[3151.00s -> 3152.00s]  and you can take a step
[3152.00s -> 3153.00s]  in that direction
[3153.00s -> 3154.00s]  and then you'll go down
[3154.00s -> 3155.00s]  a little bit,
[3155.00s -> 3156.00s]  feel again with your feet
[3156.00s -> 3157.00s]  to figure out
[3157.00s -> 3158.00s]  which way is down
[3158.00s -> 3159.00s]  and then repeat
[3159.00s -> 3160.00s]  over and over again
[3160.00s -> 3161.00s]  and kind of hope
[3161.00s -> 3162.00s]  that you'll end up
[3162.00s -> 3163.00s]  at the bottom
[3163.00s -> 3164.00s]  of the valley eventually.
[3164.00s -> 3165.00s]  So this also seems
[3165.00s -> 3166.00s]  like a relatively
[3166.00s -> 3167.00s]  simple algorithm
[3167.00s -> 3168.00s]  but actually this one
[3168.00s -> 3169.00s]  tends to work
[3169.00s -> 3170.00s]  really well in practice
[3170.00s -> 3171.00s]  if you kind of
[3171.00s -> 3172.00s]  get all the details right.
[3172.00s -> 3173.00s]  So this is generally
[3173.00s -> 3174.00s]  the strategy
[3174.00s -> 3175.00s]  that will follow
[3175.00s -> 3176.00s]  when training these
[3176.00s -> 3177.00s]  large neural networks
[3177.00s -> 3178.00s]  and linear classifiers
[3178.00s -> 3179.00s]  and then like that
[3179.00s -> 3180.00s]  was a little hand wavy
[3180.00s -> 3181.00s]  so what is slope?
[3181.00s -> 3182.00s]  If you remember
[3182.00s -> 3183.00s]  back to your calculus class
[3183.00s -> 3184.00s]  then at least
[3184.00s -> 3185.00s]  in one dimension
[3185.00s -> 3186.00s]  the slope is equal
[3186.00s -> 3187.00s]  to the derivative
[3187.00s -> 3188.00s]  of this function.
[3188.00s -> 3189.00s]  So if we've got
[3189.00s -> 3190.00s]  some one dimensional
[3190.00s -> 3191.00s]  function f
[3191.00s -> 3192.00s]  that takes in a scalar x
[3192.00s -> 3193.00s]  and then outputs
[3193.00s -> 3194.00s]  the height
[3194.00s -> 3195.00s]  of some curve
[3195.00s -> 3196.00s]  then we can compute
[3196.00s -> 3197.00s]  the slope
[3197.00s -> 3198.00s]  or derivative
[3198.00s -> 3199.00s]  at any point
[3199.00s -> 3200.00s]  by imagining
[3200.00s -> 3201.00s]  like if we take
[3201.00s -> 3202.00s]  a small step h
[3202.00s -> 3203.00s]  in any direction
[3203.00s -> 3204.00s]  and then look,
[3204.00s -> 3205.00s]  compare like
[3205.00s -> 3206.00s]  what is the slope
[3206.00s -> 3207.00s]  of that,
[3207.00s -> 3208.00s]  step h
[3208.00s -> 3209.00s]  and compare the difference
[3209.00s -> 3210.00s]  in the function value
[3210.00s -> 3211.00s]  over that step
[3211.00s -> 3212.00s]  and then drive
[3212.00s -> 3213.00s]  the step size to zero
[3213.00s -> 3214.00s]  that will give us
[3214.00s -> 3215.00s]  the slope
[3215.00s -> 3216.00s]  of that function
[3216.00s -> 3217.00s]  at that point.
[3217.00s -> 3218.00s]  And this generalizes
[3218.00s -> 3219.00s]  quite naturally
[3219.00s -> 3220.00s]  to multivariable functions
[3220.00s -> 3221.00s]  as well.
[3221.00s -> 3222.00s]  So in practice
[3222.00s -> 3223.00s]  our x is maybe
[3223.00s -> 3224.00s]  not a scalar
[3224.00s -> 3225.00s]  but a whole vector
[3225.00s -> 3226.00s]  because remember
[3226.00s -> 3227.00s]  x is something
[3227.00s -> 3228.00s]  like the,
[3228.00s -> 3229.00s]  so x might be
[3229.00s -> 3230.00s]  a whole vector
[3230.00s -> 3231.00s]  so we need to
[3231.00s -> 3232.00s]  generalize this notion
[3232.00s -> 3233.00s]  to multivariable things.
[3233.00s -> 3234.00s]  And the generalization
[3234.00s -> 3235.00s]  that we use
[3235.00s -> 3236.00s]  of the derivative
[3236.00s -> 3237.00s]  in the multivariable setting
[3237.00s -> 3238.00s]  is the gradient.
[3238.00s -> 3239.00s]  So the gradient
[3239.00s -> 3240.00s]  is a vector
[3240.00s -> 3241.00s]  of partial derivatives
[3241.00s -> 3242.00s]  telling,
[3242.00s -> 3243.00s]  so the gradient
[3243.00s -> 3244.00s]  will have the same shape
[3244.00s -> 3245.00s]  as x
[3245.00s -> 3246.00s]  and each element
[3246.00s -> 3247.00s]  of the gradient
[3247.00s -> 3248.00s]  will tell us
[3248.00s -> 3249.00s]  what is the slope
[3249.00s -> 3250.00s]  of the function f
[3250.00s -> 3251.00s]  if we move
[3251.00s -> 3252.00s]  in that coordinate direction.
[3252.00s -> 3253.00s]  And there,
[3253.00s -> 3254.00s]  and the gradient
[3254.00s -> 3255.00s]  turns out to have
[3255.00s -> 3256.00s]  these very nice properties
[3256.00s -> 3257.00s]  which is that
[3257.00s -> 3258.00s]  the direction,
[3258.00s -> 3259.00s]  so the gradient
[3259.00s -> 3260.00s]  is now a vector
[3260.00s -> 3261.00s]  of partial derivatives
[3261.00s -> 3262.00s]  but it points
[3262.00s -> 3263.00s]  in the direction
[3263.00s -> 3264.00s]  of greatest increase
[3264.00s -> 3265.00s]  of the function
[3265.00s -> 3266.00s]  and correspondingly
[3266.00s -> 3267.00s]  if you look at
[3267.00s -> 3268.00s]  the negative gradient direction
[3268.00s -> 3269.00s]  that gives you
[3269.00s -> 3270.00s]  the direction
[3270.00s -> 3271.00s]  of greatest decrease
[3271.00s -> 3272.00s]  of the function.
[3272.00s -> 3273.00s]  And more generally
[3273.00s -> 3274.00s]  if you want to know
[3274.00s -> 3275.00s]  what is the slope
[3275.00s -> 3276.00s]  of my landscape
[3276.00s -> 3277.00s]  in any direction
[3277.00s -> 3278.00s]  then that's equal
[3278.00s -> 3279.00s]  to the dot product
[3279.00s -> 3280.00s]  of the gradient
[3280.00s -> 3281.00s]  with a unit vector
[3281.00s -> 3283.00s]  describing that direction.
[3283.00s -> 3284.00s]  So this gradient
[3284.00s -> 3285.00s]  is super important
[3285.00s -> 3286.00s]  because it gives you
[3286.00s -> 3287.00s]  this sort of linear
[3287.00s -> 3288.00s]  first order approximation
[3288.00s -> 3289.00s]  to your function
[3289.00s -> 3290.00s]  at your current point.
[3290.00s -> 3291.00s]  So in practice
[3291.00s -> 3292.00s]  a lot of deep learning
[3292.00s -> 3293.00s]  is about computing gradients
[3293.00s -> 3294.00s]  of your functions
[3294.00s -> 3295.00s]  and then using those gradients
[3295.00s -> 3296.00s]  to iteratively update
[3296.00s -> 3298.00s]  your parameter vector.
[3299.00s -> 3301.00s]  So one kind of naive way
[3301.00s -> 3302.00s]  that you might imagine
[3302.00s -> 3303.00s]  actually evaluating
[3303.00s -> 3304.00s]  this gradient
[3304.00s -> 3305.00s]  on a computer
[3305.00s -> 3306.00s]  is using the method
[3306.00s -> 3307.00s]  of finite differences.
[3307.00s -> 3308.00s]  Kind of going back
[3308.00s -> 3309.00s]  to the limit definition
[3309.00s -> 3310.00s]  of the gradient.
[3310.00s -> 3311.00s]  So here on the left
[3311.00s -> 3312.00s]  we imagine that
[3312.00s -> 3313.00s]  our current w
[3313.00s -> 3314.00s]  is this parameter vector
[3314.00s -> 3315.00s]  that maybe gives us
[3315.00s -> 3316.00s]  some current loss
[3316.00s -> 3318.00s]  of maybe 1.25.
[3318.00s -> 3319.00s]  And our goal
[3319.00s -> 3320.00s]  is to compute
[3320.00s -> 3321.00s]  the gradient dw
[3321.00s -> 3322.00s]  which will be a vector
[3322.00s -> 3323.00s]  of the same shape
[3323.00s -> 3324.00s]  as w
[3324.00s -> 3325.00s]  and each slot
[3325.00s -> 3326.00s]  in that gradient
[3326.00s -> 3327.00s]  will tell us
[3327.00s -> 3328.00s]  how much will the loss change
[3328.00s -> 3329.00s]  if we move
[3329.00s -> 3330.00s]  a tiny infinitesimal amount
[3330.00s -> 3332.00s]  in that coordinate direction.
[3332.00s -> 3333.00s]  So one thing
[3333.00s -> 3334.00s]  you might imagine
[3334.00s -> 3335.00s]  is just computing this
[3335.00s -> 3336.00s]  via finite differences.
[3336.00s -> 3337.00s]  That we have our w
[3337.00s -> 3338.00s]  we might try to increment
[3338.00s -> 3340.00s]  the first element of w
[3340.00s -> 3342.00s]  by a small value h
[3342.00s -> 3343.00s]  and then recompute the loss
[3343.00s -> 3344.00s]  using our loss function
[3344.00s -> 3345.00s]  and our classifier
[3345.00s -> 3346.00s]  and all that.
[3346.00s -> 3347.00s]  And maybe in this setting
[3347.00s -> 3348.00s]  if we move a little bit
[3348.00s -> 3349.00s]  in the first dimension
[3349.00s -> 3350.00s]  then our loss
[3350.00s -> 3351.00s]  will decrease a little bit
[3351.00s -> 3355.00s]  from 1.2534 to 1.25322.
[3355.00s -> 3356.00s]  And then we can use
[3356.00s -> 3357.00s]  this limit definition
[3357.00s -> 3359.00s]  to come up with this
[3359.00s -> 3361.00s]  finite differences approximation
[3361.00s -> 3362.00s]  to the gradient
[3362.00s -> 3364.00s]  in this first dimension.
[3364.00s -> 3365.00s]  And now you can imagine
[3365.00s -> 3366.00s]  repeating this procedure
[3366.00s -> 3367.00s]  in the second dimension
[3367.00s -> 3368.00s]  where now we take
[3368.00s -> 3369.00s]  the first dimension
[3369.00s -> 3370.00s]  set it back
[3370.00s -> 3371.00s]  to the original value
[3371.00s -> 3372.00s]  and now increment
[3372.00s -> 3373.00s]  the second direction
[3373.00s -> 3374.00s]  by a small step
[3374.00s -> 3375.00s]  and again recompute the loss
[3375.00s -> 3376.00s]  and use this finite
[3376.00s -> 3377.00s]  differences approximation
[3377.00s -> 3378.00s]  to compute
[3378.00s -> 3379.00s]  an approximation
[3379.00s -> 3380.00s]  to the gradient
[3380.00s -> 3381.00s]  in the second slot.
[3381.00s -> 3382.00s]  And now repeat this again
[3382.00s -> 3383.00s]  for the third
[3383.00s -> 3385.00s]  and on and on and on.
[3385.00s -> 3386.00s]  So this is actually
[3386.00s -> 3387.00s]  a terrible idea
[3387.00s -> 3389.00s]  because it's super slow.
[3389.00s -> 3390.00s]  So you might imagine
[3390.00s -> 3392.00s]  that computing this function f
[3392.00s -> 3393.00s]  might actually be super slow
[3393.00s -> 3394.00s]  if it's like a large
[3394.00s -> 3396.00s]  convolutional neural network.
[3396.00s -> 3398.00s]  And this parameter vector w
[3398.00s -> 3399.00s]  probably will not have
[3399.00s -> 3401.00s]  10 entries like it does here.
[3401.00s -> 3402.00s]  It might have like
[3402.00s -> 3403.00s]  tens of millions
[3403.00s -> 3404.00s]  or even hundreds of millions
[3404.00s -> 3405.00s]  for some of these large
[3405.00s -> 3407.00s]  complex deep learning models.
[3407.00s -> 3408.00s]  So in practice
[3408.00s -> 3409.00s]  you'll never want to compute
[3409.00s -> 3410.00s]  your gradients
[3410.00s -> 3411.00s]  via finite differences
[3411.00s -> 3412.00s]  because you'd have to wait
[3412.00s -> 3413.00s]  for hundreds of millions
[3413.00s -> 3414.00s]  potentially of function
[3414.00s -> 3415.00s]  evaluations
[3415.00s -> 3416.00s]  to get a single gradient.
[3416.00s -> 3417.00s]  And that would be super slow
[3417.00s -> 3419.00s]  and super bad.
[3419.00s -> 3420.00s]  But thankfully
[3420.00s -> 3422.00s]  we don't have to do that.
[3422.00s -> 3423.00s]  Hopefully you took
[3423.00s -> 3424.00s]  a calculus course
[3424.00s -> 3425.00s]  at some point in your lives.
[3425.00s -> 3426.00s]  So you know
[3426.00s -> 3427.00s]  that thanks to these guys
[3427.00s -> 3428.00s]  we can just take
[3428.00s -> 3429.00s]  kind of write down
[3429.00s -> 3431.00s]  the expression for our loss
[3431.00s -> 3432.00s]  and then use the
[3432.00s -> 3434.00s]  magical hammer of calculus
[3434.00s -> 3435.00s]  to just write down
[3435.00s -> 3436.00s]  an expression
[3436.00s -> 3437.00s]  for what this gradient
[3437.00s -> 3438.00s]  should be.
[3438.00s -> 3439.00s]  So this will be
[3439.00s -> 3440.00s]  way more efficient
[3440.00s -> 3441.00s]  than trying to compute
[3441.00s -> 3442.00s]  it analytically
[3442.00s -> 3443.00s]  via finite differences.
[3443.00s -> 3444.00s]  One, it'll be exact
[3444.00s -> 3445.00s]  and two it'll be
[3445.00s -> 3446.00s]  much faster
[3446.00s -> 3447.00s]  since we just need
[3447.00s -> 3448.00s]  to compute
[3448.00s -> 3450.00s]  this single expression.
[3450.00s -> 3451.00s]  So what this would look like
[3451.00s -> 3452.00s]  is now
[3452.00s -> 3453.00s]  if we go back
[3453.00s -> 3454.00s]  to this picture
[3454.00s -> 3455.00s]  of our current w
[3455.00s -> 3456.00s]  rather than
[3456.00s -> 3457.00s]  sort of iterating
[3457.00s -> 3458.00s]  over all the dimensions
[3458.00s -> 3459.00s]  of w
[3459.00s -> 3460.00s]  we'll figure out
[3460.00s -> 3461.00s]  ahead of time
[3461.00s -> 3462.00s]  what is the analytic expression
[3462.00s -> 3463.00s]  for the gradient
[3463.00s -> 3464.00s]  and then just write it down
[3464.00s -> 3465.00s]  and go directly
[3465.00s -> 3466.00s]  from the w
[3466.00s -> 3467.00s]  and sort of compute
[3467.00s -> 3468.00s]  one step.
[3468.00s -> 3469.00s]  And that will be
[3469.00s -> 3471.00s]  much better in practice.
[3471.00s -> 3472.00s]  So in summary
[3472.00s -> 3474.00s]  this numerical gradient
[3474.00s -> 3476.00s]  is something that's simple
[3476.00s -> 3477.00s]  and kind of makes sense
[3477.00s -> 3478.00s]  but you won't really
[3478.00s -> 3479.00s]  use it in practice.
[3479.00s -> 3480.00s]  In practice
[3480.00s -> 3481.00s]  you'll always
[3481.00s -> 3482.00s]  take an analytic gradient
[3482.00s -> 3483.00s]  and use that
[3483.00s -> 3484.00s]  when actually performing
[3484.00s -> 3485.00s]  these gradient computations.
[3485.00s -> 3487.00s]  However, one interesting note
[3487.00s -> 3489.00s]  is that these numeric gradients
[3489.00s -> 3490.00s]  are actually a very useful
[3490.00s -> 3491.00s]  debugging tool.
[3491.00s -> 3493.00s]  So say you've written
[3493.00s -> 3494.00s]  some code
[3494.00s -> 3495.00s]  and you wrote some code
[3495.00s -> 3496.00s]  that computes
[3496.00s -> 3497.00s]  the loss
[3497.00s -> 3498.00s]  and the gradient of the loss
[3498.00s -> 3499.00s]  then how do you
[3499.00s -> 3500.00s]  debug this thing
[3500.00s -> 3501.00s]  and how do you make sure
[3501.00s -> 3502.00s]  that this analytic expression
[3502.00s -> 3503.00s]  that you derived
[3503.00s -> 3504.00s]  and wrote down in code
[3504.00s -> 3506.00s]  is actually correct.
[3506.00s -> 3507.00s]  So a really common
[3507.00s -> 3508.00s]  debugging strategy
[3508.00s -> 3509.00s]  for these things
[3509.00s -> 3510.00s]  is to use
[3510.00s -> 3511.00s]  the numeric gradient
[3511.00s -> 3512.00s]  as sort of a unit test
[3512.00s -> 3513.00s]  to make sure
[3513.00s -> 3514.00s]  that your analytic gradient
[3514.00s -> 3515.00s]  was correct.
[3515.00s -> 3516.00s]  Again,
[3516.00s -> 3517.00s]  because this is super slow
[3517.00s -> 3518.00s]  and inexact
[3518.00s -> 3519.00s]  then when doing
[3519.00s -> 3521.00s]  this numeric gradient checking
[3521.00s -> 3522.00s]  as it's called
[3522.00s -> 3523.00s]  you'll tend to scale down
[3523.00s -> 3524.00s]  the parameter
[3524.00s -> 3525.00s]  of the problem
[3525.00s -> 3526.00s]  and it actually runs
[3526.00s -> 3527.00s]  in a reasonable amount of time
[3527.00s -> 3528.00s]  but this ends up being
[3528.00s -> 3529.00s]  a super useful
[3529.00s -> 3530.00s]  debugging strategy
[3530.00s -> 3531.00s]  when you're writing
[3531.00s -> 3532.00s]  your own gradient computations.
[3532.00s -> 3533.00s]  So this is actually
[3533.00s -> 3534.00s]  very commonly used
[3534.00s -> 3535.00s]  in practice
[3535.00s -> 3536.00s]  and you'll do this
[3536.00s -> 3539.00s]  on your assignments as well.
[3539.00s -> 3540.00s]  So then
[3540.00s -> 3541.00s]  once we know
[3541.00s -> 3542.00s]  how to compute the gradient
[3542.00s -> 3543.00s]  then it leads us
[3543.00s -> 3545.00s]  to this super simple algorithm
[3545.00s -> 3546.00s]  that's like three lines
[3546.00s -> 3547.00s]  but turns out to be
[3547.00s -> 3548.00s]  at the heart
[3548.00s -> 3549.00s]  of how we train
[3549.00s -> 3550.00s]  even these very biggest
[3550.00s -> 3551.00s]  most complex
[3551.00s -> 3552.00s]  deep learning algorithms
[3552.00s -> 3553.00s]  and that's gradient descent.
[3553.00s -> 3555.00s]  So gradient descent is like
[3555.00s -> 3557.00s]  first we initialize our w
[3557.00s -> 3558.00s]  as some random thing
[3558.00s -> 3559.00s]  then while true
[3559.00s -> 3561.00s]  we'll compute our loss
[3561.00s -> 3562.00s]  and our gradient
[3562.00s -> 3563.00s]  and then we'll update
[3563.00s -> 3564.00s]  our weights
[3564.00s -> 3566.00s]  in the opposite
[3566.00s -> 3567.00s]  of the gradient direction
[3567.00s -> 3568.00s]  because remember
[3568.00s -> 3569.00s]  that the gradient
[3569.00s -> 3570.00s]  was pointing in the direction
[3570.00s -> 3571.00s]  of greatest increase
[3571.00s -> 3572.00s]  of the function
[3572.00s -> 3573.00s]  so minus gradient points
[3573.00s -> 3574.00s]  in the direction
[3574.00s -> 3575.00s]  of greatest decrease
[3575.00s -> 3576.00s]  so we'll take a small step
[3576.00s -> 3577.00s]  in the direction
[3577.00s -> 3578.00s]  of minus gradient
[3578.00s -> 3579.00s]  and just repeat this forever
[3579.00s -> 3580.00s]  and eventually
[3580.00s -> 3581.00s]  your network will converge
[3581.00s -> 3582.00s]  and you'll be very happy
[3582.00s -> 3583.00s]  hopefully.
[3583.00s -> 3584.00s]  But this step size
[3584.00s -> 3585.00s]  is actually a hyperparameter
[3585.00s -> 3586.00s]  and this tells us
[3586.00s -> 3587.00s]  that every time
[3587.00s -> 3588.00s]  we compute the gradient
[3588.00s -> 3589.00s]  how far do we step
[3589.00s -> 3590.00s]  in that direction
[3590.00s -> 3591.00s]  and this step size
[3591.00s -> 3592.00s]  also sometimes
[3592.00s -> 3593.00s]  called a learning rate
[3593.00s -> 3594.00s]  is probably one
[3594.00s -> 3595.00s]  of the single most
[3595.00s -> 3596.00s]  important hyperparameters
[3596.00s -> 3597.00s]  that you need to set
[3597.00s -> 3598.00s]  when you're actually
[3598.00s -> 3599.00s]  training these things
[3599.00s -> 3600.00s]  in practice.
[3600.00s -> 3601.00s]  Actually for me
[3601.00s -> 3602.00s]  when I'm training these things
[3602.00s -> 3603.00s]  trying to figure out
[3603.00s -> 3604.00s]  this step size
[3604.00s -> 3605.00s]  or this learning rate
[3605.00s -> 3606.00s]  is the first hyperparameter
[3606.00s -> 3607.00s]  that I always check.
[3607.00s -> 3609.00s]  Things like model size
[3609.00s -> 3611.00s]  or regularization strength
[3611.00s -> 3612.00s]  I kind of leave
[3612.00s -> 3613.00s]  until a little bit later
[3613.00s -> 3614.00s]  and getting the learning rate
[3614.00s -> 3615.00s]  or step size correct
[3615.00s -> 3616.00s]  is the first thing
[3616.00s -> 3617.00s]  that I try to set
[3617.00s -> 3619.00s]  at the beginning.
[3619.00s -> 3620.00s]  So kind of pictorially
[3620.00s -> 3621.00s]  what this looks like
[3621.00s -> 3623.00s]  is that maybe we've got this,
[3623.00s -> 3624.00s]  here's a kind of simple
[3624.00s -> 3625.00s]  example in two dimensions.
[3625.00s -> 3626.00s]  So here we've got
[3626.00s -> 3629.00s]  maybe this bowl
[3629.00s -> 3630.00s]  that's showing
[3630.00s -> 3631.00s]  our loss function
[3631.00s -> 3633.00s]  where this red region
[3633.00s -> 3634.00s]  in the center
[3634.00s -> 3635.00s]  is this region
[3635.00s -> 3636.00s]  of low loss
[3636.00s -> 3637.00s]  that we want to get to
[3637.00s -> 3638.00s]  and these blue
[3638.00s -> 3639.00s]  and green regions
[3639.00s -> 3640.00s]  towards the edge
[3640.00s -> 3641.00s]  of the loss
[3641.00s -> 3642.00s]  that we want to avoid.
[3642.00s -> 3643.00s]  So now we're gonna start
[3643.00s -> 3644.00s]  off our W
[3644.00s -> 3645.00s]  at some random point
[3645.00s -> 3646.00s]  in the space
[3646.00s -> 3647.00s]  and then we'll compute
[3647.00s -> 3648.00s]  the negative gradient direction
[3648.00s -> 3649.00s]  which will sort of
[3649.00s -> 3650.00s]  hopefully point us
[3650.00s -> 3651.00s]  in the direction
[3651.00s -> 3652.00s]  of the minima eventually.
[3652.00s -> 3653.00s]  And if we repeat this
[3653.00s -> 3654.00s]  over and over again
[3654.00s -> 3655.00s]  we'll hopefully eventually
[3655.00s -> 3657.00s]  get to the exact minima.
[3657.00s -> 3658.00s]  And what this kind of
[3658.00s -> 3659.00s]  looks like in practice
[3659.00s -> 3661.00s]  is,
[3661.00s -> 3662.00s]  oh man we've got
[3662.00s -> 3664.00s]  this mouse problem again.
[3664.00s -> 3665.00s]  So what this looks like
[3665.00s -> 3666.00s]  in practice is that
[3666.00s -> 3667.00s]  if we repeat this thing
[3667.00s -> 3668.00s]  over and over again
[3669.00s -> 3671.00s]  then we will start off
[3671.00s -> 3672.00s]  at some point
[3672.00s -> 3673.00s]  and then eventually
[3673.00s -> 3675.00s]  just taking tiny gradient steps
[3675.00s -> 3676.00s]  each time
[3676.00s -> 3677.00s]  you'll see that the parameter
[3677.00s -> 3678.00s]  will arc in
[3678.00s -> 3679.00s]  towards the center
[3679.00s -> 3681.00s]  of this region of minima.
[3681.00s -> 3682.00s]  And that's really
[3682.00s -> 3683.00s]  what you want
[3683.00s -> 3684.00s]  because you want
[3684.00s -> 3685.00s]  to get to low loss.
[3685.00s -> 3686.00s]  And by the way
[3686.00s -> 3687.00s]  as a bit of a teaser
[3687.00s -> 3688.00s]  we saw on the previous slide
[3688.00s -> 3689.00s]  this example of like
[3689.00s -> 3691.00s]  very simple gradient descent
[3691.00s -> 3692.00s]  where at every step
[3692.00s -> 3693.00s]  we're just stepping
[3693.00s -> 3694.00s]  in the direction
[3694.00s -> 3695.00s]  of the gradient.
[3695.00s -> 3696.00s]  But in practice
[3696.00s -> 3697.00s]  over the next couple lectures
[3697.00s -> 3699.00s]  there are slightly fancier
[3699.00s -> 3700.00s]  step,
[3700.00s -> 3701.00s]  what are called
[3701.00s -> 3702.00s]  these update rules
[3702.00s -> 3703.00s]  where you can take
[3703.00s -> 3704.00s]  slightly fancier things
[3704.00s -> 3705.00s]  to incorporate gradients
[3705.00s -> 3706.00s]  across multiple time steps
[3706.00s -> 3707.00s]  and stuff like that
[3707.00s -> 3708.00s]  that tend to work
[3708.00s -> 3709.00s]  a little bit better
[3709.00s -> 3710.00s]  in practice and are used
[3710.00s -> 3711.00s]  much more commonly
[3711.00s -> 3713.00s]  than this vanilla gradient descent
[3713.00s -> 3714.00s]  when training these things
[3714.00s -> 3715.00s]  in practice.
[3715.00s -> 3716.00s]  And then as a bit
[3716.00s -> 3717.00s]  of a preview
[3717.00s -> 3718.00s]  we can look at some of these
[3718.00s -> 3719.00s]  fancier,
[3719.00s -> 3720.00s]  some of these slightly fancier
[3720.00s -> 3721.00s]  methods on optimizing
[3721.00s -> 3722.00s]  this same problem.
[3722.00s -> 3723.00s]  So again the black
[3723.00s -> 3724.00s]  will be this
[3724.00s -> 3725.00s]  same gradient computation
[3725.00s -> 3726.00s]  and these green,
[3726.00s -> 3728.00s]  I forgot which color they are,
[3728.00s -> 3729.00s]  but these two other curves
[3729.00s -> 3730.00s]  are using slightly fancier
[3730.00s -> 3731.00s]  update rules
[3731.00s -> 3732.00s]  to decide exactly
[3732.00s -> 3733.00s]  how to use
[3733.00s -> 3734.00s]  the gradient information
[3734.00s -> 3736.00s]  to make our next step.
[3736.00s -> 3738.00s]  So one of these
[3738.00s -> 3740.00s]  is gradient descent
[3740.00s -> 3741.00s]  with momentum,
[3741.00s -> 3742.00s]  the other is this
[3742.00s -> 3743.00s]  Adam optimizer
[3743.00s -> 3744.00s]  and we'll see more details
[3744.00s -> 3745.00s]  about those later
[3745.00s -> 3746.00s]  in the course.
[3746.00s -> 3747.00s]  But the idea is that
[3747.00s -> 3748.00s]  we have this very basic
[3748.00s -> 3749.00s]  algorithm called
[3749.00s -> 3750.00s]  gradient descent
[3750.00s -> 3751.00s]  where we use the gradient
[3751.00s -> 3752.00s]  at every time step
[3752.00s -> 3753.00s]  to determine where
[3753.00s -> 3754.00s]  to step next.
[3754.00s -> 3755.00s]  And there exist
[3755.00s -> 3756.00s]  update rules which tell us
[3756.00s -> 3757.00s]  how exactly do we use
[3757.00s -> 3759.00s]  that gradient information.
[3759.00s -> 3760.00s]  But it's all kind of
[3760.00s -> 3761.00s]  the same basic algorithm
[3761.00s -> 3762.00s]  of trying to go downhill
[3762.00s -> 3764.00s]  at every time step.
[3770.00s -> 3771.00s]  But there's actually
[3771.00s -> 3772.00s]  one more little wrinkle
[3772.00s -> 3773.00s]  that we should talk about.
[3773.00s -> 3775.00s]  So remember that we
[3775.00s -> 3776.00s]  defined our loss function
[3776.00s -> 3777.00s]  as this,
[3777.00s -> 3778.00s]  we defined a loss
[3778.00s -> 3779.00s]  that computes how bad
[3779.00s -> 3780.00s]  is our classifier doing
[3780.00s -> 3782.00s]  at any single training example
[3782.00s -> 3783.00s]  and then we said
[3783.00s -> 3784.00s]  that our full loss
[3784.00s -> 3785.00s]  over the data set
[3785.00s -> 3786.00s]  was going to be
[3786.00s -> 3787.00s]  the average loss
[3787.00s -> 3789.00s]  across the entire training set.
[3789.00s -> 3790.00s]  But in practice,
[3790.00s -> 3791.00s]  this N could be
[3791.00s -> 3793.00s]  very, very large.
[3793.00s -> 3794.00s]  If we're using
[3794.00s -> 3795.00s]  the ImageNet data set
[3795.00s -> 3796.00s]  for example,
[3796.00s -> 3797.00s]  that we talked about
[3797.00s -> 3798.00s]  in the first lecture,
[3798.00s -> 3799.00s]  then N could be
[3799.00s -> 3800.00s]  like 1.3 million.
[3800.00s -> 3802.00s]  So actually computing this loss
[3802.00s -> 3803.00s]  could be actually
[3803.00s -> 3804.00s]  very expensive
[3804.00s -> 3805.00s]  and require computing
[3805.00s -> 3806.00s]  perhaps millions of evaluations
[3806.00s -> 3808.00s]  of this function.
[3808.00s -> 3809.00s]  So that could be
[3809.00s -> 3810.00s]  really slow.
[3810.00s -> 3811.00s]  And actually because
[3811.00s -> 3812.00s]  the gradient
[3812.00s -> 3813.00s]  is a linear operator,
[3813.00s -> 3814.00s]  when you actually try to
[3814.00s -> 3815.00s]  compute the gradient
[3815.00s -> 3816.00s]  of this expression,
[3816.00s -> 3817.00s]  you see that the gradient
[3817.00s -> 3818.00s]  of our loss
[3818.00s -> 3819.00s]  is now the sum
[3819.00s -> 3820.00s]  of the gradient
[3820.00s -> 3821.00s]  of the losses
[3821.00s -> 3822.00s]  for each of
[3822.00s -> 3823.00s]  the individual terms.
[3823.00s -> 3824.00s]  So now if we want
[3824.00s -> 3825.00s]  to compute the gradient,
[3825.00s -> 3826.00s]  again it sort of
[3826.00s -> 3827.00s]  requires us to iterate
[3827.00s -> 3828.00s]  over the entire
[3828.00s -> 3829.00s]  training data set
[3829.00s -> 3830.00s]  all N of these examples.
[3830.00s -> 3831.00s]  So if our N
[3831.00s -> 3832.00s]  was like a million,
[3832.00s -> 3833.00s]  this would be
[3833.00s -> 3834.00s]  super, super slow
[3834.00s -> 3835.00s]  and we would have
[3835.00s -> 3836.00s]  to wait a very,
[3836.00s -> 3837.00s]  very long time
[3837.00s -> 3838.00s]  before we make
[3838.00s -> 3839.00s]  any individual update to W.
[3839.00s -> 3840.00s]  So in practice,
[3840.00s -> 3841.00s]  we tend to use
[3841.00s -> 3842.00s]  what is called
[3843.00s -> 3844.00s]  rather than computing
[3844.00s -> 3845.00s]  the loss and gradient
[3845.00s -> 3846.00s]  over the entire training set,
[3846.00s -> 3848.00s]  instead at every iteration,
[3848.00s -> 3850.00s]  we sample some small set
[3850.00s -> 3851.00s]  of training examples
[3851.00s -> 3852.00s]  called a mini-batch.
[3852.00s -> 3853.00s]  Typically,
[3853.00s -> 3854.00s]  usually this is a power
[3854.00s -> 3855.00s]  of two by convention,
[3855.00s -> 3857.00s]  like 32, 64, 128
[3857.00s -> 3858.00s]  are kind of common numbers.
[3858.00s -> 3859.00s]  And then we'll use
[3859.00s -> 3860.00s]  this small mini-batch
[3860.00s -> 3862.00s]  to compute an estimate
[3862.00s -> 3863.00s]  of the full sum
[3863.00s -> 3864.00s]  and an estimate
[3864.00s -> 3865.00s]  of the true gradient.
[3865.00s -> 3867.00s]  And now this is stochastic
[3867.00s -> 3868.00s]  because you can view this
[3868.00s -> 3871.00s]  as maybe a Monte Carlo estimate
[3871.00s -> 3872.00s]  with some expectation
[3872.00s -> 3875.00s]  of the true value.
[3875.00s -> 3876.00s]  So now,
[3876.00s -> 3877.00s]  this makes our algorithm
[3877.00s -> 3878.00s]  slightly fancier,
[3878.00s -> 3879.00s]  but it's still
[3879.00s -> 3880.00s]  only four lines.
[3880.00s -> 3881.00s]  So now it's well true.
[3881.00s -> 3882.00s]  Sample some random
[3882.00s -> 3884.00s]  mini-batch of data.
[3884.00s -> 3885.00s]  Now, evaluate your loss
[3885.00s -> 3886.00s]  and gradient
[3886.00s -> 3887.00s]  on the mini-batch
[3887.00s -> 3888.00s]  and now make an update
[3888.00s -> 3889.00s]  on your parameters
[3889.00s -> 3890.00s]  based on this estimate
[3890.00s -> 3891.00s]  of the loss
[3891.00s -> 3892.00s]  and this estimate
[3892.00s -> 3894.00s]  of the gradient.
[3894.00s -> 3895.00s]  And again,
[3895.00s -> 3896.00s]  we'll see slightly fancier
[3896.00s -> 3897.00s]  update rules
[3897.00s -> 3899.00s]  of exactly how to
[3899.00s -> 3900.00s]  sort of integrate
[3900.00s -> 3901.00s]  the gradients over time.
[3901.00s -> 3902.00s]  But this is kind of
[3902.00s -> 3903.00s]  the basic training algorithm
[3903.00s -> 3904.00s]  that we use
[3904.00s -> 3905.00s]  for pretty much
[3905.00s -> 3906.00s]  all deep neural networks
[3906.00s -> 3907.00s]  in practice.
[3907.00s -> 3908.00s]  So we have another
[3908.00s -> 3909.00s]  interactive web demo
[3909.00s -> 3911.00s]  about how actually playing around
[3911.00s -> 3912.00s]  with linear classifiers
[3912.00s -> 3913.00s]  and training these things
[3913.00s -> 3915.00s]  via stochastic gradient descent.
[3915.00s -> 3916.00s]  But given how miserable
[3916.00s -> 3918.00s]  the web demo was last time,
[3918.00s -> 3919.00s]  I'm not actually
[3919.00s -> 3920.00s]  gonna open the link.
[3920.00s -> 3921.00s]  Instead,
[3921.00s -> 3922.00s]  I'll just instead
[3922.00s -> 3924.00s]  play this video.
[3924.00s -> 3925.00s]  So here,
[3925.00s -> 3926.00s]  but I encourage you
[3926.00s -> 3927.00s]  to go check this out
[3927.00s -> 3928.00s]  and play with it online
[3928.00s -> 3929.00s]  because it actually helps
[3929.00s -> 3930.00s]  some intuition
[3930.00s -> 3931.00s]  about linear classifiers
[3931.00s -> 3932.00s]  and training them
[3932.00s -> 3933.00s]  via gradient descent.
[3933.00s -> 3934.00s]  So here you can see
[3934.00s -> 3935.00s]  on the left,
[3935.00s -> 3937.00s]  we've got this problem
[3937.00s -> 3938.00s]  where we're categorizing
[3938.00s -> 3939.00s]  three different classes
[3939.00s -> 3940.00s]  and we've drawn
[3940.00s -> 3941.00s]  the decision,
[3941.00s -> 3942.00s]  we've got these green,
[3942.00s -> 3943.00s]  blue, and red points
[3943.00s -> 3944.00s]  that are our training samples
[3944.00s -> 3946.00s]  from these three classes.
[3946.00s -> 3947.00s]  And now we've drawn
[3947.00s -> 3949.00s]  the decision boundaries
[3949.00s -> 3950.00s]  for these classes,
[3950.00s -> 3951.00s]  which are the colored
[3951.00s -> 3952.00s]  background regions,
[3952.00s -> 3954.00s]  as well as these directions
[3954.00s -> 3956.00s]  giving you the direction
[3956.00s -> 3957.00s]  of increase
[3957.00s -> 3958.00s]  for the class scores
[3958.00s -> 3960.00s]  for each of these three classes.
[3960.00s -> 3961.00s]  And now if you see,
[3961.00s -> 3963.00s]  if you actually go
[3963.00s -> 3965.00s]  and play with this thing online,
[3965.00s -> 3966.00s]  you can see that we can
[3966.00s -> 3968.00s]  go in and adjust the Ws
[3968.00s -> 3969.00s]  and changing the values
[3969.00s -> 3970.00s]  of the Ws
[3970.00s -> 3971.00s]  will cause these decision
[3971.00s -> 3972.00s]  boundaries to rotate.
[3972.00s -> 3974.00s]  If you change the biases,
[3974.00s -> 3975.00s]  then the decision boundaries
[3975.00s -> 3976.00s]  will not rotate
[3976.00s -> 3977.00s]  but will instead move
[3977.00s -> 3978.00s]  side to side
[3978.00s -> 3979.00s]  or up and down.
[3979.00s -> 3980.00s]  Then we can actually make steps
[3980.00s -> 3981.00s]  that are trying
[3981.00s -> 3982.00s]  to update this loss.
[3982.00s -> 3983.00s]  You can change the step size
[3983.00s -> 3984.00s]  with this slider.
[3984.00s -> 3985.00s]  You can hit this button
[3985.00s -> 3986.00s]  to actually run the thing.
[3986.00s -> 3987.00s]  So now with a big step size,
[3987.00s -> 3988.00s]  we're running gradient descent
[3988.00s -> 3989.00s]  right now,
[3989.00s -> 3990.00s]  and these decision boundaries
[3990.00s -> 3991.00s]  are flipping around
[3991.00s -> 3994.00s]  and trying to fit the data.
[3994.00s -> 3996.00s]  So it's kind of doing okay now.
[3996.00s -> 3998.00s]  But then we can actually
[3998.00s -> 3999.00s]  change the loss function
[3999.00s -> 4000.00s]  in real time
[4000.00s -> 4001.00s]  between these different
[4001.00s -> 4002.00s]  SVM formulations
[4002.00s -> 4003.00s]  and the different softmax.
[4003.00s -> 4004.00s]  And you can see that
[4004.00s -> 4005.00s]  as you flip between
[4005.00s -> 4006.00s]  these different formulations
[4006.00s -> 4008.00s]  of loss functions,
[4008.00s -> 4009.00s]  it's kind of generally
[4009.00s -> 4010.00s]  doing the same thing.
[4010.00s -> 4011.00s]  Our decision regions
[4011.00s -> 4012.00s]  are mostly in the same place,
[4012.00s -> 4014.00s]  but exactly how they end up
[4014.00s -> 4015.00s]  relative to each other
[4015.00s -> 4016.00s]  and exactly what
[4016.00s -> 4017.00s]  our trade-offs are
[4017.00s -> 4018.00s]  between categorizing
[4018.00s -> 4019.00s]  these different things
[4019.00s -> 4021.00s]  changes a little bit.
[4021.00s -> 4022.00s]  So I really encourage you
[4022.00s -> 4023.00s]  to go online
[4023.00s -> 4024.00s]  and play with this thing
[4024.00s -> 4025.00s]  to try to get some intuition
[4025.00s -> 4026.00s]  for what it actually looks like
[4026.00s -> 4027.00s]  to try to train
[4027.00s -> 4028.00s]  these linear classifiers
[4028.00s -> 4032.00s]  via gradient descent.
[4032.00s -> 4034.00s]  So now as an aside,
[4034.00s -> 4035.00s]  I'd like to talk about
[4035.00s -> 4037.00s]  another idea,
[4037.00s -> 4038.00s]  which is that of
[4038.00s -> 4039.00s]  image features.
[4039.00s -> 4040.00s]  So far we've talked about
[4040.00s -> 4041.00s]  linear classifiers,
[4041.00s -> 4042.00s]  which is just maybe
[4042.00s -> 4043.00s]  taking our raw image pixels
[4043.00s -> 4044.00s]  and then feeding
[4044.00s -> 4045.00s]  the raw pixels themselves
[4045.00s -> 4048.00s]  into our linear classifier.
[4048.00s -> 4049.00s]  But as we talked about
[4049.00s -> 4051.00s]  in the last lecture,
[4051.00s -> 4052.00s]  this is maybe not such
[4052.00s -> 4053.00s]  a great thing to do
[4053.00s -> 4054.00s]  because of things like
[4054.00s -> 4056.00s]  multimodality and whatnot.
[4056.00s -> 4057.00s]  So in practice,
[4057.00s -> 4058.00s]  actually feeding raw
[4058.00s -> 4059.00s]  pixel values
[4059.00s -> 4060.00s]  into linear classifiers
[4060.00s -> 4062.00s]  tends to not work so well.
[4062.00s -> 4063.00s]  So it was actually
[4063.00s -> 4064.00s]  kind of common
[4064.00s -> 4065.00s]  before the dominance
[4065.00s -> 4067.00s]  of deep neural networks
[4067.00s -> 4068.00s]  was instead to have
[4068.00s -> 4069.00s]  this two-stage approach,
[4069.00s -> 4070.00s]  where first you would
[4070.00s -> 4071.00s]  take your image
[4071.00s -> 4072.00s]  and then compute
[4072.00s -> 4073.00s]  various feature
[4073.00s -> 4074.00s]  representations
[4074.00s -> 4075.00s]  of that image
[4075.00s -> 4076.00s]  that are maybe computing
[4076.00s -> 4078.00s]  different kinds of quantities
[4078.00s -> 4079.00s]  relating to the appearance
[4079.00s -> 4080.00s]  of the image
[4080.00s -> 4081.00s]  and then concatenate
[4081.00s -> 4082.00s]  these different feature vectors
[4082.00s -> 4083.00s]  to give you some
[4083.00s -> 4084.00s]  feature representation
[4084.00s -> 4085.00s]  of the image.
[4085.00s -> 4086.00s]  And now this feature
[4086.00s -> 4087.00s]  representation of the image
[4087.00s -> 4088.00s]  would be fed into
[4088.00s -> 4089.00s]  a linear classifier
[4089.00s -> 4090.00s]  rather than feeding
[4090.00s -> 4091.00s]  the raw pixels themselves
[4091.00s -> 4093.00s]  into the classifier.
[4093.00s -> 4094.00s]  And kind of the motivation here
[4094.00s -> 4096.00s]  is that there might be,
[4096.00s -> 4097.00s]  so imagine we have
[4097.00s -> 4098.00s]  a training data set
[4098.00s -> 4099.00s]  on the left
[4099.00s -> 4100.00s]  of these red points
[4100.00s -> 4101.00s]  and red points in the middle
[4101.00s -> 4103.00s]  and blue points around that.
[4103.00s -> 4104.00s]  And for this kind of data set,
[4104.00s -> 4105.00s]  there's no way that we can
[4105.00s -> 4107.00s]  draw a linear decision boundary
[4107.00s -> 4108.00s]  to separate the red points
[4108.00s -> 4109.00s]  from the blue points.
[4109.00s -> 4110.00s]  And we kind of saw more
[4110.00s -> 4111.00s]  examples of this
[4111.00s -> 4112.00s]  in the last lecture.
[4112.00s -> 4113.00s]  But if we use
[4113.00s -> 4114.00s]  a clever feature transform,
[4114.00s -> 4115.00s]  in this case
[4115.00s -> 4117.00s]  transforming to polar coordinates,
[4117.00s -> 4118.00s]  then now after we do
[4118.00s -> 4119.00s]  the feature transform,
[4119.00s -> 4120.00s]  then this kind of complex
[4120.00s -> 4121.00s]  data set
[4121.00s -> 4122.00s]  actually might become
[4122.00s -> 4123.00s]  linearly separable
[4123.00s -> 4124.00s]  and actually could be
[4124.00s -> 4125.00s]  classified correctly
[4125.00s -> 4127.00s]  by a linear classifier.
[4127.00s -> 4128.00s]  And the whole trick here now
[4128.00s -> 4129.00s]  is to kind of figure out
[4129.00s -> 4130.00s]  what is the right
[4130.00s -> 4131.00s]  feature transform
[4131.00s -> 4132.00s]  that is sort of computing
[4132.00s -> 4133.00s]  the right quantities
[4133.00s -> 4134.00s]  for the problem
[4134.00s -> 4135.00s]  that you care about.
[4135.00s -> 4136.00s]  So like for images,
[4136.00s -> 4138.00s]  maybe converting your pixels
[4138.00s -> 4139.00s]  to polar coordinates
[4139.00s -> 4140.00s]  doesn't make sense,
[4140.00s -> 4141.00s]  but you actually can
[4141.00s -> 4142.00s]  try to write down
[4142.00s -> 4143.00s]  feature representations
[4143.00s -> 4144.00s]  of images
[4144.00s -> 4145.00s]  that might make sense
[4145.00s -> 4146.00s]  and actually might help you out
[4146.00s -> 4147.00s]  and might do better
[4147.00s -> 4148.00s]  than putting in raw pixels
[4148.00s -> 4150.00s]  into the classifier.
[4150.00s -> 4151.00s]  So one example
[4151.00s -> 4152.00s]  of this type
[4152.00s -> 4153.00s]  of feature representation
[4153.00s -> 4154.00s]  that's super simple
[4154.00s -> 4155.00s]  is this idea
[4155.00s -> 4156.00s]  of a color histogram.
[4156.00s -> 4157.00s]  So you'll take
[4157.00s -> 4158.00s]  maybe each pixel,
[4158.00s -> 4159.00s]  you'll take this
[4159.00s -> 4161.00s]  like hue color spectrum
[4161.00s -> 4162.00s]  and divide it into buckets,
[4162.00s -> 4163.00s]  and then for every pixel,
[4163.00s -> 4164.00s]  you'll map it
[4164.00s -> 4165.00s]  into one of those
[4165.00s -> 4166.00s]  color buckets
[4166.00s -> 4167.00s]  and then count up
[4167.00s -> 4168.00s]  how many pixels
[4168.00s -> 4169.00s]  fall into each
[4169.00s -> 4170.00s]  of these different buckets.
[4170.00s -> 4171.00s]  So this kind of
[4171.00s -> 4172.00s]  tells you globally
[4172.00s -> 4173.00s]  what kinds of colors
[4173.00s -> 4174.00s]  are in the image.
[4174.00s -> 4175.00s]  So maybe for this example
[4175.00s -> 4176.00s]  of a frog,
[4176.00s -> 4177.00s]  this feature vector
[4177.00s -> 4178.00s]  would tell us
[4178.00s -> 4179.00s]  that there's kind of
[4179.00s -> 4180.00s]  a lot of green stuff
[4180.00s -> 4181.00s]  and maybe not a lot
[4181.00s -> 4182.00s]  of purple or red stuff.
[4182.00s -> 4183.00s]  And this is kind of
[4183.00s -> 4184.00s]  a simple feature vector
[4184.00s -> 4185.00s]  that you might see
[4185.00s -> 4186.00s]  in practice.
[4186.00s -> 4187.00s]  Another kind of
[4187.00s -> 4188.00s]  common feature vector
[4188.00s -> 4189.00s]  that we saw
[4189.00s -> 4190.00s]  before the rise
[4190.00s -> 4191.00s]  or before the dominance
[4191.00s -> 4192.00s]  of neural networks
[4192.00s -> 4193.00s]  was this histogram
[4193.00s -> 4194.00s]  of oriented gradients.
[4194.00s -> 4195.00s]  So remember
[4195.00s -> 4196.00s]  from the first lecture
[4196.00s -> 4197.00s]  that Hubel and Wiesel
[4197.00s -> 4198.00s]  found these oriented edges
[4198.00s -> 4199.00s]  are really important
[4199.00s -> 4201.00s]  in the human visual system.
[4201.00s -> 4202.00s]  And this histogram
[4202.00s -> 4203.00s]  of oriented gradients
[4203.00s -> 4204.00s]  feature representation
[4204.00s -> 4205.00s]  kind of tries to capture
[4205.00s -> 4206.00s]  the same intuition
[4206.00s -> 4207.00s]  and measure
[4207.00s -> 4208.00s]  the local orientation
[4208.00s -> 4210.00s]  of edges on the image.
[4210.00s -> 4211.00s]  So what this thing
[4211.00s -> 4212.00s]  is going to do
[4212.00s -> 4213.00s]  is take our image
[4213.00s -> 4214.00s]  and then divide it
[4214.00s -> 4215.00s]  into these little
[4215.00s -> 4217.00s]  eight by eight pixel regions.
[4217.00s -> 4218.00s]  And then within each
[4218.00s -> 4219.00s]  of those eight
[4219.00s -> 4220.00s]  by eight pixel regions,
[4220.00s -> 4221.00s]  we'll compute what is
[4221.00s -> 4223.00s]  the dominant edge direction
[4223.00s -> 4224.00s]  at each pixel,
[4224.00s -> 4225.00s]  quantize those edge
[4225.00s -> 4226.00s]  directions into several
[4226.00s -> 4227.00s]  buckets, and then within
[4227.00s -> 4228.00s]  each of those regions,
[4228.00s -> 4229.00s]  compute a histogram
[4229.00s -> 4230.00s]  over these different
[4230.00s -> 4232.00s]  edge orientations.
[4232.00s -> 4233.00s]  And now your full
[4233.00s -> 4234.00s]  feature vector
[4234.00s -> 4235.00s]  will be these different
[4235.00s -> 4236.00s]  bucketed histograms
[4236.00s -> 4238.00s]  of edge orientations
[4238.00s -> 4239.00s]  across all the different
[4239.00s -> 4240.00s]  eight by eight regions
[4240.00s -> 4241.00s]  in the image.
[4241.00s -> 4242.00s]  So this kind of
[4242.00s -> 4243.00s]  gives you some,
[4243.00s -> 4244.00s]  this is in some sense
[4244.00s -> 4245.00s]  dual to the
[4245.00s -> 4246.00s]  color histogram classifier
[4246.00s -> 4247.00s]  that we saw before.
[4247.00s -> 4248.00s]  So color histogram
[4248.00s -> 4249.00s]  is kind of saying
[4249.00s -> 4250.00s]  globally what colors
[4250.00s -> 4251.00s]  exist in the image.
[4251.00s -> 4252.00s]  And this is kind of saying
[4252.00s -> 4253.00s]  overall what types
[4253.00s -> 4254.00s]  of edge information
[4254.00s -> 4255.00s]  exist in the image.
[4255.00s -> 4256.00s]  And even localized
[4256.00s -> 4257.00s]  to different parts
[4257.00s -> 4258.00s]  of the image,
[4258.00s -> 4259.00s]  what types of edges
[4259.00s -> 4261.00s]  exist in different regions.
[4261.00s -> 4262.00s]  So maybe for this frog
[4262.00s -> 4263.00s]  on the left,
[4263.00s -> 4264.00s]  you can see
[4264.00s -> 4265.00s]  he's sitting on a leaf
[4265.00s -> 4266.00s]  and these leaves
[4266.00s -> 4267.00s]  have these dominant
[4267.00s -> 4268.00s]  diagonal edges.
[4268.00s -> 4269.00s]  And if you kind of
[4269.00s -> 4270.00s]  visualize the histogram
[4270.00s -> 4271.00s]  of oriented gradient
[4271.00s -> 4272.00s]  features, then you can see
[4272.00s -> 4273.00s]  that in this region
[4273.00s -> 4274.00s]  we've got kind of
[4274.00s -> 4275.00s]  a lot of diagonal edges
[4275.00s -> 4276.00s]  that this histogram
[4276.00s -> 4277.00s]  of oriented gradient
[4277.00s -> 4278.00s]  feature representation
[4278.00s -> 4279.00s]  is capturing.
[4279.00s -> 4280.00s]  So this was
[4280.00s -> 4281.00s]  a super common
[4281.00s -> 4282.00s]  feature representation
[4282.00s -> 4283.00s]  and was used a lot
[4283.00s -> 4284.00s]  for object recognition
[4284.00s -> 4287.00s]  actually not too long ago.
[4287.00s -> 4288.00s]  Another feature representation
[4288.00s -> 4289.00s]  that you might see
[4289.00s -> 4290.00s]  kind of out there
[4290.00s -> 4291.00s]  is this idea
[4291.00s -> 4292.00s]  of bag of words.
[4292.00s -> 4293.00s]  So here we've got this,
[4293.00s -> 4294.00s]  this is kind of
[4294.00s -> 4295.00s]  taking inspiration
[4295.00s -> 4296.00s]  from natural language
[4296.00s -> 4297.00s]  processing.
[4297.00s -> 4298.00s]  So if you've got
[4298.00s -> 4299.00s]  like a paragraph,
[4299.00s -> 4300.00s]  then kind of a way
[4300.00s -> 4301.00s]  that you might represent
[4301.00s -> 4302.00s]  a paragraph by a feature
[4302.00s -> 4303.00s]  vector is kind of
[4303.00s -> 4304.00s]  counting up the occurrences
[4304.00s -> 4305.00s]  of different words
[4305.00s -> 4306.00s]  in that paragraph.
[4306.00s -> 4307.00s]  So we want to kind of
[4307.00s -> 4308.00s]  take that intuition
[4308.00s -> 4309.00s]  and apply it to images
[4309.00s -> 4310.00s]  in some way.
[4310.00s -> 4311.00s]  But the problem is that
[4311.00s -> 4312.00s]  there's no really simple
[4312.00s -> 4313.00s]  straightforward analogy
[4313.00s -> 4315.00s]  of words to images.
[4315.00s -> 4316.00s]  So we need to define
[4316.00s -> 4317.00s]  our own vocabulary
[4317.00s -> 4319.00s]  of visual words.
[4319.00s -> 4320.00s]  So then we take this
[4320.00s -> 4321.00s]  kind of two-stage approach
[4321.00s -> 4322.00s]  where first we'll get
[4322.00s -> 4325.00s]  a bunch of images,
[4325.00s -> 4326.00s]  sample a whole bunch
[4326.00s -> 4327.00s]  of tiny random crops
[4327.00s -> 4328.00s]  from those images,
[4328.00s -> 4329.00s]  and then cluster them
[4329.00s -> 4330.00s]  using something like k-means
[4330.00s -> 4331.00s]  to come up with
[4331.00s -> 4333.00s]  these different cluster centers
[4333.00s -> 4334.00s]  that are representing
[4334.00s -> 4335.00s]  maybe different types
[4335.00s -> 4337.00s]  of visual words in the images.
[4337.00s -> 4338.00s]  So if you look at this example
[4338.00s -> 4339.00s]  on the right here,
[4339.00s -> 4340.00s]  this is kind of
[4340.00s -> 4341.00s]  a real example
[4341.00s -> 4342.00s]  of clustering actually
[4342.00s -> 4343.00s]  different image patches
[4343.00s -> 4344.00s]  from images.
[4344.00s -> 4345.00s]  And you can see that
[4345.00s -> 4346.00s]  after this clustering step,
[4346.00s -> 4347.00s]  our visual words capture
[4347.00s -> 4348.00s]  like these different colors,
[4348.00s -> 4351.00s]  like red and blue and yellow,
[4351.00s -> 4352.00s]  as well as these different
[4352.00s -> 4353.00s]  types of oriented edges
[4353.00s -> 4355.00s]  in different directions.
[4355.00s -> 4356.00s]  Which is kind of interesting
[4356.00s -> 4357.00s]  that now we're starting
[4357.00s -> 4358.00s]  to see these oriented edges
[4358.00s -> 4359.00s]  kind of come out
[4359.00s -> 4360.00s]  from the data
[4360.00s -> 4361.00s]  in a data-driven way.
[4361.00s -> 4362.00s]  And now once we've got
[4362.00s -> 4363.00s]  these set of visual words,
[4363.00s -> 4364.00s]  also called a codebook,
[4364.00s -> 4366.00s]  then we can encode our image
[4366.00s -> 4367.00s]  by trying to say
[4367.00s -> 4369.00s]  for each of these visual words,
[4369.00s -> 4371.00s]  how much does this visual word
[4371.00s -> 4373.00s]  occur in the image?
[4373.00s -> 4374.00s]  And now this gives us,
[4374.00s -> 4375.00s]  again, some slightly
[4375.00s -> 4376.00s]  different information
[4376.00s -> 4377.00s]  about what is the visual
[4377.00s -> 4378.00s]  appearance of this image.
[4378.00s -> 4379.00s]  And this was another,
[4379.00s -> 4380.00s]  actually this is
[4380.00s -> 4382.00s]  a type of feature representation
[4382.00s -> 4383.00s]  that Fei Fei worked on
[4383.00s -> 4385.00s]  when she was a grad student.
[4385.00s -> 4386.00s]  So it's actually,
[4386.00s -> 4387.00s]  so this is something
[4387.00s -> 4388.00s]  that you saw in practice
[4388.00s -> 4391.00s]  not too long ago.
[4391.00s -> 4392.00s]  So then kind of
[4392.00s -> 4395.00s]  as a bit of a teaser,
[4395.00s -> 4396.00s]  kind of tying this
[4396.00s -> 4397.00s]  all back together,
[4397.00s -> 4398.00s]  kind of the way
[4398.00s -> 4400.00s]  that this image classification
[4400.00s -> 4401.00s]  pipeline might have looked like
[4401.00s -> 4402.00s]  maybe about five
[4402.00s -> 4403.00s]  to 10 years ago,
[4403.00s -> 4404.00s]  would be that you would
[4404.00s -> 4405.00s]  take your image
[4405.00s -> 4406.00s]  and then compute
[4406.00s -> 4407.00s]  these different feature
[4407.00s -> 4408.00s]  representations of your image,
[4408.00s -> 4409.00s]  things like bag of words
[4409.00s -> 4410.00s]  or histogram
[4410.00s -> 4412.00s]  of oriented gradients,
[4412.00s -> 4413.00s]  concatenate a whole bunch
[4413.00s -> 4414.00s]  of features together,
[4414.00s -> 4415.00s]  and then feed
[4415.00s -> 4416.00s]  these feature extractors
[4416.00s -> 4417.00s]  down into some
[4417.00s -> 4419.00s]  linear classifier.
[4419.00s -> 4420.00s]  I'm simplifying a little bit.
[4420.00s -> 4421.00s]  The pipelines were
[4421.00s -> 4422.00s]  a little bit more complex
[4422.00s -> 4423.00s]  than that,
[4423.00s -> 4424.00s]  but this is kind of
[4424.00s -> 4425.00s]  the general intuition.
[4425.00s -> 4426.00s]  And then the idea here
[4426.00s -> 4428.00s]  was that after you extracted
[4428.00s -> 4429.00s]  these features,
[4429.00s -> 4430.00s]  this feature extractor
[4430.00s -> 4431.00s]  would be kind of
[4431.00s -> 4432.00s]  a fixed block
[4432.00s -> 4433.00s]  that would not be updated
[4433.00s -> 4434.00s]  during training.
[4434.00s -> 4435.00s]  And during training,
[4435.00s -> 4436.00s]  you would only update
[4436.00s -> 4437.00s]  the linear classifier
[4437.00s -> 4438.00s]  that's working
[4438.00s -> 4439.00s]  on top of features.
[4439.00s -> 4440.00s]  And actually,
[4440.00s -> 4441.00s]  I would argue that
[4441.00s -> 4442.00s]  once we move
[4442.00s -> 4443.00s]  to convolutional
[4443.00s -> 4444.00s]  neural networks
[4444.00s -> 4445.00s]  and these deep
[4445.00s -> 4446.00s]  neural networks,
[4446.00s -> 4447.00s]  it actually doesn't
[4447.00s -> 4448.00s]  look that different.
[4448.00s -> 4449.00s]  The only difference
[4449.00s -> 4450.00s]  is that rather than
[4450.00s -> 4451.00s]  going ahead of time,
[4451.00s -> 4452.00s]  we're going to learn
[4452.00s -> 4453.00s]  the features directly
[4453.00s -> 4454.00s]  from the data.
[4454.00s -> 4456.00s]  So we'll take our raw pixels
[4456.00s -> 4457.00s]  and feed them
[4457.00s -> 4458.00s]  into this convolutional network,
[4458.00s -> 4459.00s]  which will end up
[4459.00s -> 4460.00s]  computing through
[4460.00s -> 4461.00s]  many different layers
[4461.00s -> 4462.00s]  some type of feature
[4462.00s -> 4463.00s]  representation driven
[4463.00s -> 4464.00s]  by the data.
[4464.00s -> 4465.00s]  And then we'll actually
[4465.00s -> 4466.00s]  train this entire weights
[4466.00s -> 4467.00s]  for this entire network
[4467.00s -> 4468.00s]  rather than just the weights
[4468.00s -> 4469.00s]  of the linear classifier
[4469.00s -> 4471.00s]  on top.
[4471.00s -> 4472.00s]  So next time,
[4472.00s -> 4473.00s]  we'll really start
[4473.00s -> 4474.00s]  diving into this idea
[4474.00s -> 4475.00s]  in more detail,
[4475.00s -> 4476.00s]  and we'll introduce
[4476.00s -> 4477.00s]  some neural networks
[4477.00s -> 4478.00s]  and start talking
[4478.00s -> 4480.00s]  about that as well.
