# Detected language: en (p=1.00)

[0.00s -> 8.16s]  Today, we're going to continue our discussion
[8.16s -> 9.16s]  of transactional memory.
[9.16s -> 14.68s]  We'll talk about software and hardware implementations.
[14.68s -> 18.96s]  Just to kind of remind you of where we left off
[18.96s -> 22.44s]  at the end of last lecture, we talked about,
[22.44s -> 24.20s]  well, during the lecture, we talked
[24.20s -> 28.28s]  about all the transactional memory properties
[28.28s -> 30.08s]  that we wanted to preserve.
[30.08s -> 33.20s]  And we talked about the advantages,
[33.20s -> 37.96s]  and we talked about the different kinds of aspects
[37.96s -> 39.88s]  of the transactional memory design
[39.88s -> 41.00s]  that you need to consider.
[41.00s -> 43.24s]  One was sort of what data versioning
[43.24s -> 49.04s]  policy you use to keep track of the old transactional data
[49.04s -> 52.80s]  that had been committed and the new transactional data that
[52.80s -> 55.14s]  is being created as the transactions execute,
[55.14s -> 56.64s]  and then how we actually detected
[56.68s -> 59.20s]  conflicts between transactions.
[59.20s -> 61.28s]  And we said there were two data versioning
[61.28s -> 64.08s]  policies, eager and lazy.
[64.08s -> 66.84s]  And then we said that there were two conflict detection
[66.84s -> 71.96s]  policies, pessimistic, where you detect the conflict
[71.96s -> 75.28s]  as the memory references are made,
[75.28s -> 79.00s]  and then optimistic, which assumes that there
[79.00s -> 80.28s]  will be no conflicts.
[80.28s -> 86.12s]  And you detect the conflicts when any transaction commits.
[87.00s -> 90.52s]  And so we did some examples, and we
[90.52s -> 98.52s]  saw that in a certain case for the pessimistic conflict
[98.52s -> 102.88s]  detection policy, that sometimes we
[102.88s -> 106.08s]  had a situation where we had live lock,
[106.08s -> 108.84s]  and we needed to detect that, and the transactional memory
[108.84s -> 112.40s]  system would have to recover from live lock
[112.40s -> 115.24s]  to make sure that the application could make forward
[115.24s -> 116.72s]  progress.
[116.72s -> 119.00s]  And then we also did the optimistic case,
[119.00s -> 122.26s]  and we saw in the case where you had live lock.
[122.26s -> 127.08s]  In the pessimistic case, you didn't have live lock here.
[127.08s -> 131.04s]  And the reason is, of course, that the committing transaction
[131.04s -> 132.88s]  always wins.
[132.88s -> 135.14s]  And the committing transaction, remember, the check
[135.14s -> 139.64s]  was you compare the write state of the committing
[139.64s -> 142.88s]  transaction with the read state of all the outstanding
[142.88s -> 145.24s]  transactions that are still in execution.
[145.24s -> 147.84s]  And if there's a conflict, then those transactions
[147.84s -> 152.44s]  have to abort, and the committing transaction
[152.44s -> 154.16s]  gets to commit.
[154.16s -> 157.56s]  OK, so any questions on what we've
[157.56s -> 159.76s]  discussed so far as far as the way
[159.76s -> 161.84s]  transactional memory is supposed to work
[161.84s -> 167.24s]  and the properties that needed to be preserved?
[167.24s -> 173.28s]  All right, so let's discuss an explicit implementation
[173.28s -> 175.60s]  or a couple of explicit implementations.
[175.60s -> 177.50s]  As you can imagine, given that you've
[177.50s -> 180.96s]  got a space of policies that you can implement,
[180.96s -> 185.96s]  there's a wide range of different implementations
[185.96s -> 190.92s]  that pick different points in the design space of the policies
[190.92s -> 192.68s]  that you select, right?
[192.68s -> 194.64s]  So you can have lazy optimistic.
[194.64s -> 200.64s]  You can have eager optimistic, eager pessimistic.
[200.64s -> 205.36s]  As we've already discussed, eager optimistic
[205.36s -> 206.84s]  doesn't work out too well, right?
[206.84s -> 209.64s]  Because at the point at which you
[209.64s -> 213.20s]  detect that there is a conflict,
[213.20s -> 216.52s]  you may have already kind of modified the memory in ways
[216.52s -> 218.84s]  that are difficult to recover from.
[218.84s -> 220.52s]  So those are the software systems.
[220.52s -> 222.60s]  The software system that we're going to look at
[222.60s -> 225.52s]  is, in fact, a kind of hybrid that
[225.52s -> 229.20s]  is eager optimistic for reads, but pessimistic for writes.
[229.20s -> 233.92s]  And we'll see how you can have both at the same time
[233.92s -> 237.20s]  by treating reads and writes as slightly differently.
[237.20s -> 239.64s]  And then, of course, we'll also look at some hardware TM
[239.64s -> 240.12s]  systems.
[240.12s -> 242.36s]  First of all, we're going to motivate why you want hardware.
[242.36s -> 243.88s]  And of course, you always want hardware
[243.88s -> 245.64s]  if you want to improve performance, right?
[245.64s -> 249.32s]  And so there are performance overheads
[249.32s -> 252.24s]  that we'll see from the software systems that
[252.28s -> 256.48s]  can be mitigated by using hardware.
[256.48s -> 258.12s]  And of course, you can have some sort
[258.12s -> 260.84s]  of hybrid where you kind of mix hardware and software,
[260.84s -> 264.48s]  and you try and attack the components of the software
[264.48s -> 269.76s]  systems that are really going to provide the most overhead
[269.76s -> 271.84s]  and try and reduce those overheads
[271.84s -> 275.16s]  by judicious use of hardware.
[275.16s -> 279.32s]  OK, so let's start by looking at software transactional memory
[279.32s -> 280.04s]  implementation.
[280.08s -> 282.96s]  And this is sort of a canonical way
[282.96s -> 286.12s]  of implementing a transactional system in software,
[286.12s -> 289.16s]  but it's applied in this situation
[289.16s -> 290.76s]  to transactional memory.
[290.76s -> 293.24s]  But you could imagine implementing something
[293.24s -> 295.52s]  like this for other cases where
[295.52s -> 300.04s]  you wanted to keep data consistent in some manner.
[300.04s -> 303.72s]  All right, so given this atomic region,
[303.72s -> 310.04s]  you've got reads and writes that are shown in the code
[310.04s -> 311.92s]  here, in the statements of the code.
[311.92s -> 318.20s]  And somehow, you need to indicate to the transactional
[318.20s -> 324.28s]  memory system what actions are being taken place
[324.28s -> 326.00s]  inside the program.
[326.00s -> 327.56s]  And that is done by the use of what
[327.56s -> 329.32s]  we call software barriers.
[329.32s -> 330.68s]  And software barriers are basically
[330.68s -> 335.16s]  calls into the transactional memory system
[335.16s -> 342.48s]  to indicate to the system what bookkeeping has to be taken.
[342.48s -> 345.80s]  So the idea is that you've got to rewrite the code
[345.80s -> 349.96s]  that the programmer gives you and add these extra calls
[349.96s -> 351.96s]  into the transactional memory system.
[351.96s -> 359.68s]  So for every write, you need to put in a TM write.
[359.68s -> 365.92s]  And for every read, you need to make that into a TM read.
[365.92s -> 369.48s]  So these are called software barriers,
[369.48s -> 372.24s]  not to be confused with barriers for synchronization.
[372.24s -> 375.68s]  This is just basically a call into the transactional memory
[375.68s -> 383.36s]  system specified by these TM read and TM write functions.
[383.36s -> 386.42s]  So you need this bookkeeping, of course,
[386.42s -> 388.20s]  to keep track of what's happening
[388.20s -> 390.92s]  with the state of the transactional memory system.
[390.92s -> 395.04s]  As we'll see, a lot of these TM calls are redundant.
[395.04s -> 397.28s]  And you can kind of optimize them away.
[397.28s -> 402.04s]  And we'll see an example of that in just a moment.
[402.04s -> 403.92s]  All right, so we essentially now have
[403.92s -> 409.20s]  to translate this code on the left, which the programmer wrote
[409.20s -> 411.92s]  into something that interacts with the transactional memory
[411.92s -> 412.68s]  system.
[412.68s -> 415.32s]  And of course, if you have code
[415.32s -> 418.00s]  that is both executed inside transactions
[418.00s -> 420.56s]  and outside transactions, then you need two copies,
[420.56s -> 424.00s]  one that doesn't have the instrumentation on the left
[424.00s -> 426.16s]  and one that does on the right.
[426.16s -> 433.60s]  And if you've got some sort of virtual memory system,
[433.60s -> 436.80s]  such as if you're running in Java or in C Sharp,
[436.80s -> 439.78s]  then maybe you can do some of this what's called function
[439.78s -> 443.28s]  cloning automatically, where you clone the function
[443.28s -> 446.44s]  and you add the appropriate software barriers
[446.44s -> 448.40s]  to interact with the transactional memory system.
[450.92s -> 455.84s]  All right, so now the question is what data structures
[455.84s -> 460.76s]  do you need to keep track of the transactional memory
[460.76s -> 464.36s]  state and make sure that everything operates correctly?
[464.36s -> 470.36s]  So we need some way of sort of mapping the data that
[470.36s -> 473.20s]  is being manipulated by the application
[473.24s -> 476.84s]  into the data structures that need
[476.84s -> 481.52s]  to be used to track the transactional memory state.
[481.52s -> 485.76s]  And so this is called a transactional descriptor.
[485.76s -> 488.32s]  And this can be two components.
[488.32s -> 490.08s]  This is a transactional descriptor,
[490.08s -> 491.80s]  which is per thread, right?
[491.80s -> 498.92s]  So this is information about undo logs, write buffers,
[498.92s -> 501.24s]  conflict detection and the sort,
[501.28s -> 503.40s]  so you need the read set.
[503.40s -> 507.56s]  And the write set is also part of the transaction descriptor.
[507.56s -> 510.20s]  And this is per thread or per transaction.
[510.20s -> 511.88s]  You need a transaction descriptor.
[511.88s -> 514.16s]  And then you need a transaction record
[514.16s -> 516.60s]  per data element.
[516.60s -> 521.04s]  And this could be associated with objects
[521.04s -> 524.36s]  or with a finer granularity of data within the objects,
[524.36s -> 525.08s]  right?
[525.08s -> 529.52s]  And so essentially here with the transaction record,
[529.52s -> 531.68s]  you're going to essentially keep
[531.68s -> 536.72s]  the metadata that tells you how the data is
[536.72s -> 537.68s]  being accessed, right?
[537.68s -> 541.44s]  And so if you make an analog to our discussion of cache
[541.44s -> 543.04s]  coherence, right, where we said
[543.04s -> 546.28s]  that you needed the cache state to tell you
[546.28s -> 553.60s]  whether the data was shared by multiple CPUs,
[553.60s -> 555.56s]  or in this case, multiple transactions,
[555.56s -> 559.00s]  or whether it was exclusive to one transaction, right?
[559.00s -> 562.60s]  So there's a direct analog between the way
[562.60s -> 565.68s]  that we handle transactions and the way
[565.68s -> 568.88s]  that we discuss doing cache coherency, OK?
[568.88s -> 573.84s]  So then the question is, how do you actually
[573.84s -> 579.16s]  map the data that is being used by the program
[579.16s -> 587.56s]  to the data to the transaction record?
[587.60s -> 590.32s]  And so you can do this in two ways.
[590.32s -> 595.80s]  You can do it per object, where each object has
[595.80s -> 598.24s]  a transaction record.
[598.24s -> 600.88s]  And so this is shown by the kind of top.
[600.88s -> 603.04s]  You either use some sort of table
[603.04s -> 606.76s]  associated with the object, or you have some hash
[606.76s -> 611.32s]  into a global data table.
[611.32s -> 617.60s]  Or you could do it per individual data element, right?
[617.60s -> 620.84s]  And of course, in C or C++, where you might
[620.84s -> 623.88s]  have to do this based on address.
[623.88s -> 625.84s]  And so the question is, of course,
[625.84s -> 629.44s]  this is a finer granularity of information
[629.44s -> 634.32s]  about the data than doing it per object, right?
[634.32s -> 636.56s]  So there is some trade-off to be had.
[636.56s -> 639.36s]  And as you're already thinking about this,
[639.36s -> 643.44s]  because you're making the analogy to cache coherence,
[643.44s -> 646.40s]  you're thinking, so what is the trade-off
[646.40s -> 648.48s]  that we're going to see between doing things
[648.48s -> 650.72s]  at the object level or doing things
[650.72s -> 653.92s]  at a finer granularity at the individual field
[653.92s -> 665.04s]  or individual 32-bit or 64-bit data element level?
[665.04s -> 666.64s]  Yeah?
[666.72s -> 670.40s]  I guess when you have something that's an object level,
[670.40s -> 674.60s]  you assume all the properties are handled together,
[674.60s -> 676.76s]  even though they could actually be independent.
[676.76s -> 679.52s]  X and Y could be independent, so you prefer finer granularity.
[679.52s -> 683.52s]  So what is going to be the impact of doing that?
[683.52s -> 686.16s]  If it's not independent, then the transactions
[686.16s -> 688.88s]  that we saw previously, the way GenDAC, when it works,
[688.88s -> 692.32s]  it will have unnecessary reports and restarts.
[692.32s -> 693.12s]  Exactly, right.
[693.12s -> 696.48s]  You'll have more conflicts than really required, right?
[696.48s -> 698.64s]  So this is exactly the trade-off.
[698.64s -> 701.92s]  And so with object granularity, you're
[701.92s -> 705.08s]  going to have lower overhead of actually keeping
[705.08s -> 708.64s]  track of the data.
[708.64s -> 712.84s]  And you might have some opportunities for optimization
[712.84s -> 716.44s]  if you are doing repeated, you're
[716.44s -> 718.92s]  previously using the same object,
[718.92s -> 726.40s]  then you can amortize the overhead of managing the data
[726.40s -> 728.04s]  record for that object.
[728.04s -> 729.72s]  But the key thing that you do get
[729.72s -> 731.56s]  with this course of granularity
[731.56s -> 736.44s]  are these false conflicts, as you pointed out, right?
[736.44s -> 739.36s]  So here's the example then, right?
[739.36s -> 743.12s]  So if you've got element or field granularity,
[743.12s -> 747.04s]  then you can reduce the incidence of false conflicts
[747.04s -> 749.80s]  and potentially improve concurrency.
[749.80s -> 753.08s]  And if you just look at the example on the right here,
[753.12s -> 758.96s]  we access the field x and y from object A in transaction 1.
[758.96s -> 765.16s]  And we access the field z in transaction 2.
[765.16s -> 768.60s]  And if you're doing things on an object level, of course,
[768.60s -> 770.80s]  these two transactions conflict.
[770.80s -> 773.80s]  But if you're doing things at a field level of granularity,
[773.80s -> 775.80s]  then there is no conflict, right?
[775.80s -> 777.52s]  So that's the benefit.
[777.52s -> 784.24s]  And of course, the downside of doing things at the element
[784.24s -> 787.56s]  or field granularity is increased overhead, right?
[787.56s -> 789.48s]  And then, of course, you can imagine
[789.48s -> 792.44s]  doing things at cache line because that matches what
[792.44s -> 794.04s]  you might do in hardware.
[794.04s -> 796.00s]  But then this doesn't have a direct analog
[796.00s -> 797.92s]  with what you wrote as a programmer,
[797.92s -> 801.56s]  so it might be difficult to understand what's going on.
[801.56s -> 804.32s]  And if you're doing things in software,
[804.32s -> 805.96s]  you could mix and match things, right?
[805.96s -> 808.80s]  You might say, well, for objects,
[808.80s -> 816.20s]  I'm going to do things at the object level.
[816.20s -> 818.68s]  And then for arrays, I might think
[818.68s -> 821.16s]  about doing things at the element level.
[821.16s -> 825.80s]  And this turns out to be a pretty good trade-off.
[825.80s -> 831.96s]  OK, so with that in mind, so we have a way of thinking
[831.96s -> 835.12s]  about data structures per transaction.
[835.64s -> 838.12s]  The transaction descriptor, and then we
[838.12s -> 841.84s]  have the data record for the individual data.
[841.84s -> 848.76s]  So now let's talk about a specific STM algorithm.
[848.76s -> 854.44s]  And the one that we're going to use comes from Intel.
[854.44s -> 856.32s]  It was developed quite a few years ago.
[856.32s -> 858.64s]  It's called MacCarty.
[858.72s -> 865.40s]  And it is an eager versioning with optimistic reads,
[865.40s -> 867.56s]  which means, we'll say explicitly
[867.56s -> 870.92s]  how the reads are optimistic in just a moment,
[870.92s -> 872.96s]  and then pessimistic writes, which
[872.96s -> 875.36s]  means you're going to take locks in order
[875.36s -> 878.36s]  to do the writes.
[878.36s -> 882.00s]  And so it's based on this idea of versioning or time
[882.00s -> 886.76s]  stamping to determine what data is being
[886.76s -> 889.16s]  shared by multiple transactions.
[889.16s -> 891.48s]  And so the idea is that there is a global time
[891.48s -> 895.16s]  stamp that is incremented when a transaction commits.
[895.16s -> 898.52s]  So any transaction that commits in the system
[898.52s -> 900.84s]  increments the global time stamp.
[900.84s -> 902.92s]  And then there's a local time stamp,
[902.92s -> 907.56s]  which gets stamped on the transaction when it begins.
[907.56s -> 912.56s]  And so this will be used for determining whether conflicts
[912.56s -> 914.48s]  happen on data.
[914.48s -> 919.52s]  And then there's a transaction record associated
[919.52s -> 921.80s]  with each of the transactions.
[927.00s -> 929.04s]  So transaction record is associated with each
[929.04s -> 930.88s]  of the data, I should say.
[930.88s -> 939.16s]  And it's a 32-bit value with the most significant bit.
[939.16s -> 944.44s]  Sorry, the least significant bit is
[944.44s -> 948.64s]  either 0, which means it's locked.
[948.64s -> 954.00s]  And that means that this would be a transaction pointer.
[959.72s -> 964.28s]  And then if the least significant bit is a 1,
[964.28s -> 967.76s]  then this is essentially a version number.
[967.76s -> 971.04s]  So by looking at the least significant bit
[971.04s -> 972.80s]  of the transaction record, you can
[972.84s -> 981.08s]  tell whether the particular object or the data element is.
[981.08s -> 987.36s]  So you can think of this as when you've got a 1,
[987.36s -> 991.84s]  how should you think about the state of the data?
[991.84s -> 995.08s]  Make the analog to cache coherence.
[995.08s -> 996.00s]  Yeah?
[996.00s -> 997.04s]  It's shared, right?
[997.04s -> 1000.16s]  And when it's a 0, it's exclusive, right?
[1000.20s -> 1003.64s]  And it's being written by this transaction, the one that
[1003.64s -> 1005.64s]  says the pointer, right?
[1005.64s -> 1007.40s]  So that's the way to think about things.
[1007.40s -> 1012.72s]  So we're either in shared or exclusive mode.
[1012.72s -> 1017.52s]  And the transaction record indicates that.
[1017.52s -> 1019.68s]  All right, so now that we've got
[1019.68s -> 1025.80s]  the notion of version tracking and transaction record,
[1025.80s -> 1028.48s]  let's think about exactly what operations
[1028.52s -> 1031.08s]  we need to perform in order to implement
[1031.08s -> 1034.84s]  transactional memory in this software environment, right?
[1034.84s -> 1039.20s]  So you've got, for every time that you want to do a read,
[1039.20s -> 1041.96s]  you're going to call STM read.
[1041.96s -> 1044.88s]  And this is optimistic in the sense
[1044.88s -> 1047.52s]  that even though, as we'll see,
[1047.52s -> 1053.12s]  you do try and validate the data you are about to read,
[1053.12s -> 1056.20s]  once you've done that, you're golden
[1056.24s -> 1061.28s]  until some other transaction decides to commit.
[1061.28s -> 1063.48s]  All right, so in this case, it's eager.
[1063.48s -> 1066.84s]  So you would just direct read from the memory location.
[1066.84s -> 1069.28s]  You validate the day that you're
[1069.28s -> 1072.12s]  trying to read by checking to see that it's unlocked, right?
[1072.12s -> 1075.24s]  So if it's locked, then what do you
[1075.24s -> 1078.60s]  know about the day that you're trying to read?
[1078.60s -> 1079.92s]  Yeah?
[1079.92s -> 1084.12s]  It could be invalid.
[1084.16s -> 1089.56s]  What do you know specifically about the day
[1089.56s -> 1091.24s]  that you're trying to read?
[1091.24s -> 1093.60s]  It could be stale because someone else could be read.
[1093.60s -> 1105.20s]  Well, let's suppose that, to make things a little more
[1105.20s -> 1111.92s]  clear, let's assume that you're
[1111.92s -> 1115.76s]  at a point in the program and you're doing this STM read
[1115.76s -> 1119.20s]  and you direct read from the memory,
[1119.20s -> 1127.52s]  but you could potentially not read, right?
[1127.52s -> 1129.48s]  You could do the check first.
[1129.48s -> 1131.24s]  Let's assume you do the check first.
[1131.24s -> 1132.24s]  Then what do you know?
[1132.24s -> 1140.76s]  If the data is locked, what do you know?
[1148.08s -> 1149.40s]  It might be a problem.
[1149.40s -> 1151.56s]  Let's not make it a problem, so thank you.
[1151.56s -> 1154.48s]  You know that there is a transaction that
[1154.48s -> 1156.52s]  is writing that data, right?
[1156.52s -> 1159.16s]  So should you go ahead and read it?
[1159.16s -> 1160.08s]  Probably not.
[1160.08s -> 1163.08s]  You should probably, you know, your contention manager
[1163.08s -> 1165.76s]  might say, well, let's just wait.
[1165.76s -> 1168.28s]  Let's just wait for that transaction to finish.
[1168.28s -> 1170.68s]  And when it's done, then we'll read the data, right?
[1170.68s -> 1172.60s]  So you can imagine at this point,
[1172.60s -> 1175.80s]  if it's locked, that you just wait.
[1175.80s -> 1181.40s]  If it's unlocked, that means that some other,
[1181.40s -> 1183.64s]  if it's unlocked and the data version
[1183.64s -> 1186.40s]  is less than the local timestamp, what does that mean?
[1190.08s -> 1193.04s]  Yeah.
[1193.04s -> 1196.08s]  It's in a shared state that you can take it and submit it.
[1196.08s -> 1198.52s]  OK, but what does it mean that the data version is
[1198.52s -> 1200.32s]  less than the local timestamp?
[1200.32s -> 1201.40s]  Yeah.
[1201.40s -> 1203.52s]  That means that something hasn't changed
[1203.52s -> 1205.44s]  before the transaction started.
[1205.44s -> 1207.28s]  Right, so voila.
[1207.28s -> 1210.64s]  After the transaction started, some other transaction
[1210.64s -> 1212.72s]  has modified it.
[1212.72s -> 1215.40s]  Now, does it mean that this transaction
[1215.40s -> 1218.04s]  has read the wrong thing?
[1218.04s -> 1219.40s]  Not yet.
[1219.40s -> 1223.88s]  Because, in fact, the data has been updated.
[1223.88s -> 1227.20s]  You know, it got the latest copy, right?
[1227.20s -> 1231.24s]  But now it needs to make sure that all the rest of its data
[1231.24s -> 1232.64s]  is actually good.
[1232.64s -> 1236.04s]  And so it tries to validate the whole of its read set
[1236.04s -> 1238.60s]  again, and we'll talk about exactly what that means.
[1238.60s -> 1240.02s]  But it just wants to make sure
[1240.02s -> 1242.08s]  that everything is up to date.
[1242.08s -> 1246.04s]  OK, so if we validate the read data,
[1246.04s -> 1248.12s]  then we can insert it in the read set
[1248.12s -> 1249.56s]  and return the value, right?
[1249.56s -> 1253.08s]  So clearly there's more work to do with transactional memory
[1253.08s -> 1254.96s]  than just reading the data, right?
[1254.96s -> 1257.92s]  You need to keep track of the state
[1257.92s -> 1259.48s]  of the transactional memory.
[1259.48s -> 1263.08s]  Now, STM write is pessimistic in that you now
[1263.08s -> 1266.20s]  have to check and grab a lock, right?
[1266.20s -> 1271.04s]  So you check if it's unlocked and the data version is
[1271.04s -> 1273.92s]  less than the local timestamp.
[1273.92s -> 1278.68s]  And if back to check passes, then you acquire the lock,
[1278.68s -> 1283.92s]  insert it in the write set, create your undo log entry,
[1283.92s -> 1286.44s]  and write the data in place because this is, in fact,
[1286.44s -> 1287.20s]  eager, right?
[1287.20s -> 1292.60s]  So you grab the lock, and that makes this policy
[1292.60s -> 1295.36s]  for doing writes pessimistic.
[1295.36s -> 1300.04s]  But it's optimistic for reads because you didn't actually
[1300.04s -> 1305.36s]  check to see that there were any readers that had read
[1305.36s -> 1307.96s]  this data here, right?
[1307.96s -> 1312.60s]  So the readers can blithely go along.
[1312.60s -> 1316.28s]  It might turn out that later, when this transaction tries
[1316.28s -> 1319.28s]  to commit, that the conflict gets detected.
[1319.28s -> 1321.52s]  But it doesn't get detected at the time
[1321.52s -> 1322.76s]  that the write is happening.
[1322.76s -> 1327.76s]  So in that sense, it's optimistic for reads.
[1327.76s -> 1328.80s]  OK, any questions?
[1330.04s -> 1335.64s]  All right, so how do we do read set validation?
[1335.64s -> 1337.64s]  Well, we get the global timestamp.
[1337.64s -> 1343.52s]  And for each of the entries in the read set,
[1343.52s -> 1347.88s]  we check that it's either if it's locked
[1347.88s -> 1350.20s]  or the data version is greater than the local timestamp
[1350.20s -> 1352.44s]  that we have to abort.
[1352.44s -> 1354.68s]  What do we know has happened if it's locked?
[1357.44s -> 1357.96s]  Yeah?
[1358.84s -> 1360.52s]  Right.
[1360.52s -> 1363.80s]  And you basically read the wrong thing.
[1363.80s -> 1366.68s]  And it's also true if the data version is greater
[1366.68s -> 1367.72s]  than the local timestamp.
[1367.72s -> 1371.16s]  So that means that we've got bad data,
[1371.16s -> 1375.48s]  and we need to abort this transaction and start again.
[1375.48s -> 1379.20s]  Suppose every element in the read set checks,
[1379.20s -> 1386.96s]  then we can update the local timestamp for this transaction
[1386.96s -> 1390.68s]  because it's as if it started executing
[1390.68s -> 1393.64s]  at this point in which you just got the local timestamp as
[1393.64s -> 1396.64s]  opposed to the original time that it got its local
[1396.64s -> 1400.00s]  timestamp, which was earlier, because you've completely
[1400.00s -> 1403.52s]  validated the read set at this point.
[1403.52s -> 1404.40s]  OK.
[1404.40s -> 1404.92s]  Yeah?
[1404.92s -> 1405.88s]  I have a question.
[1405.88s -> 1407.56s]  Could you write a local read checks,
[1407.56s -> 1410.20s]  like lock or a data version?
[1410.20s -> 1413.76s]  I don't think I need to see.
[1413.76s -> 1414.24s]  No.
[1414.24s -> 1416.16s]  What's the difference?
[1416.16s -> 1418.80s]  Can someone enlighten us about what's
[1418.80s -> 1422.16s]  the difference between the data being locked
[1422.16s -> 1426.04s]  and the timestamp being updated?
[1426.04s -> 1426.84s]  Yeah?
[1426.84s -> 1428.84s]  I'm pretty sure so that if it's locked,
[1428.84s -> 1431.52s]  then it turns back and you're currently writing that data.
[1431.52s -> 1432.00s]  Yeah.
[1432.00s -> 1435.04s]  Or if the data is readable, then it's already locked.
[1435.04s -> 1435.84s]  Exactly.
[1435.84s -> 1437.48s]  That's the difference.
[1437.48s -> 1438.40s]  Right?
[1438.40s -> 1441.00s]  One is the write's in progress.
[1441.00s -> 1443.92s]  One is the write has already happened.
[1443.92s -> 1445.76s]  Good.
[1445.76s -> 1449.28s]  OK, so STM commit, right?
[1449.28s -> 1453.16s]  So now you need to increment the timestamp.
[1453.16s -> 1454.88s]  You're going to do it by two.
[1454.88s -> 1456.00s]  Why do we do it by two?
[1460.40s -> 1464.72s]  Yeah, because the least significant bit is actually
[1464.72s -> 1466.88s]  keeping track of whether it's locked or not, right?
[1466.88s -> 1469.68s]  So we always have to increment it
[1469.68s -> 1478.52s]  by two to make sure that we can use that least
[1478.52s -> 1480.32s]  significant bit.
[1480.32s -> 1482.20s]  OK?
[1482.20s -> 1484.48s]  Then we're going to take the pre-incremented, the old
[1484.48s -> 1492.12s]  timestamp, and validate the read set,
[1492.12s -> 1494.40s]  just to make sure that there aren't any recently committed
[1494.40s -> 1497.68s]  transactions that would cause us to abort.
[1497.68s -> 1501.12s]  And then for each item in the write set,
[1501.12s -> 1504.08s]  we're going to release the lock and set the version number
[1504.08s -> 1505.88s]  to the global timestamp.
[1505.88s -> 1508.60s]  OK?
[1508.60s -> 1510.48s]  So that's it.
[1510.48s -> 1513.36s]  So now let's look at an example,
[1513.36s -> 1517.64s]  which is a slight modification of what I just told you,
[1517.64s -> 1523.36s]  but it's close enough to make sense.
[1523.36s -> 1525.88s]  So here what we're trying to do
[1525.88s -> 1529.64s]  is we have two transactions, x1 and x2.
[1529.64s -> 1543.80s]  And x1 is going to atomically copy object foo to object bar,
[1543.80s -> 1548.48s]  and then x2 is going to read object bar,
[1548.48s -> 1551.68s]  and it should read also atomically, right?
[1551.72s -> 1555.92s]  And so it should read either 00 or 97, right?
[1555.92s -> 1560.60s]  It shouldn't see any partial updates to the object.
[1560.60s -> 1562.28s]  That clear?
[1562.28s -> 1566.24s]  All right, so let's see how this works.
[1566.24s -> 1579.08s]  So we start by executing the first read of foo,
[1579.12s -> 1582.40s]  the x element of foo, right?
[1582.40s -> 1585.64s]  And so this needs to go into the read set.
[1585.64s -> 1589.48s]  So it goes into the read set as foo,
[1589.48s -> 1591.92s]  and the timestamp is three, right?
[1591.92s -> 1594.68s]  So the idea is that there are no local or global timestamps
[1594.68s -> 1595.16s]  here.
[1595.16s -> 1600.52s]  Let's just assume that each of the objects has a timestamp.
[1600.52s -> 1604.68s]  And the initial object timestamp for foo is three,
[1604.68s -> 1607.16s]  and for bar is five, right?
[1607.16s -> 1613.52s]  So we've got the read of foo in the read set
[1613.52s -> 1616.20s]  with timestamp three.
[1616.20s -> 1625.16s]  And then we have a read of bar, and it gets timestamp five.
[1625.16s -> 1630.76s]  And so that goes into the read set of x2, OK?
[1630.76s -> 1638.92s]  Then we get a write to bar, right?
[1638.92s -> 1641.00s]  And so what has to happen, right?
[1641.00s -> 1643.12s]  We have to take the write lock, right?
[1643.12s -> 1650.24s]  So we have a write lock, and that's
[1650.24s -> 1655.48s]  indicated by the transaction point to x1, OK?
[1655.52s -> 1663.76s]  Now we need to indicate that in the transaction descriptor
[1663.76s -> 1669.24s]  by updating the write set and the undo log,
[1669.24s -> 1673.48s]  since this is a eager mode, right?
[1673.48s -> 1674.76s]  So we have to have an undo log.
[1674.76s -> 1679.72s]  So we put the old value of bar.x into the undo log, right?
[1679.72s -> 1689.60s]  We continue executing, and we are going to read bar dot y.
[1689.60s -> 1692.64s]  So what should happen here?
[1692.64s -> 1693.16s]  Yeah?
[1693.16s -> 1696.48s]  Well, I think because the granularity of the old bar,
[1696.48s -> 1699.16s]  then it should say that's invalid.
[1699.16s -> 1700.52s]  So what should happen?
[1700.52s -> 1704.28s]  So it stalls, or yeah, it stalls.
[1704.28s -> 1708.92s]  Yeah, so maybe it's going to see that bar is locked,
[1708.92s -> 1710.00s]  right?
[1710.00s -> 1717.20s]  So it tries to do the validate for the read of bars
[1717.20s -> 1722.68s]  and say the object is locked, and so let's assume
[1722.68s -> 1723.80s]  that we just wait.
[1723.80s -> 1727.28s]  We stall, OK?
[1727.28s -> 1730.60s]  So x2 waits, OK?
[1730.60s -> 1734.28s]  So x1 continues.
[1734.36s -> 1742.08s]  We have another read of foo, foo dot y,
[1742.08s -> 1748.24s]  and that gets put into the read set.
[1748.24s -> 1754.20s]  And then we have another write of bar with that value,
[1754.20s -> 1760.48s]  and that all gets put into the write set, OK?
[1760.60s -> 1766.24s]  All right, now we need to commit, right?
[1766.24s -> 1769.52s]  And so what do we need to do on commit?
[1773.80s -> 1776.84s]  Yeah?
[1776.84s -> 1778.24s]  Is that the first thing we do?
[1778.24s -> 1780.96s]  No, I think that you do that eventually.
[1780.96s -> 1785.92s]  First you want to actually do the commit and update the log.
[1785.92s -> 1787.52s]  So what do we do before that?
[1787.52s -> 1788.72s]  Validate the read set.
[1788.72s -> 1790.00s]  Validate the read set.
[1790.04s -> 1790.88s]  Exactly.
[1790.88s -> 1796.00s]  So first we want to make sure that the reads that we got
[1796.00s -> 1797.52s]  are, in fact, valid.
[1797.52s -> 1802.56s]  And so we check, and the timestamp for foo
[1802.56s -> 1806.28s]  is still the same for both of the reads of foo.
[1806.28s -> 1812.08s]  And so we say read set validated, we're cool,
[1812.08s -> 1814.12s]  and we can go ahead and commit.
[1814.12s -> 1822.84s]  And the commit is going to increment the timestamp on bar
[1822.84s -> 1826.68s]  by 2, so it goes from 5 to 7, OK?
[1826.68s -> 1827.68s]  Yeah?
[1827.68s -> 1832.16s]  So when we had to read the two separate, like,
[1832.16s -> 1835.16s]  the two separate things in the read set,
[1835.16s -> 1837.16s]  but there's only one in the write set,
[1837.16s -> 1839.64s]  so let's look at what that is.
[1839.64s -> 1842.64s]  When we did bar.x, it was 3.
[1842.64s -> 1848.16s]  That means we had whatever was bar.x and the part
[1848.16s -> 1849.64s]  behind the read set.
[1849.64s -> 1850.12s]  Right.
[1850.12s -> 1853.12s]  And then we wrote bar.y in the read set.
[1853.12s -> 1854.60s]  It was just, we just added to it,
[1854.60s -> 1856.76s]  and we just didn't add that on the write set.
[1856.76s -> 1859.64s]  Yeah, that probably is incorrect.
[1859.64s -> 1862.48s]  Yeah, yeah, we probably should add an entry to the write set
[1862.48s -> 1865.24s]  for both writes to bar.
[1865.24s -> 1865.74s]  OK.
[1865.74s -> 1868.32s]  Yeah.
[1868.32s -> 1878.72s]  Although, yeah, you know, since it's doing it by,
[1878.72s -> 1881.72s]  yeah, you probably shouldn't.
[1881.72s -> 1884.08s]  Why about maybe it was because it's.
[1884.08s -> 1885.60s]  Oh, shit.
[1885.64s -> 1889.44s]  I mean, it is at the object level granularity.
[1889.44s -> 1899.52s]  And so once you know that, yeah, in this case,
[1899.52s -> 1903.44s]  the write set isn't so important, right?
[1903.44s -> 1910.20s]  Because you're doing pessimistic detection anyway,
[1910.20s -> 1913.60s]  and you have an undo log, right?
[1913.60s -> 1920.60s]  So figuring out, in this case, exactly what's in the write
[1920.60s -> 1924.48s]  set isn't that important.
[1924.48s -> 1934.52s]  You need, of course, to potentially on a conflict
[1934.52s -> 1936.44s]  use the undo log.
[1936.44s -> 1940.50s]  But you know, to be completely correct, you're right.
[1940.50s -> 1942.08s]  You should probably have another entry
[1942.08s -> 1944.24s]  in here for the other write to bar.
[1948.76s -> 1953.40s]  All right, so we commit.
[1953.40s -> 1964.16s]  And now we can, let's get rid of, OK,
[1964.16s -> 1966.72s]  so we're at the point the x2 waits.
[1966.72s -> 1968.96s]  We get to the commit.
[1968.96s -> 1971.72s]  All right, now what happens is we
[1971.72s -> 1980.60s]  can now release the stall on x2, right?
[1980.60s -> 1988.76s]  And then we get another read from the x2 transaction.
[1988.76s -> 1993.44s]  And this, of course, now has the new timestamp, OK?
[1993.44s -> 1996.24s]  So then x2 gets to the commit point.
[1996.24s -> 1999.56s]  And what do we do first?
[1999.56s -> 2001.12s]  Validate the read set, OK?
[2001.12s -> 2003.36s]  So we validate the first read.
[2003.36s -> 2006.36s]  And does it validate?
[2006.36s -> 2007.90s]  No, right?
[2007.90s -> 2010.04s]  So you have to abort.
[2010.04s -> 2015.96s]  And once you've aborted, you can re-execute and then
[2015.96s -> 2021.64s]  get the right values for the bar object.
[2021.64s -> 2024.40s]  OK, any questions on how this works?
[2024.40s -> 2025.80s]  Yeah?
[2025.80s -> 2027.92s]  On the SCMR version side, I don't
[2027.92s -> 2029.92s]  think this really makes that much of a difference.
[2030.12s -> 2033.40s]  Do we atomically update the global timestamp
[2033.40s -> 2037.16s]  and then use the old one to make the read set validation?
[2037.16s -> 2037.72s]  Yes.
[2037.72s -> 2041.72s]  Or do we do it the other way around?
[2041.72s -> 2045.92s]  Well, let's go back to what was said.
[2045.92s -> 2049.60s]  Yeah, so atomically increment the global timestamp.
[2049.60s -> 2052.96s]  And then you use the old global timestamp
[2052.96s -> 2055.04s]  to validate the read set.
[2055.04s -> 2055.54s]  Yeah.
[2055.54s -> 2056.96s]  So the global timestamp is updated?
[2056.96s -> 2057.96s]  Right, yeah.
[2060.92s -> 2062.76s]  OK.
[2062.76s -> 2064.12s]  Any other questions on that?
[2067.16s -> 2068.88s]  All right, then to summarize, right?
[2068.88s -> 2071.88s]  So we've talked about the various options
[2071.88s -> 2073.76s]  for TM implementation.
[2073.76s -> 2076.16s]  We need to be optimistic.
[2076.16s -> 2078.04s]  So we can be optimistic or pessimistic
[2078.04s -> 2079.04s]  in terms of contention.
[2079.04s -> 2082.80s]  We can be lazy or eager in terms of data versioning.
[2082.80s -> 2086.86s]  And that with the software TM systems,
[2086.86s -> 2093.34s]  the compiler or the virtual memory system
[2093.34s -> 2098.40s]  adds these barriers, these software transaction memory
[2098.40s -> 2100.54s]  barriers or instrumentation code
[2100.54s -> 2103.70s]  to call into the transactional memory system
[2103.70s -> 2105.94s]  to do the bookkeeping.
[2105.94s -> 2110.34s]  And the basic data structures are the transaction descriptor
[2110.34s -> 2114.38s]  per thread or per transaction and transactional records
[2114.38s -> 2119.06s]  per data that keeps track of the state of the data,
[2119.06s -> 2122.18s]  whether it's locked or version number.
[2122.18s -> 2124.18s]  So it turns out, of course, that these barriers
[2124.18s -> 2125.58s]  add overhead.
[2125.58s -> 2129.42s]  And a lot of the work that gets done inside the barriers
[2129.42s -> 2130.82s]  is, in fact, redundant.
[2130.82s -> 2133.82s]  And so if you can decompose the barriers
[2133.82s -> 2136.86s]  into the individual components, right?
[2136.86s -> 2142.82s]  In fact, you've got a log, logging of the object,
[2142.82s -> 2144.66s]  opening it for write.
[2144.66s -> 2150.18s]  And you'll see that a lot of the operations that
[2150.18s -> 2153.66s]  are composed in the barriers are redundant
[2153.66s -> 2155.78s]  and can be optimized away.
[2155.78s -> 2158.10s]  And so you get lower overhead.
[2161.26s -> 2165.58s]  And then the question is, once you've
[2165.58s -> 2168.30s]  done with this kind of optimization,
[2168.30s -> 2171.90s]  what kind of performance do you get, right?
[2171.94s -> 2175.06s]  So this is some data from one of the papers that
[2175.06s -> 2177.58s]  was published on Makati.
[2177.58s -> 2181.30s]  And it's looking at two data structures, HashMap and TreeMap.
[2181.30s -> 2183.58s]  And for HashMap, I think it was sort of 80%
[2183.58s -> 2193.82s]  gets and 20% puts, right?
[2193.82s -> 2197.54s]  And so you look at the different bars,
[2197.54s -> 2199.90s]  this is the overhead on a single processor, right?
[2200.14s -> 2203.18s]  Trying to look at how much overhead you get.
[2203.18s -> 2209.06s]  And so 0% would be the non-thread safe code.
[2209.06s -> 2212.42s]  The blue bar represents the overhead
[2212.42s -> 2216.66s]  by using a single coarse-grain lock synchronized
[2216.66s -> 2218.74s]  over the whole method.
[2218.74s -> 2226.06s]  And the kind of reddish bar, purple, purple reddish bar,
[2226.06s -> 2229.74s]  is what happens if you use a software transactional memory
[2229.78s -> 2232.98s]  that does not have any compiler optimization, right?
[2232.98s -> 2237.78s]  So then you get 70% to 80% overhead on a single processor.
[2237.78s -> 2242.58s]  However, if you combine an STM with compiler optimizations,
[2242.58s -> 2244.74s]  the overhead comes down dramatically,
[2244.74s -> 2249.22s]  as shown by this pinkish bar, or pinkish orange bar.
[2249.22s -> 2253.34s]  And then the overhead is roughly 40%
[2253.34s -> 2256.12s]  over no concurrency control and 30%
[2256.12s -> 2260.80s]  over a single, kind of coarse-grain lock-based
[2260.80s -> 2261.60s]  synchronization.
[2261.60s -> 2262.12s]  Yeah?
[2262.12s -> 2262.60s]  All right.
[2262.60s -> 2263.60s]  Could you correct me?
[2263.60s -> 2266.56s]  Does it have to do with the same amount of work
[2266.56s -> 2268.84s]  that people can do, right?
[2268.84s -> 2272.08s]  It's how much extra time it takes
[2272.08s -> 2276.64s]  to run your code with these overheads in, right?
[2276.64s -> 2280.52s]  And so if you have no extra code,
[2280.52s -> 2284.08s]  then that's the time that you just
[2284.08s -> 2287.28s]  got by running your code without any support
[2287.28s -> 2290.20s]  for transactional memory or any synchronization at all, right?
[2290.20s -> 2292.28s]  So it's not thread-safe.
[2292.28s -> 2294.36s]  So your code's not thread-safe.
[2294.36s -> 2296.84s]  You can make it thread-safe with a coarse-grain lock.
[2296.84s -> 2298.80s]  As we've seen, that's fairly easy.
[2298.80s -> 2301.32s]  In Java, just put synchronize around the whole method,
[2301.32s -> 2302.80s]  right?
[2302.80s -> 2303.52s]  What happened?
[2303.52s -> 2306.76s]  It seems that would be a lot faster than you do.
[2306.76s -> 2308.84s]  It's faster than the code.
[2308.84s -> 2310.44s]  Faster in what sense?
[2310.44s -> 2312.36s]  As in going faster.
[2312.36s -> 2312.96s]  Yeah.
[2312.96s -> 2314.20s]  Yeah, it is faster.
[2314.20s -> 2317.00s]  But remember, this is on a single processor.
[2317.00s -> 2318.04s]  It won't scale.
[2318.04s -> 2319.36s]  We saw that earlier, right?
[2319.36s -> 2320.58s]  It didn't scale, right?
[2320.58s -> 2323.00s]  So now if you want something that's scalable,
[2323.00s -> 2325.04s]  then maybe you have to go to fine-grain locking,
[2325.04s -> 2328.00s]  which is actually not shown here, right?
[2328.00s -> 2331.56s]  Or you go to transactions, and on a single processor,
[2331.56s -> 2336.04s]  your overhead is 30% to 40%, but you really
[2336.04s -> 2341.92s]  should compare it to the overhead of synchronization
[2341.92s -> 2345.72s]  since, after all, if you don't put anything in your code,
[2345.72s -> 2346.88s]  then it's not thread-safe.
[2346.88s -> 2349.96s]  And so you couldn't run it correctly in parallel mode,
[2349.96s -> 2350.68s]  right?
[2350.68s -> 2355.00s]  And now with transactional memory and compiler
[2355.00s -> 2357.80s]  optimization, you get something that's
[2357.80s -> 2360.28s]  both scalable with reasonable overheads.
[2363.72s -> 2366.04s]  OK.
[2366.04s -> 2370.64s]  So just to refresh your memory, let's
[2370.64s -> 2373.40s]  suppose that we have an atomic region,
[2373.40s -> 2383.00s]  and we are assigning 42 to object field of F1,
[2383.00s -> 2385.64s]  F1 field of the object.
[2385.64s -> 2388.92s]  What steps are required to implement the atomic region,
[2388.92s -> 2391.84s]  assuming that there's some way of getting a transaction
[2391.84s -> 2394.80s]  descriptor, and we're going to do things
[2394.80s -> 2396.60s]  at the granularity of an object?
[2396.60s -> 2401.00s]  What are the operations we need to perform?
[2403.68s -> 2406.24s]  Once we have the transaction descriptor, what should we do?
[2408.92s -> 2412.68s]  Oh, yeah, it's optimistic read, pessimistic write,
[2412.68s -> 2417.00s]  eager versioning, just like the one example
[2417.00s -> 2418.60s]  that we just described.
[2418.60s -> 2423.32s]  We want to implement this atomic region.
[2423.32s -> 2425.08s]  What steps do we need to take?
[2427.60s -> 2429.08s]  Yeah?
[2429.08s -> 2431.04s]  Check if it's on lock.
[2431.04s -> 2433.56s]  Right.
[2433.56s -> 2436.52s]  I'd like to validate that.
[2436.52s -> 2440.44s]  And then you get the lock, and your write
[2440.44s -> 2445.36s]  said add it to the undo log, and then write the data.
[2445.36s -> 2446.52s]  OK, good, right.
[2446.52s -> 2449.60s]  So open the write is kind of your check,
[2449.60s -> 2451.72s]  and you're going to log it, and then you actually
[2451.72s -> 2454.28s]  can do the write, since it's eager.
[2454.28s -> 2456.32s]  Good.
[2456.32s -> 2460.60s]  All right, so here's some more data
[2460.60s -> 2461.96s]  about the performance of STM.
[2461.96s -> 2464.88s]  But here, this STM is unoptimized.
[2464.88s -> 2469.00s]  So it's not as optimized as the McCarty example
[2469.00s -> 2471.92s]  with the compiler optimizations I just showed you.
[2471.92s -> 2474.40s]  But the problem is that you've got this overhead
[2474.40s -> 2480.84s]  in terms of performance, or in this case,
[2480.84s -> 2484.28s]  it's two to eight times per thread
[2484.28s -> 2488.56s]  due to the software transactional memory barriers.
[2488.56s -> 2493.44s]  And if we decompose the time, we see in these two examples,
[2493.44s -> 2496.72s]  vacation is this three-tier server example.
[2496.72s -> 2499.12s]  k-means is just k-means clustering.
[2499.12s -> 2501.00s]  And you see that a lot of the time,
[2501.00s -> 2507.52s]  the white and the black portions of the stack bar chart
[2507.52s -> 2513.36s]  are due to the STM write, which is black,
[2513.36s -> 2515.44s]  and the STM read, which is right.
[2515.44s -> 2521.12s]  So the STM barriers add a lot of overhead,
[2521.12s -> 2526.52s]  and sort of committing also adds a bunch of overhead.
[2526.52s -> 2529.28s]  And so the question is, how can we
[2529.28s -> 2533.40s]  remove those other heads by judiciously using hardware?
[2533.40s -> 2537.00s]  Well, we could try and attack the need
[2537.00s -> 2539.56s]  to have barriers at all, right?
[2539.56s -> 2543.08s]  And so if you have a hardware mechanism
[2543.08s -> 2545.24s]  for doing transactional memory, then you
[2545.24s -> 2547.20s]  don't need explicit barriers, right?
[2547.20s -> 2550.24s]  You don't need to tell the transactional memory system,
[2550.24s -> 2553.48s]  hey, I'm doing a read, or hey, I'm doing a write.
[2553.48s -> 2556.28s]  Why don't you go take the necessary actions
[2556.28s -> 2558.68s]  to implement the transactional memory?
[2558.68s -> 2562.20s]  All I need to do is just do loads and stores
[2562.20s -> 2565.00s]  and let the hardware transactional memory
[2565.00s -> 2569.00s]  system do the correct thing.
[2569.00s -> 2572.64s]  And so as you might imagine, the way
[2572.64s -> 2576.00s]  that we are going to implement this in hardware
[2576.00s -> 2579.00s]  is we're going to ride on top or exploit the fact
[2579.00s -> 2583.08s]  that we already have a coherent shared memory system,
[2583.08s -> 2586.84s]  right, and we're going to already have caches.
[2586.84s -> 2591.08s]  And so we can do the data versioning in the caches,
[2591.08s -> 2593.12s]  and we can do the conflict detection
[2593.12s -> 2595.72s]  by riding on top of coherency, OK?
[2595.72s -> 2597.56s]  So we already have the mechanisms
[2597.56s -> 2600.24s]  for implementing transactional memory
[2600.24s -> 2604.44s]  inside a modern processor anyway.
[2604.44s -> 2610.28s]  So we use the caches to do the write buffer or the undo log,
[2610.28s -> 2615.76s]  and the conflict detection can be done by enhancing
[2615.76s -> 2618.04s]  the coherency protocol, OK?
[2618.04s -> 2621.44s]  So the one component that you also
[2621.44s -> 2625.28s]  need in order to take a checkpoint so that you can come
[2625.32s -> 2628.72s]  back and abort is you need to checkpoint the register state,
[2628.72s -> 2629.20s]  right?
[2629.20s -> 2632.92s]  So you've got a context associated
[2632.92s -> 2635.72s]  with the register contents of the processor,
[2635.72s -> 2639.16s]  and you need to hold those values around
[2639.16s -> 2642.88s]  so that when you abort you can come back and get
[2642.88s -> 2644.12s]  to the same state.
[2644.12s -> 2646.36s]  So you need to checkpoint those away somehow,
[2646.36s -> 2650.64s]  and a lot of processes allow you to do that.
[2650.64s -> 2654.36s]  All right, so let's look at a hardware transactional memory
[2654.36s -> 2656.00s]  design.
[2656.00s -> 2657.96s]  So what we're going to do is we are
[2657.96s -> 2662.44s]  going to enhance our metadata on the cache line, right?
[2662.44s -> 2670.48s]  So for each cache line, we've got our messy state bits,
[2670.48s -> 2671.64s]  right?
[2671.64s -> 2673.72s]  Our coherence bits that we've already discussed,
[2673.72s -> 2677.84s]  and we're going to add two other bits, an R bit and a W
[2677.84s -> 2680.04s]  bit, right?
[2680.04s -> 2681.16s]  What do these represent?
[2681.16s -> 2687.20s]  The read and write state, right?
[2687.20s -> 2691.24s]  So if the R bit says, hey, this cache line
[2691.24s -> 2695.12s]  is part of my read state, if the W bit says,
[2695.12s -> 2699.12s]  this cache line is part of my write state, OK?
[2699.12s -> 2701.64s]  So then now you can think about how you
[2701.64s -> 2703.52s]  do the conflict detection, right?
[2703.52s -> 2709.72s]  So if you see a shared request to a line that
[2709.72s -> 2712.88s]  is in the has the W bit set, you
[2712.88s -> 2717.16s]  know that is, in fact, a conflict, a read-write
[2717.16s -> 2718.00s]  conflict, right?
[2718.00s -> 2720.52s]  So you've already written this line,
[2720.52s -> 2725.40s]  and some other processor wants to read it,
[2725.40s -> 2730.72s]  and so there's a potential conflict there.
[2730.72s -> 2736.52s]  If you see a exclusive request to a bit that's
[2737.24s -> 2740.16s]  to a cache line that has the R bit set,
[2740.16s -> 2744.72s]  then you know that we have a write-read conflict, right?
[2744.72s -> 2747.20s]  You've already read the particular cache line,
[2747.20s -> 2750.80s]  and some other processor wants to read it, OK?
[2750.80s -> 2754.88s]  So that, again, of course, is a conflict.
[2754.88s -> 2757.96s]  Another example would be, hey, the W bit set,
[2757.96s -> 2760.12s]  and you see an exclusive request that says, hey,
[2760.12s -> 2761.52s]  there's a write-write conflict.
[2761.52s -> 2763.68s]  I've already written this cache line,
[2763.68s -> 2766.68s]  and this is other processor that also wants to write it,
[2766.68s -> 2770.88s]  and we're currently both inside a transaction,
[2770.88s -> 2773.80s]  and so something has to be done to make sure that we
[2773.80s -> 2776.00s]  deal with that conflict.
[2776.00s -> 2782.16s]  All right, so with that in mind, let's look at a example.
[2782.16s -> 2785.36s]  So we have in our CPU the registers,
[2785.36s -> 2790.02s]  and the red state is indicating the extra state
[2790.02s -> 2793.74s]  that we need to implement transactional memory.
[2793.74s -> 2795.74s]  So we've got a registered checkpoint
[2795.74s -> 2797.34s]  that we need to take, and then we
[2797.34s -> 2801.70s]  have some extra state associated with abort handlers.
[2801.70s -> 2805.42s]  Where exactly do you go in the program
[2805.42s -> 2809.06s]  to deal with aborts if there's any software cleanup
[2809.06s -> 2811.46s]  that needs to be done?
[2811.46s -> 2821.34s]  And then the key elements that we've added are read R and W
[2821.34s -> 2827.54s]  bits, which indicate transactional memory state.
[2827.54s -> 2830.74s]  So R says it's part of the read set,
[2830.74s -> 2833.90s]  and W says it's part of the write set.
[2833.90s -> 2840.50s]  So given an example now, it's a very simple transaction.
[2840.54s -> 2847.62s]  Loads A and B and stores 5 to C, and then we commit.
[2847.62s -> 2853.54s]  All right, so first thing that happens is we load A,
[2853.54s -> 2856.90s]  the cache line becomes valid, let's assume,
[2856.90s -> 2862.54s]  for the sake of simplicity, the cache line size
[2862.54s -> 2865.76s]  is a single word.
[2865.76s -> 2868.78s]  And we also have to indicate that it's
[2868.78s -> 2869.98s]  part of the reset, right?
[2869.98s -> 2873.28s]  So we need to set the R bit.
[2873.28s -> 2882.22s]  Similarly for B, we set the R bit for the B address.
[2885.58s -> 2895.74s]  And then for C, we probably want to put a 5 in here,
[2895.74s -> 2897.66s]  just to be correct, right?
[2897.74s -> 2903.46s]  So this needs to be part of the write set, OK?
[2903.46s -> 2909.58s]  So now that we get to the commit point,
[2909.58s -> 2916.06s]  we need to let's assume that we did not see,
[2916.06s -> 2920.30s]  well, we'll get to the commit point
[2920.30s -> 2924.54s]  and assuming that there have been no other conflicts,
[2924.54s -> 2925.46s]  what should we do?
[2927.66s -> 2938.86s]  So in particular, is this value
[2938.86s -> 2943.74s]  that we wrote to C, how do we indicate
[2943.74s -> 2946.58s]  to the rest of the system that, in fact, this
[2946.58s -> 2953.90s]  is data that is actually committed state?
[2953.90s -> 2956.50s]  So at the moment, it's inside our cache.
[2956.54s -> 2959.42s]  We've indicated it's part of our write set,
[2959.42s -> 2965.38s]  but the rest of the other transactions and threads
[2965.38s -> 2967.26s]  don't actually know about it, right?
[2967.26s -> 2970.90s]  Because remember, the property of transactions that we want
[2970.90s -> 2972.22s]  is isolation, right?
[2972.22s -> 2975.34s]  That each of these transactions is isolated from each other.
[2975.34s -> 2977.66s]  And the way that we get that is that each of these
[2977.66s -> 2981.18s]  transactions are operating on their individual caches, OK?
[2981.18s -> 2983.46s]  So now we get to the commit point,
[2983.46s -> 2986.62s]  and we need to publicize the fact that we
[2986.62s -> 2993.10s]  have written to the address C, OK?
[2993.10s -> 2994.90s]  So how do we do that?
[2994.90s -> 2996.90s]  Yeah?
[2996.90s -> 2999.10s]  You need to upgrade to the exclusive state.
[2999.10s -> 3001.46s]  That's exactly what has to happen.
[3001.46s -> 3005.22s]  So we need to get an upgrade, and so now we
[3005.22s -> 3010.30s]  indicate that C is now in the dirty state or exclusive state.
[3010.34s -> 3014.14s]  And we send that out to all the other processors,
[3014.14s -> 3022.86s]  and let's suppose that some other processor was doing
[3022.86s -> 3026.42s]  commits of a transaction with writes to A and D
[3026.42s -> 3029.82s]  before this transaction committed.
[3029.82s -> 3031.46s]  What would happen, right?
[3031.46s -> 3034.98s]  Well, if we saw an upgrade to A,
[3034.98s -> 3037.62s]  then we would say what, conflict, right?
[3041.30s -> 3042.26s]  Right?
[3042.26s -> 3050.14s]  Because A is in our cache, and it's part of our read set,
[3050.14s -> 3050.74s]  right?
[3050.74s -> 3056.30s]  And so note, maybe you read A before the transaction
[3056.30s -> 3057.70s]  started.
[3057.70s -> 3059.38s]  Then it could be in the cache but not
[3059.38s -> 3063.86s]  have the read orbit set, right?
[3063.86s -> 3067.66s]  So just the fact that A is in the cache
[3067.66s -> 3070.94s]  doesn't mean that there's going to be a conflict.
[3070.94s -> 3074.10s]  But if there is, in fact, the orbit set, then it says,
[3074.10s -> 3076.54s]  hey, not only is it in the cache,
[3076.54s -> 3079.38s]  but I read it during the transaction, OK?
[3079.38s -> 3083.22s]  And so now this would be a conflict.
[3083.22s -> 3087.82s]  And there's other cases where I see an upgrade for D.
[3087.82s -> 3091.10s]  What happens as far as this transaction is concerned?
[3091.10s -> 3091.86s]  Nothing, right?
[3091.86s -> 3093.58s]  D is not in the cache.
[3093.58s -> 3098.66s]  There's no match with the entries in our cache.
[3098.66s -> 3101.74s]  And so there can be no conflict, OK?
[3101.74s -> 3107.34s]  So let's assume then that we, oh,
[3107.34s -> 3108.98s]  going back to the commit, right?
[3108.98s -> 3117.86s]  So we validate by requesting the write set lines.
[3117.86s -> 3119.94s]  We commit.
[3119.98s -> 3127.78s]  So as kind of described so far, how
[3127.78s -> 3143.86s]  would you describe the policies in this hardware system?
[3143.86s -> 3144.66s]  Think about it.
[3144.66s -> 3149.26s]  Well, in terms of the data versioning policy
[3149.26s -> 3165.14s]  and the conflict-detective policy, it's eager, right?
[3165.14s -> 3168.14s]  Because as when the, well, is it eager?
[3171.34s -> 3173.98s]  I mean, every time you're trying to write the read,
[3173.98s -> 3178.30s]  but when do the upgrades actually take place?
[3178.30s -> 3180.26s]  At the end.
[3180.26s -> 3182.74s]  Sounds optimistic to me, doesn't it?
[3188.02s -> 3201.38s]  And then in terms of the data versioning,
[3201.38s -> 3203.42s]  it's lazy optimistic.
[3203.42s -> 3204.42s]  Yeah.
[3204.42s -> 3205.62s]  That's a way to think back.
[3206.18s -> 3209.78s]  Using the cache is like the box system.
[3209.78s -> 3212.02s]  It's the write buffer.
[3212.02s -> 3213.66s]  It's the write buffer.
[3213.66s -> 3215.54s]  It's buffering the writes.
[3215.54s -> 3218.58s]  And then at the end, when you get to the commit point,
[3218.58s -> 3221.70s]  you say, OK, here are all the things I wrote.
[3221.70s -> 3223.94s]  Now I'm going to publish them to the world.
[3223.94s -> 3227.46s]  And anybody else who happened to have,
[3227.46s -> 3230.70s]  so it's exactly the case that we described, right?
[3230.70s -> 3232.90s]  Which is you get to the commit point.
[3232.90s -> 3235.06s]  You have a bunch of things in your write set.
[3235.14s -> 3237.90s]  You're going to compare them to everybody's read set.
[3237.90s -> 3239.34s]  And how are you going to do that?
[3239.34s -> 3243.38s]  By sending out these upgrades.
[3243.38s -> 3244.38s]  Yeah.
[3244.38s -> 3247.38s]  I'm trying to relate this with the existing cache code.
[3247.38s -> 3251.38s]  Like, now when we try to store C, in general,
[3251.38s -> 3255.38s]  what we would do is we would code an exclusive data.
[3255.38s -> 3257.38s]  It was not present in any other cache.
[3257.38s -> 3258.38s]  Right.
[3258.38s -> 3260.38s]  Here we use the exclusive data as a way
[3260.38s -> 3263.38s]  to indicate that, hey, I'm upgrading my limiting.
[3263.38s -> 3264.38s]  Right.
[3264.70s -> 3267.70s]  In this particular pratma, do we think
[3267.70s -> 3269.70s]  about whenever we need something,
[3269.70s -> 3271.70s]  go to the modified state first.
[3271.70s -> 3273.70s]  And then I'm just trying to think
[3273.70s -> 3275.70s]  of it from a protocol positive perspective.
[3275.70s -> 3283.70s]  So, you know, as far as the state of the cache line,
[3283.70s -> 3288.70s]  remember, it's all, it's isolated, right?
[3289.02s -> 3294.02s]  So you don't really participate in coherence
[3294.02s -> 3297.02s]  until you get to the commit point.
[3297.02s -> 3299.02s]  Right.
[3299.02s -> 3302.02s]  Yeah.
[3302.02s -> 3304.02s]  Yeah.
[3304.02s -> 3308.02s]  For the cache lines that are in the read set,
[3308.02s -> 3311.02s]  I know for the write set, we have to issue a read exclusive.
[3311.02s -> 3314.02s]  But what do we have to do for the read set?
[3314.34s -> 3317.34s]  We just issue like a, just like a read.
[3317.34s -> 3319.34s]  Not exclusive, or?
[3319.34s -> 3321.34s]  Yeah, you just do a read, right?
[3321.34s -> 3325.34s]  Let's assume that, you know, you go get it from memory.
[3325.34s -> 3327.34s]  It's shared.
[3327.34s -> 3328.34s]  Right.
[3328.34s -> 3331.34s]  Or you do a read, you get it from memory,
[3331.34s -> 3333.34s]  or you get it from whoever commit,
[3333.34s -> 3337.34s]  whoever had it in its modified state last, right?
[3339.34s -> 3340.34s]  Okay.
[3340.66s -> 3346.66s]  So we've determined that this is in fact lazy optimistic, right?
[3346.66s -> 3353.66s]  All right, so we can do the review.
[3353.66s -> 3357.66s]  Let's do one more example just to kind of, you know,
[3357.66s -> 3366.66s]  really, this is in fact an explicit lazy optimistic scheme
[3366.98s -> 3370.98s]  in which you're not, the way that you do coherence
[3370.98s -> 3373.98s]  is with the transactional memory system, okay?
[3373.98s -> 3378.98s]  So the idea here is that when you get to a commit point,
[3378.98s -> 3381.98s]  you take all the state that you have
[3381.98s -> 3384.98s]  and you update everybody else, okay?
[3384.98s -> 3389.98s]  Which is classic, you know, lazy optimistic,
[3389.98s -> 3392.98s]  but there are no other states, right?
[3393.30s -> 3398.30s]  Because all that ever happens is transactions execute,
[3398.30s -> 3400.30s]  they get to the commit point,
[3400.30s -> 3402.30s]  and then they send out all the state that they've modified
[3402.30s -> 3405.30s]  and update everybody else, okay?
[3405.30s -> 3407.30s]  So that's what's happening here.
[3407.30s -> 3410.30s]  We've got a bunch of processes, three,
[3410.30s -> 3417.30s]  and then each of them is executing transactions
[3417.62s -> 3423.62s]  that do reads and writes to different variables.
[3423.62s -> 3428.26s]  And, you know, the assumptions are there's one commit
[3428.26s -> 3431.14s]  per execution step across all processes.
[3431.14s -> 3434.62s]  And the reason you want that for simplicity is, hey,
[3434.62s -> 3437.30s]  there's only one, it's kind of like a bus based scheme
[3437.30s -> 3440.14s]  that only one transaction is committing at a time
[3440.14s -> 3442.86s]  and it's updating everybody else, okay?
[3442.86s -> 3446.78s]  And when one transaction causes another transaction
[3446.78s -> 3450.18s]  to abort and re-execute, you can assume that the commit
[3450.18s -> 3453.30s]  of one transaction can overlap with the begin
[3453.30s -> 3456.34s]  of another transaction, okay?
[3456.34s -> 3459.34s]  So with that in mind, then, the idea is,
[3459.34s -> 3462.82s]  given these processes, three processes
[3462.82s -> 3467.82s]  and these four transactions, why don't you lay out the,
[3470.42s -> 3475.42s]  fill out the table as shown here, right?
[3476.82s -> 3480.82s]  So what actions, what's the read set on P1,
[3480.82s -> 3484.34s]  the write set on P1, and the actions and read
[3484.34s -> 3486.98s]  and write sets on the other processes, right?
[3486.98s -> 3491.70s]  And so it's partially filled in here, right?
[3491.70s -> 3496.70s]  So we started executing transaction one on processor one.
[3499.50s -> 3504.50s]  And the point at which we do this write of C,
[3506.86s -> 3511.86s]  A is in the read set and A is in the write set
[3513.06s -> 3516.32s]  and C is in the write set for processor one.
[3519.78s -> 3523.54s]  A is in the write set, read set of processor two
[3524.42s -> 3525.70s]  is in the, and so on.
[3525.70s -> 3529.38s]  Okay, so now we're at the point,
[3530.86s -> 3533.94s]  which, let's see.
[3536.78s -> 3538.78s]  Which C commits, okay?
[3547.66s -> 3552.66s]  So what do we do in order to validate the?
[3556.54s -> 3561.54s]  Sorry, processor one, C, yeah, this commits.
[3562.54s -> 3566.86s]  C is the commit.
[3566.86s -> 3571.40s]  So yeah, transaction one commits on processor one.
[3571.40s -> 3572.54s]  What do we need to do?
[3576.16s -> 3577.00s]  Yeah?
[3578.70s -> 3581.54s]  Nothing in its read set is someone else's write set.
[3582.74s -> 3585.30s]  So what's the rule for lazy optimistic?
[3592.18s -> 3593.98s]  What's the rule for lazy optimistic?
[3596.66s -> 3598.06s]  We just talked about it.
[3601.46s -> 3606.46s]  We saw it last lecture a couple times today.
[3606.82s -> 3607.66s]  Someone tell me.
[3609.94s -> 3612.02s]  You meant to put the write set
[3612.02s -> 3613.34s]  to other people's read sets.
[3613.34s -> 3615.34s]  Exactly, okay, so what's the write set
[3615.34s -> 3616.94s]  of this committing transaction?
[3616.94s -> 3621.94s]  Okay, what is the write set of?
[3629.18s -> 3631.56s]  A read set of the other transactions?
[3635.22s -> 3639.92s]  Doesn't have any, so it looks like we're okay.
[3647.82s -> 3652.82s]  Ah, I should've gone back to this last case.
[3664.30s -> 3665.60s]  How about this case?
[3667.68s -> 3669.26s]  Transaction two's committing.
[3669.26s -> 3674.26s]  Transaction four has to restart.
[3677.50s -> 3679.86s]  Yes, transaction four has to restart
[3679.86s -> 3683.10s]  because the read set of transaction four
[3683.10s -> 3685.74s]  contains E, right, so.
[3690.78s -> 3695.78s]  So the rule says hey, when transaction two commits,
[3696.46s -> 3701.46s]  we can overlap that with the beginning of transaction four
[3702.06s -> 3705.66s]  which had to abort and restart, okay?
[3705.66s -> 3707.74s]  So anyway, you can go and look at the details,
[3707.74s -> 3711.66s]  but in general you should be able to solve problems
[3711.66s -> 3714.02s]  something like this, right, given a sequence
[3714.02s -> 3719.02s]  of transactions and you can figure out
[3719.02s -> 3721.46s]  and some rules to follow, you can figure out what happens.
[3721.46s -> 3722.30s]  Yeah?
[3722.30s -> 3727.30s]  Yeah, just assume that at the time
[3734.46s -> 3737.46s]  that the commit happens, right,
[3737.46s -> 3742.18s]  it's globally known what was in the write state
[3742.18s -> 3745.86s]  of the committing transaction or processor.
[3752.54s -> 3756.66s]  No, I mean, we could imagine systems
[3756.66s -> 3760.62s]  that are not buses in which that happens
[3760.62s -> 3763.02s]  and in which case the problem would be a lot more
[3763.02s -> 3765.66s]  difficult, but with a bus, the nice thing about it
[3765.66s -> 3770.02s]  is only one transaction can be on it at any one time
[3770.02s -> 3771.54s]  and that simplifies matters, right,
[3771.54s -> 3775.30s]  because we always like to serialize things
[3775.30s -> 3778.78s]  because serialization means it's easier to think about
[3778.82s -> 3781.14s]  and easier to reason about
[3781.14s -> 3784.30s]  and buses give you that serialization point.
[3784.30s -> 3785.38s]  Yeah?
[3785.38s -> 3787.10s]  From an instruction set perspective,
[3787.10s -> 3790.18s]  so we just put atomic and then brackets around,
[3790.18s -> 3793.34s]  let's say, for getting the dignity when I'm coming.
[3793.34s -> 3794.42s]  Yeah, yeah, yeah.
[3794.42s -> 3797.74s]  So is that an explicit instruction in hardware
[3797.74s -> 3799.02s]  from an ISA standpoint?
[3800.54s -> 3803.20s]  Yes, it would be, it would be.
[3805.14s -> 3805.98s]  Yeah?
[3806.06s -> 3809.90s]  At this stage now, transaction P1 is committing
[3811.26s -> 3814.10s]  and it has a C in the write set
[3814.10s -> 3818.26s]  and transaction of P3 also has a C in the write set,
[3819.42s -> 3822.02s]  so it should evolve that way.
[3822.02s -> 3823.58s]  Sure?
[3823.58s -> 3825.90s]  No, it's not clear to read set.
[3825.90s -> 3827.06s]  How do you read set matters?
[3827.06s -> 3828.14s]  It's like overwriting.
[3828.14s -> 3829.46s]  It's overwriting, yeah.
[3829.46s -> 3830.30s]  That doesn't matter.
[3830.30s -> 3831.14s]  Yeah.
[3832.14s -> 3835.90s]  The question is who gets there first, right?
[3837.78s -> 3841.66s]  But from the point of view of the consistency
[3841.66s -> 3846.30s]  of the data, it never read it.
[3847.26s -> 3848.42s]  It just overrode it.
[3850.26s -> 3852.86s]  So you'll still get the right result, yeah.
[3852.86s -> 3854.78s]  When you said like bus in place,
[3854.78s -> 3856.46s]  you just duplicate the bus in place,
[3856.46s -> 3858.98s]  is that okay before you get the hours of the bus?
[3859.70s -> 3863.30s]  Um, you can have multiple buses, right?
[3863.30s -> 3865.10s]  And then if you have multiple buses,
[3865.10s -> 3869.56s]  then you've lost the serialization benefit, right?
[3869.56s -> 3872.26s]  And now you've got to think about different schemes.
[3874.34s -> 3875.18s]  Right.
[3876.26s -> 3877.10s]  Yeah.
[3878.02s -> 3879.34s]  All of these things are necessary
[3879.34s -> 3880.74s]  if you want scalable performance
[3880.74s -> 3882.66s]  and buses don't scale, right?
[3882.66s -> 3884.78s]  We only think about buses because they're easier
[3884.78s -> 3887.22s]  to talk about in classes like this where,
[3887.26s -> 3888.98s]  you know, if you go back in time,
[3888.98s -> 3890.92s]  yes, the earliest multi-port processes
[3890.92s -> 3892.58s]  were in fact bus-based, right?
[3892.58s -> 3894.34s]  Because that was the simplest to implement
[3894.34s -> 3897.46s]  and building hardware was very expensive.
[3897.46s -> 3901.26s]  But today, nobody builds anything with a bus, right?
[3901.26s -> 3903.42s]  It just doesn't perform well.
[3903.42s -> 3904.54s]  Yeah.
[3904.54s -> 3905.86s]  At the end of this example,
[3905.86s -> 3908.02s]  like on this very slide,
[3908.02s -> 3910.82s]  if we just like go to the end of the table,
[3910.82s -> 3913.34s]  we see that when we're committing T4,
[3913.34s -> 3916.84s]  um, can we restart, I believe?
[3917.90s -> 3919.54s]  Uh, where?
[3919.54s -> 3921.74s]  If you just go to like one more next slide.
[3924.74s -> 3928.06s]  So in this case, I just want to clarify that.
[3928.06s -> 3932.22s]  The reason for this is because
[3932.22s -> 3934.66s]  there is a conflict between,
[3937.00s -> 3939.08s]  is it the conflict between E,
[3939.12s -> 3941.32s]  where, which is like it's in the,
[3941.32s -> 3945.98s]  it's in the read set for T4, but.
[3945.98s -> 3949.40s]  So the write set for T4 is?
[3949.40s -> 3950.86s]  There's no B and C.
[3953.00s -> 3955.52s]  But there's no conflict between the write set of T4
[3955.52s -> 3957.14s]  and the read set of T3.
[3958.24s -> 3962.72s]  Begin at T, um, so, well, hold on.
[3962.72s -> 3965.16s]  You said it was restarting,
[3965.16s -> 3967.58s]  but I didn't, I don't see the restart.
[3967.58s -> 3969.58s]  For CT3, is that how it?
[3969.58s -> 3971.42s]  No, that's committing.
[3971.42s -> 3972.24s]  So.
[3972.24s -> 3975.86s]  Is that a start between W86 and CT3?
[3975.86s -> 3976.76s]  Yeah.
[3976.76s -> 3977.60s]  That's stop.
[3985.98s -> 3986.82s]  Sorry, I'm not really sure.
[3986.82s -> 3988.14s]  So can you only comment one,
[3988.14s -> 3990.14s]  or did I miss that Y is coming?
[3990.14s -> 3992.42s]  Yeah, yeah, that was the rule,
[3992.42s -> 3994.10s]  that you can only commit one.
[3994.10s -> 3997.08s]  Yeah, oh, I see, yeah, yeah.
[3997.58s -> 3999.16s]  Yeah, yeah, yeah.
[4008.16s -> 4010.40s]  Yeah, yeah, it's possible.
[4011.50s -> 4013.50s]  We'll guarantee there's no bugs in there.
[4013.50s -> 4017.24s]  Yeah, probably should fix that.
[4019.90s -> 4021.26s]  Somebody's paying attention.
[4022.74s -> 4023.74s]  Any other questions?
[4025.38s -> 4026.22s]  All right.
[4028.06s -> 4030.16s]  All right, so, you know,
[4030.16s -> 4031.52s]  hardware transaction memory support
[4031.52s -> 4034.54s]  has been implemented in Intel architectures
[4034.54s -> 4035.50s]  for various reasons.
[4035.50s -> 4037.34s]  It didn't work out so well.
[4037.34s -> 4041.74s]  You know, the software to use it wasn't developed
[4043.18s -> 4044.70s]  as well as one would hope,
[4044.70s -> 4049.44s]  and it was not all that capable
[4049.44s -> 4052.72s]  in the sense that transactions could abort
[4052.72s -> 4055.54s]  for almost any reason whatsoever,
[4055.54s -> 4060.48s]  and a lot of the times you had more aborts than necessary,
[4060.48s -> 4064.38s]  but what finally kind of put the nail in the coffin
[4064.38s -> 4066.32s]  of the transaction memory support
[4066.32s -> 4071.06s]  was it opened up security holes that people exploited,
[4071.06s -> 4074.70s]  and so, you know, there's still,
[4074.70s -> 4077.46s]  in the Intel roadmap,
[4077.46s -> 4079.92s]  there is the plan to put in transactional memory,
[4079.92s -> 4083.30s]  but we haven't seen any implementations yet.
[4083.30s -> 4088.30s]  You know, so if you look at the Intel ISA manual,
[4089.66s -> 4094.18s]  then there are the definitions of a bunch of instructions
[4094.18s -> 4096.46s]  to support transactional memory,
[4096.46s -> 4100.02s]  but the latest implementations don't have them in.
[4101.34s -> 4104.62s]  So just to recap then, you know,
[4104.62s -> 4106.98s]  transactional memory implementation,
[4106.98s -> 4109.30s]  you can do software or hardware.
[4109.30s -> 4113.34s]  You have the ability to make these trade-offs
[4113.34s -> 4117.08s]  between the different points in the design space,
[4117.08s -> 4120.02s]  and that if you really want to reduce
[4120.02s -> 4121.64s]  the overheads of software,
[4121.64s -> 4126.02s]  then hardware is kind of a key requirement,
[4126.02s -> 4129.06s]  and once you kind of understand cache coherence,
[4129.06s -> 4130.62s]  it's, you know, a minor
[4133.30s -> 4136.82s]  delta to understanding how you would implement
[4136.82s -> 4138.46s]  a transactional memory system.
[4138.46s -> 4139.30s]  Yeah?
[4139.30s -> 4142.18s]  It's just surprising that towards the end of the lecture,
[4142.18s -> 4144.74s]  you just told that maybe Intel is not,
[4144.74s -> 4147.46s]  currently it's not very widespread
[4147.46s -> 4149.58s]  that they use a transactional memory,
[4149.58s -> 4152.46s]  and so this is with the x86 engine
[4152.46s -> 4154.10s]  because it spooks a lot of Intel,
[4154.10s -> 4156.86s]  but does this file even think about
[4156.86s -> 4158.50s]  supporting something like this?
[4158.50s -> 4161.66s]  Almost fine, it's, you know,
[4161.66s -> 4164.06s]  it's flexible, and the, you know.
[4164.06s -> 4166.78s]  So the extension means that I could add something?
[4166.78s -> 4169.18s]  You could add something to it if you wanted.
[4169.18s -> 4172.18s]  You know, IBM has also implemented processes,
[4172.18s -> 4176.42s]  and so has Sun before they got acquired by Oracle.
[4176.42s -> 4180.26s]  So lots of people have experimented with it.
[4180.26s -> 4184.58s]  You know, in terms of sort of commercial uses of it,
[4184.58s -> 4186.02s]  there'd be more software,
[4186.02s -> 4189.98s]  but software where you kind of localize
[4189.98s -> 4192.44s]  the transactional memory use to places
[4192.44s -> 4193.70s]  that would be very difficult
[4193.70s -> 4195.18s]  to implement with locking, right?
[4195.26s -> 4199.58s]  So the certain database system implementations
[4199.58s -> 4201.54s]  to kind of use, you know,
[4201.54s -> 4204.58s]  at the core of the database management system,
[4204.58s -> 4206.36s]  you use transactional memory ideas
[4206.36s -> 4210.42s]  in order to implement parts of the system.
[4210.42s -> 4211.26s]  Yeah?
[4211.26s -> 4212.38s]  I had a follow-up on that.
[4212.38s -> 4214.82s]  So I think one thing I realized in the previous slide
[4214.82s -> 4216.26s]  was one of the major problems
[4216.26s -> 4219.14s]  was if something goes out of the gadget, for example,
[4219.14s -> 4221.82s]  then you basically abort and restart.
[4221.82s -> 4222.66s]  Right.
[4222.66s -> 4226.78s]  Like having companies start by having a separate cache
[4226.78s -> 4229.02s]  or somehow prioritizing.
[4229.02s -> 4230.58s]  Well, you never want to specialize
[4230.58s -> 4233.18s]  because if you specialize,
[4233.18s -> 4235.78s]  then you don't get to use those resources
[4235.78s -> 4238.22s]  for other processes, for programs
[4238.22s -> 4239.54s]  that are not using transactional memory.
[4239.54s -> 4241.54s]  So you'd like to reuse the same resources.
[4241.54s -> 4244.74s]  What about some sort of a priority order?
[4244.74s -> 4248.66s]  Like, so I understand we use these pieces of data.
[4248.66s -> 4252.38s]  There's lots of literature
[4252.62s -> 4254.58s]  on every kinds of, you know,
[4254.58s -> 4256.70s]  transactional memory has been an area
[4256.70s -> 4259.02s]  that has generated a huge amount of it.
[4259.02s -> 4263.02s]  You know, you go back 10 years or so
[4263.02s -> 4266.82s]  and there's just a lot of work in this area
[4266.82s -> 4268.10s]  and I can point you to it.
[4268.10s -> 4270.74s]  And, you know, probably, you know,
[4270.74s -> 4275.38s]  if you just type in transactional memory to Google,
[4275.38s -> 4279.98s]  you'll see an unlimited number of ideas and papers.
[4279.98s -> 4280.82s]  Yeah.
[4280.94s -> 4282.98s]  I guess my take away is it seems
[4282.98s -> 4286.02s]  that period transactional memory looks better
[4286.02s -> 4290.70s]  in binary and multi-game, but it's not a big problem.
[4290.70s -> 4294.86s]  But in practice, it's not really used that much.
[4294.86s -> 4301.62s]  It's not used that much because, you know, well,
[4301.62s -> 4305.66s]  it's not used that much because you have
[4305.66s -> 4311.78s]  to have this transactional memory system implemented
[4311.78s -> 4313.90s]  in software, right, and you need,
[4313.90s -> 4315.66s]  in order to really get good performance,
[4315.66s -> 4318.38s]  you need compiler support, right.
[4318.38s -> 4321.66s]  And, you know, and then you've got something
[4321.66s -> 4327.66s]  that is not completely general, right,
[4327.66s -> 4333.62s]  in the sense that now you now need to clone your code
[4333.62s -> 4337.38s]  in order to make it work with a transactional memory system.
[4337.38s -> 4342.86s]  And so that makes it, the whole system, a little less robust.
[4342.86s -> 4348.26s]  And so the right way to think about the ways in which
[4348.26s -> 4349.98s]  transactional memory has really been used
[4349.98s -> 4353.42s]  is people come up with data structures that
[4353.42s -> 4356.86s]  are optimized for working with transactional memory.
[4356.86s -> 4360.14s]  And then they have part of their system
[4360.14s -> 4365.50s]  that is being implemented with transactional memory.
[4365.50s -> 4367.90s]  But, you know, they're not using transactional memory
[4367.90s -> 4370.34s]  over the whole of their parallel system.
[4370.34s -> 4372.86s]  They just kind of localize the components of it.
[4372.86s -> 4378.38s]  And is that localized component in your actual game protocol?
[4378.38s -> 4380.66s]  It's usually, most of the commercial systems
[4380.66s -> 4383.34s]  that use transactional memory use software-based.
[4383.34s -> 4383.98s]  Excuse me.
[4383.98s -> 4385.42s]  No one uses a patch for years.
[4385.42s -> 4388.42s]  Almost nobody uses the hardware because the hardware is broken.
[4391.14s -> 4392.14s]  Yeah.
[4392.14s -> 4396.14s]  Is there any, like, time to be sure it's actually going to work?
[4396.14s -> 4398.14s]  Thank you very much.
[4398.14s -> 4399.14s]  That's a good question.
[4399.14s -> 4401.14s]  It's an open question.
[4401.14s -> 4404.14s]  Yeah.
[4404.14s -> 4406.14s]  All right.
[4406.14s -> 4408.14s]  I think we don't have much time
[4408.14s -> 4410.14s]  to talk about heterogeneous parallelism.
[4410.14s -> 4414.14s]  But let me kind of get going a little.
[4414.14s -> 4422.14s]  So, so far we kind of talked about, you know,
[4422.14s -> 4432.14s]  using processes and GPUs and sort of SIMD execution
[4432.14s -> 4435.14s]  to exploit parallelism.
[4435.14s -> 4443.14s]  And the focus has been on some general purpose computation
[4444.14s -> 4451.14s]  and writing parallel programs that are fairly general.
[4451.14s -> 4457.14s]  However, it turns out that if you want to get much better
[4457.14s -> 4462.14s]  efficiency in terms of the use of your hardware,
[4462.14s -> 4466.14s]  you might want something which is much more specialized,
[4466.14s -> 4467.14s]  right?
[4467.14s -> 4471.14s]  And so what we want to talk about today,
[4471.14s -> 4475.14s]  or in the couple of minutes kind of just introduce the idea,
[4475.14s -> 4480.14s]  is sort of what can you do to specialize your CPU to make
[4480.14s -> 4482.14s]  it tremendously more efficient, right?
[4482.14s -> 4486.14s]  So we've talked so far about given the current set
[4486.14s -> 4489.14s]  of resources that you might have in a modern CPU
[4489.14s -> 4493.14s]  or modern GPU, how do you use those most effectively?
[4493.14s -> 4495.14s]  But you can go further and say,
[4495.14s -> 4499.14s]  what if I've got something that I want to,
[4499.14s -> 4502.14s]  some algorithm that I want to run very efficiently,
[4502.14s -> 4507.14s]  how can I make that work much more energy efficiently, okay?
[4507.14s -> 4514.14s]  So, you know, you kind of look across
[4514.14s -> 4516.14s]  most real world applications.
[4516.14s -> 4519.14s]  They've got complex workload characteristics.
[4519.14s -> 4523.14s]  Some of them have parallelism that you can exploit
[4523.14s -> 4524.14s]  using multiple threads.
[4524.14s -> 4527.14s]  Some of them have SIMD parallelism that you can exploit
[4527.14s -> 4531.14s]  by using vector and SIMD execution.
[4531.14s -> 4534.14s]  If you look at the memory accesses,
[4534.14s -> 4537.14s]  which of course are a key component to the performance
[4537.14s -> 4540.14s]  of your application, some parts of those,
[4540.14s -> 4543.14s]  some of the memory accesses that have been made
[4543.14s -> 4545.14s]  are predictable, and if they're predictable,
[4545.14s -> 4549.14s]  then of course you can prefetch those accesses.
[4549.14s -> 4553.14s]  If they're not predictable, then maybe they cache well, right?
[4553.14s -> 4557.14s]  So hopefully you can get one or potentially
[4557.14s -> 4560.14s]  both of these characteristics, okay?
[4560.14s -> 4562.14s]  And so the question then is sort of,
[4562.14s -> 4565.14s]  if you want to be able to exploit
[4565.14s -> 4568.14s]  the wide variety of characteristics
[4568.14s -> 4571.14s]  of the application space, then maybe you want to
[4571.14s -> 4576.14s]  specialize your components to exploit
[4576.14s -> 4582.14s]  the particular characteristics of your application, right?
[4582.14s -> 4587.14s]  And so that's the idea of heterogeneity.
[4587.14s -> 4590.14s]  And it turns out that all modern processes
[4590.14s -> 4592.14s]  are in fact heterogeneous, right?
[4592.14s -> 4596.14s]  So take the Skylake processor,
[4596.14s -> 4600.14s]  that's the Intel core, i7 core architecture
[4600.14s -> 4603.14s]  in your, in myth machines, right?
[4603.14s -> 4605.14s]  They are in fact heterogeneous, right?
[4605.14s -> 4609.14s]  So, you know, the focus of our programming
[4609.14s -> 4613.14s]  has been on the CPU cores, right?
[4613.14s -> 4616.14s]  And their associated caches, but of course
[4616.14s -> 4620.14s]  they've also got an integrated GPU
[4620.14s -> 4624.14s]  and graphics and media unit, which takes
[4624.14s -> 4627.14s]  a significant amount of the area of the chip,
[4627.14s -> 4631.14s]  and they've also got some other
[4631.14s -> 4636.14s]  specialized components, right?
[4636.14s -> 4639.14s]  So the point here is that this heterogeneity
[4639.14s -> 4643.14s]  means that you can get greater efficiency
[4643.14s -> 4649.14s]  in the use of the resources of the silicon,
[4649.14s -> 4652.14s]  and in terms of the use of energy,
[4652.14s -> 4655.14s]  but of course it creates a more difficult
[4655.14s -> 4657.14s]  programming paradigm, right?
[4657.14s -> 4659.14s]  We've already seen that, you know,
[4659.14s -> 4661.14s]  you need multiple threads for the CPU cores,
[4661.14s -> 4663.14s]  you need some sort of data parallel
[4663.14s -> 4668.14s]  programming model like CUDA for the GPU,
[4668.14s -> 4669.14s]  and you might need a different
[4669.14s -> 4673.14s]  programming model for the media components, right?
[4673.14s -> 4675.14s]  And so next time we want to talk
[4675.14s -> 4677.14s]  a little bit more about the types
[4677.14s -> 4680.14s]  of heterogeneity that you might see
[4680.14s -> 4682.14s]  in the modern computing landscape,
[4682.14s -> 4685.14s]  and talk about how you address
[4685.14s -> 4688.14s]  some of these programming aspects,
[4688.14s -> 4692.14s]  especially for, you know, applications
[4692.14s -> 4695.14s]  such as the ones that you're doing
[4695.14s -> 4697.14s]  in your programming assignment,
[4697.14s -> 4702.14s]  so machine learning applications,
[4702.14s -> 4708.14s]  so we'll continue that next week.
