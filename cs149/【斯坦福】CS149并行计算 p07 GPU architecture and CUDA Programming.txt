# Detected language: en (p=1.00)

[0.00s -> 13.00s]  Okay, all right. Shall we get started? Everything else going okay? All right, week four. How
[13.00s -> 18.68s]  about we talk about GPUs? We're going to talk about some GPUs today. All right.
[18.68s -> 23.84s]  So here's what I thought I'd do today. I wanted to start with just a little bit
[23.84s -> 30.80s]  of history on how we got here with modern GPU computing, which basically is like how
[30.80s -> 38.16s]  did these chips that folks like NVIDIA and AMD were producing, that were producing largely
[38.16s -> 44.50s]  to play Quake, how did they get us to a point where NVIDIA is a trillion dollar company
[44.50s -> 48.44s]  that's much, much bigger than Intel and everybody's fighting over how to buy these
[48.44s -> 53.80s]  things. So I'm going to give some history and then we're going to do a conversation about how
[53.80s -> 60.08s]  to program these GPUs using a language that's provided by NVIDIA. Most people write GPU
[60.08s -> 65.08s]  code in CUDA these days. CUDA you're going to find very similar to ISPC. In fact,
[65.08s -> 71.20s]  ISPC, if you read that blog post about the history of ISPC, actually is a reaction to CUDA.
[71.20s -> 74.96s]  They're like, oh, people are programming GPUs with CUDA, why can't we program the same
[75.00s -> 80.68s]  way on CPUs? And so they hacked up ISPC in response. So the programming model should be
[80.68s -> 86.28s]  pretty familiar. And then we're going to talk a little bit about how modern GPU architectures
[86.28s -> 95.24s]  work and how they run that CUDA code. So I want you to think back to basically the first
[95.24s -> 102.64s]  week of class, so the second week of class, and we talked about, and you've done a written
[102.64s -> 106.96s]  assignment now, really working through the details of ideas like multi-threading and
[106.96s -> 112.24s]  SIMD and multi-core execution. And you've had some experience running that code on
[112.24s -> 116.44s]  CPUs. So the first thing I want to tell you is that there's not going to be anything
[116.44s -> 123.24s]  new today. It's the same ideas, multi-core, SIMD, multi-threading, and stuff like that.
[123.24s -> 128.20s]  They're just going to be deployed at a little bit bigger scale for a modern GPU. So in
[128.32s -> 135.44s]  some sense, no new concepts today, just another instantiation of those ideas. All right. So
[135.44s -> 141.68s]  how many of you have taken, looking for familiar faces, have taken 148 or sit around
[141.68s -> 147.60s]  for 148 after this, or 248 with me or something like that? Okay. So some of you know a little
[147.60s -> 151.44s]  bit about graphics. You know a little bit about how pictures are rendered. For the
[151.44s -> 163.00s]  rest of you, here is CS 248 in like five slides. So the goal of an original GPU design was
[163.00s -> 170.96s]  to solve one specific problem. And that one specific problem was, given some mathematical
[170.96s -> 177.16s]  description of a scene, and that mathematical description was kind of the geometry of surfaces,
[177.20s -> 185.12s]  the location of lights, the position of some fake virtual camera, perform a simulation,
[185.12s -> 190.24s]  simulate the bouncing of light in the scene, simulate all these materials, and give me a
[190.24s -> 197.56s]  picture of what I would have seen if that was a real camera at that location. That was the
[197.56s -> 204.04s]  goal of a graphics chip. And graphics chips do that quite well. You know, here's an image,
[204.04s -> 210.76s]  this is actually a pretty old image now, it's 2015. This image, 60 frames per second easily
[210.76s -> 216.68s]  on a high-end GPU in 2015. If we step forward to a little bit more modern graphics, this is
[216.68s -> 221.56s]  something that we can render in real time on unreasonable high-end GPUs. So this is from
[221.56s -> 229.92s]  the Nanite demo from Epic. And the workload, like the actual computation, if I asked you to
[229.92s -> 238.00s]  write some software that ran, you know, that computed this stuff, is pretty simple. And so
[238.00s -> 246.84s]  here's CS248 in maybe 45 seconds. So the name of the game is to render a surface like this. And
[246.84s -> 253.04s]  so the way you work with a GPU in the old days, or at least if you're doing graphics,
[253.04s -> 257.88s]  is you provide it some notion of what your surface is. And the surface that we,
[258.20s -> 263.52s]  the representation that we typically use in computer graphics are triangle meshes. So
[263.52s -> 269.26s]  you give the pipeline, you give the GPU a list of points in 3D space where all your triangle
[269.26s -> 277.92s]  vertices are. And you basically ask the GPU, given these points in 3D space and given like
[277.92s -> 284.44s]  a camera position or orientation, hey GPU, you go do a little bit of math that we teach you in
[284.48s -> 291.96s]  CS248 to figure out where on screen all of these little vertices project onto. So it's like if I
[291.96s -> 297.92s]  held up a camera right here, where is every vertex going to be on screen? And for every
[297.92s -> 302.84s]  pixel in the image that's covered by these triangles, once I know where they're on screen,
[302.84s -> 307.40s]  I need to compute the color of that triangle. And if I want to compute the color of the
[307.40s -> 312.04s]  triangle, if you look around this room, the color of all the various surfaces in the room,
[312.08s -> 317.32s]  it comes from what material they're made out of. And if you think about like, if we have to simulate
[317.32s -> 324.60s]  how light is bouncing off these chairs or off this bamboo texture or off like the shiny metal
[324.60s -> 329.88s]  of the door handles, you quickly go, wow, there's a lot of different materials in the
[329.88s -> 337.48s]  world, right? And early graphics said, oh, and like here's an example of a bunch of materials,
[337.48s -> 340.68s]  these all kind of look alike, but you know, if we can get pretty sophisticated with
[340.68s -> 345.00s]  our materials, like if we look at the glints and the shoes, if we look at the fact that
[345.00s -> 349.64s]  human skin light enters the skin, bounces around a lot and comes out somewhere else,
[349.64s -> 354.88s]  simulating how light bounces off these materials to just compute the color of these
[354.88s -> 361.44s]  triangles, people were like, the materials of the world are so different, we just want to
[361.44s -> 365.64s]  write a program that does this. It's not like we're going to say the color is red or the
[365.64s -> 373.12s]  color is blue, we're just going to write a program. And so in the mid, the early 2000s
[373.12s -> 379.08s]  was the time when people started developing these little programming languages that were
[379.08s -> 384.60s]  run for every pixel on screen, which given some information about the current
[384.60s -> 389.60s]  material and the orientation of the surface. So the code doesn't, the details of the code
[389.60s -> 394.08s]  doesn't really matter, but it's like, there's like a coordinate, a 2D coordinate of where
[394.12s -> 398.84s]  I am on the surface. There's the normal of the surface, which is like the vector pointing
[398.84s -> 405.52s]  away from the surface. And there's details of like texture maps, which if we just do a little
[405.52s -> 410.64s]  bit of work, we have algorithms for computing, given the orientation of the surface,
[410.64s -> 418.88s]  given that I want like this image to be wrapped over the chair surface. If I run this code for
[418.88s -> 423.72s]  every single pixel on screen, I compute a color. If you look at the output, return vector
[423.76s -> 431.36s]  is a color. This is like KD is a vec3 RGB. And that last component there is alpha. Running
[431.36s -> 436.12s]  this function for every element or every pixel gives me an image that looks like this.
[436.12s -> 442.92s]  So this is just basic data parallel computation like we've talked about. For every pixel,
[442.92s -> 448.56s]  compute some inputs and then run this completely independent function on every pixel
[448.56s -> 454.72s]  to compute some colors. That's kind of CS248 in a nutshell. And so the reason why GPUs
[454.72s -> 462.28s]  started adding multicore and adding SIMD is because they're like, I got a process of 4K
[462.28s -> 468.40s]  image, you know, millions of pixels, and I need to do it 60 times a second or more. You know,
[468.40s -> 472.64s]  I've got to get through hundreds of millions of pixels to billions of pixels per second. So
[472.64s -> 477.48s]  they just started like adding more and more GPUs to compute color and more of these. Sorry,
[477.72s -> 483.24s]  they added more and more cores and more and more ALUs in order to compute the color much more fast,
[483.24s -> 491.48s]  much more quickly. And so if you go back about almost exactly 20 years now and we go back to
[491.48s -> 495.92s]  lecture one, remember we said that like there were more and more transistors. The green line
[495.92s -> 501.76s]  was the number of transistors per chip. But all these ways to turn these transistors
[501.76s -> 507.40s]  into faster single threaded processors were kind of not working anymore, right? We couldn't
[507.40s -> 512.56s]  ramp clock speed because like power was going through the roof with 4 gigahertz processors.
[512.56s -> 520.00s]  We couldn't do super scaler anymore because like there was no ILP to find in threads. But
[520.00s -> 524.96s]  these GPUs were adding more, had already gone to parallelism. They're just gonna add more and
[524.96s -> 530.92s]  more cores as the green line went up. And so some folks around the country, some folks here
[530.92s -> 535.88s]  at Stanford, some folks at the University of North Carolina, and a few other places said,
[536.16s -> 544.20s]  Intel's processors are not getting any faster. But these GPUs are adding more cores every single
[544.20s -> 549.76s]  time the transistor doubles and they can run code. They run these little programs that compute the
[549.76s -> 556.52s]  color of things. So some folks just started to hack. And they started to hack by doing the
[556.52s -> 563.52s]  following. Let's say you were rendering an image to a screen that was 512 by 512. They said,
[563.52s -> 568.20s]  here's what we're going to do. We're going to render two triangles so that the entire
[568.20s -> 575.12s]  screen is covered by those triangles. And then we're going to, instead of running a program
[575.12s -> 578.12s]  that computes the color of something, we're just going to have that program do something
[578.12s -> 583.44s]  interesting like time step a particle in a physics simulation or do some posting folding
[583.44s -> 589.00s]  or something like that. So there was this big hack where you actually drew two triangles
[589.00s -> 595.72s]  on the screen basically to create 512 squared function calls. And then inside this thing
[595.72s -> 599.56s]  that was supposed to compute the color of a surface, you just said, eh, do something
[599.56s -> 603.96s]  that I want to do. And so there are these hacks. Like if you look at the dates on these,
[603.96s -> 611.20s]  about 20 years ago, people said, oh, I can treat that RGBA output as XYZ position of a
[611.20s -> 614.72s]  fluid simulation or something like that. So people started hacking. And they're like,
[614.72s -> 620.24s]  these GPUs are really, really fast. We can actually run a lot faster than what we would
[620.24s -> 626.28s]  do if we ran threaded code in C++ or something like that. Now, there was a research project
[626.28s -> 632.12s]  at Stanford in 2004, which kind of said, wait a minute, this is a hack. This is pretty stupid,
[632.12s -> 636.44s]  right? Like if we're going to use these things as parallel processors and we all know
[636.44s -> 641.44s]  about data parallelism and all this other stuff, could we just have a proper programming system
[641.48s -> 649.36s]  to just treat them as such, right? And so there was this project where folks used the data
[649.36s -> 657.16s]  parallel programming model, which we'll talk about in more detail on Thursday. But the main
[657.16s -> 662.04s]  idea is there's some function here, like in this case, scale, which takes as input an A
[662.04s -> 669.04s]  and a B and just scales A by amount. But scale is evaluated not on scalar values,
[669.16s -> 673.52s]  but on collection. So I'm making up some syntax here. Think about this as like a vector
[673.52s -> 679.04s]  of a thousand numbers. And you're applying scale to every element of that collection.
[679.04s -> 683.48s]  So this was called like the stream programming model or data parallel programming model.
[683.48s -> 689.12s]  And this code, which was actually valid code in this language called Brooke, actually
[689.12s -> 693.64s]  got compiled source to source to this graphics program that drew two triangles and
[693.64s -> 699.76s]  used the GPU. Again, like less of a hack for the users, but a total hack from the
[699.76s -> 704.92s]  implementation standpoint, okay? So around this time, NVIDIA was making faster and
[704.92s -> 709.52s]  faster processors. They were starting to think about, well, maybe these things can do more
[709.52s -> 716.68s]  than just graphics because they run code. And so inspired by projects like this and others,
[716.68s -> 723.24s]  NVIDIA says, you know what? We should actually just do it ourselves to allow people to write
[723.24s -> 727.68s]  general purpose code for these things because the chip could already do that.
[727.68s -> 734.40s]  So if we remember a little bit about how CPUs work, imagine you had a multi-core CPU
[734.40s -> 740.24s]  here that has two cores, two total execution contexts. How do you run some code on this
[740.24s -> 747.24s]  processor? Or in other words, what does the operating system do? Like actually think
[747.24s -> 752.96s]  through it. Imagine you were taking, let's hypothesize, many of you probably have not,
[752.96s -> 757.72s]  but imagine you're taking an operating systems class. And as an operating system,
[757.72s -> 760.80s]  you're supposed to, let's say we want to run two different programs on this two
[760.80s -> 765.48s]  core machine. How do you get that program started?
[765.48s -> 770.24s]  So if you have two different programs, ideally you have like two stream instructions.
[770.24s -> 776.04s]  I have two programs. I compile my program. And then as the operating system, if I want
[776.04s -> 778.04s]  that program to run, what happens? What do I do?
[778.04s -> 782.04s]  If you have two streams that interrupt each of the streams, is there a way to kind of
[782.04s -> 785.04s]  take parts of each stream and assign it to each of the cores?
[785.04s -> 789.04s]  Yeah, so the operating system is going to say, here's instruction stream one.
[789.04s -> 793.04s]  Hey core, like let me initialize the value of your registers. I'm going to set the
[793.04s -> 797.04s]  next instruction to execute to the top of that program. You just tell it go.
[797.04s -> 801.04s]  So it runs a thread here. And if you want to run another thread, the operating system
[801.04s -> 804.04s]  might say, I've got this other thread from this other program. I'm going to run
[804.04s -> 809.04s]  it here on core zero. So you kind of say, here's your program binary.
[809.04s -> 815.04s]  Initialize the state of the thread and go. And you do this one thread at a time.
[815.04s -> 819.04s]  And that's exactly what an operating system does if you took an operating
[819.04s -> 825.04s]  system, of course. That's how we run code on CPUs.
[825.04s -> 831.04s]  But how we ran code on GPUs before NVIDIA said, hey, we need to give you access,
[831.04s -> 836.04s]  was there was no like run a thread. It was literally the command you sent the GPU
[836.04s -> 841.04s]  was draw these triangles and use this program to compute the color of the pixels
[841.04s -> 845.04s]  after you figured out what pixels need to be colored in. And so that interface
[845.04s -> 849.04s]  to the GPU was this thing we call the graphics pipeline that has all the
[849.04s -> 854.04s]  algorithms of CS248 in it. But the interface to software is literally
[854.04s -> 859.04s]  software gives the GPU a list of triangles and a program to compute the
[859.04s -> 864.04s]  color of the surface and the location of the camera and then the pipeline just runs.
[864.04s -> 869.04s]  And it runs that program whenever it needs to compute the color of a pixel.
[869.04s -> 874.04s]  Very, very different way of thinking about how to kick off computation.
[874.04s -> 879.04s]  So in 2007 NVIDIA said we need something
[879.04s -> 884.04s]  kind of in between those two. Like it seems stupid to draw triangles to use our processor to do things
[884.04s -> 889.04s]  that have nothing to do with graphics. And at the same time like we don't
[889.04s -> 894.04s]  really want users to create all these different threads
[894.04s -> 899.04s]  because all these different threads aren't going to run very well on my GPU that has 32 wide SIMD and has terrible
[899.04s -> 904.04s]  single threaded performance and stuff like that. So they created this new abstraction
[904.04s -> 909.04s]  called compute mode which exists today and the interface to the hardware is the
[909.04s -> 914.04s]  following now. It says you write yourself a little program like some function
[914.04s -> 919.04s]  foo. In this case I call it my kernel because often we call like in a data parallel sense the code
[919.04s -> 924.04s]  you run is called the kernel. And then you say I want to
[924.04s -> 929.04s]  invoke end copies of this kernel.
[929.04s -> 934.04s]  You just you know GPU chip you figure out like how to do that but I just want this
[934.04s -> 939.04s]  kernel run end times. So what programming model is this? This is
[939.04s -> 944.04s]  classic SPMD programming. I'm going to give you some code. It's going to have a thread ID in here
[944.04s -> 949.04s]  somewhere and we're going to launch end copies and run end copies of this. So you can think about this
[949.04s -> 954.04s]  as like you know similar programming model to something like ISPC.
[954.04s -> 959.04s]  And that programming model is what's embodied in what's called the CUDA programming language which
[959.04s -> 964.04s]  in modern GPU programming is basically C, C++ with the one exception
[964.04s -> 969.04s]  of you don't create threads. You just say here's my program please run
[969.04s -> 974.04s]  end copies of it. And then the GPU says okay I'll go figure out how to
[974.04s -> 979.04s]  parallelize those copies however I wish.
[979.04s -> 984.04s]  Before it was just computing two triangles and it wasn't in an SPMD function?
[984.04s -> 989.04s]  Yeah I mean the difference from before it was literally
[989.04s -> 994.04s]  you would open up OpenGL and you would say
[994.04s -> 999.04s]  OpenGL draw these triangles and these positions and you'd set up the positions very carefully so that
[999.04s -> 1004.04s]  every pixel on screen was covered. It was just you know it was a hack.
[1004.04s -> 1009.04s]  So now they're just like let's use proper SPMD programming abstractions to essentially do the same thing.
[1009.04s -> 1014.04s]  Yeah exactly. Okay so here's the plan. So that's just some history
[1014.04s -> 1019.04s]  and that's where we are today with CUDA and why.
[1019.04s -> 1024.04s]  So I'm going to talk a little bit about this programming model and while you're thinking about it I want you to think about
[1024.04s -> 1029.04s]  is this you know like should I be thinking about this as data parallel? Should I be thinking about it as
[1029.04s -> 1034.04s]  SPMD? Is this message passing or a shared address space?
[1034.04s -> 1039.04s]  You know if you had to organize this in your head how would you draw analogies to
[1039.04s -> 1044.04s]  ISPC and some things that you know? Okay. And I'm
[1044.04s -> 1049.04s]  going to describe since we're talking about CUDA today I'm going to use
[1049.04s -> 1054.04s]  CUDA terminology. So when I talked about ISPC remember I said there was like a
[1054.04s -> 1059.04s]  gang of program instances and you kind of thought about program instances as
[1059.04s -> 1064.04s]  a thread logically but you kind of knew under the hood the implementation was not like a hardware
[1064.04s -> 1069.04s]  execution context. It was a little vector lane. In CUDA
[1069.04s -> 1074.04s]  a program instance in ISPC is called a
[1074.04s -> 1079.04s]  CUDA thread. So when I say thread in this lecture unless I'm being careful to say like hardware
[1079.04s -> 1084.04s]  execution context I mean a CUDA thread and a CUDA thread is whatever
[1084.04s -> 1089.04s]  NVIDIA defines it to be.
[1089.04s -> 1094.04s]  So here we go. So just like in ISPC when you called an ISPC function
[1094.04s -> 1099.04s]  you launched a gang of program instances and the number of instances in the
[1099.04s -> 1104.04s]  gang was N. It was kind of set at compile time. Usually 8 or
[1104.04s -> 1109.04s]  10 or something like that. And when you called ISPC tasks
[1109.04s -> 1114.04s]  you said that I'm going to run this function, run this code and I need to run
[1114.04s -> 1119.04s]  an N time. So when you're programming CUDA you're going to basically
[1119.04s -> 1124.04s]  think about it the same way. So here's a call from regular C++
[1124.04s -> 1129.04s]  code which says I want you to run the CUDA function matrix add
[1129.04s -> 1134.04s]  and to be honest I want you to run it given these arrays as input arguments
[1134.04s -> 1139.04s]  A, B and C are just normal C style arrays. And the only thing that's kind of funky
[1139.04s -> 1144.04s]  here in this weird bracket syntax is instead of saying I want you to run it N times
[1144.04s -> 1149.04s]  the number of instances you create
[1149.04s -> 1154.04s]  is this multidimensional value. And the reason why it's multidimensional is because it's sort of
[1154.04s -> 1159.04s]  convenient for graphics and tensor operations. It can be helpful to say
[1159.04s -> 1164.04s]  create N threads but organize them like in a 4x3 grid or something like that.
[1164.04s -> 1169.04s]  So in other words thread IDs are going to be multidimensional.
[1169.04s -> 1174.04s]  And the one additional detail is that
[1174.04s -> 1179.04s]  the instances are grouped into these blocks.
[1179.04s -> 1184.04s]  So what this code does is I'm creating
[1184.04s -> 1189.04s]  thread blocks of size 4x3 threads
[1189.04s -> 1194.04s]  12 threads per block and I'm creating a total of
[1194.04s -> 1199.04s]  the number of blocks I'm creating is 12 divided by 4
[1199.04s -> 1204.04s]  and 6 divided by 3. So in other words imagine I'm going to add
[1204.04s -> 1209.04s]  element wise add two matrices that are each of size 12 by 6.
[1209.04s -> 1214.04s]  And I want every CUDA thread to kind of take on the job
[1214.04s -> 1219.04s]  of adding one element. So I need to create 72 threads.
[1219.04s -> 1224.04s]  Now here I decided to just say
[1224.04s -> 1229.04s]  instead of just creating 72 threads and saying create a 72 threads
[1229.04s -> 1234.04s]  I just said create these thread blocks where each thread block is 4x3.
[1234.04s -> 1239.04s]  And the reason why I'm illustrating that now is I just want you to know about the concept of thread blocks
[1239.04s -> 1244.04s]  so it comes back later. If all I was trying to do in practice of adding two arrays
[1244.04s -> 1249.04s]  I might have just chosen to say that they're 1D thread blocks and I'm just going to have one block of size 72.
[1249.04s -> 1254.04s]  So here's some basic CUDA syntax.
[1254.04s -> 1259.04s]  The code I showed on the previous slide is what's up here. That's like main.cpp
[1259.04s -> 1264.04s]  calls the CUDA matrix add function. It does so with a certain number of thread blocks
[1264.04s -> 1269.04s]  and then the CUDA matrix add function looks a whole lot
[1269.04s -> 1274.04s]  like an SPMD program that you're useful with ISPC.
[1274.04s -> 1279.04s]  The only difference is instead of program count and program index
[1279.04s -> 1284.04s]  there are certain variables block index, block dimension and thread index
[1284.04s -> 1289.04s]  that allow you to kind of figure out your unique thread ID.
[1289.04s -> 1294.04s]  So in this case thread ID is the ID of the thread in the block
[1294.04s -> 1299.04s]  like 0010 or so on and so on.
[1299.04s -> 1304.04s]  The block ID is the block and then to know
[1304.04s -> 1309.04s]  your unique position you need to know the size of the blocks. So you can kind of think about it as your thread ID
[1309.04s -> 1314.04s]  in the block is kind of like your program instance ID
[1314.04s -> 1319.04s]  your program instance. The block ID is kind of like your task ID
[1319.04s -> 1324.04s]  in ISPC. So there's like this level of hierarchy.
[1324.04s -> 1329.04s]  ISPC organizes things into tasks and gangs inside the task.
[1329.04s -> 1334.04s]  It organizes itself into thread blocks and then can create many thread blocks at the same time.
[1334.04s -> 1339.04s]  No new concepts, just kind of some different names and some small
[1339.04s -> 1344.04s]  differences in the details. So if you look here
[1344.04s -> 1349.04s]  this is a CUDA function called matrix add. The first thing it does is
[1349.04s -> 1354.04s]  every thread computes which element of the input arrays it is responsible for based on its thread ID
[1354.04s -> 1359.04s]  and its block ID and then every thread carries out the work that it's
[1359.04s -> 1364.04s]  responsible for. And keep in mind that in my program 72 instances
[1364.04s -> 1369.04s]  of this function are being executed concurrently. So the entire array is added
[1369.04s -> 1374.04s]  together. Each thread copies or adds one element and I create
[1374.04s -> 1379.04s]  one thread per element of the matrix. Ok so there's some questions.
[1379.04s -> 1384.04s]  I'm showing you the 2D system
[1384.04s -> 1389.04s]  in these slides because it exists and I kind of want you to know about it.
[1389.04s -> 1394.04s]  I could have done all of these intro examples as thread ID and block ID being
[1394.04s -> 1399.04s]  one dimensional. And that would have worked just fine for this example.
[1399.04s -> 1404.04s]  It turns out that in a lot of things that we do on GPUs, image processing, tensor
[1404.04s -> 1409.04s]  processing, graphics, it actually can be a little bit helpful to have your thread
[1409.04s -> 1414.04s]  ID be two dimensional because then you just go IJ. Whereas if your thread ID
[1414.04s -> 1419.04s]  was one dimensional and then you had to compute your location like in an ND
[1419.04s -> 1424.04s]  tensor there would be a lot of divides. So actually you can skip
[1424.04s -> 1429.04s]  a lot of divides by giving you the multi-dimensional location
[1429.04s -> 1434.04s]  and so you can compute your memory address more efficiently. So that's the history of that.
[1434.04s -> 1439.04s]  But it's not fundamental at all. And every year I consider changing
[1439.04s -> 1444.04s]  these as the initial intro to be 1D thread IDs but then I have to introduce 2D later
[1444.04s -> 1449.04s]  and so I just decide to save some time by doing that. So at this point you should be thinking about the
[1449.04s -> 1454.04s]  two worlds to live in.
[1455.04s -> 1460.04s]  There's the world that's running the normal C++ code over here.
[1460.04s -> 1465.04s]  That's like void main. And you should think about that as running on a thread
[1465.04s -> 1470.04s]  on the CPU. And then there's the world that runs
[1470.04s -> 1475.04s]  all of these instances of matrix add. And in ISPC that's what we said
[1475.04s -> 1480.04s]  oh it's a gang of program instances. In ISPC we said the implementation
[1480.04s -> 1485.04s]  will send the instructions in that same thread.
[1485.04s -> 1490.04s]  Now without getting into too many implementation details the implementation of running
[1490.04s -> 1495.04s]  all of these CUDA threads is actually going to be execution on the GPU.
[1495.04s -> 1500.04s]  So that call of matrix add double B here is
[1500.04s -> 1505.04s]  the point at which in the underlying implementation if we get into that
[1505.04s -> 1510.04s]  that's going to be the communication to start running things on the GPU.
[1517.04s -> 1522.04s]  For reasons that don't seem necessary in this example
[1522.04s -> 1527.04s]  I have organized my code instead of just saying launch 72 threads
[1527.04s -> 1532.04s]  I have said launch a certain number of thread blocks
[1532.04s -> 1537.04s]  and each block is a 4 by 3 grid of threads.
[1537.04s -> 1542.04s]  So the thread ID is two dimensional, those are these numbers.
[1542.04s -> 1547.04s]  The block dims are not on this slide but the block dim is 4 by 3.
[1547.04s -> 1552.04s]  And then the block index is whatever the current block is.
[1552.04s -> 1557.04s]  So it's just a multidimensional addressing there.
[1558.04s -> 1563.04s]  So the CUDA memory model is I have like void main
[1563.04s -> 1568.04s]  presumably running on the CPU that has its own address space
[1568.04s -> 1573.04s]  and in simplistic CUDA then there's the device
[1573.04s -> 1578.04s]  the GPU side of the world that has its own memory address space
[1578.04s -> 1583.04s]  that can be accessed by the CUDA threads.
[1584.04s -> 1589.04s]  So that means that if you allocate an array in C
[1589.04s -> 1594.04s]  and try and dereference that pointer in CUDA
[1594.04s -> 1598.04s]  that won't work because these are in different address spaces.
[1598.04s -> 1603.04s]  So here's an example of this code. Here's my void main running on the CPU.
[1603.04s -> 1607.04s]  I allocate in array A a normal C allocation
[1607.04s -> 1610.04s]  that's an allocation in your CPU address space.
[1610.04s -> 1614.04s]  Then I allocate, or is it CUDA malloc?
[1614.04s -> 1619.04s]  Then I allocate an array in the GPU side of the fence, the device address space.
[1619.04s -> 1624.04s]  And then I use library calls to move the values from the CPU address space
[1624.04s -> 1626.04s]  to the GPU address space.
[1626.04s -> 1633.04s]  And then when I call a CUDA kernel I pass up the pointers to device A
[1633.04s -> 1637.04s]  because the CUDA kernels running on the GPU need to access the memory
[1637.04s -> 1639.04s]  in the address space of the GPU.
[1640.04s -> 1642.04s]  This is kind of nuts and bolts CUDA.
[1642.04s -> 1648.04s]  Now these days on more modern systems it's easy to actually just directly pass
[1648.04s -> 1650.04s]  a C pointer into a CUDA kernel
[1650.04s -> 1654.04s]  but it actually means that your GPU memory access
[1654.04s -> 1657.04s]  is actually going to go over PCIe and things like that.
[1657.04s -> 1662.04s]  So I want to stick to kind of straightforward separate address spaces for now.
[1665.04s -> 1667.04s]  Alright, questions? Yes.
[1667.04s -> 1669.04s]  So, given that the memory is separate,
[1669.04s -> 1672.04s]  I'm going to give you an example of a pattern.
[1672.04s -> 1677.04s]  In this case, are there any situations where it's useful to
[1677.04s -> 1680.04s]  like pre-fetch data from the CPU to the GPU
[1680.04s -> 1682.04s]  knowing what computation to use?
[1682.04s -> 1687.04s]  So this CUDA memcpy right here is a movement from data,
[1687.04s -> 1690.04s]  now we're talking about implementation,
[1690.04s -> 1694.04s]  is a movement of data from CPU memory.
[1695.04s -> 1699.04s]  And if we're thinking about a system that has a GPU that's like a discrete card,
[1699.04s -> 1702.04s]  like a RTX GPU or an A100,
[1702.04s -> 1707.04s]  this memcpy is actually moving bytes over the PCIe bus
[1707.04s -> 1710.04s]  and putting it in the GPU's DRAM, two different DRAMs.
[1710.04s -> 1713.04s]  So that's actually a slow copy, right?
[1713.04s -> 1717.04s]  And yes, there are ways to do that asynchronous and stuff like that.
[1717.04s -> 1720.04s]  So what does this feel like actually in terms of,
[1720.04s -> 1722.04s]  last time didn't we talk about message passing?
[1722.04s -> 1725.04s]  Kind of in some sense, this memcpy is a message pass
[1725.04s -> 1727.04s]  from one address space to another.
[1727.04s -> 1730.04s]  And you very well might want to do it asynchronously
[1730.04s -> 1733.04s]  to hide latency and stuff like that, absolutely.
[1734.04s -> 1735.04s]  Yeah.
[1735.04s -> 1739.04s]  So, during the previous example when we had matrix add,
[1739.04s -> 1744.04s]  what we want is, we're passing A, B and C into this function, right?
[1744.04s -> 1746.04s]  So we would want A, B and C...
[1746.04s -> 1751.04s]  A, B and C in this example are CUDA device allocations, right?
[1751.04s -> 1754.04s]  And actually I'm glad you brought me back to this example.
[1754.04s -> 1758.04s]  Here's an example I wanted to check everybody's understanding here.
[1758.04s -> 1761.04s]  Okay, so this is the same matrix add example,
[1761.04s -> 1765.04s]  but I changed the matrix size to 11 by 5.
[1767.04s -> 1770.04s]  But I kept the thread block size to 4 by 3.
[1772.04s -> 1775.04s]  I did this just for explanatory purposes.
[1775.04s -> 1778.04s]  So I have 55 things to do, right?
[1779.04s -> 1781.04s]  But I have thread blocks, I create work
[1781.04s -> 1784.04s]  in the granularity of groups of 12 threads.
[1786.04s -> 1788.04s]  So I have, you know, if I round up,
[1788.04s -> 1791.04s]  and if you look at my little math there for num blocks,
[1791.04s -> 1793.04s]  I'm rounding up.
[1793.04s -> 1797.04s]  Which means I'm creating more threads than just 55 threads.
[1797.04s -> 1800.04s]  So when each of these threads does this work
[1800.04s -> 1802.04s]  to figure out its I and J,
[1802.04s -> 1806.04s]  notice that some of those threads would actually be out of bounds in that array.
[1808.04s -> 1810.04s]  And I have an if statement in the code to say,
[1810.04s -> 1814.04s]  hey, this thread, check to make sure that you're actually responsible for valid work,
[1814.04s -> 1817.04s]  and if you're not, don't do anything.
[1817.04s -> 1820.04s]  So this really underscores, it should underscore,
[1820.04s -> 1823.04s]  that this is an SPMD programming model.
[1823.04s -> 1825.04s]  You're just creating threads,
[1825.04s -> 1829.04s]  and the threads use the program to figure out what they're supposed to do.
[1829.04s -> 1831.04s]  You are not doing something which says,
[1831.04s -> 1834.04s]  for every element in an array, launch a thread.
[1834.04s -> 1836.04s]  That's not the programming model.
[1836.04s -> 1839.04s]  The programming model is, you launch a thread for all your blocks,
[1839.04s -> 1842.04s]  and for all your threads per block,
[1842.04s -> 1845.04s]  and inside in my code, I might write some code
[1845.04s -> 1847.04s]  that happens to do one thing per thread,
[1847.04s -> 1849.04s]  or I could do whatever I want.
[1849.04s -> 1851.04s]  So that if statement there,
[1851.04s -> 1853.04s]  what would happen if I didn't have that if statement?
[1856.04s -> 1859.04s]  I would basically read and write to memory
[1859.04s -> 1862.04s]  that was off the end of my array.
[1862.04s -> 1864.04s]  So who knows, my program would crash.
[1864.04s -> 1867.04s]  So you're saying the abstraction is that you're kind of creating threads,
[1867.04s -> 1870.04s]  but it's just the implementation that's very different.
[1870.04s -> 1872.04s]  It's like CPU threads.
[1872.04s -> 1876.04s]  Correct. The abstraction, to be precise, is you're creating CUDA threads.
[1876.04s -> 1878.04s]  But you're not creating them one by one.
[1878.04s -> 1881.04s]  You're creating them with a bulk thread launch.
[1881.04s -> 1884.04s]  So it's like, imagine you had a C++ API
[1884.04s -> 1888.04s]  which said, standard thread create,
[1888.04s -> 1890.04s]  but you gave it some parameters after that,
[1890.04s -> 1893.04s]  which was like, I want 150 of them.
[1893.04s -> 1895.04s]  But really it would be like,
[1895.04s -> 1897.04s]  it's not only that I want 150 of them,
[1897.04s -> 1899.04s]  I want 150 blocks of threads,
[1899.04s -> 1901.04s]  and each block should be four by three.
[1901.04s -> 1904.04s]  So there's this bulk launch of threads.
[1904.04s -> 1906.04s]  And I haven't told you how it's implemented yet,
[1906.04s -> 1909.04s]  other than the GPU is going to run those threads.
[1909.04s -> 1911.04s]  Yeah.
[1911.04s -> 1917.04s]  Can you go back a few slides and add your CUDA threads?
[1918.04s -> 1920.04s]  Are you declaring CUDA?
[1920.04s -> 1922.04s]  Yeah, I know.
[1922.04s -> 1923.04s]  What happened?
[1923.04s -> 1924.04s]  Yeah.
[1924.04s -> 1929.04s]  So out here, after you've done this CUDA function,
[1929.04s -> 1933.04s]  I guess all the computations you will be performing
[1933.04s -> 1935.04s]  will be on device A, right?
[1935.04s -> 1939.04s]  All the CUDA computations I'll be doing will be on device A.
[1939.04s -> 1940.04s]  Correct.
[1940.04s -> 1943.04s]  And in this code right here, like in this C++ code,
[1943.04s -> 1948.04s]  if I said printf device A sub zero,
[1948.04s -> 1952.04s]  convince yourself that that should segfault.
[1952.04s -> 1956.04s]  Because device A is actually a pointer into an address space
[1956.04s -> 1958.04s]  that I can't reference from here.
[1958.04s -> 1960.04s]  This is basically, this is low level C code,
[1960.04s -> 1963.04s]  so you can do that kind of stuff and mess up.
[1963.04s -> 1965.04s]  Okay.
[1965.04s -> 1969.04s]  All right, so the basics of the memory model
[1969.04s -> 1972.04s]  is that you have an address space for your CPU threads,
[1972.04s -> 1976.04s]  you have an address space for your GPU CUDA threads,
[1976.04s -> 1980.04s]  and it turns out that you actually have additional address spaces in CUDA.
[1980.04s -> 1982.04s]  We have a block of threads.
[1982.04s -> 1985.04s]  And one reason why they have this concept of a block of threads
[1985.04s -> 1991.04s]  is they say that, oh, every thread block has an address space
[1991.04s -> 1994.04s]  that only those threads can access.
[1994.04s -> 1998.04s]  And every thread has its own local address space,
[1998.04s -> 2001.04s]  like its own local stack that only it can access.
[2001.04s -> 2006.04s]  So local variables to a thread can only be accessed by the thread.
[2006.04s -> 2009.04s]  That makes sense. That's like ISPC.
[2009.04s -> 2012.04s]  But now there's like per-thread block variables,
[2012.04s -> 2016.04s]  which are kind of like uniform variables in ISPC, actually.
[2016.04s -> 2018.04s]  There's per-thread block variables,
[2018.04s -> 2021.04s]  and then there's overall device global memory
[2021.04s -> 2024.04s]  that can be accessed by any thread with loads and stores.
[2024.04s -> 2027.04s]  So you just see this organization that's popping up.
[2027.04s -> 2029.04s]  You're kind of saying, here's a bunch of threads
[2029.04s -> 2032.04s]  that are going to work together on something.
[2032.04s -> 2035.04s]  Here's another group of threads that are going to work together on something.
[2035.04s -> 2039.04s]  And to do that work together, they're going to have some shared space.
[2039.04s -> 2041.04s]  And so hopefully you're starting to think,
[2041.04s -> 2045.04s]  oh, we're giving hints to the GPU on how to co-locate all these threads
[2045.04s -> 2047.04s]  for locality purposes.
[2047.04s -> 2049.04s]  Okay, so let's do some CUDA practice
[2049.04s -> 2052.04s]  with one of the simplest things I can think of in CUDA,
[2052.04s -> 2055.04s]  which is let's not add two matrices,
[2055.04s -> 2062.04s]  but let's perform a simple 1D convolution.
[2062.04s -> 2065.04s]  So I'm going to take the output is the average
[2065.04s -> 2067.04s]  of the inputs in the same location.
[2067.04s -> 2071.04s]  So this is a basic operation image processing or signal processing.
[2071.04s -> 2074.04s]  It's actually the 1D equivalent of a convolutional layer
[2074.04s -> 2077.04s]  in a deep neural network, something that you might want to do.
[2077.04s -> 2079.04s]  So look at the top of the slide,
[2079.04s -> 2083.04s]  and I have illustrated the input and output location.
[2083.04s -> 2086.04s]  And I have some red arrows there that suggest
[2086.04s -> 2090.04s]  what input elements are needed to compute every output element.
[2090.04s -> 2093.04s]  So first of all, confirm that you agree
[2093.04s -> 2096.04s]  the input is two elements bigger than the output,
[2096.04s -> 2099.04s]  so I don't have to worry about any boundary conditions or crap like that.
[2099.04s -> 2102.04s]  Okay, so here's the code for that.
[2102.04s -> 2107.04s]  So first of all, we call the convolve CUDA kernel.
[2107.04s -> 2111.04s]  If the array is 1024 in size,
[2111.04s -> 2115.04s]  let's say the array is a million elements, 1024 in size,
[2115.04s -> 2121.04s]  I decided to create, if my block size is threads per block,
[2121.04s -> 2125.04s]  notice that I'm spawning n over threads per block blocks,
[2125.04s -> 2127.04s]  assuming that that divides evenly.
[2127.04s -> 2129.04s]  Let's keep my integer map.
[2129.04s -> 2132.04s]  And then for every thread, we just treat it as a 1D array index,
[2132.04s -> 2134.04s]  in this case.
[2134.04s -> 2136.04s]  And I say, okay, I'm going to compute my index
[2136.04s -> 2138.04s]  that's responsible for computing.
[2138.04s -> 2142.04s]  I'm going to load three values from memory,
[2142.04s -> 2145.04s]  multiply them together, and output the result.
[2145.04s -> 2149.04s]  So first of all, confirm that this is a correct implementation
[2149.04s -> 2152.04s]  of a 1D convolution in CUDA.
[2152.04s -> 2159.04s]  I will produce 1024 squared output elements.
[2159.04s -> 2162.04s]  All right, so far in this example,
[2162.04s -> 2166.04s]  I didn't use the idea of threads per block really at all.
[2166.04s -> 2168.04s]  Like, I really would have preferred a programming model
[2168.04s -> 2170.04s]  that just said, get rid of all this stuff,
[2170.04s -> 2174.04s]  just say launch n threads, and I'm good.
[2174.04s -> 2177.04s]  Like, I don't have to deal with any of this blocking and stuff like that.
[2177.04s -> 2180.04s]  Okay, so if you look at this code,
[2180.04s -> 2184.04s]  this code, every thread reads three elements,
[2184.04s -> 2187.04s]  and different threads, like kind of neighboring threads,
[2187.04s -> 2190.04s]  actually overlap in the elements that they read.
[2190.04s -> 2195.04s]  So in some sense, this thread is issuing load instructions for data,
[2195.04s -> 2200.04s]  and then the other thread is issuing load instructions for the same data.
[2200.04s -> 2203.04s]  Now, maybe there could be some cache hits and stuff like that
[2203.04s -> 2207.04s]  to amplify that, but if we really wanted to write an efficient program,
[2207.04s -> 2209.04s]  let me show you something else that I can...
[2209.04s -> 2211.04s]  So here's a much more complex program,
[2211.04s -> 2215.04s]  but gives you a sense of what you can do with this idea of a block
[2215.04s -> 2218.04s]  that has memory local to the block.
[2218.04s -> 2220.04s]  Okay, so the same thing down here.
[2220.04s -> 2223.04s]  I'm still launching the same number of thread blocks.
[2223.04s -> 2226.04s]  My thread block size is what? 128 here.
[2226.04s -> 2232.04s]  So to compute 128 outputs, how many inputs do I need?
[2232.04s -> 2236.04s]  128 plus 2, right? I need 130 inputs.
[2236.04s -> 2239.04s]  So now look at what my thread does.
[2239.04s -> 2245.04s]  My thread allocates this shared array of 130 elements,
[2245.04s -> 2248.04s]  and that prefix, that type modifier shared,
[2248.04s -> 2252.04s]  is saying this is not a per-thread allocation.
[2252.04s -> 2255.04s]  This is a per-thread block allocation.
[2255.04s -> 2259.04s]  So this is kind of like a uniform variable in ISPC.
[2261.04s -> 2264.04s]  And now look what the threads do. This is kind of interesting.
[2264.04s -> 2270.04s]  So every thread loads one value.
[2270.04s -> 2274.04s]  If I go right to that first line, index equals...
[2274.04s -> 2278.04s]  So I compute the index, and then the first line of code in the block
[2278.04s -> 2285.04s]  is copying a value from the input array into this shared allocation.
[2285.04s -> 2289.04s]  So the first thread in the block computes something
[2289.04s -> 2292.04s]  and then copies that value into that shared variable
[2292.04s -> 2297.04s]  at address 0, at index 0, and so on and so on.
[2297.04s -> 2299.04s]  And then I say if the thread index,
[2299.04s -> 2303.04s]  and remember that's a block local thread index, is less than 2.
[2303.04s -> 2307.04s]  So in other words, if your thread's 0 or 1 in the block,
[2307.04s -> 2310.04s]  what do they do?
[2313.04s -> 2315.04s]  They move the last two elements.
[2315.04s -> 2318.04s]  So I have 128 threads in the block.
[2318.04s -> 2322.04s]  Every thread is responsible for grabbing one piece of data from memory
[2322.04s -> 2324.04s]  and putting it in this shared array.
[2324.04s -> 2327.04s]  And then because I need 130 things and not 128 things,
[2327.04s -> 2330.04s]  I had to have two of these folks do a little bit more work.
[2330.04s -> 2332.04s]  So ugly.
[2332.04s -> 2335.04s]  And then ignore this for a second,
[2335.04s -> 2338.04s]  and then every thread just does what it's supposed to do.
[2338.04s -> 2339.04s]  It computes the convolution,
[2339.04s -> 2342.04s]  but instead of accessing main global memory,
[2342.04s -> 2347.04s]  it accesses this local support, the shared variable.
[2349.04s -> 2351.04s]  Now this is the last little piece of this,
[2351.04s -> 2357.04s]  and that sync threads is the equivalent of a CUDA barrier
[2357.04s -> 2360.04s]  within a thread block.
[2360.04s -> 2362.04s]  So remember, all these threads, it's SPMD,
[2362.04s -> 2364.04s]  they might be running concurrently.
[2364.04s -> 2370.04s]  All the threads cooperatively load some data into this shared array.
[2370.04s -> 2374.04s]  They barrier, which is waiting for all the threads in the block
[2374.04s -> 2376.04s]  to do their share of the work.
[2376.04s -> 2379.04s]  And then once they know that all the data is there,
[2379.04s -> 2382.04s]  they run completely again in parallel, independently,
[2382.04s -> 2385.04s]  computing their output element.
[2385.04s -> 2387.04s]  So this is kind of an interesting thing about,
[2387.04s -> 2391.04s]  why did I spend all this time orchestrating this computation?
[2391.04s -> 2395.04s]  Well the reason is that these shared variables
[2395.04s -> 2401.04s]  are backed by storage that you can think of as a high-performance L1 cache.
[2401.04s -> 2405.04s]  So what CUDA knows, and now I'm talking implementation,
[2405.04s -> 2409.04s]  is that it's going to put all the threads on the same thread block,
[2409.04s -> 2412.04s]  kind of on the same core,
[2412.04s -> 2416.04s]  and those threads can now access this fast local memory
[2416.04s -> 2418.04s]  whenever there is reuse.
[2418.04s -> 2420.04s]  So this code is actually saying that, look,
[2420.04s -> 2423.04s]  no thread accesses the same data twice.
[2423.04s -> 2428.04s]  Every thread, you know, no one thread ever reuses data.
[2428.04s -> 2432.04s]  But one thread uses data that its neighbor also uses.
[2432.04s -> 2435.04s]  So we cooperate to bring all that data in once,
[2435.04s -> 2439.04s]  and then we do our work running out of really fast shared memory,
[2439.04s -> 2441.04s]  or local memory. Yes sir.
[2442.04s -> 2445.04s]  Why do you need the sync threads? Can you not, like,
[2445.04s -> 2448.04s]  I'm going to say, I've lowered this small gate,
[2448.04s -> 2450.04s]  and then, like, you know, this one will come from this small gate.
[2450.04s -> 2452.04s]  This is a great question, I want us to think about that as a class.
[2452.04s -> 2455.04s]  If I remove that sync thread's call,
[2455.04s -> 2459.04s]  why could my program be incorrect?
[2459.04s -> 2463.04s]  Do the access thread, like, the access to memory access,
[2463.04s -> 2466.04s]  have it been downloaded into the shared?
[2466.04s -> 2471.04s]  So there's no guarantee in what order a GPU is going to run this code.
[2471.04s -> 2474.04s]  It's just all these threads are potentially concurrent.
[2474.04s -> 2476.04s]  So one thread might load its data in,
[2476.04s -> 2478.04s]  and then immediately start doing the computation,
[2478.04s -> 2481.04s]  but we don't know if all the other threads have brought its data in yet.
[2481.04s -> 2483.04s]  So that sync thread is the barrier,
[2483.04s -> 2485.04s]  which, when you proceed past it,
[2485.04s -> 2488.04s]  you are guaranteed that this shared local memory
[2488.04s -> 2490.04s]  has been initialized properly.
[2490.04s -> 2492.04s]  Good question.
[2492.04s -> 2496.04s]  Why do we need a separate thread locally in the block memory?
[2496.04s -> 2499.04s]  Why couldn't we have a large block memory?
[2499.04s -> 2501.04s]  Oh, I see. So you're saying, like,
[2501.04s -> 2505.04s]  why do I have local variables like result?
[2505.04s -> 2508.04s]  I think I wouldn't necessarily stress too much about it.
[2508.04s -> 2511.04s]  I mean, it's pretty, I feel like it's, you know,
[2511.04s -> 2515.04s]  clearly the programming model needs to allocate thread local variables.
[2515.04s -> 2518.04s]  And then this, so by default,
[2518.04s -> 2520.04s]  everything is a thread local variable.
[2520.04s -> 2521.04s]  There's a copy per thread.
[2521.04s -> 2528.04s]  And then there are, it's also the ability to allocate once per block variables.
[2528.04s -> 2532.04s]  So how, you know, where these variables get allocated,
[2532.04s -> 2537.04s]  like the implementation could allocate these in a piece of memory
[2537.04s -> 2540.04s]  that's a big block for all the threads and stuff like that,
[2540.04s -> 2542.04s]  but that's up to the compiler.
[2542.04s -> 2544.04s]  Conceptually it makes sense that I need private variables
[2544.04s -> 2547.04s]  that are private to my own thread's execution,
[2547.04s -> 2550.04s]  and I might need shared variables as well.
[2551.04s -> 2554.04s]  These threads run in SPMD fashion, right?
[2554.04s -> 2556.04s]  CUDA code is SPMD, yes.
[2560.04s -> 2563.04s]  SPMD you're confusing with SIMD.
[2563.04s -> 2569.04s]  SPMD is a programming model question independent of the implementation.
[2569.04s -> 2572.04s]  So, like, ISPC is an SPMD programming model.
[2572.04s -> 2575.04s]  CUDA is a SPMD programming model.
[2575.04s -> 2577.04s]  What that means is I write one program,
[2577.04s -> 2579.04s]  like this program right here,
[2579.04s -> 2583.04s]  and the system will run it many times with different thread IDs.
[2583.04s -> 2589.04s]  Now, how we execute all those threads or all those instances efficiently
[2589.04s -> 2591.04s]  is up to the implementation.
[2591.04s -> 2597.04s]  And ISPC use SIMD instructions to execute things very efficiently.
[2597.04s -> 2601.04s]  I have not yet told you how the GPU implements things.
[2601.04s -> 2604.04s]  And yes, there will be SIMD involved, but it's a little bit different.
[2604.04s -> 2608.04s]  Are we the absolute programmer to do threads for blocks?
[2608.04s -> 2610.04s]  This is completely under your control.
[2610.04s -> 2612.04s]  Like, in the same way that it's up to you
[2612.04s -> 2615.04s]  to figure out how many threads you want to launch in C++.
[2615.04s -> 2617.04s]  Here it's up to you to figure out how many threads you want to launch,
[2617.04s -> 2619.04s]  but it's also up to you to tell the system
[2619.04s -> 2621.04s]  how you want to organize them into blocks.
[2621.04s -> 2625.04s]  Should I always view the number of threads in a single core P1?
[2625.04s -> 2631.04s]  Well, it cannot be.
[2631.04s -> 2632.04s]  Let me tell you this way.
[2632.04s -> 2636.04s]  You are not allowed for it to be more threads than a single core can run.
[2636.04s -> 2638.04s]  It can certainly be less,
[2638.04s -> 2643.04s]  because I can put multiple blocks on a core.
[2643.04s -> 2647.04s]  In this case, we were able to do this block data mean,
[2647.04s -> 2650.04s]  because we were creating data from the same part of the memory,
[2650.04s -> 2652.04s]  like, exactly what we created.
[2652.04s -> 2654.04s]  But if we were reading, say, something like a matrix,
[2654.04s -> 2658.04s]  and you want to read the column of the matrix,
[2658.04s -> 2662.04s]  would it be more efficient to do the same thing
[2662.04s -> 2664.04s]  we were reading per block memory,
[2664.04s -> 2668.04s]  or would you want to read it, say, by, like...
[2668.04s -> 2670.04s]  Well, all I'm asking is, like, does it matter
[2670.04s -> 2672.04s]  if you're reading contiguous in block or...
[2672.04s -> 2675.04s]  It will certainly matter your performance, absolutely.
[2675.04s -> 2677.04s]  Like, the nice thing here is that, like,
[2677.04s -> 2679.04s]  all these threads are accessing consecutive memory addresses,
[2679.04s -> 2681.04s]  and you can imagine that's going to be quite friendly under the hood.
[2681.04s -> 2683.04s]  But if they were accessing...
[2683.04s -> 2687.04s]  Certainly it's possible they could access arbitrary memory addresses
[2687.04s -> 2688.04s]  and bring them in.
[2688.04s -> 2690.04s]  The program would still work,
[2690.04s -> 2694.04s]  but obviously the memory system may provide lower performance
[2694.04s -> 2697.04s]  because of cache locality and other things.
[2697.04s -> 2700.04s]  And so do we have control over how, like,
[2700.04s -> 2702.04s]  CUDA-MALLOC stores memory?
[2702.04s -> 2704.04s]  Well, you have control over...
[2704.04s -> 2706.04s]  Like, when you ask CUDA-MALLOC for data,
[2706.04s -> 2709.04s]  it will allocate you a continuous part of the address space,
[2709.04s -> 2712.04s]  just like MALLOC does in C++.
[2712.04s -> 2715.04s]  How you lay out and place your matrix elements
[2715.04s -> 2717.04s]  in that block of memory is completely up to you,
[2717.04s -> 2719.04s]  is how you wrote that program.
[2720.04s -> 2722.04s]  Here I'm assuming contiguous row major,
[2722.04s -> 2725.04s]  but that was my choice as the writer.
[2725.04s -> 2728.04s]  Okay. So, you know, unlike ISPC, you actually...
[2728.04s -> 2731.04s]  You know how, like, ISPC had those cross-lane operations
[2731.04s -> 2734.04s]  to sort of communicate with some of the instances?
[2734.04s -> 2737.04s]  CUDA has a variety of synchronization constructs.
[2737.04s -> 2739.04s]  It has, like, a per-thread block barrier.
[2739.04s -> 2740.04s]  It has atomic operations.
[2740.04s -> 2742.04s]  And then, of course, there's, like,
[2742.04s -> 2744.04s]  actually synchronization that you have to do
[2744.04s -> 2745.04s]  between the host and the device.
[2745.04s -> 2746.04s]  Like, start these CUDA threads,
[2746.04s -> 2748.04s]  wait for them to be done, and stuff like that.
[2748.04s -> 2750.04s]  And you'll run into all of that on Assignment 3.
[2750.04s -> 2753.04s]  So, the programming model for CUDA
[2753.04s -> 2755.04s]  is an SPMD programming model
[2755.04s -> 2757.04s]  where you write a single program
[2757.04s -> 2759.04s]  that defines what a thread does.
[2759.04s -> 2762.04s]  What a thread does is based on its thread ID,
[2762.04s -> 2764.04s]  its block ID, and so on and so on.
[2764.04s -> 2767.04s]  Those threads can synchronize in a block
[2767.04s -> 2769.04s]  with some barriers, if they so wish.
[2769.04s -> 2772.04s]  And when I launch threads,
[2772.04s -> 2774.04s]  I launch threads en masse
[2774.04s -> 2776.04s]  for a large number of threads all at once.
[2778.04s -> 2779.04s]  Alright.
[2780.04s -> 2781.04s]  Okay.
[2781.04s -> 2786.04s]  So, now, let's start thinking about implementation.
[2786.04s -> 2787.04s]  So, take a look at this program.
[2787.04s -> 2790.04s]  This is the same convolved program as before.
[2790.04s -> 2793.04s]  Exactly the same thing as the previous slide.
[2793.04s -> 2795.04s]  Thread block size is 128.
[2795.04s -> 2797.04s]  But look down here in the host code.
[2797.04s -> 2799.04s]  This is the C++ code.
[2799.04s -> 2802.04s]  And I decide to create
[2802.04s -> 2808.04s]  about a million threads.
[2808.04s -> 2811.04s]  And if you divide a million by 128,
[2811.04s -> 2813.04s]  it's about 8K thread blocks.
[2813.04s -> 2816.04s]  So, how do you think this is going to be implemented?
[2816.04s -> 2820.04s]  Do you think NVIDIA is going to launch...
[2820.04s -> 2822.04s]  Like, semantically, this says
[2822.04s -> 2825.04s]  we need to run a million CUDA threads.
[2826.04s -> 2829.04s]  Do you think it's actually going to go try and find
[2829.04s -> 2831.04s]  a million thread contexts around these threads?
[2831.04s -> 2833.04s]  Why not?
[2833.04s -> 2835.04s]  Probably doesn't even have that capability, you know,
[2835.04s -> 2837.04s]  to have that many execution contexts.
[2837.04s -> 2841.04s]  So, what does this big bulk launch remind you of?
[2843.04s -> 2845.04s]  Yeah, it feels a lot like tasks.
[2845.04s -> 2848.04s]  It's like, I'm going to give you a whole bunch of work.
[2848.04s -> 2850.04s]  Do all of these thread blocks.
[2850.04s -> 2852.04s]  That's all the work you've got to do.
[2852.04s -> 2854.04s]  And just like an ISPC task breaks down
[2854.04s -> 2856.04s]  in the program instances,
[2856.04s -> 2859.04s]  a CUDA thread block breaks down into CUDA threads.
[2860.04s -> 2863.04s]  So, we want to be able to run this thing
[2863.04s -> 2865.04s]  without the user.
[2865.04s -> 2868.04s]  This code should work regardless of the number of cores you have
[2868.04s -> 2872.04s]  or the number of your SIMD width and so on and so on.
[2872.04s -> 2875.04s]  We want this thing to work on a big GPU with a bunch of cores
[2875.04s -> 2878.04s]  or a mid-range GPU with a small number of cores
[2878.04s -> 2881.04s]  or like a Grace Hopper chip that costs $100,000
[2881.04s -> 2883.04s]  or whatever it is these days.
[2883.04s -> 2885.04s]  We want it to run on any of these things.
[2885.04s -> 2888.04s]  Alright, so now let's talk about implementation.
[2888.04s -> 2890.04s]  Okay, so here's our program again.
[2890.04s -> 2892.04s]  Host, CPU code, device code.
[2892.04s -> 2895.04s]  And when I compile this device code now,
[2895.04s -> 2898.04s]  I not only get the instruction stream,
[2898.04s -> 2900.04s]  but I also have a little bit of metadata
[2900.04s -> 2902.04s]  about what it requires to run.
[2902.04s -> 2904.04s]  I might have something like it says,
[2904.04s -> 2907.04s]  oh, well, I need 128 threads per block
[2907.04s -> 2909.04s]  that the programmer asserted I needed.
[2909.04s -> 2913.04s]  It also says, oh, like, if it's 128 threads per block,
[2913.04s -> 2917.04s]  it means that that shared allocation is 130 elements wide.
[2917.04s -> 2919.04s]  Right?
[2919.04s -> 2921.04s]  So, I know that I need 130 floats.
[2921.04s -> 2926.04s]  I need 512 bytes of, like, storage in order to do that allocation.
[2926.04s -> 2931.04s]  So, I know I need 128 threads of resources
[2931.04s -> 2936.04s]  and I need 520 bytes of storage
[2936.04s -> 2938.04s]  in order to actually run this thing.
[2938.04s -> 2940.04s]  Okay?
[2940.04s -> 2942.04s]  Alright, so what's going to happen is,
[2942.04s -> 2944.04s]  imagine we have a little GPU here,
[2944.04s -> 2946.04s]  has four cores.
[2946.04s -> 2948.04s]  Let's not think about the details.
[2948.04s -> 2953.04s]  But I just created those 8,000 thread blocks up at the top.
[2953.04s -> 2955.04s]  So, you're pretty good at this now,
[2955.04s -> 2958.04s]  or at least for those of you that have been doing a little bit of thread pool stuff.
[2958.04s -> 2959.04s]  How's this going to work?
[2959.04s -> 2961.04s]  You just got all these tasks, all these thread blocks,
[2961.04s -> 2963.04s]  and you're just going to go poof, poof, poof, poof,
[2963.04s -> 2965.04s]  you know, to all your different workers.
[2965.04s -> 2967.04s]  And by the way, in CUDA, you could actually set up dependencies
[2967.04s -> 2969.04s]  between some of these thread blocks, and so...
[2969.04s -> 2973.04s]  But instead of, like, what you're doing in assignment two,
[2973.04s -> 2980.04s]  and writing some software that decides what worker threads get what work,
[2980.04s -> 2986.04s]  what you're doing in assignment two is embedded in GPU hardware in silicon.
[2986.04s -> 2989.04s]  So, the API that you're implementing in assignment two,
[2989.04s -> 2992.04s]  like, run this task this many times,
[2992.04s -> 2995.04s]  and remember, that's the interface to the hardware now.
[2995.04s -> 2997.04s]  So, the interface is not,
[2997.04s -> 3001.04s]  here's a program, here's a PC, start running it on this thread.
[3001.04s -> 3002.04s]  The interface to the hardware is,
[3002.04s -> 3005.04s]  here's an instruction stream, a program binary,
[3005.04s -> 3008.04s]  please run all of these thread blocks on it.
[3008.04s -> 3011.04s]  And in the hardware, right at the top of the GPU,
[3011.04s -> 3014.04s]  is something that says, oh, I have to run 8,000 thread blocks,
[3014.04s -> 3017.04s]  I've got four cores, let's get to work.
[3017.04s -> 3021.04s]  So, this is another instance of this design pattern of
[3021.04s -> 3023.04s]  you declare independent work,
[3023.04s -> 3026.04s]  and you let a scheduler assign it to workers.
[3026.04s -> 3029.04s]  So, let's talk a little bit about those workers now.
[3029.04s -> 3033.04s]  So, I'm going to give you diagrams for Volta V100.
[3033.04s -> 3036.04s]  I will, at some point later in the week,
[3036.04s -> 3039.04s]  once I get done with one of my own personal deadlines,
[3039.04s -> 3041.04s]  and once you get into the CUDA assignment, just for kicks,
[3041.04s -> 3043.04s]  I'll go ahead and make a few slides on what, like,
[3043.04s -> 3046.04s]  a modern Grace Hopper chip looks like.
[3046.04s -> 3049.04s]  But it's pretty similar, not much has changed since then.
[3049.04s -> 3051.04s]  So, okay, so I want you to think about, like,
[3051.04s -> 3056.04s]  basically like a mini little GPU processor, a GPU core.
[3056.04s -> 3059.04s]  And it's got a fetch and decode unit at the top,
[3059.04s -> 3061.04s]  like we're used to.
[3061.04s -> 3063.04s]  And ignore all the rainbow colors here,
[3063.04s -> 3066.04s]  off to the side here are 16 ALUs
[3066.04s -> 3068.04s]  that have to run in a SIMD fashion.
[3068.04s -> 3071.04s]  Okay, so that's, again, my orange boxes.
[3071.04s -> 3074.04s]  So, that's the 16 execution units.
[3074.04s -> 3079.04s]  And then here, I have all of my execution context storage.
[3079.04s -> 3081.04s]  Now, I'm drawing it this way,
[3081.04s -> 3083.04s]  a little bit different for a reason,
[3083.04s -> 3084.04s]  is I want you to think about,
[3084.04s -> 3087.04s]  here's the storage for thread 0.
[3087.04s -> 3090.04s]  So, R0, R1, R2, R3.
[3090.04s -> 3092.04s]  Here's the storage for thread 1.
[3092.04s -> 3095.04s]  Here's the storage for thread 2.
[3095.04s -> 3097.04s]  Here's the storage for thread 31.
[3097.04s -> 3100.04s]  And I'm going to keep going and going and going.
[3100.04s -> 3105.04s]  So, I have execution context for, at least in this diagram,
[3105.04s -> 3110.04s]  128 threads, right? Yeah.
[3110.04s -> 3112.04s]  Heavily multi-threaded chip.
[3112.04s -> 3116.04s]  128 threads now of execution context.
[3116.04s -> 3120.04s]  I have no super scaler in what I've drawn so far.
[3120.04s -> 3126.04s]  And I have execution units that are 16 wide SIMD.
[3126.04s -> 3127.04s]  Make sense?
[3127.04s -> 3131.04s]  And the reason why I have other things,
[3131.04s -> 3133.04s]  this is floating point execution SIMD.
[3133.04s -> 3136.04s]  It could also do an integer operation SIMD.
[3136.04s -> 3138.04s]  It could also do some weird funky math,
[3138.04s -> 3140.04s]  like an eight-wide SIMD,
[3140.04s -> 3142.04s]  like trig functions and stuff like that.
[3142.04s -> 3144.04s]  It can also do loads in stores.
[3144.04s -> 3146.04s]  So, just imagine there's a whole bunch of different types
[3146.04s -> 3150.04s]  of SIMD operations that the thing can run.
[3150.04s -> 3153.04s]  And this is actually called like an SN sub-core.
[3153.04s -> 3155.04s]  That's their terminology.
[3155.04s -> 3162.04s]  Okay, so these are scaler registers for one thread.
[3162.04s -> 3164.04s]  Now remember we talked about SIMD on a CPU.
[3164.04s -> 3166.04s]  We said vector registers.
[3166.04s -> 3168.04s]  So, these are not vector registers.
[3168.04s -> 3170.04s]  These are scaler registers.
[3170.04s -> 3172.04s]  The execution context of an NVIDIA thread
[3172.04s -> 3175.04s]  is a bunch of scaler registers.
[3175.04s -> 3178.04s]  Here are scaler registers for thread two.
[3178.04s -> 3182.04s]  So, two different execution contexts.
[3182.04s -> 3184.04s]  Now, here's the one major difference
[3184.04s -> 3188.04s]  between how SIMD execution works on CPUs
[3188.04s -> 3192.04s]  and how SIMD execution pops up on GPUs.
[3192.04s -> 3196.04s]  So, on CPUs, if we had a thread and it ran SIMD,
[3196.04s -> 3199.04s]  the compiler generated SIMD instructions.
[3199.04s -> 3204.04s]  On GPUs, because it's SPMD interface to the hardware,
[3204.04s -> 3210.04s]  the hardware knows that it's running N copies of this program.
[3211.04s -> 3214.04s]  So, what it's going to do is that every single thread here
[3214.04s -> 3216.04s]  has its own program counter.
[3216.04s -> 3218.04s]  It's one of the registers.
[3218.04s -> 3222.04s]  In the event that 32 consecutive threads
[3222.04s -> 3225.04s]  are on the same program counter,
[3225.04s -> 3228.04s]  that same instruction is going to get executed
[3228.04s -> 3231.04s]  in SIMD by all the GPUs.
[3231.04s -> 3233.04s]  So, it's all the same idea.
[3233.04s -> 3236.04s]  There's just a small implementation detail difference.
[3236.04s -> 3241.04s]  On CPUs, the compiler generates that SIMD instruction.
[3241.04s -> 3244.04s]  On GPUs, I have all of these CUDA threads
[3244.04s -> 3246.04s]  that are all running the same program.
[3246.04s -> 3249.04s]  If they all happen to be in the same place,
[3249.04s -> 3250.04s]  the GPU goes,
[3250.04s -> 3254.04s]  oh, let's run them with my SIMD unit all at once.
[3254.04s -> 3256.04s]  So, this is called implicit SIMD.
[3256.04s -> 3258.04s]  But at the end of the day, it's the same idea.
[3258.04s -> 3260.04s]  And one way you can think about it
[3260.04s -> 3262.04s]  is you can think about it as,
[3262.04s -> 3266.04s]  up until very, very recently, especially in this older GPU,
[3266.04s -> 3270.04s]  the only way to run multiple threads in SIMD
[3270.04s -> 3274.04s]  is if the threads with consecutive IDs
[3274.04s -> 3277.04s]  had the same operation.
[3277.04s -> 3279.04s]  Like, NVIDIA would never take a thread from here
[3279.04s -> 3281.04s]  and a thread from there and run these SIMD.
[3281.04s -> 3286.04s]  So, effectively, a group of 32 execution contexts
[3286.04s -> 3290.04s]  is kind of like a CPU thread with vector registers.
[3290.04s -> 3293.04s]  You see how it's basically the same thing.
[3293.04s -> 3296.04s]  And NVIDIA calls this concept a warp.
[3296.04s -> 3301.04s]  So, a warp is a block of execution context for 32 CUDA threads
[3301.04s -> 3305.04s]  that, when those CUDA threads are doing the same thing,
[3305.04s -> 3311.04s]  those 32 CUDA threads actually get SIMD-like execution on this hardware.
[3316.04s -> 3319.04s]  So, threads in a warp are executed in a SIMD manner
[3319.04s -> 3322.04s]  if they're at the same point in the program.
[3322.04s -> 3325.04s]  And it's up to the hardware to identify
[3325.04s -> 3327.04s]  that it's the same point in the program.
[3327.04s -> 3329.04s]  They just basically compare PCs,
[3329.04s -> 3331.04s]  and if all the PCs are the same,
[3331.04s -> 3333.04s]  across all of the threads in the warp,
[3333.04s -> 3336.04s]  boom, one clock, SIMD instruction.
[3336.04s -> 3337.04s]  Yes?
[3337.04s -> 3339.04s]  You said that it's 16A.
[3339.04s -> 3340.04s]  Yeah. Okay.
[3340.04s -> 3344.04s]  So, I just said that a warp is 32 threads,
[3344.04s -> 3348.04s]  and I've only got 16 SIMD ALUs.
[3348.04s -> 3351.04s]  Something's weird here, right?
[3351.04s -> 3353.04s]  Boom, boom.
[3353.04s -> 3357.04s]  So, it actually will only run an instruction every other clock.
[3359.04s -> 3361.04s]  So, it'll do that SIMD instruction
[3361.04s -> 3365.04s]  as 16 wide, 16 wide on consecutive clocks.
[3365.04s -> 3367.04s]  And you might be wondering,
[3367.04s -> 3369.04s]  well, doesn't that mean that instruction fetch and decode
[3369.04s -> 3372.04s]  is just idle every other clock?
[3372.04s -> 3374.04s]  It's not, because on that extra clock,
[3374.04s -> 3378.04s]  it actually starts dispatching instructions to the other things.
[3378.04s -> 3380.04s]  Right?
[3380.04s -> 3382.04s]  So, they say, oh, we can actually get by
[3382.04s -> 3384.04s]  with less instruction fetch and decode.
[3384.04s -> 3387.04s]  We'll issue a 32 wide operation every clock,
[3387.04s -> 3389.04s]  but it means we don't have to fill that unit up again
[3389.04s -> 3391.04s]  for another two clocks, which gives me some time
[3391.04s -> 3393.04s]  to issue floating point now and integer in the next clock,
[3393.04s -> 3396.04s]  and then back to floating point, and so on and so on.
[3396.04s -> 3398.04s]  So, the instructions actually look a little bit like this.
[3398.04s -> 3400.04s]  This is time and clocks.
[3400.04s -> 3404.04s]  This is like instruction 0, instruction 1, instruction 2.
[3404.04s -> 3406.04s]  And so, the actual schedule of the processor is,
[3406.04s -> 3409.04s]  let's dispatch this warp instruction.
[3409.04s -> 3411.04s]  It takes two cycles.
[3411.04s -> 3413.04s]  Let's dispatch the next warp instruction
[3413.04s -> 3416.04s]  that might be I++ on all the threads, its integer,
[3416.04s -> 3419.04s]  and then two cycles later, we can dispatch back to the FP32 unit
[3419.04s -> 3421.04s]  and stuff like that.
[3421.04s -> 3424.04s]  This is all really low-level implementation details.
[3424.04s -> 3426.04s]  It doesn't matter the course, but I thought you might find it cool.
[3426.04s -> 3428.04s]  Are these schedulers intelligent enough
[3428.04s -> 3431.04s]  so this sort of is doing instruction decode
[3431.04s -> 3434.04s]  and figuring out you have n number of units
[3434.04s -> 3436.04s]  and having an execution?
[3436.04s -> 3439.04s]  Like, it has a buffer, right, to send me through that?
[3439.04s -> 3440.04s]  Yeah, yeah, for sure.
[3440.04s -> 3442.04s]  I mean, this is your standard and substance
[3442.04s -> 3444.04s]  out-of-order pipeline processor.
[3444.04s -> 3445.04s]  Yeah, yeah.
[3445.04s -> 3447.04s]  I didn't expect that to be a CPU processor.
[3447.04s -> 3448.04s]  It's actually pretty simple.
[3448.04s -> 3450.04s]  It's not being that dynamic.
[3450.04s -> 3452.04s]  It's just saying, when I issue a floating point instruction,
[3452.04s -> 3454.04s]  I know I can issue the next one in two cycles.
[3454.04s -> 3455.04s]  It's not that hard.
[3455.04s -> 3458.04s]  Yeah, it's not nearly CPU-level sophistication.
[3458.04s -> 3461.04s]  And so those different banks are its floating-point ALU,
[3461.04s -> 3464.04s]  its 8-bit integer, its 32-bit integer, its load store,
[3464.04s -> 3465.04s]  its transcendental math.
[3465.04s -> 3467.04s]  So basically that fetch and decode unit
[3467.04s -> 3471.04s]  is just going down the program, the CUDA program,
[3471.04s -> 3474.04s]  and going, hmm, look, this block of 32 threads
[3474.04s -> 3476.04s]  needs to run a floating point.
[3476.04s -> 3478.04s]  OK, this block of 32 threads needs to run integer.
[3478.04s -> 3479.04s]  OK.
[3479.04s -> 3480.04s]  Yeah.
[3480.04s -> 3481.04s]  So that's just how it works,
[3481.04s -> 3482.04s]  but the details are not super important.
[3482.04s -> 3483.04s]  Yeah.
[3483.04s -> 3488.04s]  Is there a full PC or 108 PCs?
[3488.04s -> 3491.04s]  There's every thread has its own PC.
[3491.04s -> 3495.04s]  Every thread has its own PC.
[3495.04s -> 3497.04s]  But you're only going to get SIMD
[3497.04s -> 3500.04s]  if that PC is the same for everything in the warp.
[3500.04s -> 3505.04s]  So it's effectively the same as if there were only four PCs.
[3505.04s -> 3506.04s]  Effectively.
[3506.04s -> 3509.04s]  But there are a few things NVIDIA could do
[3509.04s -> 3511.04s]  with this abstraction, right?
[3511.04s -> 3514.04s]  They could reorganize the threads
[3514.04s -> 3519.04s]  to get better SIMD coherence for you if they wanted to.
[3519.04s -> 3521.04s]  They only very recently,
[3521.04s -> 3525.04s]  only very recently under some very limited conditions,
[3525.04s -> 3527.04s]  they'll actually do that.
[3527.04s -> 3528.04s]  Right?
[3528.04s -> 3530.04s]  So they're, in some sense,
[3530.04s -> 3534.04s]  99% of the time behaves like a 32-wide SIMD machine
[3534.04s -> 3536.04s]  with four unique PCs.
[3536.04s -> 3538.04s]  And you can think about, like, as a warp
[3538.04s -> 3540.04s]  is basically like a conventional thread
[3540.04s -> 3541.04s]  with vector instructions.
[3541.04s -> 3543.04s]  But that's actually not true.
[3543.04s -> 3547.04s]  There really are a different PC for every CUDA thread,
[3547.04s -> 3549.04s]  and this chip is dynamically checking
[3549.04s -> 3551.04s]  to make sure the PCs are all the same
[3551.04s -> 3553.04s]  before it actually uses the vector instruction.
[3553.04s -> 3557.04s]  Can you, I mean, where that warp came from?
[3557.04s -> 3560.04s]  Oh, like in this diagram.
[3560.04s -> 3563.04s]  I have execution context for,
[3563.04s -> 3565.04s]  oh, sorry, I expanded myself now
[3565.04s -> 3570.04s]  to 60 warps worth of execution.
[3570.04s -> 3572.04s]  That's a lot of execution.
[3572.04s -> 3574.04s]  But no, that's actually true.
[3574.04s -> 3578.04s]  But let me just go back to this diagram.
[3578.04s -> 3579.04s]  This will be better.
[3579.04s -> 3580.04s]  Hold on.
[3580.04s -> 3581.04s]  Oh, shoot.
[3581.04s -> 3582.04s]  Okay, there's a reason why I'm doing that.
[3582.04s -> 3584.04s]  I'll tell you in one second.
[3584.04s -> 3588.04s]  But really there's 16 of these warps.
[3588.04s -> 3593.04s]  So there's 16 times 32 execution context here.
[3593.04s -> 3596.04s]  And that's how many PCs there are.
[3596.04s -> 3599.04s]  But there can only be one of those
[3599.04s -> 3601.04s]  uniquely executing at once.
[3601.04s -> 3602.04s]  Yeah.
[3602.04s -> 3605.04s]  So a warp is like a collection of threads.
[3605.04s -> 3607.04s]  A warp is a hardware implementation detail.
[3607.04s -> 3609.04s]  It's a collection of threads.
[3609.04s -> 3612.04s]  So how is a warp a thread block?
[3612.04s -> 3614.04s]  Or like is there any relationship?
[3614.04s -> 3616.04s]  Because a thread block is part of the programming model.
[3616.04s -> 3618.04s]  A warp you never see.
[3618.04s -> 3621.04s]  All you saw is you created 128 CUDA threads,
[3621.04s -> 3625.04s]  and NVIDIA is going to use four warps of execution context to actually run it.
[3625.04s -> 3627.04s]  Okay, let me put it all together.
[3627.04s -> 3629.04s]  Again, we're not going to do any of this stuff on the exam.
[3629.04s -> 3631.04s]  I just think it's kind of fun to nerd out on this a little bit.
[3631.04s -> 3638.04s]  The actual core of this chip is this thing replicated four times.
[3638.04s -> 3643.04s]  Okay, so now this is why I was numbering my execution context this way.
[3643.04s -> 3647.04s]  So there's actually this core, this big core,
[3647.04s -> 3652.04s]  actually has 64 warps worth of execution context.
[3652.04s -> 3657.04s]  So that's 2,000 CUDA threads, right? 64 times 32.
[3657.04s -> 3662.04s]  Now it's got four different banks of these vector ALUs.
[3662.04s -> 3666.04s]  And it's got four different instruction fetch and decodes.
[3666.04s -> 3668.04s]  So there's all one thing.
[3668.04s -> 3672.04s]  And it's got like shared memory attached to all of this.
[3672.04s -> 3676.04s]  So the way it actually works, you can think about it as
[3676.04s -> 3682.04s]  we have super scaler four wide and every clock, every fetch and decode
[3682.04s -> 3685.04s]  tries to find a warp to run and runs it.
[3685.04s -> 3687.04s]  In practice it's a little bit limited.
[3687.04s -> 3690.04s]  That fetch and decode is only going to pick amongst these warps.
[3690.04s -> 3693.04s]  And this fetch and decode is only going to pick amongst these warps.
[3693.04s -> 3697.04s]  So you can think about this almost as like four separate cores,
[3697.04s -> 3700.04s]  but it's actually not true because all these execution contexts
[3700.04s -> 3703.04s]  share the same shared memory storage.
[3703.04s -> 3707.04s]  So you could create a thread block on this chip, to your question earlier,
[3707.04s -> 3713.04s]  that has 2,000 CUDA threads and has 128 kilobytes of shared storage
[3713.04s -> 3715.04s]  and that would occupy this thing.
[3715.04s -> 3718.04s]  And then this core, which is called an SM,
[3718.04s -> 3723.04s]  would actually do 2,000 way multithreading of those 2,000 CUDA threads.
[3723.04s -> 3727.04s]  Every clock it could actually make progress on up to four warps
[3727.04s -> 3731.04s]  that it selects from any of the 64 warps that are available to it.
[3731.04s -> 3734.04s]  There's a ton of latency hiding here.
[3734.04s -> 3736.04s]  But notice, there's no new concept here, right?
[3736.04s -> 3743.04s]  Like it's multithreading, SIMD, and actually simultaneous multithreading
[3743.04s -> 3746.04s]  and super scaler coming from the instruction sections.
[3746.04s -> 3749.04s]  Sorry, I was just going to find, you said four warps but
[3749.04s -> 3753.04s]  like four, four, four, four, so it could be 16 at a time, right?
[3753.04s -> 3756.04s]  No, no, just think about only what's on the slide.
[3756.04s -> 3761.04s]  Every fetch and decode is going to choose a warp to run.
[3761.04s -> 3763.04s]  So I can only run four warps at a time, right?
[3763.04s -> 3766.04s]  For any one clock I'm dispatching four warps.
[3766.04s -> 3771.04s]  Maybe one warp runs here, one warp runs here, one warp runs here, one warp runs here.
[3771.04s -> 3777.04s]  So it's like I can run, I can make progress on four warps at a time
[3777.04s -> 3779.04s]  but I've got 64 to choose from.
[3779.04s -> 3782.04s]  It'd almost be like a processor that can run instructions,
[3782.04s -> 3786.04s]  like an Intel processor can run vector instructions from four threads at once
[3786.04s -> 3789.04s]  and it had 64 hyper-threads to choose from.
[3789.04s -> 3792.04s]  You're not running the entire warp, right?
[3792.04s -> 3796.04s]  Because there might be some threads with the same program count for another warp.
[3796.04s -> 3802.04s]  Yeah, I'm selecting a warp and finding all the threads of the same PC and running it.
[3802.04s -> 3804.04s]  Otherwise I'm masking.
[3804.04s -> 3807.04s]  Same SIMD mask stuff, yes, you're right.
[3807.04s -> 3810.04s]  Sorry, if you're saying PC, I'm not a mister. What is PC?
[3810.04s -> 3816.04s]  PC is just a, there's one register here in any program which is the program counter
[3816.04s -> 3818.04s]  and that's like the pointer to the next instruction to run.
[3818.04s -> 3820.04s]  So the way to think of it is you look around and you look to say,
[3820.04s -> 3825.04s]  okay, this warp, like all of my different threads are all at the same point in the program
[3825.04s -> 3827.04s]  and they're all runnable.
[3827.04s -> 3830.04s]  Let's go dispatch this warp and use the SIMD ALUs to do it.
[3830.04s -> 3832.04s]  Yeah, exactly.
[3832.04s -> 3833.04s]  Okay.
[3833.04s -> 3836.04s]  I want to move on because I don't want to get too in the leads for too long here.
[3836.04s -> 3843.04s]  Can you just give us the advantage of not having the SIMD and having this warp concept,
[3843.04s -> 3846.04s]  like you can run SIMD but not all of these?
[3846.04s -> 3850.04s]  Well, it behaves exactly like the SIMD that you know, right?
[3850.04s -> 3853.04s]  If all the threads in a warp don't have the same program counter,
[3853.04s -> 3855.04s]  we're just going to run all the ones that do
[3855.04s -> 3859.04s]  and then we'll run another instruction that runs the other program counter.
[3859.04s -> 3862.04s]  So it'll behave just like you're masking from assignment two.
[3862.04s -> 3866.04s]  The only difference now is the hardware is figuring that out for you.
[3866.04s -> 3870.04s]  You are not asserting it at compile time what runs together.
[3870.04s -> 3875.04s]  So NVIDIA is leaving themselves the option to, in a future chip,
[3875.04s -> 3878.04s]  change the SIMD width to 64.
[3878.04s -> 3882.04s]  If Intel changes their SIMD width, you have to recompile your code
[3882.04s -> 3887.04s]  because your binaries say this is an AVX instruction with SIMD width 8.
[3887.04s -> 3891.04s]  They want to allow themselves to change that SIMD width in some future chip
[3891.04s -> 3895.04s]  and your code will still run because it's all just scalar instructions.
[3895.04s -> 3901.04s]  They want the ability to reorder the operations to re-improve coherence for you
[3901.04s -> 3904.04s]  under the hood if you have divergence.
[3904.04s -> 3906.04s]  And they do a little bit of that sometimes.
[3906.04s -> 3911.04s]  But for most code, if you just think about it like straightforward SIMD that you know,
[3911.04s -> 3915.04s]  you'll have a great model of how it works on a modern GPU.
[3915.04s -> 3919.04s]  You were talking about the user being able to use the box size,
[3919.04s -> 3925.04s]  but let's say across NVIDIA architectures they make some changes if the user thinks...
[3925.04s -> 3927.04s]  Correct. If you put something in your program,
[3927.04s -> 3930.04s]  you are asserting that the hardware has to handle it.
[3930.04s -> 3937.04s]  So let's say on this particular processor you could create a block size of 2048 threads
[3937.04s -> 3939.04s]  because I can fit 2048 CUDA threads here
[3939.04s -> 3943.04s]  and they can all have access to the same shared memory in the block.
[3943.04s -> 3949.04s]  Earlier GPUs had a maximum number of execution context per SM core of like 256.
[3949.04s -> 3953.04s]  So you could not have compiled, it will compile time fail.
[3953.04s -> 3955.04s]  You cannot run this program on this GPU
[3955.04s -> 3959.04s]  because your program needs 2000 threads per block
[3959.04s -> 3962.04s]  and this GPU can only support 256.
[3962.04s -> 3967.04s]  But at the time of writing the program, let's say you wrote it for the BEEX architecture,
[3967.04s -> 3971.04s]  it's not going to scale the way you just first wrote it.
[3971.04s -> 3977.04s]  If I wrote a program that used 256 threads per block
[3977.04s -> 3980.04s]  and then NVIDIA comes out with a new GPU that supports 2000,
[3980.04s -> 3986.04s]  I could change my program or I could just keep it the same
[3986.04s -> 3990.04s]  and NVIDIA might be able to run 4 blocks per core.
[3990.04s -> 3996.04s]  So yes, whenever you start manually scheduling something yourself,
[3996.04s -> 3999.04s]  you are making assumptions about what the hardware can do.
[3999.04s -> 4001.04s]  And that's what NVIDIA is saying here is,
[4001.04s -> 4004.04s]  we prefer you not to manually schedule the SIMD.
[4004.04s -> 4007.04s]  Let us do it because we might change the block size.
[4007.04s -> 4009.04s]  We might want to reorder things for you
[4009.04s -> 4012.04s]  and we'll do it better than you will sometimes.
[4012.04s -> 4014.04s]  Yeah, cool.
[4014.04s -> 4019.04s]  Okay, so to wrap this section up, let's go back to this code.
[4019.04s -> 4022.04s]  Remember this is something that has 128 threads per block
[4022.04s -> 4025.04s]  and needs 512 bytes of storage to run.
[4025.04s -> 4029.04s]  So when CUDA says, oh, I need to run this thing on a core,
[4029.04s -> 4031.04s]  it will take this thread block,
[4031.04s -> 4034.04s]  allocate 512 bytes out of the shared memory here
[4034.04s -> 4040.04s]  and allocate how many execution contexts?
[4040.04s -> 4047.04s]  128 threads, which actually here ends up being 4 warps worth of execution context.
[4047.04s -> 4050.04s]  And I intelligently allocated them across the slices
[4050.04s -> 4052.04s]  so that they can kind of run concurrently.
[4053.04s -> 4056.04s]  Now this diagram here, what you're seeing,
[4056.04s -> 4062.04s]  is what NVIDIA calls the streaming multiprocessor, the SM.
[4062.04s -> 4067.04s]  And this is the block that's replicated 80 times on the V100.
[4070.04s -> 4072.04s]  And so if you just do the math,
[4072.04s -> 4075.04s]  there's a 1.2 GHz clock, I think at normal clock speed.
[4075.04s -> 4077.04s]  There's 80 of these things per chip.
[4077.04s -> 4081.04s]  Each of these 80 has kind of these four columns
[4081.04s -> 4086.04s]  and those columns have 16 wide vector ALUs for floating point math.
[4086.04s -> 4088.04s]  So you multiply all those together,
[4088.04s -> 4092.04s]  that's 12.7 teraflops of performance for floating point math.
[4092.04s -> 4101.04s]  And if you do 80 times 64 warps per SM times 32 threads per warp,
[4101.04s -> 4105.04s]  that's 163,000 concurrent live CUDA threads on the chip at once.
[4105.04s -> 4109.04s]  So your Intel processor has support for two hyper-threads.
[4109.04s -> 4113.04s]  This thing has support for 163,000 CUDA threads.
[4113.04s -> 4115.04s]  They can't all execute at the same time,
[4115.04s -> 4118.04s]  but they're all there on the chip at the same time.
[4118.04s -> 4119.04s]  Yes?
[4119.04s -> 4122.04s]  Here's how you mentioned that each thread's memory
[4122.04s -> 4125.04s]  will get allocated in the shared memory, right?
[4125.04s -> 4127.04s]  Those shared variables, right?
[4127.04s -> 4130.04s]  So remember this program had that shared variable,
[4130.04s -> 4133.04s]  shared float support, says this thread block.
[4133.04s -> 4136.04s]  So the shared memory is the place where you start
[4136.04s -> 4139.04s]  asserting actual scheduling details as the programmer.
[4139.04s -> 4141.04s]  So there's a compromise, right?
[4141.04s -> 4144.04s]  Like CUDA said, you should just tell us how many blocks to run
[4144.04s -> 4146.04s]  and we'll run them however we wish.
[4146.04s -> 4149.04s]  But they said memory is super important,
[4149.04s -> 4154.04s]  so we're going to allow you to assert that I need 512 bytes of storage.
[4154.04s -> 4156.04s]  And what NVIDIA is going to do,
[4156.04s -> 4158.04s]  what we're getting to that,
[4158.04s -> 4160.04s]  is if we run this thing,
[4160.04s -> 4164.04s]  to run a thread block it's going to choose warp's execution context
[4164.04s -> 4166.04s]  on the same core processor.
[4166.04s -> 4168.04s]  It's going to allocate that storage
[4168.04s -> 4171.04s]  so all those execution contexts can read and write from it.
[4171.04s -> 4174.04s]  And since all these threads are kind of there in the same core,
[4174.04s -> 4177.04s]  things like a barrier between them will be very fast.
[4177.04s -> 4181.04s]  Whereas if I happen to take some of the threads from this thread block
[4181.04s -> 4185.04s]  and put them somewhere else on this machine over here,
[4185.04s -> 4189.04s]  imagine a barrier would be communication across all of these things.
[4189.04s -> 4192.04s]  So the thread block is the granularity of synchronization
[4192.04s -> 4195.04s]  and data locality when you're writing a program.
[4195.04s -> 4200.04s]  So that means that the amount of data memory
[4200.04s -> 4202.04s]  that has to be done at compile time,
[4202.04s -> 4203.04s]  like you forget the memory?
[4203.04s -> 4204.04s]  Sure, yes.
[4204.04s -> 4206.04s]  And I'm giving you a simplistic version of CUDA,
[4206.04s -> 4209.04s]  like if you go to the modern CUDA docs that are going to say,
[4209.04s -> 4212.04s]  sure, you can call malloc on shared memory also,
[4212.04s -> 4216.04s]  but then there's going to be some, you know, it gets harder.
[4216.04s -> 4219.04s]  So I want to give the simple version first, right?
[4219.04s -> 4221.04s]  So let's talk about running this thing.
[4221.04s -> 4223.04s]  We create this, you know, like we write a program,
[4223.04s -> 4226.04s]  we have all of these thread blocks, we need 512 bytes of storage,
[4226.04s -> 4229.04s]  and let's imagine a very simple GPU, you know,
[4229.04s -> 4231.04s]  not the thing I just showed you, but a very simple GPU,
[4231.04s -> 4233.04s]  which can run one instruction per clock,
[4233.04s -> 4237.04s]  actually has 32 wide SIMDs, so I drew it out,
[4237.04s -> 4243.04s]  and has execution context per SM core for 384 CUDA threads,
[4243.04s -> 4246.04s]  which is 12 ORPs, okay?
[4246.04s -> 4247.04s]  So what do you think is going to happen?
[4247.04s -> 4250.04s]  So we run this thing, we say, okay, we need 8,000 thread blocks,
[4250.04s -> 4254.04s]  you're implementing thread pools, you know how it works,
[4254.04s -> 4256.04s]  you're going to say, I need to execute this task,
[4256.04s -> 4259.04s]  this convolve function, my arguments are this,
[4259.04s -> 4264.04s]  and the number of blocks are 1,000, okay?
[4264.04s -> 4267.04s]  And my GPU work scheduler, just like probably,
[4267.04s -> 4269.04s]  very much like your implementation says, well,
[4269.04s -> 4273.04s]  first thing I'm going to do is I'm going to allocate 128 execution context over here,
[4273.04s -> 4275.04s]  I'm going to allocate 512 bytes of shared memory,
[4275.04s -> 4278.04s]  and I'm going to increment my next pointer to one,
[4278.04s -> 4282.04s]  because I've already dispatched one of these thread blocks.
[4282.04s -> 4285.04s]  Okay, what happens next?
[4285.04s -> 4288.04s]  Scheduler, can it schedule more work?
[4288.04s -> 4291.04s]  Absolutely, so where do we put it?
[4291.04s -> 4293.04s]  Probably should put it on the other core, right?
[4293.04s -> 4297.04s]  So the next thread block comes out, we allocate 128 thread blocks,
[4297.04s -> 4300.04s]  we grab 512 bytes of storage, we're good.
[4300.04s -> 4306.04s]  Now the next pointer up to the next thread block is 2,
[4306.04s -> 4308.04s]  should we keep going?
[4308.04s -> 4310.04s]  Now we can, let's schedule the next block.
[4310.04s -> 4312.04s]  So now I'll notice multiple blocks are being scheduled on the same processor,
[4312.04s -> 4314.04s]  I've got a ton of execution context.
[4314.04s -> 4317.04s]  So now I grabbed another 128 execution,
[4317.04s -> 4320.04s]  I grabbed another 512 bytes,
[4320.04s -> 4323.04s]  let me go ahead and fill this thing up with the fourth thread block,
[4323.04s -> 4327.04s]  and now the fifth thread block cannot be scheduled right now,
[4327.04s -> 4329.04s]  I do not have the resources.
[4329.04s -> 4333.04s]  Why do I not have, I actually have the execution context,
[4333.04s -> 4335.04s]  but I actually didn't have the shared memory
[4335.04s -> 4338.04s]  in this example I only had 1.5k of shared memory.
[4338.04s -> 4344.04s]  So NVIDIA is going to not schedule things that it knows it can't fit.
[4344.04s -> 4346.04s]  So we'll just wait.
[4346.04s -> 4349.04s]  At some point one of those thread blocks finishes.
[4349.04s -> 4352.04s]  When that thread block finishes, some resources are available,
[4352.04s -> 4355.04s]  and now let's schedule thread block 4.
[4355.04s -> 4358.04s]  And now notice that since this is SPMD,
[4358.04s -> 4361.04s]  and all of these thread blocks use the same amount of resources,
[4361.04s -> 4364.04s]  it's pretty easy to just, whenever someone finishes,
[4364.04s -> 4366.04s]  the exact amount of resources are free to start a new thing.
[4366.04s -> 4369.04s]  It's like kind of very homogeneous resource requirements.
[4369.04s -> 4372.04s]  You don't have to solve an allocation problem or anything like that.
[4372.04s -> 4375.04s]  So that's why it's nice to know the resources up front.
[4375.04s -> 4380.04s]  So things will just keep going and going and going and going and going.
[4380.04s -> 4384.04s]  And that's what the NVIDIA GPU is going to do.
[4384.04s -> 4389.04s]  So let's check our understanding just a little bit in the last 4 or 5 minutes of class.
[4389.04s -> 4392.04s]  So if you understand the next 3 slides,
[4392.04s -> 4395.04s]  and you are really thinking through step by step,
[4395.04s -> 4397.04s]  you are in your head like going through,
[4397.04s -> 4400.04s]  okay, I can just play out in my head what the implementation is doing.
[4400.04s -> 4403.04s]  Okay, so here's an example.
[4403.04s -> 4407.04s]  Okay, so now I made a core, and this is not the full thing.
[4407.04s -> 4412.04s]  Notice that let's just say it only has 128 execution contexts.
[4412.04s -> 4415.04s]  So 4 warps here, I made a tiny little core.
[4415.04s -> 4421.04s]  And I have my thread block with 256 CUDA threads.
[4421.04s -> 4425.04s]  So I have a core that only has execution context for 128.
[4425.04s -> 4432.04s]  I have a program that I wrote, 256.
[4432.04s -> 4437.04s]  Why can't, so NVIDIA is going to say, sorry, you can't run this.
[4437.04s -> 4442.04s]  But why can't it, why doesn't it just run the first 128 threads in the thread block,
[4442.04s -> 4446.04s]  wait until they finish, and then run the next 128 threads?
[4446.04s -> 4451.04s]  I think it's because the programmer has specified that the data locality,
[4451.04s -> 4456.04s]  it's new more than the full 256 CUDA.
[4456.04s -> 4460.04s]  Okay, and that's sort of true, because that's the rules of the programming model.
[4460.04s -> 4463.04s]  But why are the rules of the programming model this way?
[4463.04s -> 4467.04s]  Why don't they just say you can ask for as many threads per block as you want,
[4467.04s -> 4468.04s]  and we're just going to run them,
[4468.04s -> 4471.04s]  and we're going to run the first half of the threads to completion,
[4471.04s -> 4472.04s]  and then we'll run the next half.
[4472.04s -> 4475.04s]  I'm still going to give you your shared memory.
[4475.04s -> 4479.04s]  Because it's based on what you know about the problem,
[4479.04s -> 4482.04s]  it's going to run more efficiently, like that way.
[4482.04s -> 4483.04s]  That's true.
[4483.04s -> 4486.04s]  I mean, what you're saying is that if the programmer told you to do something,
[4486.04s -> 4488.04s]  it's good in a system to do it.
[4488.04s -> 4491.04s]  But there are a lot of times when the underlying system implementer says,
[4491.04s -> 4492.04s]  I think I know better.
[4492.04s -> 4497.04s]  As long as I can do it correctly, I'm fine.
[4497.04s -> 4498.04s]  So look at this program.
[4498.04s -> 4501.04s]  It has a barrier in it.
[4501.04s -> 4506.04s]  So that barrier says that we will not proceed until all threads get here.
[4506.04s -> 4511.04s]  What if we ran 128 threads, they stall at that barrier,
[4511.04s -> 4515.04s]  they're still having those execution contexts,
[4515.04s -> 4517.04s]  and what happens?
[4517.04s -> 4518.04s]  Nothing.
[4518.04s -> 4522.04s]  They never yield them, and so the other threads never get a chance to run.
[4522.04s -> 4526.04s]  So unless you start thinking about more advanced implementations
[4526.04s -> 4529.04s]  where we're going to preempt these threads, rip them off the processor,
[4529.04s -> 4534.04s]  put the other ones on, which would be highly not performant,
[4534.04s -> 4537.04s]  this program's going to deadlock if I try and run it on this machine.
[4537.04s -> 4541.04s]  So that's why NVIDIA says, look, we're a high-performance programming system.
[4541.04s -> 4546.04s]  If the programmer decided to do something, we need to respect their wishes.
[4546.04s -> 4551.04s]  We don't want to do crazy things that might impact performance in a bad way
[4551.04s -> 4553.04s]  just to get the program to run correctly.
[4553.04s -> 4559.04s]  So standard CUDA, you should think about it as no preemption,
[4559.04s -> 4562.04s]  and things that would be magical are not done.
[4562.04s -> 4565.04s]  And so this is why CUDA will reject this program.
[4565.04s -> 4569.04s]  The idea is that the threads in a thread block are not just SPMD threads
[4569.04s -> 4572.04s]  but running concurrently at the same time,
[4572.04s -> 4574.04s]  live at the same time on the same SM,
[4574.04s -> 4577.04s]  because you may do things like want them to cooperate with barriers
[4577.04s -> 4579.04s]  or atomics and stuff like that.
[4579.04s -> 4582.04s]  So this thing is busted.
[4582.04s -> 4584.04s]  All right, two more things.
[4584.04s -> 4586.04s]  So this is just a review slide.
[4586.04s -> 4587.04s]  Let's think about this one.
[4587.04s -> 4590.04s]  Let's think about a CUDA program that creates a histogram.
[4590.04s -> 4594.04s]  I create a bunch of threads, and all of them,
[4594.04s -> 4596.04s]  in whatever thread blocks they're in,
[4596.04s -> 4600.04s]  just compute some value and then increment a bin.
[4600.04s -> 4605.04s]  And CUDA has the ability to have atomic operations like atomic add.
[4605.04s -> 4607.04s]  So imagine all of us are CUDA threads.
[4607.04s -> 4610.04s]  We all grab a variable, compute some function,
[4610.04s -> 4612.04s]  and increment bin 42.
[4612.04s -> 4618.04s]  So you go to bin 42 and do an atomic safe update.
[4618.04s -> 4626.04s]  Is this a valid CUDA program?
[4626.04s -> 4631.04s]  I kind of have threads in different thread blocks touching the same memory.
[4631.04s -> 4635.04s]  How can you synchronize between the different thread blocks and throw in different SMs?
[4635.04s -> 4641.04s]  So imagine that this atomic add is atomic out to global memory.
[4641.04s -> 4643.04s]  This is OK.
[4643.04s -> 4646.04s]  I've done nothing here that makes any assumptions at all
[4646.04s -> 4649.04s]  about how CUDA runs these thread blocks.
[4649.04s -> 4651.04s]  It's just going to run them whenever it wants,
[4651.04s -> 4655.04s]  and they might contend for an atomic, but so what?
[4655.04s -> 4657.04s]  So this is OK.
[4657.04s -> 4660.04s]  I'm treating the thread block as I'm just going to launch as many as I need
[4660.04s -> 4663.04s]  and execute them in whatever order you want.
[4663.04s -> 4665.04s]  But now think about this one.
[4665.04s -> 4668.04s]  Imagine a piece of code where I launch two thread blocks,
[4668.04s -> 4674.04s]  and one of them says, OK, update this variable memory,
[4674.04s -> 4680.04s]  and then the other thread block says wait until that variable is updated.
[4680.04s -> 4682.04s]  So one thread block says I did something.
[4682.04s -> 4684.04s]  The other thread block waits for it.
[4684.04s -> 4690.04s]  Imagine running this code on a GPU that only can run one thread block at a time.
[4690.04s -> 4695.04s]  If it runs thread block one first, thread block zero never gets to go,
[4695.04s -> 4697.04s]  and thread block one is waiting all the time.
[4697.04s -> 4699.04s]  So you see how there's a difference.
[4699.04s -> 4701.04s]  Communication is possible,
[4701.04s -> 4705.04s]  but you cannot make assumptions about the order in which stuff is done.
[4705.04s -> 4709.04s]  So thread blocks need to be, they can interact,
[4709.04s -> 4712.04s]  but they cannot make assumptions about order.
[4712.04s -> 4714.04s]  Whereas threads, inside a thread block,
[4714.04s -> 4716.04s]  you can assume that they're all running at the same time
[4716.04s -> 4718.04s]  and you can use barriers and stuff like that.
[4718.04s -> 4721.04s]  OK, I should stop here so you can go, but feel free to ask any questions.
