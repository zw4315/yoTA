# Detected language: en (p=1.00)

[0.00s -> 12.76s]  Alright, so let's get started. I'd like to bring this back up from last time really
[12.76s -> 17.68s]  quick. Remember we closed last Thursday, we were running this, we basically walked
[17.68s -> 22.32s]  through a case study of optimizing a program. If you remember the program, it was this
[22.32s -> 26.92s]  numerical program where what we did was we divided the program into various phases
[26.92s -> 32.52s]  and I'll just page it back in for you. I said on phase one, we're going to update all of
[32.52s -> 38.50s]  these black cells in a grid and we updated every black cell by looking at the neighbors.
[38.50s -> 42.62s]  And then we said, hold up, wait till everybody does their work, everybody's got to check
[42.62s -> 47.66s]  to see if conversions were supposed to stop and then we're going to repeat back, repeat
[47.66s -> 50.96s]  the next iteration if we haven't converged. And the last thing that all of you did in
[50.96s -> 57.52s]  class, like our last three or four minutes of class last time, was you all told me why
[57.52s -> 64.60s]  there were three barriers in this code. And it's just a quick review, like remember all of the
[64.60s -> 71.96s]  threads computed some elements and computed how much of their, how much did they change the
[71.96s -> 77.04s]  final output. And then all of those threads accumulated their partial sums into a variable
[77.88s -> 84.20s]  and then if that variable exceeded some value, if we hadn't converged, all the threads just went
[84.20s -> 90.12s]  back to the top of the loop again and continued. And we had a number of barriers in there.
[90.12s -> 95.08s]  We said, well this barrier is here because all the threads need to know that all other
[95.08s -> 99.56s]  threads have accumulated into this sum so it's the final value before we check if we're going
[99.56s -> 105.24s]  to continue. That was why we had this barrier. And then we had this barrier because we were
[105.24s -> 110.28s]  like, well wait a minute here, we need to make sure that everybody has checked the actual
[110.28s -> 115.56s]  value to determine if they should continue before any thread gets to the top of the loop
[115.56s -> 120.60s]  again and resets those values. And then there was even a third barrier which says,
[120.60s -> 124.52s]  well we need to prevent anybody from going through this section of the code and actually
[124.52s -> 129.76s]  updating that value perhaps before it got reset or something like that or checked by someone
[129.88s -> 135.88s]  So there were these three barriers. And I asked, did anybody, and maybe you posted this on the website,
[135.88s -> 137.96s]  did anybody come up with a way to do this with one?
[143.56s -> 147.72s]  Nobody puzzled over this. This would be a good thing to talk about. So let me just give you like,
[148.52s -> 151.56s]  what is the reason why we have to keep waiting here?
[154.04s -> 157.88s]  There's a variable that we keep checking, right? And what's that variable?
[158.88s -> 167.20s]  That's mydiff. And what does mydiff do? Mydiff is a single variable that holds the value of the
[167.20s -> 173.12s]  amount of change per iteration. And the problem that we have is that this is code with multiple
[173.12s -> 182.16s]  iterations and that mydiff value is being reused over and over again to represent the change
[182.80s -> 189.12s]  in each of these iterations. So the problem is that all these threads have to get out of iteration
[189.12s -> 196.40s]  i before anybody can start doing any work with mydiff on iteration i plus one, right?
[197.92s -> 203.28s]  So why not just have different variables for all the different iterations?
[205.84s -> 209.36s]  Like if all we do is have a different variable, if we had mydiff for iteration i,
[210.32s -> 215.04s]  everybody in iteration i would just accumulate into the iteration i version of mydiff,
[216.08s -> 220.24s]  check that, and then you don't have to wait at all because when you move on to the next iteration
[220.24s -> 225.20s]  you're starting with a different variable, or you're reading and writing to a different variable.
[225.20s -> 229.68s]  So this was a case where I have kind of falsely created a dependency in the program
[229.68s -> 234.32s]  that didn't need to be there because I was reusing one variable for a couple of different
[234.32s -> 241.20s]  purposes, you know, every iteration. So a very simple solution would be to go to one barrier here
[244.48s -> 247.44s]  and instead take this diff variable and make it an array.
[249.52s -> 254.88s]  Now conceptually what I've done is I could have made a different array of size, you know,
[254.88s -> 259.76s]  diff is an array of size number of iterations, but really all I need is I needed three
[259.84s -> 265.44s]  different copies of diff. I need to maintain a diff from the last iteration, the current iteration,
[265.44s -> 270.00s]  and the next iteration because all threads are only going to be kind of in one of those three
[270.00s -> 275.36s]  iterations. They're either finishing up from last time, working on the present, or trying to
[275.36s -> 280.72s]  move forward to the next. So what I did is I just said oh I'm just going to duplicate that variable
[281.28s -> 287.76s]  and make sure that the accumulation goes into the right copy of the variable every time. So
[287.76s -> 293.68s]  this is kind of like the same idea as what we did right here where we we had that one single
[293.68s -> 299.44s]  variable my diff and remember we made a local copy of it to accumulate our partial sum.
[300.96s -> 304.40s]  So that was again the same technique, it's like we had one copy of a variable and everybody
[304.40s -> 310.16s]  was synchronizing on it. So we said conceptually everybody can write to a different copy of the
[310.16s -> 316.48s]  variable and we can get things all synced up at the end. So that was just, you know,
[316.72s -> 320.64s]  it's interesting it's like it's actually the same trick just used in a different context.
[324.56s -> 325.06s]  Okay.
[329.84s -> 333.60s]  So at the beginning of week three let's finally start talking about some
[333.60s -> 337.04s]  program optimization techniques and the way this work is going to go,
[337.60s -> 341.20s]  the way this week is going to go is today I want to talk about
[341.92s -> 348.48s]  assignment or ideas for how to figure out what thread or what worker does each piece of work.
[349.04s -> 353.52s]  So today is going to be about workload assignment and scheduling.
[354.64s -> 359.60s]  And then Thursday is going to be about doing that scheduling in a manner
[360.48s -> 366.24s]  that reduces communication costs, reduces stalls due to memory and stuff like that. So today is
[366.24s -> 369.92s]  going to be about, we're not going to think much about memory at all, we're just going to
[369.92s -> 374.40s]  think about synchronizing a bunch of workers and making sure everybody has a good workload
[374.40s -> 379.20s]  balance. Thursday it's going to be about let's make sure we're communicating and accessing
[379.20s -> 384.16s]  memory efficiently. Okay. So here's where we're going with this. Okay. So and
[386.56s -> 393.20s]  yeah. My first disclaimer before we get into any of the work this week or and even next week,
[393.20s -> 398.72s]  we're going to now start telling you about strategies for scheduling programs efficiently.
[399.60s -> 404.48s]  And I cannot overemphasize and you know, case one being your assignments and in particular,
[404.48s -> 409.36s]  this is going to be true in assignment three, is people go, oh, I heard about these complex
[410.56s -> 414.56s]  all these ideas in lecture. I'm going to think about the problem that they give me on my
[414.56s -> 419.36s]  programming assignments and I'm going to figure out how to apply some of the more advanced
[419.36s -> 426.72s]  techniques. And without question, I want you to take a different approach. I want you to take
[426.72s -> 432.72s]  the simplest possible approach that gets the program working. And then the simplest possible
[432.72s -> 438.16s]  approach to paralyze it. And I want you to measure your performance. I want you to do the
[438.16s -> 443.20s]  types of things that we forced you to do on assignment one. Put in some timers. Figure out
[443.20s -> 448.40s]  which thread is doing the most work. Those kinds of things. And then only if performance is bad,
[449.60s -> 454.56s]  do you start doing something more sophisticated. Okay. And this is the case, this happens
[454.56s -> 459.36s]  every year to I would say about 10% of students in the class. They read the handout. They read
[459.36s -> 464.40s]  the lectures. They get on the whiteboard and they come up with a pretty sophisticated scheme
[464.40s -> 469.20s]  to solve the assignment. And they do that for multiple days. And then they say, you know,
[469.20s -> 471.60s]  three or four days before the assignment is due, they're like, we're just going to code it
[471.60s -> 475.76s]  up. As soon as we get it to work, it'll be great. And it turns out often that their
[475.76s -> 482.48s]  complex scheme is slower than a simple scheme. Okay. So always do the simple thing first
[483.28s -> 489.28s]  and use measurements, use data to govern what you do next. Okay. So I'm going to start with
[489.28s -> 494.88s]  the simplest thing here. And the next sequence of slides is going to all be about
[495.68s -> 500.48s]  balancing work. And the name of the game, when you need to balance work, here's an example where
[500.48s -> 505.76s]  I have four processor cores. And for whatever reason, one of these cores, processor four here,
[505.76s -> 510.64s]  core four, does a lot more work than the others. And the reason why that limits speed
[510.64s -> 516.56s]  up is we have effectively serialized parts of our program. A large part of execution time,
[516.56s -> 521.28s]  only one of these workers is doing anything. It is effectively running as a serial program.
[523.68s -> 528.32s]  So if you have a bunch of work and you have a bunch of cores or threads, whatnot,
[528.88s -> 536.72s]  what is probably the simplest way to divide work onto those workers? Or in other words,
[536.72s -> 545.36s]  where do we start on program one of assignment one? Just give everybody an equal. I'm not even
[545.36s -> 549.68s]  going to think about what's contiguous or not. But we did something like this. We said,
[549.68s -> 555.04s]  we're just going to break up the work completely evenly. Now we need to kind of
[555.04s -> 561.28s]  dig in a little bit on what we mean by even. So on program one, evenly meant
[562.16s -> 572.40s]  same number of pixels. And so here are a couple of different strategies for breaking that work up.
[572.40s -> 579.52s]  So this was the original version of assignment one. If these were actually pixel size rows,
[579.52s -> 582.08s]  you might have ended up with a solution that looked a little bit like this.
[583.76s -> 589.28s]  If I didn't make the constraint that said your solution should probably work for any viewpoint,
[590.08s -> 596.32s]  you might have done something like this. Where you just kind of manually said okay, I kind of know
[596.96s -> 602.96s]  what the expense of every pixel was and therefore I divided up the work equally.
[603.92s -> 608.48s]  Now in these diagrams actually, how do I know how much work is associated with the pixel?
[609.04s -> 613.04s]  One of the reasons why we use this as a starting example is that it's pretty clear.
[614.08s -> 618.24s]  Yeah, the brightness of the pixel is kind of how expensive it is to compute.
[618.24s -> 623.92s]  So the regions down here are not a lot of work. The regions in here are a lot of work.
[623.92s -> 629.68s]  And as a result, this scheme yielded a significant amount of workload and balance.
[631.36s -> 638.56s]  Now some of you decided to come up with a solution where you interleaved the assignment
[638.56s -> 648.32s]  of rows of pixels to the threads. And why is that actually an okay solution?
[648.96s -> 652.88s]  Because each of those rows has different amounts of work for sure, right?
[653.68s -> 656.72s]  Different regions of the image have very different amounts of work.
[657.28s -> 659.36s]  So why did that actually work out pretty well?
[659.36s -> 663.76s]  Yeah.
[663.76s -> 670.00s]  Because on average, like one thread getting silent would most of the work it needs because
[670.00s -> 671.28s]  they're interleaved.
[671.28s -> 675.60s]  Yeah, and we're kind of assuming that like rows right next to each other kind of have
[675.60s -> 679.76s]  the same amount of work because there's kind of some continuity to the figure.
[679.76s -> 684.64s]  So if every like really local piece of the image gets divided up into four rows
[684.64s -> 688.48s]  and go to different processors, that means all the processors on average
[688.48s -> 690.96s]  will end up having a reasonable amount of work.
[690.96s -> 696.96s]  Now you could imagine like a different workload, well that would have been a terrible strategy.
[697.60s -> 701.36s]  Like for example, imagine it was like all white at the top
[701.36s -> 704.08s]  and then slowly went down this way or something like that.
[704.08s -> 706.00s]  Like the top half was a triangle.
[706.56s -> 711.04s]  That would mean that processor or thread one would always get a little bit more work than thread
[711.04s -> 713.20s]  two which would always get a little bit more work than thread three.
[713.20s -> 715.76s]  But in this case, it actually turned out to be pretty good.
[716.32s -> 721.60s]  So what are some situations where you think you can pull off this,
[721.60s -> 724.56s]  you know, and the title of the slide is static assignment.
[724.56s -> 729.76s]  In other words, you can go without ever like actually looking at the execution characteristics
[729.76s -> 732.72s]  of the program, I can kind of just say if you give me the size of the image,
[732.72s -> 734.32s]  if you give me the number of workers I have,
[734.88s -> 737.20s]  I'll give you a scheme that's always going to be pretty good.
[739.36s -> 742.32s]  So like what is, what are the properties of this that we knew about?
[743.04s -> 746.00s]  I guess you have to know what parts of the program are going to be
[746.00s -> 748.00s]  more intensive than other parts.
[748.00s -> 752.32s]  Like this would have been very easy if all pixels were the same cost, okay.
[752.32s -> 756.32s]  So you have to have some ability to predict the cost of things.
[757.12s -> 761.76s]  In this particular case, you actually can't really predict the cost of any one pixel
[762.40s -> 769.28s]  but you definitely can predict on average that on average stuff is going to be about the same.
[770.16s -> 776.24s]  So there's this notion of predictability that kind of makes this problem pretty easy, right.
[776.24s -> 779.52s]  Like you can just say, well, if I have eight threads or so many threads
[779.52s -> 784.00s]  and I have this many pixels, here's a way I can just describe the policy up front
[784.00s -> 786.96s]  for who does what and I'm going to get a pretty good work load balance.
[787.60s -> 790.00s]  I'm going to schedule this out onto the machine, right.
[790.96s -> 796.16s]  So this is like, you know, really applicable when the side, you know,
[796.72s -> 800.80s]  the cost of the chunks of work and here the cost of the work is maybe the vertical height
[800.80s -> 805.60s]  of these bars is equal. Like if I know I have 12 pieces of work and they all have equal time
[805.60s -> 812.08s]  and I have four different workers, four processors, sure, I might just divide 12 by 4
[812.08s -> 818.40s]  and I get three pieces of work per worker. Really simple, simple way of doing it.
[818.40s -> 823.68s]  And this is great because once I tell P1 and P2 like you're responsible for the first
[823.68s -> 827.76s]  quarter, you're responsible for the second quarter, these threads don't have to synchronize,
[827.76s -> 832.24s]  there's no communication, like they can compute what they have to do without knowing what anybody
[832.24s -> 838.16s]  else is doing. And that's very helpful because it minimizes communication amongst the threads.
[839.68s -> 843.84s]  Static assignment as we saw in that fractal example can also be applicable when you don't
[843.84s -> 849.28s]  know the actual cost of any one piece of work but you know that on average you're going to
[849.36s -> 855.28s]  end up with on average everything is the same, might have some variance. And if I give enough
[855.28s -> 860.80s]  jobs to every worker, if I add up the length of the dark blue bars and like plot them like
[860.80s -> 865.04s]  this and stack them, you know, all the different threads end up getting about the same amount of
[865.04s -> 870.08s]  work. So here's an example where on average everything is the same and just because there's
[870.08s -> 874.40s]  variance that doesn't mean that I don't get a good workload about balance from a simple
[875.04s -> 880.72s]  just chop it all up strategy. And that fractal was a good example.
[883.28s -> 889.44s]  There are a lot of applications, especially if you're, anything that runs for a long time,
[889.44s -> 892.80s]  like let's say you're a super computing simulation, this is a figure from a
[896.48s -> 901.20s]  turbulent simulation, what you see here in white is actually the profile of a wing.
[901.20s -> 905.60s]  So you're actually looking down at the edge of the wing, like towards the fuselage,
[905.60s -> 911.68s]  and this is a mesh that's discretized all of the the fluid flow around the wing. And so you could
[912.56s -> 916.00s]  see a super high resolution around here, that's actually because you need high resolution
[916.00s -> 922.40s]  around areas of high pressure. And the coloring suggests what cells of the mesh are processed by
[922.40s -> 927.84s]  what thread, so different colors. So in space there are very different amounts of space
[927.84s -> 933.52s]  per thread, but probably if we look carefully and counted the number of cells per thread it'd
[933.52s -> 939.28s]  probably be about the same. And you can imagine that like if we're simulating this airline,
[940.32s -> 945.52s]  this wing for like a long period of time, maybe during takeoff or landing or as the
[945.52s -> 953.68s]  wing geometry changed, the distribution might change. So it would be perfectly fine if,
[953.68s -> 960.72s]  for example, we decided to go with a static assignment of grid cells to the threads
[960.72s -> 965.52s]  at the beginning of the program. Maybe we let it run for a minute or two and then maybe
[965.52s -> 969.92s]  the pressure changes and we have to update the number of cells, and then we do another
[969.92s -> 974.64s]  assignment again. So I like the, you know, like there's this term semi-static assignment,
[974.64s -> 978.32s]  as you can kind of think about it as given the state of the world I'm going to up front
[979.28s -> 983.84s]  assign work to workers and then I'm not going to change that assignment for some amount of time.
[984.88s -> 989.04s]  So I just want people to not walk away going static assignment means it's like
[989.04s -> 992.24s]  sort of known at compile time when you write the program or something like that.
[993.60s -> 997.60s]  Another good one would be, you know, any kind of long-running machine learning computation.
[997.60s -> 1003.20s]  You might run that training for a while, understand where you have some workload imbalance,
[1003.20s -> 1008.00s]  adjust that workload imbalance, and then run again for another hour and so on and so on.
[1009.04s -> 1017.12s]  Okay, now the flip side of static assignment would be to do the assignment dynamically as we go.
[1018.00s -> 1022.88s]  So let's say that we cannot predict the amount of time that a task might take. It might be very
[1022.88s -> 1027.44s]  short, it might vary very long, so there's not really any way we can up front say, okay,
[1027.44s -> 1032.64s]  you are going to do all these and you are going to do all these. Okay, so here's an example
[1032.64s -> 1040.64s]  program. I wrote it in just kind of pseudo-code. Imagine there's a void main and then here's a
[1040.64s -> 1046.40s]  sequential program for i equals zero to n. I want to compute whether or not some number in
[1046.40s -> 1050.80s]  the input array is prime, which is something that's a little bit harder to predict how
[1050.80s -> 1056.72s]  expensive that would be, for example. And now imagine that I have a version of this code that
[1056.72s -> 1060.72s]  runs in an SPMD fashion. Not necessarily written in
[1062.80s -> 1068.96s]  ISPC but it runs in the SPMD programming model, which means however many threads I'm running in
[1068.96s -> 1074.64s]  my program, they're all running this code and they just have a different thread ID or something
[1074.64s -> 1080.00s]  like that. So imagine that in the parallel version of this code, there are n threads running
[1080.00s -> 1086.24s]  this code. And the way my code works is all the threads continue, they just do their thing,
[1086.72s -> 1094.16s]  until all elements of the array have been processed. And the key thing here is there's a counter
[1095.52s -> 1101.04s]  and there's a lock on that counter. So the counter starts at zero and counts up to n
[1101.84s -> 1107.20s]  and then every thread says, well, if I have nothing to do, grab the lock, increment the counter
[1107.76s -> 1111.28s]  and then whatever the counter's value is, I'm responsible for that number.
[1112.16s -> 1114.24s]  So first of all, let's check to make sure that
[1118.96s -> 1121.44s]  do all n elements of the array get processed?
[1123.52s -> 1129.52s]  Yes. It would be bad if two threads did the same work. Is there any way for two threads
[1129.52s -> 1133.68s]  to do the same work? No. And what ensures that that's the case?
[1133.68s -> 1143.12s]  This lock here, right? So this lock says that no one else is going to be reading that counter
[1143.12s -> 1146.80s]  while I'm incrementing it. So I'm guaranteed that I will perform that update,
[1146.80s -> 1150.88s]  increment the counter, I'll take the current number. It's like basically a ticket at the
[1150.88s -> 1155.44s]  deli counter or something like that. And then everybody coming later that looks for more
[1155.44s -> 1159.44s]  work will get us a later number. And we're all going to get unique numbers and as a result,
[1159.44s -> 1162.88s]  we're going to do all the work. And does this code terminate?
[1164.72s -> 1168.48s]  It will terminate because if a thread fails to get a number, it will quit.
[1169.92s -> 1172.24s]  And once all threads have quit, we are done.
[1174.24s -> 1178.56s]  So even though this is basically some C code with a lock and a counter,
[1178.56s -> 1183.68s]  what have I really done here? If you were trying to explain this in high level terms on
[1183.68s -> 1190.64s]  what is the work assignment strategy? Basically just whichever thread is available,
[1190.64s -> 1195.44s]  just give it work and you keep doing that. Yeah. So how does this program define the work to do?
[1197.52s -> 1202.40s]  Basically defines it by whichever thread we all can do it.
[1203.04s -> 1208.08s]  That's how it's going to be. That defines how a thread is going to get assigned work.
[1208.08s -> 1213.12s]  But what I'm looking for here is like when you read this program, first of all,
[1213.12s -> 1216.40s]  if you look at this program and talk to like your neighbor sitting right next to you, you'd say
[1216.96s -> 1223.60s]  what's the definition of the work here? The work is to compute the primality of all of these
[1223.60s -> 1231.84s]  numbers. And a single piece of work is what? Computing the primality of one number. And
[1232.56s -> 1237.84s]  how does this program define all the work we need to do? Basically it defines it via
[1238.80s -> 1243.68s]  basically the number n. Right? The array and the number of n. So basically I've said look,
[1243.68s -> 1249.60s]  there's a whole array of stuff to do and we're going to divvy up that array by indices.
[1250.64s -> 1253.60s]  And the way it's going to work is we're all just going to keep grabbing the next
[1254.64s -> 1259.12s]  index that nobody else has done yet until they've all been done. And you might even want
[1259.12s -> 1263.84s]  to say like effectively, even though you don't see a Q data structure or anything here,
[1263.84s -> 1267.20s]  effectively I've thrown all of the things I need to do into a big Q
[1268.00s -> 1274.72s]  and I've implemented the Q by an array and a counter. So conceptually this is kind of like
[1274.72s -> 1280.72s]  I have all this work, I threw all the work in a shared work Q and I have a bunch of worker
[1280.72s -> 1284.88s]  threads going to the Q and say pop me the next one, pop me the next one. Now in this case
[1284.88s -> 1289.20s]  I have a very efficient implementation of that Q because I know all my data is stored in this
[1289.20s -> 1296.16s]  array already. And I can implement, going back to the thing, I can implement
[1296.16s -> 1299.52s]  Q pop by essentially just an atomic increment of that counter.
[1301.12s -> 1304.80s]  So I'd like to make sure everybody, does that make sense? Right? Like conceptually that's
[1304.80s -> 1311.68s]  just a work Q and so on and so on. Okay. So this is like your canonical dynamic
[1311.68s -> 1316.80s]  assignment thing is you just expose all of the potentially parallel work that can be done
[1316.80s -> 1323.28s]  in parallel and then you let the underlying system dynamically assign work to your workers,
[1323.28s -> 1327.36s]  to your threads based on if anybody goes idle they should probably do more work.
[1333.52s -> 1343.36s]  Now, yeah. In program three a lot of people asked me this question about
[1343.92s -> 1349.52s]  well, when I created these ISPC tasks shouldn't I have,
[1353.04s -> 1356.32s]  shouldn't I just create eight tasks? I'm on a machine with eight threads, right?
[1360.00s -> 1365.92s]  And people are like why the heck do I need to like create more tasks?
[1365.92s -> 1375.84s]  So you're saying that you can distribute it into this many different parallel this execution
[1375.84s -> 1383.36s]  and then the code can decide to do that parallely to up to that extent whenever it wants.
[1383.36s -> 1387.84s]  And so if you just like cut this thing into tasks and you cut it into eight tasks
[1389.84s -> 1392.24s]  or let's just say you had a computer with four cores,
[1392.96s -> 1398.96s]  haven't you just divided the work up into this? And what do we know that's wrong about this scheme?
[1401.76s -> 1406.72s]  It's really unbalanced. Now what if instead I cut this up into a bunch of pieces, not that
[1406.72s -> 1412.56s]  scheme over there but just into a bunch of smaller pieces and then I throw all those
[1412.56s -> 1419.68s]  pieces in a work queue. Those pieces are all of different cost but why am I going to end up
[1419.68s -> 1426.00s]  in a good place? Because the program is better able to distribute the work.
[1427.04s -> 1430.88s]  Something like that might happen, right? And as long as I have enough pieces of work
[1431.76s -> 1435.68s]  and they're relatively short, all those threads are just going to keep taking the
[1435.68s -> 1445.44s]  next thing until everything is done. So what's the cost of this dynamic scheme?
[1450.24s -> 1458.96s]  Let's say under the hood I replace n to be number of tasks and is primed to test
[1458.96s -> 1464.48s]  primality is run task i. What is the cost of the dynamic allocation here?
[1468.48s -> 1474.72s]  What do you not have to do in programming assignment one or program one? We have to use
[1475.52s -> 1481.92s]  so you don't have to synchronize. So the question is, is the good workload balance,
[1482.56s -> 1489.68s]  the benefits of that compared to the cost of this, does it overcome the potential workload
[1489.68s -> 1495.36s]  imbalance that you might have if you use the static allocation? And it turns out that in this
[1495.36s -> 1503.04s]  case it certainly did. So that kind of gets to what constitutes a piece of work. So in this
[1503.04s -> 1510.24s]  program the unit of scheduling or the unit of assignment is what? It's one number, right?
[1511.52s -> 1516.80s]  And so if I plot this thing out, let's just think about this being time,
[1517.52s -> 1522.40s]  and the vertical blue bars are actually, imagine this is a plot of one thread,
[1523.44s -> 1529.52s]  and the blue bars are time spent testing primality or running the ISPC task or something like that.
[1530.08s -> 1535.04s]  And then the white bars are like time spent grabbing the lock, incrementing that counter,
[1535.04s -> 1543.92s]  and getting out of the lock. So the question is, do we have a problem?
[1549.12s -> 1554.64s]  How would you know? It would depend on how long it takes to do the synchronization compared
[1554.64s -> 1558.32s]  to how long it takes to do each task. Another way to think about it would be,
[1558.32s -> 1562.00s]  imagine I did this. I gave you this program and I said,
[1563.84s -> 1568.64s]  I want you to make it go as fast as you can. Are you done? What would be your next step
[1568.64s -> 1573.04s]  in thinking about this? I gave you this program and I said, are you done or is there
[1573.04s -> 1578.00s]  something better you could do? How would we go about even figuring out the answer to that
[1578.00s -> 1580.72s]  question? Okay, get us started. I want to talk through this debugging exercise.
[1581.52s -> 1584.48s]  We would first get timing for this program.
[1585.04s -> 1589.36s]  The first thing I do is I put a timer at the beginning at the end and it says it took 5.9
[1589.36s -> 1599.28s]  seconds. Okay, put a timer in the program. What percentage of the program time is spent
[1599.28s -> 1602.00s]  in the lock or waiting for the lock as opposed to...
[1602.00s -> 1607.36s]  Yes, so the next thing I would do is I would put my timers right around test primality
[1607.36s -> 1614.72s]  and I would compute this. And then the difference between the timer around the whole thing
[1615.76s -> 1621.68s]  and the sum of all of the timings of test primality is the overhead, right? I could also
[1621.68s -> 1626.64s]  put my timer around the lock and do the opposite, but you get the point. So if I know
[1626.64s -> 1631.52s]  how much time my program takes and I know how much time I've spent doing useful work,
[1632.88s -> 1636.32s]  what do I do next? I've got a lot of useful information now.
[1636.32s -> 1643.92s]  You usually want to compare it with like how much time it takes.
[1643.92s -> 1647.04s]  There's a couple of questions I could have. So first of all, I could implement the static
[1647.04s -> 1652.32s]  version of the program and hopefully I might have done that first. And so that's the speed
[1652.32s -> 1656.64s]  I got. It was like, oh, it's like eight seconds. So I'm like, okay, I'm already like
[1656.64s -> 1661.92s]  5.9 seconds. I'm doing better. But now my question is, can I do even better than the 5.9
[1661.92s -> 1668.08s]  seconds? And the information I have is the total 5.9 seconds. And let's say that I end up saying I
[1668.08s -> 1676.40s]  spend 5.75 seconds in all of my calls to test primality. What do I conclude?
[1681.84s -> 1684.40s]  You probably can't really do better than that.
[1684.40s -> 1688.32s]  Yeah, you're like, well, like if 5.9 seconds is the whole computation,
[1688.32s -> 1693.68s]  5.75 seconds is like all the useful work. The only thing I could do is shave off like, you know,
[1693.68s -> 1699.20s]  0.1 or something like that. And then I decide, you know, if I'm a high frequency trader, maybe
[1699.20s -> 1703.20s]  I have to win that race and maybe I go for it. But if I'm working on an assignment, I say,
[1703.20s -> 1705.84s]  screw it, I'm going on to the next class. Like there's not much more I can do.
[1706.96s -> 1713.36s]  Exactly. Now, if this timer around test primality says all calls to test primality are
[1713.92s -> 1719.84s]  total maybe 2.5 seconds, now I'm like, well, wait a minute here. More than half my time is
[1719.84s -> 1724.72s]  probably being spent elsewhere. And since this program is so simple, I know exactly where it's
[1724.72s -> 1731.52s]  being spent. It's being spent in this lock. So what do I do next? Let's say my timers tell me
[1732.32s -> 1739.52s]  over 50% of the time is spent in this lock. And I'm like, yeah, I wouldn't mind trying
[1739.52s -> 1752.24s]  to make this thing two times faster. Okay. So one conclusion, let's say this was my
[1752.24s -> 1755.68s]  first program. And this would probably be a really simple program to write first.
[1755.68s -> 1762.48s]  You could say maybe I don't need this dynamic mechanism. Maybe I should just back off and do a
[1762.48s -> 1768.32s]  static assignment and remove that. That would be a potentially, that would be a very viable
[1768.40s -> 1772.96s]  strategy. There's a few simpler things I might try first that don't require me to rewrite everything.
[1778.00s -> 1784.40s]  Oh, okay. So what we did here is we said we're going to break this work up into the smallest
[1784.40s -> 1790.16s]  pieces that are possible. Maybe, okay, I'm sure you could paralyze inside the test
[1790.16s -> 1792.88s]  primality, but we're not going to do that. Let's say this is a black box.
[1793.12s -> 1800.16s]  And the reason why I divided things into the smallest pieces as possible was what? I didn't
[1800.16s -> 1805.52s]  explicitly state that. Yeah. It's like you want to have the smallest increment so that you
[1806.64s -> 1811.52s]  have a better probability of getting like a even workload. Correct. So the more
[1811.52s -> 1816.72s]  little pieces I have, the easier it's going to be for me to come up with a partitioning
[1817.20s -> 1823.12s]  where everybody has about the same amount of work. Imagine I only had, let's say there were
[1823.12s -> 1827.20s]  eight threads, and imagine I divided the work into eight pieces and I used the dynamic
[1827.20s -> 1833.84s]  scheduling scheme. Not that helpful, right? So I do want a lot of pieces to give me the
[1833.84s -> 1839.60s]  flexibility to arrive at a good workload balance. But the smaller I make those pieces,
[1839.60s -> 1845.36s]  the larger this overhead potentially is, right? So I like this suggestion. Let's just,
[1846.08s -> 1850.40s]  I mean, maybe if we just double the size of the pieces, we still have enough stuff to do,
[1850.40s -> 1853.20s]  because maybe, let's say n is like a million and I only have eight threads.
[1854.32s -> 1860.08s]  So how would you implement taking two pieces of data out of the queue at once? Yeah.
[1863.12s -> 1868.24s]  You just increment counter by two, and what that's done is it's going to reduce the number
[1868.24s -> 1873.84s]  of white bars or white regions here by two, right? And I could just play with that number,
[1873.84s -> 1878.72s]  like I didn't do it by two in my example, I did it by granularity, I parameterized it,
[1878.72s -> 1883.04s]  because I'm a good software engineer, and I can mess with that parameter right before the
[1883.04s -> 1888.80s]  deadline to get the best performance. And so I just, I increase the granularity of my task
[1888.80s -> 1893.84s]  to reduce overhead. And it's very likely on a lot of modern systems, like taking this small
[1893.84s -> 1899.20s]  little increment is not that expensive. So most likely you probably can make your workload
[1899.20s -> 1903.28s]  granularity with just making it a little bit bigger, can basically probably wash out the
[1903.28s -> 1909.52s]  overheads in a lot of dynamic situations. So that's the trade-off, like we want as many
[1909.52s -> 1915.52s]  tasks as we can get by with, or we want enough tasks that we can nicely schedule the work,
[1915.52s -> 1922.00s]  but we don't want unnecessarily many tasks such that overhead got too high. For example,
[1922.00s -> 1927.12s]  I don't know if any of you tried actually doing one task per row, like creating like a
[1927.12s -> 1933.04s]  thousand tasks, and at some point you might have started to see the speed up curve fall off.
[1933.04s -> 1936.16s]  You almost certainly would have seen it fall off if you made one task be a pixel.
[1938.88s -> 1944.08s]  So here's an example, imagine I have these 16 tasks, so this is task 0, 1, 2, 3, all the
[1944.08s -> 1949.92s]  way to 15 over there. And I just purposely made the last task much, much longer than the rest.
[1951.84s -> 1956.08s]  Now imagine some dynamic scheduling approach, just like what we talked about.
[1956.64s -> 1961.36s]  And here's a possible final assignment as a result of the dynamic scheduling.
[1964.40s -> 1968.48s]  So notice that even though I did dynamic assignment, I got a little bit unlucky,
[1970.48s -> 1977.04s]  because the work at the end of the queue was actually really big stuff. So like the last
[1977.04s -> 1982.00s]  piece of work that I assigned was like the longest running piece of work, and so I still
[1982.00s -> 1986.72s]  ended up with like a long tail here. And there's some strategies you could try, like if you happen
[1986.72s -> 1993.04s]  to know a little bit more about your tasks, you might do something where you assign the biggest
[1993.04s -> 1997.76s]  tasks first. So I'm going to take this, now look at the P4 got assigned the big task right
[1997.76s -> 2003.68s]  at the end. If we actually decided to like sort things by cost, maybe I'll assign P4 the biggest
[2003.68s -> 2009.20s]  task at the very beginning. And notice that P4 only does two tasks, but everybody kind of ends
[2009.20s -> 2014.80s]  up finishing at about the same time. So the more you know about your tasks, the better you can do
[2014.80s -> 2026.56s]  some scheduling. Now the other strategy we've talked about, and that was just we just talked
[2026.56s -> 2031.76s]  about it with these barriers or that mydiff variable, was if we have a bunch of communication,
[2031.76s -> 2036.80s]  what is being shared in the work queue example? So in the example that I just showed you with the
[2037.20s -> 2042.72s]  the pool of worker threads, the shared variable is the counter, is the lock and the counter.
[2043.68s -> 2048.88s]  And maybe if we were running this on a big computer with like 128 threads or something
[2048.88s -> 2055.52s]  like that, that contention for that counter might actually be pretty non-trivial for
[2056.08s -> 2062.32s]  especially if the work was small. So I want to get into a little bit more advanced mode
[2062.32s -> 2070.56s]  on how you might alleviate the cost of having a shared work queue by actually distributing,
[2070.56s -> 2076.88s]  making copies of that work queue for every thread. And so this is going to be the same idea
[2077.44s -> 2081.36s]  as remember we took the single mydiff variable and made local copies of it.
[2082.00s -> 2086.88s]  So you worked on your local mydiff and then only aggregated those results when you had to at
[2086.88s -> 2091.36s]  the very end. We're going to do the same thing here for work queues. Now you're not going to
[2091.36s -> 2096.48s]  do this for your assignment by the way. This is just for your own information. But the other
[2096.48s -> 2101.12s]  thing I wanted to just point out, something you will do for your assignment, is that so far in
[2101.12s -> 2106.40s]  this lecture all the pieces of work that we threw into the queue were independent. You
[2106.40s -> 2111.28s]  process them in any order and it didn't matter. Something that's very common in a lot of
[2111.28s -> 2116.00s]  different systems is those pieces of the work, many of them might be executed in parallel,
[2116.00s -> 2121.20s]  might be independent, but they have dependencies between these two. So here's an example where
[2121.20s -> 2126.64s]  I wrote some code where I said okay let's say I had an API to tell the system here's a new piece
[2126.64s -> 2132.72s]  of work. So I want you to do task foo. And then we say I want you to then do task bar
[2133.92s -> 2140.48s]  but please only do bar after you've done foo. So I'm building up this dependency graph
[2141.12s -> 2148.56s]  that can strain or limit what can run when. So your assignment to the final part of it
[2148.56s -> 2154.80s]  will be I give you a bunch of tasks like this with the dependencies and you have to implement
[2154.80s -> 2160.00s]  a work queue that runs those tasks but respects all those dependencies.
[2161.04s -> 2164.32s]  And so it can be a little tricky right because like maybe one task finishes
[2164.96s -> 2169.04s]  and that means that you can now start running a whole bunch of future tasks that you had
[2169.04s -> 2175.12s]  already been given but could not start just yet. So this is a very very common scheduling problem
[2175.12s -> 2178.72s]  that exists in a bunch of different systems. So that's what your assignment two is going to be.
[2183.52s -> 2188.80s]  Okay so just a little bit of summary that I probably stated a bunch of times.
[2190.00s -> 2191.20s]  Any questions so far? Yeah.
[2192.16s -> 2196.40s]  It's not fair to me like why stealing a task from another thread like
[2196.40s -> 2199.44s]  makes it faster because then doesn't the work queue still remain empty?
[2200.16s -> 2211.28s]  Um let's let's get into it. Let's get into it. Okay so so far in this class because it's pretty
[2211.28s -> 2216.64s]  easy to think about all almost all of the code that we have looked at almost all of the programs
[2218.08s -> 2224.64s]  the parallelism comes from doing about the same thing on different pieces of data from a
[2224.64s -> 2229.76s]  collection. So in the Mandelbrot fractal is compute the color of a pixel for all pixels.
[2230.56s -> 2235.04s]  For many of the other examples in assignment one it was like compute the sign of this number
[2235.04s -> 2241.84s]  or so on and so on. So all the code that we have have looked at kind of had this structure
[2241.84s -> 2248.96s]  you know for each element of the array. For each task every task does something different.
[2249.60s -> 2253.92s]  Later in the course we'll talk about functional abstractions which says
[2253.92s -> 2259.76s]  run the function foo on every element of the array A and put the output in every element of the array
[2259.76s -> 2264.80s]  B. So this is more of like a PyTorch kind of a way of thinking about things. There are other
[2264.80s -> 2268.56s]  things that you'll see when we get into CUDA and other types of programming. Like there's a
[2268.56s -> 2274.80s]  little C extension called OpenMP. OpenMP you put these little pragmas in front of your for loop
[2274.80s -> 2280.08s]  and basically this pragma tells the compiler this is a normal C for loop but I promise the
[2280.08s -> 2285.28s]  loop iterations are independent. Under the hood launch actual threads and execute these iterations
[2285.28s -> 2289.60s]  in parallel. It's like the easiest way to get parallelism in C is just use these little
[2289.60s -> 2294.08s]  OpenMP kind of things. And when we get into programming in CUDA it's going to be the same
[2294.08s -> 2298.48s]  thing. It's like I want you to run this function foo like this many times and I want you to
[2298.48s -> 2302.32s]  run on those arrays. So everything that we've seen so far has been like let me just give
[2302.32s -> 2306.24s]  you some arrays or some tensors and I want you to do the same thing on on everything.
[2306.96s -> 2312.24s]  And so that's one way of programming that we've talked about. The other way of programming that
[2312.24s -> 2317.92s]  you're probably much more accustomed to is if you want parallelism you don't describe independent
[2317.92s -> 2323.52s]  things you actually create workers and have them do stuff. So here's an example where I
[2323.52s -> 2330.72s]  actually create a bunch of p threads or C++ threads excuse me in this case. So I am saying as the
[2330.72s -> 2336.56s]  programmer I want you to create these n threads and this is what they will do. So in the first
[2336.56s -> 2341.84s]  case I was thinking in terms of data. For all of these pieces of data you can do this
[2341.84s -> 2346.32s]  independently. When we're threaded programming we think a little bit more like here are my
[2346.32s -> 2351.20s]  threads and here's what they should do. And another way of thinking about programs that's
[2351.20s -> 2356.64s]  sort of pretty elegant there's another class of algorithms that like if you took 161 or
[2356.72s -> 2360.72s]  something like that you know you would think about them in a very divide and conquer kind of way.
[2361.28s -> 2367.36s]  So the canonical example of that is quicksort. So can I just quick check
[2367.36s -> 2373.20s]  quicksort is something that everybody has seen? Okay all right great and so maybe just a little
[2373.20s -> 2377.52s]  bit of a review of quicksort just I know you've seen it would be given an array I want
[2377.52s -> 2382.64s]  to sort it and I sort it in a divide and conquer way and the summary of quicksort remember
[2382.64s -> 2388.64s]  is I pick some element in that array the details of which matter for its asymptotic complexity but
[2388.64s -> 2392.56s]  for the sake of this class pick some elements of the array at random or even pick the first one
[2393.68s -> 2397.84s]  and then I'm going to partition I'm going to move all the elements below less than that value
[2397.84s -> 2402.48s]  to one side of the array move all the elements above that value to the other side of the array
[2402.48s -> 2407.68s]  and then I'm going to recurse and that's what you see here so the recursive calls to quicksort.
[2407.76s -> 2414.88s]  So this partition is picking the pivot and then it's picking the pivot and moving everything less
[2414.88s -> 2418.08s]  than the pivot to the left side of the array moving everything greater than the pivot to the
[2418.08s -> 2422.08s]  right side of the array now it means that everything on the left side of the array can
[2422.08s -> 2426.40s]  be sorted and everything on the right side of the array can be sorted and recursively we
[2426.40s -> 2433.20s]  have a sorted array. Just a little review of quicksort. Okay so you know the skill that
[2433.20s -> 2436.48s]  you're supposed to be building up in this class is if you look at this code
[2437.60s -> 2442.40s]  step one in any programming at parallel programming exercise is identify potential
[2442.40s -> 2447.68s]  parallelism and where is that in this program let's assume that partition is going
[2447.68s -> 2453.68s]  to be done serially for now let's treat that as a black box yeah two recursive calls to
[2453.68s -> 2457.52s]  quicksort we can do the left side and the right side in parallel so we're kind of used
[2457.52s -> 2462.48s]  to this data parallel thinking where we say oh we got an array of 10 million elements everything
[2462.48s -> 2468.80s]  can be done in parallel here we're saying hey functionally we have two pieces two things that
[2468.80s -> 2474.32s]  can be done in parallel probably not enough to saturate a big parallel computer with just
[2474.32s -> 2478.72s]  this one recursive call but where do i get my a lot of my lots of parallelism from
[2480.88s -> 2484.16s]  each of those calls is going to make more calls right each of those calls is going to create
[2484.16s -> 2489.68s]  two more things and so on and so on so i have this tree of potential parallelism so this is
[2489.68s -> 2495.92s]  independent work and if i recurse ultimately at some point i get down to a level of the tree
[2495.92s -> 2501.44s]  where i have a lot of parallel things to do so this is a recursive algorithm that progressively
[2501.44s -> 2506.32s]  reveals its parallelism unlike this data parallel stuff where i say here's a million things to do
[2509.28s -> 2515.28s]  okay so we're good on the workload right all right so what i'm going to talk about now is
[2515.28s -> 2523.68s]  a programming abstraction that makes it a lot easier to write these divide and conquer parallel
[2523.68s -> 2528.64s]  programs and you're going to have at your disposal and by the way this is a this is a
[2528.64s -> 2535.04s]  programming system called silk it exists in most modern c c++ compilers today so you
[2535.04s -> 2540.64s]  probably can on your myth machines write code like this if you wish and there's only two
[2540.64s -> 2547.20s]  changes to c in principle so typically we think about a a normal function call
[2548.40s -> 2554.08s]  what we're going to do is we're going to be able to put this little keyword spawn in front
[2554.08s -> 2559.68s]  of a function call and then we're going to add one more construct to the language called
[2559.68s -> 2565.20s]  sync now let's talk and be really clear about what this means
[2565.20s -> 2575.20s]  so what silk spawn a function means is please invoke foo just like any other function call would
[2576.08s -> 2585.12s]  but unlike a normal function call the caller may continue executing asynchronously with the callee
[2586.08s -> 2589.76s]  this is one of those cases where i want to be very clear about in your mind
[2590.56s -> 2597.60s]  do does the semantics of what the programming model means is that clear to you independent
[2597.60s -> 2606.16s]  of how it might be implemented so in normal c if we call a function the calling thread of control
[2606.80s -> 2615.20s]  goes into the function the function must return and then the caller continues in silk if it is a
[2615.20s -> 2623.20s]  spawn foo what it means is that the caller can just continue and asynchronously foo may run
[2626.56s -> 2631.04s]  and then sink just says for any calls that i have spawned
[2633.04s -> 2639.52s]  sync means that you are guaranteed that those calls have completed have returned prior to
[2639.52s -> 2644.24s]  continuing from this function so you think about sync as a barrier for all spawn functions
[2645.20s -> 2648.08s]  okay questions
[2651.60s -> 2657.84s]  notice that i haven't said anything about c++ threads implementation or anything like that
[2659.04s -> 2665.52s]  okay so this all is clear to everybody okay all right so you might be thinking oh
[2665.52s -> 2672.32s]  this silk spawn foo feels a lot like forking a thread and the sink feels a whole lot like
[2672.32s -> 2679.12s]  joining a thread but there's a difference between creating threads and spawning work
[2680.24s -> 2685.92s]  you can think about this is there's some work to do that is execute the function foo on these
[2685.92s -> 2695.44s]  arguments and you can do it whenever you want at some future time that's different from forking
[2695.44s -> 2700.16s]  a thread which says or creating a thread which says here is a thread of control it is now
[2700.16s -> 2703.84s]  running it needs an execution context processor is running it
[2706.32s -> 2711.92s]  let's see if further elaboration makes sense but i'm expecting questions soon
[2713.12s -> 2719.44s]  so here's an example my function that calls foo and bar and i'm starting to adopt this
[2719.44s -> 2725.60s]  illustration here on the right which illustrates kind of the stack or the sequence of execution of
[2725.60s -> 2731.84s]  this thread of control so in this thread of control there's like part a the part of my
[2731.84s -> 2738.48s]  func before the the call to foo then there will be the execution of foo then the execution
[2738.48s -> 2748.08s]  of bar and then everything after bar that's just normal c function calls in silk spawning
[2748.08s -> 2754.80s]  foo is like creating an asynchronous concurrent logical instruction stream
[2756.08s -> 2761.04s]  that may or may not be run at the same time so you can think about spawning foo is more
[2761.04s -> 2764.96s]  like creating a piece of work some note that says you got to run foo at some point
[2766.16s -> 2772.48s]  and notice what i'm drawing here is i'm showing you the concurrency in the program using my
[2772.48s -> 2780.64s]  arrows so spawn foo and then calling bar and then sinking notice that foo is spawned off to the side
[2780.64s -> 2786.08s]  bar is actually executed as a normal c function by the caller that's why it's straight down and
[2786.08s -> 2791.44s]  then the sink means that the main caller has to wait for the foo to complete before it
[2791.44s -> 2797.60s]  continues does that diagram make sense and then just check your understanding i can also
[2797.60s -> 2801.36s]  spawn foo spawn bar and then sink and notice that the main
[2802.16s -> 2806.80s]  the initial logical thread doesn't do anything right it just sits there and just waits for
[2806.80s -> 2815.28s]  foo and bar to get to finish yes guaranteed that's a unlike the second example of
[2815.60s -> 2818.80s]  one
[2819.36s -> 2830.08s]  so there we should be careful about um i want you to think about this as logical work so
[2830.08s -> 2836.88s]  what is true in the semantics is that bar and foo are two pieces of work that can run
[2836.88s -> 2842.32s]  asynchronously that run asynchronously with this i am care very being very careful not to
[2842.32s -> 2848.32s]  telling you what like if there's a thread pool under the hood what threads are actually going
[2848.32s -> 2856.16s]  to run these things but logically i have three logical threads of control here that
[2856.16s -> 2861.28s]  are defining work that needs to be done and can be done concurrently at the same time
[2863.44s -> 2867.44s]  copies of their parameters for now just think about as a shared address space just completely
[2867.44s -> 2873.20s]  share the address space and just one more check here spawn food bar fizz buzz happens as a
[2873.20s -> 2879.12s]  conventional c call c function call notice that it's right here on the main thread and then i
[2879.12s -> 2885.28s]  sync to make sure all the asynchronous stuff is done before continuing on okay so oops
[2887.20s -> 2894.56s]  so silk says nothing about when this asynchronous work is done other than it has to get done
[2894.56s -> 2902.40s]  before sync happens before sync returns and so a question to you is this question here on the
[2902.40s -> 2909.04s]  slide is an implementation of silk correct meaning does it get a valid answer to the program
[2909.92s -> 2916.64s]  if i just took out all of the spawns and sinks from the code if my implementation of the silk
[2916.64s -> 2922.80s]  compiler is take this text string search for silk underscore spawn remove it
[2923.92s -> 2929.28s]  silk search for sync remove it and then compile the resulting program as a normal c program
[2930.40s -> 2935.52s]  yes it is a correct program and the language is actually intentionally designed for this to be
[2935.52s -> 2940.64s]  the case can you see that
[2943.20s -> 2948.72s]  now of course would it be a correct program if the implementation here would be whenever
[2948.72s -> 2956.24s]  there was a sync silk spawn it spawned a p thread to run foo spawned another p uh c++
[2956.24s -> 2962.32s]  thread to run bar spawned another c++ thread to run fizz and then silk sync was just a join
[2962.32s -> 2968.96s]  of all three of those p threads that's also a valid implementation is it yeah question yeah
[2970.00s -> 2974.88s]  those are two valid implementations of this programming model there's like no constraints
[2974.88s -> 2980.48s]  on order we'll have dependencies and if we don't get you that there's no dependency here
[2980.48s -> 2986.24s]  between foo bar and fizz the only dependency is that foo can run asynchronously with the main
[2986.24s -> 2994.72s]  thread and foo must be done by the time the main thread returns from silk sync those are the only
[2994.72s -> 3001.44s]  scheduling constraints so i could run foo bar fizz sequentially i could run foo bar fizz on
[3001.44s -> 3008.00s]  different threads i can run foo and bar on one thread and fizz on another those are all valid
[3008.00s -> 3012.40s]  potential schedules or implementations of this program they will all get the same answer
[3012.64s -> 3019.44s]  would fizz bar foo be valid by that case fizz bar foo in uh if you said spawn because it
[3019.44s -> 3024.80s]  doesn't really would it not really matter yeah because this this is a this foo is asynchronous
[3024.80s -> 3030.24s]  with the main thing bar is asynchronous with the main thing so it must be asynchronous with
[3030.24s -> 3035.20s]  foo and if fizz is asynchronous with the main thing which is asynchronous with bar and foo
[3035.20s -> 3039.20s]  yes they're all different do not think about this as like a command buffer or something like
[3039.20s -> 3044.72s]  that and i'm pushing things into a command yeah i'm just saying foo bar and fizz are all completely
[3044.72s -> 3050.24s]  independent things that can be scheduled in any order just hey you better be done with them
[3050.24s -> 3054.72s]  by the time this call returns that's the only guarantee i get as a programmer
[3057.12s -> 3062.40s]  so i'm just i'm just kicking off parallel work basically now the difference is that like these
[3062.40s -> 3066.56s]  foods and bars they can be recursive functions and they can actually kick off their own parallel
[3067.12s -> 3074.56s]  also okay okay so here's quicksort implemented in this programming model
[3074.56s -> 3082.56s]  notice that the only thing i did more or less was uh call spawn oh by the way uh there's an
[3082.56s -> 3086.88s]  implicit sync at the end of the function so i didn't write it here but when a function returns
[3087.92s -> 3092.24s]  all things that is spawned have uh have to sync at the end of the function so that's why
[3092.24s -> 3097.84s]  you don't see a sink here and the only other thing is i have this like if the size of the
[3097.84s -> 3103.76s]  array is sufficiently small just do it sequentially because at that point so i don't want any
[3103.76s -> 3108.08s]  overhead of dealing with concurrency or anything like that so there's a little bit of a base
[3108.08s -> 3113.76s]  case there now but i didn't need to put that in there for for correctness okay and now look
[3113.76s -> 3120.32s]  to see if you can confirm that this is correct so we start with the main thread the main thread
[3120.32s -> 3128.64s]  runs partition which is a normal c function based on the partition it spawns uh well first of all
[3128.64s -> 3135.68s]  it does a normal it spawns parallel work so that's the edge off to the right and then it calls
[3135.68s -> 3142.56s]  quicksort recursively on the main the current thread of control i could have spawned both of
[3142.56s -> 3147.04s]  them but i just decided to do one of them on the main thread they're essentially the same
[3147.04s -> 3154.88s]  okay and then that's where i get these two gray boxes and then we recurse inside the gray boxes
[3154.88s -> 3159.84s]  there's a partition call and then another spawn and so on and so on all the way down to
[3159.84s -> 3168.72s]  the leaves of this tree where the leaves are just calling standard std sort and you know that
[3168.72s -> 3174.24s]  small little implementation detail in my cutoff any questions about this program and the
[3174.32s -> 3175.60s]  corresponding diagram
[3178.08s -> 3185.20s]  good cool okay so so the main idea here is just the programmer's responsibility
[3186.08s -> 3194.48s]  is just to reveal work and it can reveal it in a recursive way okay it's silk's responsibility
[3195.60s -> 3201.44s]  to schedule these programs so your goal is to make sure that the recursion generates enough
[3202.40s -> 3208.72s]  parallel tasks such that a good scheduler can schedule them and get good workload balance
[3208.72s -> 3211.44s]  if i don't create enough parallel tasks there's nothing anybody can do
[3214.32s -> 3222.48s]  okay all right and so as i've alluded to like one simple the simplest possible implementation
[3222.48s -> 3226.48s]  of silk is ignore all of the spawn and sync keywords and run everything sequentially
[3227.44s -> 3233.20s]  the other crazy simple implementation is turn every spawn into an actual standard thread spawn
[3233.92s -> 3238.64s]  and every sink into a joint of everything that i've spawned and that is a valid implementation
[3239.36s -> 3245.36s]  but it would be a little bit slow because like the cost of spawning threads is pretty heavy
[3245.36s -> 3250.32s]  remember that demo i showed last week i was like 300 times faster by using a thread pool versus
[3251.20s -> 3257.44s]  uh okay so yeah so this is a little bit much so what we're going to do is is we're going to do
[3257.44s -> 3262.80s]  what everybody does is we're going to create a thread pool let's say i have a processor that
[3262.80s -> 3267.04s]  has like eight execution contexts at the beginning of the program i'm going to fire up
[3267.04s -> 3272.88s]  eight threads and all of those threads are going to be just running while there is still work to
[3272.88s -> 3277.68s]  do in the system i'm going to go grab the next piece of work i'm going to go grab a piece
[3277.68s -> 3284.72s]  of work and i'm going to get it done that's the thing here so all of these spawn calls are
[3284.72s -> 3291.12s]  actually going to be like adding work to a list of things to do and we need an algorithm that
[3291.12s -> 3299.52s]  each of the threads are running that achieves good workload balance and uh uh minimizes
[3299.52s -> 3304.96s]  synchronization cost okay so let's think about an example here silk spawn foo run bar on the
[3304.96s -> 3309.60s]  main thread and then sync that's what we have off to the side and just to give some things
[3309.60s -> 3317.52s]  some names i'm going to call the asynchronous call the child and i'm going to call the main the
[3317.52s -> 3323.44s]  rest of the main thread the continuation just to give things some names so if we only have
[3323.44s -> 3331.28s]  one core we have a choice to make we can either run the child first or we run the continuation
[3331.84s -> 3333.36s]  a normal c program does what
[3336.24s -> 3341.04s]  this is a normal c code does it run the child first or does it run the continuation
[3341.68s -> 3344.64s]  runs the child first because we go into the the function call exactly
[3345.44s -> 3352.00s]  so now let's assume that we have a pool of two threads okay all right let's let's uh assume
[3352.00s -> 3356.72s]  that we have a pool of of two threads and i got ahead of myself like if this was a single
[3356.72s -> 3362.00s]  threaded implementation i just have a stack right and what would happen is when i call foo
[3362.56s -> 3370.16s]  i push my current frame onto the stack i go run foo and then when i return i return back
[3370.16s -> 3376.64s]  to the caller and that's just how it works so imagine i'm running uh in this case i'm running
[3376.64s -> 3383.76s]  uh foo but while i'm running foo which is asynchronous with the caller and silk my other
[3383.76s -> 3392.56s]  thread is just idle right so the name of the game here is that thread one could be running
[3392.56s -> 3397.28s]  you know bar it could be running the caller and so what's going to happen is
[3401.04s -> 3408.16s]  when this thread calls foo it starts running foo and then it puts bar you know it reveals
[3408.16s -> 3415.04s]  its stack to all other threads and if thread one over there is idle and sees that thread
[3415.04s -> 3419.76s]  zero has bar in its work queue thread one is just going to rip that out of the stack
[3420.80s -> 3424.88s]  and start running bar or really it's just going to put bar in its own stack
[3425.92s -> 3431.20s]  or its own work queue okay so instead of the the stack just being kind of
[3431.20s -> 3435.20s]  sitting there in the heap and and uninterpretable to other threads what's going to happen is when
[3435.20s -> 3441.12s]  we spawn foo we're actually going to create this data record that says there's foo and there's
[3441.12s -> 3446.72s]  the continuation and there's bar both of these are in my work queue one of the threads will
[3446.72s -> 3452.80s]  take foo the other thread will take bar so in this multiple queue situation every worker thread
[3452.80s -> 3457.92s]  has a queue of stuff that it's supposed to do next and if my queue ever goes empty i'm
[3457.92s -> 3459.28s]  going to go steal from somebody else
[3461.52s -> 3470.08s]  okay all right so we have some options when i look at this foo and bar i
[3474.08s -> 3478.00s]  i could either start running like the thread that actually runs this code
[3478.80s -> 3482.96s]  it could either start running bar and put foo on the stack for stealing
[3483.52s -> 3488.00s]  or it can start running foo and put bar on the stack did i say the same thing twice i
[3488.00s -> 3493.20s]  can't remember i might have but i think you get the point right so running the continuation
[3493.20s -> 3501.20s]  first is queuing the function call for later stealing running the child first is in queuing the
[3501.84s -> 3508.40s]  continuation for later stealing this is the same order as a serial program this is a completely
[3508.40s -> 3515.52s]  different order okay so it's kind of unclear which one is better so let's take a look at
[3515.52s -> 3519.76s]  this program i have a for loop for i equals zero to n spawn spawn spawn spawn spawn
[3520.64s -> 3524.96s]  so the uh the work looks like this like i get all these edges off to the right side
[3526.08s -> 3535.60s]  okay and if we uh if we run the continuation first every one of those spawns is just going
[3535.60s -> 3541.28s]  to be adding foo zero foo one foo two foo three into my work queue
[3543.28s -> 3547.52s]  and the running the the existing thread is just going to keep running that for loop
[3547.52s -> 3550.88s]  because that's the continuation it just keeps running the caller
[3551.76s -> 3553.92s]  and this queue just kind of fills up with a bunch of work
[3556.16s -> 3561.36s]  can you see how that works what's going to happen if we run the child first
[3561.44s -> 3569.92s]  the thread gets to the first foo you know spawn of foo zero and what does it start doing
[3570.56s -> 3575.44s]  yeah it's going to start doing the work for that call and it's not going to need
[3575.44s -> 3578.72s]  more work right so this thread is going to start doing foo zero
[3579.36s -> 3584.24s]  and it's going to put its continuation on its stack on on its on its work queue so it says
[3584.24s -> 3589.60s]  i'm working on foo zero and the rest of that for loop is the task that i'm going to put in
[3589.60s -> 3594.88s]  my queue is something i have to do later so it might look a little bit like this so in my
[3594.88s -> 3602.08s]  indication my my notation here is thread zero is running foo zero and thread zero's work queue
[3602.08s -> 3606.40s]  has in it the rest of the loop which i noted there is the continuation of the loop
[3607.12s -> 3613.92s]  starting at the iteration i equals one the next iteration so i just start doing what the the
[3613.92s -> 3620.00s]  child and i just shove everything else onto the queue okay now what happens in this scheme
[3620.00s -> 3630.08s]  in this run child first scheme if i have two threads so what is thread so thread zero is
[3630.08s -> 3639.76s]  running foo it put the rest of the continuation into its stack there's some other thread that
[3639.76s -> 3647.84s]  steals the rest of the for loop starts running iteration one which causes it to start running
[3647.84s -> 3655.04s]  iteration one and then i need um sorry i made one mistake
[3655.68s -> 3660.48s]  and then the rest of the work to steal is the for loop beginning iteration two right
[3664.00s -> 3668.72s]  and then if thread zero gets done it steals the continuation back starts running foo two
[3669.28s -> 3671.92s]  and puts the continuation with iteration three into its queue
[3674.48s -> 3676.56s]  so in the first example uh
[3678.72s -> 3682.00s]  well in this example the continuation is just going to be bouncing between the threads
[3683.84s -> 3688.88s]  in the other example the caller just created all of the iterations of the work in its own
[3688.88s -> 3692.24s]  queue and thread the second thread was just going to steal all these little pieces
[3693.52s -> 3698.24s]  okay this is a good one to talk through in the future or offline it's like a good one
[3698.32s -> 3702.24s]  just to make sure you work through so at this point you're like this is i'm not you're probably
[3702.24s -> 3705.76s]  not feeling very good about this because like work's like bouncing around all over the place
[3705.76s -> 3711.20s]  and but let's think about this when your algorithm is recursive okay so let's go back
[3711.20s -> 3718.56s]  to quicksort and think about what happens if we're doing a run child first scheme with
[3718.56s -> 3724.16s]  a recursive divide and conquer algorithm let's just say the size of the array was uh
[3725.04s -> 3732.64s]  uh i think 200 elements 200 elements yeah so the first thing that happened would be
[3734.08s -> 3740.80s]  i'm going to call quicksort recursively on elements zero through 100 and i'm going to put
[3740.80s -> 3746.08s]  onto the my future queue i need to do the quicksort for the other the back half of the
[3746.08s -> 3752.64s]  array 100 to 200 so the first thing i pushed onto my queue was quicksort for the rest and
[3752.64s -> 3756.88s]  i started working on quicksort zero to 100 and then what happens next
[3759.44s -> 3765.68s]  i have to push uh zero of 50 to 100 onto my stack and i start working on zero to 50
[3766.40s -> 3772.08s]  then what happens next i push 26 to 50 onto my stack and i start working on zero to 25
[3773.28s -> 3777.04s]  so all these recursive calls are building up here notice that each
[3777.84s -> 3782.24s]  piece of work that i threw in is smaller than the previous one
[3784.16s -> 3791.28s]  so the tiny tasks are down here the big tasks are up here so now all my
[3791.28s -> 3794.64s]  tasks are different sizes because the recursive nature of this program
[3795.84s -> 3801.04s]  okay so now let's think about what these idle threads one and two are going to do
[3801.04s -> 3802.72s]  so they're idle so what are they going to do
[3802.72s -> 3809.60s]  they need to still work because they're not making any progress so now here's a question
[3810.24s -> 3814.48s]  there are three pieces of work here which one should thread one steal
[3817.44s -> 3824.40s]  what are some thoughts yeah okay so one vote is for stealing from the top of the queue
[3824.40s -> 3827.84s]  really these are decks so you can actually steal from the bottom or the top or anywhere
[3827.84s -> 3833.92s]  so they're not actually cute but what is your justification for stealing from the top
[3833.92s -> 3838.64s]  i was gonna say like we saw like earlier in like trips you do the big work last and you
[3838.64s -> 3844.48s]  end up having one thread like working at the very end okay yeah that's a that's a good reason
[3844.48s -> 3847.76s]  there's that's actually a pretty interesting reason i i actually wasn't even looking for
[3847.76s -> 3851.44s]  that there's even a more obvious reason why you want to steal from the top you all are correct
[3852.16s -> 3855.84s]  yeah it's just going to break up into a bunch of smaller chunks again
[3856.32s -> 3864.16s]  uh so this is a big task this is quick sorting 100 elements this is a much smaller task
[3864.16s -> 3870.08s]  quick sorting 25 elements if another thread is going to go through the overhead of stealing
[3870.08s -> 3875.60s]  some work to give themselves something to do they should go steal the biggest thing and get
[3875.60s -> 3880.32s]  working on it immediately actually for both of your reasons right so to minimize overhead it
[3880.32s -> 3884.64s]  makes sense for me to look at the state of the world and not go oh i'm going to take this
[3884.64s -> 3888.08s]  little tiny thing away from you but i'm going to go take a big thing from you
[3889.36s -> 3895.36s]  so there's the advantage of this thread zero is always going to grab the next task off the
[3895.36s -> 3900.16s]  bottom of its if it's local queue because this is like the same order as a sequential
[3900.16s -> 3904.32s]  program in some sense it's also good because you get data locality and a lot of other nice
[3904.32s -> 3910.08s]  properties remote threads are going to steal from the top of my queue for actually two cool
[3910.08s -> 3914.72s]  reasons one is the one that you mentioned if you steal a lot of work you won't have to
[3914.72s -> 3918.88s]  synchronize again for a long time and two there are actually if you make sure that
[3920.16s -> 3924.48s]  thread zero is only touching this and all other threads are touching the top
[3924.48s -> 3929.04s]  there are very efficient data structures that you can use that don't require nearly as much
[3929.04s -> 3936.40s]  synchronization actually so here let's say thread one steals the biggest item and thread two steals
[3936.40s -> 3940.32s]  the next small or sorry did i get that right no thread two stole the big thing in this case
[3940.32s -> 3948.48s]  and thread one stole the medium size thing yep exactly so and then like things just start
[3948.48s -> 3954.08s]  getting to work and now everybody's processing their own jobs and all of those jobs keep in
[3954.08s -> 3960.64s]  mind are creating recursive calls and now shoving data into the various queues so now
[3960.64s -> 3966.40s]  all these threads are independently working on their subtasks not communicating at all
[3967.44s -> 3970.56s]  and actually revealing more parallel work as they go
[3972.48s -> 3976.88s]  okay so the idea is that the local thread pushes and pops from the bottom of this queue
[3976.88s -> 3981.28s]  from the tail remote threads steal from the top so they stay out of each other's way and
[3981.28s -> 3987.12s]  they steal big amounts of work okay and as time goes on notice that each of these threads just
[3987.12s -> 3990.96s]  by doing the recursion are actually going to build up their own work queues and so they're
[3990.96s -> 3996.16s]  going to have tons of stuff to do and until one of them goes idle there is no more synchronization
[3996.16s -> 4002.64s]  again so compare this to a single work queue implementation where every single time i run out
[4002.64s -> 4008.16s]  of work i have to go take that shared variable to go get the next thing now i just go look at
[4008.16s -> 4013.60s]  my local you know my own local my diff my own local queue and i can avoid synchronization with
[4013.60s -> 4019.20s]  the other threads okay and let me just continue this thread the slide out a little bit and we
[4019.20s -> 4027.04s]  we've worked yeah sorry i shouldn't have built that okay so does it make sense cool yeah
[4032.88s -> 4035.44s]  so let's say that thread zero finishes all of its work
[4036.72s -> 4037.76s]  oh the current task yeah
[4037.76s -> 4040.88s]  and now everyone wants to get something to get the next one we'll just grab this
[4040.88s -> 4045.60s]  so to grab that it has to make sure that no other thread does not there are ways to be
[4045.60s -> 4050.24s]  careful because i know that i thread zero know that i'm only going to be accessing this side
[4050.24s -> 4054.48s]  of the data structure and other threads are going to be asking the other side of the data
[4054.48s -> 4059.12s]  structure there are very fast data structures where you can you know that can help you with
[4059.60s -> 4061.68s]  correct yes yeah
[4066.72s -> 4069.84s]  let's just say thread through goes idle for any reason at all it gets done yep
[4071.28s -> 4075.52s]  from zero oh boy great question so now there are multiple queues
[4076.32s -> 4079.68s]  and we have some options so is it correct to steal from any one of them
[4082.80s -> 4087.76s]  why not well you get the right answer what i mean by correct you're actually going to get
[4087.76s -> 4092.24s]  the right answer as long as if your idols still work and start doing it the question is what's
[4092.24s -> 4097.20s]  the most efficient thing to scale to steal from it would be whichever has the most work
[4098.24s -> 4102.40s]  you you could say that there's a you know maybe you should steal from the busiest
[4102.40s -> 4108.24s]  busiest thing it turns out that theoretically asymptotically it is optimal to randomly select
[4108.24s -> 4112.64s]  a queue and steal from it and that's been proven like it there are proofs to say that if
[4112.64s -> 4117.76s]  you just randomly select a victim you are within a constant of theoretically optimal
[4118.48s -> 4121.68s]  so you could probably improve on that but if you improve on that you're only
[4122.48s -> 4126.96s]  improving your constants and it's actually a little bit trickier because if everybody makes that
[4126.96s -> 4132.00s]  same decision to steal from the same thread all of a sudden that steal might get a little
[4132.00s -> 4139.20s]  bit more expensive because it could be it could be contention it also might mean that like
[4139.20s -> 4143.60s]  when i look at the system i'm like wow this guy has a lot of work by the time other threads
[4143.60s -> 4146.80s]  make the same decision some of that work might be gone and you might be stealing from someone
[4146.80s -> 4150.48s]  that has a very small amount of work so it turns out that the simplest thing to do is
[4150.48s -> 4157.12s]  randomly pick a queue that has work and steal from it so just a few other things like some
[4157.12s -> 4165.12s]  elegance of this policy um let's go back to my for loop example that i started this with
[4165.12s -> 4170.40s]  for i equals zero to n essentially this is like work that says for every element of an array do
[4170.40s -> 4176.56s]  some stuff in parallel i'd like you to convince yourself offline that this code that i wrote
[4177.84s -> 4183.84s]  will sequentially generate asynchronous work either the main thread
[4185.68s -> 4192.08s]  serially spawns all these things or the continuation bounces back and forth is constantly
[4192.08s -> 4199.12s]  getting stolen so this is what i want you to think about offline that code there will serially
[4199.12s -> 4204.16s]  create the reveal the parallel work which is actually not good because i have all these threads
[4204.16s -> 4209.84s]  and i'd like to really quickly reveal all the work a much better way to write this for loop
[4210.64s -> 4217.44s]  is actually this divide and conquer while loop which says here's an array of n things let's
[4217.44s -> 4222.16s]  recurse into doing the first half and the second half then let's recurse into doing the first
[4222.16s -> 4226.96s]  quad you know the first quarter and the second quarter and so on and so on this allows half
[4226.96s -> 4232.24s]  the array to get stolen and then it splits up like quicksort and you get parallel generation
[4232.24s -> 4237.44s]  of the independent work so actually if you see like silk actually has like a silk for
[4237.44s -> 4242.00s]  concept in it so if you go like silk for i equals zero to n under the hood they're
[4242.00s -> 4246.56s]  going to implement it that way because it's the scheduler will ultimately be able to create work
[4246.56s -> 4252.48s]  faster and you get to running in parallel much faster it's actually kind of a core artifact of
[4252.48s -> 4257.76s]  stuff okay so in the last three or four minutes i'm just going to kind of give you a quick
[4257.76s -> 4261.68s]  sketch of the last little piece of this how do we implement sink
[4264.32s -> 4267.60s]  that's the last piece of this like up until now it's been pretty simple
[4268.56s -> 4272.48s]  but what sink means i have to go check to see if all this work that's been spawned
[4273.04s -> 4276.72s]  which might have been stolen by other threads i need to know if it's complete
[4277.76s -> 4282.96s]  so there's some more bookkeeping that i have to do okay so let's let's think about quicksort
[4282.96s -> 4287.60s]  again um or no let's think about the for loop again because it's easier to think about
[4287.60s -> 4295.12s]  let's imagine that we're in some state where i am working on foo nine someone else is working
[4295.12s -> 4298.96s]  on six seven eight or basically at the end of the iteration and the only thing left is like
[4299.92s -> 4305.52s]  the rest of the loop like basically finish the loop and go on bar so in this case let's just say
[4305.52s -> 4314.16s]  that uh one of the workers finishes what do you think's gonna happen the next time a worker
[4314.16s -> 4323.84s]  finishes work just apply the rules that we've already talked about the next the next thread
[4324.00s -> 4328.80s]  that finishes will go idle it will select something to steal in this case it will
[4328.80s -> 4333.76s]  steal the thread zero continuation that continuation basically says it's done with
[4333.76s -> 4343.92s]  the for loop so what happens next yeah it'll join on all it needs to join on all the other
[4343.92s -> 4348.24s]  threads right so if this was implemented sequentially the join is trivial you don't
[4348.32s -> 4353.92s]  have to do anything um but it needs to know that everybody's done okay all right so let's take a
[4353.92s -> 4358.56s]  look at this so what i'm going to do is i'm going to say that i'm going to call this like this
[4358.56s -> 4362.96s]  all this area before the sink just a basic block and i'm just going to give it a name i'm
[4362.96s -> 4368.88s]  going to get the name a and then there's this so this is the sink for all calls spawned within
[4368.88s -> 4379.52s]  block a just to give give things a name so what's going on here is i have thread zero is working
[4379.52s -> 4384.64s]  on foo nine and it has continuation here okay it's the rest of block a
[4389.76s -> 4396.96s]  and if there's if this item has never been stolen then what do we do
[4398.88s -> 4406.00s]  like i at thread zero i put this on my queue if it's never been stolen i know that nobody
[4406.00s -> 4410.96s]  else is doing anything related to this so sink is just a no up you just continue
[4412.00s -> 4417.92s]  so the only thing you have to do something clever on is when it actually gets stolen so
[4417.92s -> 4422.00s]  let's think about that so let's go back to the very beginning here and say the very
[4422.00s -> 4427.12s]  beginning i'm working on boo zero and the continuation is the rest of the loop for block a
[4428.00s -> 4434.80s]  and at some point the thing may get stolen and if it gets stolen so it's going to have to keep
[4434.80s -> 4439.52s]  a note off to the side so there's this note of here are the things that have gotten stolen
[4439.52s -> 4444.16s]  from your work queue okay now what i'm not talking about here is how to manage the
[4444.16s -> 4448.88s]  the locking and and and unlocking of all these shared data structures but there's the same hey
[4449.12s -> 4456.80s]  hey look one worker has stolen work that's related to block a and zero works remote
[4456.80s -> 4463.44s]  workers have reported that they have finished work for block a so in this case this this
[4463.44s -> 4469.68s]  field is now just empty because it's been stolen and what thread one has the continuation
[4469.68s -> 4477.92s]  of block a so thread one starts doing its job and at some point so now thread one starts
[4477.92s -> 4481.68s]  working on foo one and my reference count of spawn things is two
[4484.48s -> 4488.96s]  and so on and so on and so on as more threads steal now my reference count is three
[4490.96s -> 4497.84s]  okay now some thread finishes some work from block a block a has been stolen so when we
[4497.84s -> 4503.76s]  finish work we have to update the done counter so that means three different threads are working
[4503.76s -> 4508.40s]  on a or haven't worked on on sub work from a one of those things is done
[4510.40s -> 4514.48s]  and at this point thread zero will go steal some more work and so on and so on
[4515.20s -> 4521.44s]  and start working on foo three and now let's get to the very end of that for loop when we're on
[4521.44s -> 4529.20s]  spawn 10 we're done with nine of these things these threads have no work to do because they're
[4529.20s -> 4537.36s]  stuck on oh they have no work to do thread two seems to be doing the last iteration of the
[4537.36s -> 4545.36s]  for loop when it gets done it says oh that spawn is complete or that block is complete
[4546.24s -> 4552.40s]  and now it can move on to working on bar so there's just the the sink is going to basically
[4552.40s -> 4557.92s]  it basically has to keep a record of all of the spawns related to that block whenever you
[4557.92s -> 4564.64s]  complete work from some stolen block you update your your reference counts and whichever thread
[4564.64s -> 4570.88s]  finishes the last piece of work we'll just continue on with the the continuation at that
[4570.88s -> 4576.80s]  time okay so this is kind of called greedy join scheduling it means always all threads are
[4576.80s -> 4582.40s]  always attempting to steal so they're being greedy threads only go idle if there's nothing
[4582.40s -> 4591.04s]  in any queue and the worker thread that initiates the spawn so in that example i just gave you like
[4591.04s -> 4598.08s]  the original spawn was started on thread zero but the spawn was finished up by thread two
[4598.08s -> 4602.80s]  because it was the last thread to do work related to it so it says okay i'll just keep going with
[4602.80s -> 4607.76s]  this with this with this thread of control so so it's pretty pretty elegant it's actually a
[4607.76s -> 4613.12s]  fun thing to play around with the algorithms for scheduling are based on some pretty sound
[4613.12s -> 4616.96s]  theoretical guarantees so that if you know if you do some things randomly you're guaranteed
[4616.96s -> 4621.76s]  to be within a constant factor of the optimum schedule it allows you to write divide and conquer
[4621.76s -> 4626.80s]  programs almost as easily as if you weren't thinking about parallelism and under the hood
[4626.80s -> 4633.28s]  there's a scheduling policy that maximizes locality minimizes communication amongst all these
[4633.28s -> 4640.32s]  threads but still achieves a pretty good workload balance so this is much more advanced than what
[4640.32s -> 4644.32s]  you're expected to implement in your assignment too but i just thought usually people like to
[4644.32s -> 4650.72s]  know about it so this is a pretty common and distributed systems as well and other things
[4650.72s -> 4655.20s]  so cool all right we're done and i'll see you on thursday
