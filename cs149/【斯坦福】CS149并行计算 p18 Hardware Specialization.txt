# Detected language: en (p=1.00)

[0.00s -> 8.42s]  OK, so today we're going to continue our discussion
[8.42s -> 11.76s]  of energy efficient computing.
[11.76s -> 14.26s]  At the end of the last lecture, we talked about heterogeneity.
[14.26s -> 16.98s]  We said that the motivation for heterogeneity
[16.98s -> 21.14s]  is because you had different kinds of program
[21.14s -> 22.98s]  characteristics that could be exploited
[22.98s -> 26.78s]  by more specialized architectures, right?
[26.80s -> 31.58s]  And the key one that we've been focusing on so far
[31.58s -> 34.62s]  is the idea of data parallel computation
[34.62s -> 38.82s]  that can be exploited by architectures like GPUs.
[38.82s -> 42.92s]  And the key capability that you get
[42.92s -> 45.54s]  is high performance and, much more importantly,
[45.54s -> 46.38s]  energy efficiency.
[46.38s -> 50.38s]  And we're going to dig into energy efficiency
[50.38s -> 52.98s]  in a little more detail today and talk
[52.98s -> 56.76s]  about why this is such a pressing concern
[56.76s -> 58.96s]  in modern computing environments.
[58.96s -> 63.28s]  OK, so hardware specialization and algorithm
[63.28s -> 67.24s]  specific programming, which would be kind of the next step
[67.24s -> 70.90s]  beyond a heterogeneous compute environment,
[70.90s -> 73.00s]  would be one that's very specialized
[73.00s -> 75.08s]  for a particular application, right?
[75.08s -> 77.52s]  So energy efficient computing, right?
[77.52s -> 81.16s]  So we're constrained by energy today
[81.16s -> 84.60s]  because of the state of the underlying
[84.60s -> 86.12s]  semiconductor technology, right?
[86.16s -> 88.88s]  So there was a time when, as you
[88.88s -> 94.04s]  got new generations of processing technology,
[94.04s -> 97.16s]  every generation gave you more transistors,
[97.16s -> 100.44s]  but those transistors dissipated less power, right?
[100.44s -> 102.84s]  And so you could get more performance
[102.84s -> 105.08s]  for the same amount of power.
[105.08s -> 107.00s]  This was called Dennard scaling.
[107.00s -> 109.16s]  So that ended about 10 years ago.
[109.16s -> 112.68s]  So now, every time you increase the number of transistors,
[112.68s -> 113.96s]  you dissipate more power.
[113.96s -> 115.72s]  And now you're constrained, right?
[115.80s -> 119.48s]  So we're in this power energy constrained environment.
[119.48s -> 122.40s]  And to kind of understand how this works, right?
[122.40s -> 127.32s]  So energy is power times time.
[127.32s -> 129.60s]  So the amount of power we have is fixed.
[129.60s -> 132.88s]  I don't know why this is going on its own.
[132.88s -> 137.32s]  So we're at this point where, in order
[137.32s -> 139.92s]  to increase performance, right, we
[139.92s -> 145.48s]  have to decrease the amount of energy per operation, right?
[145.60s -> 147.76s]  So this is fundamentally where we are,
[147.76s -> 152.24s]  where given a fixed amount of power we can dissipate,
[152.24s -> 154.04s]  if we want more performance, then we
[154.04s -> 156.80s]  have to improve our energy efficiency.
[156.80s -> 160.48s]  And the key mechanism for increasing energy efficiency
[160.48s -> 162.96s]  is to become more specialized, to get rid
[162.96s -> 166.08s]  of the excess power we dissipate
[166.08s -> 170.48s]  by doing things that don't focus on moving
[170.48s -> 172.52s]  the computation forward.
[172.56s -> 177.88s]  All right, so why are we energy constrained?
[177.88s -> 180.72s]  Across the computing landscape, we've got energy constraints,
[180.72s -> 181.24s]  right?
[181.24s -> 183.32s]  So if you're thinking about supercomputers, where
[183.32s -> 191.24s]  you've got hundreds of thousands of cores in the data center,
[191.24s -> 195.68s]  and you have to supply power and cooling
[195.68s -> 200.88s]  to keep all the systems running, and this, of course,
[200.88s -> 203.32s]  has a huge energy cost, right?
[203.32s -> 207.36s]  So you're constrained in large supercomputer environments.
[207.36s -> 209.92s]  You're also constrained in data centers
[209.92s -> 217.24s]  behind the large websites, like Google and Facebook.
[217.24s -> 221.68s]  There, again, the cost of supplying the energy
[221.68s -> 229.72s]  for powering and cooling the computing, possibly over time,
[229.76s -> 233.00s]  over, say, a three-year lifetime of the computing
[233.00s -> 236.80s]  resources, is more than the cost of actually acquiring
[236.80s -> 238.28s]  the computation.
[238.28s -> 239.80s]  When we talk about mobile devices,
[239.80s -> 241.88s]  you're energy constrained because, of course, you
[241.88s -> 243.92s]  have no fan in your mobile device,
[243.92s -> 246.44s]  because a fan would be inconvenient.
[246.44s -> 251.24s]  And so the heat dissipation has to happen passively.
[251.24s -> 253.16s]  And then, of course, you have a battery
[253.16s -> 258.40s]  that has to provide the power, the energy, to the compute.
[258.44s -> 260.64s]  And so there, you're also energy constrained.
[260.64s -> 262.60s]  So across the computing landscape,
[262.60s -> 263.76s]  you're energy constrained.
[263.76s -> 265.96s]  So this is the equation we were looking at.
[265.96s -> 270.40s]  So energy, we said, is power times time, right?
[270.40s -> 277.48s]  And power was fixed for semiconductor processing
[277.48s -> 278.56s]  constraints.
[278.56s -> 282.00s]  And so if we wanted to improve performance,
[282.00s -> 284.68s]  then we had to become more energy efficient.
[284.68s -> 288.36s]  And the way to do that is to do specialized functionality that
[288.36s -> 289.84s]  reduces the overhead.
[289.84s -> 293.72s]  And the question is, what is the magnitude of this improvement
[293.72s -> 295.72s]  that you can get from specialization
[295.72s -> 298.08s]  over a general purpose processing
[298.08s -> 300.40s]  environment composed of CPUs?
[300.40s -> 302.08s]  So let's dig into that, right?
[302.08s -> 306.56s]  And so we've already looked at specializing
[306.56s -> 313.24s]  for data parallel applications using GPUs at large scale.
[313.24s -> 318.60s]  And of course, within the architecture of a GPU,
[318.60s -> 321.08s]  we see SIMD processing, which is also
[321.08s -> 323.56s]  exploiting data parallelism.
[323.56s -> 327.00s]  And so the rules of thumb are that you
[327.00s -> 328.56s]  can get a tremendous improvement.
[328.56s -> 334.08s]  We'll come back to this about what improvement we can get.
[334.08s -> 338.44s]  But the question is, we spent a lot of time
[338.44s -> 340.84s]  talking in this class about how to get the most
[340.88s -> 343.72s]  performance you can for particular algorithms
[343.72s -> 348.96s]  from a modern CPU and GPUs, right?
[348.96s -> 353.48s]  So the question is, why are CPUs so fundamentally
[353.48s -> 354.64s]  inefficient?
[354.64s -> 357.48s]  So let's look at this, right?
[357.48s -> 361.32s]  So if you look at the energy dissipated
[361.32s -> 367.04s]  in executing an instruction, right, say a multiply, add,
[367.08s -> 370.44s]  where you'll see that most of the energy
[370.44s -> 373.52s]  actually does not go into performing
[373.52s -> 375.44s]  the actual computation, right?
[375.44s -> 378.16s]  So in this case, it's 6%.
[378.16s -> 382.64s]  And the rest of the energy is spent
[382.64s -> 385.40s]  dealing with the instruction and figuring out
[385.40s -> 388.44s]  what the instruction is going to do,
[388.44s -> 393.12s]  fetching and dealing with the data, moving the data.
[393.12s -> 398.08s]  And the overhead of controlling the circuitry
[398.08s -> 401.00s]  and distributing the clock, which of course
[401.00s -> 403.92s]  keeps everything synchronous, right?
[403.92s -> 408.00s]  So if you look at all of the things
[408.00s -> 410.12s]  that one has to do to execute instruction,
[410.12s -> 411.60s]  you've got to read the instruction.
[411.60s -> 414.08s]  You've got to figure out what the instruction is going to do.
[414.08s -> 416.48s]  You've got to check to see how the instruction is
[416.48s -> 418.96s]  dependent on other instructions that are being executed.
[418.96s -> 421.38s]  You've got to figure out whether the resource that you want
[421.38s -> 424.98s]  to use to execute this instruction is available.
[424.98s -> 427.98s]  You've got to figure out where the operands are.
[427.98s -> 429.94s]  You've got to fetch them from the register file.
[429.94s -> 432.28s]  You've got to, if this happens to be a loader or store,
[432.28s -> 434.82s]  you may have to move data from caches.
[434.82s -> 440.30s]  And then way down here is the actual perform
[440.30s -> 442.02s]  the arithmetic operation.
[442.02s -> 444.90s]  And then you still have to move the results, right?
[444.90s -> 446.54s]  And so at the end of the day,
[446.54s -> 448.26s]  you end up spending very little
[448.26s -> 451.18s]  of the energy for a particular instruction.
[451.18s -> 455.14s]  So the question is, how can you make this situation better?
[455.14s -> 460.34s]  So how does how does SIMD make this pie chart look better?
[460.34s -> 460.94s]  Yeah?
[460.94s -> 464.94s]  You have better reach the instructions to be perfect
[464.94s -> 467.78s]  because you want to push more instructions in particular.
[467.78s -> 468.42s]  Right, right.
[468.42s -> 471.94s]  So you amortize all of the parts that are not
[471.94s -> 475.06s]  green over more green stuff, right?
[475.06s -> 479.86s]  So you are executing across more data elements, right?
[479.86s -> 481.86s]  And the width of your SIMD is going
[481.86s -> 485.38s]  to tell you how efficient, potentially, you can be, right?
[485.38s -> 493.18s]  And so if I can do eight data operations in one instruction,
[493.18s -> 499.50s]  why don't I do 16 or 32 or 64?
[499.50s -> 500.46s]  Yeah?
[500.46s -> 502.78s]  Because you partly realize why the SIMD is so good.
[502.78s -> 503.94s]  Right, exactly, right?
[503.94s -> 507.50s]  The wider you make it, your peak is great,
[507.50s -> 511.54s]  but your average is going to be a lot worse because you may not
[511.54s -> 516.22s]  be able to fill up all of those SIMD slots, right?
[516.22s -> 518.70s]  And so this is the question, right?
[518.70s -> 520.34s]  Is that, hey, you can do better,
[520.34s -> 523.74s]  and you might go to extremes, but ultimately, you're
[523.74s -> 527.66s]  not going to see the data parallelism or the SIMD data
[527.66s -> 531.66s]  parallelism that you need to keep all the SIMD units busy.
[531.66s -> 534.78s]  All right, so the question is, does SIMD improve things?
[534.78s -> 535.42s]  Well, it does.
[535.66s -> 539.02s]  This is a study from a few years back
[539.02s -> 541.22s]  that was done at Stanford that looked
[541.22s -> 554.18s]  at how much energy gets consumed in a SIMD-enabled CPU
[554.18s -> 558.62s]  for H.264 video encoding, which
[558.62s -> 563.30s]  is a pretty data parallel SIMD-friendly application.
[563.30s -> 569.22s]  And you see that the components of SIMD energy
[569.22s -> 572.10s]  shown by the red boxes is not that high.
[578.62s -> 582.22s]  So the question is, if you want to do better,
[582.22s -> 585.38s]  then you need to think about actually implementing
[585.38s -> 588.50s]  more specialized components, right?
[588.50s -> 593.46s]  And so what we want to look at is
[593.46s -> 598.18s]  to look at some other types of architecture.
[598.18s -> 600.58s]  You see that the best, in this case,
[600.58s -> 603.54s]  for doing fast Fourier transform, which
[603.54s -> 608.66s]  is the core of many signal processing applications, right?
[608.66s -> 613.38s]  And it's a really well-studied algorithm.
[613.38s -> 617.58s]  Some people call it the most important algorithm ever.
[617.62s -> 621.62s]  And so you can think about implementing specialized hardware
[621.62s -> 622.54s]  for that.
[622.54s -> 625.06s]  And you can get a tremendous improvement
[625.06s -> 628.38s]  in terms of the use of your silicon area
[628.38s -> 630.74s]  and the energy efficiency, right?
[630.74s -> 634.46s]  So in this case, this is a fairly old study, 40 nanometers,
[634.46s -> 637.66s]  which is ancient.
[637.66s -> 645.38s]  But what you see is that the ASIC, which is this diamond,
[645.38s -> 652.82s]  you can see these diamonds then represent, in terms of,
[652.82s -> 657.98s]  compared to a CPU, which is the core I7.
[657.98s -> 660.78s]  So the CPU is the core I7, which
[660.78s -> 663.86s]  is the lowest in terms of gigaflops
[663.86s -> 666.06s]  per millimeter squared.
[666.06s -> 670.46s]  And the star ASIC is the highest, right?
[670.46s -> 674.82s]  So I reverse what I was saying a moment ago.
[674.82s -> 677.38s]  And so the diamonds, which is the CPU,
[677.38s -> 680.66s]  gives you the lowest energy efficiency and the lowest
[680.66s -> 683.18s]  use of the chip area.
[683.18s -> 688.30s]  And basically, it's a factor of 1,000
[688.30s -> 690.98s]  in terms of the use of the chip area
[690.98s -> 695.38s]  and a factor of 100th in terms of energy efficiency
[695.38s -> 697.46s]  that you get with a CPU versus something
[697.46s -> 701.38s]  that's very specialized for a particular algorithm.
[701.38s -> 704.74s]  So what's the downside of the ASIC approach?
[704.74s -> 707.22s]  Yeah?
[707.22s -> 709.92s]  You can only use it for that one algorithm,
[709.92s -> 711.62s]  and you have to design it, right?
[711.62s -> 714.58s]  And so if you want to get your application going
[714.58s -> 716.38s]  and you've got a new idea, are you
[716.38s -> 719.38s]  going to wait 18 months to go design an ASIC?
[719.38s -> 723.98s]  And then, well, it better be a really important algorithm,
[723.98s -> 726.34s]  like FFT.
[726.34s -> 731.50s]  So in order to justify ASIC implementations.
[731.50s -> 733.62s]  But there are other ways of getting more efficiency
[734.10s -> 735.02s]  than CPUs.
[735.02s -> 738.54s]  One of the ideas that gets used extensively
[738.54s -> 740.82s]  is this idea of digital signal processes.
[740.82s -> 742.18s]  And it's the idea that, hey, you
[742.18s -> 745.30s]  want to do a lot of processing of signals
[745.30s -> 751.06s]  using DSP algorithms like FFT and filtering, IR filtering.
[751.06s -> 754.42s]  And it turns out that the instructions and the addressing
[754.42s -> 759.54s]  modes in general purpose computers can be improved upon.
[759.54s -> 761.10s]  So that's what DSPs do.
[761.10s -> 763.58s]  They're very complex instructions that
[763.58s -> 766.26s]  do just what you want for specific algorithms.
[766.26s -> 770.14s]  They have very complex addressing modes
[770.14s -> 774.72s]  that allow you to do the bit reversed addressing that you
[774.72s -> 776.98s]  need for FFT, for example.
[776.98s -> 780.16s]  Now the question is, if I gave you this complex instruction
[780.16s -> 782.54s]  set, could you write a compiler for it?
[782.54s -> 784.22s]  And the answer is probably no.
[784.22s -> 789.42s]  So along with the very complex instructions
[789.42s -> 795.90s]  that these DSPs have come with low level programming
[795.90s -> 797.44s]  that has to be done for implementing
[797.44s -> 798.78s]  all these algorithms.
[798.78s -> 802.98s]  But it's a hell of a lot easier than developing an ASIC.
[802.98s -> 806.66s]  But it's also much more difficult than programming
[806.66s -> 808.18s]  a general purpose CPU.
[808.18s -> 809.66s]  So there are these trade-offs now
[809.66s -> 815.30s]  between efficiency and programmability that you get.
[815.30s -> 818.66s]  Another example of a specialized compute unit
[818.66s -> 821.38s]  is one that was developed by D. E. Shaw.
[821.38s -> 824.94s]  So D. E. Shaw made a bunch of money in the financial area
[824.94s -> 827.78s]  and decided that he was going to spend some of that money
[827.78s -> 830.58s]  doing things for humanity.
[830.58s -> 833.50s]  And one of the things he decided to do was develop
[833.50s -> 839.58s]  a specialized accelerator for molecular dynamics.
[839.58s -> 843.94s]  So if you want to understand how proteins fold,
[843.94s -> 846.78s]  it comes down to figuring out the interaction
[846.78s -> 850.38s]  between molecules, right?
[850.38s -> 856.30s]  And so molecular dynamics is an important area in chemistry.
[856.30s -> 859.34s]  People have won Nobel prizes for it and so on.
[859.34s -> 863.66s]  And so he developed this accelerator called ANTON.
[863.66s -> 869.94s]  And by carefully designing the algorithm with the hardware,
[869.94s -> 872.94s]  they got tremendous performance improvements
[872.94s -> 875.50s]  over CPUs and GPUs, right?
[875.54s -> 877.74s]  So you want to do an M-body simulation.
[877.74s -> 881.74s]  Given M-bodies, figure out the interaction between them.
[881.74s -> 885.90s]  And they've got specialized hardware for doing that.
[885.90s -> 889.50s]  And I think they've got three generations of ANTON
[889.50s -> 890.38s]  at this point.
[890.38s -> 894.18s]  And each one is better.
[894.18s -> 896.22s]  So the aside is, of course, there
[896.22s -> 898.66s]  are ways of doing this with accelerators.
[898.66s -> 900.66s]  But there are also ways of solving the problem
[900.66s -> 901.46s]  statistically.
[901.46s -> 904.06s]  And so there was this tension between the group of people
[904.06s -> 906.02s]  who are doing accelerators and a group of people
[906.02s -> 912.18s]  who are doing broad scale statistical approaches.
[912.18s -> 916.98s]  These days, machine learning is all the rage,
[916.98s -> 918.34s]  as you may have noticed, right?
[918.34s -> 921.78s]  So in fact, your last programming assignment
[921.78s -> 924.50s]  was around machine learning.
[924.50s -> 927.22s]  One of the accelerators that kicked off
[927.22s -> 931.62s]  a lot of the interest in developing new architectures
[931.62s -> 934.98s]  for machine learning was the accelerator from Google
[934.98s -> 936.74s]  called the Tensor Processing Unit.
[936.74s -> 938.98s]  And the way to think about the Tensor Processing Unit
[938.98s -> 945.50s]  is that it made dense matrix multiply go very fast, right?
[945.50s -> 949.30s]  But dense matrix multiplies that are large, like 128.
[949.30s -> 954.82s]  Well, it started out at 256 by 256 integer matrix multiplies.
[954.82s -> 960.10s]  And then future TPU versions had a,
[960.34s -> 962.06s]  they went to 128 by 128.
[962.06s -> 967.50s]  But then you were doing a 16-bit floating point
[967.50s -> 969.58s]  multiplies.
[969.58s -> 975.06s]  But lots of, this is a few years old
[975.06s -> 978.30s]  in terms of the citations, but lots
[978.30s -> 982.74s]  of work in the architecture area
[982.74s -> 984.94s]  to try and understand how to develop
[984.94s -> 989.70s]  new specific architectures for the machine learning domain.
[989.82s -> 993.14s]  So most of these architectures are, in fact,
[993.14s -> 995.06s]  somewhat programmable because you
[995.06s -> 1000.26s]  need to adapt to the changes in the machine learning
[1000.26s -> 1001.86s]  algorithms.
[1001.86s -> 1004.34s]  But fundamentally, they are focused
[1004.34s -> 1012.14s]  on doing the core compute kernel in these ML algorithms,
[1012.14s -> 1014.02s]  which is matrix multiply, right?
[1014.02s -> 1015.02s]  It could be dense.
[1015.02s -> 1017.22s]  Most of the case, it's dense.
[1017.22s -> 1023.34s]  But there are sparse versions that are interesting, too.
[1023.34s -> 1027.66s]  All right, so there's this issue
[1027.66s -> 1033.74s]  of doing hardware that is fixed
[1033.74s -> 1035.58s]  for a particular algorithm.
[1035.58s -> 1037.94s]  And the question is, is there a middle ground
[1037.94s -> 1041.02s]  that will allow you to develop hardware that
[1041.02s -> 1043.34s]  is somewhat programmable, right?
[1043.38s -> 1047.86s]  So this is the whole reason and motivation
[1047.86s -> 1053.06s]  for hardware architectures that are called
[1053.06s -> 1054.66s]  field programmable gate arrays.
[1054.66s -> 1056.42s]  And some of you, of course, may
[1056.42s -> 1060.86s]  have played with this sort of technology
[1060.86s -> 1063.50s]  in a digital design class.
[1063.50s -> 1065.30s]  And the key idea is that you've
[1065.30s -> 1068.82s]  got a bunch of what are called configurable logic
[1068.82s -> 1074.54s]  blocks, which are basically lookup tables for Boolean
[1074.54s -> 1075.54s]  algebra, right?
[1075.54s -> 1080.78s]  So it will give you some function
[1080.78s -> 1081.86s]  of some number of inputs.
[1081.86s -> 1086.62s]  In this case, a four input Boolean function
[1086.62s -> 1089.18s]  can be computed using the lookup table.
[1089.18s -> 1093.82s]  And then combined with a combinational block
[1093.82s -> 1097.30s]  is a register, which gives you storage, right?
[1097.30s -> 1100.38s]  And so then you put these kinds
[1100.38s -> 1102.98s]  of configurable logic blocks in an array,
[1102.98s -> 1104.66s]  and you connect them together.
[1104.66s -> 1112.50s]  And then you can connect them into more complex logic blocks.
[1112.50s -> 1116.02s]  For instance, if you had a six input lookup table
[1116.02s -> 1120.98s]  and you wanted to generate a 40 input AND gate,
[1120.98s -> 1126.14s]  you could cascade these six input logic blocks together
[1126.14s -> 1130.58s]  to create a more complex function, OK?
[1130.58s -> 1132.38s]  So the lookup table basically just
[1132.38s -> 1136.58s]  maps a binary number to an output
[1136.58s -> 1139.38s]  and allows you to compute functions
[1139.38s -> 1141.14s]  of a different variety.
[1141.14s -> 1148.78s]  So modern FBJs combine the configurable logic blocks
[1148.78s -> 1156.34s]  with more dedicated functions, such as dense memory
[1156.34s -> 1160.78s]  and also multipliers of what are called DSP blocks, right?
[1160.78s -> 1164.70s]  So the problem with constructing everything out
[1164.70s -> 1167.14s]  of configurable logic blocks is it
[1167.14s -> 1169.74s]  gives you the most flexibility, but it turns out
[1169.74s -> 1171.18s]  there's a lot of overhead, right?
[1171.18s -> 1174.78s]  There's the overhead in connecting these blocks together,
[1174.78s -> 1177.34s]  and there's the overhead of actually implementing
[1178.22s -> 1182.94s]  the compute elements using this CLB source of technology.
[1182.94s -> 1186.82s]  And so if you want to have a more dense, more efficient use
[1186.82s -> 1189.58s]  of the silicon area, then you
[1189.58s -> 1194.82s]  want these hard macro blocks for memory
[1194.82s -> 1199.82s]  and for multiplication, right?
[1199.82s -> 1203.62s]  And so you can combine these together.
[1203.62s -> 1207.66s]  And then if you actually want to use them,
[1207.66s -> 1210.54s]  you could come visit my lab.
[1210.54s -> 1215.02s]  I could show you how to access them.
[1215.02s -> 1220.34s]  Or you could go to Amazon EC2, and they also
[1220.34s -> 1222.74s]  provide FPGA resources, right?
[1222.74s -> 1227.06s]  And so they've got some quite advanced FPGA capabilities
[1227.06s -> 1231.78s]  that you can access using cloud services.
[1231.78s -> 1239.38s]  And then these have both, of course, links to memory, DDR4.
[1239.38s -> 1241.82s]  We haven't said a lot about memory,
[1241.82s -> 1244.66s]  but maybe if we have time at the end of this lecture,
[1244.66s -> 1248.54s]  we can talk about the different kinds of memory technologies,
[1248.54s -> 1254.78s]  interfaces to CPUs through PCIe, and links to other FPGAs.
[1254.78s -> 1258.14s]  And then they have a whole environment
[1258.14s -> 1262.98s]  that allows you to do the software development in order
[1262.98s -> 1265.82s]  to actually program these FPGAs.
[1265.82s -> 1273.06s]  So you're looking across the spectrum here from easiest
[1273.06s -> 1278.06s]  to program, general purpose CPU to ASIC, right?
[1278.06s -> 1282.42s]  We see this trade-off between energy efficiency
[1282.42s -> 1284.22s]  and programmability, right?
[1284.22s -> 1288.30s]  Across the space of computing technologies
[1288.30s -> 1292.02s]  that you could apply, and as a system designer,
[1292.02s -> 1295.02s]  you need to pick the right one, which
[1295.02s -> 1298.74s]  is what are the constraints that you have in terms
[1298.74s -> 1300.86s]  of the energy efficiency you need
[1300.86s -> 1303.54s]  and how quickly you need to get your application
[1303.54s -> 1307.06s]  or how much effort you're willing to spend to get
[1307.06s -> 1308.58s]  your application working, right?
[1308.58s -> 1311.50s]  So you can imagine that if you
[1311.50s -> 1313.98s]  can get the performance you need with a CPU,
[1314.38s -> 1318.86s]  just go do it and program your application using
[1318.86s -> 1322.26s]  a high-level language.
[1322.26s -> 1324.42s]  If you need more performance, then you
[1324.42s -> 1326.70s]  keep going to the right.
[1326.70s -> 1329.46s]  GPUs, you may have to write some CUDA.
[1329.46s -> 1334.14s]  DSP, you might have to do assembly language programming.
[1334.14s -> 1336.26s]  Domain-specific compute, well, this
[1336.26s -> 1340.22s]  might work quite well if your domain is something
[1340.22s -> 1342.38s]  like machine learning, and you can program
[1342.38s -> 1345.98s]  this accelerator using a framework like PyTorch
[1345.98s -> 1347.74s]  or TensorFlow.
[1347.74s -> 1348.90s]  That might work.
[1348.90s -> 1353.18s]  If you have to do an FPGA, then you basically
[1353.18s -> 1355.02s]  have to become a hardware designer,
[1355.02s -> 1358.66s]  and ASIC is definitely you're a hardware designer, right?
[1358.66s -> 1361.18s]  So as you move to the right, you
[1361.18s -> 1365.26s]  get more energy efficiency, dramatically more energy
[1365.26s -> 1367.90s]  efficient if you do an ASIC, but then you
[1367.90s -> 1369.90s]  have to work much harder, and you've
[1369.90s -> 1372.10s]  got to spend a lot more money in order
[1373.10s -> 1376.66s]  to move to the right, especially if you
[1376.66s -> 1379.14s]  move all the way to the right.
[1379.14s -> 1381.54s]  OK, any questions so far on sort
[1381.54s -> 1387.82s]  of the space of trade-offs between energy efficiency
[1387.82s -> 1391.78s]  and programmability, the different points in the space?
[1396.74s -> 1397.58s]  Yeah?
[1397.58s -> 1400.18s]  So obviously, if we're talking about basic,
[1400.18s -> 1402.66s]  it's like thousands, and it's much better.
[1402.66s -> 1406.34s]  But in the middle there, it's kind of an interesting value.
[1406.34s -> 1411.98s]  OK, you can get the TPU, but why not just get two GPUs,
[1411.98s -> 1413.86s]  and why bother?
[1413.86s -> 1417.70s]  It's probably way less expensive to get.
[1417.70s -> 1420.66s]  Wow, I mean, I think the jury is out, right,
[1420.66s -> 1423.46s]  between sort of whether, I mean,
[1423.46s -> 1428.10s]  clearly the GPU is taking pages out of the TPU, right?
[1428.10s -> 1431.10s]  It said, OK, it was this general purpose thread thing.
[1431.10s -> 1434.46s]  Oh, but, hold, let's put these tensor core units in
[1434.46s -> 1435.90s]  and make it more specialized.
[1435.90s -> 1438.10s]  But now, you as a CUDA programmer
[1438.10s -> 1440.42s]  try to program those tensor core units.
[1440.42s -> 1442.14s]  I mean, we didn't do that in this class,
[1442.14s -> 1444.06s]  but it's actually pretty challenging, right?
[1444.06s -> 1448.50s]  So the space is in flux, and it's
[1448.50s -> 1451.90s]  driven by this really high value application called
[1451.90s -> 1453.02s]  machine learning, right?
[1453.02s -> 1456.38s]  So everybody's kind of tilling their architecture
[1456.38s -> 1457.78s]  to exploit that.
[1457.78s -> 1459.02s]  Yeah, yeah?
[1459.02s -> 1462.90s]  Why is the programmable DSP higher than TPUs?
[1462.90s -> 1465.66s]  Because I think DSP can be used to be like more efficient,
[1465.66s -> 1466.58s]  like fun average.
[1466.58s -> 1472.46s]  Well, I mean, it tends to be more focused on DSP.
[1472.46s -> 1477.34s]  And it's got a lot of specialized addressing mechanisms
[1477.34s -> 1478.74s]  and compute for that.
[1478.74s -> 1482.18s]  So given that you're trying to do digital signal processing,
[1482.18s -> 1483.98s]  it's going to be more efficient.
[1483.98s -> 1485.90s]  But if you're trying to do machine learning,
[1485.90s -> 1487.74s]  probably not, yeah.
[1487.74s -> 1496.10s]  All right, so now let's kind of look
[1496.10s -> 1497.78s]  at sort of what it would take to move
[1497.78s -> 1500.14s]  to the right a little bit more, right?
[1500.14s -> 1501.94s]  So we spent a lot of time in this class
[1501.94s -> 1505.74s]  thinking about how we program the fixed set of resources
[1505.74s -> 1510.14s]  that we provide you in a existing architecture,
[1510.14s -> 1515.46s]  such as a general purpose processor or a GPU.
[1515.50s -> 1518.74s]  But now let's think a little bit about what
[1518.74s -> 1524.34s]  it would take to either specify or program
[1524.34s -> 1530.18s]  a accelerator, where you get to specify a bunch of things
[1530.18s -> 1532.50s]  that you don't get to control if you're
[1532.50s -> 1535.58s]  thinking about a general purpose environment.
[1535.58s -> 1540.10s]  In particular, you get to have some custom memory system,
[1540.10s -> 1540.58s]  right?
[1540.58s -> 1542.34s]  A lot of the performance improvement
[1542.34s -> 1546.46s]  you can get in any particular piece of hardware
[1546.46s -> 1550.34s]  has to do with how you organize the memory to exploit
[1550.34s -> 1555.14s]  the particular characteristics of locality and access behavior
[1555.14s -> 1558.02s]  that you see in your application.
[1558.02s -> 1561.38s]  And you get specialized compute that
[1561.38s -> 1564.06s]  matches what you need in your application, right?
[1564.06s -> 1566.98s]  So the question is, how do we think about
[1567.06s -> 1572.34s]  or how do we program specialized processes
[1572.34s -> 1573.42s]  or accelerators, right?
[1573.42s -> 1577.86s]  So traditionally, you had to become a hardware designer
[1577.86s -> 1581.02s]  and think about things at the level of what's
[1581.02s -> 1583.98s]  called the registered transfer level or the hardware
[1583.98s -> 1585.26s]  description level, right?
[1585.26s -> 1587.70s]  So you had to write in languages
[1587.70s -> 1589.86s]  like VHDL or Verilog.
[1589.86s -> 1592.26s]  How many people have written Verilog?
[1592.26s -> 1594.92s]  Oh, a good, fair number of you.
[1594.92s -> 1598.32s]  So you understand the pain involved in programming
[1598.32s -> 1600.04s]  at the Verilog level, right?
[1600.04s -> 1603.36s]  Now, recently, there has been this idea
[1603.36s -> 1605.28s]  called high-level synthesis.
[1605.28s -> 1609.02s]  And the approach is, hey, I'm going to write a C program.
[1609.02s -> 1613.48s]  And then I'm going to have some smart compiler convert that
[1613.48s -> 1615.12s]  into hardware.
[1615.12s -> 1617.12s]  There are two things wrong with this idea.
[1617.12s -> 1619.52s]  One is that C programs were not
[1619.52s -> 1622.32s]  intended to be descriptions of hardware, right?
[1622.32s -> 1625.76s]  So you have to make all kinds of inferences about what
[1625.76s -> 1628.76s]  the hardware should be doing because the C program was
[1628.76s -> 1631.12s]  designed for a general purpose processor.
[1631.12s -> 1633.00s]  It was not designed for hardware.
[1633.00s -> 1634.88s]  So that's the first problem.
[1634.88s -> 1637.18s]  The second problem is that in order
[1637.18s -> 1640.16s]  to kind of get around the deficiencies of C,
[1640.16s -> 1642.16s]  they put in these pragmas, right?
[1642.16s -> 1645.64s]  So you get to direct the compiler to do certain things.
[1645.64s -> 1648.56s]  The problem is putting in all these pragmas,
[1648.56s -> 1651.28s]  you essentially have to know a lot about hardware
[1651.28s -> 1653.32s]  to put in the pragmas in the right way.
[1653.32s -> 1655.88s]  And so you've kind of defeated the whole purpose
[1655.88s -> 1662.02s]  of kind of rising up to the level of C.
[1662.02s -> 1664.88s]  And then, in fact, in order to get anything
[1664.88s -> 1667.52s]  that's worthwhile and performs well,
[1667.52s -> 1670.20s]  you've got to kind of descend down to the level of hardware
[1670.20s -> 1672.92s]  by using these pragmas, OK?
[1672.92s -> 1676.98s]  So today, instead of kind of looking at high-level synthesis,
[1676.98s -> 1681.04s]  what we want to look at as a language that we call spatial,
[1681.04s -> 1683.88s]  which is a high-level language for designing hardware
[1683.88s -> 1689.68s]  accelerators that's designed to enable performance-oriented
[1689.68s -> 1692.28s]  programmers to specify hardware.
[1692.28s -> 1694.24s]  So everybody in this class at this point
[1694.24s -> 1696.48s]  is a performance-oriented programmer, right?
[1696.48s -> 1698.70s]  That's what you've been doing all quarter,
[1698.70s -> 1701.24s]  and so you guys qualify.
[1701.24s -> 1705.12s]  And so the key thing that performance-oriented programmers
[1705.12s -> 1709.84s]  like to think about is parallelism and locality.
[1709.84s -> 1713.24s]  This is what we've been dealing with in the whole quarter,
[1713.42s -> 1713.80s]  right?
[1713.80s -> 1717.20s]  And in terms of locality, we want to think maybe
[1717.20s -> 1719.06s]  about some specialized memories
[1719.44s -> 1720.76s]  and how you do the data movement.
[1720.76s -> 1722.52s]  So here's spatial.
[1722.58s -> 1730.22s]  So a quick one-slide description of spatial is it's designed
[1730.36s -> 1735.20s]  for it's a domain-specific language
[1735.28s -> 1736.60s]  for accelerator design.
[1737.48s -> 1741.30s]  And it has contracts to express parallel patterns,
[1741.30s -> 1743.00s]  which you're also pretty familiar with, right?
[1743.00s -> 1751.16s]  So data parallel patterns over collections, so map, zip,
[1751.16s -> 1755.32s]  reduce, these are all concepts you're familiar with.
[1755.56s -> 1757.52s]  And what we want to do is we want to think about how
[1757.52s -> 1761.72s]  to execute these parallel patterns using two types
[1761.72s -> 1764.68s]  of parallelism, one that you're very familiar with,
[1764.92s -> 1766.80s]  which is independent parallelism, right?
[1766.80s -> 1771.88s]  So thinking about taking a map and running the map
[1771.88s -> 1774.12s]  with independent computation units.
[1774.34s -> 1778.88s]  And the other is dependent parallelism, which I think some
[1778.88s -> 1780.66s]  of you are quite familiar with too, right?
[1780.96s -> 1784.20s]  Dependent parallelism is where you've got parallel units
[1784.20s -> 1785.82s]  that are dependent on each other.
[1786.18s -> 1790.52s]  So how would you execute dependent parallel units
[1790.56s -> 1799.72s]  at those of you who, how do you execute things,
[1799.98s -> 1809.22s]  how do you execute a set of computation
[1809.22s -> 1810.68s]  in which the components
[1810.68s -> 1812.82s]  of the computation are in fact dependent?
[1814.76s -> 1814.94s]  Yeah?
[1814.94s -> 1824.00s]  It's a concept we haven't talked about explicitly here.
[1824.20s -> 1827.66s]  So it's not something that you, unless you've heard about it
[1827.66s -> 1830.78s]  in some other context, maybe a hardware design context.
[1830.78s -> 1830.94s]  Yeah?
[1831.52s -> 1833.30s]  Would you like do kind of what you do
[1833.30s -> 1834.74s]  with the CPU instruction pipeline?
[1834.74s -> 1836.50s]  Exactly, right, right.
[1836.58s -> 1837.98s]  So the different components
[1837.98s -> 1840.52s]  of an instruction execution pipeline are all dependent.
[1840.82s -> 1842.80s]  But you've got a bunch of independent instructions
[1843.32s -> 1846.96s]  and you execute them the same way you would if you were doing
[1847.28s -> 1850.52s]  in a factory and you were working on a car.
[1850.68s -> 1853.38s]  You create an assembly line and each
[1853.38s -> 1856.18s]  of the stations do things independently
[1856.50s -> 1858.02s]  and then you get parallelism
[1858.16s -> 1860.86s]  across the different sections of the pipeline.
[1861.18s -> 1865.68s]  So pipelining is the other way of doing parallelism
[1865.80s -> 1868.54s]  where you've got dependencies, right?
[1868.54s -> 1872.26s]  So we want to look at how to do pipeline parallelism.
[1873.46s -> 1875.02s]  Parallel patterns can be nested
[1875.02s -> 1876.90s]  so you can get hierarchical control.
[1878.00s -> 1880.82s]  We said that one of the key mechanisms
[1880.82s -> 1886.70s]  that a hardware designer or somebody who wants
[1886.70s -> 1892.30s]  to control the locality or exploit locality
[1892.30s -> 1893.86s]  in the application is to be able
[1893.86s -> 1897.96s]  to explicitly specify the memory hierarchy
[1897.96s -> 1899.04s]  and how that gets used.
[1899.50s -> 1902.06s]  There's also this notion of being able to look
[1902.06s -> 1904.98s]  at the whole design space using parameters
[1905.64s -> 1908.14s]  and you want to expose these to the compiler
[1908.58s -> 1910.04s]  and allow the compiler
[1910.38s -> 1913.78s]  to potentially explore the design space for you.
[1915.10s -> 1920.60s]  So the key here is that let's focus on what is interesting
[1920.60s -> 1923.66s]  and important in terms of getting high performance,
[1924.02s -> 1926.06s]  which as we said repeatedly is
[1926.06s -> 1928.42s]  about how you exploit parallelism,
[1928.42s -> 1931.70s]  both independent parallelism and dependent parallelism
[1932.00s -> 1937.70s]  and how you manage and figure out the locality, right?
[1937.70s -> 1941.18s]  And I would claim that it's kind of more intuitive than thinking
[1941.18s -> 1945.08s]  about things from a thread level for these kinds
[1945.08s -> 1948.24s]  of applications that you might see
[1948.24s -> 1950.90s]  in a machine learning context.
[1951.48s -> 1956.68s]  All right, so let's talk about the spatial language
[1956.68s -> 1958.88s]  and let's start with the memory templates, right?
[1958.88s -> 1962.76s]  So as I said, you have this explicit memory hierarchy.
[1963.38s -> 1968.86s]  So you get to specify what memory is on chip.
[1969.22s -> 1972.58s]  Ah, my pen's back.
[1973.20s -> 1975.96s]  And what memory is off chip.
[1975.96s -> 1981.24s]  So you might have SRAM on chip and you might have a data type,
[1982.18s -> 1986.44s]  right, in this case unsigned in 8 and a,
[1987.28s -> 1990.36s]  in this case it's an array,
[1990.36s -> 1995.08s]  so you might specify how many elements.
[1995.98s -> 2002.56s]  And then you can also specify DRAM, right, in this case,
[2002.92s -> 2009.52s]  again, it's 8 bit value and this is a 2 dimensional array.
[2010.24s -> 2012.46s]  So you have in this case image and buffer.
[2012.46s -> 2015.62s]  And then of course you've got redishes
[2015.62s -> 2018.48s]  and you've got a variety of different kinds of redishes.
[2018.96s -> 2024.58s]  You've got cumulators, you've got FIFOs, which are just Qs,
[2024.58s -> 2026.70s]  so we'll say a lot about using FIFOs.
[2027.58s -> 2030.22s]  You might, if you're doing image processing,
[2030.22s -> 2031.76s]  have the idea of a line buffer,
[2031.76s -> 2034.34s]  which is this 2 dimensional array
[2034.34s -> 2036.88s]  that can be shifted by lines.
[2037.42s -> 2040.92s]  And then you might have a shift register, which is similar
[2041.26s -> 2044.04s]  in spirit to the line buffer.
[2044.68s -> 2054.18s]  All right, so when we're dealing with CPU there's only the main address space,
[2054.58s -> 2059.06s]  the address space of memory that is visible to the programmer, right,
[2059.06s -> 2064.62s]  and then you as the programmer can write code that is cache friendly
[2064.62s -> 2069.38s]  but you don't control how data moves between the main memory and the cache, right,
[2069.38s -> 2073.50s]  that's handled automatically by the underlying hardware controller.
[2073.90s -> 2075.50s]  In spatial that's not the case.
[2075.74s -> 2079.74s]  You, the programmer, have to explicitly move data back and forth
[2079.74s -> 2081.62s]  between the different levels of the memory hierarchy
[2081.88s -> 2087.70s]  and in this case you're moving data from the DRAM, from the image,
[2087.94s -> 2092.54s]  to the buffer, right, with a load operation, right.
[2093.18s -> 2097.18s]  Okay, so this is a dense data movement.
[2097.36s -> 2102.08s]  The next is a gather, right, so we've talked about gather,
[2102.16s -> 2104.82s]  so can someone tell me how gather works here?
[2108.20s -> 2121.32s]  Right, so you're saying that you're going to get it from image and in this case 10 elements
[2121.82s -> 2127.62s]  and the addresses or the locations are going to be specified in some array A, right,
[2128.18s -> 2132.78s]  and then you're going to, so essentially you are taking sparse data
[2132.78s -> 2136.32s]  and making it dense in buffer, right.
[2136.32s -> 2142.94s]  So you can imagine there's load and gather and then there's store and scatter, right.
[2146.50s -> 2153.54s]  Okay, and then you can also create streams and you can stream data in and out
[2153.54s -> 2158.50s]  and streaming will be a key component of getting efficiency.
[2158.90s -> 2160.92s]  Alright, what about control templates?
[2160.92s -> 2167.48s]  Well, the idea is that, oh by the way, the spatial language is embedded
[2167.48s -> 2172.88s]  in a language called scalar for historical reasons, we won't get into it,
[2172.88s -> 2176.52s]  but scalar is actually a very nice language to embed DSLs
[2176.52s -> 2178.24s]  into it because it's very flexible.
[2178.24s -> 2183.68s]  We've actually seen scalar when we were talking about Spark, right,
[2183.80s -> 2189.58s]  so you have in fact seen it before and so it was very popular at one time
[2189.58s -> 2197.14s]  as embedding languages go, but it has certain deficiencies around the use of the JVM
[2197.14s -> 2202.78s]  which kind of limited its wide use, widespread use.
[2203.36s -> 2209.66s]  Okay, so you've got these Excel blocks, right, which are going to divide your program
[2209.66s -> 2215.52s]  into parts that are accelerated and parts that just run on the CPU and the question is
[2215.52s -> 2218.84s]  whether you run the Excel block once or whether you run it continuously
[2219.14s -> 2225.38s]  which is the Excel star syntax and then there's this idea of finite state machines
[2225.70s -> 2228.56s]  which we are not going to focus on at all.
[2228.82s -> 2235.38s]  What we will focus on is the key mechanisms for doing parallel patterns which is for each
[2235.38s -> 2241.40s]  which is essentially a map and then reduce which is a reduce, right, and this says,
[2241.72s -> 2246.42s]  you know, for, you know, all the elements in C and you're going to step through it
[2246.42s -> 2265.94s]  by one do the following code which is the block of the for loop as specified within the braces.
[2269.44s -> 2275.82s]  Okay, so there are a bunch of design parameters.
[2276.34s -> 2277.28s]  That you can specify.
[2277.28s -> 2287.14s]  You can specify how much particular for each and reduces are paralyzed.
[2287.82s -> 2290.26s]  You can specify how they get scheduled.
[2290.66s -> 2294.60s]  You can say that you want this to be pipelined or you want it to be streamed
[2294.60s -> 2298.28s]  and we'll say a little bit about each of these in just a moment.
[2298.60s -> 2306.84s]  You can specify parameters such as you want the rate, size of the buffer to be a range,
[2306.84s -> 2312.96s]  you know, default to be 64 but you want, might want the range to be 64 to 1024
[2313.64s -> 2319.56s]  and that can be explored then potentially by the compiler.
[2319.56s -> 2328.42s]  All right, so if you specify things that require the use
[2328.42s -> 2331.42s]  of memory banking the compiler will handle it for you, right.
[2331.42s -> 2335.48s]  So if you paralyze something and the paralyzation implies
[2335.56s -> 2342.74s]  that you have multiple accesses to a particular memory unit then it's the responsibility
[2342.74s -> 2348.16s]  of the compiler to make sure that you can actually achieve that paralyzation factor
[2348.36s -> 2352.60s]  by duplicating memories or figuring out how to bank the memories appropriately.
[2353.18s -> 2356.52s]  But that's a detail that you don't have to consider.
[2356.52s -> 2359.78s]  So that's something that the compiler deals for you.
[2360.28s -> 2364.84s]  All right, so let's look at an example to bring these concepts home, right.
[2364.84s -> 2370.84s]  So we're going to do inner product, your favorite little kernel and we want
[2370.84s -> 2374.20s]  to build an accelerator in spatial, right.
[2374.20s -> 2376.92s]  And so we're going to have these three, we're going to have the code here,
[2377.22s -> 2383.34s]  we have the sketch of the generator hardware below and let's see what happens.
[2383.34s -> 2388.36s]  So let's start with the C code, right, just to make sure everybody's clear.
[2388.36s -> 2393.74s]  So we're going to mallet two vectors, vec1 and vec2 and then we are going
[2393.74s -> 2401.08s]  to compute the inner product using this simple for loop, right.
[2401.08s -> 2403.68s]  We multiply each of the elements and we add them all together.
[2404.60s -> 2406.36s]  Okay, that's clear, right.
[2407.16s -> 2409.86s]  So that's what you do if you're writing for a C code, right.
[2409.86s -> 2413.90s]  So what do you do if you want to build an accelerator for this, right.
[2413.90s -> 2419.12s]  Well, as you remember, you now have to control all the memory.
[2419.18s -> 2424.00s]  So let's assume that vec1 and vec2 are integer arrays in DRAM, right.
[2424.00s -> 2428.82s]  And so it's clear here by this specification of DRAM
[2428.82s -> 2432.90s]  that these two arrays are going to live in DRAM, right.
[2433.34s -> 2440.52s]  Now we haven't said exactly how DRAM works, but let's assume that we have a way of moving data
[2440.52s -> 2445.60s]  between the DRAM and the accelerator using direct memory access, right,
[2445.60s -> 2452.38s]  which is an efficient way of moving data between the main memory and the accelerator, right.
[2452.44s -> 2460.58s]  And so we have an Excel block, right, which is where we are going to do the acceleration.
[2461.32s -> 2461.76s]  And.
[2467.52s -> 2477.00s]  All right, so the first thing we're going to do is we need to move data from the DRAM
[2477.42s -> 2482.06s]  into the accelerator, and we need a place for that data to land, right.
[2482.66s -> 2489.66s]  So we have to define some data structure within the accelerator for that,
[2489.66s -> 2496.54s]  and we're going to create two SRAM blocks for this purpose, right, tile one and tile two,
[2496.90s -> 2505.74s]  and these are going to have size, tile size, and they are going to be in SRAM, right.
[2506.68s -> 2510.70s]  And, you know, there's a question of how large they should be,
[2510.70s -> 2513.36s]  but let's talk about that in just a moment.
[2514.24s -> 2519.06s]  So then the first thing we need to do is we need to think about, because we are going
[2519.06s -> 2527.10s]  to do these things by tiling, we need a doubly nested loop, right, single nest won't work
[2527.10s -> 2531.96s]  because we're going to compute our inner product using by tiles, right.
[2532.22s -> 2541.26s]  So why would we want to fetch a tile of data from DRAM instead of single elements?
[2543.80s -> 2543.98s]  Yeah.
[2544.40s -> 2550.44s]  You're going to get much better use of the interface
[2550.44s -> 2553.00s]  between the DRAM and the accelerator, right.
[2553.00s -> 2555.56s]  It's like going to the grocery store, you never just pick
[2555.56s -> 2557.26s]  up one thing, that would be really wasteful.
[2557.58s -> 2561.92s]  You take the effort to go all the way to the grocery store, you get a whole bunch
[2561.92s -> 2566.88s]  of things and bring them and put them in your fridge or your pantry, right,
[2566.88s -> 2568.42s]  so that you don't have to go back
[2568.90s -> 2573.46s]  to the grocery store every time you want to eat something, right.
[2573.52s -> 2580.20s]  So same idea here, costly to go to DRAM, you want to get more than one thing,
[2580.20s -> 2589.30s]  you might want to get a whole tile size amount of data and so you need to have a place
[2589.88s -> 2592.42s]  in your memory to hold that, right.
[2592.54s -> 2598.20s]  So of course, if this was a CPU, this might just be general purpose cache, right,
[2598.20s -> 2601.76s]  and the movement of data would be controlled by the caching algorithms.
[2601.94s -> 2606.74s]  Here, you get to explicitly move the data and program the data movement.
[2607.60s -> 2615.88s]  All right, so the first thing now we do is we're going to have a reduction over tile size
[2615.88s -> 2624.44s]  because at the end of the day, we need to reduce the elements using addition in order
[2624.44s -> 2628.90s]  to generate the output, right.
[2629.34s -> 2640.76s]  And so first of all, we're going to load the two vectors, right, load a tile size element
[2640.76s -> 2649.40s]  of data from vec1 into tile 1 and a tile size element of vector 2 into tile 2, right.
[2649.90s -> 2662.44s]  Then we are going to reduce within the tile, right, in step 2 and then reduce
[2662.44s -> 2665.72s]  across the tiles in step 3, okay.
[2667.40s -> 2674.28s]  So now we've got the kind of this three step process where we load a tile,
[2674.28s -> 2678.98s]  we do the intra tile accumulate and then we do the accumulate across the tiles.
[2679.64s -> 2684.48s]  Okay, so now the question is, I want to improve the performance of my hardware
[2685.08s -> 2689.96s]  and for that I'm going to, you know, need to exploit parallelism.
[2689.96s -> 2691.94s]  So where is the parallelism in this algorithm?
[2696.10s -> 2696.26s]  Yeah.
[2696.26s -> 2706.10s]  Okay, so that would be an example of pipelining, right.
[2706.10s -> 2712.18s]  So you could pipeline that so that that's one place that we could exploit parallelism
[2712.18s -> 2716.26s]  in this, in a product representation.
[2716.84s -> 2717.48s]  Where else?
[2718.06s -> 2718.18s]  Yeah.
[2718.78s -> 2731.94s]  Okay, so you're saying, so within each tile we can do what?
[2737.78s -> 2741.74s]  Whoa, whoa, whoa, the innermost reduces is only within a tile.
[2742.70s -> 2747.02s]  Yes, I'm saying you can do that several times at once.
[2747.02s -> 2750.88s]  Yeah, right, so you want to parallelize step 2, okay.
[2752.16s -> 2753.54s]  Yeah, so we can parallelize step 2.
[2753.68s -> 2759.30s]  How, what might be the best way to parallelize step 2, given what you know?
[2761.10s -> 2763.76s]  What would be the most efficient way of parallelizing step 2?
[2763.76s -> 2775.00s]  Pull something out of your pocket that you are very familiar with.
[2780.34s -> 2780.60s]  Yeah.
[2780.60s -> 2783.96s]  If you see that you're always using the same instructions of SIMD.
[2783.96s -> 2787.70s]  SIMD, SIMD would be a good way of parallelizing step 2
[2787.70s -> 2793.06s]  since we're doing the same operation to all of the elements
[2793.06s -> 2799.84s]  in the tile, right, and that is,
[2799.84s -> 2804.82s]  so we are doing this multiplication and reduction.
[2805.16s -> 2809.00s]  Right, so if you want to think about this reduce,
[2809.00s -> 2812.10s]  if you want to parallelize this reduce, what do you do?
[2813.22s -> 2815.14s]  You're going to need some sort
[2815.14s -> 2818.76s]  of parallel multiply followed by a reduction tree, right.
[2824.06s -> 2828.56s]  Okay, so spatial allows you to do that.
[2828.56s -> 2830.72s]  It's got this notion of reduction, right.
[2830.72s -> 2835.60s]  It's got this notion of reduction trees, and so you can parallelize by 2 in this case,
[2835.88s -> 2839.32s]  you know, there's not much of a tree, but you're doing 2 multiplies,
[2839.32s -> 2842.18s]  but you can go wider, right, go as wide as you want.
[2842.80s -> 2845.80s]  And what would be the downside of going wider?
[2848.00s -> 2849.70s]  The reduction tree is larger.
[2849.70s -> 2851.64s]  The reduction tree is larger, what else is larger?
[2853.34s -> 2857.76s]  What? The hardware, right, you use more resources, right.
[2858.20s -> 2862.40s]  You get to control, right, you say, hey, I want more parallelism,
[2862.78s -> 2867.20s]  there's no free lunch here, right, you're going to use more resources.
[2869.00s -> 2872.42s]  But you get to control how many resources you use based
[2872.42s -> 2876.16s]  on how much performance improvement you want.
[2876.16s -> 2881.20s]  Okay, so that's, let's hold this idea of pipelining for just a second,
[2881.48s -> 2888.90s]  but you could also control the tile size such that you could decide how much data you want
[2888.90s -> 2893.66s]  to fetch every time you go to memory, and optimize that,
[2894.02s -> 2896.98s]  so you could specify what tile size you want to use.
[2897.48s -> 2903.20s]  And lastly, this idea of, you could say, hey,
[2903.46s -> 2911.06s]  instead of running the outer reduce one step at a time, let's overlap them
[2911.06s -> 2914.52s]  by specifying a pipeline schedule, right.
[2914.52s -> 2918.54s]  So, pipelining here would say, now I want to overlap.
[2918.54s -> 2923.32s]  Now, what is the key thing that you need for pipelining to work?
[2923.76s -> 2928.52s]  And it's kind of shown in this picture here, and maybe you could read, but.
[2929.06s -> 2940.10s]  Yeah, but we know there are no hazards, right, because, okay.
[2940.60s -> 2942.62s]  But what extra resources do we need?
[2944.32s -> 2944.50s]  Yeah.
[2946.40s -> 2952.74s]  Yeah, we need to be able to, we need some double buffering, right.
[2952.74s -> 2961.92s]  So, while stage one is working on some, filling data from memory, right, stage two has
[2961.92s -> 2965.54s]  to be able to work on data that has been generated
[2965.60s -> 2969.34s]  from the previous execution of stage one, right.
[2969.44s -> 2976.66s]  You essentially need some sort of double buffering, and so that's extra, so pipelining is
[2976.66s -> 2980.78s]  as close to a free lunch as you might get in hardware, but it's not completely free
[2980.78s -> 2982.96s]  because you add memory, right.
[2984.10s -> 2987.84s]  So, you decide the pipeline, and it's, you get, you know, in this case,
[2988.44s -> 2991.84s]  best case would be you pipeline to a depth of three,
[2991.84s -> 2994.10s]  and you get a 3x improvement in performance.
[2994.64s -> 3000.48s]  That's, of course, not always true, but the overhead to doing
[3000.48s -> 3010.18s]  that would be the extra tile memory that you need at every stage interface in order
[3010.18s -> 3015.54s]  to make sure that data doesn't get overwritten while you're doing the pipelining.
[3016.10s -> 3018.02s]  Does everybody follow that?
[3018.02s -> 3026.34s]  Good. So, you, so, we saw kind of three types of optimization, parallelization,
[3026.88s -> 3030.96s]  how we deal with the data locality, and pipelining.
[3032.18s -> 3036.80s]  Okay. So, just to make sure that you all understand then,
[3036.80s -> 3042.56s]  so spatial programmers responsibility, what, as a spatial programmer, are you responsible for?
[3048.02s -> 3051.12s]  Yeah.
[3052.20s -> 3053.20s]  What you're actually doing.
[3053.66s -> 3057.04s]  Yeah. Can you be a little more specific about what you're actually doing?
[3057.04s -> 3060.56s]  Basically, how do you, like some algorithm, how do you compose
[3060.60s -> 3062.72s]  that into the different types of hardware?
[3063.66s -> 3065.62s]  Like, who is that student?
[3065.62s -> 3074.02s]  Right. You've got to be able to express your algorithm in using the for each
[3074.02s -> 3077.96s]  and reduce constructs, what else are you responsible for?
[3077.96s -> 3082.58s]  Yeah. Handling memory, the explicit memory hierarchy, figuring out what,
[3082.58s -> 3088.42s]  where the data should live in the different memories that you define.
[3088.64s -> 3089.20s]  What else?
[3090.74s -> 3091.88s]  Specifying parallelism.
[3091.88s -> 3095.30s]  So, actually, specifying parallelism, how much and where.
[3096.10s -> 3101.16s]  I think that's about it, right?
[3102.56s -> 3107.68s]  Specifying the algorithm, specifying the memory hierarchy, doing explicit data movement,
[3108.04s -> 3111.52s]  and then picking the tiling factors, parallelism, and scheduling.
[3112.16s -> 3118.34s]  Right? And then the compiler's responsibility is this banking and buffering of memories
[3118.38s -> 3124.84s]  to maximize the performance and minimize resources and some lower level things
[3124.84s -> 3129.76s]  about generating configurations for explicit targets.
[3130.88s -> 3135.38s]  And, of course, if you want to improve performance and understand performance,
[3135.38s -> 3139.02s]  you need some way of getting feedback about the performance
[3139.26s -> 3145.36s]  that your particular code can achieve on any one of these targets and then what sort
[3145.36s -> 3146.86s]  of resources they might use.
[3148.70s -> 3157.04s]  Okay. So, you know, Spatial's being used to convert TensorFlow representations
[3157.04s -> 3160.54s]  of machine learning algorithms into hardware.
[3161.24s -> 3167.54s]  I think more interesting might be, you know, something that you're very familiar with,
[3168.00s -> 3173.18s]  which is sort of how to optimize an algorithm like attention.
[3173.72s -> 3177.66s]  Okay? So this is something that you've just been thinking about.
[3177.66s -> 3180.22s]  So we talked about fused attention, right?
[3180.68s -> 3183.26s]  And so what was the big benefit of fused attention?
[3185.62s -> 3186.02s]  Yeah?
[3186.02s -> 3191.90s]  You don't have to materialize a fuel-attention matrix.
[3191.90s -> 3201.34s]  You kind of tile things into blocks, and then you compute a block at a time,
[3201.68s -> 3208.66s]  and then you also get this idea of fusing the different components
[3209.08s -> 3215.84s]  of the attention algorithm together and minimizing memory bandwidth
[3216.38s -> 3225.04s]  by doing that, and then also you minimize memory bandwidth,
[3225.04s -> 3228.04s]  and that gives you the benefit, right?
[3228.48s -> 3239.60s]  And so you get this performance and memory size benefits.
[3240.06s -> 3250.16s]  So it turns out that if you kind of write things using this Spatial program,
[3250.16s -> 3254.16s]  streaming programming model, you can get a lot of these benefits
[3254.56s -> 3256.96s]  with a simpler programming model, right?
[3256.96s -> 3264.06s]  So a model where you don't have to write an explicit fused kernel.
[3264.06s -> 3266.64s]  So let's see how that works, right?
[3266.82s -> 3272.66s]  So let's kind of go back to the time before flash attention, right?
[3273.24s -> 3277.76s]  And, you know, before flash attention, right,
[3277.76s -> 3283.24s]  you had this kernel-based execution model, and the flash attention, you know,
[3283.24s -> 3285.98s]  as we said prevents the materialization of the full matrix.
[3286.68s -> 3291.22s]  With the streaming execution model, you also get these benefits, but you didn't have
[3291.28s -> 3297.76s]  to write the flash attention kernel, and in particular,
[3297.76s -> 3300.62s]  you didn't have to pay extra computation, right?
[3300.62s -> 3313.72s]  So it turns out that to deal with the softmax, you had to do extra computation
[3313.72s -> 3323.04s]  in order to deal with the fact that you had this row computation that you needed the whole row
[3323.04s -> 3329.00s]  in order to compute the softmax, and it turns out that with streaming you can get away
[3329.10s -> 3334.88s]  from doing this at the cost of having potentially a little bit more memory.
[3336.64s -> 3338.74s]  All right, so let's see how that works, right?
[3338.74s -> 3341.84s]  So if we think about softmax, right, as you know,
[3341.84s -> 3346.18s]  it's actually a three-step procedure, right?
[3346.18s -> 3357.72s]  So first of all, you've got to compute the exponential for the particular values of Sij,
[3358.36s -> 3369.18s]  and then you have to do the row-wise reduction, and then you have to do the division
[3369.22s -> 3373.76s]  of the exponential by the row-wise information, right?
[3374.12s -> 3382.10s]  And so this three-step process is shown here pictorially, right?
[3382.10s -> 3387.60s]  So first the exponential, then the reduction, which is row-wise, and then the division.
[3387.60s -> 3400.28s]  So if you do this without the optimization of flash attention, right,
[3400.34s -> 3408.52s]  then you have this materialization of the whole matrix, and of course
[3408.52s -> 3412.94s]  that increases your memory footprint and increases your memory bandwidth.
[3418.60s -> 3421.92s]  And so this kind of shows you the overview.
[3421.92s -> 3429.90s]  It shows all the data that has to be both materialized and moved between the accelerator,
[3429.90s -> 3436.96s]  which you think is happening up top, and the GPU memory which is happening, which is below,
[3436.96s -> 3445.70s]  right, and so all the data that crosses this line then is memory bandwidth that has
[3445.80s -> 3456.20s]  to be used in order to compute the attention, right?
[3456.20s -> 3462.92s]  So with the streaming execution model, you can avoid the materialization of the matrix,
[3462.92s -> 3467.18s]  so let's show you how that works, how the streaming works, right?
[3467.18s -> 3473.26s]  So essentially, you know, in this example we're going to compute the exponential,
[3473.58s -> 3481.10s]  and then we are going to compute the row sum, right, by reducing the row.
[3481.10s -> 3485.78s]  And so the way that you would write this in spatial, right, is that you're going
[3486.04s -> 3489.86s]  to have the first for each, which is kind of a map,
[3490.82s -> 3495.38s]  do the computation of the exponential, right?
[3496.26s -> 3504.04s]  But then instead of putting the output into another matrix,
[3504.12s -> 3509.06s]  we're just going to enqueue the output in a FIFO, right?
[3510.50s -> 3516.56s]  And so the semantics of spatial are this for each
[3516.56s -> 3519.42s]  and this for each are executing at the same time, right?
[3520.38s -> 3528.30s]  So now you can think of the first for each as being the producer,
[3529.18s -> 3538.36s]  and the second for each is consuming the output of the producer, and so, you know,
[3538.36s -> 3543.96s]  essentially the reduction then happens by dequeueing an element
[3544.04s -> 3555.48s]  from the first for each and keeping the, having this continuous sum,
[3555.48s -> 3566.30s]  and then finally when you're done, you generate the output also in a FIFO, right?
[3566.30s -> 3567.32s]  So there's a single element.
[3567.60s -> 3570.08s]  So does everybody follow how this works, right?
[3570.16s -> 3574.84s]  You essentially have these two for eachs, and they're operating in a pipeline, right?
[3575.10s -> 3580.76s]  And between the pipeline are a FIFO, right?
[3580.76s -> 3593.60s]  So initially what happens is we define the on-chip memory for S and we define the two FIFOs.
[3593.60s -> 3608.94s]  We do the enqueue, we compute the exponential element here,
[3610.00s -> 3617.02s]  and then we do enqueue the data on the FIFO.
[3617.02s -> 3626.20s]  We dequeue the FIFO in the second for each, right?
[3627.80s -> 3634.34s]  And so this just shows how things work with streaming, right?
[3634.34s -> 3640.86s]  So before streaming you would materialize the whole of the N by N matrix.
[3640.86s -> 3646.28s]  With streaming data just moves through, in this case, the two element FIFOs.
[3646.28s -> 3651.38s]  So this was the equivalent of the double buffering that we showed in the first example,
[3651.38s -> 3656.58s]  but here you've got an explicit FIFO, and so because you have figured
[3656.58s -> 3659.90s]  out that that's all you need, you get a tremendous reduction
[3659.90s -> 3661.44s]  in the amount of memory you need.
[3661.74s -> 3665.92s]  But in order for this to work, your programming model has to think
[3665.92s -> 3671.96s]  about these two kernels operating at the same time being connected with a FIFO,
[3672.84s -> 3676.24s]  which is a very natural hardware way to think about things.
[3676.60s -> 3680.24s]  But it's not a natural software way to think about things, right?
[3680.24s -> 3690.14s]  So Spatial kind of allows you to think about this idea of pipeline parallelism in ways
[3690.14s -> 3696.04s]  that, you know, match what you want to do in an efficient hardware implementation
[3696.30s -> 3700.04s]  which don't match what you typically would do with software.
[3700.92s -> 3708.38s]  So back to the original kernel, kernel by kernel scheme
[3708.38s -> 3711.68s]  in which you are materializing all the memory, you've got all this memory
[3711.68s -> 3714.98s]  that gets materialized, you've got all this data movement that's happening,
[3715.32s -> 3721.30s]  which we showed was unnecessary, and that if you do things with streaming,
[3721.30s -> 3727.36s]  then the data can just move through FIFOs between the different kernels
[3727.92s -> 3734.80s]  because one kernel puts data in a FIFO, the next kernel picks it up and does the compute,
[3735.28s -> 3737.76s]  and you never have to materialize the whole matrix.
[3738.22s -> 3744.38s]  And in cases where you need data, for instance, for doing the row operation,
[3744.38s -> 3748.72s]  then you've got to be able to accumulate a whole row of data in your FIFO.
[3748.88s -> 3751.76s]  And so the limit is that you need to be able
[3751.76s -> 3756.78s]  to have this FIFO be the length of a row of the matrix, right?
[3757.68s -> 3762.40s]  Okay? Now, that could potentially become a limit, and you can go through the details,
[3762.80s -> 3767.64s]  and the details will be clear, right?
[3767.76s -> 3776.66s]  And so the question is, so you need a, oh, this is just showing that you need a row
[3777.34s -> 3786.82s]  of data in order to compute the P matrix, or so, an element of the P matrix.
[3788.00s -> 3792.52s]  And you can look at the elision.
[3792.52s -> 3796.28s]  And so the question is, you know, could we do better with flash attention?
[3796.78s -> 3798.96s]  The answer is yes, right?
[3798.96s -> 3804.46s]  So there's still room for optimizations like flash attention because maybe at some point
[3804.46s -> 3808.80s]  if your matrix size, your sequence length gets really long,
[3809.04s -> 3812.48s]  then even a row of data is too much, right?
[3812.56s -> 3822.42s]  And so if you want to, you know, limit the size of your FIFO, you can apply flash attention
[3822.72s -> 3831.10s]  to this sort of, to a streaming-based optimization, okay?
[3832.38s -> 3835.62s]  Flash attention, you know, reorders the operations and uses a running sum
[3835.62s -> 3840.86s]  and rescaling instead of naive reduction in order to do this computation.
[3840.86s -> 3852.54s]  And now we can dramatically reduce the need for this row amount of data in our FIFO, right?
[3852.82s -> 3862.00s]  All right, so if you would kind of sort of compare streaming versus kernel by kernel,
[3862.64s -> 3873.68s]  so what you get with a streaming implementation is, you know,
[3873.68s -> 3877.62s]  you get this idea that you can exploit more parallelism, right?
[3877.90s -> 3882.88s]  Because you have this ability to overlap the computation of the kernels
[3883.14s -> 3889.28s]  between each other using this pipeline type of execution, you get more,
[3889.28s -> 3891.24s]  the ability to exploit more parallelism.
[3892.34s -> 3900.12s]  And you can spatially map each computation without, with pipeline communication, right?
[3900.12s -> 3903.82s]  And so this is what you get with a streaming execution model.
[3904.14s -> 3910.44s]  And then you can overlap and pipeline the computation for different output tiles, right?
[3910.44s -> 3918.20s]  So you get that extra dimension of parallelism and performance with a streaming implementation.
[3918.96s -> 3923.76s]  The other benefit that you potentially get with a streaming implementation is
[3923.76s -> 3929.48s]  that you don't have to explicitly create a fused kernel, right?
[3929.62s -> 3934.58s]  So you can imagine these kernels are implemented individually
[3934.74s -> 3941.48s]  and you can either use the capability of the compiler to do this double buffering technique.
[3941.70s -> 3946.60s]  But then if you want even further optimization, then you can replace the double buffer
[3946.68s -> 3954.56s]  with a FIFO-like programming expression, as I just showed you in this example,
[3954.70s -> 3956.34s]  and get even more efficiency.
[3956.64s -> 3963.92s]  And that's easier than creating this, you know, explicitly fused kernel as you would have
[3964.00s -> 3969.08s]  to do with a traditional programming model, right?
[3969.08s -> 3973.78s]  So the streaming execution model gives you this extra degree of freedom, you know,
[3973.78s -> 3976.42s]  operations get fused automatically if you write.
[3976.64s -> 3981.18s]  Things using FIFOs, or they get, you know, even if you write them using buffers,
[3981.48s -> 3985.30s]  the double buffering technique can be employed, right?
[3985.66s -> 3990.04s]  And then the compiler can automatically generate the fused execution, okay?
[3990.48s -> 3991.34s]  Any questions here?
[3991.34s -> 3991.50s]  Yeah?
[3992.30s -> 3992.40s]  Yeah?
[3992.40s -> 3996.48s]  So just to confirm, the streaming execution model is independent from the notion
[3996.48s -> 3999.80s]  of accelerated design, like you can write a streaming program,
[3999.80s -> 4001.54s]  but it can run on like pre-existing.
[4002.28s -> 4003.64s]  Yeah, it can.
[4003.64s -> 4009.94s]  I mean, you can imagine it running on a existing architecture.
[4009.94s -> 4014.82s]  The question is sort of whether, so it's very difficult in CUDA
[4014.82s -> 4016.62s]  to write a streaming program, right?
[4017.16s -> 4020.70s]  I mean, you fundamentally don't have the ability to have different parts
[4020.70s -> 4025.26s]  of the kernel operating independently in ways,
[4025.42s -> 4030.32s]  and so what you typically would write is you would write some sort of fused kernel, right?
[4030.32s -> 4034.84s]  And so you need, you can imagine that, I think, you know,
[4034.84s -> 4039.78s]  you talk to the people developing CUDA, they're trying to enable this kind of execution,
[4040.14s -> 4042.32s]  but I don't know how to do it yet.
[4042.90s -> 4047.04s]  You know, I'm not saying, it'll never be possible, but it's not possible today.
[4049.04s -> 4049.20s]  Yeah?
[4050.72s -> 4054.20s]  In the execution model, what if the FIFO depth is getting too large,
[4054.20s -> 4057.96s]  then is there an alternative, if it's streaming without the FIFO?
[4059.92s -> 4065.60s]  If the FIFO depth is getting too large, then is there an alternative, if it's streaming without the FIFO?
[4065.60s -> 4069.30s]  Yeah, so then you might, you could use buffering, right?
[4069.30s -> 4071.96s]  You could just use SRAM instead of Redis's.
[4075.36s -> 4078.12s]  And you might have to write your application slightly differently.
[4078.90s -> 4085.42s]  Or you would try to think about, you know, using techniques that fundamentally reduce the FIFO size,
[4085.42s -> 4086.66s]  like flash attention, right?
[4087.94s -> 4095.16s]  So, you know, sort of, you can get it in a sort of brain-dead way just by using a FIFO,
[4095.16s -> 4098.98s]  and you don't have to work too hard, but then eventually maybe the FIFO gets too big,
[4098.98s -> 4103.78s]  and then you have to work harder and do something that actually transforms the application.
[4104.94s -> 4105.14s]  Yeah?
[4105.72s -> 4110.44s]  So just to kind of clarify, when you came up with the word using spatial,
[4110.44s -> 4110.94s]  Yeah.
[4110.94s -> 4116.56s]  But spatial is like kind of giving you, like, the feature, is it that you take those, right?
[4116.56s -> 4117.44s]  It's not like...
[4117.44s -> 4120.12s]  No, you could target all sorts of things.
[4120.12s -> 4121.04s]  Oh, you could target, yeah.
[4121.04s -> 4121.54s]  Yeah, yeah, yeah.
[4121.54s -> 4122.04s]  Oh, okay.
[4122.04s -> 4123.44s]  Yeah, yeah, yeah, yeah, yeah.
[4125.00s -> 4130.44s]  You know, it was intended as a hardware model.
[4130.44s -> 4138.74s]  It works very well for a new style of architecture that we define called reconfigurable data flow.
[4138.74s -> 4142.34s]  It doesn't really match GPUs.
[4142.34s -> 4144.94s]  You could, of course, make it run on CPUs.
[4144.94s -> 4147.34s]  But, yeah.
[4147.34s -> 4150.74s]  Other questions?
[4150.74s -> 4156.34s]  But the whole point I want you to get is this idea of the streaming execution model,
[4157.24s -> 4164.24s]  this idea of having kernels that operate in parallel at the same time in a pipeline execution mode,
[4164.24s -> 4170.24s]  and the notion that you can get the benefit of fusing and tiling without it being explicit.
[4174.24s -> 4176.24s]  Okay.
[4176.24s -> 4183.24s]  Okay, so the accelerator summary is the significant energy efficiency improvements from acceleration, right?
[4183.34s -> 4187.34s]  So you can get, you know, 100 to 1,000x improvement.
[4187.34s -> 4192.34s]  Designing accelerators is all about understanding your application,
[4192.34s -> 4197.34s]  which is something that we've been focusing on the whole of this course,
[4197.34s -> 4204.34s]  and then figuring out how to exploit, you know, the specific parallelism and locality
[4204.34s -> 4208.34s]  that is exhibited by your application, right?
[4208.34s -> 4210.34s]  And we kind of have seen that.
[4210.44s -> 4214.44s]  But now, in the context of accelerator design,
[4214.44s -> 4222.44s]  you get to define explicitly what resources are going to be used to exploit parallelism,
[4222.44s -> 4229.44s]  and then how the memory hierarchy is going to be designed to make sure that you can get maximum locality, right?
[4229.44s -> 4231.44s]  And so you get to find the sizes,
[4231.44s -> 4236.44s]  you get to find when the data moves from one part of the memory hierarchy
[4236.44s -> 4238.44s]  to another part of the memory hierarchy.
[4238.54s -> 4242.54s]  Okay, so you need insight into the application.
[4242.54s -> 4245.54s]  Where's the bottleneck? Is it memory bound? Is it compute bound?
[4245.54s -> 4249.54s]  How does that change as I change my algorithm or as I change my implementation?
[4249.54s -> 4253.54s]  And then spatial is kind of a way of programming model
[4253.54s -> 4258.54s]  and a way of thinking about how to explore the design space
[4258.54s -> 4261.54s]  when you want to make these sorts of trade-offs, right?
[4261.54s -> 4265.54s]  So it's kind of, you know, a small step beyond what you've been doing here
[4265.64s -> 4270.64s]  in that you now understand the application, you understand parallelism,
[4270.64s -> 4272.64s]  you understand the different types of parallelism.
[4272.64s -> 4278.64s]  Well, now what would you do if you actually had control over the hardware that you get to define?
[4278.64s -> 4282.64s]  And then you think about what that means.
[4282.64s -> 4289.64s]  Okay, so we don't have time to talk about the design of memory.
[4289.64s -> 4293.64s]  Maybe Kayvon will talk about it on Thursday, maybe not.
[4293.64s -> 4294.64s]  It's up to him.
[4294.74s -> 4298.74s]  But as far as sort of acceleration and heterogeneous compute,
[4298.74s -> 4300.74s]  we're done with that.
[4300.74s -> 4302.74s]  Alright, thanks.
