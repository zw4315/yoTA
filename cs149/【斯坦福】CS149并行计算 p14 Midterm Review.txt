# Detected language: en (p=1.00)

[0.00s -> 8.92s]  Any, any questions about logistics first?
[8.92s -> 14.44s]  Okay, so I mean I think there's some broad themes of the course, right?
[14.44s -> 19.80s]  So one broad things of the course is kind of lecture one, two, three, do you understand
[19.80s -> 20.80s]  multi-core architecture?
[20.80s -> 24.40s]  And you better believe that we're going to ask you something about do you understand
[24.40s -> 25.40s]  that?
[25.40s -> 27.84s]  So that's a topic we can go over today.
[27.84s -> 33.72s]  Another major theme is just general tactics for optimizing parallel programs.
[33.72s -> 38.44s]  Now most of that stuff is stuff that you kind of get in your programming assignment and
[38.44s -> 41.00s]  it's just sort of like you know you just kind of figure it out.
[41.00s -> 45.76s]  So in general like when students come into my office hours we don't spend as much time
[45.76s -> 46.76s]  on that before the exam.
[46.76s -> 51.80s]  I'm like if I give you a program that has workload imbalance like try and identify it.
[51.80s -> 56.52s]  The other set of optimizations typically are you have a program that might have very
[56.52s -> 58.56s]  bad cache performance.
[58.56s -> 62.40s]  Can you improve the order of some loops or something like that to improve the cache
[62.40s -> 64.40s]  performance?
[64.40s -> 67.04s]  Then we talked a little bit about, what else did we talk about?
[67.04s -> 74.64s]  So we talked about GPU architecture which first of all you should think of as just
[74.64s -> 78.36s]  like everything from the first three lectures just you had it again and it's no surprise
[78.36s -> 81.60s]  that GPUs work the same way.
[81.60s -> 86.24s]  I'd like you to understand the basics of the CUDA programming model and that there's
[86.24s -> 88.96s]  an organization in the thread blocks.
[88.96s -> 92.48s]  There are threads in those thread blocks but it's something you're presumably well acquainted
[92.48s -> 96.60s]  with after the third programming assignment.
[96.60s -> 99.16s]  We had a lecture on deep neural networks.
[99.16s -> 105.12s]  Now the specifics of any deep neural network are not anything that you need to care about.
[105.12s -> 108.76s]  But what was like maybe the two or three main concepts from the deep neural networks
[108.76s -> 109.76s]  lecture?
[109.76s -> 114.80s]  One was cache locality really matters.
[114.80s -> 122.28s]  So we talked about blocking and the other stuff was kind of all specialized hardware
[122.28s -> 124.00s]  and things like that.
[124.00s -> 128.48s]  We'll spend more time on that in the course so I think that's more of a finals topic
[128.48s -> 129.84s]  than anything else.
[129.84s -> 134.52s]  We talked about data parallel thinking so there's the possibility, it's fair game for
[134.52s -> 139.28s]  me to ask you to go through a data parallel thinking exercise that reinforces were you
[139.28s -> 143.60s]  the one on your programming team that actually did fine repeats or did you delegate that
[143.60s -> 147.48s]  to your partner and now it shows.
[147.48s -> 152.88s]  And then we talked about the last lecture I gave was fine grain locking.
[152.88s -> 157.20s]  And we talked about compare and swap and implementing some locks.
[157.20s -> 161.28s]  We talked about the relationship of that to how cache coherence works.
[161.28s -> 164.40s]  Some interesting relationship.
[164.40s -> 168.80s]  And we talked about using locks in like a fine grain locking setting and I've given
[168.80s -> 170.84s]  you some practice problems on that.
[170.84s -> 175.56s]  I can tell you that at the midterm level the fine grain locking practice problems will
[175.56s -> 176.96s]  be simpler.
[176.96s -> 181.80s]  Like probably nothing more sophisticated if I give you one on maybe a linked list or
[181.80s -> 183.60s]  something like that.
[183.60s -> 188.36s]  But you see some practice problems on hash tables and graphs and trees and other stuff.
[188.36s -> 192.48s]  I'd like you to have a little bit more time to wrestle with that before I think
[192.48s -> 196.08s]  it's reasonable for me to ask you a tough question.
[196.08s -> 201.64s]  So in light of that, and then of course Kunle definitely talked about the MSI protocol
[201.64s -> 206.56s]  and what cache coherence means and the main ideas behind that.
[206.56s -> 211.32s]  And we talked about the idea of relaxed memory consistency which is even though a program,
[211.32s -> 217.00s]  a thread might write X and then read Y, other processors might see those writes or
[217.00s -> 222.64s]  reads in different orders and there's some interesting implications of that.
[222.64s -> 226.40s]  Cool.
[226.40s -> 231.56s]  So in light of that, I'm happy to go over anything.
[231.56s -> 236.16s]  And so I think those that raised their hand and vote quickly will get more topics of
[236.16s -> 237.16s]  their coverage.
[237.16s -> 240.00s]  Otherwise I can start making some topics up but that may not be well.
[240.00s -> 242.00s]  So sir, you were the first to raise your hand.
[242.00s -> 261.64s]  So we talked about this idea of compare and swap.
[261.64s -> 264.28s]  This is something I would like you to know.
[264.28s -> 274.80s]  So up until this point in the class, we basically did all of our synchronization in two ways.
[274.80s -> 280.00s]  We said that there were locks and what does a lock ensure?
[280.00s -> 284.32s]  Or in other words, what does mutual exclusion mean?
[284.32s -> 285.32s]  Question for the class.
[285.32s -> 286.32s]  Yeah.
[286.32s -> 289.68s]  One code is only run by one thread at a time.
[289.68s -> 291.58s]  And this is a really important detail.
[291.58s -> 297.98s]  Mutual exclusion means, as was just said, that only one thread, one actor, one worker,
[297.98s -> 303.30s]  whatever is allowed in a particular section of code at one time.
[303.30s -> 307.66s]  Which means that if I'm in that section of code, all of you are prevented from running
[307.66s -> 310.06s]  it.
[310.06s -> 314.26s]  And an atomic operation is basically of that mentality, right?
[314.26s -> 319.38s]  Because an atomic operation is usually used on a read-modify-write.
[319.38s -> 325.78s]  It says that if I am reading and modifying this variable, nobody else is reading and
[325.78s -> 328.52s]  modifying this variable.
[328.52s -> 332.92s]  And compare and swap is one form of an atomic operation.
[332.92s -> 340.78s]  It says I'm going to, it's a read-modify or really read-compare-write.
[340.78s -> 345.82s]  It says I'm going to read the value of this variable and if it has a particular value,
[345.82s -> 351.30s]  if it has in this case the value given by the argument compare, I'm going to write this
[351.30s -> 353.54s]  new value to that position.
[353.54s -> 357.26s]  Otherwise I'm essentially going to write the old value to the position and leave it
[357.26s -> 358.26s]  unchanged.
[358.26s -> 359.26s]  That's an atomic operation.
[359.26s -> 363.82s]  It's all the hard, whatever hardware platform you were running on, if you call compare
[363.82s -> 367.14s]  and swap, it will guarantee that that is run atomically.
[367.14s -> 370.14s]  That's what you can take as a given.
[370.14s -> 379.52s]  And then I asked, we talked about given atomic compare and swap, how can we implement an
[379.52s -> 382.34s]  atomic min operation?
[382.34s -> 387.38s]  So one way we can think about the atomic min operation as the hardware just gives
[387.38s -> 393.98s]  me a new primitive, atomic min, and it ensures mutual exclusion because it's atomic.
[393.98s -> 397.42s]  It's like if I'm doing a min on this variable, it's going to make sure that no other threads
[397.42s -> 400.06s]  or cores can read or write to it.
[400.06s -> 402.74s]  But look how I implement, you know, that's a little bit, you know, that's not how we
[402.74s -> 403.74s]  design systems.
[403.74s -> 408.08s]  We don't build a bunch of new primitives for every single thing that we want to do.
[408.08s -> 413.54s]  We like to have one primitive that's rock solid and maybe fast and use it in a variety
[413.54s -> 415.18s]  of context.
[415.18s -> 420.70s]  So look at this atomic min operation and let's go over the philosophy of what happens.
[420.70s -> 422.54s]  So what does min require?
[422.54s -> 427.80s]  Min requires me reading the value from memory, checking if the value from memory is less
[427.80s -> 433.64s]  than the new value that I'm trying to min with it, and if my value is smaller, we
[433.64s -> 437.36s]  need to update the value of memory into something smaller, right, and that all has
[437.36s -> 438.36s]  to be atomic.
[438.36s -> 445.84s]  Now, first of all, I want you to make sure you can confirm to yourself that if you
[445.84s -> 451.24s]  could do this easily without compare and swap, if I gave you a lock, you would take
[451.24s -> 455.38s]  the lock, you would read the value, you would check to see if it was smaller,
[455.38s -> 458.50s]  you would potentially update the value, and then you would unlock.
[458.50s -> 460.62s]  And that would be absolutely correct.
[460.62s -> 462.06s]  And it would ensure mutual exclusion.
[462.06s -> 466.66s]  Only one thread is trying to do the atomic min at once.
[466.66s -> 469.86s]  So look, can someone describe to me what this code is doing?
[469.86s -> 475.10s]  It's not ensuring mutual exclusion on the entire min operation.
[475.10s -> 478.58s]  It's definitely not true.
[478.58s -> 483.02s]  Different threads can have read that value from memory and be checking if their value
[483.02s -> 485.06s]  is less than it at the same time.
[485.06s -> 492.30s]  If you look at the predicate upon which the min confusion would reach, it's ensuring
[492.30s -> 495.38s]  that the predicate is still valid when it writes that confusion.
[495.38s -> 496.38s]  Right.
[496.38s -> 500.70s]  So the idea is saying, if I read the value from memory, then I start doing whatever
[500.70s -> 501.78s]  work I need to do.
[501.78s -> 504.58s]  In this case, it's computing the min of something.
[504.58s -> 509.38s]  But maybe it was like atomic test primality or something, you know, or that probably
[509.38s -> 512.54s]  wouldn't work because I'd return a bool, but anyways.
[512.54s -> 515.74s]  But let's say that's a more expensive operation, then I have to atomically perform
[515.74s -> 519.98s]  some math, and then I write the value back to...
[519.98s -> 521.34s]  I do my work.
[521.34s -> 526.46s]  In this case, I do the work, which is to take the min of the old value and the
[526.46s -> 528.50s]  value that I have.
[528.50s -> 534.44s]  And then I go back and I say, all right, if the value in memory is still the value
[534.44s -> 540.90s]  that it was when I got started, I can be assured that no one else has come in.
[540.98s -> 545.38s]  Or if they've come in, they at least haven't updated the min value.
[545.38s -> 546.98s]  And that's what the atomic cast does.
[546.98s -> 552.02s]  It says, if the value in this address is still the same as old, please update it to
[552.02s -> 553.60s]  new.
[553.60s -> 558.66s]  And I know if that is true, because atomic cast returns old.
[558.66s -> 564.02s]  And so I check to see if the old, if it's still, sorry, it returns the value in memory.
[564.02s -> 568.02s]  And if that's old, that's the check.
[568.02s -> 569.90s]  So let's think about this.
[569.90s -> 576.62s]  Will that atomic compare and swap succeed if while I was trying to do my atomic min,
[576.62s -> 581.94s]  you as another thread came in, read the value, checked it, and decided that you
[581.94s -> 585.30s]  didn't need to update it because your value is too large?
[585.30s -> 587.54s]  My atomic min succeeds.
[587.54s -> 590.22s]  So what happened here is there's no mutual exclusion.
[590.22s -> 594.70s]  We're both running at the same time in so-called this important synchronized
[594.70s -> 596.18s]  section.
[596.18s -> 604.22s]  But unless both of our actions create a situation where synchronization is needed, we can proceed.
[604.22s -> 607.34s]  In other words, we are speculating.
[607.34s -> 612.90s]  We're hoping that we're not going to conflict, and we're just going to go ahead.
[612.90s -> 617.04s]  And then at the last second, before I do my write, I'm going to go check to see
[617.04s -> 619.54s]  if that assumption was true.
[619.54s -> 623.72s]  And if it's true, all the work I did is great, and I move on.
[623.72s -> 628.58s]  If it's false, what's the cost of this?
[628.58s -> 630.12s]  I have to do it again.
[630.12s -> 634.88s]  And all the work that I did on that old value is potentially wasted.
[634.88s -> 640.56s]  So what if this function was not min, but was something really expensive, like compute
[640.56s -> 644.60s]  some exponent on that value and then store the exponent value backward?
[644.60s -> 650.12s]  Then I've done work, and I just got to throw it out, essentially.
[650.12s -> 654.90s]  Every time I go through this while loop, I throw out my min work that I did, and I do
[654.90s -> 656.58s]  it again.
[656.58s -> 660.84s]  So this lock-free thinking is more like, hey, I need to make sure that I give the
[660.84s -> 665.64s]  appearance, that I get the same results as mutual exclusion.
[665.64s -> 669.16s]  But I'm actually not enforcing mutual exclusion.
[669.16s -> 671.12s]  And that's the big mental difference.
[671.12s -> 674.72s]  And so what you're going to see after Thanksgiving is actually, Kunle is going to give
[674.72s -> 678.20s]  a bunch of lectures, and two of them are actually going to be about the idea of transactional
[678.20s -> 685.52s]  memory, which kind of extends this idea to an arbitrary function writing to many variables.
[685.52s -> 689.80s]  So you just write your function, it may read and write a bunch of variables, and the system
[689.80s -> 694.72s]  figures out if any of those reads and writes conflict with another processor's reads
[694.72s -> 700.16s]  and writes, and will actually abort and back one of the processors out.
[700.16s -> 707.52s]  That's the level that I'd like you to know lock-free for tomorrow's assessment.
[707.52s -> 711.92s]  And later in the lecture, if you're interested, I show you how to implement lock-free stacks
[711.92s -> 713.88s]  and stuff like that.
[713.88s -> 718.40s]  Like a lock-free linked list is given in some parallel computing classes, it would
[718.40s -> 721.98s]  be complicated enough to get right that it would take me the entire lecture, it would
[721.98s -> 725.22s]  only be insert and delete on a lock-free linked list.
[725.22s -> 730.12s]  So I would not ask you how to do a lock-free insertion and deletion on an exam, for
[730.12s -> 735.08s]  example, because it's actually incredibly tricky to get right.
[735.12s -> 738.00s]  I'm not sure if I could do it off the top of my head on an exam.
[738.00s -> 745.52s]  Can you say that lock-free programming might lead to maybe a higher risk of starvation?
[745.52s -> 749.48s]  Well, I mean, there's nothing in this implementation that's good.
[749.48s -> 752.56s]  So first of all, what's the idea of starvation?
[752.56s -> 755.82s]  Starvation is that the system is making progress.
[755.82s -> 760.64s]  Some threads are making progress, but for whatever reason, one thread does not.
[760.64s -> 764.52s]  One thread never succeeds here.
[765.16s -> 771.08s]  There's nothing in this provision, in this implementation, to ensure fairness amongst
[771.08s -> 772.20s]  the threads.
[772.20s -> 777.96s]  But if you just had atomic lock and unlock implemented like we did last time, with the
[777.96s -> 782.26s]  exception of that ticket lock example, there's also no provision of fairness.
[782.26s -> 789.20s]  So lock-free versus mutual exclusion is not really, it's orthogonal from fairness.
[789.28s -> 795.12s]  I could do something here which says after I've iterated through this loop, if I fail,
[795.12s -> 798.24s]  maybe I should sleep, you know, or something like that.
[798.24s -> 801.44s]  I could do like an exponential back-off here, just like I could do an exponential
[801.44s -> 803.44s]  back-off in a lock.
[803.44s -> 804.44s]  Okay.
[804.44s -> 807.40s]  Oh, actually, did I miss anybody?
[807.40s -> 808.40s]  Yeah, go ahead.
[808.40s -> 809.40s]  Okay.
[809.40s -> 815.36s]  Is there any extra overhead that's caused when you call it from the past, like regardless
[815.36s -> 816.36s]  of whether...
[816.36s -> 818.20s]  Well, let's talk about it.
[818.20s -> 824.08s]  What is the overhead of atomic CAS?
[824.08s -> 828.06s]  So first of all, let's think about it in terms of a cache coherence protocol.
[828.06s -> 831.16s]  So first of all, actually, how would you implement atomic CAS if you didn't have cache
[831.16s -> 837.06s]  coherence?
[837.06s -> 838.06s]  But it's atomic.
[838.06s -> 841.90s]  How do you even implement...
[841.90s -> 844.84s]  It's not just a read, it's a read and a write.
[844.84s -> 851.08s]  You have to have some special provision in your system that kind of could say this is
[851.08s -> 857.76s]  a current atomic operation on this address, and so no other core can access this address.
[857.76s -> 862.44s]  You basically build like a simple version of cache coherence for like a single address
[862.44s -> 864.02s]  or something like that.
[864.02s -> 871.80s]  But if you did have cache coherence, what is the cost of a CAS?
[871.80s -> 879.28s]  I mean, it's certainly a bus read X, so it's a write.
[879.28s -> 884.60s]  Even if it fails, it's gotta be a write, and that's expensive, right?
[884.60s -> 886.84s]  And it's also a...
[886.84s -> 889.72s]  Yeah, yeah, basically it would be a...
[889.72s -> 892.36s]  It's not just a bus read X.
[892.36s -> 900.28s]  It's a bus read X that you read the value, and then the cache coherence protocol cannot
[900.28s -> 909.72s]  let that line be ripped away by another processor until this comparison and potential write
[909.72s -> 910.96s]  has happened.
[910.96s -> 915.74s]  In the same way that the cache coherence protocol, if you call bus read X, bus read
[915.74s -> 919.88s]  X puts the data in your cache line, and it's also kind of assumed that that processor's
[919.88s -> 923.96s]  actually gonna execute its write before it gets ripped away.
[923.96s -> 927.04s]  Because if you didn't guarantee that, you might get into a live lock situation of two
[927.04s -> 932.48s]  processors are just constantly bus read X, bus read X, and then the line gets ripped
[932.48s -> 935.36s]  away before they actually do the write, and so they're both sitting there on the same
[935.36s -> 937.64s]  instruction just continuously.
[937.64s -> 943.06s]  So you should think about it as, once I get the line in the writable state, here
[943.06s -> 947.36s]  there needs to be a comparison with another value and a write.
[947.36s -> 949.24s]  So the cache logic has to...
[949.24s -> 954.44s]  The processor just has to be a little bit more complex for that.
[954.44s -> 955.44s]  Cool.
[955.44s -> 956.44s]  Okay.
[956.84s -> 957.84s]  New topic?
[957.84s -> 958.84s]  Any other questions?
[958.84s -> 960.84s]  Well, here, let's go here, and then here.
[960.84s -> 961.84s]  Yeah.
[961.84s -> 965.84s]  Sorry, I was doing back into a couple of weeks, but I was wondering if we could go
[965.84s -> 968.84s]  over, like, map reduce from distributed data parallel.
[968.84s -> 970.84s]  Sure, yeah, yeah, absolutely.
[970.84s -> 974.12s]  Let's see, let me cut this off.
[974.12s -> 975.64s]  And what specific is the question?
[975.64s -> 981.24s]  So, all right, I would think that the run map would use, like, implementation, the
[981.24s -> 984.96s]  job with, like, a bunch of diagrams, and you have the key values and how they're
[984.96s -> 985.96s]  increased.
[986.48s -> 988.48s]  And then you're talking about MapReduce itself, right?
[988.48s -> 989.48s]  Right, yeah.
[989.48s -> 991.48s]  So it kind of looked a little bit like this?
[991.48s -> 992.48s]  Yeah.
[992.48s -> 993.48s]  Yeah.
[993.48s -> 994.48s]  Let me just throw a little of that.
[994.48s -> 995.48s]  Okay.
[995.48s -> 1002.00s]  So, in all of these programming systems, the first thing to think about is, you
[1002.00s -> 1009.80s]  know, kind of, what is my API, and what is the meaning of the API functions?
[1009.80s -> 1012.56s]  And in MapReduce, there actually is only two.
[1012.56s -> 1016.24s]  So the history of this, and I won't spend too much time on the history, is MapReduce
[1016.24s -> 1019.68s]  was a very simple paper that came out of Google, and I can't remember what it was.
[1019.68s -> 1021.04s]  It was probably, like, the early 2000s.
[1021.04s -> 1024.08s]  This was before Spark, before a lot of...
[1024.08s -> 1028.36s]  And the Google folks were saying that there's a lot of jobs that have to run at Google
[1028.36s -> 1031.84s]  on an insane amount of data.
[1031.84s -> 1035.12s]  And they was like, well, to handle an insane amount of data, like our entire index of
[1035.12s -> 1040.00s]  the web, we've actually built, big data, we've actually built this shared file system,
[1040.00s -> 1041.00s]  right?
[1041.44s -> 1044.52s]  So you basically, all the data in the world is in this file system, and the file system
[1044.52s -> 1047.52s]  is distributed across a ton of computers.
[1047.52s -> 1051.24s]  And they basically had a bunch of applications, like you can just imagine if you're like
[1051.24s -> 1055.36s]  DevOps or something, you have a bunch of applications that are kind of data mining
[1055.36s -> 1059.32s]  stuff on this big database.
[1059.32s -> 1065.96s]  So the canonical example here would be like mining through a bunch of web logs or something
[1065.96s -> 1066.96s]  like that.
[1066.96s -> 1072.60s]  Imagine you just had a huge file, terabytes of text, and every line in the file was like
[1072.60s -> 1076.84s]  a log from Apache.
[1076.84s -> 1079.12s]  So there's two function calls, right?
[1079.12s -> 1083.36s]  The first function call was called map, and it was called map because it was inspired
[1083.36s -> 1090.04s]  by data parallel map, and the second function call was called reduce, which was inspired
[1090.04s -> 1094.32s]  by reduce, but they're a little bit different than the definitions that I gave you in
[1094.36s -> 1097.56s]  the data parallel thinking lecture.
[1097.56s -> 1098.56s]  Map is the following.
[1098.56s -> 1105.28s]  Let's say we're going to map, in this case it was, right, yeah, it was, we're
[1105.28s -> 1108.60s]  going to iterate over all lines of all files.
[1108.60s -> 1112.76s]  Imagine you just have this gigantic terabyte file with a ton of lines in it, and different
[1112.76s -> 1116.52s]  pieces of that file were just spread across different machines.
[1116.52s -> 1119.52s]  And so before, like if you were a programmer at Google, you had to like somehow like
[1119.52s -> 1123.04s]  get the data from the machine, bring it in, and do a bunch of stuff, right?
[1123.04s -> 1126.80s]  Like it was just complicated because your files were spread out all over the machine.
[1126.80s -> 1132.28s]  So map just says for every line in the file in this case, or for every thing, I want
[1132.28s -> 1137.20s]  you to run this function f, just like map, okay?
[1137.20s -> 1143.40s]  Now in MapReduce specifically, the output of map, you know, if the input is like a
[1143.40s -> 1150.10s]  line of the file, the output is not just like some arbitrary type T, but is actually
[1150.10s -> 1152.68s]  a key value pair, okay?
[1152.68s -> 1157.40s]  So in this case, if I remember the example correctly, the input was a string, a line in
[1157.40s -> 1162.88s]  the file, and then the f, the function that was mapped onto all the lines of the file,
[1162.88s -> 1169.24s]  for every line in every file basically produced a key value pair, and in this case
[1169.24s -> 1174.20s]  the key was like the user agent, the browser, and the value in this case was actually just
[1174.20s -> 1175.74s]  the number one.
[1175.74s -> 1179.48s]  But you could actually imagine like the key value pairs were like browser error message
[1179.48s -> 1180.48s]  or something like that.
[1180.48s -> 1182.94s]  Just keys and values.
[1182.94s -> 1187.72s]  And then there's some magic that happens, and this is where MapReduce was really clever
[1187.72s -> 1192.92s]  in that so much simplicity was able to work for a lot of things, or really janky in
[1192.92s -> 1198.00s]  that this is very hard coded, is that the system then takes all of those key value
[1198.00s -> 1199.00s]  pairs.
[1199.00s -> 1203.16s]  Now keep in mind that that map function can run across all the machines if your file is
[1203.16s -> 1208.48s]  distributed across a thousand machines, well it'll just run that map function on the lines
[1208.48s -> 1213.12s]  of the file on the machine that already holds the file, so the data doesn't move.
[1213.12s -> 1218.22s]  And now comes the big data communication part.
[1218.22s -> 1223.96s]  Every little, every invocation of f produces a key and a value, and then what MapReduce
[1223.96s -> 1229.88s]  says is hard coded by design in the API, we're going to organize all of the tuples
[1229.88s -> 1233.80s]  that have the same key into a file.
[1233.80s -> 1237.68s]  So I have different files now for all of the different keys.
[1237.68s -> 1241.76s]  And that file is just a list of all the values that match that key.
[1241.76s -> 1243.84s]  So that's where the data just moved everywhere.
[1243.84s -> 1246.28s]  It's like a big resort.
[1246.28s -> 1251.76s]  In Spark, this would be called group by key or shuffle, yeah.
[1251.76s -> 1258.02s]  So map produces keys and values, and they get shuffled all together and they're reorganized,
[1258.02s -> 1261.00s]  here's all the data for this key, here's all the data for that key, here's all
[1261.00s -> 1262.56s]  the data for that key.
[1262.56s -> 1267.06s]  And because they had a shared file system, they already had it built, they actually
[1267.44s -> 1270.06s]  stored this information in files.
[1270.06s -> 1274.82s]  And so your parallel file system was exactly the transport mechanism to get all this data
[1274.82s -> 1278.26s]  shuffling around.
[1278.26s -> 1281.54s]  Then you have reduce.
[1281.54s -> 1288.14s]  And reduce takes as arguments the key and a list of values and produces a result.
[1288.14s -> 1296.00s]  So the reduce function is actually mapped onto all of the unique keys.
[1296.00s -> 1301.40s]  So the reduce function gets its input, the key that it's working on, plus the list of
[1301.40s -> 1306.24s]  values that were associated with that key, essentially the contents of a file.
[1306.24s -> 1310.26s]  And now at this point it does whatever the heck it wants and produces a result.
[1310.26s -> 1314.36s]  And presumably it was called reduce because in this case it takes all of those values
[1314.36s -> 1321.52s]  with the same key and in this case just aggregated the number of lines with that
[1321.52s -> 1324.04s]  key and returned the result.
[1324.08s -> 1327.96s]  So the name reduce is actually not parallel reduce or anything like that.
[1327.96s -> 1336.40s]  It's actually map group by key, map, from a map where every element of the map gets a
[1336.40s -> 1339.96s]  list and returns it to a value.
[1339.96s -> 1342.84s]  So in modern parallel programming that's what it is.
[1342.84s -> 1345.92s]  And so if you're like, that's pretty weird, I had all these computers, my data was
[1345.92s -> 1351.24s]  distributed amongst all the computers, really our way of communicating would be to
[1351.24s -> 1356.40s]  write to the file system and then read it back in from another node.
[1356.40s -> 1358.96s]  That's what Spark said, that's stupid.
[1358.96s -> 1363.88s]  And said, oh, we should just like do all this in memory.
[1363.88s -> 1367.08s]  And that was the whole idea behind Spark.
[1367.08s -> 1370.86s]  And do it with a proper set of data parallel operators, not just map reduce and group
[1370.86s -> 1371.86s]  by key.
[1371.86s -> 1372.86s]  Yeah.
[1372.86s -> 1377.78s]  Can you push on that a little bit more, like why map reduce required?
[1377.78s -> 1379.84s]  This was how it was implemented.
[1379.84s -> 1385.08s]  And the only thing you can write in map reduce is map to produce a bunch of key values.
[1385.08s -> 1387.88s]  Then the system does a group by key for you.
[1387.88s -> 1390.20s]  You have no question about that.
[1390.20s -> 1394.24s]  And then you run another map, which is actually called the reducer function, which
[1394.24s -> 1400.90s]  is map the reducer function onto all unique keys to produce one output per key.
[1400.90s -> 1405.48s]  So the reducer function for a given key takes all of the values with that key and
[1405.48s -> 1407.28s]  reduces it to something.
[1407.28s -> 1409.08s]  But that's map over all keys.
[1409.08s -> 1410.08s]  Yeah.
[1410.08s -> 1412.08s]  I'm not sure about this.
[1412.08s -> 1418.08s]  I think one reason why they were using this process for the original map reduce
[1418.08s -> 1424.08s]  is because, like, when I saw how this was reached by the time, I was like, I can't
[1424.08s -> 1425.08s]  begin.
[1425.08s -> 1430.08s]  They don't really care about the latency because the amount of data is not that huge
[1430.08s -> 1431.08s]  compared to right now.
[1431.08s -> 1436.08s]  Like, for example, the original Google file system into map reduce, there's no replication
[1436.08s -> 1437.88s]  plans to server.
[1437.88s -> 1441.88s]  It's like a single point of view here.
[1441.88s -> 1442.88s]  OK.
[1442.88s -> 1445.08s]  So the comment here was about some of the motivations of map reduce.
[1445.08s -> 1451.08s]  And I wasn't there, so I can't speak authoritatively at all about it.
[1451.08s -> 1454.76s]  But there was comments about, oh, like, the disk latency wasn't so bad.
[1454.76s -> 1457.96s]  But first of all, that gives me a teaching opportunity.
[1457.96s -> 1465.48s]  Let's first think about this as, am I worried about going to disk because of latency?
[1465.48s -> 1467.28s]  I'm not worried about disk latency at all.
[1467.28s -> 1473.68s]  Why am I not worried about disk latency?
[1473.68s -> 1481.08s]  Why am I a heck of a lot more worried about disk bandwidth?
[1481.08s -> 1483.88s]  I care about throughput, but latency can impact throughput, right?
[1483.88s -> 1488.68s]  Like, if I do one operation and then wait for a long time and then do the next operation,
[1488.68s -> 1490.68s]  latency can definitely impact throughput.
[1490.68s -> 1493.68s]  In this case, I'm only worried about throughput, though.
[1493.88s -> 1497.88s]  You have a ton of nodes and they're all trying to write back to disk.
[1497.88s -> 1499.88s]  Like, that's a huge bottleneck.
[1502.88s -> 1503.88s]  Right.
[1503.88s -> 1507.88s]  But first of all, that's why definitely I care about bandwidth.
[1507.88s -> 1511.88s]  But I guess one thing I want to stress is, why do I not care about latency?
[1511.88s -> 1513.88s]  Can you just hide all that?
[1513.88s -> 1516.88s]  This is massively parallel, right?
[1516.88s -> 1518.88s]  There's billions of lines in a file.
[1518.88s -> 1521.88s]  If I have to write a key value pair out,
[1522.08s -> 1525.08s]  I send the write out and I go start working on something else.
[1525.08s -> 1530.08s]  Or in the reduce phase, if I'm reducing over all the different keys,
[1530.08s -> 1534.08s]  I start loading the data for the next key now while I'm working on this key.
[1534.08s -> 1538.08s]  So, all of our ideas of multithreading, whenever you have massive parallelism,
[1538.08s -> 1541.08s]  latency can be hidden.
[1541.08s -> 1543.08s]  You care about bandwidth and throughput.
[1543.08s -> 1547.08s]  So, the real problem is that in between these phases of computation,
[1547.28s -> 1550.28s]  the entire data set, terabytes of data,
[1550.28s -> 1554.28s]  is read from disk and put back on disk and then read back in.
[1555.28s -> 1562.28s]  And that's true even if the group by key doesn't really do much at all.
[1562.28s -> 1564.28s]  It's all narrow dependencies.
[1564.28s -> 1567.28s]  And so, people at the time were like,
[1567.28s -> 1573.28s]  wow, I can write code in ten lines of code that use a thousand machines.
[1573.48s -> 1575.48s]  Like there's no, like I get fault tolerance,
[1575.48s -> 1578.48s]  I get this parallelism, it's all handled in those two abstractions.
[1578.48s -> 1582.48s]  And the, you know, some of the folks that did Spark
[1582.48s -> 1585.48s]  and there's this paper that I have in my lecture slides about
[1585.48s -> 1588.48s]  or in Kunle's lecture slides now, I guess,
[1588.48s -> 1590.48s]  about this paper that somebody wrote that said,
[1590.48s -> 1592.48s]  yeah, but the applications that you're,
[1592.48s -> 1596.48s]  like people were like, oh, we can do everything in Spark and MapReduce
[1596.48s -> 1599.48s]  because it's so parallel and so awesome
[1599.48s -> 1601.48s]  but so low bandwidth.
[1601.68s -> 1606.68s]  And then some folks popped open a laptop
[1606.68s -> 1609.68s]  and said you're using five thousand nodes in a Spark
[1609.68s -> 1612.68s]  or MapReduce cluster to process a data set
[1612.68s -> 1615.68s]  that's like five hundred gigabytes or something like that.
[1615.68s -> 1618.68s]  Five hundred gigabytes fits in memory in my laptop.
[1618.68s -> 1620.68s]  And they showed that one core of a laptop
[1620.68s -> 1624.68s]  could outperform a five thousand node Spark cluster
[1624.68s -> 1627.68s]  just because people were getting enamored with parallelism
[1627.68s -> 1629.68s]  and forgetting about locality and bandwidth.
[1631.68s -> 1634.68s]  What kind of laptop fits five hundred gigabytes?
[1637.68s -> 1640.68s]  I think the actual experiment was,
[1640.68s -> 1644.68s]  it was a laptop actually,
[1644.68s -> 1646.68s]  I could go bring it up, I'll send it,
[1646.68s -> 1647.68s]  is they streamed it.
[1647.68s -> 1649.68s]  So they streamed it off disk
[1649.68s -> 1654.68s]  and showed that instead of going back out to disk every time,
[1654.68s -> 1657.68s]  just keep a chunk in memory and then do five iterations
[1657.68s -> 1660.68s]  and then you can keep up with the Spark cluster.
[1660.88s -> 1663.88s]  Another version of it would be these days in 2023,
[1663.88s -> 1666.88s]  it would be for three bucks an hour
[1666.88s -> 1668.88s]  I can go get a machine on AWS
[1668.88s -> 1670.88s]  with five hundred and twelve gigabytes of memory.
[1670.88s -> 1672.88s]  And most of us most of the time
[1672.88s -> 1674.88s]  are not working on problems
[1674.88s -> 1677.88s]  that are exceeding what you can buy for pennies.
[1677.88s -> 1679.88s]  And so it makes a lot more sense
[1679.88s -> 1682.88s]  to think about locality and brace that
[1682.88s -> 1686.88s]  and maybe scale to four terabyte size machines
[1687.08s -> 1690.08s]  instead of trying to make your code robust
[1690.08s -> 1692.08s]  to fault tolerance and failure
[1692.08s -> 1694.08s]  on ten thousand nodes in the cluster.
[1695.08s -> 1697.08s]  There's a great article
[1697.08s -> 1699.08s]  by Amazon Web Streaming,
[1699.08s -> 1701.08s]  the video folks,
[1701.08s -> 1703.08s]  Prime Video.
[1703.08s -> 1706.08s]  So some of you may have heard of Amazon Lambdas.
[1706.08s -> 1708.08s]  Amazon Lambdas is like you just give them a function
[1708.08s -> 1711.08s]  and say, hey, whenever I need to run this function
[1711.08s -> 1713.08s]  I'll let you know and run it for me.
[1713.08s -> 1715.08s]  I don't want to think about servers and stuff like that.
[1715.28s -> 1717.28s]  The idea is to do a computation
[1717.28s -> 1719.28s]  maybe a second or so less.
[1719.28s -> 1721.28s]  It's great because if I have a website
[1721.28s -> 1723.28s]  and I want it to always be running
[1723.28s -> 1725.28s]  and nobody ever comes
[1725.28s -> 1727.28s]  I don't pay a dime if nobody's coming.
[1727.28s -> 1729.28s]  And if people burst
[1729.28s -> 1731.28s]  it's really great because
[1731.28s -> 1733.28s]  they'll just execute all of those parallel functions
[1733.28s -> 1735.28s]  for all these requests
[1735.28s -> 1737.28s]  and I don't have to worry about scaling
[1737.28s -> 1739.28s]  even though the cost of those functions
[1739.28s -> 1741.28s]  is actually quite high.
[1741.28s -> 1743.28s]  So Prime Video built their whole system
[1743.48s -> 1745.48s]  on Lambdas.
[1745.48s -> 1747.48s]  Here's a little chunk of video
[1747.48s -> 1749.48s]  go do it with a Lambda and stuff like that.
[1749.48s -> 1751.48s]  And this is Amazon
[1751.48s -> 1753.48s]  who's trying to sell the Lambda service
[1753.48s -> 1755.48s]  and their Prime Video team has this great blog post
[1755.48s -> 1757.48s]  from like a year and a half ago
[1757.48s -> 1759.48s]  which says we built our whole system on Amazon Lambda
[1759.48s -> 1761.48s]  scale out massively
[1761.48s -> 1763.48s]  and now we've returned it back
[1763.48s -> 1765.48s]  to a few small servers
[1765.48s -> 1767.48s]  because of locality and other things.
[1767.48s -> 1769.48s]  And so they said they're ten times more cost efficient
[1769.48s -> 1771.48s]  with a small number of big servers
[1771.68s -> 1773.68s]  than with thousands of tiny servers
[1773.68s -> 1775.68s]  because communication just bites you over
[1775.68s -> 1777.68s]  and over and over in any system.
[1777.68s -> 1779.68s]  They still get the same level of performance
[1779.68s -> 1781.68s]  is that like the data?
[1781.68s -> 1783.68s]  Yeah, like ISO performance
[1783.68s -> 1785.68s]  10x lower cost.
[1785.68s -> 1787.68s]  And that's similar to some of these observations with Spark
[1787.68s -> 1789.68s]  so these systems that are scale out
[1789.68s -> 1791.68s]  there's one reason why you always want
[1791.68s -> 1793.68s]  you want to think about scaling out
[1793.68s -> 1795.68s]  if you believe that the data sets
[1795.68s -> 1797.68s]  you're going to work on
[1797.68s -> 1799.68s]  are going to be so big
[1799.88s -> 1801.88s]  and there's a chance of keeping them in memory any time soon
[1801.88s -> 1803.88s]  that's the reason why
[1803.88s -> 1805.88s]  these systems are super valuable
[1805.88s -> 1807.88s]  right, like if I have a database of terabytes
[1807.88s -> 1809.88s]  and terabytes of data
[1809.88s -> 1811.88s]  I have no hope of ever keeping that in one node
[1811.88s -> 1813.88s]  so I should build fundamentally from day one
[1813.88s -> 1815.88s]  in terms of this data can't fit
[1815.88s -> 1817.88s]  anywhere and let's scale out.
[1817.88s -> 1819.88s]  Okay, but I do want to get this back on topic
[1819.88s -> 1821.88s]  so let's move
[1821.88s -> 1823.88s]  to another question
[1823.88s -> 1825.88s]  yeah
[1826.88s -> 1828.88s]  let's actually do a cache coherence one
[1828.88s -> 1830.88s]  I think that would be a good
[1830.88s -> 1832.88s]  I actually did it in my lock free lecture recently
[1832.88s -> 1834.88s]  so let's just pull up those slides
[1834.88s -> 1836.88s]  so we started the lock free lecture
[1836.88s -> 1838.88s]  in terms of this MSI protocol
[1838.88s -> 1840.88s]  let me go ahead and pop it up
[1840.88s -> 1842.88s]  now on the exam
[1842.88s -> 1844.88s]  well first of all it's open notes
[1844.88s -> 1846.88s]  and second of all
[1846.88s -> 1848.88s]  if we've ever asked you to
[1848.88s -> 1850.88s]  do anything with the MSI protocol
[1850.88s -> 1852.88s]  I've given you this diagram
[1852.88s -> 1854.88s]  I've given you this diagram
[1854.88s -> 1856.88s]  I've given you this diagram on the exam
[1856.88s -> 1858.88s]  so memorizing the diagram
[1858.88s -> 1860.88s]  probably not necessary
[1860.88s -> 1862.88s]  but in general
[1862.88s -> 1864.88s]  it's hard for you to answer a question
[1864.88s -> 1866.88s]  without that diagram fully in your head
[1866.88s -> 1868.88s]  so they're kind of the same, right
[1868.88s -> 1870.88s]  like if you're learning the state transitions
[1870.88s -> 1872.88s]  so should we just go over it?
[1872.88s -> 1874.88s]  there's a question especially
[1874.88s -> 1876.88s]  with the atomic tasks
[1876.88s -> 1878.88s]  we were saying that was like a bus read
[1878.88s -> 1880.88s]  but I thought we could do a reading
[1880.88s -> 1882.88s]  just the default MSI
[1882.88s -> 1884.88s]  the default MSI has to do the bus read
[1884.88s -> 1886.88s]  and then the bus read, is that incorrect?
[1886.88s -> 1888.88s]  that is incorrect
[1888.88s -> 1890.88s]  so let's just say I have
[1890.88s -> 1892.88s]  let me make sure, I believe it's incorrect
[1892.88s -> 1894.88s]  so let's just say we have an address
[1894.88s -> 1896.88s]  that is not in my cache
[1896.88s -> 1898.88s]  so if it's not in my cache
[1898.88s -> 1900.88s]  that address is conceptually in what state?
[1900.88s -> 1902.88s]  it's invalid
[1902.88s -> 1904.88s]  now keep in mind I have a cache
[1904.88s -> 1906.88s]  and every line in the cache says
[1906.88s -> 1908.88s]  this is the current address
[1908.88s -> 1910.88s]  so it's not like the cache has a bit
[1910.88s -> 1912.88s]  it's invalid on all possible addresses
[1912.88s -> 1914.88s]  if it's not in the cache, it's invalid
[1914.88s -> 1916.88s]  so then I do a read
[1916.88s -> 1918.88s]  and if I do a read
[1918.88s -> 1920.88s]  I will read it into the shared state
[1920.88s -> 1922.88s]  correct?
[1922.88s -> 1924.88s]  and then let's say
[1924.88s -> 1926.88s]  I want to write to it
[1926.88s -> 1928.88s]  if nobody else has read it
[1928.88s -> 1931.88s]  it can be in the modified state now
[1931.88s -> 1933.88s]  ok, so what is the question?
[1933.88s -> 1935.88s]  the question was like
[1935.88s -> 1938.88s]  that seems like a two-stage process
[1938.88s -> 1940.88s]  that is a two-stage process
[1940.88s -> 1942.88s]  so for atomic CAS
[1942.88s -> 1944.88s]  it's worth reading and then writing
[1944.88s -> 1946.88s]  is it that two-stage process?
[1946.88s -> 1948.88s]  absolutely not, that's why I'm saying
[1948.88s -> 1950.88s]  it's a write bus transaction
[1950.88s -> 1952.88s]  because think of what can happen here
[1952.88s -> 1954.88s]  let's say we don't implement CAS atomically
[1954.88s -> 1956.88s]  the processor will elevate
[1956.88s -> 1958.88s]  to the shared state
[1958.88s -> 1960.88s]  if it's not atomic
[1960.88s -> 1962.88s]  some other processor can now do whatever it wants
[1962.88s -> 1964.88s]  it might generate bus traffic
[1964.88s -> 1966.88s]  it might write to the value
[1966.88s -> 1968.88s]  it might read from the value
[1968.88s -> 1970.88s]  and it will do its thing
[1970.88s -> 1972.88s]  and then when I do the write
[1972.88s -> 1974.88s]  perhaps it's still in shared
[1974.88s -> 1976.88s]  or not
[1976.88s -> 1978.88s]  and it will then get elevated
[1978.88s -> 1980.88s]  so that's why the nature of CAS
[1980.88s -> 1982.88s]  or test and set
[1982.88s -> 1984.88s]  or any of these things being atomic
[1984.88s -> 1986.88s]  means you need to think about them
[1986.88s -> 1988.88s]  from the performance protocol
[1988.88s -> 1990.88s]  as a write
[1990.88s -> 1992.88s]  irregardless of whether the variable
[1992.88s -> 1994.88s]  is actually updated by the transaction
[1994.88s -> 1996.88s]  from I all the way to M right at once
[2000.88s -> 2002.88s]  otherwise I would need special
[2002.88s -> 2004.88s]  other logic to ensure
[2004.88s -> 2006.88s]  the atomicity of that operation
[2006.88s -> 2008.88s]  in light of MSI
[2008.88s -> 2010.88s]  this is something we did last time
[2010.88s -> 2012.88s]  would you like to go through a sequence?
[2012.88s -> 2014.88s]  no? or are there any higher priority items?
[2014.88s -> 2016.88s]  I see some yeses
[2016.88s -> 2018.88s]  yes? okay
[2018.88s -> 2020.88s]  seems to be general, okay
[2020.88s -> 2022.88s]  so this is the same sequence as last time
[2022.88s -> 2024.88s]  let's do the whole thing last time
[2024.88s -> 2026.88s]  let's go ahead and just do it
[2026.88s -> 2028.88s]  and I think this actually makes sense
[2028.88s -> 2030.88s]  if we have
[2030.88s -> 2032.88s]  like I just
[2032.88s -> 2034.88s]  physically feeling it
[2034.88s -> 2036.88s]  if someone wants to come up here
[2036.88s -> 2038.88s]  if we have two caches it can be very helpful
[2038.88s -> 2040.88s]  so let's just do it
[2040.88s -> 2042.88s]  I have somebody else
[2042.88s -> 2044.88s]  come here
[2044.88s -> 2046.88s]  I need two folks
[2046.88s -> 2048.88s]  if you're sleeping and you need some energy
[2048.88s -> 2050.88s]  it's like the first day of class
[2050.88s -> 2052.88s]  so you're going to be cache 1
[2052.88s -> 2054.88s]  nice
[2054.88s -> 2056.88s]  I can be cache 2 or somebody else can be cache 2
[2056.88s -> 2058.88s]  oh we got cache 2
[2058.88s -> 2060.88s]  why not? okay
[2060.88s -> 2062.88s]  so here we go
[2062.88s -> 2064.88s]  this is what I want to
[2064.88s -> 2066.88s]  nice
[2066.88s -> 2068.88s]  you've got to be above 6 feet tall
[2068.88s -> 2070.88s]  to be a cache in this class
[2070.88s -> 2072.88s]  that's why I'm not volunteering
[2072.88s -> 2074.88s]  so here's the sequence because you can look at it
[2074.88s -> 2076.88s]  so let's say that
[2076.88s -> 2078.88s]  cache 1 is in the same
[2078.88s -> 2080.88s]  so let's have ub0 and ub1
[2080.88s -> 2082.88s]  I'm forgetting names sorry
[2082.88s -> 2084.88s]  cache 0 and cache 1
[2084.88s -> 2086.88s]  and let's get started here
[2086.88s -> 2088.88s]  and I want you to communicate
[2088.88s -> 2090.88s]  like the protocol
[2090.88s -> 2092.88s]  and then you can write on the board
[2092.88s -> 2094.88s]  like the state that your line is in
[2094.88s -> 2096.88s]  so you can write on the board something like
[2096.88s -> 2098.88s]  I have x in the shared state
[2098.88s -> 2100.88s]  so at the moment
[2100.88s -> 2102.88s]  nobody has anything in their cache
[2102.88s -> 2104.88s]  so you have nothing
[2104.88s -> 2106.88s]  okay
[2106.88s -> 2108.88s]  so let's just say the first thing happens
[2108.88s -> 2110.88s]  is you say you want to load
[2110.88s -> 2112.88s]  okay I have it in the shared state
[2112.88s -> 2114.88s]  well not yet
[2114.88s -> 2116.88s]  you actually have to say I want to load
[2116.88s -> 2118.88s]  oh I want to load
[2118.88s -> 2120.88s]  and now you have to acknowledge
[2120.88s -> 2122.88s]  and once you get the acknowledgement
[2122.88s -> 2124.88s]  you're like oh that's cool
[2124.88s -> 2126.88s]  I know that nobody is writing to it
[2126.88s -> 2128.88s]  so now I can put it in the shared state
[2128.88s -> 2130.88s]  so let's go ahead and write x in shared
[2130.88s -> 2132.88s]  there
[2132.88s -> 2134.88s]  alright
[2134.88s -> 2136.88s]  so we got that acknowledgement
[2136.88s -> 2138.88s]  and then again
[2138.88s -> 2140.88s]  you load x
[2140.88s -> 2142.88s]  so what do you do?
[2142.88s -> 2144.88s]  you don't have to do anything
[2144.88s -> 2146.88s]  right? and that's an important detail
[2146.88s -> 2148.88s]  there's not even a message
[2148.88s -> 2150.88s]  that has to be sent
[2150.88s -> 2152.88s]  why does a message not need to be sent?
[2152.88s -> 2154.88s]  by the nature of cache 0
[2154.88s -> 2156.88s]  having it in the shared state
[2156.88s -> 2158.88s]  we are guaranteed
[2158.88s -> 2160.88s]  that
[2160.88s -> 2162.88s]  everybody else
[2162.88s -> 2164.88s]  might have it in the shared state
[2164.88s -> 2166.88s]  but they certainly don't have it in the modified state
[2166.88s -> 2168.88s]  and it's okay to continue to read
[2168.88s -> 2170.88s]  so nothing changes
[2170.88s -> 2172.88s]  and we're good
[2172.88s -> 2174.88s]  so now you need to write
[2174.88s -> 2176.88s]  yeah
[2176.88s -> 2178.88s]  but I want to write
[2178.88s -> 2180.88s]  okay I see that
[2180.88s -> 2182.88s]  I still do nothing right?
[2182.88s -> 2184.88s]  yeah so you're going to do nothing
[2184.88s -> 2186.88s]  and why do you shrug this off?
[2186.88s -> 2188.88s]  you don't even have it in the cache
[2188.88s -> 2190.88s]  whatever the heck you want
[2190.88s -> 2192.88s]  I'm not doing anything
[2192.88s -> 2194.88s]  so what are you going to do in your cache now?
[2194.88s -> 2196.88s]  we're going to move from the s to the m state
[2196.88s -> 2198.88s]  and that m state means
[2198.88s -> 2200.88s]  now that cache 0
[2200.88s -> 2202.88s]  is guaranteed that no other cache
[2202.88s -> 2204.88s]  has the value
[2204.88s -> 2206.88s]  so you can proceed with your write
[2206.88s -> 2208.88s]  and in this case actually
[2208.88s -> 2210.88s]  let's go ahead and write the value
[2210.88s -> 2212.88s]  you just updated the value
[2212.88s -> 2214.88s]  so it's x in m and has the value 1
[2214.88s -> 2216.88s]  and so if I'm like the third party
[2216.88s -> 2218.88s]  you know the under 6 foot memory system
[2218.88s -> 2220.88s]  you know like there's
[2220.88s -> 2222.88s]  there's a value of x and y
[2222.88s -> 2224.88s]  out here in memory
[2224.88s -> 2226.88s]  and what's the value of x?
[2226.88s -> 2228.88s]  it's actually still 0
[2228.88s -> 2230.88s]  it's stale
[2230.88s -> 2232.88s]  okay now let's keep going
[2232.88s -> 2234.88s]  so processor 0
[2234.88s -> 2236.88s]  okay so you're being greedy
[2236.88s -> 2238.88s]  you're going to write again
[2238.88s -> 2240.88s]  and you're going to update the value to 2
[2240.88s -> 2242.88s]  okay so you're being greedy
[2242.88s -> 2244.88s]  value 2
[2244.88s -> 2246.88s]  which again no update
[2246.88s -> 2248.88s]  no coherence traffic
[2248.88s -> 2250.88s]  needs to happen because we're guaranteed
[2250.88s -> 2252.88s]  that cache 1 does not have the data
[2252.88s -> 2254.88s]  and there's no business telling somebody
[2254.88s -> 2256.88s]  something that they don't need to know
[2256.88s -> 2258.88s]  value is 2 right?
[2258.88s -> 2260.88s]  cool okay
[2260.88s -> 2262.88s]  now processor 1
[2262.88s -> 2264.88s]  now you're going to read it
[2264.88s -> 2266.88s]  did I get that right?
[2266.88s -> 2268.88s]  oh sorry
[2268.88s -> 2270.88s]  you're storing
[2270.88s -> 2272.88s]  so you're missing stuff that has to happen
[2280.88s -> 2282.88s]  so hold on
[2282.88s -> 2284.88s]  let's sequence this very precisely
[2284.88s -> 2286.88s]  you correctly said hey I need to read
[2286.88s -> 2288.88s]  how do you react?
[2292.88s -> 2294.88s]  yeah so you've got to flush
[2294.88s -> 2296.88s]  so in some sense you send the value back to me
[2296.88s -> 2298.88s]  and I update this
[2298.88s -> 2300.88s]  and I update this with the value
[2300.88s -> 2302.88s]  2
[2302.88s -> 2304.88s]  and then what do you do about your cache line?
[2304.88s -> 2306.88s]  and then
[2306.88s -> 2308.88s]  yeah
[2308.88s -> 2310.88s]  okay and it's invalid
[2310.88s -> 2312.88s]  and now you have the value of this cache line
[2312.88s -> 2314.88s]  which you read from memory
[2314.88s -> 2316.88s]  and it has the value 2
[2316.88s -> 2318.88s]  and then you update its value to 3
[2318.88s -> 2320.88s]  you see that whole sequence?
[2320.88s -> 2322.88s]  now if those of you are wondering
[2322.88s -> 2324.88s]  why did this have to go back to memory
[2324.88s -> 2326.88s]  and then all the way back
[2326.88s -> 2328.88s]  why not just transfer the cache line
[2328.88s -> 2330.88s]  from one cache to the other
[2330.88s -> 2332.88s]  any modern system has this
[2332.88s -> 2334.88s]  so you studied MSI
[2334.88s -> 2336.88s]  you studied MESI
[2336.88s -> 2338.88s]  and let's talk about that for a second
[2338.88s -> 2340.88s]  Intel AMD modern
[2340.88s -> 2342.88s]  cache coherent systems are actually five stage systems
[2342.88s -> 2344.88s]  and that fifth stage
[2344.88s -> 2346.88s]  is a special shared state
[2346.88s -> 2348.88s]  which basically says
[2348.88s -> 2350.88s]  if someone else needs the line
[2350.88s -> 2352.88s]  and I have it
[2352.88s -> 2354.88s]  I will provide it to them without them
[2354.88s -> 2356.88s]  and I'm going to go get it from memory
[2356.88s -> 2358.88s]  so it's just an obvious optimization
[2358.88s -> 2360.88s]  but that optimization is not an MSI
[2360.88s -> 2362.88s]  okay where are we now?
[2362.88s -> 2364.88s]  I think I just read it
[2364.88s -> 2366.88s]  so your next one is you just read
[2366.88s -> 2368.88s]  and that's
[2368.88s -> 2370.88s]  we're good
[2370.88s -> 2372.88s]  and now
[2372.88s -> 2374.88s]  processor 0 wants to load
[2374.88s -> 2376.88s]  so I gotta tell you to write it
[2376.88s -> 2378.88s]  to flush it to memory
[2378.88s -> 2380.88s]  so first of all you go I got it
[2380.88s -> 2382.88s]  we gotta do a flush
[2382.88s -> 2384.88s]  flush to 3
[2384.88s -> 2386.88s]  and now
[2396.88s -> 2398.88s]  okay so this is an interesting detail
[2398.88s -> 2400.88s]  so you went to invalid
[2400.88s -> 2402.88s]  now first of all
[2402.88s -> 2404.88s]  here's a question, this is not the MSI
[2404.88s -> 2406.88s]  protocol that you executed
[2406.88s -> 2408.88s]  but is it correct
[2408.88s -> 2410.88s]  as in like is it correct
[2410.88s -> 2412.88s]  cash coherence
[2412.88s -> 2414.88s]  it's absolutely correct cash coherence
[2414.88s -> 2416.88s]  well
[2416.88s -> 2418.88s]  I want to make it clear that what you did
[2418.88s -> 2420.88s]  is correct and that cash coherence
[2420.88s -> 2422.88s]  is still working
[2422.88s -> 2424.88s]  there's just an inefficiency
[2424.88s -> 2426.88s]  and the inefficiency is you could have kept it
[2426.88s -> 2428.88s]  in your shared state
[2428.88s -> 2430.88s]  keep in mind that the fact that processor 1
[2430.88s -> 2432.88s]  is invalid
[2432.88s -> 2434.88s]  this being in the shared state is okay
[2434.88s -> 2436.88s]  because all this guarantees is that
[2436.88s -> 2438.88s]  cash 0 can't write to the value
[2438.88s -> 2440.88s]  so it was correct
[2440.88s -> 2442.88s]  just not a correct application of the MSI protocol
[2442.88s -> 2444.88s]  so yes if you were following MSI
[2444.88s -> 2446.88s]  you would have said I'm going to keep this around
[2446.88s -> 2448.88s]  in shared state because there's no reason for me to
[2448.88s -> 2450.88s]  drop it and I might read from it again
[2450.88s -> 2452.88s]  and if I read from it again it'd be great
[2452.88s -> 2454.88s]  not to have to go get it from memory
[2454.88s -> 2456.88s]  right so now we're in a nice shared state
[2456.88s -> 2458.88s]  okay
[2458.88s -> 2460.88s]  and now it might get a little repetitive but let's go ahead and do it anyways
[2460.88s -> 2462.88s]  so
[2462.88s -> 2464.88s]  we have store
[2464.88s -> 2466.88s]  so you are storing
[2466.88s -> 2468.88s]  do you have it?
[2468.88s -> 2470.88s]  let's see
[2470.88s -> 2472.88s]  let's look at your caches
[2472.88s -> 2474.88s]  and actually you
[2474.88s -> 2476.88s]  remember you're doing all of this
[2476.88s -> 2478.88s]  without ever seeing the other caches
[2478.88s -> 2480.88s]  data structures
[2480.88s -> 2482.88s]  well how do you know
[2482.88s -> 2484.88s]  that another
[2484.88s -> 2486.88s]  cache may have it
[2486.88s -> 2488.88s]  with only your own data structures
[2488.88s -> 2490.88s]  and I'm blocking everything
[2490.88s -> 2492.88s]  look here
[2492.88s -> 2494.88s]  it's in shared
[2494.88s -> 2496.88s]  so you have no guarantees on
[2496.88s -> 2498.88s]  perhaps somebody has it
[2498.88s -> 2500.88s]  so that's how you know
[2500.88s -> 2502.88s]  what you can and cannot do
[2502.88s -> 2504.88s]  but in this case there's a store
[2504.88s -> 2506.88s]  so you have to
[2506.88s -> 2508.88s]  hold on
[2508.88s -> 2510.88s]  notify everybody first
[2510.88s -> 2512.88s]  now keep in mind
[2512.88s -> 2514.88s]  notice what happens
[2514.88s -> 2516.88s]  if cache 1 updated that value
[2516.88s -> 2518.88s]  and changed its state
[2518.88s -> 2520.88s]  before notifying
[2520.88s -> 2522.88s]  cache 1
[2522.88s -> 2524.88s]  it may have proceeded with the write
[2524.88s -> 2526.88s]  processor 1
[2526.88s -> 2528.88s]  if it doesn't know that it needs to tear down
[2528.88s -> 2530.88s]  and invalidate might continue with reads
[2530.88s -> 2532.88s]  and that read may be
[2532.88s -> 2534.88s]  potentially out of date
[2534.88s -> 2536.88s]  so before you update that variable
[2536.88s -> 2538.88s]  you have to get exclusive access
[2538.88s -> 2540.88s]  to get exclusive access you have to say
[2540.88s -> 2542.88s]  I intend to write
[2542.88s -> 2544.88s]  everybody else playing, all the other caches playing the game
[2544.88s -> 2546.88s]  go ooh okay I got to get rid of this thing
[2546.88s -> 2548.88s]  go ahead and now you can perform your write
[2548.88s -> 2550.88s]  good
[2550.88s -> 2552.88s]  and then
[2552.88s -> 2554.88s]  would you write that to memory now?
[2554.88s -> 2556.88s]  or I guess
[2556.88s -> 2558.88s]  well let's talk about it
[2558.88s -> 2560.88s]  so there's a difference between what the
[2560.88s -> 2562.88s]  MSI protocol is
[2562.88s -> 2564.88s]  and what would be correct to do
[2564.88s -> 2566.88s]  but potentially inefficient
[2566.88s -> 2568.88s]  so we didn't write to memory after the write last time
[2568.88s -> 2570.88s]  and we didn't do that
[2570.88s -> 2572.88s]  because why are you guaranteed
[2572.88s -> 2574.88s]  to get the updated value
[2574.88s -> 2576.88s]  whenever you read in the future
[2576.88s -> 2578.88s]  let's think about it
[2579.88s -> 2581.88s]  and in fact actually the next line is you reading
[2581.88s -> 2583.88s]  so let's just go ahead
[2583.88s -> 2585.88s]  the next line is processor 1
[2585.88s -> 2587.88s]  loads x
[2587.88s -> 2589.88s]  so you're reading
[2589.88s -> 2591.88s]  memory is out of date with 3
[2591.88s -> 2593.88s]  you have the invalid cache line
[2593.88s -> 2595.88s]  correct, so you state your intent to read
[2595.88s -> 2597.88s]  you go oop
[2601.88s -> 2603.88s]  and the memory, yeah, you're going to flush to me
[2603.88s -> 2605.88s]  the other thing we're not writing down is whether or not it's dirty
[2605.88s -> 2607.88s]  I guess if it's in the M state
[2607.88s -> 2609.88s]  it's dirty, so is it 4?
[2609.88s -> 2611.88s]  4, now you can get 4
[2618.88s -> 2620.88s]  and we're good
[2620.88s -> 2622.88s]  now there's one more detail here
[2622.88s -> 2624.88s]  this is the first time that it's a load of y
[2624.88s -> 2626.88s]  and not x
[2626.88s -> 2628.88s]  so both of you have
[2628.88s -> 2630.88s]  x in the S state
[2630.88s -> 2632.88s]  and now there's a load of y
[2638.88s -> 2640.88s]  but you have to say you have to load y
[2640.88s -> 2642.88s]  and you shrug it off
[2642.88s -> 2645.88s]  and you shrug it off because you don't have y
[2645.88s -> 2647.88s]  so just keep in mind that coherence
[2647.88s -> 2649.88s]  is actually about that diagram
[2649.88s -> 2651.88s]  that MSI diagram
[2651.88s -> 2654.88s]  is about keeping coherence between the same address
[2654.88s -> 2656.88s]  there's instances
[2656.88s -> 2658.88s]  there's another copy of this diagram
[2658.88s -> 2660.88s]  another state machine rolling around
[2660.88s -> 2662.88s]  for any other address
[2662.88s -> 2664.88s]  because there might be multiple values in cache
[2664.88s -> 2666.88s]  in this case you're keeping y and s
[2666.88s -> 2668.88s]  and x and s
[2668.88s -> 2670.88s]  and then finally when
[2670.88s -> 2673.88s]  processor 1 goes to write y
[2675.88s -> 2677.88s]  modify y, you shrug it off
[2677.88s -> 2679.88s]  because you don't care
[2679.88s -> 2681.88s]  and then if you ever write y
[2681.88s -> 2683.88s]  now you're going to have to request it
[2683.88s -> 2686.88s]  there'll be a tear down, there'll be a flush of 1
[2686.88s -> 2688.88s]  and then so on and so on
[2688.88s -> 2690.88s]  exactly
[2690.88s -> 2693.88s]  so it's very important to think about
[2693.88s -> 2697.88s]  any change in state to my local cache
[2697.88s -> 2701.88s]  will result in shouting to all the other caches
[2701.88s -> 2704.88s]  because the data structures of the other caches
[2704.88s -> 2706.88s]  have state in them
[2706.88s -> 2709.88s]  that allows those caches to make decisions
[2709.88s -> 2711.88s]  about what they can and cannot do
[2711.88s -> 2714.88s]  and if my state changes I gotta tell everybody else
[2715.88s -> 2718.88s]  how we tell everybody else is not something we talk about in this class
[2718.88s -> 2721.88s]  we talk about it as shouting as if it's a broadcast
[2721.88s -> 2723.88s]  but typically under the hood it'll be point to point messages
[2723.88s -> 2725.88s]  and stuff like that
[2725.88s -> 2727.88s]  and that's a different detail for another class
[2727.88s -> 2729.88s]  alright, thank you very much for your demonstration
[2729.88s -> 2731.88s]  but any other questions on cache coherence?
[2731.88s -> 2733.88s]  yes
[2733.88s -> 2736.88s]  one cache has the data in a modified state
[2736.88s -> 2739.88s]  ok, one cache has the data in a modified state, ok
[2739.88s -> 2741.88s]  another cache wants to do the right to it
[2741.88s -> 2743.88s]  do we have to send the data back to me?
[2743.88s -> 2747.88s]  because the cache has the data in the modified state
[2748.88s -> 2752.88s]  so let's say that cache 0 has the data in the modified state
[2752.88s -> 2754.88s]  cache 1 wants to write to it
[2754.88s -> 2756.88s]  and what's the question?
[2756.88s -> 2759.88s]  why does the data need to come from the memory?
[2759.88s -> 2763.88s]  why does the first processor need to flush the data back to memory
[2763.88s -> 2766.88s]  and then processor 1 gets it from memory
[2766.88s -> 2768.88s]  rather than processor 0
[2768.88s -> 2772.88s]  is your question why does processor 0 not give the data to processor 1?
[2772.88s -> 2774.88s]  well that's what I was saying earlier
[2774.88s -> 2777.88s]  in a more complicated cache coherence protocol
[2777.88s -> 2780.88s]  five state protocol, it's called like MESIF or MOSI
[2780.88s -> 2783.88s]  there's a fifth state which says
[2783.88s -> 2786.88s]  if you're trying to get data from memory
[2786.88s -> 2790.88s]  and another cache has an up-to-date copy of the data
[2790.88s -> 2794.88s]  that cache will provide the data
[2794.88s -> 2798.88s]  but I do want to be clear about one thing
[2798.88s -> 2801.88s]  remember we're talking about cache lines here
[2801.88s -> 2804.88s]  even though we kind of assumed in that exercise
[2804.88s -> 2806.88s]  that the cache line size was one value
[2806.88s -> 2808.88s]  so when we say load x
[2808.88s -> 2813.88s]  we really mean the cache line containing x
[2813.88s -> 2816.88s]  so let's say x is here
[2816.88s -> 2819.88s]  and I write 4 to x
[2819.88s -> 2822.88s]  keep in mind that there's data here
[2822.88s -> 2827.88s]  in the rest of the cache line that is untouched
[2827.88s -> 2830.88s]  so the reason why I write it back out
[2830.88s -> 2833.88s]  the cache line all the way back out to memory
[2833.88s -> 2837.88s]  is that I need to update that whole cache line
[2837.88s -> 2839.88s]  like I can't just send 4
[2839.88s -> 2842.88s]  because I don't know what's dirty in this cache line at all
[2842.88s -> 2846.88s]  so just make sure even though that demo was in terms of single values
[2846.88s -> 2851.88s]  the information that's being flushed or communicated in a more advanced protocol
[2851.88s -> 2854.88s]  are entire cache lines
[2854.88s -> 2856.88s]  you would have to have a more advanced cache
[2856.88s -> 2861.88s]  that was going to track dirtiness at the byte level
[2878.88s -> 2881.88s]  what that does is avoids the flush
[2881.88s -> 2883.88s]  it avoids the flush
[2883.88s -> 2885.88s]  so if I'm a cache and I have the line
[2885.88s -> 2888.88s]  in the shared state that you need
[2888.88s -> 2889.88s]  if you read it
[2889.88s -> 2892.88s]  I'll just give you the line and now we both have it shared
[2892.88s -> 2895.88s]  if I have a line in the modified state
[2895.88s -> 2898.88s]  and you need it to read
[2898.88s -> 2900.88s]  I will give it to you
[2900.88s -> 2904.88s]  and flush it to memory and we'll both be shared
[2904.88s -> 2907.88s]  if I have it in the modified state
[2907.88s -> 2909.88s]  and you want it in the modified state
[2909.88s -> 2911.88s]  I can actually just give it to you without flushing
[2911.88s -> 2913.88s]  and you have it in the modified state
[2913.88s -> 2915.88s]  and now you're responsible for flushing it
[2915.88s -> 2921.88s]  so that's more advanced details that we don't
[2921.88s -> 2926.88s]  ok, so what about this messy, this 4 stage protocol
[2926.88s -> 2928.88s]  this might be a good check your understanding
[2928.88s -> 2929.88s]  do we have
[2929.88s -> 2932.88s]  I don't know if I have kunle slides
[2932.88s -> 2934.88s]  so hold on one second
[2934.88s -> 2937.88s]  so in MESI
[2937.88s -> 2941.88s]  you still have the same modified, shared and invalid states
[2941.88s -> 2942.88s]  as before
[2942.88s -> 2945.88s]  but there's this little exclusive state
[2947.88s -> 2949.88s]  did we talk about that?
[2949.88s -> 2950.88s]  yeah, we did, right?
[2950.88s -> 2952.88s]  and what's the advantage of this 4th state?
[2952.88s -> 2955.88s]  and let's think about a particular use case
[2955.88s -> 2957.88s]  where you read a value
[2957.88s -> 2959.88s]  and then at some point later you write to it
[2959.88s -> 2962.88s]  very common case in computer science
[2963.88s -> 2965.88s]  but I don't want to tell anyone else
[2965.88s -> 2968.88s]  correct, so let's think about MSI
[2968.88s -> 2970.88s]  in the case where a thread reads and then writes
[2970.88s -> 2974.88s]  if a thread reads, I bring it in as a shared state
[2974.88s -> 2976.88s]  all I know at this point is that
[2976.88s -> 2979.88s]  there may be other processors with the data
[2979.88s -> 2981.88s]  so if I go to write to it
[2981.88s -> 2983.88s]  I also have to speak up and say
[2983.88s -> 2985.88s]  hey, I'm going to write now
[2985.88s -> 2986.88s]  ok
[2986.88s -> 2990.88s]  MESI says when you load data
[2990.88s -> 2993.88s]  when I say hey, I want to load the data
[2993.88s -> 2996.88s]  you look at what the other person says
[2996.88s -> 2997.88s]  if the other person says
[2997.88s -> 2999.88s]  oh yeah, I've got that in the shared state
[2999.88s -> 3001.88s]  you go, ok, I should have it in the shared state too
[3001.88s -> 3003.88s]  because I know that it's possible
[3003.88s -> 3005.88s]  for other people to have the data
[3005.88s -> 3008.88s]  if nobody says I have it in the shared state
[3008.88s -> 3009.88s]  if everybody goes
[3009.88s -> 3011.88s]  I don't care, why are you telling me?
[3011.88s -> 3013.88s]  I can load it in the E state
[3013.88s -> 3015.88s]  which says that it's not dirty
[3015.88s -> 3017.88s]  I haven't written to it
[3017.88s -> 3019.88s]  but I'm guaranteed that nobody else has it
[3020.88s -> 3022.88s]  and so if I ever want to write to it
[3022.88s -> 3023.88s]  without telling anybody
[3023.88s -> 3025.88s]  I can just flip from E to M
[3025.88s -> 3027.88s]  because nobody else has it
[3027.88s -> 3029.88s]  so nobody cares about what I have to say about that line
[3035.88s -> 3037.88s]  so if I have it in the exclusive state
[3037.88s -> 3041.88s]  and, well I haven't finished the diagram, sorry
[3043.88s -> 3045.88s]  if I have it in the exclusive state
[3045.88s -> 3047.88s]  and I hear someone else's read, what do I do?
[3049.88s -> 3050.88s]  I drop to shared
[3050.88s -> 3052.88s]  and hope that should be on the diagram now
[3052.88s -> 3054.88s]  I still haven't finished the diagram, sorry
[3054.88s -> 3057.88s]  ok, so if I'm in the exclusive state
[3057.88s -> 3059.88s]  and I see a bus read
[3059.88s -> 3062.88s]  I don't do anything and I drop to S
[3065.88s -> 3066.88s]  go
[3066.88s -> 3068.88s]  the cache is running at volume
[3068.88s -> 3070.88s]  can you ever go back to the exclusive state
[3070.88s -> 3072.88s]  like the two caches talking about shared
[3072.88s -> 3075.88s]  can you ever go back to the exclusive state
[3075.88s -> 3077.88s]  now if there's another fossil
[3078.88s -> 3080.88s]  and you can see it from this diagram
[3080.88s -> 3082.88s]  that there is no transition
[3082.88s -> 3084.88s]  what it would take to do that
[3084.88s -> 3085.88s]  let's talk about it
[3085.88s -> 3087.88s]  let's design a cache coherence protocol that's better
[3087.88s -> 3090.88s]  what would it take
[3090.88s -> 3093.88s]  to be able to elevate from S to E
[3098.88s -> 3103.88s]  you'd have to tell all the other processors to go invalid
[3103.88s -> 3105.88s]  so you would have it in your shared state
[3105.88s -> 3107.88s]  and then every time you wrote to it
[3107.88s -> 3109.88s]  you'd have to tell all the other processors
[3109.88s -> 3110.88s]  hey, I'm writing again
[3110.88s -> 3112.88s]  do you have it still?
[3112.88s -> 3116.88s]  so that kind of almost defeats the value of the shared state
[3116.88s -> 3118.88s]  but what we could do is that
[3118.88s -> 3121.88s]  whenever any processor went to invalid
[3121.88s -> 3124.88s]  normally when you go to invalid
[3124.88s -> 3126.88s]  it's in reaction to other things
[3126.88s -> 3129.88s]  let's say you go to invalid because like
[3129.88s -> 3131.88s]  you got evicted from your cache
[3131.88s -> 3132.88s]  not based on anybody else
[3132.88s -> 3135.88s]  you could tell everybody else I'm going to invalid
[3135.88s -> 3138.88s]  and that could be a trigger for all the caches to report
[3138.88s -> 3140.88s]  do I have it in the shared state or not
[3140.88s -> 3143.88s]  and if only one cache had it in the shared state
[3143.88s -> 3145.88s]  then you would know you could elevate to E
[3145.88s -> 3147.88s]  just be a more advanced cache coherence protocol
[3157.88s -> 3160.88s]  when we loaded Y, X was already in a modified state
[3160.88s -> 3162.88s]  would we be required to first flush X?
[3162.88s -> 3163.88s]  this is a great question
[3163.88s -> 3165.88s]  I want to make sure everybody understands it
[3165.88s -> 3166.88s]  so the question was
[3166.88s -> 3168.88s]  we have X in some state
[3168.88s -> 3169.88s]  let's say it's the modified state
[3169.88s -> 3171.88s]  the cache line X
[3171.88s -> 3175.88s]  if the cache line Y is loaded into the cache
[3175.88s -> 3179.88s]  does that change the behavior of X in any way?
[3179.88s -> 3180.88s]  no
[3180.88s -> 3182.88s]  because they're different
[3182.88s -> 3187.88s]  coherence is about a state machine for every address in the system
[3187.88s -> 3190.88s]  and different addresses do not interact
[3190.88s -> 3192.88s]  the only way they interact
[3192.88s -> 3193.88s]  it's nothing to do with parallelism
[3193.88s -> 3196.88s]  is maybe bringing cache line Y into the cache
[3196.88s -> 3199.88s]  cause an eviction of cache line X
[3199.88s -> 3203.88s]  and then that would be something that caused X to invalidate
[3203.88s -> 3205.88s]  and then it would have to flush
[3205.88s -> 3207.88s]  but that's unrelated to the discussion of cache coherence
[3207.88s -> 3209.88s]  that's just a normal cache behavior
[3212.88s -> 3215.88s]  if we have an example where our cache is full
[3215.88s -> 3218.88s]  and then we're going to do some operation
[3218.88s -> 3221.88s]  is there like an implement write back
[3221.88s -> 3223.88s]  before we even do a processor mean
[3223.88s -> 3224.88s]  or something like that?
[3224.88s -> 3225.88s]  absolutely
[3225.88s -> 3227.88s]  like let's say
[3227.88s -> 3230.88s]  we could talk about normal cache behavior unrelated to coherence
[3230.88s -> 3232.88s]  so imagine that the cache
[3232.88s -> 3235.88s]  and by the way the cache is always full
[3235.88s -> 3237.88s]  but you read a new value
[3237.88s -> 3240.88s]  which means you're going to have to kick out an existing value
[3240.88s -> 3243.88s]  you decide which one to kick out using whatever policy
[3243.88s -> 3244.88s]  and if it is dirty
[3244.88s -> 3246.88s]  the process is
[3246.88s -> 3248.88s]  write
[3248.88s -> 3251.88s]  or if it's dirty it's already in the modified state
[3251.88s -> 3254.88s]  so the process will be flush the data
[3254.88s -> 3255.88s]  and go to invalid
[3255.88s -> 3257.88s]  and then bus read X on the new thing
[3257.88s -> 3259.88s]  or sorry, bus read on the new thing
[3262.88s -> 3264.88s]  no, because the other line
[3264.88s -> 3266.88s]  is some other address
[3266.88s -> 3269.88s]  and again this is how we take this one address
[3269.88s -> 3273.88s]  so from the perspective of this cache coherence diagram
[3273.88s -> 3276.88s]  you just would not initiate the bus read X
[3276.88s -> 3278.88s]  on address X
[3278.88s -> 3281.88s]  until all the appropriate room in the cache has been made
[3281.88s -> 3283.88s]  to put X in your cache
[3283.88s -> 3286.88s]  so there would be some other diagram
[3286.88s -> 3289.88s]  for state Y that would now be moving
[3289.88s -> 3292.88s]  from modified or shared to invalid
[3292.88s -> 3294.88s]  yes, ok
[3294.88s -> 3296.88s]  so you mentioned that
[3296.88s -> 3299.88s]  this state is making per address
[3299.88s -> 3301.88s]  per cache Y
[3302.88s -> 3304.88s]  ok, should we move on to a different topic?
[3304.88s -> 3306.88s]  or still, let's move on
[3306.88s -> 3308.88s]  unless you had one last follow up
[3308.88s -> 3310.88s]  oh, ok
[3310.88s -> 3312.88s]  so maybe shout out some different topics
[3312.88s -> 3314.88s]  because I think if we go an extended session
[3314.88s -> 3316.88s]  we can either get two more in or one more in
[3316.88s -> 3318.88s]  so let's aggregate
[3318.88s -> 3320.88s]  what are some topics?
[3320.88s -> 3322.88s]  segment scan
[3322.88s -> 3323.88s]  ok
[3323.88s -> 3325.88s]  data parallel thinking, anything else?
[3325.88s -> 3327.88s]  CUDA in general
[3327.88s -> 3329.88s]  you might need to be more specific than that
[3330.88s -> 3332.88s]  like how the warps
[3332.88s -> 3334.88s]  how are warps organized
[3334.88s -> 3336.88s]  how are warps organized onto a machine
[3336.88s -> 3338.88s]  ok, and then
[3338.88s -> 3340.88s]  memory consistency and how to solve the problem
[3340.88s -> 3342.88s]  no secret sauce
[3342.88s -> 3344.88s]  just think about all
[3344.88s -> 3346.88s]  ok, let me answer that one first
[3346.88s -> 3348.88s]  because I don't think doing a problem will be the best
[3348.88s -> 3351.88s]  I think just doing it on your own will be best
[3351.88s -> 3354.88s]  is, one of the things in this class
[3354.88s -> 3357.88s]  is very much a skill that we want you to have
[3357.88s -> 3359.88s]  is think through what operation
[3359.88s -> 3361.88s]  is running on a machine at what time
[3361.88s -> 3363.88s]  like, what's the schedule
[3363.88s -> 3365.88s]  so you're like, oh, this thread
[3365.88s -> 3367.88s]  is running this instruction on this core
[3367.88s -> 3369.88s]  when this thread is running this instruction on this core
[3369.88s -> 3371.88s]  ok
[3371.88s -> 3373.88s]  so there was a practice problem, if I remember correctly
[3373.88s -> 3375.88s]  this week, which was instruction interleaves
[3375.88s -> 3377.88s]  right, so we said
[3377.88s -> 3379.88s]  let's say I just have a thread
[3383.88s -> 3385.88s]  T0
[3385.88s -> 3387.88s]  that does instruction 1
[3387.88s -> 3389.88s]  2
[3389.88s -> 3391.88s]  and 3
[3391.88s -> 3393.88s]  and I have another thread, T1
[3393.88s -> 3395.88s]  that does 4
[3395.88s -> 3397.88s]  5
[3397.88s -> 3399.88s]  and 6
[3399.88s -> 3401.88s]  ok, so the only thing that's required
[3401.88s -> 3403.88s]  is that T1 better do 1 before
[3403.88s -> 3405.88s]  better observe 1
[3405.88s -> 3407.88s]  then 2 then 3
[3407.88s -> 3409.88s]  and T1, maybe on a different processor
[3409.88s -> 3411.88s]  or a different thread on the same core
[3411.88s -> 3413.88s]  has to do 4 then 5 then 6
[3413.88s -> 3415.88s]  there's nothing preventing you know
[3415.88s -> 3417.88s]  1 coming before 4, 4 coming
[3417.88s -> 3419.88s]  after 2, 3 happening
[3419.88s -> 3421.88s]  in here and so on and so on
[3421.88s -> 3423.88s]  we want you to be able to think through
[3423.88s -> 3425.88s]  all the possibilities of what could
[3425.88s -> 3427.88s]  happen and what the
[3427.88s -> 3429.88s]  observed results would be
[3429.88s -> 3431.88s]  ok, so what I just did
[3431.88s -> 3433.88s]  there kind of assumes sequential consistency
[3433.88s -> 3435.88s]  right, like there's like some total
[3435.88s -> 3437.88s]  order that exists
[3437.88s -> 3439.88s]  when we go to relax consistency
[3439.88s -> 3441.88s]  what can happen is
[3441.88s -> 3443.88s]  there is not a possible
[3443.88s -> 3445.88s]  way to put all 6 operations
[3445.88s -> 3447.88s]  just on a timeline
[3447.88s -> 3449.88s]  when you say the effects of the program
[3449.88s -> 3451.88s]  are consistent with this being the order
[3451.88s -> 3453.88s]  that's what
[3453.88s -> 3455.88s]  relax consistency is
[3455.88s -> 3457.88s]  it's like, if this is a write and this
[3457.88s -> 3459.88s]  is a write, well this processor clearly
[3459.88s -> 3461.88s]  has to do 1 before 2
[3461.88s -> 3463.88s]  because that's the program
[3463.88s -> 3465.88s]  but the values, those updates
[3465.88s -> 3467.88s]  might appear in
[3467.88s -> 3469.88s]  different orders to somebody else
[3469.88s -> 3471.88s]  so that's the whole concept
[3471.88s -> 3473.88s]  there's not a lot of magic about it
[3473.88s -> 3475.88s]  and it's different
[3475.88s -> 3477.88s]  from coherence
[3477.88s -> 3479.88s]  because here we're talking about the order
[3479.88s -> 3481.88s]  of the effects of 1
[3481.88s -> 3483.88s]  and the effects of 2
[3483.88s -> 3485.88s]  operations to different
[3485.88s -> 3487.88s]  addresses, coherence
[3487.88s -> 3489.88s]  is all about operations on the same
[3489.88s -> 3491.88s]  address, so they are orthogonal
[3491.88s -> 3493.88s]  concepts, completely
[3493.88s -> 3495.88s]  ok, so segmented scan and
[3495.88s -> 3497.88s]  a little bit of just CUDA mapping
[3497.88s -> 3499.88s]  yes
[3509.88s -> 3511.88s]  let me be very precise about this
[3511.88s -> 3513.88s]  if we have 1
[3513.88s -> 3515.88s]  thread of control
[3515.88s -> 3517.88s]  your compiler
[3517.88s -> 3519.88s]  your hardware is broken
[3519.88s -> 3521.88s]  if you get results
[3521.88s -> 3523.88s]  that are in the opposite
[3523.88s -> 3525.88s]  order, that are not consistent
[3525.88s -> 3527.88s]  with program order
[3527.88s -> 3529.88s]  so nothing about relaxed consistency
[3529.88s -> 3531.88s]  can be visible if you
[3531.88s -> 3533.88s]  only have 1 thread
[3533.88s -> 3535.88s]  in the same way that superscalar execution
[3535.88s -> 3537.88s]  is broken
[3537.88s -> 3539.88s]  if a processor decides to execute
[3539.88s -> 3541.88s]  instructions out of order or in parallel
[3541.88s -> 3543.88s]  and you get different results
[3543.88s -> 3545.88s]  so you're saying comments are in order either way in a single thread
[3545.88s -> 3547.88s]  in a single thread
[3547.88s -> 3549.88s]  relaxed consistency is saying
[3549.88s -> 3551.88s]  if T0 believes
[3551.88s -> 3553.88s]  it sees write to X
[3553.88s -> 3555.88s]  and then write to Y
[3555.88s -> 3557.88s]  if you read
[3557.88s -> 3559.88s]  if you read
[3559.88s -> 3561.88s]  a value in here you would see
[3561.88s -> 3563.88s]  the X done and the Y not done
[3563.88s -> 3565.88s]  if you read a value
[3565.88s -> 3567.88s]  here you would see both of them done
[3567.88s -> 3569.88s]  what relaxed consistency
[3569.88s -> 3571.88s]  says is that if T1
[3571.88s -> 3573.88s]  ever reads X and Y
[3573.88s -> 3575.88s]  it can observe
[3575.88s -> 3577.88s]  if you are relaxing write-write
[3577.88s -> 3579.88s]  ordering for example
[3579.88s -> 3581.88s]  T1 might see the value of Y
[3581.88s -> 3583.88s]  being updated
[3583.88s -> 3585.88s]  before it sees the value of X
[3585.88s -> 3587.88s]  being updated
[3587.88s -> 3589.88s]  and I want to be really clear
[3589.88s -> 3591.88s]  that this is only something
[3591.88s -> 3593.88s]  that when we're talking about the observation
[3593.88s -> 3595.88s]  of another processor's writes
[3595.88s -> 3597.88s]  because if we're talking about
[3597.88s -> 3599.88s]  my own writes
[3599.88s -> 3601.88s]  and I compile a freaking program
[3601.88s -> 3603.88s]  and it gives me an answer that's not consistent
[3603.88s -> 3605.88s]  with C semantics I throw out the computer
[3605.88s -> 3607.88s]  and the compiler and say this is all broken
[3607.88s -> 3609.88s]  so I want to be very clear
[3609.88s -> 3611.88s]  about that, and keep in mind
[3611.88s -> 3613.88s]  that within a thread
[3613.88s -> 3615.88s]  on the first day of class I told you that
[3615.88s -> 3617.88s]  instructions were reordered
[3617.88s -> 3619.88s]  superscaler does that, bunch of reasons
[3619.88s -> 3621.88s]  done that, but you never ever
[3621.88s -> 3623.88s]  observe that they are
[3623.88s -> 3625.88s]  ordered, so it's still
[3625.88s -> 3627.88s]  completely in program order
[3633.88s -> 3635.88s]  I would like to see
[3635.88s -> 3637.88s]  if we can go over
[3637.88s -> 3639.88s]  all the possible
[3639.88s -> 3641.88s]  parallel
[3641.88s -> 3643.88s]  quick headings on common
[3643.88s -> 3645.88s]  you just increased scope
[3645.88s -> 3647.88s]  in the last 12 minutes here
[3647.88s -> 3649.88s]  considerably
[3649.88s -> 3651.88s]  wait, so
[3651.88s -> 3653.88s]  what, what are you asking?
[3659.88s -> 3661.88s]  I'll put it this way, it's extremely hard
[3661.88s -> 3663.88s]  to even
[3663.88s -> 3665.88s]  create questions
[3665.88s -> 3667.88s]  that do much more than write-write
[3667.88s -> 3669.88s]  write-write reordering
[3669.88s -> 3671.88s]  so, so, they're done
[3671.88s -> 3673.88s]  extremely hard
[3673.88s -> 3675.88s]  so I think
[3675.88s -> 3677.88s]  write-write reordering is a big one
[3677.88s -> 3679.88s]  so, but basically
[3679.88s -> 3681.88s]  not even write-write reordering, just think about
[3681.88s -> 3683.88s]  if you have a write
[3683.88s -> 3685.88s]  think about the implications of moving
[3685.88s -> 3687.88s]  reads or other writes above
[3687.88s -> 3689.88s]  or below it, I think that's a good
[3689.88s -> 3691.88s]  a good guidance
[3691.88s -> 3693.88s]  like if we tell you
[3693.88s -> 3695.88s]  or it can be easy to just say
[3695.88s -> 3697.88s]  everything is relaxed, like there's no guarantees
[3697.88s -> 3699.88s]  on anything other than program order
[3699.88s -> 3701.88s]  but the easiest
[3701.88s -> 3703.88s]  questions to write are definitely write-write
[3703.88s -> 3705.88s]  reordering
[3711.88s -> 3713.88s]  oh, I see, this is the data parallel
[3713.88s -> 3715.88s]  lecture, so there was a quick comment on
[3715.88s -> 3717.88s]  segmented scan
[3719.88s -> 3721.88s]  all I want you to know on segmented scan
[3721.88s -> 3723.88s]  if you have it on the slide, is the definition
[3723.88s -> 3725.88s]  so
[3725.88s -> 3727.88s]  segmented scan
[3727.88s -> 3729.88s]  is just a scan, imagine that we
[3729.88s -> 3731.88s]  sequentially, we had
[3731.88s -> 3733.88s]  a list of lists, or
[3733.88s -> 3735.88s]  if I wanted to be more precise, a sequence of sequences
[3735.88s -> 3737.88s]  so here's a sequence of two sequences
[3737.88s -> 3739.88s]  the first sequence is
[3739.88s -> 3741.88s]  three elements, and the second sequence
[3741.88s -> 3743.88s]  is five elements
[3743.88s -> 3745.88s]  right?
[3745.88s -> 3747.88s]  imagine we did a scan
[3747.88s -> 3749.88s]  on the first three
[3749.88s -> 3751.88s]  the first sequence, what would we get?
[3753.88s -> 3755.88s]  let's say it's a sum
[3755.88s -> 3757.88s]  an exclusive sum
[3757.88s -> 3759.88s]  so if it's an exclusive sum, we would get
[3759.88s -> 3761.88s]  zero, one, one plus two
[3761.88s -> 3763.88s]  three, three plus three, six
[3763.88s -> 3765.88s]  and then
[3765.88s -> 3767.88s]  so we know what a scan is
[3767.88s -> 3769.88s]  now imagine that
[3769.88s -> 3771.88s]  now I just do a scan on the next thing
[3771.88s -> 3773.88s]  and I'd get again
[3773.88s -> 3775.88s]  zero, four, nine
[3775.88s -> 3777.88s]  fifteen, twenty-two
[3777.88s -> 3779.88s]  thirty
[3779.88s -> 3781.88s]  so just two scans
[3781.88s -> 3783.88s]  so if you know scan, you know
[3783.88s -> 3785.88s]  what the right answer should be for segmented scan
[3787.88s -> 3789.88s]  now segmented scan
[3789.88s -> 3791.88s]  algorithm that I gave you on the page
[3791.88s -> 3793.88s]  is a way to compute
[3793.88s -> 3795.88s]  those two scans
[3795.88s -> 3797.88s]  with O of N parallelism
[3797.88s -> 3799.88s]  where N is the total length of all
[3799.88s -> 3801.88s]  the sub-sequences
[3801.88s -> 3803.88s]  and that algorithm looks a little bit like this
[3803.88s -> 3805.88s]  I visually described it like this
[3805.88s -> 3807.88s]  I put this in the lecture because I thought
[3807.88s -> 3809.88s]  you might find it cool
[3809.88s -> 3811.88s]  what I want you to be able to do, however
[3811.88s -> 3813.88s]  is yes, if I gave you the definition
[3813.88s -> 3815.88s]  if I gave you the bottom half of the slide
[3815.88s -> 3817.88s]  and I asked you to use
[3817.88s -> 3819.88s]  segmented scan in some higher level
[3819.88s -> 3821.88s]  algorithm development, that is fair game
[3821.88s -> 3823.88s]  but I won't be like
[3823.88s -> 3825.88s]  hey, you need to implement data parallel scan
[3825.88s -> 3827.88s]  or if I ask you to implement it
[3827.88s -> 3829.88s]  it's going to be
[3829.88s -> 3831.88s]  some very guided thing
[3831.88s -> 3833.88s]  where the real principle is something else
[3833.88s -> 3835.88s]  and not the segmented scan
[3835.88s -> 3837.88s]  ok, so that was that, and then CUDA
[3837.88s -> 3839.88s]  so let's try and, let's see what we can do with CUDA
[3839.88s -> 3841.88s]  ok
[3841.88s -> 3843.88s]  so I hope you all remember
[3843.88s -> 3845.88s]  from assignment 3
[3845.88s -> 3847.88s]  that when you, and really from your assignment 2
[3847.88s -> 3849.88s]  so in assignment 2
[3849.88s -> 3851.88s]  we had these bulk tasks
[3851.88s -> 3853.88s]  right, so
[3853.88s -> 3855.88s]  in some sense you had a task system
[3855.88s -> 3857.88s]  and the application sent you a command
[3857.88s -> 3859.88s]  and the command
[3859.88s -> 3861.88s]  was run this task
[3861.88s -> 3863.88s]  n times
[3863.88s -> 3865.88s]  right, and then it can send you
[3865.88s -> 3867.88s]  multiple commands which said
[3867.88s -> 3869.88s]  ok, here's a new task that you need to run n times
[3869.88s -> 3871.88s]  but you can't do any of these until
[3871.88s -> 3873.88s]  you're done with all these previous bulk launches
[3873.88s -> 3875.88s]  so CUDA is a bulk launch system
[3875.88s -> 3877.88s]  much like your
[3877.88s -> 3879.88s]  your tasking API
[3879.88s -> 3881.88s]  it just kind of has, instead of the number n tasks
[3881.88s -> 3883.88s]  it just kind of has two numbers in there
[3883.88s -> 3885.88s]  it says, I want you to run
[3885.88s -> 3887.88s]  n thread blocks
[3887.88s -> 3889.88s]  and each thread block has
[3889.88s -> 3891.88s]  n threads in it
[3891.88s -> 3893.88s]  so it's the same bulk launch concept
[3893.88s -> 3895.88s]  where you say, here's the number of thread blocks
[3895.88s -> 3897.88s]  and I think thread block
[3897.88s -> 3899.88s]  and like, kind of your task
[3899.88s -> 3901.88s]  or an ispc task have a bit of correspondence
[3901.88s -> 3903.88s]  and inside that task
[3903.88s -> 3905.88s]  you're going to
[3905.88s -> 3907.88s]  do parallel work using
[3907.88s -> 3909.88s]  t threads
[3909.88s -> 3911.88s]  so the fact that you can do this in 2D versus
[3911.88s -> 3913.88s]  3D versus 1D is not significant
[3913.88s -> 3915.88s]  right, it's just create this many
[3915.88s -> 3917.88s]  thread blocks
[3917.88s -> 3919.88s]  and each thread block needs this many resources
[3919.88s -> 3921.88s]  it needs this much shared memory
[3921.88s -> 3923.88s]  and it needs this many threads
[3923.88s -> 3925.88s]  so in
[3925.88s -> 3927.88s]  this setup
[3927.88s -> 3929.88s]  the details of the program are not important
[3929.88s -> 3931.88s]  but we wrote a CUDA program
[3931.88s -> 3933.88s]  that if I remember correctly
[3933.88s -> 3935.88s]  had 128 threads
[3935.88s -> 3937.88s]  per block
[3937.88s -> 3939.88s]  and needed
[3939.88s -> 3941.88s]  I can't remember
[3941.88s -> 3943.88s]  it needed like a little bit
[3943.88s -> 3945.88s]  over, I think it needed like
[3945.88s -> 3947.88s]  it needed a little bit over
[3947.88s -> 3949.88s]  512 bytes of storage
[3949.88s -> 3951.88s]  so in other words, if we have
[3951.88s -> 3953.88s]  on every core of the GPU
[3953.88s -> 3955.88s]  every SM of the GPU
[3955.88s -> 3957.88s]  if we have execution context
[3957.88s -> 3959.88s]  for 384
[3959.88s -> 3961.88s]  threads, how many thread blocks
[3961.88s -> 3963.88s]  can we fit?
[3969.88s -> 3971.88s]  128, you know, you're doing the math
[3971.88s -> 3973.88s]  can we fit 2 or 3, I can't remember
[3973.88s -> 3975.88s]  we can fit
[3975.88s -> 3977.88s]  we can fit 3, yeah, we can fit 3 thread blocks
[3977.88s -> 3979.88s]  worth of threads
[3979.88s -> 3981.88s]  and if those thread blocks needed a little bit
[3981.88s -> 3983.88s]  over 1 third of this shared memory
[3983.88s -> 3985.88s]  how many thread blocks can I fit
[3985.88s -> 3987.88s]  from a storage perspective
[3987.88s -> 3989.88s]  just 2, right, so in this case
[3989.88s -> 3991.88s]  the number of thread blocks that we can get on
[3991.88s -> 3993.88s]  to this processor at one time is actually
[3993.88s -> 3995.88s]  limited by our shared memory
[3995.88s -> 3997.88s]  but for example, if it uses 0 shared memory
[3997.88s -> 3999.88s]  I can fit 3
[4000.88s -> 4002.88s]  so I say that this
[4002.88s -> 4004.88s]  there's an execution context
[4004.88s -> 4006.88s]  for 384 CUDA threads here
[4006.88s -> 4008.88s]  and so the NVIDIA scheduler
[4008.88s -> 4010.88s]  you know, their version of your assignment 2
[4010.88s -> 4012.88s]  is going to say, alright
[4012.88s -> 4014.88s]  here's some work
[4014.88s -> 4016.88s]  I want you to run 1000 thread blocks
[4016.88s -> 4018.88s]  worth of this program
[4018.88s -> 4020.88s]  and at this point, it just starts handing it
[4020.88s -> 4022.88s]  to its worker cores
[4022.88s -> 4024.88s]  and so, or really
[4024.88s -> 4026.88s]  its worker execution context
[4026.88s -> 4028.88s]  on its cores
[4028.88s -> 4030.88s]  one is to say, well look, there's nothing allocated
[4030.88s -> 4032.88s]  right now, let's get the first
[4032.88s -> 4034.88s]  thread block running on core 0
[4034.88s -> 4036.88s]  and we're going to give it
[4036.88s -> 4038.88s]  128 threads worth of
[4038.88s -> 4040.88s]  execution contexts
[4040.88s -> 4042.88s]  and we're going to give it a little bit over a third
[4042.88s -> 4044.88s]  520 bytes of storage
[4044.88s -> 4046.88s]  and there's still a lot more
[4046.88s -> 4048.88s]  worker capability in this machine
[4048.88s -> 4050.88s]  so we're going to sign the next thread block over here
[4050.88s -> 4052.88s]  on core 1
[4052.88s -> 4054.88s]  now question, a good exam
[4054.88s -> 4056.88s]  question would be
[4056.88s -> 4058.88s]  why did this scheduler, which was an implementation
[4058.88s -> 4060.88s]  detail, but why is it a fairly sane
[4060.88s -> 4062.88s]  decision
[4062.88s -> 4064.88s]  to put block 0
[4064.88s -> 4066.88s]  on core 0 and block 1 on core 1
[4072.88s -> 4074.88s]  if I were to put block 0 and block 1
[4074.88s -> 4076.88s]  both here, what is
[4076.88s -> 4078.88s]  the performance implication of that
[4078.88s -> 4080.88s]  so remember, these are 128 threads
[4080.88s -> 4082.88s]  these are 128 threads, we have
[4082.88s -> 4084.88s]  one core, and that core
[4084.88s -> 4086.88s]  in this case had a 32 wide
[4086.88s -> 4088.88s]  SIMD capability
[4088.88s -> 4090.88s]  what's the problem with loading this
[4090.88s -> 4092.88s]  up with a bunch of threads and leaving the other
[4092.88s -> 4094.88s]  core idle
[4094.88s -> 4096.88s]  yeah?
[4096.88s -> 4098.88s]  because remember
[4098.88s -> 4100.88s]  these threads are
[4100.88s -> 4102.88s]  interleaved onto these processing resources
[4102.88s -> 4104.88s]  so I'm having everybody
[4104.88s -> 4106.88s]  that I'm creating telling to do work share the
[4106.88s -> 4108.88s]  same resources while I have resources over there
[4108.88s -> 4110.88s]  that are idle, exactly
[4110.88s -> 4112.88s]  exactly, ok
[4112.88s -> 4114.88s]  I have more room, I can fill up this chip
[4114.88s -> 4116.88s]  and this chip can simultaneously support
[4116.88s -> 4118.88s]  4 concurrent thread blocks
[4120.88s -> 4122.88s]  ok, and I cannot put further
[4122.88s -> 4124.88s]  thread blocks on here because I'm out of resources
[4124.88s -> 4126.88s]  so
[4126.88s -> 4128.88s]  when a thread block completes
[4128.88s -> 4130.88s]  just like in your assignment
[4130.88s -> 4132.88s]  when a task completes
[4132.88s -> 4134.88s]  that opens up the ability to put another
[4134.88s -> 4136.88s]  task on the machine
[4136.88s -> 4138.88s]  so let's say we pull the first block
[4138.88s -> 4140.88s]  completed first, which makes sense
[4140.88s -> 4142.88s]  and the scheduler can fill that gap
[4142.88s -> 4144.88s]  notice that
[4144.88s -> 4146.88s]  the fact that all thread blocks have the same
[4146.88s -> 4148.88s]  number of resources and the same allocations
[4148.88s -> 4150.88s]  is useful for filling this gap
[4150.88s -> 4152.88s]  otherwise you have an allocation problem
[4152.88s -> 4154.88s]  and we just continue with that
[4154.88s -> 4156.88s]  just putting stuff on, putting stuff on
[4156.88s -> 4158.88s]  until we're done
[4158.88s -> 4160.88s]  so that's the thread block scheduling
[4160.88s -> 4162.88s]  part of CUDA, is there any question
[4162.88s -> 4164.88s]  about warp scheduling part of CUDA?
[4164.88s -> 4166.88s]  or are you
[4166.88s -> 4168.88s]  good there?
[4168.88s -> 4170.88s]  my question was
[4170.88s -> 4172.88s]  why isn't a single warp
[4172.88s -> 4174.88s]  at a single clock
[4174.88s -> 4176.88s]  because how many warps
[4176.88s -> 4178.88s]  can we actually have
[4178.88s -> 4180.88s]  ok, so
[4180.88s -> 4182.88s]  I want to preface this
[4182.88s -> 4184.88s]  because this is recorded and I don't want to be
[4188.88s -> 4190.88s]  doxxed because I'm saying the wrong
[4190.88s -> 4192.88s]  thing about modern NVIDIA GPUs
[4192.88s -> 4194.88s]  but for most NVIDIA GPUs
[4194.88s -> 4196.88s]  it's perfectly fine
[4196.88s -> 4198.88s]  the only way to think about it in the following way
[4198.88s -> 4200.88s]  is that the chip
[4200.88s -> 4202.88s]  has a bank of ALUs, now this is not an NVIDIA GPU
[4202.88s -> 4204.88s]  I made this up
[4204.88s -> 4206.88s]  my proxy NVIDIA GPU here
[4206.88s -> 4208.88s]  has 32 SIMD ALUs
[4208.88s -> 4210.88s]  and the way it runs
[4210.88s -> 4212.88s]  CUDA threads
[4212.88s -> 4214.88s]  is that it takes 32 CUDA threads
[4214.88s -> 4216.88s]  which is called one warp
[4216.88s -> 4218.88s]  and it runs them
[4218.88s -> 4220.88s]  one instruction
[4220.88s -> 4222.88s]  for each of those 32 CUDA threads
[4222.88s -> 4224.88s]  on this ALU and makes progress
[4224.88s -> 4226.88s]  on the warp
[4226.88s -> 4228.88s]  and then the next clock it might choose a different warp
[4228.88s -> 4230.88s]  so you can think about
[4230.88s -> 4232.88s]  a warp's worth of execution
[4232.88s -> 4234.88s]  as 32 CUDA threads
[4234.88s -> 4236.88s]  all doing the same thing at the same time
[4236.88s -> 4238.88s]  and concurrently running on the chip
[4246.88s -> 4248.88s]  I'm going to answer the question
[4248.88s -> 4250.88s]  does the NVIDIA GPU always
[4250.88s -> 4252.88s]  choose threads from the warp
[4252.88s -> 4254.88s]  in the same thread block
[4254.88s -> 4256.88s]  officially NVIDIA says nothing
[4256.88s -> 4258.88s]  about their implementation
[4258.88s -> 4260.88s]  so I cannot definitively tell you
[4260.88s -> 4262.88s]  do they do that or not
[4262.88s -> 4264.88s]  historically the answer about their implementation
[4264.88s -> 4266.88s]  has been absolutely yes
[4266.88s -> 4268.88s]  the threads that are used
[4268.88s -> 4270.88s]  to run at the same time
[4270.88s -> 4272.88s]  are thread IDs 0, 1, 2, 3
[4272.88s -> 4274.88s]  all the way to 32
[4274.88s -> 4276.88s]  thread ID mod 32
[4276.88s -> 4278.88s]  0 through 31
[4278.88s -> 4280.88s]  now this might be a
[4280.88s -> 4282.88s]  mode way to finish up here
[4282.88s -> 4284.88s]  but at the last minute let's think about this way
[4284.88s -> 4286.88s]  NVIDIA has full flexibility
[4288.88s -> 4290.88s]  since you wrote things as threads
[4290.88s -> 4292.88s]  if they want to change the SIMD width
[4292.88s -> 4294.88s]  and cut the number of ALUs by half
[4294.88s -> 4296.88s]  you would never know
[4296.88s -> 4298.88s]  because you just created 128 threads
[4298.88s -> 4300.88s]  you didn't think about warps
[4300.88s -> 4302.88s]  you didn't think about any of that stuff
[4302.88s -> 4304.88s]  so when you compile your thread block code
[4304.88s -> 4306.88s]  it's just a regular sequential
[4306.88s -> 4308.88s]  scalar thread
[4308.88s -> 4310.88s]  and it happens to run as many as you ask for
[4310.88s -> 4312.88s]  concurrently
[4312.88s -> 4314.88s]  that's different from the implementation
[4314.88s -> 4316.88s]  detail of SIMD instructions
[4316.88s -> 4318.88s]  on a machine
[4318.88s -> 4320.88s]  if we went next year and Intel produced
[4320.88s -> 4322.88s]  a processor with like 64 wide
[4322.88s -> 4324.88s]  SIMD
[4324.88s -> 4326.88s]  you would go back to ISPC
[4326.88s -> 4328.88s]  you would change the gang size to 64
[4328.88s -> 4330.88s]  you would recompile your code
[4330.88s -> 4332.88s]  and it would produce one thread
[4332.88s -> 4334.88s]  with 64 wide vector instructions
[4334.88s -> 4336.88s]  that's the one difference
[4336.88s -> 4338.88s]  between GPU SIMD and CPU SIMD
[4338.88s -> 4340.88s]  in CPU SIMD it's the responsibility
[4340.88s -> 4342.88s]  of the compiler to generate
[4342.88s -> 4344.88s]  instructions that are exposed
[4344.88s -> 4346.88s]  by the machine architecture
[4346.88s -> 4348.88s]  in GPU assembly the compiler
[4348.88s -> 4350.88s]  doesn't do nearly as much
[4350.88s -> 4352.88s]  it just generates sequential things
[4352.88s -> 4354.88s]  and says like in your programming assignment
[4354.88s -> 4356.88s]  we're going to do thousands of them at once
[4356.88s -> 4358.88s]  in group in block sizes of
[4358.88s -> 4360.88s]  in this case 128
[4360.88s -> 4362.88s]  it's up to NVIDIA to decide how it wants to run that
[4362.88s -> 4364.88s]  historically it's taken all your threads
[4364.88s -> 4366.88s]  in a thread block, chopped them into groups
[4366.88s -> 4368.88s]  of consecutive 32 addresses
[4368.88s -> 4370.88s]  and run those in parallel
[4370.88s -> 4372.88s]  these days they actually do have
[4372.88s -> 4374.88s]  the ability to rearrange things for you
[4374.88s -> 4376.88s]  in order to try and discover
[4376.88s -> 4378.88s]  better SIMD coherence if you
[4378.88s -> 4380.88s]  have diversions and if statements
[4380.88s -> 4382.88s]  the extent to which they can successfully
[4382.88s -> 4384.88s]  do that is complete trade secret
[4384.88s -> 4386.88s]  and not something that's well documented
