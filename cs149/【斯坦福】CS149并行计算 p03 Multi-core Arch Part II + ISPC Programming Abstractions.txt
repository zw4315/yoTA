# Detected language: en (p=1.00)

[0.00s -> 7.54s]  Welcome to week two, how was the week?
[9.00s -> 10.22s]  Forty-niners remain undefeated.
[13.12s -> 14.16s]  Giant season is over.
[15.98s -> 20.54s]  All right, yeah, yeah, exactly.
[20.70s -> 22.66s]  So just as a reminder from last week,
[22.66s -> 26.56s]  on Tuesday we had this pretty substantial lecture
[26.56s -> 29.52s]  where we talked about three kind of big ideas
[29.52s -> 30.58s]  in throughput computing.
[31.22s -> 33.54s]  We talked about multi-core execution,
[33.54s -> 37.42s]  we talked about SIMD execution, and at the very end,
[37.42s -> 39.52s]  we started talking about another idea called hardware
[39.52s -> 41.20s]  multi-threading, which again happened at the end
[41.20s -> 43.02s]  of the class when people were starting to get a little tired
[43.02s -> 44.86s]  or something like that.
[45.08s -> 47.86s]  So I want to really review the hardware multi-threading,
[47.86s -> 48.68s]  that's what I'm going to start with.
[48.68s -> 51.24s]  I'm going to review big concepts from last time,
[51.62s -> 54.06s]  and then we're going to go into the back half of the lecture.
[54.06s -> 56.98s]  We'll actually talk about ISPC programming,
[56.98s -> 58.90s]  which is the language that you do a lot of your work
[58.94s -> 61.08s]  in assignment one end, not for the purposes
[61.08s -> 63.22s]  of teaching you a language, but to underscore some
[63.22s -> 65.54s]  of the concepts from last time.
[65.54s -> 66.46s]  Any questions?
[67.78s -> 70.80s]  If not, let me pull up the slides from last time,
[73.32s -> 74.98s]  where I had a figure up that looked like this.
[75.62s -> 78.34s]  Okay. So that last ten minutes
[78.34s -> 81.64s]  of the lecture last time had one very simple idea in it,
[82.36s -> 85.88s]  which was if you are waiting on something to happen,
[86.88s -> 88.06s]  you go do something else.
[88.06s -> 89.82s]  That was the whole idea.
[90.58s -> 94.86s]  The first part of the lecture was largely about adding more
[94.86s -> 96.34s]  and more units to the processor.
[96.90s -> 100.64s]  This last section was about making use
[100.64s -> 103.26s]  of those units more efficiently.
[103.88s -> 105.48s]  So I closed with a diagram
[105.48s -> 106.58s]  that looked a little bit like this.
[107.16s -> 113.42s]  I said we introduced a SIMD core, a processor
[113.42s -> 114.86s]  that can execute, in this case,
[114.86s -> 117.26s]  one instruction per clock, but every one
[117.26s -> 119.08s]  of those instructions is doing eight things at once,
[119.34s -> 120.20s]  the same eight things.
[120.20s -> 122.32s]  So it's like adding two eight-wide vectors
[122.32s -> 123.84s]  or something like that given my diagram.
[124.48s -> 126.54s]  And we talked last time about what does it mean
[126.54s -> 129.06s]  to execute instructions, and you all told me
[129.36s -> 131.94s]  that when I run an instruction, the result of that is going
[131.94s -> 134.74s]  to be reflected in the change in the value in some register,
[135.88s -> 137.46s]  or maybe a change in value in memory.
[138.16s -> 141.32s]  So running a thread, running an instruction stream,
[141.32s -> 144.14s]  says I have my state over here, which is like the state
[144.14s -> 147.50s]  of all variables, and then I have operations or instructions
[147.50s -> 149.60s]  which change the results of those variables.
[151.90s -> 153.82s]  So then we said, well, wait a minute here.
[153.82s -> 156.48s]  If I'm trying to run operations and I can't do
[156.48s -> 158.94s]  that next operation because I'm waiting on something,
[159.86s -> 161.56s]  and what was the biggest example
[161.56s -> 164.40s]  of what we might be waiting on in class, or last class?
[165.22s -> 170.32s]  Yeah? We were waiting on memory to respond to a request,
[170.32s -> 172.86s]  which could be hundreds or more of cycles.
[173.50s -> 177.32s]  Now, generalize that thought to waiting on anything, right?
[177.32s -> 180.44s]  It actually could be waiting on some previous operation
[180.44s -> 183.00s]  to complete, like doing some complex math instruction,
[183.42s -> 186.10s]  but we introduced this idea of waiting for a very,
[186.10s -> 187.44s]  very long memory request.
[188.20s -> 190.98s]  So the big idea is that instead of sit there and wait,
[191.80s -> 193.14s]  go work on something else.
[194.10s -> 197.02s]  So if this blue box is encapsulating the state
[197.02s -> 199.00s]  of all my variables for a thread,
[199.84s -> 202.30s]  what I did is I just added more blue boxes
[202.30s -> 204.96s]  to the processor, I said, well, there's no reason why if I,
[204.96s -> 208.00s]  I'm not going to change any execution capability,
[208.06s -> 211.74s]  this processor can still only do one SIMD instruction
[211.74s -> 215.08s]  per clock, but I'm going to go ahead and allow it
[215.08s -> 217.62s]  to manage state for four threads.
[218.98s -> 220.92s]  So now when one thread can't make progress,
[220.92s -> 223.40s]  when I can't do that next instruction, boom,
[223.40s -> 226.20s]  the hardware just switches to the next instruction
[226.20s -> 228.74s]  for the next thread, and whenever there's a stall
[228.74s -> 232.04s]  like this, it doesn't waste any time, it just moves over
[232.04s -> 234.96s]  and it does something else, switch on stall,
[234.96s -> 237.40s]  switch on stall, switch on stall, switch on stall,
[237.66s -> 239.64s]  and at some point, maybe it gets back
[239.64s -> 240.90s]  when this data is ready,
[240.90s -> 242.40s]  it comes back to running this thread.
[243.74s -> 246.24s]  So in my diagram, time is going down here,
[246.24s -> 249.76s]  down the vertical axis, the threads 1, 2, 3, 4,
[249.76s -> 253.10s]  on the diagram, and I have these eight little columns inside
[253.10s -> 254.36s]  each thread to indicate
[254.36s -> 257.68s]  that they're issuing SIMD vector instructions, okay?
[258.74s -> 260.80s]  So how many instructions does this core run per clock?
[262.68s -> 266.98s]  One. That instruction is an 8-wide vector instruction
[266.98s -> 267.76s]  in this diagram.
[269.26s -> 271.72s]  How many threads does it run at any one time?
[272.92s -> 276.56s]  One. What is the utilization of the processor?
[277.90s -> 281.18s]  100%. If I go to any point in time and I say,
[281.18s -> 284.74s]  is something running, I will conclude that the answer is yes.
[286.56s -> 289.66s]  Now what's the cost of this idea, which by the way,
[289.66s -> 293.16s]  was largely invented by Kuomint.
[294.38s -> 298.82s]  When was the first, I mean I guess there was all the U-Dub
[298.82s -> 300.62s]  stuff with Intel and then there was,
[301.96s -> 304.24s]  but they weren't really doing this, right?
[304.24s -> 307.28s]  So this was Niagara, and what was your start
[307.28s -> 307.88s]  up before Niagara?
[307.88s -> 317.76s]  Okay, so now keep in mind that there's a cost.
[318.32s -> 321.30s]  One cost is that I have to burn some more chip space
[321.30s -> 322.98s]  to hold all these execution contexts.
[323.72s -> 329.18s]  Another cost is that every one of my threads gets done
[329.18s -> 331.26s]  with what it was supposed to do a little bit slower
[331.94s -> 334.26s]  because it's sharing this processor with the other threads.
[335.30s -> 337.64s]  So I want you to keep that in mind.
[338.10s -> 340.28s]  We are at 100% utilization.
[340.70s -> 345.70s]  We cannot get all the work done any faster, but any one
[345.78s -> 347.98s]  of those pieces of work, any one of those threads,
[348.56s -> 350.78s]  will be sharing the processor with other threads.
[350.78s -> 351.88s]  So the amount of time it takes
[351.88s -> 353.48s]  to do one thing is actually going up.
[354.94s -> 357.56s]  So if you had a very latency-driven application,
[357.56s -> 362.76s]  you might want to do this, and this cost is, you know,
[362.76s -> 365.44s]  the cost of the storage, you can kind of think about it
[365.44s -> 368.92s]  as like there's a fixed amount of storage on chip
[369.04s -> 371.44s]  that I can dedicate to these execution contexts.
[372.28s -> 375.56s]  So for example, I can take that fixed amount of storage
[375.58s -> 379.18s]  and I can say we're going to have a bunch of execution contexts.
[379.18s -> 380.58s]  I can interleave a ton of threads,
[380.58s -> 383.56s]  like 16 threads in this diagram, but every one
[383.56s -> 384.82s]  of those threads is only going
[384.82s -> 387.30s]  to have a pretty small register file, small amount of state.
[387.78s -> 389.74s]  Or I can divvy it up in other ways and say, look,
[389.74s -> 392.34s]  I want threads with a lot of registers, a lot of access
[392.40s -> 396.66s]  to local state, but I want a little bit less ability
[396.66s -> 399.42s]  to go do something else and so on and so on.
[399.42s -> 400.76s]  So here's my new core.
[401.62s -> 404.02s]  This is a core that in this class I'll use the term
[404.08s -> 405.34s]  hardware multithreaded.
[405.68s -> 407.84s]  It means the hardware has the ability
[407.84s -> 410.96s]  to run two different instruction streams, or has the ability
[410.96s -> 413.60s]  to maintain state for two different instruction streams,
[414.32s -> 416.10s]  but this core, as I've drawn it,
[416.32s -> 420.12s]  can only run one instruction per clock, and in this case,
[420.12s -> 422.54s]  I've actually redrawn it to be a scalar instruction.
[423.08s -> 425.12s]  You can see from the diagram.
[425.40s -> 428.78s]  So single core processor, multithreaded core,
[429.08s -> 432.00s]  can run one scalar instruction per clock
[432.80s -> 433.72s]  from one of the threads.
[434.22s -> 436.56s]  So the hardware, every single clock tick is going, all right,
[436.56s -> 438.52s]  which thread should I run?
[438.52s -> 438.92s]  I don't know.
[438.92s -> 440.64s]  I'll pick one of them that's available to run.
[440.64s -> 442.20s]  I'm going to take the next instruction
[442.20s -> 443.34s]  from that instruction stream,
[443.34s -> 445.42s]  and I'm going to tell my execution unit to do it.
[446.18s -> 447.28s]  That's how the chip runs.
[447.56s -> 448.34s]  Okay, there was a question.
[448.72s -> 448.82s]  Yeah.
[449.04s -> 450.06s]  Two things.
[450.18s -> 451.90s]  First, in the previous scenario,
[451.90s -> 453.74s]  I didn't get a quiet working school.
[454.54s -> 457.42s]  Secondly, how is this scenario going?
[457.42s -> 460.12s]  Let's say there's 15 students at office hours going
[460.12s -> 462.18s]  to ask me questions tomorrow during my office hours,
[462.18s -> 463.44s]  which by the way, I had to move
[463.44s -> 466.68s]  because of a faculty meeting conflict to 11 a.m. tomorrow,
[467.08s -> 468.84s]  just so you know, so announcement, my office hours.
[469.72s -> 472.00s]  Imagine that I have 15 students there,
[472.64s -> 473.94s]  and I give them a bit of a hint,
[473.94s -> 475.30s]  and then I tell them to go think about it.
[476.12s -> 479.50s]  Okay? Imagine that when you go think about it, let's say,
[479.88s -> 482.08s]  I just sat there and waited for you to think about it,
[482.08s -> 485.38s]  and then you came back to me, and we iterated on this,
[485.44s -> 486.92s]  and then I went to help the next student.
[487.56s -> 488.66s]  You would get out of there earlier.
[489.10s -> 491.16s]  But I would be very inefficient.
[491.34s -> 492.66s]  All the other students would be pissed off
[492.66s -> 493.42s]  because they'd be waiting.
[494.42s -> 495.90s]  Imagine I tell you, go think about it,
[495.90s -> 498.98s]  and then get in the back of the line, right?
[499.38s -> 501.50s]  So then what's going to happen is I'm constantly
[501.50s -> 502.66s]  asking questions.
[502.66s -> 504.16s]  I'm 100 percent utilized.
[504.56s -> 506.34s]  You could make no better use of your professor.
[507.42s -> 510.12s]  But you might, if you have a couple of follow-ups,
[510.12s -> 511.50s]  you might not get out of there for a while.
[512.58s -> 514.92s]  If my goal is to get you out of my office as quickly
[514.92s -> 516.88s]  as possible, I'm going to sit there and wait,
[517.08s -> 518.38s]  and I will be inefficient.
[518.88s -> 521.08s]  If my goal is to help the most amount of students
[521.08s -> 523.02s]  in a fixed amount of time, I'm going to interleave you.
[523.78s -> 525.66s]  That's what I mean by any one student is slower.
[526.54s -> 528.30s]  Okay? So let's think about this a little.
[528.30s -> 529.12s]  Here's a diagram.
[529.12s -> 530.34s]  Now I've flipped the direction.
[530.34s -> 533.54s]  Now I, just because I need to whiff, time is going this way.
[533.54s -> 534.58s]  These are clock cycles.
[535.54s -> 539.82s]  Now imagine I have a program here which says it's going
[539.82s -> 543.16s]  to do three math operations, and then it's going
[543.16s -> 545.22s]  to wait on memory, okay?
[545.22s -> 546.60s]  And let's just say it can't make progress
[546.60s -> 547.94s]  until that comes back, okay?
[547.94s -> 549.98s]  So it's going to do three math operations,
[550.12s -> 551.44s]  which I'm going to denote here.
[551.44s -> 554.88s]  Notice that blue indicates the processor is busy running
[554.88s -> 558.24s]  thread zero, and then there's going to be a memory operation
[558.24s -> 560.72s]  which, for the sake of this slide, I want you to think
[560.72s -> 562.94s]  of as 12 cycles of latency.
[563.92s -> 566.00s]  Okay? So time again is going this way.
[566.00s -> 566.60s]  I changed it up.
[568.12s -> 568.46s]  All right.
[568.96s -> 570.04s]  So here's the question.
[570.42s -> 573.62s]  Let's say this program repeats over and over and over again.
[574.16s -> 576.48s]  You know, I can get going again, then it stalls,
[576.48s -> 577.58s]  and so on and so on, right?
[577.58s -> 578.26s]  So this makes sense.
[579.26s -> 581.08s]  My question is, you know,
[581.08s -> 582.86s]  what is the utilization of the processor?
[583.38s -> 586.60s]  I guess I can draw it all the way out for you.
[591.26s -> 594.44s]  Sorry? You just got to count, right?
[597.18s -> 599.88s]  I do three cycles of work, and then I wait for 12.
[600.80s -> 602.84s]  So out of every group of 15 cycles,
[602.84s -> 604.28s]  I'm doing three cycles of work.
[605.00s -> 607.78s]  So three out of 15 is one-fifth, right?
[607.78s -> 609.38s]  So I'm at 20% utilized.
[611.88s -> 613.00s]  Now let's think about what happens
[613.00s -> 613.84s]  if I add another thread.
[616.00s -> 618.44s]  So now this core can, it says, well,
[618.44s -> 619.86s]  thread zero can't make progress.
[619.86s -> 620.78s]  Shoot, I'm going to go ahead
[620.78s -> 622.34s]  and run instructions from thread one.
[623.28s -> 625.84s]  So we get three instructions from thread one,
[625.84s -> 630.22s]  then thread one stalls, and so on and so on.
[630.78s -> 632.70s]  So now what's the utilization of my processor?
[635.18s -> 635.96s]  You know, I doubled, right?
[635.96s -> 638.34s]  Because now I can kind of do three instructions from you
[638.34s -> 640.44s]  and three instructions from you, and then I've got to wait
[640.44s -> 643.14s]  until some stuff comes back, and then I get going, OK?
[643.80s -> 645.64s]  All right, so now with your, you know,
[645.64s -> 647.82s]  your best friend sitting next to you, I want to know,
[647.82s -> 649.90s]  talk this over for 15, 20 seconds,
[650.34s -> 651.48s]  how many threads do you need
[651.48s -> 654.46s]  if you want 100% efficiency on this program?
[655.62s -> 656.64s]  How many threads do you need?
[657.68s -> 658.82s]  OK? Give it a shot.
[658.82s -> 666.02s]  Look at the pattern.
[667.30s -> 670.30s]  I'm doing three units of work every 15 cycles,
[670.60s -> 673.44s]  three plus 12 stall, three plus 12 stall.
[673.68s -> 676.44s]  Yeah. OK, what do you think?
[677.44s -> 678.90s]  Anybody want to volunteer an answer?
[679.30s -> 680.60s]  How many threads?
[681.30s -> 683.04s]  Why did you say five?
[683.04s -> 683.86s]  That's the correct answer.
[686.80s -> 687.16s]  Help me out.
[688.00s -> 691.04s]  One over one-fifth.
[691.46s -> 694.74s]  OK. Or, yeah, you can think about it that way.
[694.74s -> 696.38s]  Another way to think about it, the way I like to think
[696.38s -> 700.06s]  about it is how much, if I have one thread and it stalls,
[701.12s -> 702.62s]  how much latency do I need to cover?
[702.62s -> 705.30s]  I need to cover four cycles of latency.
[705.30s -> 709.34s]  And in this program, every thread gives me three cycles
[709.34s -> 712.48s]  of coverage, so I'm going to need four additional threads
[712.48s -> 714.52s]  to cover that gap, right?
[714.52s -> 716.18s]  So I'm going to need the one I just stalled
[716.22s -> 719.28s]  on plus four additional threads to be runnable to get
[719.28s -> 722.20s]  to five threads required for 100% utilization.
[722.78s -> 724.22s]  OK. So here's a question.
[724.72s -> 727.64s]  Now imagine that I move to a processor
[727.64s -> 730.06s]  that can interleave eight threads at once
[730.06s -> 731.04s]  with the same program.
[732.26s -> 734.50s]  How much faster do I run?
[735.46s -> 736.02s]  Talk it over.
[739.56s -> 739.88s]  All right.
[740.84s -> 741.34s]  What do you think?
[742.30s -> 743.72s]  And really I was a little cavalier
[743.72s -> 745.62s]  in saying how much faster do I run?
[745.96s -> 747.56s]  I probably should have been more specific.
[747.56s -> 750.94s]  I should say, what is the utilization of my processor?
[751.42s -> 754.00s]  Or what is the rate at which I finish things
[754.00s -> 754.82s]  or get stuff done?
[755.42s -> 757.44s]  Am I doing any better than before?
[758.38s -> 758.50s]  Yes?
[758.50s -> 761.30s]  I mean I was already running at 100%.
[762.18s -> 765.30s]  And now I'm going to have let's say a bunch more threads
[765.30s -> 768.70s]  in this case, I guess I have like seven other threads
[768.78s -> 770.36s]  to cover the latency of the stall.
[771.58s -> 774.22s]  By the time that memory gets back, like I only need it,
[774.42s -> 777.14s]  like you told me last time, four other threads, a total of five
[777.14s -> 778.32s]  in order to cover that stall.
[779.18s -> 781.54s]  So what, you know, would you build,
[781.54s -> 783.54s]  if this was the only program you ever had to run,
[783.54s -> 786.24s]  would you build an eight-way threaded processor?
[786.24s -> 788.76s]  Probably pretty inefficient because you'd end
[788.76s -> 793.00s]  up burning more chip space to store these execution contexts.
[793.12s -> 796.58s]  And every single thread would have higher overall
[796.58s -> 799.16s]  completion latency because it's sharing this one resource
[799.16s -> 800.70s]  with more and more threads.
[801.42s -> 803.66s]  OK. Now let me change this a second.
[804.40s -> 807.62s]  So now I change the program, not the computer.
[808.32s -> 811.66s]  I change the program so there's six arithmetic instructions
[811.90s -> 813.90s]  followed by that memory access.
[815.16s -> 817.20s]  Now go tell me, again, take your 30 seconds,
[817.24s -> 819.08s]  tell me how many threads you need to run
[819.18s -> 821.10s]  at full tilt, full utilization.
[821.52s -> 827.68s]  And by the way, first convince yourself I'm currently running
[827.68s -> 829.82s]  at 33% utilization, right?
[829.82s -> 840.30s]  So you've got to hide a six-cycle stall,
[840.30s -> 843.44s]  I'm sorry, a 12-cycle stall, how many threads do you need
[843.44s -> 844.78s]  if every thread gives you six?
[845.30s -> 849.30s]  You need two more threads, so you need a total of three threads
[849.30s -> 851.06s]  to run at 100% utilization.
[851.46s -> 853.30s]  So this is kind of interesting, right?
[853.30s -> 860.14s]  Like the ratio of math to latency is what determines how
[860.14s -> 863.04s]  much multi-threading you need to run at peak utilization.
[863.56s -> 867.40s]  So if you have programs that have a higher ratio of math
[867.52s -> 873.76s]  to memory latency, then you have the ability to get
[873.76s -> 874.88s]  by with fewer and fewer threads.
[876.40s -> 879.06s]  Or equivalently, you know, if you build a big data cache,
[879.06s -> 881.74s]  a data cache might absorb some of these cache misses
[882.28s -> 885.06s]  and actually bring in your memory access time, right?
[885.06s -> 888.20s]  So a big data cache probably means you need fewer threads.
[888.20s -> 890.54s]  If you take out the data cache, you take more misses
[890.54s -> 893.60s]  and you actually need more threads, okay?
[894.04s -> 897.80s]  So takeaway number one from the sequence is we haven't added
[897.80s -> 899.62s]  anything to the processor in terms
[899.62s -> 902.56s]  of what it can execute at any one time.
[902.56s -> 904.88s]  We have given it no more peak throughput.
[905.72s -> 909.40s]  However, we've given it a new mechanism to be able
[909.40s -> 913.18s]  to utilize those resources more efficiently, okay?
[914.92s -> 917.96s]  And yeah, and we talked about how this is
[917.96s -> 920.34s]  about hiding the effects of memory latency.
[920.34s -> 923.46s]  Keep in mind that we haven't reduced memory latency at all.
[923.46s -> 924.78s]  It was always 12 cycles.
[925.48s -> 927.58s]  We were just hiding its effects.
[927.58s -> 929.84s]  We were just not stalling when we were waiting for that.
[929.92s -> 930.16s]  Yes, sir?
[931.16s -> 932.36s]  This is kind of really interesting,
[932.36s -> 935.84s]  but I'm a little bit confused with that, but if we wanted
[935.84s -> 939.16s]  to compute like the theoretical max benefit
[939.28s -> 943.64s]  of additional hardware threads, then, you know,
[943.72s -> 947.84s]  we could imagine a case where there's a lot of cache misses
[947.84s -> 949.26s]  and you're waiting around a lot
[949.26s -> 952.20s]  and then perhaps it cleanly divides the workload,
[952.44s -> 955.34s]  but you might be understanding why that's not the case
[955.34s -> 958.06s]  as we just said, so if you wanted to think
[958.06s -> 960.36s]  of like a theoretical max, would we think
[960.36s -> 961.50s]  about just scaling it or?
[961.82s -> 965.56s]  Well, the theoretical max would be 100% utilization,
[966.94s -> 968.48s]  and then if you wanted to know the speed
[968.48s -> 969.96s]  up due to hardware multithreading,
[969.96s -> 971.14s]  you'd have to know the speed
[971.14s -> 974.08s]  of only running one thread at a time, okay?
[974.08s -> 976.20s]  So the two numbers are what do I measure
[976.20s -> 979.94s]  when I do one thread at a time, and then given what I know
[979.94s -> 982.16s]  about what the processor, if I just look it up in a book,
[982.16s -> 984.76s]  what this processor can do per clock or estimate it,
[985.52s -> 987.36s]  multithreading might be able to get me close
[987.36s -> 988.28s]  to that high-water mark.
[988.28s -> 992.52s]  I'm going to take your question, and then we're going
[992.52s -> 993.98s]  to iterate on that just a little bit more.
[993.98s -> 994.80s]  Yes, in the back.
[995.40s -> 997.32s]  Does it only take one cycle to go
[997.32s -> 998.56s]  from one thread to another?
[998.78s -> 1000.76s]  Yeah, let's just, for the sake of this class,
[1000.96s -> 1001.74s]  let's just assume that.
[1002.02s -> 1004.24s]  I mean, you should just think about it if you're familiar
[1004.24s -> 1005.54s]  with underlying processor details.
[1005.60s -> 1008.44s]  All it's doing is switching the current PC to another PC.
[1011.22s -> 1014.38s]  Okay, so there were three ideas from last time,
[1014.38s -> 1018.04s]  multi-core, SIMD, and multithreading,
[1018.04s -> 1019.94s]  and then the first class actually had superscaler,
[1019.94s -> 1020.58s]  but let's think about it.
[1020.58s -> 1023.56s]  So here was a fake chip that has 16 cores.
[1024.36s -> 1027.84s]  Every core is four-way threaded right now.
[1028.64s -> 1031.84s]  So if I have a program that creates 64 threads,
[1033.16s -> 1034.86s]  those threads will just get distributed
[1034.86s -> 1036.92s]  to those 64 execution contexts.
[1037.60s -> 1040.42s]  Every single one of these cores, independently,
[1040.74s -> 1044.14s]  every clock is picking one of those instructions from one
[1044.14s -> 1047.74s]  of those four threads to go run, and those instructions,
[1047.74s -> 1049.66s]  in this case, can be eight-wide vector.
[1050.58s -> 1055.78s]  So if I need 64 threads, each one can be doing eight pieces
[1055.78s -> 1058.98s]  of data at once, if, let's say, we were processing an array
[1058.98s -> 1060.66s]  like computing the sign of all the numbers,
[1060.98s -> 1063.72s]  I would need 512 independent things to do,
[1063.72s -> 1067.88s]  an array of at least 512 to run this thing at peak rate
[1068.20s -> 1070.06s]  with maximal latency hiding ability.
[1070.18s -> 1074.24s]  That's what I want to make sense to you.
[1075.88s -> 1077.50s]  You know, like, if you look at this, you might say, oh,
[1077.50s -> 1080.64s]  this is a 16-core processor, and I'm telling you,
[1081.44s -> 1086.52s]  it has a peak throughput of 16 times 8, 128 execution units,
[1086.52s -> 1089.00s]  or 128 pieces of data can be processed in parallel,
[1089.46s -> 1092.64s]  but in order to hide latency, you better give it 512 things.
[1093.14s -> 1096.80s]  Okay? This is all of this put together.
[1097.94s -> 1099.92s]  Now, you asked me a question about x86.
[1100.72s -> 1102.10s]  Let me take one of those cores
[1102.68s -> 1104.60s]  and give you a course approximation
[1104.60s -> 1106.72s]  to what's in a myth machine, okay?
[1107.14s -> 1111.12s]  A myth machine is a two-way multi-threaded machine.
[1112.64s -> 1116.46s]  It's also a traditional Intel core, which means it can run
[1117.46s -> 1121.50s]  in a superscalar fashion multiple instructions per clock
[1121.50s -> 1122.16s]  inside a core.
[1123.12s -> 1125.04s]  So what this thing is going to do is it's going to get,
[1125.04s -> 1126.52s]  it's going to look at its two threads.
[1127.30s -> 1130.24s]  Now, my core here could only run one instruction per clock,
[1130.72s -> 1132.70s]  so it had to pick a thread and then run the instruction.
[1133.40s -> 1136.44s]  But imagine you had a core, again, just talking about one
[1136.44s -> 1138.88s]  of these things, they could run multiple instructions per
[1138.88s -> 1141.04s]  clock, which is what we used to call, like,
[1141.04s -> 1142.60s]  a superscalar processor or something.
[1143.04s -> 1145.48s]  So I have a couple of different execution units,
[1146.32s -> 1149.72s]  and then I have two threads I can go grab instructions from.
[1150.72s -> 1153.02s]  So if I had to, like, cartoon out myth,
[1153.84s -> 1155.34s]  it might look something like this.
[1156.32s -> 1160.02s]  It's got two threads, Intel likes to call these hyper-threads
[1160.14s -> 1162.24s]  that it can draw from, and it has
[1163.12s -> 1166.66s]  at least three eight-wide vector ALUs and a bunch
[1166.66s -> 1169.62s]  of scalar, you know, I plus one stuff that it can do.
[1170.24s -> 1174.64s]  So in every clock, this chip is going, I can utilize
[1174.64s -> 1177.56s]  up to three vector operations per clock from these ALUs,
[1178.92s -> 1179.80s]  from those two threads.
[1181.50s -> 1183.24s]  So in some sense, it can find a mole add,
[1183.24s -> 1185.48s]  it can find another mole add, it can find another mole add.
[1186.92s -> 1188.76s]  You know, the details of this, you know, are not,
[1188.76s -> 1191.14s]  you're not expected to be that precise on the assignment.
[1191.14s -> 1193.76s]  So you can kind of just think about it as there's,
[1195.36s -> 1198.54s]  in general, if there were no vector instructions,
[1198.54s -> 1201.08s]  it can actually do about three ALU,
[1201.08s -> 1202.98s]  scalar ALU ops per clock.
[1203.96s -> 1206.90s]  With vector instructions, it can actually end up doing at max
[1206.90s -> 1208.78s]  about three vector operations per clock.
[1209.28s -> 1212.70s]  So you should see about in what speed up from vector.
[1213.24s -> 1220.40s]  Yeah, like my max throughput is about three scalar ops,
[1220.96s -> 1222.92s]  and my max throughput is about three vector ops.
[1222.92s -> 1224.56s]  So if I move from vector to scalar,
[1225.16s -> 1230.46s]  I could get up to eight X. Now, it turns out that
[1230.84s -> 1233.78s]  if you're just using one thread, you know,
[1233.78s -> 1237.24s]  there might not be instruction level parallelism in the thread
[1237.24s -> 1238.70s]  to find those three vector ops.
[1239.38s -> 1241.12s]  So when you throw that extra thread in there,
[1241.60s -> 1243.84s]  the chip is actually going, oh, I've got two threads.
[1243.84s -> 1245.08s]  Almost all the time, I'm going
[1245.08s -> 1246.80s]  to have the three vector operations there
[1246.80s -> 1248.40s]  to actually use, okay?
[1248.76s -> 1252.26s]  So you're going to see a performance benefit from going
[1252.26s -> 1254.28s]  from running one thread on this core to two threads
[1254.28s -> 1256.44s]  on this core, because one thread is not going
[1256.44s -> 1258.30s]  to saturate all those execution resources.
[1258.78s -> 1258.88s]  Yes?
[1260.16s -> 1263.34s]  And what I just said, by the way, is if you, like,
[1263.56s -> 1266.28s]  now we're getting into real, like, taking the basic concepts
[1266.28s -> 1268.20s]  for the course, which I would like you
[1268.24s -> 1271.24s]  to understand in this diagram, and then applying it
[1271.24s -> 1273.36s]  in a much more messy real-world setting,
[1273.84s -> 1276.28s]  that you can understand that jump, you're doing very well.
[1277.40s -> 1277.50s]  Yeah?
[1278.46s -> 1280.34s]  So this is a superscalar core, right?
[1280.34s -> 1280.44s]  Yeah.
[1280.44s -> 1282.72s]  And it's a multi-threaded core.
[1283.52s -> 1284.20s]  So the two threads.
[1284.20s -> 1285.14s]  And it's a SIMD core.
[1285.56s -> 1286.38s]  The two threads, they can-
[1286.38s -> 1287.72s]  And it's part of a multi-threaded chip.
[1287.92s -> 1289.50s]  Oh, sorry, multi-core chip.
[1289.82s -> 1290.24s]  Yeah, you're right.
[1290.64s -> 1293.28s]  Just saying the two threads, they are theoretically, again,
[1293.40s -> 1294.74s]  just both going to parallel.
[1294.74s -> 1297.88s]  Like, do I have to fetch code sequentially for the two threads,
[1297.88s -> 1299.18s]  or do I have to do something else?
[1299.18s -> 1299.36s]  Absolutely.
[1299.36s -> 1302.40s]  I mean, just, again, I'm being very, very loose now.
[1302.40s -> 1305.36s]  But think about it as I have all these fetch and decodes.
[1306.28s -> 1308.52s]  All those fetch and decodes are doing is they're running
[1308.52s -> 1312.08s]  their superscalar instruction logic to say,
[1312.42s -> 1314.78s]  what independent instructions can I find in thread zero?
[1315.50s -> 1317.84s]  What independent instructions can I find in thread one?
[1318.66s -> 1321.12s]  I've got all these yellow boxes that I want to fill up.
[1321.62s -> 1323.74s]  If I find a mixture of instructions across any
[1323.74s -> 1325.92s]  of those threads that I can shove into yellow boxes,
[1325.98s -> 1328.00s]  I'm going to do it and try and saturate the machine.
[1329.54s -> 1331.46s]  Yep. Let me go here, and then here.
[1331.58s -> 1331.68s]  Yeah.
[1332.48s -> 1334.46s]  So if there are three vector ALUs,
[1334.64s -> 1338.80s]  how many instructions can you give to the value?
[1339.74s -> 1342.16s]  Well, if there are three vector ALUs, I better be able
[1342.16s -> 1343.94s]  to execute three operations at once.
[1343.94s -> 1347.04s]  Otherwise, I shouldn't have built these ALUs, right?
[1347.22s -> 1349.46s]  So if I can run three vector ops at once,
[1349.46s -> 1351.94s]  how many total floating-point operations am I doing?
[1354.18s -> 1355.16s]  Three times eight, yeah.
[1355.56s -> 1357.94s]  And honestly, three times eight times two if you count them
[1357.94s -> 1359.70s]  as a multiply add all in one thing.
[1359.82s -> 1361.22s]  So it's a lot of ops per clock.
[1362.90s -> 1363.04s]  Yeah.
[1363.36s -> 1368.74s]  How does having extra fetch-decode functions help
[1368.74s -> 1370.52s]  in your sense compared to the previous technique?
[1371.40s -> 1373.86s]  I'm just drawing these orange boxes
[1373.86s -> 1377.62s]  to indicate one box per instruction that can get shoved
[1377.62s -> 1378.72s]  into the pipeline at once.
[1379.52s -> 1381.00s]  You can just as well think about it
[1381.00s -> 1383.22s]  as a fetch and decode unit that's a fancy fetch
[1383.22s -> 1385.18s]  and decode unit that has the ability
[1385.18s -> 1386.70s]  to dispatch multiple instructions.
[1387.24s -> 1389.00s]  But at the end of the day, if you have a bunch
[1389.00s -> 1391.86s]  of execution units, you're going to have
[1391.90s -> 1394.60s]  to be dispatching more than one instruction per clock to them
[1394.60s -> 1395.62s]  in order to keep them full.
[1396.16s -> 1396.74s]  Yeah. Okay.
[1397.74s -> 1404.46s]  So now, like, everything here composes.
[1405.26s -> 1406.44s]  It's a multi-threaded chip.
[1407.14s -> 1407.98s]  It's a SIMD chip.
[1408.38s -> 1409.70s]  It's a superscalar chip.
[1410.10s -> 1412.38s]  And the implementation of superscalar tends
[1412.38s -> 1414.76s]  to mean finding instructions within the same thread.
[1415.40s -> 1416.42s]  You should think about it this way
[1416.42s -> 1418.66s]  if you're an Intel architect, and correct me if I'm wrong.
[1418.66s -> 1420.06s]  They were sitting around with all
[1420.06s -> 1421.54s]  of the yellow boxes in one thread.
[1422.24s -> 1424.24s]  And they were having trouble because of the diagram
[1424.24s -> 1426.72s]  that I showed you in the class of finding enough work
[1426.72s -> 1429.50s]  in that one thread in order to fill
[1429.50s -> 1431.14s]  up all of their execution units.
[1431.70s -> 1433.80s]  So they just said, oh, let's just think
[1433.80s -> 1435.60s]  about the second thread as a source
[1435.60s -> 1438.42s]  of independent instructions and feed that right
[1438.42s -> 1440.50s]  into my global scheduler.
[1441.10s -> 1443.10s]  Because now I don't have to, I can assume instructions
[1443.10s -> 1444.62s]  from two threads are obviously independent
[1444.96s -> 1445.76s]  because they're in different threads.
[1446.40s -> 1446.98s]  Yes, ma'am, in the back.
[1447.26s -> 1450.68s]  Correct. So it's also, you know, it's also a form
[1450.68s -> 1454.14s]  of simultaneous multi-threading, which you have two forms
[1454.14s -> 1455.56s]  of simultaneous multi-threading.
[1455.56s -> 1457.22s]  You have different threads on different cores,
[1457.22s -> 1459.70s]  and then you have the simultaneous multi-threading
[1459.70s -> 1461.56s]  inside a single core with the different threads.
[1462.56s -> 1462.66s]  Yeah.
[1463.22s -> 1466.30s]  Just to clarify, the two threads are normally the same,
[1466.30s -> 1467.74s]  eight, one, after the ALU?
[1469.38s -> 1471.24s]  An execution unit does one thing at a time.
[1472.56s -> 1475.18s]  So you could use it in this clock, and I could use it
[1475.18s -> 1477.70s]  in a different clock, but the idea that we're going
[1477.70s -> 1480.28s]  to give two instructions to one unit and expect
[1480.28s -> 1483.62s]  to do two things in one clock is not feasible.
[1483.62s -> 1484.62s]  It's not what the hardware does.
[1485.36s -> 1485.54s]  Yes.
[1486.30s -> 1489.54s]  So the instructions in Excel box is basically finding seven
[1489.54s -> 1491.06s]  instructions across two threads
[1491.24s -> 1492.50s]  that it can represent, correct?
[1492.80s -> 1498.86s]  Yeah. Now, don't quote me as that's exactly how Cavielake
[1498.86s -> 1501.84s]  works, like I tried to read the reference manuals as best
[1501.84s -> 1503.70s]  as I could, but maybe it's six, maybe it's eight,
[1503.70s -> 1505.82s]  maybe it's, you know, but you guessed it.
[1505.82s -> 1507.80s]  That would be seven fetch decode boxes, right?
[1508.20s -> 1514.12s]  Yeah, that's, well, we should move on.
[1514.58s -> 1518.02s]  This came from me reading the Intel instruction manual,
[1518.28s -> 1524.64s]  and some of these instructions are longer latency.
[1526.04s -> 1529.28s]  So you can get by with lower fetch capability by saying,
[1529.28s -> 1531.50s]  I'm going to give this instruction unit instruction.
[1531.50s -> 1533.18s]  I know I don't have to fill it up for a while
[1533.18s -> 1534.26s]  if it's not fully pipelined.
[1535.10s -> 1537.80s]  So we're now getting into micro architectural details
[1537.80s -> 1539.98s]  that I actually could not even recite off the top of my head.
[1540.54s -> 1543.54s]  So it's not true necessarily that you need as many orange boxes
[1543.54s -> 1547.44s]  as you have yellow boxes to keep the thing in,
[1548.24s -> 1549.56s]  but let's take this one offline.
[1550.98s -> 1553.08s]  Okay, two more and then we should keep moving.
[1553.08s -> 1553.34s]  Yes, sir?
[1553.98s -> 1558.16s]  Why you can fetch more instructions
[1558.16s -> 1561.62s]  than you have execution points and so forth?
[1561.92s -> 1564.88s]  Because any one thread can have independent instructions
[1564.88s -> 1565.48s]  inside of it.
[1566.12s -> 1568.54s]  There's nothing preventing a single instruction stream
[1568.54s -> 1570.24s]  from having two independent multiplies
[1570.98s -> 1572.46s]  on two different sets of registers.
[1572.98s -> 1574.50s]  That's the idea of super scaler.
[1575.32s -> 1577.52s]  Independent instructions inside a thread
[1577.52s -> 1579.44s]  that the chip finds for you automatically.
[1581.14s -> 1581.34s]  Okay.
[1582.34s -> 1584.90s]  So the whole of it is like a thread that's a lot
[1584.90s -> 1587.90s]  of inline parallelization instructions.
[1588.30s -> 1591.38s]  So theoretically it doesn't mean that this one thread.
[1591.58s -> 1594.58s]  I don't know what the rules of this specific processor are,
[1594.58s -> 1597.28s]  but my conceptual answer to your question is yes.
[1597.28s -> 1599.88s]  If there was sufficient ILP in one thread to fill
[1599.88s -> 1602.38s]  up the machine, you don't need two.
[1603.26s -> 1606.92s]  Right? And in fact, in the early days of hyper-threading,
[1608.12s -> 1609.58s]  when that was actually the case,
[1609.70s -> 1611.68s]  the threads actually started interfering.
[1611.68s -> 1613.60s]  Like one would kick each other's data out of a cache.
[1614.20s -> 1616.24s]  And so you had the ability to turn off the second thread.
[1616.88s -> 1618.96s]  And it might, it would help on certain applications.
[1618.96s -> 1620.02s]  So threads can interfere.
[1620.02s -> 1621.24s]  They're not always a positive thing.
[1621.90s -> 1623.86s]  Okay. Let's get going and I hope
[1623.86s -> 1626.08s]  that the next sequence will answer even more questions.
[1626.46s -> 1628.92s]  First of all, you know, this was my fake chip.
[1629.74s -> 1631.90s]  This is like what one of the cores
[1631.90s -> 1633.82s]  in a modern Intel processor might look like.
[1634.38s -> 1638.48s]  Instead of 16 cores, remember that like an NVIDIA GPU has a
[1638.48s -> 1639.62s]  whole bunch of these things.
[1640.30s -> 1642.80s]  We're not going to talk about it today, but if I had
[1642.80s -> 1646.48s]  to diagram the inside of maybe like one core of one
[1646.48s -> 1649.16s]  of these 144 cores of an NVIDIA GPU,
[1649.16s -> 1650.22s]  it might look a little bit like this.
[1651.80s -> 1656.76s]  And the way you can think about it is as 32 wide vector,
[1657.42s -> 1661.70s]  four 32 wide vectors per clock drawing instructions from up
[1661.70s -> 1663.22s]  to 64 threads per core.
[1664.70s -> 1667.10s]  The numbers are, you know, the concepts are the same.
[1667.10s -> 1668.24s]  The numbers are larger.
[1668.24s -> 1669.48s]  We'll get more into that when we talk
[1669.48s -> 1670.72s]  about NVIDIA programming.
[1671.34s -> 1676.40s]  But the implication of this is that I need, you know,
[1676.40s -> 1678.94s]  in the maximum latency hiding state,
[1679.46s -> 1682.32s]  64 threads per core times something like 80
[1682.32s -> 1687.52s]  or 144 cores times 32 per thread gives you something
[1687.52s -> 1689.16s]  where if you want to fill up this chip
[1689.32s -> 1691.86s]  with my Sinex example, you're talking
[1691.86s -> 1694.92s]  about don't bother unless you have hundreds of thousands
[1694.92s -> 1697.58s]  of things in parallel, right?
[1697.58s -> 1701.66s]  This is why a small DNN does not run well on a big GPU
[1701.66s -> 1704.96s]  because there's just not enough work, okay?
[1705.46s -> 1713.24s]  Okay. Yeah, I think we've gone over this a good deal.
[1713.24s -> 1717.74s]  So let me go over one more review one more time
[1717.80s -> 1719.92s]  for everybody before we move on to the rest of the day.
[1720.80s -> 1722.34s]  Last time I gave you this program.
[1722.42s -> 1724.04s]  This was just a basic C program.
[1725.46s -> 1729.56s]  We focused on running just kind of the inside of the loop,
[1729.78s -> 1731.78s]  these instructions if we compiled it.
[1732.68s -> 1736.72s]  I told you that if I had a simple single threaded single
[1736.72s -> 1740.26s]  core processor, that processor would run one
[1740.26s -> 1744.40s]  of those instructions per clock, very simple.
[1745.44s -> 1746.58s]  On the first day of class,
[1746.58s -> 1749.62s]  I said that without you doing anything to your program,
[1750.10s -> 1751.96s]  an architect at Intel could say, you know what?
[1752.36s -> 1753.00s]  I'm going to look
[1753.00s -> 1755.66s]  for independent instructions inside a single thread.
[1756.02s -> 1758.14s]  I will find them for you automatically.
[1758.70s -> 1763.22s]  I will execute super scaler and if your program happens
[1763.22s -> 1765.84s]  to have independent instructions like what I'm showing here,
[1766.32s -> 1768.42s]  two instructions might get run per clock
[1768.88s -> 1771.72s]  from the same thread from one core.
[1772.12s -> 1774.02s]  I'm highlighting those instructions in orange.
[1777.02s -> 1782.02s]  I also told you if I changed my program, if I rewrote it
[1782.02s -> 1786.64s]  with CS149 Intrinsics or AVX Intrinsics or I had a compiler
[1786.64s -> 1788.86s]  like ISPC generate those Intrinsics,
[1789.46s -> 1793.58s]  the binary might change and it might be vector instructions.
[1793.58s -> 1797.58s]  And if I had a processor that had a vector ALU now,
[1797.58s -> 1801.58s]  a vector execution unit, when I ran one instruction per clock,
[1801.58s -> 1802.90s]  it would do eight things.
[1802.90s -> 1805.54s]  Notice how I only have one instruction per clock running
[1805.54s -> 1809.20s]  now, one orange highlight, but that's a vector instruction.
[1811.92s -> 1816.20s]  There's no reason why I can't make a super scaler processor
[1817.24s -> 1818.68s]  with different types of units.
[1819.06s -> 1823.26s]  Here's one with one scaler and one vector and one thread.
[1823.26s -> 1825.48s]  So this core is now kind of looking at the thread and go,
[1825.48s -> 1826.94s]  well, if your program happens
[1826.94s -> 1829.22s]  to have a scaler instruction that's independent
[1829.22s -> 1831.06s]  from a vector, I can do both at once.
[1831.94s -> 1834.12s]  But if I can't find a scaler independent from a vector,
[1834.12s -> 1835.86s]  I'm only going to run one of those two.
[1836.54s -> 1838.94s]  Can't do two vectors at once, just don't have the hardware.
[1842.32s -> 1845.56s]  So now super scaler with two different types of instructions.
[1847.64s -> 1849.42s]  Then I said there was this idea of multi-threading.
[1849.42s -> 1854.54s]  Multi-threading is just taking the core, I went back
[1854.54s -> 1858.38s]  to a simple single non-vector core and having two threads.
[1858.84s -> 1862.26s]  And now if my program creates two threads,
[1862.34s -> 1864.24s]  this processor can run each thread,
[1864.66s -> 1867.34s]  notice each with a different current program counter.
[1867.98s -> 1869.86s]  You can have the state for both of those threads
[1869.86s -> 1871.38s]  on the chip at the same time.
[1871.76s -> 1874.42s]  And because this core can only actually run one instruction
[1874.52s -> 1877.44s]  per clock, every clock it picks one of the two threads
[1877.44s -> 1878.50s]  and runs the next instruction.
[1878.84s -> 1880.64s]  So I'm highlighting one of those two instructions.
[1881.24s -> 1886.54s]  I could also build a super scaler core with two threads.
[1886.98s -> 1891.40s]  And on this particular clock, my super scaler core found,
[1891.54s -> 1894.70s]  to your point, two independent instructions, one scaler
[1894.70s -> 1897.90s]  and one vector in this thread and decided to run them both
[1897.90s -> 1898.88s]  on the core at the same time.
[1899.86s -> 1903.36s]  Perfectly valid solution if the execution engine supports that.
[1904.96s -> 1912.08s]  Now let me move to four threads and now the chip decided
[1912.08s -> 1915.10s]  to pick one vector and one scaler from two
[1915.10s -> 1916.52s]  of those four threads and run them
[1916.52s -> 1917.52s]  on the processor at once.
[1918.18s -> 1919.16s]  Totally within my rules.
[1920.98s -> 1922.22s]  Right? It's just trying to find work.
[1922.80s -> 1927.98s]  And of course everything I have here you can just replicate
[1927.98s -> 1930.22s]  in your head across cores.
[1930.22s -> 1932.40s]  So now I have two completely different cores,
[1932.84s -> 1936.56s]  each is four-way multi-threaded, each can actually do two,
[1936.56s -> 1938.70s]  a mixture of two instructions per clock.
[1939.20s -> 1941.46s]  So my program better create eight threads
[1941.46s -> 1942.56s]  to fill this thing up.
[1943.02s -> 1945.82s]  And amongst those eight threads, four of them are mapped
[1945.82s -> 1947.60s]  to these execution contexts.
[1947.94s -> 1951.68s]  So the core picks this vector and that scaler to do stuff.
[1953.06s -> 1956.22s]  Instruction threads four through seven are mapped
[1956.22s -> 1959.06s]  to this core and so this core happened to find a vector
[1959.06s -> 1961.42s]  and a multiply here and fills itself up.
[1962.76s -> 1962.94s]  Yeah?
[1963.28s -> 1965.38s]  So in that program you could have four threads running
[1965.50s -> 1966.14s]  at the same time?
[1967.48s -> 1970.46s]  Let me clarify, there are eight threads
[1970.46s -> 1972.48s]  that are concurrently live on the processor
[1973.12s -> 1978.86s]  and at any one time if it's possible for two of these four
[1978.86s -> 1981.54s]  to be selected for actual making progress and two
[1981.54s -> 1985.44s]  of those four, but it would not be possible for these four
[1985.44s -> 1988.08s]  all at once to be chosen at the same time, right?
[1988.34s -> 1990.90s]  Because in other words like you should think about this side
[1990.90s -> 1993.70s]  of the diagram or that side of the diagram, but never both.
[1994.48s -> 1998.66s]  And here I've decided, and now replace this core
[1999.10s -> 2000.50s]  with something more like this
[2000.50s -> 2001.80s]  and you've got something a little bit more
[2001.80s -> 2004.66s]  like a modern Intel chip and if you make four
[2004.66s -> 2007.30s]  of those that's what you have on this, ish.
[2008.96s -> 2009.56s]  Any more questions?
[2009.62s -> 2009.72s]  Yeah?
[2010.26s -> 2012.46s]  So a quick thing about the fetch decode units,
[2012.66s -> 2015.94s]  you roughly need the same number of fetch decode units
[2015.94s -> 2017.02s]  as the number of ANU's.
[2017.02s -> 2019.56s]  It's a reasonable, a good rule of thumb, yeah,
[2019.66s -> 2021.22s]  unless you start doing complicated stuff,
[2021.44s -> 2022.68s]  but we don't have to talk about that.
[2023.24s -> 2025.44s]  Now let me just give you one little detail that we'll talk
[2025.44s -> 2027.22s]  about offline if folks are interested.
[2027.78s -> 2030.58s]  You know how I said, like if you look closely at my programs
[2030.58s -> 2034.16s]  if it's a vector operation it's here in the instruction stream
[2034.16s -> 2035.80s]  like a compiler emitted vectors?
[2036.72s -> 2038.92s]  That's actually kind of how it works on Intel CPUs
[2038.92s -> 2040.14s]  and maybe even Intel GPUs,
[2040.14s -> 2042.44s]  but in video and AMD GPUs work a little bit differently.
[2042.96s -> 2045.46s]  It's the same ideas, but I just want
[2045.46s -> 2046.72s]  to see this in your mind.
[2047.58s -> 2051.24s]  They never, ever, ever generate vector instructions.
[2051.40s -> 2053.80s]  They generate scaler instructions at all times.
[2055.18s -> 2058.46s]  And the chip is designed so that, let's just say,
[2058.46s -> 2062.04s]  here's a chip with eight thread execution contexts
[2062.52s -> 2064.44s]  and under the hood there's a single,
[2064.44s -> 2066.28s]  it can do one vector operation per clock.
[2066.44s -> 2071.72s]  Now this is not, this is just an interesting detail.
[2071.72s -> 2074.34s]  So what they do is they have logic in the chip to say
[2074.76s -> 2080.02s]  if there are eight threads with the same program counter I'm
[2080.02s -> 2081.82s]  going to run all of their instructions
[2081.82s -> 2082.86s]  on this ALU at once.
[2084.08s -> 2085.40s]  So the effect is the same
[2085.40s -> 2088.06s]  as if the compiler had generated a vector instruction.
[2088.42s -> 2091.10s]  It just says I'm always trying to run eight threads
[2091.10s -> 2093.94s]  at once as long as all those threads are
[2093.94s -> 2095.40s]  at the same program counter right now.
[2096.36s -> 2097.94s]  Okay. And I can talk about that offline
[2097.94s -> 2099.30s]  if you want a little bit of clarity.
[2099.66s -> 2103.22s]  Okay. So last little question that people ask me,
[2103.48s -> 2106.82s]  imagine that you write a program that spawns two P-threads
[2106.82s -> 2110.40s]  or two C++ threads, and we're running on a processor here
[2110.40s -> 2113.42s]  that has two cores and two execution contexts per core.
[2116.00s -> 2119.44s]  What decides what thread runs on what execution context?
[2121.56s -> 2123.30s]  That's actually your OS.
[2123.94s -> 2128.00s]  So when you create a thread your OS goes oh we need a thread
[2128.00s -> 2131.08s]  of control, processor start running this PC
[2131.08s -> 2132.44s]  on execution context one.
[2133.34s -> 2134.82s]  Right. So that's the operating system.
[2135.40s -> 2136.80s]  So that's the OS right.
[2136.80s -> 2140.36s]  And if you were implementing the OS it would be an interesting
[2140.36s -> 2141.94s]  scheduling decision to say
[2142.78s -> 2145.02s]  which thread should I put on what cores?
[2146.14s -> 2149.50s]  Because you know that these two threads share
[2149.50s -> 2150.62s]  execution resources.
[2150.96s -> 2153.26s]  Imagine if we only had two threads in the machine
[2153.94s -> 2156.28s]  would it make sense to put them both on core zero?
[2157.58s -> 2158.56s]  Probably not.
[2158.56s -> 2160.84s]  It probably makes sense to put your first thread on core zero
[2160.84s -> 2162.18s]  and your next thread on core one
[2162.56s -> 2164.16s]  so they each have their full complement
[2164.16s -> 2165.36s]  of execution resources.
[2165.94s -> 2168.40s]  Now of course that answer might be different if, for example,
[2168.40s -> 2170.18s]  they were touching the same data and you wanted
[2170.18s -> 2171.86s]  to share a cache or something like that.
[2171.86s -> 2174.00s]  But that's a pretty interesting scheduling decision.
[2174.48s -> 2179.30s]  Okay. So that was sort of a very, very deep dive
[2179.30s -> 2180.76s]  into these concepts from last time.
[2181.06s -> 2181.18s]  Yes.
[2181.44s -> 2184.26s]  Yeah. Related to this last point about scheduling,
[2184.32s -> 2188.02s]  but if we have multiple execution projects it seems
[2188.02s -> 2189.76s]  like there's also a decision where,
[2190.00s -> 2191.74s]  or sorry multiple ALUs, there's a decision
[2191.74s -> 2194.74s]  where we could either run multiple hardware things
[2194.74s -> 2197.00s]  in parallel or run the superscaler.
[2197.42s -> 2199.98s]  That's not a decision for the operating system to make.
[2200.02s -> 2201.98s]  That is a chip implementation detail.
[2202.54s -> 2205.58s]  The operating system says hey chip you will run this thread
[2205.58s -> 2207.28s]  on this execution context.
[2207.84s -> 2210.84s]  The chip every single clock is now making a decision
[2210.88s -> 2215.02s]  for given the execution context it has, what instructions
[2215.02s -> 2217.84s]  to select as completely out of the regime
[2217.84s -> 2218.78s]  of the operating system.
[2218.78s -> 2220.66s]  The chip is making that decision every cycle,
[2220.66s -> 2221.78s]  a billion times per second.
[2222.26s -> 2225.50s]  The operating system if you have a thousand threads
[2225.50s -> 2228.82s]  or a thousand processors is periodically telling the chip
[2229.30s -> 2230.88s]  here are the eight that I want you to run
[2230.88s -> 2232.28s]  on your execution context.
[2232.62s -> 2235.56s]  That is an OS context switch and that might take hundreds
[2235.56s -> 2238.22s]  of thousands of cycles and that happens very infrequently.
[2238.56s -> 2238.76s]  Question?
[2238.76s -> 2245.12s]  Is there a way to like do the compiler or something force?
[2245.92s -> 2248.96s]  Yeah, modern operating systems have API's
[2248.96s -> 2251.56s]  for saying this thread goes on this execution.
[2251.56s -> 2253.40s]  Well no, just the force is super stupid,
[2253.40s -> 2254.76s]  I'm not going to know how to do something,
[2254.76s -> 2257.66s]  I'm going to do a coprocessor and I'm not even going
[2257.66s -> 2259.24s]  to do what those instructions are doing on here.
[2259.54s -> 2262.18s]  I mean to my knowledge that's not something that you get
[2262.26s -> 2264.12s]  to mess with too much.
[2264.12s -> 2266.38s]  I mean you certainly can say I'm only going to give you,
[2266.64s -> 2267.70s]  I'm going to turn off hyper-threading
[2267.70s -> 2268.38s]  or something like that.
[2268.44s -> 2270.64s]  That is capable but actually mucking
[2270.64s -> 2273.40s]  with the low level details of the instruction schedule
[2273.40s -> 2275.34s]  to my knowledge I don't know how much control you get
[2275.34s -> 2277.24s]  over that, maybe at the BIOS level but not
[2277.72s -> 2280.14s]  at a standard OS systems API level.
[2280.82s -> 2283.14s]  Okay, here's a question for you given what you know now.
[2284.74s -> 2287.06s]  I'm going to take your favorite NumPy program,
[2287.90s -> 2291.62s]  allocate a big array A, allocate a big array B,
[2292.46s -> 2293.88s]  PyTorch program if all you care.
[2294.88s -> 2297.32s]  We're going to do an element wise addition of these
[2297.32s -> 2301.56s]  or multiplication of these two arrays.
[2301.64s -> 2304.12s]  Let's say these arrays are like tens of millions
[2304.12s -> 2305.60s]  of elements, they're really big arrays.
[2307.34s -> 2311.24s]  Is this a good application to run on the types of computers
[2311.24s -> 2313.36s]  that I've told you about, yes or no?
[2314.62s -> 2315.72s]  Talk it over, why don't you think about it?
[2316.28s -> 2318.50s]  And then we're going to do a vote.
[2318.50s -> 2327.38s]  Okay, I'm going to keep it moving,
[2327.38s -> 2331.58s]  sorry for keeping the discussions a little shorter
[2331.58s -> 2332.38s]  because we did a good QA.
[2333.20s -> 2336.76s]  So who thinks this is a great application to be running
[2336.82s -> 2338.80s]  on machines that we've talked about?
[2338.80s -> 2340.52s]  Like for example, what are some properties of it
[2340.62s -> 2341.90s]  that make us think hey,
[2342.04s -> 2343.62s]  maybe this thing is going to run super fast?
[2344.58s -> 2344.70s]  Yeah?
[2345.22s -> 2349.96s]  Okay, so first of all, there's infinite parallelism.
[2349.96s -> 2351.80s]  Let's say that there's a million element arrays
[2351.80s -> 2352.92s]  or 10 million element arrays.
[2353.28s -> 2355.38s]  And not only is there parallelism,
[2355.44s -> 2357.82s]  there is vectorizable parallelism, right?
[2357.82s -> 2358.86s]  Because like this is going to map
[2358.86s -> 2361.42s]  to these SIMD operations really, really easily.
[2362.06s -> 2366.68s]  So we have way more parallelism than cores.
[2367.52s -> 2368.98s]  We have vector parallelism.
[2369.28s -> 2371.32s]  So not only are we going to use all the ALUs,
[2371.32s -> 2374.14s]  we can hide any potentially latency that occurs
[2374.14s -> 2375.70s]  when making this program.
[2376.26s -> 2380.84s]  All right, so here's where we get to the next topic.
[2380.98s -> 2383.58s]  This is probably the worst program that you can run
[2384.26s -> 2386.42s]  or it's like the worst parallel program you can
[2386.42s -> 2388.28s]  possibly run on any modern computer.
[2388.72s -> 2391.14s]  And you guys probably all run this program 100 times a day.
[2392.60s -> 2394.14s]  Okay, so let's just think about this a little bit.
[2394.14s -> 2395.80s]  I'm going to go ahead and take that NVIDIA GPU,
[2395.80s -> 2397.20s]  but think about myth or whatever.
[2398.00s -> 2401.22s]  I have, yeah, in a V100 I have 80 of these like sort
[2401.22s -> 2408.60s]  of SM blocks, and every block in it has 64 FP32 ALUs.
[2408.60s -> 2410.56s]  So I'm going to multiply those two numbers.
[2410.92s -> 2413.42s]  I have 5,000 multipliers in the chip.
[2414.14s -> 2415.22s]  And at first you're like, that's okay.
[2415.22s -> 2417.98s]  I have like millions of things to do, like no problem.
[2419.32s -> 2423.76s]  And this thing runs at about 1.6 gigahertz.
[2424.32s -> 2427.90s]  So if you just do the math, that's a lot
[2427.90s -> 2428.74s]  of data you're going to need.
[2428.74s -> 2432.46s]  And this gets to something we haven't talked about yet
[2432.46s -> 2435.46s]  in class, which is we've talked about latency,
[2436.38s -> 2438.34s]  but we haven't talked about bandwidth.
[2439.22s -> 2444.14s]  All right, how many people have lived up in San Francisco?
[2446.14s -> 2447.12s]  How many people live down here now?
[2448.96s -> 2450.90s]  How many people have lived up in San Francisco
[2450.90s -> 2452.30s]  and said, I'm going to live down here now?
[2455.02s -> 2457.12s]  How many people said, I wish I lived up in San Francisco?
[2457.44s -> 2460.40s]  Wow, very unopinionated people in this classroom.
[2460.40s -> 2464.96s]  All right, so I've got a lot of friends
[2464.96s -> 2469.06s]  that are a little tired of the fog in San Francisco.
[2469.06s -> 2472.72s]  So that's them on 101 driving down south back to Palo Alto,
[2473.10s -> 2475.80s]  because it looks like this up there during the summer,
[2476.44s -> 2477.66s]  and it looks like this down here.
[2478.46s -> 2479.62s]  And I like wearing T-shirts.
[2480.14s -> 2481.66s]  So let's say that everybody wants to get back
[2481.80s -> 2484.08s]  to the south thing, so we got 101.
[2484.96s -> 2487.60s]  And we got, let's just say that the distance between Stanford
[2487.60s -> 2489.24s]  and San Francisco, to keep our math easy,
[2489.24s -> 2490.38s]  is about 50 kilometers.
[2490.96s -> 2492.62s]  It's not, but, you know, math is.
[2493.04s -> 2496.60s]  And let's say we all drive on 101 at 100 kilometers per hour.
[2497.70s -> 2501.04s]  So, simple question, how long does it take to get to Stanford?
[2501.78s -> 2503.34s]  It takes half an hour.
[2504.30s -> 2507.18s]  That's the latency of when you leave San Francisco
[2507.18s -> 2508.50s]  and you get to Stanford, half an hour.
[2509.02s -> 2511.02s]  So the latency of driving from San Francisco
[2511.02s -> 2512.52s]  to Stanford, one half hour.
[2512.52s -> 2516.84s]  Now, let's just say there's a simple rule right now,
[2516.96s -> 2520.08s]  because we like to be safe, stay out of the way
[2520.08s -> 2524.32s]  of these AVs, and we're going to say there's only one car
[2524.32s -> 2525.64s]  that can be on the highway at once.
[2527.36s -> 2527.86s]  Very safe.
[2529.28s -> 2531.46s]  How many cars per hour get to Stanford?
[2532.34s -> 2536.26s]  Two cars per hour, right, because like one car drives
[2536.26s -> 2539.08s]  to Stanford, that's 30 minutes, and then 30 minutes later,
[2539.08s -> 2541.12s]  another car comes in, so we get two cars per hour.
[2541.72s -> 2545.60s]  So the latency of driving a car is a half hour, but the rate,
[2545.72s -> 2547.84s]  the throughput is two cars per hour.
[2549.48s -> 2555.34s]  Okay. So, one way, let's say I didn't change the rules at all
[2555.34s -> 2556.62s]  of how many cars can be on the road,
[2557.06s -> 2558.98s]  how do I increase the throughput?
[2559.84s -> 2562.38s]  Can't change the road, can't change the rules
[2562.38s -> 2567.58s]  on how many cars per, sorry?
[2568.04s -> 2570.76s]  Yeah, I can't change the road yet.
[2571.42s -> 2573.06s]  We can drive faster, right?
[2573.32s -> 2577.20s]  So let's say we drive two times faster, right?
[2577.20s -> 2578.52s]  So my throughput goes to what?
[2579.16s -> 2583.14s]  Four, my latency goes to 15 minutes, right?
[2583.14s -> 2584.58s]  Because I'm driving faster, right?
[2584.64s -> 2587.16s]  So, approach one is drive faster.
[2587.56s -> 2590.28s]  Now, let's say you can start, now notice
[2590.28s -> 2593.96s]  that driving faster reduced latency and increased throughput.
[2595.68s -> 2597.66s]  But driving faster has some limits, you know,
[2597.66s -> 2600.50s]  maybe it's dangerous, it's actually really inefficient
[2600.66s -> 2602.88s]  because you get more wind drag and stuff like that.
[2602.88s -> 2604.88s]  So, there's a lot of reasons why you can't just keep
[2604.88s -> 2605.72s]  driving faster.
[2607.46s -> 2609.84s]  Like we can't say that if we wanted to get everybody
[2609.84s -> 2612.06s]  down to San Francisco, we want a thousand cars per hour,
[2612.06s -> 2614.50s]  everybody's going to drive at like 10,000 miles per hour.
[2616.34s -> 2617.68s]  So what happens if you can change the road?
[2620.94s -> 2622.20s]  They do this all the time actually.
[2624.42s -> 2626.62s]  So what does California do on 101 about every five years?
[2627.36s -> 2629.54s]  They build more lanes.
[2630.74s -> 2635.02s]  So now, let's just say I'm back to my 5100 kilometers per hour,
[2635.42s -> 2637.94s]  my throughput is eight cars per hour
[2637.94s -> 2639.24s]  because I get two cars per lane.
[2640.18s -> 2643.88s]  So widening the road means I have increased throughput
[2644.64s -> 2646.60s]  but latency was the same as it was before.
[2646.72s -> 2649.32s]  Latency is still 30 minutes, right?
[2651.34s -> 2652.04s]  How can we do better?
[2653.08s -> 2654.72s]  Let's say we could change the rules of the road
[2654.72s -> 2656.14s]  but we're not going to change the road anymore,
[2656.14s -> 2657.46s]  that's as big as we can get it, you know,
[2657.66s -> 2659.82s]  it's falling off into the bay on one side.
[2660.50s -> 2660.60s]  Yeah?
[2662.60s -> 2666.98s]  So now I can start using the road more efficiently.
[2667.06s -> 2669.82s]  Like I can think about the road as not one big thing,
[2669.82s -> 2671.10s]  I can think about it as a bunch of pieces.
[2671.10s -> 2674.62s]  And it's pretty clear that two cars can't be in the same part
[2674.62s -> 2677.06s]  of the road at the same time, that's not allowed.
[2677.78s -> 2679.98s]  But we can definitely divvy up the road
[2679.98s -> 2682.22s]  and drive bumper to bumper or let's just say for the sake
[2682.22s -> 2686.38s]  of math, I space my cars out by one kilometer, right?
[2687.26s -> 2690.52s]  Okay. So cars are spaced out by one kilometer
[2690.74s -> 2692.88s]  which actually means one car is arriving
[2692.88s -> 2697.68s]  at Stanford every 30 seconds.
[2697.68s -> 2699.32s]  And what's my total rate of cars per hour?
[2699.88s -> 2703.08s]  It would be 100 cars per hour.
[2703.56s -> 2707.02s]  If I got my math right.
[2707.44s -> 2708.98s]  Yeah, yeah, one car every 100th of an hour.
[2709.04s -> 2709.78s]  That should be right.
[2709.94s -> 2710.44s]  I think that's right.
[2711.22s -> 2713.88s]  Doubting myself for a second but yeah, okay.
[2714.46s -> 2718.36s]  Okay. So this idea of using the road more efficiently,
[2718.36s -> 2720.88s]  I can say well, getting to San Francisco is
[2720.88s -> 2728.08s]  like taking these 50 steps and I get through every one
[2728.08s -> 2730.64s]  of these steps like one, you know,
[2730.64s -> 2735.64s]  once per every 100th of an hour, every 30 seconds.
[2738.56s -> 2741.80s]  No, it's every 120, oh yeah, I totally messed that math up.
[2742.58s -> 2743.64s]  Yeah, every 36 seconds.
[2744.10s -> 2746.86s]  I should have just said that if we space out by,
[2746.86s -> 2748.56s]  no this is wrong, no that's right.
[2749.04s -> 2750.66s]  If I space out by one kilometer
[2751.12s -> 2754.66s]  and I'm going 100 kilometers per hour, I am taking one
[2754.66s -> 2759.54s]  of those steps every 100th of an hour
[2759.54s -> 2761.04s]  and now that's, yeah, that's wrong.
[2762.40s -> 2763.96s]  Okay, I'll fix that later but you get the point.
[2765.74s -> 2769.36s]  Okay. So building upon that other math,
[2769.86s -> 2774.48s]  if I have four lanes then I just get quadruple
[2774.48s -> 2775.60s]  of throughput, right?
[2779.90s -> 2782.28s]  So let's get away from Highway 101 for a second
[2782.28s -> 2784.84s]  and let's just talk about communication in a system.
[2785.56s -> 2789.96s]  So I can talk about memory latency and I can talk
[2789.96s -> 2791.06s]  about memory bandwidth.
[2791.54s -> 2793.14s]  Memory bandwidth is a rate,
[2793.14s -> 2796.70s]  how many things are completed per unit time.
[2797.38s -> 2799.28s]  So here's an example where I'm completing,
[2799.28s -> 2803.42s]  I'm sending four squares per second and I know that's true
[2803.42s -> 2805.30s]  because I animate it, four items per second.
[2806.66s -> 2808.66s]  Now the latency of any one of those items is
[2808.66s -> 2810.74s]  about two seconds, the amount of time it gets
[2810.74s -> 2812.00s]  from memory to the processor.
[2812.56s -> 2816.04s]  I could increase the bandwidth
[2817.60s -> 2819.46s]  by transferring two things at a time.
[2820.34s -> 2821.92s]  The latency is still two seconds
[2822.30s -> 2824.52s]  but now the bandwidth is eight items per second.
[2824.52s -> 2830.24s]  So in a system where we're driving back to back
[2830.38s -> 2832.64s]  or we're sending multiple things at a time,
[2833.62s -> 2835.94s]  which I'll introduce the term pipeline in a second,
[2836.46s -> 2838.10s]  latency and bandwidth are decoupled.
[2839.14s -> 2840.76s]  Latency is about how long it takes to get
[2840.76s -> 2841.84s]  from memory to the core.
[2842.26s -> 2844.16s]  Bandwidth is actually going to be a function
[2844.16s -> 2845.22s]  of how wide your road is
[2846.08s -> 2849.52s]  if you assume everything is bumper to bumper like this.
[2850.24s -> 2853.04s]  Okay. You know another example that we do all the time
[2853.04s -> 2855.04s]  that I like to use in this class is doing your laundry.
[2855.70s -> 2859.06s]  Imagine you have laundry and you have a washer, a dryer
[2859.32s -> 2862.02s]  and yourself because there's three stages
[2862.02s -> 2862.98s]  to get your laundry done.
[2862.98s -> 2866.18s]  So I divvied up the road into a hundred stages,
[2866.58s -> 2868.64s]  now I'm divvied up laundry into three stages.
[2869.00s -> 2871.38s]  You wash, you dry and you fold.
[2872.68s -> 2873.78s]  So if you had one load
[2873.78s -> 2876.32s]  of laundry how long would it take you to finish?
[2876.74s -> 2879.04s]  Bottom of the slide, two hours, right?
[2879.04s -> 2882.08s]  Because it takes 45 minutes plus 60 plus 15
[2882.08s -> 2882.88s]  to fold your clothes.
[2883.56s -> 2890.38s]  Now what's the throughput here?
[2891.74s -> 2894.06s]  Or let's say I wanted you to increase the throughput.
[2894.88s -> 2897.28s]  So if we just did one load of laundry and we did it kind
[2897.28s -> 2899.48s]  of without ever sharing the road we're going to, you know,
[2899.48s -> 2900.96s]  it takes two hours to do laundry
[2901.04s -> 2903.66s]  so my throughput is one load every two hours,
[2903.88s -> 2904.88s]  half a load per hour.
[2906.00s -> 2907.92s]  So if I said let's double the throughput,
[2908.64s -> 2912.78s]  well you might just go down and hope for there's two washers,
[2912.78s -> 2915.76s]  two dryers, you call your best friend and you're like look,
[2916.48s -> 2919.18s]  still takes two hours to get my load done
[2920.36s -> 2924.66s]  but I get two loads done in two hours which makes sense
[2924.66s -> 2926.70s]  because none of us ever do one load of laundry, right?
[2926.70s -> 2928.50s]  Like we just kind of wait until it all piles up
[2928.50s -> 2930.62s]  and you have several loads of laundry to do at once.
[2931.62s -> 2933.84s]  Now is this a particularly efficient way
[2935.24s -> 2936.00s]  to get the job done?
[2938.00s -> 2940.26s]  It actually kind of is if you're optimizing for latency
[2940.26s -> 2942.68s]  because I get two loads done in two hours.
[2943.16s -> 2947.84s]  But there's a lot of resources that go idle, right?
[2948.16s -> 2953.00s]  So how would you probably do this if you showed
[2953.00s -> 2955.50s]  up downstairs or in the basement somewhere
[2955.50s -> 2957.02s]  and you had a bunch of loads of laundry?
[2957.02s -> 2958.48s]  Let's say you had a ton of loads of laundry.
[2959.42s -> 2961.38s]  Say like we had like 20 loads of laundry to do
[2961.38s -> 2963.46s]  because like we waited all summer or something like that.
[2964.14s -> 2964.26s]  Yeah?
[2964.26s -> 2968.54s]  So when the washer is done, you put the washer
[2968.54s -> 2972.32s]  in the next load and you keep on doing that.
[2972.32s -> 2974.78s]  Because the idea here is you don't ever want anything
[2974.78s -> 2975.30s]  to go idle.
[2975.56s -> 2977.62s]  That's just the name of the game in throughput computing.
[2977.62s -> 2980.78s]  If anything goes idle, you lost an opportunity to make progress.
[2981.30s -> 2983.48s]  So we don't want empty places on the road.
[2983.66s -> 2987.40s]  We don't want threads waiting to run and we also don't want
[2987.40s -> 2989.18s]  to wash it or dry it or not doing anything.
[2989.86s -> 2993.82s]  So now I'm drawing this diagram of what's going on here
[2994.10s -> 2997.76s]  and so look at the, you know, the x-axis is time and notice
[2997.82s -> 3002.66s]  that my first load, my laundry takes, the wash is, you know,
[3002.66s -> 3004.04s]  three ticks, 45 minutes.
[3004.72s -> 3006.86s]  Then there's four ticks for an hour and then 15 minutes.
[3006.86s -> 3011.14s]  So the latency of the first load is two hours.
[3012.14s -> 3013.72s]  So when can I start putting in the second load?
[3013.72s -> 3015.48s]  Let's just say we only had one washer and one dryer.
[3017.08s -> 3018.42s]  I put that second load in when?
[3019.18s -> 3021.82s]  Sorry, how long after the start?
[3021.82s -> 3024.78s]  45 seconds, or sorry, 45 minutes.
[3025.20s -> 3027.48s]  So at 45 minutes I can start the washer.
[3027.94s -> 3029.92s]  It gets done an hour and a half in
[3030.56s -> 3031.52s]  and then what happens here?
[3034.86s -> 3036.88s]  Yeah, like the laundry just sits in the washer
[3036.88s -> 3038.18s]  because the dryer is not done.
[3038.66s -> 3046.22s]  And let's just say that, you know, like at this point,
[3046.78s -> 3048.18s]  I will go ahead and start the washer.
[3048.18s -> 3050.40s]  Let's say actually I take the laundry out of the washer,
[3050.44s -> 3051.80s]  put it on top of the dryer
[3053.18s -> 3055.00s]  and start the washer again immediately
[3055.00s -> 3055.82s]  when the washer is done.
[3056.82s -> 3057.42s]  Does that make sense?
[3058.14s -> 3058.86s]  That's what we all do anyways.
[3061.08s -> 3063.30s]  And so what's going on in this diagram?
[3063.30s -> 3064.50s]  I have two questions for you.
[3064.56s -> 3068.34s]  Well, my question is what is the throughput
[3068.40s -> 3069.70s]  at which we are finishing laundry?
[3071.58s -> 3074.42s]  We can do a load of wash every 45 seconds.
[3075.44s -> 3078.88s]  We can do a load of, we can do a dryer load every one hour.
[3079.30s -> 3080.94s]  What is the throughput of loads of laundry?
[3084.40s -> 3085.24s]  One per hour.
[3086.76s -> 3088.40s]  But wait a minute, I thought we could do wash
[3088.40s -> 3090.04s]  at one every 45 minutes.
[3095.40s -> 3095.98s]  What's going on?
[3095.98s -> 3096.12s]  Yeah.
[3096.70s -> 3101.60s]  I'm limited by the slowest link in the chain
[3101.60s -> 3102.52s]  in the pipeline, right?
[3102.58s -> 3104.40s]  So if the dryer can only get done
[3104.40s -> 3106.80s]  with one load per hour, that's my throughput.
[3106.80s -> 3114.82s]  Now, why is the laundry here is actually sort of a,
[3114.82s -> 3117.22s]  I'm executing a load of laundry every 45 minutes.
[3118.42s -> 3119.92s]  So what's actually happening in this scenario?
[3122.96s -> 3125.02s]  Where's all this laundry, all this wet laundry going?
[3127.48s -> 3129.62s]  It's actually building up on top of the washer
[3129.62s -> 3130.48s]  or the dryer, right?
[3130.48s -> 3134.02s]  Because at this point I've only completed like one, two,
[3134.02s -> 3137.94s]  three, four loads of drying and we have more
[3137.94s -> 3138.66s]  than that sitting around.
[3138.66s -> 3141.30s]  So if we ran this for forever, this pile,
[3141.30s -> 3143.58s]  if I just kept shoving laundry into the washer
[3143.58s -> 3145.96s]  as soon as the washer was idle, this pile
[3145.96s -> 3148.52s]  between the two would just keep growing, right?
[3149.02s -> 3151.32s]  So at some point I might say this is ridiculous.
[3151.32s -> 3153.66s]  I'm not going to put new stuff in the washer
[3154.22s -> 3156.46s]  until I take another load out of the dryer
[3156.72s -> 3157.82s]  and can drain that pile.
[3158.30s -> 3160.48s]  So at some point there's going to be a finite buffer
[3160.88s -> 3164.32s]  and the fast thing is going to slow down to match the rate
[3164.32s -> 3165.78s]  of the slow thing, right?
[3166.18s -> 3168.90s]  So here we have a latency of one load takes two hours,
[3169.48s -> 3171.86s]  throughput is now one load per hour and we did
[3171.86s -> 3174.60s]  that with resources of only one washer and one dryer,
[3174.60s -> 3175.80s]  not two washers and dryer.
[3177.54s -> 3179.48s]  Now, we needed multiple loads of laundry to do this.
[3180.48s -> 3182.98s]  You know, another analogy here is imagine you had two pipes
[3183.52s -> 3185.30s]  and if we ignore the actual physics reason
[3185.30s -> 3188.06s]  that if I push more water down one pipe it speeds up
[3188.06s -> 3188.98s]  and they make rate match.
[3188.98s -> 3192.50s]  But let's say I had one pipe that could do 100 liters a second
[3192.66s -> 3194.82s]  before it burst and another pipe
[3194.82s -> 3197.14s]  that could do 50 liters per second before it burst
[3197.14s -> 3200.54s]  and I connect them together I'm only getting 50 liters per
[3200.54s -> 3202.42s]  second out of the system, right?
[3202.76s -> 3205.20s]  I'm going to run at the lower of the throughputs
[3205.20s -> 3206.32s]  when I link things together.
[3206.82s -> 3209.28s]  All right.
[3210.40s -> 3213.06s]  So let's apply this concept to a computer
[3214.74s -> 3216.54s]  and let's say we go back to, you know,
[3216.54s -> 3220.06s]  we go to a simple program that's like load 64 bytes and then add,
[3220.06s -> 3224.50s]  add, load, add, add, load, add, add, load, add, add, okay?
[3225.00s -> 3225.88s]  And let's just say
[3225.88s -> 3229.52s]  that the computer is executing one math operation per clock.
[3230.70s -> 3234.98s]  Something that's actually quite common is we haven't talked
[3234.98s -> 3237.84s]  about executing loads in stores but, you know,
[3237.84s -> 3239.60s]  I'll draw it this time and we'll ignore it the rest
[3239.60s -> 3241.46s]  of the class but often it's the case
[3241.46s -> 3244.32s]  that there's an execution unit to handle your load store that's
[3244.32s -> 3245.72s]  in parallel from your math units.
[3246.32s -> 3248.68s]  Like you don't want to waste a clock of math to do a load
[3248.68s -> 3252.16s]  in store so think about it as a two-way super scaler
[3252.16s -> 3256.06s]  and we can get 8 bytes per clock from memory, okay?
[3256.06s -> 3258.80s]  So memory has a long, has some latency
[3259.50s -> 3262.66s]  and then there's 8 bytes per clock.
[3263.04s -> 3264.52s]  So let's think about how this looks.
[3264.52s -> 3266.74s]  It's like my laundry diagram basically, right?
[3267.28s -> 3270.72s]  So here are the instructions and here's time.
[3270.92s -> 3273.80s]  So in other words here's loads of laundry and here's time.
[3274.62s -> 3277.70s]  So let's say I start by doing two math operations
[3277.70s -> 3281.40s]  and I kick off that load and then there's some latency
[3281.40s -> 3289.20s]  and then this blue bar are the 8 cycles it takes to get back a
[3289.36s -> 3292.42s]  total of 8 bytes per clock.
[3294.56s -> 3296.86s]  Oh, sorry, and then the other thing is actually,
[3296.86s -> 3297.86s]  remember we talked about caches?
[3298.84s -> 3301.18s]  Typically we ask memory for an entire cache line.
[3302.00s -> 3305.10s]  So when I say load I really mean like load a cache line.
[3306.10s -> 3310.00s]  So let's just say it takes 8 cycles to get that information
[3310.06s -> 3314.36s]  over the highway, over highway 101 back to my processor.
[3315.18s -> 3317.24s]  And so we're just going to keep doing this and notice
[3317.28s -> 3321.62s]  that at every point in time the blue bar is working.
[3321.62s -> 3324.02s]  As soon as memory gets done with the first load,
[3324.02s -> 3325.50s]  it starts working on the second load.
[3326.18s -> 3327.62s]  So there's always something working.
[3328.46s -> 3333.02s]  Now I can, if I'm doing load, sorry, like math, math, load,
[3333.02s -> 3334.58s]  math, math, load, that's like the washer.
[3334.58s -> 3340.06s]  It's going really, really fast and memory is not keeping
[3340.06s -> 3341.70s]  up because memory is the long pole.
[3341.70s -> 3345.54s]  It's like the dryer here and at some point there's
[3345.54s -> 3348.42s]  so many requests out to memory the processor is
[3348.42s -> 3349.82s]  like I've filled a buffer.
[3349.82s -> 3351.48s]  I've got to stop, okay?
[3351.78s -> 3353.10s]  So in this diagram I'm only going
[3353.10s -> 3355.08s]  to allow three outstanding loads.
[3355.08s -> 3356.08s]  I just chose that number.
[3356.56s -> 3360.18s]  So now since the processor can't issue any more loads,
[3360.18s -> 3363.52s]  the next time it tries to do a load, it's just got to wait.
[3364.96s -> 3367.60s]  It's got to wait until memory gets done with something
[3367.82s -> 3370.78s]  in order to shove that next thing in the queue, okay?
[3370.78s -> 3372.08s]  And that's just going to continue.
[3372.08s -> 3374.50s]  It's going to be, so when this thing gets back,
[3375.04s -> 3376.80s]  the processor can make progress again.
[3376.80s -> 3379.64s]  Oh, I got my road back, now I can keep going, okay?
[3380.08s -> 3383.02s]  And if we zoom out of this diagram and look what happens,
[3384.08s -> 3385.20s]  this is what it will look like.
[3386.28s -> 3389.64s]  Math, math, load.
[3390.44s -> 3394.02s]  Can't even issue my load because like the request queue
[3394.02s -> 3394.92s]  is full to memory.
[3395.54s -> 3398.02s]  Oh, one came back, okay, now I can trickle another one in.
[3398.02s -> 3399.30s]  Oh, one came back later.
[3399.30s -> 3400.48s]  Oh, I can trickle another one in.
[3400.88s -> 3402.08s]  So look at this diagram.
[3402.30s -> 3405.76s]  At every point in time memory is busy transferring data
[3405.76s -> 3408.68s]  at the fastest rate it can, eight bytes per clock.
[3409.50s -> 3411.72s]  But the processor is not busy most of the time.
[3412.92s -> 3415.96s]  All of these sections are when the processor is stalled.
[3416.66s -> 3420.66s]  So you can think about the processor
[3420.66s -> 3425.72s]  like these 5,000 ALUs, to do their work, they like need,
[3425.72s -> 3427.62s]  like if they're going to do a math op every clock,
[3427.62s -> 3429.50s]  they need data for that math op every clock.
[3430.64s -> 3432.80s]  So they're asking for all of this data.
[3433.92s -> 3436.42s]  And even if we build like the fanciest memory system
[3436.42s -> 3439.20s]  on the planet, like this modern Nvidia memory system
[3439.20s -> 3441.74s]  that has 900 gigabytes of bandwidth, it's insane.
[3442.40s -> 3448.76s]  If you do the math here, this thing needs 100 terabytes
[3448.76s -> 3453.98s]  of bandwidth in order to give the ALUs on this chip the data
[3453.98s -> 3456.96s]  they would need to actually do a math operation every clock.
[3457.74s -> 3458.68s]  Because think about it this way,
[3458.80s -> 3463.52s]  we can do 5,000 math operations at 1.6 gigahertz.
[3463.52s -> 3469.02s]  So I can do about 8 trillion math operations per second.
[3470.00s -> 3473.16s]  Every one of those math operations needs 12 bytes,
[3473.16s -> 3475.96s]  right, because it loads two arguments and stores one.
[3476.50s -> 3480.20s]  So if I can do 8 trillion operations per second
[3480.20s -> 3482.82s]  on this chip, I'm going to need to feed it
[3482.82s -> 3485.54s]  with about 100 terabytes of bandwidth.
[3486.70s -> 3488.36s]  And the fastest memory system kind
[3488.36s -> 3490.80s]  of on the planet is about one.
[3492.96s -> 3495.60s]  So you've got a pipe, the math pipe runs
[3495.60s -> 3498.88s]  at 100 times the data pipe.
[3499.52s -> 3502.60s]  So if you just think through this pipe analogy,
[3502.60s -> 3504.82s]  it means if you run this code on a modern GPU,
[3504.82s -> 3506.98s]  and it's very similar if you run it on a modern CPU,
[3506.98s -> 3509.76s]  just kind of scaled down in numbers, this thing is going
[3509.76s -> 3512.12s]  to run at approximately 1% efficiency.
[3514.72s -> 3517.80s]  Standard numpy add array to add array.
[3519.56s -> 3522.74s]  And if that's the only thing you're doing,
[3522.74s -> 3526.32s]  there's nothing you can do about that.
[3526.32s -> 3528.08s]  Convince yourself that a cache won't help.
[3529.40s -> 3531.24s]  Caches only help if you reuse data.
[3533.02s -> 3534.98s]  I'm accessing all the data once.
[3535.96s -> 3538.18s]  I'm loading it perfectly through cache lines.
[3538.18s -> 3540.16s]  So every time I load a cache line, I read all the data.
[3541.98s -> 3542.80s]  There's nothing you can do.
[3543.38s -> 3545.04s]  The only thing you can do is change your program
[3545.04s -> 3547.78s]  so that it doesn't do 12 bytes of memory traffic
[3547.78s -> 3548.98s]  for every math op it does.
[3549.44s -> 3549.58s]  Yeah.
[3550.32s -> 3556.28s]  This is a really important question.
[3556.28s -> 3558.38s]  You said couldn't we set the prefetching?
[3559.90s -> 3562.34s]  Imagine if I had a magical memory system
[3562.34s -> 3566.54s]  that had zero latency, but transferred data
[3566.54s -> 3567.74s]  at 8 bytes per clock.
[3568.58s -> 3569.42s]  Perfect prefetcher.
[3570.02s -> 3572.32s]  It doesn't matter.
[3573.70s -> 3574.88s]  Convince yourself that's true.
[3574.88s -> 3576.72s]  This is not a memory latency problem.
[3577.80s -> 3580.56s]  This is I'm trying to show, I'm trying to extract water
[3580.56s -> 3585.68s]  from a pipe at 100 terabytes per second in order
[3585.68s -> 3588.90s]  to feed my hungry ALUs, and the best pipe
[3588.90s -> 3591.08s]  in the world can give me 1 terabyte per second of bandwidth.
[3592.48s -> 3593.54s]  That's the problem that we have.
[3594.38s -> 3596.62s]  Okay. There's nothing you can do other
[3596.62s -> 3600.14s]  than change your program or wait
[3600.14s -> 3601.74s]  until a new memory system comes up.
[3602.56s -> 3605.86s]  So in almost every program you write
[3605.86s -> 3607.66s]  on these modern processors, the name
[3607.66s -> 3611.02s]  of the game is manipulate this ratio
[3611.02s -> 3613.62s]  of how many math operations you do for every memory access,
[3613.72s -> 3614.92s]  every memory access you read.
[3615.66s -> 3618.08s]  Every program you write will be bottlenecked by memory,
[3618.08s -> 3623.48s]  regardless of parallelism, unless you do this, okay?
[3623.70s -> 3625.20s]  One quick aside, a lot of students say,
[3625.20s -> 3627.40s]  how can I do a floating point operation per clock?
[3628.02s -> 3630.16s]  The same idea of pipelining applies
[3630.22s -> 3631.46s]  to an instruction pipeline.
[3631.46s -> 3635.46s]  So when I say my ALUs can do one operation per clock,
[3636.32s -> 3639.72s]  I'm actually saying they can complete one operation per clock.
[3640.36s -> 3642.60s]  Their latency might be higher.
[3643.24s -> 3645.84s]  So here's an example where the latency of an instruction,
[3646.28s -> 3649.38s]  maybe it takes one clock to fetch, get ready to execute,
[3649.38s -> 3651.54s]  execute, and put the results back in the register.
[3652.04s -> 3653.70s]  The same idea of, you know, again,
[3653.70s -> 3655.36s]  it's like the Highway 101 analogy.
[3655.68s -> 3657.24s]  When I'm talking to you about a processor
[3657.24s -> 3658.80s]  that does one instruction per clock,
[3659.22s -> 3662.94s]  I really mean completes one instruction per clock,
[3662.94s -> 3664.96s]  has a throughput of one instruction per clock.
[3665.52s -> 3667.66s]  Because any latencies and stuff are all going to kind
[3667.66s -> 3668.74s]  of be hidden under the hood
[3668.74s -> 3670.54s]  with smart scheduling and stuff like that.
[3670.54s -> 3673.58s]  So in this class, I almost always talk in terms of throughput.
[3675.90s -> 3680.14s]  Okay. Let's take a big, deep breath and any questions?
[3686.32s -> 3686.44s]  Yeah.
[3687.02s -> 3694.28s]  Bandwidth is a measure of throughput.
[3695.24s -> 3697.12s]  So usually when I say bandwidth,
[3697.12s -> 3698.42s]  I mean bytes per clock.
[3699.10s -> 3700.64s]  I could have throughput in office hours
[3700.64s -> 3702.74s]  of students per clock, right.
[3703.10s -> 3705.32s]  Just a measure of something per unit time, yes.
[3706.00s -> 3714.58s]  In general, the rate of processing increases faster
[3714.58s -> 3718.66s]  than the rate of data movement for reasons
[3718.66s -> 3720.18s]  that moving data is expensive.
[3720.94s -> 3722.32s]  Like moving anything is expensive.
[3722.76s -> 3725.02s]  So usually about every, you know, decade or so,
[3725.02s -> 3728.54s]  there's some big technology bump that changes the equation.
[3728.60s -> 3731.42s]  But it typically is like one big catch up of bandwidth
[3731.42s -> 3733.30s]  and then compute races away and then
[3733.30s -> 3734.50s]  at some point things get dire
[3734.50s -> 3736.12s]  and there's another big catch up in bandwidth.
[3736.92s -> 3739.82s]  So for like limited source, but bandwidth
[3739.82s -> 3743.28s]  and like why make a 40 to 90 unit?
[3743.28s -> 3747.68s]  That chip is not designed to be efficient to add two vectors.
[3748.62s -> 3751.34s]  If the only workload you were doing was to add two vectors,
[3751.60s -> 3753.36s]  you would take that memory system and you would rip
[3753.36s -> 3754.80s]  out 99% of the ALUs.
[3755.92s -> 3757.90s]  Luckily, things like computer graphics
[3758.54s -> 3762.98s]  and machine learning do not have this ratio
[3762.98s -> 3764.96s]  of one math operation to 12 bytes read.
[3765.46s -> 3767.96s]  It's flipped around the other direction, you know,
[3767.96s -> 3771.12s]  like on the order of 10 to 20 operations per byte read
[3771.46s -> 3773.86s]  and you start getting very close to that ratio
[3773.86s -> 3774.74s]  of compute to bandwidth.
[3775.28s -> 3776.98s]  So as a modern computer architect,
[3776.98s -> 3778.34s]  the first thing you do is you kind
[3778.34s -> 3780.50s]  of say how much bandwidth can I afford?
[3781.44s -> 3783.54s]  And given how much bandwidth can I afford,
[3783.94s -> 3786.56s]  you say how many ALUs can I put in there to keep
[3786.66s -> 3789.96s]  that memory system busy given my applications.
[3789.96s -> 3796.72s]  Okay. Yeah, we're running a little behind, but that's okay.
[3796.72s -> 3798.46s]  I think Thursday's lecture is a little shorter,
[3798.46s -> 3799.30s]  so we'll be fine.
[3799.76s -> 3803.18s]  Okay. So I want to talk a little bit about ISPC.
[3803.18s -> 3806.46s]  Now in this class by design, you are one
[3806.46s -> 3808.76s]  of the skills is go read the manual,
[3808.76s -> 3810.00s]  go figure it out for yourself.
[3810.04s -> 3811.54s]  That is sort of by design
[3811.54s -> 3813.12s]  because that's what's going to happen when you go to work.
[3813.86s -> 3816.34s]  But I do want to say a little bit about ISPC to help
[3816.34s -> 3819.66s]  out a little bit because it's such a great example
[3819.66s -> 3822.54s]  in this class many times when you come to office hours,
[3823.02s -> 3825.50s]  I'm going to say I think you're conflating,
[3825.50s -> 3830.24s]  you're confusing what the abstractions of a program mean,
[3831.00s -> 3832.62s]  which is like when I read the code,
[3832.62s -> 3834.44s]  what should the output answer be?
[3834.44s -> 3837.12s]  What should it compute from?
[3837.70s -> 3841.92s]  Or I'm confusing that with some implementation.
[3841.92s -> 3843.02s]  How is it computed?
[3843.02s -> 3844.70s]  And usually in a parallel computing class,
[3845.10s -> 3846.50s]  what order is it computed?
[3847.32s -> 3848.50s]  What is done in parallel?
[3848.50s -> 3849.00s]  What is not?
[3849.92s -> 3852.76s]  So a great example of like what you should always be asking
[3852.76s -> 3854.82s]  yourself is what is the answer?
[3854.98s -> 3857.08s]  Like, you know, what's the correct answer
[3857.08s -> 3858.44s]  that this program will compute?
[3859.22s -> 3860.86s]  And then you could say, okay, if I have to think
[3860.86s -> 3862.92s]  about implementation, I should be asking myself,
[3863.50s -> 3866.90s]  what is every part of the machine, every core,
[3866.90s -> 3869.30s]  every thread, what is it doing at some time?
[3870.18s -> 3871.14s]  What work is it doing?
[3872.30s -> 3874.68s]  So ISPC, one of the reasons why I give you this sort
[3874.68s -> 3879.20s]  of fairly obscure simple programming language to learn is
[3879.20s -> 3881.52s]  that it just makes these concepts, in my opinion, very,
[3881.52s -> 3884.92s]  very clear because of how low level it is.
[3885.56s -> 3887.62s]  This is a language, it's like people are like,
[3887.62s -> 3891.50s]  I can't find anything on Reddit or on Stack Overflow
[3891.50s -> 3892.94s]  because like there's probably like, you know,
[3893.26s -> 3895.42s]  the number of users is all of you times the number
[3895.42s -> 3898.84s]  of years of this class plus like 100 or 200 other people
[3898.84s -> 3900.34s]  that want to get really good performance
[3900.34s -> 3901.22s]  out of Intel chips.
[3901.82s -> 3904.96s]  If you want to know why you get such great performance
[3904.96s -> 3907.22s]  out of ISPC and can't get it out of C++,
[3907.22s -> 3910.32s]  this is like an amazing blog post that underscores a lot
[3910.92s -> 3915.24s]  about why it's so hard to paralyze arbitrary C code
[3915.24s -> 3917.02s]  and why they invented another language.
[3918.84s -> 3920.56s]  Okay, so here's our program from last time.
[3920.56s -> 3922.44s]  Again, for every element in an array,
[3922.44s -> 3923.64s]  compute the sign of the number.
[3925.04s -> 3928.48s]  Now I'm going to rewrite the code in ISPC, okay?
[3928.78s -> 3933.22s]  So all I'm going to do is I'm going to take this code,
[3933.66s -> 3937.04s]  this is C code, you know, you can compile it with GCC,
[3938.00s -> 3941.76s]  here's the ISPC code, so here's void main which is C code,
[3941.76s -> 3944.20s]  this is all C code and then this is,
[3944.20s -> 3946.62s]  I just took the function and I did literally nothing
[3947.12s -> 3951.16s]  and now it's an ISPC function, okay?
[3953.98s -> 3958.42s]  Okay, oh sorry, sorry, I haven't rewritten an ISPC yet,
[3958.42s -> 3961.42s]  I just factored it out, excuse me, sorry, okay.
[3962.24s -> 3965.12s]  So if this is normal C code, excuse me, I got one slide,
[3965.98s -> 3968.40s]  if we're running main, there's one thread of control,
[3968.82s -> 3970.70s]  one thread is running main and then
[3970.70s -> 3975.02s]  when you call the C function sign X, that thread
[3975.02s -> 3976.94s]  of control just jumps to sign X,
[3977.54s -> 3979.66s]  executes the logic sequentially when it gets
[3979.66s -> 3982.10s]  to the return, it returns control
[3982.28s -> 3984.24s]  on that main thread here and continues on.
[3985.28s -> 3986.54s]  So it's just, you know, it's kind of how we think
[3986.54s -> 3989.00s]  about programming naturally in one thread of control.
[3989.76s -> 3991.90s]  Now I'm going to rewrite the right hand side
[3991.90s -> 3995.22s]  of the slide in ISPC, okay?
[3995.56s -> 3998.96s]  So not much is going to change, it's just really going
[3998.96s -> 4002.72s]  to change only in the outer for loop, okay?
[4003.12s -> 4005.56s]  So I've highlighted a bunch of stuff, I'd like you
[4005.56s -> 4009.60s]  to ignore the uniform keyword for now, I don't want to get
[4009.60s -> 4011.82s]  into that, just assume it's a regular int,
[4012.62s -> 4016.26s]  but look what happens here, so now I have a program
[4016.26s -> 4018.74s]  that has access to two new variables,
[4019.44s -> 4021.10s]  one is called program count
[4021.48s -> 4025.12s]  and the other is program instance, or index, excuse me.
[4025.38s -> 4031.00s]  Program index, well, so what you should think of semantics,
[4031.00s -> 4033.20s]  again, now the meaning, you should say, oh,
[4033.78s -> 4037.80s]  when my main thread in C gets to this ISPC function,
[4038.54s -> 4040.78s]  when it invokes the ISPC function,
[4041.60s -> 4044.34s]  my thread of control doesn't start executing that function,
[4045.08s -> 4049.86s]  ISPC spawns a gang of what they call program instances
[4050.78s -> 4053.72s]  and they're very clear not to use the word thread here
[4053.72s -> 4055.06s]  because you kind of have a notion
[4055.06s -> 4057.46s]  of what a thread actually is implemented
[4057.46s -> 4061.20s]  as in a real C program and here, they're just saying,
[4061.20s -> 4065.82s]  look, there are an entire gang of program instances
[4065.86s -> 4068.52s]  and the number of instances you have is given
[4068.52s -> 4073.66s]  by program count and the current instance
[4073.66s -> 4077.78s]  that is running is given by program instance, index, excuse me.
[4078.26s -> 4081.56s]  So in this case, every single instance
[4081.56s -> 4084.46s]  of the ISPC program has its own copy of all
[4084.46s -> 4089.42s]  of the local variables, like float, numerator, index
[4090.34s -> 4093.02s]  and it does whatever the function says it should do,
[4093.52s -> 4096.92s]  but that logic is conditioned on, maybe every instance,
[4096.92s -> 4099.62s]  we'll do something a little bit different based
[4099.62s -> 4102.06s]  on the value of program index, okay.
[4102.50s -> 4107.32s]  So let's say that I create, I say I configure ISPC that I want you
[4107.32s -> 4109.28s]  to create a gang of eight instances
[4109.28s -> 4110.78s]  when we call an ISPC function.
[4111.88s -> 4113.10s]  So it should look a little bit like this,
[4113.10s -> 4118.56s]  at the point of this call, eight program instances are created,
[4118.88s -> 4122.00s]  every program instance gets a different value,
[4122.00s -> 4126.26s]  I'm going to go back to the previous slide of program index,
[4127.00s -> 4131.20s]  every program instance runs this logic sequentially
[4131.78s -> 4134.42s]  to produce some output and then
[4134.42s -> 4136.58s]  when all program instances are done,
[4136.84s -> 4139.54s]  control returns to your regular C caller.
[4141.42s -> 4143.34s]  Okay, so program count,
[4143.34s -> 4146.50s]  the number of simultaneously executing instances,
[4147.68s -> 4149.72s]  program index, which one am I?
[4151.80s -> 4154.96s]  So this program model is kind of, you'll see this term SPMD,
[4154.96s -> 4158.84s]  it stands for single program multiple data and the idea is
[4158.84s -> 4162.14s]  that I've written a single program, a single ISPC function
[4162.84s -> 4166.60s]  but its logic does different stuff, works on different data
[4166.60s -> 4170.42s]  for example, based on whatever the value of program index is,
[4170.42s -> 4173.06s]  which can be zero, one, two, three, four, five, six or seven
[4173.40s -> 4177.48s]  if my gang size is eight, this is a point in which I want
[4177.48s -> 4181.04s]  you to ask any questions about the meaning of this program
[4181.42s -> 4183.46s]  and by meaning of this program I say,
[4184.14s -> 4187.94s]  what does each program instance compute?
[4188.46s -> 4189.96s]  One good, actually here's something I want you
[4189.96s -> 4194.44s]  to do is talk amongst yourself, tell me, tell your partner,
[4195.12s -> 4200.14s]  I have eight program instances, I have an array of size N,
[4200.90s -> 4203.96s]  what program instance is responsible
[4203.96s -> 4205.70s]  for computing every element of the array?
[4206.58s -> 4209.50s]  This gets the right answer when this ISPC program,
[4209.50s -> 4212.92s]  when this function returns, when all instances are done,
[4213.50s -> 4216.80s]  for every input element in the array X,
[4217.12s -> 4220.40s]  there is a value correctly stored into results of I
[4220.40s -> 4223.64s]  that is the sign of X of I, the program is correct.
[4224.64s -> 4228.70s]  Question is, what program instance does what?
[4229.50s -> 4230.70s]  Okay, I'll let you talk about that for a second,
[4230.78s -> 4231.90s]  yeah and if you have a clarification.
[4231.90s -> 4234.60s]  What is the program count?
[4234.60s -> 4237.14s]  Program count is the total number of instances,
[4237.36s -> 4239.20s]  in this case it takes on the value eight,
[4240.14s -> 4242.24s]  program index is like my thread ID.
[4242.24s -> 4252.00s]  All right, I'm going to bring everybody back together
[4252.00s -> 4253.60s]  so we can, because I'm eyeing the clock.
[4254.68s -> 4257.66s]  So who can tell me what this program does?
[4258.60s -> 4261.42s]  Co-operatively I create a bunch of program instances,
[4263.16s -> 4263.90s]  but who does what?
[4266.00s -> 4266.16s]  Yeah.
[4266.16s -> 4274.40s]  Each program instance computes one array element,
[4274.40s -> 4275.12s]  so let's think about this.
[4275.20s -> 4278.02s]  So remember, this is the program run
[4278.02s -> 4279.40s]  by every program instance.
[4280.40s -> 4282.38s]  If I only have eight program instances
[4282.38s -> 4285.48s]  and every program instance only performs one.
[4285.76s -> 4291.80s]  Ah, okay, so tell me how it works.
[4292.18s -> 4293.70s]  This is sequential code,
[4294.44s -> 4297.20s]  like when you say what does a program instance do,
[4297.48s -> 4299.38s]  you just look at this code and you're like,
[4299.38s -> 4300.60s]  well it runs this logic.
[4301.22s -> 4302.88s]  So what does program instance zero do?
[4303.92s -> 4305.08s]  It has a for loop, it says
[4305.08s -> 4307.94s]  for my current variable I equals zero to N,
[4309.10s -> 4311.14s]  compute the value of X, you know,
[4311.14s -> 4314.78s]  compute the sign of X sub zero, put it in result sub zero,
[4314.78s -> 4315.42s]  and then what does it do?
[4315.42s -> 4319.48s]  And then it will compute zero, program count plus zero.
[4319.76s -> 4321.50s]  And then it just increments its loop pointer,
[4321.50s -> 4323.78s]  that's just normal C code at this point.
[4324.18s -> 4325.80s]  It says, okay, the next iteration I'm responsible
[4325.80s -> 4328.34s]  for eight, the next iteration I'm responsible
[4328.34s -> 4329.88s]  for 16, and so on and so on.
[4330.38s -> 4333.40s]  So if one program instance computes every eighth item,
[4333.52s -> 4335.74s]  how do I get the total overall the right result?
[4336.64s -> 4340.68s]  Like the other program instance would be like one.
[4341.72s -> 4343.60s]  Yeah, so remember this is happening
[4344.96s -> 4347.90s]  in eight different copies with different values
[4347.90s -> 4349.06s]  of program index.
[4350.04s -> 4352.76s]  And as a result, I am interleaving iteration,
[4352.76s -> 4357.34s]  or sorry, I'm interleaving elements of the array
[4357.84s -> 4359.86s]  and sort of just passing them off round robin
[4359.86s -> 4361.20s]  to the various instances.
[4361.72s -> 4362.48s]  That's just what I wrote.
[4362.48s -> 4364.46s]  I did that myself, that's how I designed the program.
[4365.64s -> 4367.58s]  I want to know if everybody can visualize that.
[4371.10s -> 4373.98s]  Now question, yeah, so this is interleaved.
[4374.64s -> 4376.54s]  So in other ways, this is like one way to think about it.
[4376.80s -> 4378.50s]  Instance zero does these things,
[4378.50s -> 4380.94s]  instance one does those things, and so on and so on.
[4381.32s -> 4383.40s]  And cooperatively together, we get the job done.
[4385.40s -> 4385.80s]  Make sense?
[4387.36s -> 4390.16s]  Okay. I haven't said anything about implementation.
[4390.88s -> 4393.58s]  Would it be valid, at least to some approximation,
[4393.58s -> 4399.64s]  if the implementation of this call was call ispc sine x
[4400.26s -> 4402.10s]  on a thread for instance zero?
[4402.66s -> 4404.94s]  When it finishes, now let's just call that function again
[4405.08s -> 4406.92s]  and set the value of program index to one.
[4407.86s -> 4408.92s]  Completely sequential.
[4410.28s -> 4411.28s]  Does it compute the right answer?
[4412.16s -> 4414.92s]  It does. You know, why would you use ispc
[4414.92s -> 4415.82s]  if that was the answer?
[4416.36s -> 4421.00s]  Unclear. Could the underlying implementation actually spawn 8p
[4421.00s -> 4424.60s]  threads, run that call on 8p threads, and come back together?
[4426.78s -> 4429.84s]  The answer is yes, that's what I want you all to say.
[4429.84s -> 4432.98s]  There's some slight little details on why ispc prevents
[4433.16s -> 4434.12s]  that from an implementation,
[4434.44s -> 4436.58s]  but conceptually you should think about it like that.
[4436.58s -> 4439.04s]  I just spawned 8 different threads of control.
[4439.46s -> 4440.80s]  I carried them out with different values
[4440.80s -> 4442.54s]  of program instance, and if I just run all 8
[4442.54s -> 4443.34s]  of them, I get the job done.
[4446.54s -> 4448.74s]  Here's a different ispc program.
[4448.74s -> 4449.82s]  I changed the program.
[4449.82s -> 4452.38s]  It computes the exact same answer, but does it differently.
[4454.48s -> 4455.98s]  Now what does every instance do?
[4457.10s -> 4458.48s]  I'll have you look at it offline,
[4458.48s -> 4459.48s]  but if you look carefully,
[4459.48s -> 4461.86s]  every program instance now does a contiguous chunk
[4461.86s -> 4462.28s]  of the array.
[4463.80s -> 4465.56s]  I changed the program.
[4465.56s -> 4467.40s]  It still computes the same output,
[4468.74s -> 4470.78s]  but there's a different assignment.
[4470.96s -> 4472.04s]  So this is kind of blocked.
[4472.24s -> 4474.10s]  So this version of the program looks more like this.
[4475.10s -> 4477.56s]  Now I'm not going to tell you which one's better.
[4477.56s -> 4479.64s]  I'm going to skip this, and I'm going
[4479.64s -> 4482.00s]  to tell you really quickly what for each means.
[4483.70s -> 4486.22s]  So for each is kind of an interesting concept.
[4486.68s -> 4490.28s]  So the two programs I just gave you, I said,
[4490.46s -> 4492.40s]  here's a version where I do all the work.
[4493.04s -> 4495.48s]  Interleaving, I wrote the program
[4495.48s -> 4497.54s]  so that every program instance takes an interleaved
[4497.90s -> 4498.72s]  next iteration.
[4499.26s -> 4500.64s]  I gave you a different program
[4501.00s -> 4503.56s]  that says every program instance takes kind
[4503.56s -> 4504.18s]  of a block of them.
[4504.18s -> 4507.32s]  I don't want to think about that stuff.
[4508.46s -> 4511.00s]  I just want to tell ispc, here's a loop.
[4511.00s -> 4514.34s]  It has iterations for 0 to n. Don't care.
[4514.90s -> 4516.44s]  I'll tell you that they're independent.
[4517.44s -> 4521.16s]  Ispc, you just go ahead and assign the iterations
[4521.28s -> 4527.04s]  to program instances however you want, and I don't care.
[4527.82s -> 4528.72s]  That's what I've said here.
[4529.94s -> 4531.62s]  So all I've done here is to say I'm not going
[4531.62s -> 4533.46s]  to manually assign anything to anything.
[4533.46s -> 4534.50s]  I'm just going to say there's a loop.
[4535.72s -> 4537.06s]  That loop is going to get carried
[4537.06s -> 4538.64s]  out by all the program instances.
[4538.64s -> 4541.30s]  Some iteration is going to get done by some program instance.
[4541.30s -> 4542.02s]  I don't care.
[4542.80s -> 4544.02s]  Let the system figure it out.
[4544.02s -> 4545.20s]  Let the system decide for me.
[4545.24s -> 4545.98s]  Choose a good answer.
[4547.30s -> 4550.72s]  And what I'd like you to walk away with is convince yourself
[4550.72s -> 4554.22s]  that all four of these gray boxes are correct
[4554.22s -> 4556.64s]  and valid implementations of that pink box.
[4558.40s -> 4560.34s]  So in other words, I want you to be able
[4560.34s -> 4562.74s]  to convince yourself what does the pink box mean
[4563.62s -> 4565.98s]  and convince yourself that if I was a compiler
[4565.98s -> 4568.54s]  and changed the pink box to any of these gray boxes,
[4569.58s -> 4571.20s]  it would be a valid implementation.
[4571.68s -> 4573.10s]  Okay? And I'll stop there.
