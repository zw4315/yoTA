# Detected language: en (p=1.00)

[0.00s -> 7.86s]  So as I said, today we're going to finish
[7.86s -> 10.80s]  our discussion of cache coherence,
[10.80s -> 15.46s]  and then we'll talk about coherence's tricky pow memory
[15.46s -> 16.90s]  consistency.
[16.90s -> 20.70s]  OK, so to refresh our memories about where we were,
[20.70s -> 24.30s]  we're talking about definition of coherence.
[24.30s -> 26.14s]  And the definition of coherence, we said,
[26.14s -> 29.66s]  was that we wanted to have a memory system that
[29.70s -> 32.38s]  even though we had these vulnerable caches that
[32.38s -> 37.78s]  shared the cached memory locations from a shared address
[37.78s -> 41.46s]  space, if we looked at any particular address, what
[41.46s -> 45.94s]  we want to see is that all of the reads and writes made
[45.94s -> 49.30s]  to that address by all the processes
[49.30s -> 52.42s]  can be ordered in some sequential order,
[52.42s -> 54.94s]  such that the ordering corresponds
[54.94s -> 59.14s]  to the program order issued by any
[59.18s -> 60.86s]  of the threads in the processor.
[60.86s -> 62.34s]  OK, so that's the first thing.
[62.34s -> 64.26s]  Second thing we want is we want to make sure
[64.26s -> 68.78s]  that in this sequential order that the value that you read,
[68.78s -> 71.22s]  it was the value that was last written
[71.22s -> 72.74s]  in the sequential order.
[72.74s -> 75.02s]  So these are the two things that we
[75.02s -> 85.54s]  want to maintain in a coherent memory system with caches.
[85.54s -> 88.72s]  OK, so we said that in order to implement this,
[88.72s -> 91.80s]  there were two invariants that we needed to maintain.
[91.80s -> 94.92s]  First, the idea of a single writer multiple reader
[94.92s -> 96.06s]  invariant, right?
[96.06s -> 100.04s]  So we can sort of abstractly divide the access
[100.04s -> 103.00s]  to any particular location into these epochs.
[103.00s -> 105.18s]  And these epochs could be a read-write epoch
[105.18s -> 108.76s]  in which only one processor has sole access
[108.76s -> 113.16s]  to the cache line, or a read-only epoch
[113.16s -> 117.20s]  in which multiple processors can read.
[117.20s -> 121.04s]  So it's read-only, but multiple processors can share the line.
[121.04s -> 123.76s]  And the idea of the data value invariant
[123.76s -> 128.00s]  was that the value that you see in the read-only epoch
[128.00s -> 130.96s]  is the value that was last written in the most
[130.96s -> 132.80s]  recent read-write epoch.
[132.80s -> 135.52s]  OK, everybody understand these two invariants?
[135.52s -> 138.24s]  OK, we're going to come back to them in just a moment.
[138.24s -> 141.28s]  So we said the issue that we have with write,
[141.28s -> 143.00s]  we could implement cache coherence
[143.00s -> 144.44s]  with write-through caches.
[144.44s -> 147.92s]  But that had performance problems
[147.92s -> 151.12s]  because every write that the processor makes
[151.12s -> 153.44s]  appears on the bus, and the bus becomes a bottleneck,
[153.44s -> 154.76s]  a performance bottleneck.
[154.76s -> 158.76s]  So in a write-back cache, we have this problem
[158.76s -> 163.08s]  that each individual processor has its own cache.
[163.08s -> 165.54s]  And if everybody is writing into that cache
[165.54s -> 168.44s]  in a write-back fashion, then of course, we
[168.44s -> 170.64s]  have an incoherent system.
[170.64s -> 173.68s]  So the question is, there are two questions.
[173.68s -> 178.00s]  One is, when, as a processor, do I
[178.00s -> 182.72s]  have the permission to actually perform a write?
[185.36s -> 187.52s]  And the way that we want to indicate
[187.52s -> 189.68s]  that a processor can perform a write
[189.68s -> 194.04s]  is by having a modified or dirty state.
[194.04s -> 198.68s]  And so the idea is that the only one processor
[198.68s -> 202.28s]  should be in the modified state at any point in time.
[202.28s -> 204.28s]  And then the second question is, OK,
[204.28s -> 207.38s]  so I have got this data in my cache,
[207.38s -> 212.44s]  and some other processor wants to read that data,
[212.44s -> 213.96s]  read that cache line.
[213.96s -> 218.24s]  So the question then is, who should provide the data
[218.24s -> 220.00s]  to that processor?
[220.00s -> 223.24s]  And it should be the owner of the cache line, which
[223.24s -> 226.52s]  is typically also the processor that has the cache
[226.52s -> 229.24s]  line in modified state.
[229.24s -> 237.32s]  All right, so let's see how we actually implement a cache
[237.32s -> 238.84s]  coherency in write-back caches.
[238.84s -> 241.68s]  And what we want is an invalidation-based write-back
[241.68s -> 248.14s]  protocol to maintain these two invariants that we've defined.
[248.14s -> 252.40s]  So the idea, as we've already pointed out,
[252.40s -> 256.28s]  is that you want to have a single line in the system
[256.28s -> 258.82s]  in modified state at any point in time.
[258.90s -> 261.58s]  And that the idea is that the processors can only
[261.58s -> 265.76s]  write when they've got the cache line in modified state.
[265.76s -> 268.38s]  And then you have to have a way of getting the data
[268.38s -> 272.74s]  from a cache that has it in modified state
[272.74s -> 277.18s]  to other caches that want to read that data at some later
[277.18s -> 278.26s]  point in time.
[278.26s -> 280.34s]  OK, so let's see how this works.
[280.34s -> 282.66s]  So we reminded you that the idea,
[282.66s -> 284.26s]  we talked about a cache line.
[284.26s -> 287.38s]  We said there's the data component and the metadata.
[287.38s -> 289.70s]  And part of the metadata is the tag
[289.70s -> 293.60s]  that indicates the address of the line that
[293.60s -> 296.70s]  is contained in this particular cache line.
[296.70s -> 299.58s]  And then the dirty bit indicating
[299.58s -> 304.70s]  that the data has been modified in the cache.
[304.70s -> 306.26s]  And then the line state, which
[306.26s -> 309.14s]  is going to be used in the cache coherence protocol.
[309.14s -> 312.46s]  All right, so we briefly introduced
[312.46s -> 316.94s]  the idea of a modified shared invalid write-back
[316.94s -> 321.18s]  invalidate write-back invalidation protocol.
[321.18s -> 323.14s]  And so what we want to make sure
[323.14s -> 327.16s]  then is that with this protocol
[327.16s -> 330.58s]  that we can maintain the two invariants to implement
[330.58s -> 332.58s]  cache coherency correctly.
[332.58s -> 335.86s]  So there are three line states invalid,
[335.86s -> 342.22s]  which basically means that the cache line is not in the cache.
[342.22s -> 345.26s]  So it's the idea that it's not there.
[346.26s -> 349.26s]  Of course, if you access that cache line in an invalid state,
[349.26s -> 351.58s]  it's going to be a miss.
[351.58s -> 354.18s]  Then there's the shared state
[354.18s -> 357.68s]  that says the line is valid in one or more caches.
[357.68s -> 360.90s]  And the important thing to remember
[360.90s -> 362.70s]  is that memory is up to date.
[362.70s -> 364.78s]  So whenever a cache line is in shared state,
[364.78s -> 366.76s]  memory is always up to date.
[366.76s -> 369.26s]  And then there's the modified state.
[369.26s -> 374.44s]  And in this state, the line is valid in exactly one cache.
[374.44s -> 377.02s]  So only one cache in the system has
[377.02s -> 379.64s]  the cache line in a modified state
[379.64s -> 384.40s]  and also means that the dirty bit is set.
[384.40s -> 387.96s]  Another way of thinking about this is it's exclusive.
[387.96s -> 390.80s]  So only one cache in the system
[390.80s -> 394.12s]  has the cache line in that state.
[394.12s -> 397.48s]  And so you can think of it as being exclusive.
[397.48s -> 400.00s]  So then there are two operations
[400.00s -> 402.00s]  that the processors can make.
[402.00s -> 405.24s]  They can make reads and writes to cache lines,
[405.24s -> 407.96s]  so processor reads and processor writes.
[407.96s -> 411.44s]  And then there are three coherence bus transactions
[411.44s -> 415.32s]  that are the result of these reads and writes that
[415.32s -> 417.52s]  get made by the processors.
[417.52s -> 421.24s]  There's a bus read which says, get me the line
[421.24s -> 423.88s]  because I want to read it.
[423.88s -> 427.04s]  Bus read exclusive that says, get me the line because I
[427.04s -> 429.08s]  want to actually modify it.
[429.08s -> 430.04s]  I want to change it.
[430.04s -> 433.92s]  So I want to get it into exclusive state.
[433.92s -> 436.56s]  And then bus write back which says, hey,
[436.56s -> 438.32s]  there's a dirty line in the cache that's
[438.32s -> 439.68s]  going to be replaced.
[439.68s -> 443.00s]  And I'm going to write it back to memory.
[443.00s -> 449.52s]  OK, so let's see how these operations get implemented
[449.52s -> 451.32s]  in a cache coherency protocol.
[451.32s -> 456.36s]  So here is the cache coherency protocol state transition
[456.36s -> 457.00s]  diagram.
[457.00s -> 461.44s]  So we have the three states, invalid, shared, and modified.
[461.44s -> 467.72s]  And we have the operations that are made by the processor
[467.72s -> 473.92s]  and the subsequent transactions that come from the bus.
[473.92s -> 478.08s]  So the way to read this state transition diagram
[478.08s -> 483.76s]  is that each of the arcs is labeled by a AB action
[483.76s -> 486.16s]  pair or transaction pair.
[486.16s -> 489.80s]  And so the A is the initiating action.
[489.80s -> 497.12s]  And then the B is the resulting behavior
[497.12s -> 500.32s]  or resulting action taken by the cache controller.
[500.32s -> 505.76s]  And the way to look at it is that the green labeled actions
[505.76s -> 508.56s]  are initiated by the processor and the red
[508.56s -> 512.36s]  are initiated by the bus.
[512.36s -> 517.00s]  So let's look at this transition diagram in more
[517.00s -> 522.88s]  detail and dissect all of the different transitions.
[522.88s -> 530.92s]  So let's start with the processor initiated arcs.
[530.92s -> 536.76s]  So let's assume that the cache line is not in the cache
[536.76s -> 538.96s]  or it's an invalid state.
[538.96s -> 548.60s]  So if a processor does a read, what is the action of the bus?
[548.60s -> 550.92s]  What's going to happen on the bus?
[550.92s -> 555.04s]  So of course, if it's invalid, this is a miss, right?
[555.04s -> 558.36s]  So you can think about any action
[558.36s -> 562.56s]  that the processor takes that results in a bus transaction.
[562.56s -> 565.60s]  Think of it as a miss.
[565.60s -> 569.36s]  So we start out in invalid state
[569.36s -> 573.52s]  and we issue a processor read, and this
[573.52s -> 575.28s]  results in a bus read.
[575.28s -> 578.36s]  So it's get me the cache line because I want to read it.
[578.36s -> 583.96s]  So that means that the line has to move from invalid
[583.96s -> 584.88s]  to shared.
[585.00s -> 594.48s]  And then if the application then
[594.48s -> 600.56s]  does a write to that address, which
[600.56s -> 605.88s]  is contained in a cache line, then it's
[605.88s -> 608.12s]  going to issue a processor read
[608.12s -> 614.40s]  and the result on the bus is a bus read exclusive.
[614.48s -> 616.64s]  And so bus read exclusive says, hey,
[616.64s -> 620.44s]  now I need to move the state of the cache line
[620.44s -> 623.32s]  from shared to modified.
[623.32s -> 626.52s]  And modified means that, of course,
[626.52s -> 629.92s]  this is the only cache line in the system that
[629.92s -> 632.48s]  is in modified state.
[632.48s -> 637.08s]  So then if we have the cache line in modified state
[637.08s -> 639.36s]  and we issue reads and writes from the processor,
[639.36s -> 641.84s]  what happens?
[641.84s -> 642.88s]  Nothing, right?
[642.88s -> 644.68s]  These are hits, right?
[644.68s -> 645.56s]  These are hits.
[645.56s -> 650.40s]  So that's indicated by sort of nothing
[650.40s -> 652.80s]  in terms of the B action.
[652.80s -> 657.12s]  And so you can do whatever you like in modified state
[657.12s -> 659.88s]  and there are no bus transactions that
[659.88s -> 662.08s]  result from that, right?
[662.08s -> 662.56s]  OK.
[662.56s -> 668.00s]  So what happens if we go from, if the processor,
[668.00s -> 671.40s]  the cache line is invalid and we issue a processor
[671.40s -> 675.84s]  write, you go straight to the modified.
[675.84s -> 676.92s]  OK, good.
[676.92s -> 679.60s]  All right, so this is what happens
[679.60s -> 685.12s]  from the processor's point of view on an individual cache.
[685.12s -> 691.52s]  But then all the other processor cache controllers
[691.52s -> 693.48s]  are snooping on the bus.
[693.48s -> 696.24s]  So whatever transactions show up on the bus
[696.24s -> 699.88s]  of a single processor, remember, all the other processes
[699.88s -> 700.88s]  get to see.
[700.88s -> 702.76s]  And they're snooping on those.
[702.76s -> 707.08s]  And if they see a transaction for an address contained
[707.08s -> 710.56s]  in their cache, then they need to take an action, right?
[710.56s -> 714.32s]  So let's look at those actions.
[714.32s -> 719.36s]  All right, so we've got a few more processor initiated,
[719.36s -> 721.48s]  well, one more processor initiated action.
[721.48s -> 724.00s]  So if we're in the shared state and the processor
[724.00s -> 726.60s]  does a read, what happens?
[726.60s -> 730.48s]  You stay there and any bus transaction?
[730.48s -> 731.68s]  No, OK?
[731.68s -> 734.96s]  So what happens if we're in the shared state
[734.96s -> 740.52s]  and we see a bus read transaction over the bus?
[743.92s -> 744.56s]  Nothing.
[744.56s -> 745.80s]  OK, it's a shared state.
[745.80s -> 751.04s]  So what does that mean about the state of the cache
[751.04s -> 754.64s]  line in the system?
[754.64s -> 755.12s]  Yeah?
[755.12s -> 757.00s]  Is the state a problem actually?
[757.00s -> 760.08s]  Or that have that line in the shared state.
[760.16s -> 765.00s]  So any cache that has that cache line in its cache
[765.00s -> 768.96s]  is going to have it in shared state, right?
[768.96s -> 770.20s]  OK?
[770.20s -> 772.48s]  Is it possible for any of those caches
[772.48s -> 775.00s]  to have it in modified state?
[775.00s -> 775.92s]  No, right?
[778.80s -> 784.72s]  All right, so suppose we see a bus read exclusive transaction
[784.72s -> 787.76s]  on the bus for a cache line in our cache.
[787.76s -> 791.68s]  Then we need to make this transition from shared
[791.68s -> 793.92s]  to invalid, right?
[793.92s -> 798.88s]  So that would be the case where some other processor
[798.88s -> 801.20s]  wants to write this cache line.
[801.20s -> 806.04s]  So what happens to the state of the cache line in my cache?
[806.04s -> 806.96s]  It goes to invalid.
[811.52s -> 817.20s]  All right, so if I see a bus read, I mean modified,
[817.24s -> 818.56s]  what has to happen?
[818.56s -> 821.96s]  So we know that if we've got the cache line in modified state
[821.96s -> 825.36s]  and it's a write back cache, so we have the only valid copy
[825.36s -> 827.92s]  of the line in the system, right?
[827.92s -> 834.08s]  So if you see a bus read transaction on the bus,
[834.08s -> 835.20s]  then what has to happen?
[835.20s -> 839.48s]  Well, I need to supply the data in my cache, right?
[839.48s -> 842.36s]  So I need to, I see the bus read
[842.36s -> 845.84s]  and the response is a bus write back.
[845.84s -> 846.84s]  Everybody follow that?
[846.96s -> 847.96s]  So I, yeah.
[847.96s -> 850.76s]  And then I also need to move back to shared state.
[850.76s -> 853.40s]  Yeah, and then transition into shared state, right?
[853.40s -> 859.20s]  Because we know that it is not possible for cache lines,
[859.20s -> 864.64s]  for anything, any other cache to have a cache line
[864.64s -> 868.00s]  in its cache if it's in modified state, right?
[868.00s -> 870.16s]  So it's got to move to shared.
[870.16s -> 870.68s]  Yeah.
[870.68s -> 873.56s]  And the write back is running back to memory.
[873.56s -> 874.48s]  Correct, right?
[874.48s -> 879.92s]  So you update, you're giving the data to the requester, right?
[879.92s -> 881.72s]  But you're also updating memory.
[881.72s -> 883.60s]  Because whenever you're in shared state,
[883.60s -> 886.00s]  memory is always up to date.
[886.00s -> 886.76s]  Yeah?
[886.76s -> 888.76s]  What if, if there's a convention,
[888.76s -> 892.60s]  what keeps a human thrashing back and forth to different states?
[892.60s -> 894.92s]  That's what you're going to do all day long, absolutely.
[894.92s -> 897.80s]  That's what's going to happen.
[897.80s -> 902.76s]  And what are you going to see in terms of performance?
[902.76s -> 904.16s]  It's going to be a lot worse, right?
[904.16s -> 906.04s]  You're going to see lots of cache misses.
[906.04s -> 910.32s]  And we'll look at a case like that in just a moment.
[910.32s -> 911.48s]  All right.
[911.48s -> 914.68s]  What about bus?
[914.68s -> 916.04s]  We're in modified state.
[916.04s -> 923.04s]  We see a bus read exclusive, right?
[923.04s -> 927.36s]  That means some other processor wants to read this cache line.
[927.36s -> 929.76s]  We have it, the only value.
[929.80s -> 934.52s]  So we have to supply the up-to-date of the most recent
[934.52s -> 935.92s]  copy of the cache line.
[935.92s -> 937.52s]  So it's got to be a bus write back.
[937.52s -> 940.84s]  But then we also have to move to invalid state.
[940.84s -> 941.48s]  OK.
[941.48s -> 942.64s]  Everybody understand that?
[942.64s -> 947.12s]  So let's do an example just to reinforce
[947.12s -> 950.28s]  what we just talked about, all right?
[950.28s -> 952.52s]  So cache clearance example.
[952.52s -> 959.00s]  Here are the processor actions by the different processors.
[959.00s -> 961.00s]  They're all simplified things.
[961.00s -> 964.12s]  We're just using one address, all right?
[964.12s -> 966.04s]  And so what we want to look at is
[966.04s -> 969.56s]  the state of the cache line in the three caches,
[969.56s -> 972.44s]  the bus transaction that results from the processor
[972.44s -> 975.64s]  action, and where the data comes from, OK?
[979.44s -> 984.32s]  So processor one does a read.
[984.32s -> 985.44s]  What's the bus transaction?
[985.44s -> 987.76s]  I suppose I gave the game away here.
[987.80s -> 990.88s]  What's the bus transaction?
[990.88s -> 992.56s]  Bus read, OK.
[992.56s -> 996.24s]  What's the state of the different caches?
[999.76s -> 1000.68s]  Huh?
[1000.68s -> 1002.12s]  Shared.
[1002.12s -> 1005.44s]  Shared in processor one's cache,
[1005.44s -> 1009.80s]  and it's invalid or not there in the other caches.
[1009.80s -> 1012.84s]  Where's the data going to come from?
[1012.84s -> 1013.88s]  All right.
[1013.88s -> 1016.40s]  Processor three doesn't read of x.
[1016.40s -> 1020.76s]  What's the bus transaction?
[1020.76s -> 1022.96s]  What's the state of the caches?
[1027.72s -> 1029.08s]  OK.
[1029.08s -> 1033.72s]  So and then where's the data coming from?
[1037.08s -> 1039.32s]  Remember, when you're in shared state,
[1039.32s -> 1042.88s]  the data always comes from memory, right?
[1042.88s -> 1044.40s]  And it's just to keep things simple.
[1044.40s -> 1046.24s]  We could imagine some other scheme,
[1046.24s -> 1049.40s]  but then we'll see an example where should you get it.
[1049.40s -> 1054.60s]  If two caches have it, who should you get it from, right?
[1054.60s -> 1055.12s]  All right.
[1055.12s -> 1058.44s]  So processor three does a write.
[1058.44s -> 1061.96s]  What's the bus transaction?
[1061.96s -> 1064.28s]  Bus read exclusive.
[1064.28s -> 1067.48s]  What's the state of the caches?
[1067.96s -> 1080.76s]  Modify in processor three and invalid in processor one,
[1080.76s -> 1082.44s]  and the data comes from memory.
[1082.44s -> 1083.40s]  OK.
[1083.40s -> 1087.88s]  Processor one does a read of x.
[1087.88s -> 1091.72s]  What's the bus transaction?
[1091.72s -> 1094.32s]  What's the state of the caches?
[1097.48s -> 1102.92s]  And where's the data coming from?
[1105.84s -> 1107.04s]  Processor three's cache.
[1107.04s -> 1107.84s]  OK.
[1107.84s -> 1112.44s]  Processor one does a read of x.
[1112.44s -> 1119.28s]  Where's the what bus transaction occurs?
[1119.28s -> 1121.56s]  Ah, it's a tricky one, right?
[1121.56s -> 1123.44s]  There's no bus transaction here, right?
[1123.44s -> 1125.84s]  It's a hit.
[1125.84s -> 1127.12s]  Yeah.
[1128.04s -> 1129.72s]  Data's coming from read.
[1129.72s -> 1132.32s]  How does memory know that it doesn't need
[1132.32s -> 1135.60s]  to reply to that read transaction?
[1135.60s -> 1137.88s]  How does memory know?
[1137.88s -> 1141.08s]  Because it knows that the state of the,
[1141.08s -> 1149.00s]  it sees the state of the, well,
[1149.00s -> 1153.72s]  basically the processor that has the cache line
[1153.72s -> 1155.80s]  tells memory to hold off.
[1155.80s -> 1159.40s]  Yeah, that's the way to think about it.
[1159.40s -> 1160.40s]  Yeah?
[1160.40s -> 1166.68s]  What if you have a P1 write after a P3 write?
[1166.68s -> 1167.24s]  Where?
[1167.24s -> 1168.64s]  This one?
[1168.64s -> 1169.16s]  Oh, yeah.
[1169.16s -> 1169.72s]  Here?
[1169.72s -> 1172.16s]  If this was a write, you tell me what happens.
[1172.16s -> 1175.40s]  What's the bus, what's the, if P1 does a write,
[1175.40s -> 1177.32s]  what's the bus transaction?
[1177.32s -> 1178.80s]  Bus read x.
[1178.80s -> 1180.96s]  And then what happens?
[1180.96s -> 1184.28s]  Then does P1 copy from P3?
[1184.28s -> 1184.76s]  Yeah.
[1184.76s -> 1188.28s]  So in that case, it's the same thing, right?
[1188.28s -> 1190.68s]  The data is going to come from P3,
[1190.68s -> 1193.84s]  but what's going to be the state of the caches?
[1193.84s -> 1197.16s]  But P1's going to be having to pick up the pi.
[1197.16s -> 1200.08s]  Yeah, right, exactly.
[1200.08s -> 1203.64s]  But the data moves, right?
[1203.64s -> 1208.40s]  OK, so in this case, nothing happens in terms of the state.
[1208.40s -> 1210.40s]  We don't have a bus transaction.
[1210.40s -> 1214.44s]  And where does the data come from?
[1214.60s -> 1215.56s]  It's on cache, right?
[1215.56s -> 1217.40s]  It was a hit.
[1217.40s -> 1219.68s]  OK.
[1219.68s -> 1221.96s]  Process do does a write of x.
[1221.96s -> 1226.04s]  What's the bus transaction?
[1226.04s -> 1227.40s]  What's the state of the cache?
[1231.76s -> 1233.04s]  Right.
[1233.04s -> 1236.68s]  And then where's the data coming from?
[1236.68s -> 1238.20s]  What?
[1238.20s -> 1240.80s]  Memory, right?
[1240.80s -> 1241.52s]  Yeah.
[1241.52s -> 1243.04s]  Is there a function of the difference
[1243.12s -> 1246.12s]  between invalid and just the dash?
[1246.12s -> 1251.96s]  Well, dash says we never saw it at all.
[1251.96s -> 1256.40s]  But as far as the processor accessing the dash line,
[1256.40s -> 1257.60s]  it's the same thing.
[1260.60s -> 1263.04s]  So essentially, if it was a dash,
[1263.04s -> 1264.80s]  then it would be a coldness, right?
[1264.80s -> 1269.48s]  As opposed to, hey, it got invalidated.
[1269.48s -> 1274.32s]  OK, so notice that we do have the single writer, multiple
[1274.32s -> 1275.88s]  reader protocol.
[1275.88s -> 1278.28s]  And so the question is, why do you need this transition
[1278.28s -> 1279.60s]  from modified to shared?
[1285.24s -> 1286.60s]  Why is that important?
[1289.84s -> 1290.32s]  Yeah.
[1290.32s -> 1293.76s]  I guess to indicate that the cache line is back in memory.
[1293.76s -> 1294.80s]  Back in memory.
[1294.80s -> 1296.88s]  And what else?
[1296.88s -> 1297.40s]  Yeah.
[1297.52s -> 1300.36s]  Indicate that another cache or another processor
[1300.36s -> 1304.08s]  is the one that can modify it in transition.
[1304.08s -> 1305.24s]  Modified to share.
[1305.24s -> 1306.52s]  Why do you need to modify it?
[1306.52s -> 1309.20s]  For instance, this case.
[1309.20s -> 1310.20s]  Why is that important?
[1310.20s -> 1310.72s]  Yeah.
[1310.72s -> 1312.96s]  Otherwise, you would have to go to an invariant.
[1312.96s -> 1313.56s]  Right.
[1313.56s -> 1316.44s]  Well, you need to remember, we can only have one.
[1316.44s -> 1319.28s]  Remember, the invariant we're trying to maintain
[1319.28s -> 1322.92s]  is single writer, multiple reader, right?
[1323.00s -> 1329.40s]  So we need, in order to track that transition,
[1329.40s -> 1332.20s]  we need the transition from modified to shared.
[1332.20s -> 1332.76s]  OK.
[1332.76s -> 1335.48s]  So the other thing to note is that communication
[1335.48s -> 1337.08s]  increases memory latency, right?
[1337.08s -> 1342.64s]  So we saw that here, is that because P3 did a write to x,
[1342.64s -> 1347.08s]  P1, which had x in its cache,
[1347.08s -> 1349.96s]  now has to do a bus transaction, right?
[1349.96s -> 1351.80s]  Which is going to take much more time
[1351.84s -> 1354.40s]  than hitting in its cache, right?
[1354.40s -> 1358.88s]  And this is the case where you had the cache hit, right?
[1358.88s -> 1360.64s]  But if you had an intervening write,
[1360.64s -> 1363.56s]  then it causes extra cache misses.
[1363.56s -> 1366.56s]  And so, of course, you're going to lose performance.
[1366.56s -> 1374.76s]  So just to reinforce this idea of how the MSI protocol
[1374.76s -> 1377.88s]  maintains cache coherence, let's
[1377.88s -> 1380.20s]  think about the cache coherence invariance.
[1380.24s -> 1384.76s]  And so how do we maintain the single writer, multiple reader
[1384.76s -> 1386.76s]  invariant?
[1386.76s -> 1387.24s]  Yeah?
[1387.24s -> 1389.32s]  That's the transition from modified to shared
[1389.32s -> 1390.56s]  that we were talking about.
[1390.56s -> 1391.72s]  Right.
[1391.72s -> 1398.24s]  And then, so that's how, in shared state,
[1398.24s -> 1400.52s]  you have multiple readers.
[1400.52s -> 1404.20s]  And how about the single writer?
[1406.76s -> 1408.44s]  Only one person.
[1408.44s -> 1410.80s]  Only one cache can be in the modified state
[1410.80s -> 1415.08s]  at any point in time, right?
[1415.08s -> 1415.60s]  Yeah?
[1415.60s -> 1417.20s]  I was going to say that, yeah.
[1417.20s -> 1418.28s]  Good.
[1418.28s -> 1420.40s]  OK, what about data value invariant?
[1420.40s -> 1421.20s]  Tell me about that.
[1424.64s -> 1427.64s]  So how do we maintain the write serialization?
[1427.64s -> 1430.52s]  We go through the bus every time, so the bus tracks
[1430.52s -> 1432.24s]  like the serialized in this case.
[1432.24s -> 1433.72s]  Right.
[1433.72s -> 1435.60s]  And what about the data value?
[1435.64s -> 1439.08s]  How do we make sure that the data is
[1439.08s -> 1446.20s]  moved from the read-write epoch to the read-only epoch?
[1446.20s -> 1447.96s]  Yeah.
[1447.96s -> 1451.52s]  We go to the shared state, so we know that's not the bus.
[1451.52s -> 1452.96s]  You go to the shared state, yeah.
[1452.96s -> 1456.28s]  I guess it's like you use the cache and the processor
[1456.28s -> 1459.12s]  that modified it, like that's the source of the data.
[1459.12s -> 1459.56s]  Right.
[1459.56s -> 1465.64s]  And that is maintained with the bus write back, right?
[1465.64s -> 1467.84s]  So the bus write back, whenever you come out
[1467.84s -> 1472.20s]  of the modified state, you, as the cache that
[1472.20s -> 1474.28s]  had the line in modified state,
[1474.28s -> 1477.08s]  have to provide the data for the cache line.
[1477.08s -> 1477.84s]  Yeah?
[1477.84s -> 1482.76s]  So the processor dates, right, the shared or modified state,
[1482.76s -> 1486.24s]  and that other processor tries to make the read list in the read.
[1486.24s -> 1489.88s]  Well, only the first processor out of the modified state,
[1489.88s -> 1491.92s]  that processor is going to write in it.
[1491.92s -> 1494.48s]  It can't be read-write in that state.
[1494.48s -> 1497.88s]  Well, it's going to complete its write.
[1497.88s -> 1503.60s]  If it gets the line, basically the bus
[1503.60s -> 1506.48s]  is going to serialize all the actions, right?
[1506.48s -> 1510.00s]  It will complete, modify its cache line,
[1510.00s -> 1512.52s]  and then it'll be time to write back
[1512.52s -> 1514.80s]  to the next processor that wants to write.
[1514.84s -> 1517.92s]  So the line could bounce all the way around,
[1517.92s -> 1521.16s]  but essentially, every time that you've
[1521.16s -> 1524.20s]  got exclusive access to the cache line,
[1524.20s -> 1525.36s]  you get to do a write.
[1525.36s -> 1526.20s]  Yeah?
[1526.20s -> 1531.28s]  Where is the logic process to snoop on the bus?
[1531.28s -> 1535.40s]  Well, this is hardware that's implemented in the cache
[1535.40s -> 1536.64s]  controller, right?
[1536.64s -> 1537.36s]  Yeah.
[1537.36s -> 1539.84s]  Does each processor have its own cache controller?
[1539.84s -> 1542.00s]  Each processor has its own cache controller
[1542.00s -> 1543.00s]  connected to the bus.
[1543.12s -> 1546.56s]  Is there also maybe a logic across them, like the track?
[1546.56s -> 1551.76s]  Well, there's not logic across, but there is some access
[1551.76s -> 1554.64s]  control logic that says who gets to go next, right?
[1557.96s -> 1558.52s]  All right.
[1558.52s -> 1564.04s]  So yeah, and the bus serializes the transaction.
[1564.04s -> 1566.84s]  So these are the two statements that we just made.
[1566.84s -> 1569.72s]  OK, any questions about how this works?
[1572.24s -> 1572.76s]  All right.
[1573.04s -> 1575.48s]  So in summary then, the way that we're going to do MSIs,
[1575.48s -> 1578.68s]  we're going to keep these three states.
[1578.68s -> 1581.96s]  We're going to make sure that only one processor can
[1581.96s -> 1583.40s]  be modified state.
[1583.40s -> 1586.12s]  And in modified state, this is the only processor
[1586.12s -> 1591.06s]  that can write in the system to a particular cache line.
[1591.06s -> 1599.72s]  And if you maintain this invariance
[1599.76s -> 1601.96s]  by having this bus which serializes
[1601.96s -> 1604.56s]  all the transactions in the system,
[1604.56s -> 1613.64s]  and make sure that we move from the state in one cache
[1613.64s -> 1617.76s]  to the next in a serialized manner.
[1617.76s -> 1622.24s]  OK, so this is the way the MSI works.
[1622.24s -> 1627.88s]  And the result is, of course, it keeps the caches coherent.
[1627.88s -> 1630.12s]  But it has a downside.
[1630.12s -> 1635.76s]  And that is, whenever you want to read a cache line
[1635.76s -> 1640.84s]  and then write, sometimes going from read from shared state
[1640.84s -> 1643.72s]  to modified state is called an upgrade.
[1643.72s -> 1651.04s]  So this is the term upgrade, OK?
[1651.04s -> 1655.68s]  But the problem is that you now have two bus transactions.
[1655.68s -> 1658.78s]  You have the bus read, which is a miss.
[1658.78s -> 1660.40s]  And then you have a bus read exclusive,
[1660.40s -> 1663.44s]  which is also a miss to upgrade the cache
[1663.44s -> 1666.52s]  line from shared to modified, right?
[1666.52s -> 1672.20s]  And so the idea is that you can optimize the MSI protocol
[1672.20s -> 1674.88s]  by adding an exclusive state, right?
[1674.88s -> 1678.76s]  This is called the MESI invalidation protocol,
[1678.76s -> 1683.16s]  not to be confused with the Argentinian International Soccer
[1683.16s -> 1685.32s]  Superstar.
[1685.32s -> 1690.40s]  And the idea is that now you're decoupling ownership
[1690.40s -> 1693.72s]  from whether you have the line in modified state, right?
[1693.72s -> 1698.72s]  So what does exclusive mean in this context?
[1702.68s -> 1705.24s]  What's another way of describing
[1705.24s -> 1707.24s]  the exclusive state?
[1707.24s -> 1707.76s]  Yeah?
[1707.76s -> 1710.32s]  Well, it's that the line is not dirty,
[1710.32s -> 1712.52s]  but all of that process of handling.
[1712.52s -> 1713.16s]  Right.
[1713.20s -> 1716.80s]  So it's clean, exclusive, right?
[1716.80s -> 1723.40s]  So it's clean, but only one cache has it, right?
[1723.40s -> 1729.68s]  And so the idea then is when you read a cache line that
[1729.68s -> 1732.44s]  is not shared by any other processor,
[1732.44s -> 1736.00s]  instead of bringing it into the cache in shared state,
[1736.00s -> 1738.40s]  you bring it in exclusive state,
[1738.40s -> 1741.96s]  because you know no other cache has it, right?
[1741.96s -> 1742.48s]  OK?
[1742.52s -> 1750.36s]  So you go from invalid to shared
[1750.36s -> 1754.04s]  would be the typical case if the line was shared.
[1754.04s -> 1757.00s]  But if the line were not shared,
[1757.00s -> 1761.36s]  then you could go directly to exclusive state.
[1761.36s -> 1766.76s]  And now the upgrade can be made without doing a bus
[1766.76s -> 1768.64s]  transaction, right?
[1768.64s -> 1771.04s]  So it's not a miss, it's a hit.
[1771.08s -> 1777.04s]  And in which case, it's going to be a lot more efficient, right?
[1777.04s -> 1781.12s]  And so given what you know about the MSI protocol,
[1781.12s -> 1784.40s]  you can fill in the rest of the transitions, right?
[1784.40s -> 1786.88s]  And so I'll leave that for you
[1786.88s -> 1790.84s]  to look at in detail at your leisure.
[1790.84s -> 1794.80s]  Any questions on M-E-S-I messy protocol?
[1794.80s -> 1795.40s]  Yeah?
[1795.40s -> 1797.24s]  Why is it more efficient than just taking
[1797.24s -> 1801.00s]  that far-left path of just bus redox?
[1805.00s -> 1807.28s]  Where?
[1807.28s -> 1810.04s]  Sorry, I think I got it wrong.
[1810.04s -> 1810.64s]  This one?
[1810.64s -> 1814.24s]  Yeah, the bus redox.
[1814.24s -> 1818.92s]  So that's if you did a write first.
[1818.92s -> 1823.16s]  I want to read first and then write.
[1823.16s -> 1825.60s]  Yeah.
[1825.64s -> 1828.96s]  OK, questions?
[1828.96s -> 1832.72s]  All right, so the problem with buses, of course,
[1832.72s -> 1834.52s]  is they don't scale, right?
[1834.52s -> 1837.32s]  You've got a single set of wires everybody's sharing.
[1837.32s -> 1843.80s]  And so that means that every transaction for every cache
[1843.80s -> 1847.12s]  line is seen by all the processors, right?
[1847.12s -> 1851.12s]  Even the processors that don't have that cache line.
[1851.12s -> 1853.40s]  So that's one problem.
[1853.56s -> 1856.04s]  One problem is, of course, all the processors
[1856.04s -> 1860.60s]  sharing this interface between the caches and memory.
[1860.60s -> 1862.76s]  And so you're going to run out of bandwidth.
[1862.76s -> 1865.92s]  And so the other issue is that you're
[1865.92s -> 1872.08s]  serializing all of the bus.
[1872.08s -> 1875.48s]  So you're serializing the transactions
[1875.48s -> 1878.88s]  to all the cache lines, whereas we really only
[1878.92s -> 1883.96s]  need to serialize actions to a particular cache line, right?
[1883.96s -> 1889.16s]  And so one way to potentially make
[1889.16s -> 1891.56s]  a system that's more scalable is
[1891.56s -> 1895.52s]  to use the idea of a directory, right?
[1895.52s -> 1899.48s]  So the idea of a directory is it stores information
[1899.48s -> 1904.36s]  about which processors actually have this particular cache
[1904.36s -> 1906.84s]  line in their cache, right?
[1906.88s -> 1909.64s]  And then, given that information,
[1909.64s -> 1912.60s]  you can send invalidates or you
[1912.60s -> 1916.44s]  can communicate with just a set of processors
[1916.44s -> 1919.36s]  that need to know, as opposed to all
[1919.36s -> 1921.44s]  of the processes in the system.
[1921.44s -> 1925.04s]  And so this makes it possible to use interconnects
[1925.04s -> 1926.32s]  that are more scalable.
[1926.32s -> 1928.00s]  So instead of having a bus, you
[1928.00s -> 1930.48s]  could have a network or a ring or something
[1930.48s -> 1933.32s]  that doesn't require this ability
[1933.32s -> 1937.96s]  to broadcast information and serialize all
[1937.96s -> 1939.92s]  of the transactions in the system.
[1939.92s -> 1943.40s]  You only need to serialize transactions
[1943.40s -> 1946.56s]  to a particular cache line.
[1946.56s -> 1948.72s]  So that's the idea of directories.
[1948.72s -> 1951.60s]  And so that's what's implemented these days
[1951.60s -> 1957.36s]  on most multicore processors, right?
[1957.36s -> 1961.52s]  For instance, the i7 that's used in your myth machines,
[1961.52s -> 1962.00s]  right?
[1962.08s -> 1964.00s]  So you've got a ring interconnect.
[1964.00s -> 1967.28s]  And the ring interconnect is not a broadcast interconnect.
[1967.28s -> 1970.20s]  And so snooping won't work here.
[1970.20s -> 1972.44s]  And it's also, of course, more scalable.
[1972.44s -> 1976.40s]  But what we have is we have a directory associated
[1976.40s -> 1978.56s]  with the L3 cache, right?
[1978.56s -> 1981.72s]  For this to work, we have to guarantee
[1981.72s -> 1988.32s]  that every cache line that could be in any of the L2s
[1988.32s -> 1991.28s]  will also be in L3.
[1991.28s -> 1996.28s]  This property is called inclusion, OK?
[1996.28s -> 2001.28s]  So L3 is an inclusive cache, right?
[2001.28s -> 2006.32s]  And then we've got a, for each of the cache lines in the L3,
[2006.32s -> 2008.24s]  we have a directory, right?
[2008.24s -> 2011.68s]  And the directory in this case is,
[2011.68s -> 2019.76s]  you can think of it as five bits, right?
[2019.76s -> 2024.12s]  So we have a bit for processor 0, a bit for processor 2,
[2024.12s -> 2030.96s]  a bit for processor 1, 2, and 3, and a dirty bit, right?
[2030.96s -> 2036.96s]  So the idea then is that if you're in shared state,
[2036.96s -> 2040.16s]  then multiple bits would be on, right?
[2040.16s -> 2043.16s]  So this would indicate that a particular cache line is
[2043.16s -> 2047.64s]  in cache for processors 0, 1, and 2.
[2047.64s -> 2051.28s]  And so if a write came in, then we would know
[2051.28s -> 2055.48s]  which processors to send invalidates to, OK?
[2055.48s -> 2058.36s]  And then if it were, if the cache line were
[2058.36s -> 2064.44s]  in modified state, then of course there would be a bit set,
[2064.44s -> 2067.76s]  the dirty bit would be set, and then just one
[2067.76s -> 2073.96s]  of these bits, say bits for processor 3, would be set.
[2073.96s -> 2077.40s]  And the rest would be zeros, OK?
[2078.12s -> 2080.56s]  Indicating that only one, we know that whenever we're
[2080.56s -> 2084.52s]  in modified state, only one of the caches
[2084.52s -> 2087.32s]  in the system can have the cache line.
[2087.32s -> 2091.76s]  So the directory then is more scalable, but of course,
[2091.76s -> 2096.92s]  you need extra information associated with some place
[2096.92s -> 2099.84s]  in the memory system that keeps the directory, right?
[2099.84s -> 2102.32s]  And there are all sorts of schemes for, you know,
[2102.32s -> 2105.24s]  implementing directories more efficiently,
[2105.24s -> 2107.24s]  but you get the idea, right?
[2107.56s -> 2109.96s]  And with a directory-based cache coherence scheme,
[2109.96s -> 2114.04s]  you can scale to, you know, tens of processors,
[2114.04s -> 2117.16s]  maybe even hundreds of processors.
[2117.16s -> 2120.76s]  OK, any questions?
[2120.76s -> 2122.76s]  All right, so what are the implications
[2122.76s -> 2126.16s]  of cache coherence to the programmer?
[2126.16s -> 2129.80s]  So the issue, of course, as we've already talked about,
[2129.80s -> 2135.92s]  is that when you have communication
[2135.92s -> 2140.40s]  in your application, since you're sharing memory
[2140.40s -> 2144.32s]  and the processes have caches, and the way
[2144.32s -> 2146.00s]  that the caches are kept coherent
[2146.00s -> 2149.60s]  is by using cache coherency mechanisms, what's
[2149.60s -> 2157.96s]  going to happen is that the distribution of accesses
[2157.96s -> 2160.44s]  in your system is going to change, right?
[2160.44s -> 2162.28s]  So what's going to happen is that you're
[2162.28s -> 2165.76s]  going to have an increased number of cache
[2165.76s -> 2169.40s]  misses at the different levels of the memory hierarchy.
[2169.40s -> 2171.60s]  And if you have a NUMA system, have we talked
[2171.60s -> 2172.40s]  about NUMA systems?
[2175.04s -> 2176.76s]  OK, so NUMA systems, so if you've
[2176.76s -> 2181.92s]  got multiple chips or multiple sockets in your system,
[2181.92s -> 2187.92s]  right, suppose socket ones, let's call this CP,
[2187.92s -> 2196.12s]  CPU 1 and CPU 2, and these are different sockets.
[2196.12s -> 2200.24s]  They have DRAM associated with them.
[2209.12s -> 2213.88s]  DRAM 1, DRAM 2, and then there's some interconnect.
[2213.88s -> 2223.44s]  And remember, inside these CPUs are multiple cores, right?
[2223.44s -> 2229.24s]  So the idea is that you have much higher bandwidth
[2229.24s -> 2233.08s]  to your local memory than you have to,
[2233.08s -> 2239.04s]  so CPU 1 has higher bandwidth access to its local DRAM
[2239.04s -> 2241.52s]  than its remote DRAM, right?
[2241.52s -> 2246.32s]  And to a first order as an application programmer,
[2246.32s -> 2250.44s]  you could care less because you'd rather not
[2250.44s -> 2251.76s]  have to deal with this issue.
[2251.76s -> 2255.36s]  But if you actually want to get to maximum performance,
[2255.36s -> 2259.52s]  then you may want to have the operating system allocate
[2259.52s -> 2266.40s]  data such that data is accessed more often
[2266.40s -> 2271.28s]  by the processor where that data is local.
[2271.28s -> 2276.36s]  But anyway, from the point of view of the access patterns,
[2276.36s -> 2279.28s]  NUMA accesses cost you more, right?
[2279.28s -> 2285.64s]  So if we're looking at, for instance, the core i7,
[2285.64s -> 2295.32s]  we see that an L3 hit is 40 cycles if it's unshared,
[2295.32s -> 2299.04s]  it's 65 cycles if it's shared,
[2299.04s -> 2303.20s]  and it's almost double, 75 cycles,
[2303.20s -> 2306.28s]  if it was modified in a different core, right?
[2306.28s -> 2310.48s]  So essentially, because you have communication
[2310.48s -> 2315.32s]  and because you've got a cache coherency protocol that
[2315.32s -> 2318.20s]  is managing this communication, communication
[2318.20s -> 2322.36s]  means extra latency, extra time,
[2322.36s -> 2325.56s]  and it also changes the distribution of misses, right?
[2325.56s -> 2331.60s]  And so if you think about the average memory access
[2331.60s -> 2334.36s]  time of your program, it's going
[2334.36s -> 2343.28s]  to be the frequency of access of a particular part
[2343.28s -> 2346.88s]  of the memory hierarchy times the latency of access.
[2346.88s -> 2351.88s]  And what happens when you go from a uniprocessor application
[2351.88s -> 2353.76s]  to a multiprocessor application
[2353.76s -> 2357.92s]  is that you change the distribution of accesses
[2357.92s -> 2363.52s]  such that you access levels of the memory hierarchy that
[2363.52s -> 2365.68s]  have longer latency, right?
[2365.68s -> 2369.64s]  So the result, of course, is your average memory access
[2369.64s -> 2374.60s]  time of a multiprocessor or a multiparallel program
[2374.60s -> 2378.40s]  is going to be larger than a sequential program, right?
[2378.40s -> 2381.84s]  And so that is the impact on the performance
[2381.84s -> 2386.20s]  of your application of the communication overhead, right?
[2386.20s -> 2392.88s]  And you can see the latencies as shown by the core i7 data
[2392.88s -> 2394.04s]  that I've shown you here.
[2394.04s -> 2398.04s]  If you want to analyze this, right, you can use system
[2398.04s -> 2399.32s]  tools, right?
[2399.32s -> 2403.56s]  So for instance, if you're on an Intel processor,
[2403.56s -> 2405.20s]  you can use Intel VTune.
[2405.20s -> 2408.60s]  And Intel VTune will interrogate the performance
[2408.60s -> 2412.44s]  counting hardware on the processor
[2412.44s -> 2414.92s]  and tell you about cache misses
[2414.92s -> 2417.16s]  and to some extent communication.
[2417.16s -> 2422.88s]  If you're on an Apple M1 machine on a laptop,
[2422.88s -> 2425.00s]  you might use Apple Xcode instruments
[2425.00s -> 2427.88s]  to get a similar source of information
[2427.88s -> 2432.28s]  to tell you about what data structures are causing cache
[2432.28s -> 2433.04s]  misses.
[2433.04s -> 2437.40s]  And then based on your knowledge of the application,
[2437.76s -> 2443.56s]  you can improve the locality to improve performance.
[2443.56s -> 2446.12s]  So one of the things that happens
[2446.12s -> 2453.60s]  when you have caches and cache lines and sharing
[2453.60s -> 2456.64s]  is sometimes you get sharing where you don't expect it.
[2456.64s -> 2458.08s]  This is called full sharing.
[2458.08s -> 2462.72s]  So let's look at an example of how and where this can happen.
[2462.72s -> 2469.00s]  Suppose I've got an array of integers, right,
[2469.00s -> 2473.20s]  based on the number of threads I have in my program.
[2473.20s -> 2480.80s]  And so the question is why I could allocate it,
[2480.80s -> 2487.72s]  the per thread variable as shown in the first instance,
[2487.72s -> 2491.80s]  or I could create a struct that looks
[2491.80s -> 2495.88s]  like this in the second instance.
[2495.88s -> 2500.74s]  So can somebody tell me why this second instance
[2500.74s -> 2502.08s]  might be more performant?
[2502.08s -> 2502.84s]  Yeah.
[2502.84s -> 2531.16s]  In the first case, you would have multiple instances
[2531.16s -> 2534.08s]  of the counter, like multiple locations
[2534.08s -> 2535.92s]  going to the same cache line.
[2535.92s -> 2537.32s]  Whereas in the second one, you're
[2537.32s -> 2540.16s]  in short that each one is exclusive to each other.
[2540.16s -> 2541.48s]  So there's no contention.
[2541.48s -> 2544.16s]  So in theory, I believe everything is independent
[2544.16s -> 2546.32s]  because we crush it into a cache line.
[2546.32s -> 2550.52s]  In the first case, we're going to have communication.
[2550.52s -> 2551.20s]  Exactly right.
[2551.20s -> 2554.84s]  So remember, a cache coherence does communication
[2554.84s -> 2557.08s]  on the basis of cache lines, right?
[2557.08s -> 2560.48s]  So ideally, if you could do it on a per word,
[2560.48s -> 2566.52s]  a per byte basis, if you could do coherence per byte,
[2566.52s -> 2568.64s]  it would be very fine-grained.
[2568.64s -> 2570.94s]  It would be great for the application programmer,
[2570.94s -> 2573.76s]  but a nightmare for a hardware designer.
[2573.76s -> 2577.68s]  But the result of using cache lines
[2577.68s -> 2579.96s]  is that we have what is called
[2579.96s -> 2582.40s]  full sharing, which is sharing where you don't expect it,
[2582.40s -> 2582.90s]  right?
[2582.90s -> 2586.52s]  So for instance, if this is the worker function, which
[2586.52s -> 2592.00s]  is essentially just going to add a increment, the per thread
[2592.00s -> 2593.40s]  variable, right?
[2593.40s -> 2600.00s]  And we could implement it, so we fire off a bunch of threads
[2600.00s -> 2605.32s]  and we time the two instances.
[2605.32s -> 2609.08s]  And in the first case, on a four-core system,
[2609.08s -> 2611.28s]  it takes 14.2 seconds.
[2611.28s -> 2614.68s]  And in the second case, it takes 4.7 seconds, right?
[2614.68s -> 2616.08s]  So dramatic.
[2616.08s -> 2618.36s]  Of course, this is a contrived situation,
[2618.36s -> 2621.80s]  but full sharing can really impact your performance
[2621.80s -> 2624.24s]  if it's happening a lot, OK?
[2627.60s -> 2632.60s]  And that's an example where, of course, the data is
[2632.60s -> 2636.20s]  actually all independent, but because of the fact
[2636.20s -> 2639.68s]  that multiple threads, local threads,
[2639.68s -> 2644.92s]  and variables are landing on the same cache line,
[2644.92s -> 2648.80s]  we get this unintended communication
[2648.80s -> 2651.04s]  called full sharing.
[2651.04s -> 2654.44s]  Full sharing also happens in numerical applications.
[2654.44s -> 2656.44s]  If you've got some sort of grid application
[2656.44s -> 2663.60s]  and you have cache lines that cross a grid boundary that's
[2663.60s -> 2665.76s]  being assigned to different processes,
[2665.76s -> 2667.32s]  then you can get full sharing.
[2667.36s -> 2670.80s]  Here's some data from some old benchmarks
[2670.80s -> 2673.48s]  that were first written at Stanford probably
[2673.48s -> 2677.32s]  around 30 years ago, but are still being used.
[2677.32s -> 2683.24s]  And so this is data showing miss rate for different cache
[2683.24s -> 2689.68s]  line sizes, for different types of misses, right?
[2689.68s -> 2692.44s]  So you've got cold misses, which we talked about,
[2692.44s -> 2695.08s]  capacity conflict misses, which we've talked about,
[2695.08s -> 2696.44s]  then true sharing misses.
[2696.48s -> 2700.28s]  So this is where the data is actually shared,
[2700.28s -> 2703.32s]  and full sharing misses where the data is only
[2703.32s -> 2707.28s]  shared because it happens to land on the same cache line.
[2707.28s -> 2708.76s]  So the first thing to notice is
[2708.76s -> 2713.36s]  that there's a significant amount of spatial locality
[2713.36s -> 2715.84s]  in truly shared data, right?
[2715.84s -> 2719.68s]  Because you see, as you increase the cache line size,
[2719.68s -> 2726.92s]  the amount of true sharing misses gets reduced.
[2726.92s -> 2729.04s]  But take the example of radiosity,
[2729.04s -> 2738.28s]  we see the amount of full sharing misses increase
[2738.28s -> 2739.88s]  with cache line size, right?
[2739.88s -> 2749.12s]  Which is, you know, you'd expect if it was truly just,
[2749.12s -> 2755.64s]  you'd expect it to go down if it was giving the increased cache
[2755.64s -> 2760.04s]  line size, but it goes up because of full sharing.
[2760.04s -> 2763.04s]  The extreme case is radix sort, where
[2763.04s -> 2766.64s]  you have a dramatic increase in full sharing due to the way
[2766.64s -> 2771.64s]  that the application is written, OK?
[2771.64s -> 2775.68s]  So of course, the longer you make the line size,
[2775.68s -> 2778.92s]  the worse the full sharing gets, right?
[2778.92s -> 2783.68s]  So you can imagine if you got to a virtual memory size line
[2783.68s -> 2786.52s]  size, the full sharing would be terrible, right?
[2786.52s -> 2787.92s]  So remember way back when I said,
[2787.92s -> 2790.08s]  so one of the ways of doing coherence
[2790.08s -> 2792.52s]  is potentially using a virtual memory
[2792.52s -> 2794.80s]  page and the operating system.
[2794.80s -> 2797.16s]  So forget about the software overhead.
[2797.16s -> 2800.64s]  Your full sharing problem is going to be horrible.
[2800.64s -> 2801.56s]  Yeah?
[2801.56s -> 2803.72s]  If you were intelligent enough and always
[2803.72s -> 2805.24s]  used some of the operations which
[2805.24s -> 2809.72s]  were the size of the cache line, because it's simply,
[2809.72s -> 2812.60s]  then you would not have this problem, right?
[2812.60s -> 2814.52s]  Well, the full sharing has everything
[2814.52s -> 2816.72s]  to do with the size of the cache line
[2816.72s -> 2822.16s]  and the data that's being accessed
[2822.16s -> 2823.60s]  by the different processes.
[2823.60s -> 2826.60s]  What happens inside the processor
[2826.60s -> 2832.36s]  is kind of not relevant, right?
[2832.36s -> 2833.04s]  Yeah?
[2833.04s -> 2834.80s]  Is it like, can you always guarantee
[2834.80s -> 2837.56s]  that you can reduce the full sharing or largely
[2837.56s -> 2839.04s]  be programmatically?
[2839.04s -> 2840.84s]  Or is it just like, is it just how it is?
[2840.84s -> 2843.16s]  Well, you can always do it programmatically
[2843.16s -> 2852.52s]  if you space the data apart such that, you know.
[2852.52s -> 2854.20s]  But then, aren't you like, you know.
[2854.24s -> 2860.20s]  But you may be increasing your, you may be wasting space, right?
[2860.20s -> 2864.92s]  So you certainly can make sure that you space things
[2864.92s -> 2867.36s]  apart such that full sharing doesn't occur.
[2867.36s -> 2869.52s]  But there may be other reasons.
[2869.52s -> 2874.92s]  Maybe you have phases to your application.
[2874.92s -> 2878.28s]  And at some point, you need to switch things around.
[2878.28s -> 2881.80s]  And it may not be convenient to organize things in ways
[2881.80s -> 2885.24s]  that minimize full sharing.
[2885.24s -> 2887.56s]  But you should be aware of it.
[2890.72s -> 2891.32s]  OK.
[2891.32s -> 2896.08s]  So in summary then, of course, the reason
[2896.08s -> 2897.80s]  that we've got this coherence problem
[2897.80s -> 2900.56s]  is that we have this shared address space being implemented
[2900.56s -> 2903.60s]  by independent separate caches.
[2903.60s -> 2907.88s]  And in order to maintain the behavior that we want,
[2907.88s -> 2911.72s]  we want to be able to serialize accesses
[2911.72s -> 2917.84s]  to any particular address such that all the processes see
[2917.84s -> 2926.64s]  the serialized access, see the same serialized access
[2926.64s -> 2929.04s]  to a particular address, right?
[2929.04s -> 2934.68s]  And we said that you could implement cache coherence using
[2934.68s -> 2939.96s]  a bus as a broadcast notification mechanism
[2939.96s -> 2941.68s]  and snooping.
[2941.68s -> 2944.76s]  However, snooping doesn't scale.
[2944.76s -> 2946.96s]  And so what you'd like is something
[2946.96s -> 2951.12s]  that scales and sort of directory mechanisms
[2951.12s -> 2953.76s]  are much more scalable.
[2953.76s -> 2957.40s]  And so from a software developer point of view,
[2957.40s -> 2961.12s]  the impact of cache coherence shows itself
[2961.12s -> 2966.88s]  up most in the ideas that you have full sharing
[2966.88s -> 2972.60s]  and that if you can kind of think about changing
[2972.60s -> 2976.04s]  your applications such that you can reduce the full sharing,
[2976.04s -> 2979.36s]  then potentially performance could be a lot better.
[2979.36s -> 2980.76s]  And one way to kind of understand
[2980.76s -> 2984.04s]  when full sharing is happening potentially
[2984.04s -> 2986.40s]  is to use some of these system tools
[2986.40s -> 2989.56s]  to analyze cache behavior.
[2989.56s -> 2990.04s]  OK.
[2990.04s -> 2993.84s]  So now we're going to, so first of all I should ask,
[2993.88s -> 2997.60s]  are there any questions about cache coherency?
[2997.60s -> 3001.68s]  So you would certainly, there are some exercises
[3001.68s -> 3004.36s]  in the assignment that cover this stuff.
[3004.36s -> 3005.60s]  Yeah?
[3005.60s -> 3007.72s]  We talked about directory protocol
[3007.72s -> 3009.14s]  and then you mentioned that you're
[3009.14s -> 3012.96s]  going to track the different types of the time
[3012.96s -> 3013.84s]  happening, right?
[3013.84s -> 3014.56s]  Yeah.
[3014.56s -> 3016.84s]  The size of that would keep increasing.
[3016.84s -> 3018.08s]  It would more costly.
[3018.08s -> 3018.84s]  Yes.
[3018.84s -> 3019.72s]  Yeah, absolutely.
[3019.72s -> 3022.96s]  And then you might have some other schemes
[3023.00s -> 3024.28s]  for reducing that, but we're not
[3024.28s -> 3026.32s]  going to get into those details.
[3026.32s -> 3027.04s]  Other questions?
[3027.04s -> 3029.04s]  I was just wondering in the benchmark example,
[3029.04s -> 3031.80s]  why did redix swap start to increase or shrink
[3031.80s -> 3034.80s]  better performance with increased cache coherence?
[3034.80s -> 3035.40s]  It doesn't.
[3035.40s -> 3036.04s]  It gets worse.
[3042.60s -> 3044.04s]  Then there was a new fine shape,
[3044.04s -> 3045.64s]  so it was decreasing.
[3045.64s -> 3046.14s]  Yeah.
[3046.14s -> 3046.64s]  So.
[3046.64s -> 3047.60s]  Why is it waiting?
[3047.60s -> 3053.36s]  So we're looking at miss rate, right?
[3056.36s -> 3059.04s]  So you're saying, why does it go down and then up?
[3059.04s -> 3059.54s]  Yeah.
[3059.54s -> 3060.88s]  You tell me.
[3060.88s -> 3061.88s]  Why do you think?
[3067.88s -> 3070.76s]  OK, so what do we know about line size
[3070.76s -> 3071.96s]  and spatial locality?
[3071.96s -> 3072.46s]  Yeah.
[3076.36s -> 3080.28s]  I'd say the spatial locality increases.
[3080.28s -> 3083.32s]  Well, spatial locality is better exploited,
[3083.32s -> 3085.24s]  and so the impact of that is.
[3085.24s -> 3087.08s]  And then you have false sharing.
[3087.08s -> 3089.76s]  False sharing, which is pushing things up, right?
[3089.76s -> 3092.32s]  And so eventually, the improvements
[3092.32s -> 3096.20s]  that you get from exploiting spatial locality
[3096.20s -> 3101.12s]  are outweighed by the detriment of false sharing.
[3101.12s -> 3102.72s]  Yeah.
[3102.72s -> 3107.00s]  Is it like the standard cache line size in most processes,
[3107.00s -> 3108.48s]  like 64 or something like that?
[3108.48s -> 3110.80s]  It depends, but usually it's 64 or 128.
[3110.80s -> 3114.40s]  Cache line sizes have been getting longer over time.
[3114.40s -> 3114.90s]  Yeah.
[3114.90s -> 3115.82s]  That was interesting.
[3115.82s -> 3118.64s]  So going back to the directory base,
[3118.64s -> 3122.24s]  like that's the validation for following in the SI and SI.
[3122.24s -> 3122.96s]  Yeah.
[3122.96s -> 3126.04s]  So we're in the SI and SI primarily using the caches
[3126.04s -> 3129.32s]  that didn't have, or like, now it
[3129.32s -> 3131.04s]  is that we had to open up to them.
[3131.04s -> 3135.08s]  Well, so the directory mechanism
[3135.08s -> 3140.08s]  is somewhat orthogonal to the exact protocol you're using,
[3140.08s -> 3140.68s]  right?
[3140.68s -> 3143.52s]  It's just sort of a way of, I mean,
[3143.52s -> 3148.84s]  you could still have modified exclusive shared,
[3148.84s -> 3150.84s]  and the question is sort of when
[3150.84s -> 3153.44s]  do you indicate to the processor
[3153.44s -> 3158.12s]  that you should pay attention to a transaction, right?
[3158.12s -> 3161.24s]  You could do it with a bus where everybody is snooping,
[3161.24s -> 3163.20s]  or you could do it with a directory which
[3163.20s -> 3166.24s]  has point-to-point messages and is more scalable.
[3166.24s -> 3168.72s]  But you'd still keep the same states, right?
[3172.72s -> 3173.60s]  Does that answer?
[3173.60s -> 3174.10s]  Yes.
[3183.64s -> 3186.60s]  Yeah, so let's talk about memory consistency, right?
[3186.64s -> 3190.16s]  Which is the kind of other side of the coin of dealing
[3190.16s -> 3193.68s]  with memory and multi-processors.
[3193.68s -> 3198.92s]  All right, so we talked about how loads and stores should
[3198.92s -> 3200.28s]  behave.
[3200.28s -> 3203.48s]  Intuitively, you want to return the latest value
[3203.48s -> 3205.04s]  written, right?
[3205.04s -> 3208.72s]  And we said coherence says we're just
[3208.72s -> 3211.76s]  going to talk about a single memory location.
[3211.76s -> 3215.84s]  Memory consistency says what about the apparent ordering
[3215.88s -> 3219.56s]  of reads and writes to different addresses
[3219.56s -> 3221.44s]  by different processors?
[3221.44s -> 3223.84s]  How should they be ordered, and how should we
[3223.84s -> 3227.36s]  interpret the behavior of a program, a shared memory
[3227.36s -> 3230.80s]  program, that does this, right?
[3230.80s -> 3236.84s]  OK, so this is going to affect how we interpret programs, what
[3236.84s -> 3238.56s]  programs mean, right?
[3238.56s -> 3245.32s]  And by telling us what is the allowed behavior of the memory
[3245.32s -> 3253.28s]  system, or the compiler and the memory system, right?
[3253.28s -> 3260.28s]  And so it's important because the way in which you allow
[3260.28s -> 3264.00s]  loads and stores to different addresses to be reordered
[3264.00s -> 3266.92s]  is going to have a big impact on how
[3266.92s -> 3270.48s]  performant our system is.
[3270.48s -> 3277.12s]  And it's also going to affect what I, as the programmer,
[3277.12s -> 3282.12s]  can do to improve the performance of my program.
[3282.12s -> 3287.32s]  OK, so what are we going to focus on?
[3290.04s -> 3290.96s]  So let's see.
[3300.48s -> 3304.04s]  So why should you care?
[3304.04s -> 3308.04s]  So if you want to implement a synchronization library,
[3308.04s -> 3309.84s]  you really need to pay attention
[3309.84s -> 3311.36s]  to memory consistency.
[3311.36s -> 3318.00s]  And if you want to develop a compiler or low level OS code,
[3318.00s -> 3319.60s]  then it's important to understand
[3319.60s -> 3322.12s]  about memory consistency.
[3322.12s -> 3326.20s]  It turns out that if you are an application programmer who
[3326.20s -> 3332.60s]  does most of their development using a compiler, so
[3332.60s -> 3336.08s]  sort of a high level language programmer,
[3336.08s -> 3342.16s]  or you are going to use kernel libraries or system
[3342.16s -> 3344.60s]  libraries, then it's not so important
[3344.60s -> 3348.80s]  to understand the details of the memory consistency model.
[3348.80s -> 3351.64s]  But if you are kind of a low level system developer,
[3351.64s -> 3354.64s]  like potentially the people in this class,
[3354.68s -> 3356.72s]  then it's important to understand.
[3356.72s -> 3361.64s]  So memory consistency differs from memory coherence
[3361.64s -> 3366.28s]  because we're talking about multiple different addresses.
[3366.28s -> 3369.24s]  So what the memory consistency focuses on
[3369.24s -> 3373.76s]  is it reads and writes to different locations
[3373.76s -> 3376.84s]  as observed by different processes.
[3376.84s -> 3381.00s]  So coherence says, hey, eventually any writes
[3381.00s -> 3383.48s]  made to a particular address will all
[3383.52s -> 3386.84s]  be seen by all the processes in the same order.
[3386.84s -> 3392.36s]  And consistency says, when writes to different reads
[3392.36s -> 3393.92s]  and writes to different addresses
[3393.92s -> 3397.64s]  get seen by reads from other processes.
[3397.64s -> 3402.24s]  So said slightly differently, goal of coherence
[3402.24s -> 3406.80s]  is to ensure the memory system behaves as
[3406.80s -> 3408.76s]  if the caches weren't there.
[3408.76s -> 3416.40s]  So you imagine that any system that has caches in the system
[3416.40s -> 3418.44s]  needs to be coherent.
[3418.44s -> 3420.36s]  It's kind of fundamental if you
[3420.36s -> 3423.44s]  don't want to work with a system that's not cache
[3423.44s -> 3424.84s]  coherent.
[3424.84s -> 3427.52s]  But even if you had a system without caches,
[3427.52s -> 3430.74s]  you would still need a memory consistency model.
[3430.74s -> 3435.36s]  So even systems without caches need a memory consistency model
[3435.36s -> 3438.60s]  because we need to define the allowed behavior
[3439.08s -> 3442.68s]  of the memory system, whether or not
[3442.68s -> 3445.52s]  there are caches in the system.
[3445.52s -> 3448.96s]  So to kind of give you a preview of what we're
[3448.96s -> 3456.20s]  going to talk about, in a modern multiprocessor system,
[3456.20s -> 3459.56s]  memory accesses are going to get reordered.
[3459.56s -> 3461.60s]  And the main reason for doing that
[3461.60s -> 3464.10s]  is to improve performance.
[3464.10s -> 3465.76s]  But then the question is, what does
[3465.80s -> 3468.88s]  that mean to the programmer?
[3468.88s -> 3472.04s]  Well, if you're an application programmer,
[3472.04s -> 3473.80s]  you probably don't care so much.
[3473.80s -> 3476.72s]  But if you are a systems programmer or a compiler
[3476.72s -> 3479.72s]  writer, then you care a lot because it's
[3479.72s -> 3483.40s]  going to affect what you can do in your compiler.
[3483.40s -> 3487.16s]  It's going to affect the behavior of your low level
[3487.16s -> 3489.52s]  system code.
[3489.52s -> 3492.68s]  So let's define a few things here.
[3492.72s -> 3498.10s]  So there are four types of memory orderings
[3498.10s -> 3499.60s]  that we can have in the system.
[3499.60s -> 3503.08s]  So you've got a program that finds a bunch of reason,
[3503.08s -> 3504.68s]  writes, loads, and stores.
[3504.68s -> 3508.14s]  And these happen in program order.
[3508.14s -> 3512.12s]  And the question is whether the memory system maintains
[3512.12s -> 3513.88s]  this program order or not.
[3518.12s -> 3520.80s]  Well, for any particular thread, it's
[3520.80s -> 3522.30s]  going to maintain the program order.
[3522.30s -> 3525.42s]  The question is sort of what it does to accesses
[3525.42s -> 3527.30s]  from different threads, right?
[3527.30s -> 3535.22s]  So you have four types of operation orderings.
[3535.22s -> 3544.94s]  A write of x to a read of y is called
[3544.94s -> 3546.62s]  is write to read memory ordering.
[3546.62s -> 3552.62s]  So it says that the write of x has
[3552.62s -> 3556.78s]  to commit and change the state of the memory
[3556.78s -> 3560.50s]  before the read of y takes place.
[3560.50s -> 3563.86s]  So that's a kind of write to read memory ordering.
[3563.86s -> 3567.38s]  There's read to read memory ordering.
[3567.38s -> 3569.38s]  There's read to write memory ordering.
[3569.38s -> 3571.90s]  And there's write to write memory ordering, right?
[3571.90s -> 3576.54s]  So the key thing, of course, is that x and y
[3576.54s -> 3580.14s]  are different addresses.
[3580.14s -> 3583.30s]  So these are the four types of memory orderings.
[3583.30s -> 3586.38s]  And the question is what memory orderings
[3586.38s -> 3589.86s]  are going to be maintained by the memory system.
[3589.86s -> 3595.46s]  So in order to get our heads thinking about this,
[3595.46s -> 3598.72s]  let's look at a very simple program.
[3598.72s -> 3602.00s]  So initially, a and b are 0.
[3602.00s -> 3607.36s]  And on processor 0, we set a to 1.
[3607.36s -> 3608.68s]  And then we print b.
[3608.68s -> 3613.00s]  And processor 1, we set b to 1.
[3613.00s -> 3614.64s]  And then print a.
[3614.64s -> 3617.16s]  So why don't you discuss with your neighbor
[3617.16s -> 3624.52s]  and tell me which one of these outputs can be printed.
[3624.52s -> 3627.00s]  So I'll give you a minute to think about it.
[3628.72s -> 3639.20s]  What do you think?
[3639.20s -> 3643.68s]  Any of them?
[3643.68s -> 3644.68s]  How do you think?
[3644.68s -> 3645.64s]  Any of them?
[3645.64s -> 3646.48s]  Any of them?
[3646.48s -> 3649.72s]  So we have options for any of them.
[3649.72s -> 3654.20s]  Do we have another view of what could be printed?
[3654.20s -> 3654.84s]  Yeah.
[3654.84s -> 3656.96s]  Just one more.
[3657.00s -> 3659.20s]  1, 1.
[3659.20s -> 3660.68s]  Well, I think that that's possible.
[3660.68s -> 3661.36s]  So 1, 1.
[3664.32s -> 3666.08s]  What else?
[3666.08s -> 3667.64s]  0, 1.
[3667.64s -> 3670.88s]  Yeah, 0, 1.
[3670.88s -> 3674.56s]  Can 1, 0 be printed?
[3674.56s -> 3675.28s]  Someone said yes.
[3675.28s -> 3676.04s]  Someone said no.
[3682.44s -> 3684.24s]  Yes?
[3684.24s -> 3686.92s]  Well, not clear.
[3686.92s -> 3688.12s]  But let's see.
[3688.12s -> 3692.20s]  So let's put in a double question mark by that.
[3692.20s -> 3693.40s]  What about 0, 0?
[3693.40s -> 3695.48s]  Is that permissible?
[3695.48s -> 3696.48s]  No.
[3696.48s -> 3697.20s]  OK.
[3697.20s -> 3698.48s]  So that's not.
[3698.48s -> 3699.00s]  All right.
[3699.00s -> 3701.24s]  So in order to kind of understand
[3701.24s -> 3704.72s]  what could be printed, a useful way to think about it
[3704.72s -> 3707.36s]  is in terms of happens before graph.
[3707.36s -> 3712.72s]  So what has to happen before what
[3712.72s -> 3717.00s]  in order to allow the output that you expect?
[3717.00s -> 3720.76s]  So remember, when you are executing in a multiprocessor
[3720.76s -> 3725.36s]  environment, the instructions of the different threads
[3725.36s -> 3727.88s]  on the different processes can be interleaved
[3727.88s -> 3729.60s]  in any manner in time.
[3729.60s -> 3730.44s]  OK?
[3730.44s -> 3733.68s]  Now, on any particular thread, the instructions
[3733.68s -> 3736.36s]  are going to be executed in program order.
[3736.36s -> 3740.24s]  But when they get interleaved into the global order,
[3740.24s -> 3744.72s]  whatever that may be, is going to be dependent on things
[3744.72s -> 3746.20s]  that you can't control.
[3746.20s -> 3746.72s]  All right?
[3746.72s -> 3754.08s]  So let's assume that we want a particular output like 0, 0.
[3754.08s -> 3754.60s]  Right?
[3754.60s -> 3756.68s]  So that says that we would need
[3756.68s -> 3763.48s]  to have the print of B happen before B gets set to 1.
[3763.48s -> 3765.52s]  Right?
[3765.52s -> 3768.76s]  And we'd also need the print of A
[3768.76s -> 3774.56s]  to happen before A gets set to 1.
[3774.56s -> 3777.96s]  And we know the program order on any particular thread
[3777.96s -> 3784.96s]  is statement 1 before statement 2 and statement 3
[3784.96s -> 3786.20s]  before statement 4.
[3786.20s -> 3787.76s]  So we put those arcs in.
[3787.76s -> 3790.16s]  And then what we have is a cycle.
[3790.16s -> 3791.48s]  Right?
[3791.48s -> 3795.88s]  So when we've got a cycle, we know
[3795.88s -> 3798.68s]  that the outcome's impossible because an event must
[3798.68s -> 3802.20s]  happen before itself.
[3802.20s -> 3808.00s]  And so that says we can't get 0, 0, and we can't get 1, 0.
[3808.00s -> 3809.12s]  Yeah?
[3809.12s -> 3813.48s]  Do you have a store buffer in your out-of-order operating
[3813.48s -> 3819.04s]  and rearrange a read to occur or write?
[3819.04s -> 3821.52s]  Could you have 0, 0?
[3821.52s -> 3822.92s]  We'll see.
[3822.92s -> 3824.60s]  That's a good question.
[3824.60s -> 3828.24s]  And we're going to investigate that.
[3828.24s -> 3830.24s]  All right.
[3830.24s -> 3834.32s]  So the question is, what should the programmer really expect?
[3834.32s -> 3839.52s]  Well, if I asked you, given what
[3839.52s -> 3842.88s]  you know about shared memory so far, what you would probably
[3842.88s -> 3846.20s]  say is, hey, there's some global ordering
[3846.20s -> 3851.60s]  to all of the accesses in the system.
[3851.60s -> 3855.08s]  And I want to respect that global ordering.
[3855.08s -> 3859.04s]  And I also want to have a global ordering that everybody
[3859.04s -> 3863.56s]  sees, that I can serialize all the accesses,
[3863.56s -> 3866.96s]  memory accesses in the system.
[3866.96s -> 3869.72s]  And also, of course, that global order
[3869.72s -> 3874.72s]  would mean that any particular thread, the memory accesses
[3874.72s -> 3876.80s]  are made in program order.
[3876.80s -> 3877.56s]  Right?
[3877.56s -> 3881.24s]  So what you would be describing loosely
[3881.24s -> 3886.04s]  is what was formally defined by Leslie Lamport in 1976
[3886.04s -> 3888.48s]  as sequential consistency.
[3888.48s -> 3892.48s]  So sequential consistency, for which he partly
[3892.48s -> 3896.52s]  won the Turing Award in 2013, basically
[3896.52s -> 3900.00s]  says that all operations are executed
[3900.00s -> 3903.80s]  in some sequential order, as if they were manipulating
[3903.80s -> 3905.76s]  a single shared memory.
[3905.76s -> 3911.08s]  And then any of the accesses or any of the operations
[3911.52s -> 3916.56s]  in any particular thread are always
[3916.56s -> 3919.76s]  happened in program order.
[3919.76s -> 3922.12s]  And so that means that sequentially consistent
[3922.12s -> 3931.12s]  systems maintain all of the four memory orderings
[3931.12s -> 3935.08s]  that we just described, write to read, read to read,
[3935.08s -> 3937.68s]  read to write, and write to write.
[3937.68s -> 3941.60s]  So we've got this sequential global order.
[3941.60s -> 3945.36s]  And notice that this diagram slightly changed
[3945.36s -> 3948.84s]  from the coherence example, in that we now
[3948.84s -> 3953.08s]  have multiple addresses, x and y, in the diagram.
[3953.08s -> 3954.40s]  And there's some global order.
[3954.40s -> 3955.16s]  Right?
[3955.16s -> 3959.40s]  One way of kind of thinking about sequential consistency
[3959.40s -> 3962.28s]  is this switch metaphor, where you've
[3962.28s -> 3963.80s]  got a single shared memory.
[3963.80s -> 3965.16s]  You've got this pointer.
[3965.16s -> 3971.52s]  And this pointer randomly points to any of the processors,
[3971.52s -> 3976.72s]  accepts a memory operation from the processor,
[3976.72s -> 3980.20s]  and then flips to another processor
[3980.20s -> 3986.28s]  and takes some random number of accesses from that processor.
[3986.28s -> 3987.40s]  OK?
[3987.40s -> 3991.16s]  So we've got this idea of sequential consistency.
[3991.16s -> 3993.00s]  So here's the picture.
[3993.00s -> 4001.36s]  We've got A equals 1, R1 equals B, B equals 1, R2 equals A.
[4001.36s -> 4004.44s]  On processor 0 and processor 1, we
[4004.44s -> 4010.84s]  start by executing a operation from processor 0,
[4010.84s -> 4014.28s]  followed by an operation from processor 1,
[4014.28s -> 4017.92s]  followed by an operation from processor 1,
[4017.92s -> 4021.44s]  and then finally an operation from processor 0.
[4021.44s -> 4022.36s]  OK?
[4022.36s -> 4025.76s]  So with a switch metaphor, then you
[4025.76s -> 4027.96s]  can kind of look at the interleavings
[4027.96s -> 4029.28s]  that you might get.
[4029.28s -> 4031.96s]  And you would see, this is just
[4031.96s -> 4034.88s]  another representation of that first example,
[4034.88s -> 4039.80s]  that both 00 and 10 are illegal.
[4039.80s -> 4043.96s]  You can never get it from sequential consistency.
[4043.96s -> 4044.92s]  OK.
[4044.92s -> 4050.68s]  So why might we want to relax these orderings,
[4050.96s -> 4053.04s]  do something other than sequential consistency?
[4053.04s -> 4054.76s]  So from the point of view of a programmer,
[4054.76s -> 4058.00s]  sequential consistency is the most obvious and intuitive
[4058.00s -> 4059.00s]  thing.
[4059.00s -> 4062.68s]  If I look at a program and I think about its multiprocessor
[4062.68s -> 4066.80s]  execution, what I want to see would be sequential consistency.
[4066.80s -> 4070.40s]  And the question is, why would any system implementer
[4070.40s -> 4072.68s]  want anything else?
[4072.68s -> 4076.80s]  And the reason is, of course, it's always performance.
[4076.80s -> 4078.48s]  That's what this class is about.
[4078.48s -> 4081.56s]  So we want to relax the orderings
[4081.56s -> 4083.08s]  so we can get more performance.
[4083.08s -> 4085.52s]  In particular, there was some comment about, hey,
[4085.52s -> 4090.72s]  what if I had a write which was going to miss the cache
[4090.72s -> 4092.28s]  and have to do all sorts of things?
[4092.28s -> 4094.48s]  It's going to take a long time.
[4094.48s -> 4096.28s]  Followed by a read, and maybe this read's
[4096.28s -> 4097.68s]  going to hit the cache.
[4097.68s -> 4098.60s]  Right?
[4098.60s -> 4099.92s]  They're two different addresses.
[4099.92s -> 4101.16s]  They're unrelated.
[4101.16s -> 4106.60s]  Why shouldn't I do the read before the write?
[4106.60s -> 4110.12s]  Or at least maybe I want to overlap the two.
[4110.12s -> 4112.28s]  So if you think about from the point of view
[4112.28s -> 4113.88s]  of the application, the application
[4113.88s -> 4117.40s]  wants to read its data as early as possible.
[4117.40s -> 4119.80s]  And who cares about the writes?
[4119.80s -> 4123.04s]  You can push them off later or try and overlap them
[4123.04s -> 4124.36s]  with other reads.
[4124.36s -> 4127.48s]  So that would be the idea is that why
[4127.48s -> 4129.24s]  wait for the write to finish, which
[4129.24s -> 4131.12s]  would be sequential consistency?
[4131.12s -> 4135.58s]  Let's at least overlap and maybe reorder.
[4135.58s -> 4139.62s]  So let's see what happens when you do that.
[4139.62s -> 4140.74s]  OK?
[4140.74s -> 4143.30s]  So the problem with SC is, of course,
[4143.30s -> 4150.70s]  that you've got this case where these two accesses are
[4150.70s -> 4155.98s]  unrelated, and you'd like to have the read happen
[4155.98s -> 4158.34s]  before or earlier than the write
[4158.34s -> 4160.82s]  because the write is going to take a long time.
[4160.82s -> 4161.54s]  OK?
[4161.54s -> 4163.78s]  So somebody said write buffer.
[4163.78s -> 4164.38s]  Good idea.
[4164.38s -> 4165.82s]  Let's put a write buffer in, right?
[4165.82s -> 4171.54s]  So the write buffer is going to basically say, hey, you write.
[4171.54s -> 4174.82s]  Eventually, it's going to be sent to the memory system.
[4174.82s -> 4177.02s]  But in the meantime, hold off.
[4177.02s -> 4178.70s]  Don't use the bus.
[4178.70s -> 4180.74s]  Don't use the resources because I'm
[4180.74s -> 4185.30s]  going to issue the read first because the processor is waiting
[4185.30s -> 4186.34s]  for the read.
[4186.34s -> 4191.98s]  And if I get the read earlier, then I
[4192.02s -> 4194.74s]  will get better performance, right?
[4194.74s -> 4198.18s]  So I'm going to put a write buffer in.
[4198.18s -> 4201.26s]  And the problem with the write buffer
[4201.26s -> 4206.22s]  is it's going to change my memory behavior, OK?
[4206.22s -> 4213.54s]  So in the case of can R1 and R2 equal 0,
[4213.54s -> 4215.50s]  we said in sequential consistency,
[4215.50s -> 4217.06s]  that can't happen.
[4217.06s -> 4220.86s]  So the question is, how could it happen with write buffers?
[4224.90s -> 4227.26s]  You asked the question, so you tell them.
[4227.26s -> 4230.78s]  If you have the writes just sitting in the write buffers
[4230.78s -> 4234.54s]  and then you go ahead and execute the reads while that's
[4234.54s -> 4237.74s]  occurring, then you would read 0, 0.
[4237.74s -> 4240.90s]  Right, so in this case, you would read 0, 0.
[4240.90s -> 4246.94s]  And that's not sequentially consistent.
[4246.98s -> 4253.86s]  It may not be obvious why this behavior should be.
[4253.86s -> 4257.74s]  And the question is, OK, so I have a write buffer.
[4257.74s -> 4259.82s]  We said the reason that you want to reorder things
[4259.82s -> 4261.46s]  is to improve performance.
[4261.46s -> 4263.54s]  This just shows you a couple of examples
[4263.54s -> 4270.18s]  where you can get performance improvement that's
[4270.18s -> 4274.14s]  pretty substantial by having a write buffer
[4274.14s -> 4279.14s]  between the processor and the cache.
[4279.14s -> 4281.18s]  Or maybe you put the write buffer
[4281.18s -> 4284.02s]  on the other side of the cache between the cache
[4284.02s -> 4286.90s]  and the bus, OK?
[4286.90s -> 4289.82s]  So write buffers are important.
[4289.82s -> 4293.10s]  Every major instruction set architecture
[4293.10s -> 4295.50s]  supports write buffers, right?
[4295.50s -> 4298.34s]  And the problem is write buffers
[4298.34s -> 4301.50s]  don't fit with sequential consistency.
[4301.50s -> 4303.48s]  You need a weaker consistency model
[4303.52s -> 4306.72s]  to have a write buffer, and you want it
[4306.72s -> 4309.16s]  because you want the improved performance that
[4309.16s -> 4310.36s]  comes from that.
[4310.36s -> 4313.60s]  But now you need a weaker memory model,
[4313.60s -> 4318.64s]  one like total store order or processor
[4318.64s -> 4323.44s]  consistent or partial store order.
[4323.44s -> 4327.16s]  And in total store order and partial store order,
[4327.16s -> 4333.16s]  you are going to relax the first ordering case, right?
[4333.20s -> 4334.64s]  So write to read order.
[4334.64s -> 4341.48s]  You're going to allow reads to happen in a write to x,
[4341.48s -> 4345.36s]  followed by a read to y to happen in a different order.
[4345.36s -> 4350.08s]  So total store ordering or partial store ordering,
[4350.44s -> 4359.60s]  this should actually be process consistency is actually
[4359.60s -> 4360.32s]  something different.
[4364.16s -> 4367.48s]  Which is PSO, all right?
[4367.48s -> 4369.76s]  And so now we're going to allow
[4369.76s -> 4376.44s]  you to overlap the read with the write
[4376.44s -> 4381.84s]  and somehow reduce the impact of the write latency.
[4381.84s -> 4385.72s]  So hide the write latency, right?
[4385.72s -> 4390.36s]  OK, so total store ordering basically
[4390.36s -> 4404.40s]  says total store ordering says process can read b
[4404.40s -> 4408.20s]  before it writes to a as seen by all the processes.
[4408.20s -> 4418.24s]  And the process of consistency, I thought, I mixed them up.
[4418.24s -> 4424.92s]  Sorry, process of consistency, I can't remember.
[4424.92s -> 4427.64s]  Yes, I think, anyway, I'll get it right.
[4427.64s -> 4428.60s]  I think this is right.
[4431.24s -> 4435.76s]  So this is the one that says, hey, you can reorder
[4435.76s -> 4436.84s]  the writes and the reads.
[4436.84s -> 4438.24s]  And the question, the difference
[4438.24s -> 4441.76s]  between total store ordering and process of consistency
[4441.76s -> 4444.80s]  is when everybody gets to see the stores.
[4444.80s -> 4447.00s]  In total store ordering, everybody
[4447.04s -> 4449.36s]  sees the stores in the same order.
[4449.36s -> 4453.56s]  In what I thought was partial store ordering,
[4453.56s -> 4455.60s]  some get to see it earlier.
[4455.60s -> 4460.56s]  But anyway, I'll make sure that I actually get, you know.
[4463.56s -> 4466.28s]  There is a joke that says something
[4466.28s -> 4469.44s]  like there are two hard things in computer science,
[4469.44s -> 4472.88s]  naming things and memory consistency.
[4472.88s -> 4473.64s]  So this is the.
[4473.68s -> 4477.68s]  Exactly.
[4477.68s -> 4481.60s]  So anyway, all right, so coherency
[4481.60s -> 4485.44s]  because we've got caches, consistency
[4485.44s -> 4490.12s]  because we need to have a way of understanding what
[4490.12s -> 4492.52s]  our programs mean.
[4492.52s -> 4501.04s]  We can do things that are even more aggressive than allowing
[4501.04s -> 4504.88s]  writes to bypass, allowing reads to bypass writes.
[4504.88s -> 4507.32s]  We can reorder writes.
[4507.32s -> 4510.44s]  And this is partial store ordering.
[4510.44s -> 4515.44s]  And in this case, if we are protecting
[4515.44s -> 4519.44s]  a change of a variable from a flag
[4519.44s -> 4521.04s]  we're trying to do synchronization,
[4521.04s -> 4523.08s]  then things could get reordered
[4523.08s -> 4528.96s]  and our behavior would be not what we expect.
[4528.96s -> 4533.28s]  So again, we're going to do this to improve performance,
[4533.28s -> 4537.96s]  the way that we can get rid of them all.
[4537.96s -> 4541.28s]  And by the way, who has a cell phone?
[4544.24s -> 4550.08s]  ARM uses what's called relaxed consistency.
[4550.08s -> 4555.56s]  So it doesn't have any of these orderings in its memory
[4555.56s -> 4556.80s]  consistency model.
[4556.80s -> 4560.32s]  And so how do you fix this problem?
[4560.32s -> 4563.96s]  Well, you put fences in, right?
[4563.96s -> 4568.40s]  How do you fix any problem in parallel computing?
[4568.40s -> 4571.84s]  You slow things down, right?
[4571.84s -> 4575.80s]  Locking is a mechanism for slowing things down.
[4575.80s -> 4579.64s]  You tell and fencing is even worse, right?
[4579.64s -> 4582.32s]  So fencing says, we're going to wait for the memory
[4582.32s -> 4584.72s]  system to quiesce, right?
[4584.72s -> 4590.04s]  And everybody is going to make sure that their memory,
[4590.04s -> 4594.28s]  all the processes are going to make sure
[4594.28s -> 4600.00s]  that they have issued all their memory, their operations
[4600.00s -> 4604.84s]  before we allow any of them to go beyond the fence, right?
[4604.84s -> 4606.88s]  So this is a hardware mechanism
[4606.88s -> 4608.00s]  that gets implemented.
[4608.00s -> 4609.52s]  Yeah?
[4609.52s -> 4611.56s]  Are search for some reason because of this?
[4611.56s -> 4615.08s]  Or is it just simple implementation?
[4615.08s -> 4618.96s]  Well, it's both a simpler implementation and also
[4618.96s -> 4620.28s]  higher performance.
[4620.28s -> 4622.76s]  But it means from a systems programmer's point of view,
[4622.76s -> 4624.48s]  it's more tricky to deal with.
[4624.48s -> 4628.52s]  But the way to deal with it is to add fences, right?
[4628.52s -> 4629.96s]  And you can have fences.
[4629.96s -> 4635.60s]  You can have store fences and load fences.
[4635.60s -> 4639.32s]  And this all gets pretty tricky and refer
[4639.32s -> 4641.84s]  you to lots of online documentation
[4641.84s -> 4643.68s]  if you want the details.
[4643.68s -> 4644.24s]  OK?
[4644.24s -> 4646.28s]  So the problem with data races is
[4646.28s -> 4651.32s]  that you've got two accesses to the same address.
[4651.32s -> 4653.24s]  At least one of them is a store.
[4653.24s -> 4656.64s]  And you don't have any synchronization
[4656.64s -> 4660.16s]  between the loads and the stores.
[4660.16s -> 4664.12s]  And so you have an unsynchronized program.
[4664.12s -> 4666.48s]  And whenever you have unsynchronized program,
[4666.48s -> 4668.36s]  then you can get data races.
[4668.40s -> 4670.16s]  And when you have data races, you
[4670.16s -> 4672.16s]  can get unintended behavior.
[4672.16s -> 4673.92s]  And so the question is, how do you
[4673.92s -> 4678.12s]  make sure that you can run programs
[4678.12s -> 4681.88s]  on a machine that doesn't support sequential consistency
[4681.88s -> 4685.24s]  but still have behavior that you can understand?
[4685.24s -> 4687.48s]  You make them synchronize, right?
[4687.48s -> 4691.20s]  So that's the whole idea behind data race
[4691.20s -> 4695.56s]  free programming is don't write code
[4695.64s -> 4699.00s]  that uses unsynchronized accesses to data.
[4699.00s -> 4700.92s]  Whenever you've got shared data, make
[4700.92s -> 4703.28s]  sure you put in synchronization.
[4703.28s -> 4705.08s]  And then the people who actually
[4705.08s -> 4707.24s]  write the synchronization libraries
[4707.24s -> 4710.00s]  have to understand the memory consistency
[4710.00s -> 4712.04s]  behavior of the underlying processor.
[4712.04s -> 4714.60s]  And the people who develop the compilers
[4714.60s -> 4716.36s]  have to understand these issues.
[4716.36s -> 4718.72s]  But you, as the application programmer,
[4718.72s -> 4721.88s]  can kind of just rely on the fact
[4721.88s -> 4726.28s]  that somebody who's much more knowledgeable than you
[4726.28s -> 4730.20s]  has done the right thing and given you a library call
[4730.20s -> 4731.88s]  that you could just make.
[4731.88s -> 4735.24s]  OK, so I think we're at time.
[4735.24s -> 4737.12s]  So we're going to get just, you know,
[4737.12s -> 4739.84s]  turns out that memory consistency really
[4739.84s -> 4742.76s]  needs to be dealt with at two levels, at the hardware
[4742.76s -> 4745.28s]  level, what the hardware provides,
[4745.28s -> 4747.36s]  but also at the language level.
[4747.36s -> 4750.52s]  So we'll say a few words about that next time.
[4750.52s -> 4752.40s]  See you next week.
