# Detected language: en (p=1.00)

[0.00s -> 11.28s]  So today, we're going to talk about something a little bit different, more of an algorithms
[11.28s -> 15.80s]  lecture than anything else actually, not a systems lecture. And reminder, when we get
[15.80s -> 21.24s]  together next week, we're basically moving the course back to hardware for a little
[21.24s -> 24.68s]  bit. So Kuma is going to take over next week. We're going to talk about cache coherence
[24.68s -> 26.48s]  I think is the next lecture. Is that right?
[26.48s -> 27.48s]  Yeah.
[27.96s -> 33.24s]  Actually, you're going to do Spark first. Yeah, because it follows the, yeah. Okay, so sorry,
[33.24s -> 38.20s]  we're not going to actually talk about hardware just yet. But Kuma will be lecturing next
[38.20s -> 43.16s]  time. Okay, so but everybody's doing okay? All right, let's kind of get to it then.
[43.16s -> 47.20s]  So this lecture, sometimes like people come up to me and go, I've never thought
[47.20s -> 51.48s]  about things that way. So what we're going to talk about today, about everything in
[51.48s -> 60.52s]  the course so far has been thinking about parallel computation in terms of threads
[60.52s -> 66.16s]  or in terms of a very, very simple sort of organization of let's do something to every
[66.16s -> 69.88s]  piece of data in an array. That's basically all we've done so far is I've given you
[69.88s -> 76.08s]  arrays of stuff and you've essentially performed a loop body on those arrays or you've thought
[76.08s -> 79.48s]  about, okay, I'm going to create these threads and this is what this thread is going
[79.48s -> 86.88s]  to do. So today I want to elaborate on and go into more detail about this way of thinking
[86.88s -> 91.32s]  of I have a big array and we're going to do something for everything on the array.
[91.32s -> 94.88s]  Except instead of just run a function on every element of array, we're going to start
[94.88s -> 104.16s]  expressing computation in terms of a richer set of primitives on arrays or collections
[104.16s -> 110.00s]  of data. Okay? So first of all, just to ground this, remember last time when we talked about
[110.00s -> 113.88s]  GPUs and I said something about like the total number of execution contexts is about
[113.88s -> 120.84s]  163,000 on one chip? So that means that you probably want to be working on data sets
[120.84s -> 125.00s]  of at least that size, right? Otherwise you're not going to get the full complement
[125.00s -> 132.36s]  of parallelism or latency hiding. So hopefully at this point in the class I have established
[132.36s -> 138.56s]  that even if you're working on a single chip, you need a lot of parallelism. Like you need
[138.56s -> 144.02s]  to actually be working on programs that have hundreds of thousands or more pieces
[144.02s -> 148.56s]  of parallel things to do, otherwise you're going to be in a bit of trouble in actually
[148.56s -> 152.02s]  using these things. So what we're talking about today is not going to be outlandish
[152.02s -> 156.68s]  at all, right? So if you're just going to write code for a reasonable size GPU,
[156.68s -> 162.08s]  that's what we're talking about, you got to have parallelism in the 200,000 regime.
[162.08s -> 169.52s]  Okay. All right. So also so far in this course, just a little bit more of background,
[169.52s -> 174.24s]  I think that whenever we throw a piece of code up on the board or I ask you to do a programming
[174.24s -> 180.20s]  assignment, step one is almost always, where the heck are the dependencies of my program?
[180.20s -> 185.16s]  Because if I know the dependencies or equivalently if I know where there are not dependencies,
[185.16s -> 189.24s]  that's where I'm going to get the parallelism. So here's an example of just like a simple set
[189.24s -> 193.44s]  of expressions and I just wrote out the fundamental dependencies here in this graph.
[193.44s -> 199.00s]  Like I cannot execute an operation that's a node in that graph until I have executed the prior
[199.00s -> 203.60s]  ones. And then you know in part or I guess part B of your assignment, that's in fact
[203.60s -> 210.76s]  your understanding dependencies and respecting them. Okay. So everything we're going to do
[210.76s -> 220.76s]  today is about writing code in some operations where we're going to just assume that someone
[220.76s -> 229.36s]  has really good widely parallel implementations of those operations. So instead of thinking
[229.36s -> 234.68s]  necessarily about the dependencies at a fine grain, I'm just going to say I'm going to try
[234.68s -> 240.80s]  and boil my program, my algorithm down into calls to these functions. And these functions
[240.80s -> 246.16s]  are known to be written in a manner that they can execute highly, highly parallel.
[246.16s -> 251.16s]  So the thought process is well if all my code calls these functions and all these
[251.16s -> 256.08s]  functions are highly parallel, then any of my programs are also highly parallel. That's the
[256.08s -> 261.88s]  basic gist of things. Okay. And you do do this all the time, right? For any of you
[261.88s -> 266.44s]  writing NumPy code or pick your favorite tensor language or something like that,
[266.44s -> 274.40s]  you allocate variables like some big array A or some big array B and you use operations
[274.40s -> 279.64s]  that are operating on those arrays, those collections, right? Like in this class,
[279.64s -> 286.32s]  in this example if A, B and C are these big NumPy vectors, then the operation plus is
[286.32s -> 290.64s]  an operation that you hope is implemented as fast as possible on those two arrays.
[290.88s -> 293.92s]  You don't think about threads, you don't think about dependencies or anything like that.
[293.92s -> 301.56s]  So I want to just generalize this notion of a NumPy array a little bit and to give it a name,
[301.56s -> 305.48s]  I'm going to call, I'm not going to call it tensors or vectors because the kind of vector
[305.48s -> 314.72s]  has a specific meaning in C++. I like the term sequences which I've adopted from other
[314.72s -> 319.88s]  researchers and other colleagues of mine. And so I want you to just think about a sequence
[320.12s -> 326.96s]  as an ordered collection of elements. So it's not a set, it's ordered. And different languages
[326.96s -> 333.68s]  have different data types that basically embody this concept. Like in C++, there's actually
[333.68s -> 341.24s]  a sequence in Scala, it's lists, in Python you can use DataFrame or a NumPy array. Arguably
[341.24s -> 346.76s]  in PyTorch or TensorFlow you have tensors. The important thing is that unlike arrays,
[347.16s -> 354.40s]  in an array you can just say I want the element 43 at this position and you get access to it.
[354.40s -> 363.56s]  Unlike arrays, programs can only access elements of sequences through very specific operations.
[363.56s -> 368.00s]  And that's the big difference. So when you write your ISPC code, you write your CUDA code,
[368.00s -> 372.52s]  your C code, if I give you an array, even though you can think about it as for every element in
[372.52s -> 378.80s]  the array do X, you could actually, in your code, you could write a sub i, which means you
[378.80s -> 383.80s]  could access any element at any time. Now we're going to restrict that. And the ability to
[383.80s -> 389.92s]  access a at any time is the way you create dependencies that can really blow up your program.
[389.92s -> 395.32s]  You're like, oh, this iteration of the loop accessed a sub i, the next one accessed a sub
[395.32s -> 399.12s]  i again, maybe there's a dependency there. So we're just not going to let you do that
[399.44s -> 405.24s]  so that your code is free of dependencies. So let's talk about the first of these
[405.24s -> 411.00s]  specific operations that you are very familiar with. We just didn't really call it as such.
[411.00s -> 419.64s]  So that's an operation called map. Almost every piece of code we've written in this class
[419.64s -> 426.56s]  so far was basically a map. It's taking a function and applying it to every element of an input
[426.56s -> 431.52s]  array to produce an output array. Now, how many people have seen this more functional
[431.52s -> 435.28s]  language syntax, you know, like the A or B syntax? Have you seen that? So a couple
[435.28s -> 439.96s]  people. Has everybody seen it? I've not seen it. Okay. So plenty of people have not seen it.
[439.96s -> 447.64s]  So map is a higher order function because it takes input variables. So in this case,
[447.64s -> 453.48s]  the arguments to map are the input array A and maybe B and this output array B. And
[454.24s -> 461.56s]  map also takes another argument, which is a function. So let's just say the input collection,
[461.56s -> 469.32s]  excuse me, the input collection is a collection of integers. So then, and now imagine a function
[469.32s -> 475.68s]  that adds two to an integer. Okay. So it takes this input and integer f of x,
[475.68s -> 483.44s]  and it returns x plus two. So that function has signature int to int, right? Input argument
[483.44s -> 488.32s]  is an integer, the output argument is an integer. So this notation here, this A arrow B,
[488.32s -> 493.56s]  just says it's a function that takes this input something of type A, like an integer,
[493.56s -> 500.68s]  and output something of type B, which in my example here is also an integer. So if we look
[500.68s -> 506.88s]  at the actual signature of the map function, the map function takes two arguments. It takes
[507.04s -> 515.64s]  a function like add two to something, which in this case, let's say is a, in my, yeah,
[515.64s -> 521.56s]  so sorry, in my, in this program, what is my program doing? It's adding 10, excuse me. So
[521.56s -> 527.68s]  in this case, my function is adding 10. So the function takes an integer to an integer. And so
[527.68s -> 534.76s]  map takes that function, which is integer to integer, plus a sequence of integers and produces
[534.76s -> 544.44s]  what? An output sequence, which is of type sequence of integers. If I gave map a function
[544.44s -> 549.40s]  that took integers to strings and applied it to a collection of integers, what would my output
[549.40s -> 555.04s]  type be? Collection of strings, right, exactly. So all I'm doing here is this is a functional
[555.04s -> 560.44s]  notation that says map takes two arguments. The first argument is a function of A to B. The
[560.44s -> 566.24s]  second argument is a sequence of type A, and the output is a sequence of type B. So if you
[566.24s -> 570.28s]  take programming languages or a bunch of other classes, you'll see that. And it's not quite
[570.28s -> 575.76s]  so elegant in other languages like C++, but we have all that functionality. Like, okay,
[575.76s -> 579.64s]  there's no actual, like I have a sequence here, which I'm actually representing as an array,
[579.64s -> 585.40s]  and I have a function from the standard library transform, which takes, in some sense,
[585.40s -> 590.80s]  this is a pointer, this is an iterator starting at the beginning of A, and this is A plus eight,
[590.80s -> 593.40s]  so one, two, three, four, five. This is the end of the array. So basically, this is just
[593.40s -> 600.76s]  defining a sequence. And it says, please apply the function F to all elements of the sequence
[600.76s -> 606.00s]  and put your results here and pointed to you by this iterator. So it's the same idea,
[606.00s -> 613.40s]  just like everything in C++ and a little bit janky or syntax. A lot more clean in a functional
[613.44s -> 619.20s]  language like Haskell, I have a sequence A, I have a function X, which takes X to X plus 10,
[619.20s -> 628.56s]  and then if I map F onto A, I get a new sequence B. Despite me ripping on C++,
[628.56s -> 635.52s]  I would still write this code. Okay, so here's a question. Now put your head, you know,
[635.52s -> 641.12s]  as the implementer of map. So now you're implementing the map library. Is this something
[641.28s -> 648.60s]  that you feel very confident about being parallel? Absolutely, because by the definition of the
[648.60s -> 659.44s]  function, every invocation of the argument F is parallel. And by the way, F is only given access
[659.44s -> 665.28s]  to individual elements of the array, right? Like F is just X plus 10. The only thing F
[665.32s -> 672.88s]  knows about is X. So there's nothing F can do accidentally, for example, to create a dependency.
[672.88s -> 679.08s]  And not only that, the implementer of F doesn't even have to think about operating on collections.
[679.08s -> 684.76s]  The implementer F just says, just give me one element and I'll tell you how to process it.
[684.76s -> 690.04s]  That's a nice clean way to think, right? Okay. So how would you, let's say I gave you,
[690.64s -> 696.24s]  I told you to implement map, and I said it needs to work on sequences of arbitrary length,
[696.24s -> 702.68s]  and you're going to get some black box function F of X. How would you implement map,
[702.68s -> 710.88s]  if you want to do it in parallel? Yeah, go for it. Yeah.
[710.88s -> 726.68s]  So you could actually say, if I had the source to F, I could recompile F in a SIMD mode. That'd
[726.68s -> 731.08s]  be pretty cool. But that SIMD thing is only going to, let's say, it's only going to do
[731.08s -> 735.24s]  eight things at once. So what are you going to do about the fact that I could give you a
[735.32s -> 744.32s]  collection that might have a hundred thousand things in it? Okay, I can spawn threads. I could spawn
[744.32s -> 748.68s]  like a, I could create a worker pool of threads and those threads could all call execute this
[748.68s -> 752.60s]  thing. You know, when I first asked the question, I was thinking about is imagine I gave
[752.60s -> 758.60s]  you a C++ function already, you know, a header file, you know, you didn't necessarily even have
[758.60s -> 762.36s]  to recompile the thing. You just call F and you can call it sequentially or you could spawn a
[762.36s -> 766.12s]  bunch of threads and call it in parallel. You know, in general, like here's my little
[766.12s -> 772.44s]  implementation of map given F onto sequence S would be partition sequence S into P smaller
[772.44s -> 779.36s]  sequences. Let's say if I had P processors or P threads and for each subsequence SI in parallel,
[779.36s -> 786.64s]  sequentially just apply map to produce a partial output and now I can catenate all the outputs,
[786.64s -> 790.60s]  which is basically like I'm going to create a worker thread pool and those threads are
[790.64s -> 796.36s]  going to process different elements of the sequence. Nothing too fancy here. So this is
[796.36s -> 804.36s]  pretty easy. All right, so now let's go to some more interesting operations. So here's
[804.36s -> 810.48s]  another operation that's extremely important used all over the place. It's called fold. So fold,
[810.48s -> 820.28s]  if you read that signature, takes a function and that function F now, instead of taking A to B,
[820.48s -> 827.72s]  takes a pair. It takes an A and a B and produces a B. So I see people nodding, so that's good.
[827.72s -> 833.84s]  And so fold takes sort of like a starting element B, that's not all that important right now,
[833.84s -> 845.80s]  but it takes that function F and a sequence A and produces what? A single element. I'm having
[845.80s -> 853.12s]  a little bit of a problem here. One second, okay. So if we write this in Scala, here's an example.
[853.12s -> 858.84s]  Fold left takes an A and a B. Yeah, exactly. So it takes a sequence of A's,
[858.84s -> 869.08s]  a function that takes an A and a B, produces a B, and then essentially iteratively applies
[869.08s -> 874.68s]  this. So hopefully, just make sure that you can confirm that fold left with the function 10,
[874.68s -> 882.24s]  sorry, with the function plus, not the function 10. The function plus takes an A and an A to an A,
[882.24s -> 889.44s]  right? Int and int to an int. So this function is expecting a sequence of integers and produces
[889.44s -> 896.64s]  a single integer as output. And the result is what? The sum of everything in the array. So in
[896.64s -> 903.08s]  this case, fold of 10 on the sequence is 53. So here's an interesting question.
[903.92s -> 911.12s]  Can you parallelize fold? So I see some disagreement, actually. Some people are like,
[911.12s -> 918.48s]  no way. Other people are, yes, we can. It's like the first problem of the first vector.
[918.48s -> 928.08s]  We can use some small sums and then sum the small sums. Okay, so good point. So we have
[928.08s -> 933.04s]  definitely computed the sum of a bunch of integers in parallel in this class. Absolutely.
[933.92s -> 939.56s]  Does anybody agree with this? Anybody disagree with this? We have a little bit of disagreement.
[939.56s -> 944.80s]  We don't know that the function we're getting passed is associative. We don't know that the
[944.80s -> 950.28s]  order in which we apply it will be the same every time. Like if we have a list of true
[950.28s -> 957.68s]  and false and our function is exclusive or, then we could get... Right, so if you interpreted
[957.68s -> 966.96s]  my question as, can fold plus be paralyzed? Yeah, we can think of some ways to do it. As
[966.96s -> 971.92s]  you pointed out, we've done it. But this is general fold on any arbitrary function f,
[971.92s -> 978.24s]  and we don't know the properties of f. And a parallel implementation, for example,
[978.24s -> 986.72s]  that chop the sequence up into sub-sequences, in parallel, folded maybe sequentially,
[986.76s -> 992.96s]  the sub-sequences, and then folded those results is not guaranteed to get the same answer unless
[992.96s -> 997.28s]  you know certain properties about f. And in particular, in this case, f needs to be
[997.28s -> 1010.92s]  associated. So there's some other versions of fold where you give fold a little bit more
[1010.92s -> 1018.52s]  information. We say, well, I'm gonna give you a combiner function also. And in this case,
[1018.52s -> 1022.28s]  the combiner... Well, not in this case. The combiner function actually needs to take b to b.
[1022.28s -> 1032.08s]  And so now I can split up my input sequence, do all of these sequential but parallel versions
[1032.08s -> 1039.48s]  of this, and then on each thread, I'm gonna get a single b as output. And then I can combine
[1039.64s -> 1048.24s]  those if I want to. But I pushed this off to the user a little bit, right? Often it's the case
[1048.24s -> 1056.04s]  that you care about the f being b to b to b, like the plus operator. And if f is a pair
[1056.04s -> 1061.88s]  of b's to a b, then you basically also can specify f and combine with one function. Does
[1061.88s -> 1068.20s]  that make sense? And so as long as that f is associative, you can perform a parallel fold by
[1068.20s -> 1072.80s]  just giving a single f. And the way it will be implemented is it'll apply f, f, f, f, f,
[1072.80s -> 1079.80s]  and then it'll use f to combine everything. Note that f does not need to be commutative,
[1079.80s -> 1089.00s]  right? If I implement it correctly, I'm still doing all the math in the same order. It just
[1089.00s -> 1097.56s]  needs to be associative. So all of a sudden, for some f's... So for some potential f's,
[1098.24s -> 1106.52s]  we've got a parallel implementation of this thing. And now we have the ability to map
[1106.52s -> 1111.04s]  data in parallel, and we have the ability to do sort of forms of data reduction,
[1111.04s -> 1118.44s]  sums and averages and stuff like that. Now, there's actually something that's cool that I
[1118.44s -> 1122.40s]  don't have on the slides or in this lecture, but think about the following. Imagine you wanted
[1122.44s -> 1131.48s]  to do a dot product or something like that. So you first had a map, and the map actually
[1131.48s -> 1138.00s]  took a pair of elements. That's a bad example. But imagine you want to multiply all numbers by
[1138.00s -> 1144.44s]  like 10. That's a map. And then compute the sum. So you would write that in these primitives,
[1144.44s -> 1151.88s]  right? As map times 10 on the sequence. You get out what? A new sequence. And then you apply
[1151.88s -> 1162.08s]  fold with the plus operator to compute the sum. But if I know the definition of these operators,
[1162.08s -> 1167.84s]  and I saw a program that was map and then fold, you could imagine that I could perform
[1167.84s -> 1173.00s]  a transformation to that function, which turned it into something kind of interesting,
[1173.00s -> 1180.52s]  right? Like I would have turned it into a fold that actually did the multiply by some number,
[1180.60s -> 1185.04s]  and then immediately folded in the result. So you can start thinking about transformations that I
[1185.04s -> 1190.24s]  could do that don't actually require two passes over all the data, provided that you know the
[1190.24s -> 1195.56s]  definition of map and fold. And that's actually what these more sophisticated JIT compilers are
[1195.56s -> 1201.00s]  actually doing on your PyTorch code and stuff like that, just so you know. Okay. So let
[1201.00s -> 1206.52s]  me give you another one. So we have fold, which is sequence to scalar. Now let's think
[1206.52s -> 1215.12s]  about scan. So scan is sequence to sequence for an associated binary operator that takes an A and
[1215.12s -> 1220.44s]  an A to an A. So let's just kind of simplify here. Let's just say that operator's plus again.
[1220.44s -> 1230.32s]  And so what scan computes is the repeated application of that operator up to and including
[1230.32s -> 1235.52s]  the current element. Okay? So if we look at this result here, the result of scan with a
[1235.52s -> 1242.12s]  plus operator is the first element of the output is the sum of all elements up to and including
[1242.12s -> 1249.28s]  this element in the input. The second element of the output sequence is the sum of all elements
[1249.28s -> 1256.52s]  up to and including the same position in the input element. Okay? So the last element of the
[1256.52s -> 1263.72s]  scan is going to be the fold, but now I'm producing all of the partials. So first of all,
[1263.72s -> 1268.16s]  you see the difference between fold and scan. Like, fold produces a single value.
[1268.16s -> 1274.04s]  Scan gives you all the partials. And if I had to implement scan sequentially,
[1274.04s -> 1277.28s]  it might look a little bit like this. You know, like you'd do this in a second.
[1277.28s -> 1281.88s]  The first element of the output is the first element of the input, and for i equals 0 to n,
[1281.88s -> 1287.16s]  the output element is the previous output element plus the new input element.
[1287.16s -> 1296.96s]  So the question is going to be, you know, is this parallel? And things start getting a little bit
[1296.96s -> 1301.44s]  trickier now. Before we move on, just as a detail, sometimes in some libraries,
[1301.44s -> 1306.32s]  a scan will be an exclusive scan, and sometimes it will be an inclusive scan.
[1306.32s -> 1313.52s]  I have inclusive here, which is up to and including the current element. An exclusive scan
[1313.52s -> 1319.20s]  is up to but not including, so it excludes the current element. If you have an exclusive scan,
[1319.20s -> 1327.68s]  how do you compute an inclusive scan? Exactly. Like, you just take the output of the exclusive
[1327.68s -> 1333.04s]  scan and you would add the current element. Cool. Okay. So you probably have a sense of
[1333.04s -> 1337.92s]  where we're going here. Now we need to figure out how to parallelize something like scan.
[1337.92s -> 1342.84s]  All right. So let's think about this a little bit. So, and again, let me get a little bit
[1342.84s -> 1348.56s]  more formal. Here are two different definitions of scan, inclusive and exclusive.
[1348.56s -> 1355.84s]  The output element is always just going to be the repeated application of that binary operator
[1355.84s -> 1368.04s]  to everything that I've gotten to so far. Okay. All right. So ignore my broken build.
[1368.24s -> 1375.00s]  Let's just think about it. How would you go about this? This is going to be an inclusive scan,
[1375.00s -> 1380.64s]  by the way. And like you said, we can always just minus the element if you want the exclusive.
[1380.64s -> 1386.64s]  So let's think about this. How would you do it? Any thoughts on how you go about it?
[1386.64s -> 1399.28s]  Okay. So you're going to find the total sum. Okay. Well, we actually already have a really
[1399.28s -> 1404.48s]  good implementation of total sum. We know how to do that. You do divide and conquer
[1404.48s -> 1408.26s]  onto your threads and add them up. Okay. So let's just say that we have a subroutine
[1408.26s -> 1413.24s]  that computes the total sum. That's fine. And that would take cost what? That would
[1413.28s -> 1418.32s]  be basically if this is of length n, it would be n over number of threads would be the total
[1418.32s -> 1423.64s]  time, right? Because that would just break the array up into t pieces. Everybody computes a
[1423.64s -> 1428.64s]  partial sum. I combine the results. Pretty easily paralyzable. So yes, we can do that.
[1428.64s -> 1438.52s]  Does that help? It's a reasonable starting point. Okay. Let's keep going. Any other ideas?
[1438.52s -> 1453.96s]  Yeah. Okay. So one thing we could do is we could just say, let's just say we magically had the
[1453.96s -> 1458.16s]  sum of the first half. Is that right? Is that what you're saying? Then if we knew the sum
[1458.16s -> 1464.28s]  of elements a0 through a7, I could definitely apply that sum to all of these elements in
[1464.28s -> 1480.84s]  parallel. It's kind of interesting. Okay. And so basically that's, you're just taking this idea
[1480.84s -> 1485.40s]  of dividing the thing in half because essentially if we performed a scan over the first half,
[1485.40s -> 1490.00s]  then we could just take that result and then we add it into the scan results for this half.
[1490.00s -> 1496.84s]  That's correct. And we could break that up into as many pieces as we want. Yep.
[1496.84s -> 1515.72s]  Yeah, that fold is basically this way to do the parallel sum, right? Okay. I like this.
[1515.72s -> 1527.60s]  I think there's some good answers here. What if I make it even harder and say,
[1527.60s -> 1534.76s]  let's imagine we're running this on a GPU. And I'm going to come back to some of those
[1534.76s -> 1538.36s]  ideas that you just told me. And we have the same number of threads as we have elements.
[1538.96s -> 1551.56s]  Let's think about this in the massive parallel machine. I want a really parallel solution.
[1551.56s -> 1566.32s]  So let me just start hinting at a few things. What did I do? So I have parallelism that's now
[1566.80s -> 1579.60s]  O of N and I just added up every neighbor. Now what did I do? So the notation in my slide
[1579.60s -> 1586.76s]  is a value. I'm showing you all the values that go into the sum at a current location,
[1586.76s -> 1592.56s]  right? So after the first step, the sum of A0 and A1 is in the second element. And after
[1592.56s -> 1598.64s]  the second step, the sum of A0, 1, and 2 is in the second element. Or really,
[1598.64s -> 1609.88s]  I should look here. After two steps, the sum of A0 through 3 is there. And if I keep going,
[1609.88s -> 1623.24s]  at some point, I have my scan. So first of all, what is the... If I did a scan sequentially,
[1623.24s -> 1630.12s]  how much work do I have to do? Remember that C code I just put on the slide? How much work
[1630.12s -> 1634.48s]  did I do if the size of the array was N and the cost of the algorithm was? Well then,
[1634.68s -> 1640.16s]  I just walked on my array. What's the cost of this algorithm in terms of the amount of work done?
[1640.16s -> 1647.84s]  N log N. Right? Because the first step I do N work, or N over 2 work, then I do N over 4 work,
[1647.84s -> 1654.80s]  then I do... So... Oh no, sorry, sorry, I don't do N over 4 work. I basically do N over 2 work,
[1654.80s -> 1662.00s]  and then I do similar work. Yeah. So it's N log N cost. I have O of N work, and I have N steps.
[1662.00s -> 1669.16s]  Now, what's the longest chain of sequential steps, which is often something we call span
[1669.16s -> 1674.00s]  in a parallel algorithm? If you had an infinite number of processors, this would take log N time.
[1674.00s -> 1684.48s]  The total amount of computation you do is N log N. So I'm doing O of N work, N steps.
[1684.48s -> 1695.56s]  I don't like the fact that I asymptotically increased work that I'm doing. Because if I'm
[1695.56s -> 1706.00s]  a parallel machine, and I have N as a million, log N is non-trivial. So it turns out that
[1706.00s -> 1711.04s]  there's a very... There's a way... This is actually a pretty clever algorithm that came up
[1711.04s -> 1717.20s]  with... I think this was Guy Bullock that did this, where I can do... I can keep my log N
[1717.20s -> 1725.40s]  span, and I can be more clever, and get it in O of N work. And here's a little bit what
[1725.40s -> 1730.12s]  the algorithm looks like. And this is something that I think is a lot easier just to stare at
[1730.12s -> 1738.36s]  on your own. But the gist of this is that there's two phases now. There's like,
[1738.36s -> 1742.84s]  if you think about there being a combining tree in phase one, and then like a splatting
[1742.84s -> 1749.76s]  tree, an inverse tree on phase two. And if you read documentation of this on the Internet,
[1749.76s -> 1754.08s]  people will say that there's an up sweep phase and a down sweep phase. So the up
[1754.08s -> 1760.64s]  sweep phase is computing these partial sums in this tree, where... Kind of to the point
[1760.64s -> 1765.40s]  that you all are alluding at, look at this. Here is the sum of the first... The second
[1765.44s -> 1771.68s]  half of the array. Here is the sum of the first half of the array. And then along the way,
[1771.68s -> 1777.64s]  we've computed some partial sums. So if you look at it, I have the first half here,
[1777.64s -> 1781.72s]  sorry, the second half there, the first half there, and then I still have these
[1781.72s -> 1789.80s]  interesting partials throughout the result of my combining tree. And then very much to your
[1789.80s -> 1793.28s]  point earlier, where you said, what we're gonna do is we're gonna take that partial sum,
[1793.80s -> 1798.68s]  the back half of the array, and if we just had some way to apply it to the appropriate elements,
[1798.68s -> 1803.48s]  or the partial sum from the front half of the array, for that matter, and apply it to the
[1803.48s -> 1809.48s]  appropriate elements, we could just kind of rebase everything. So look what happens. After
[1809.48s -> 1816.36s]  you do this up sweep, first of all, we're almost done here. We're done here. And so
[1816.36s -> 1823.32s]  we take that partial sum and shove it over here. Now at this step, I just overwrote. And
[1823.32s -> 1829.80s]  then it basically becomes, take your partial sum and bring it back, and then propagate it.
[1829.80s -> 1836.16s]  That's kind of the way the combining tree works. And if you look at this, this actually
[1836.16s -> 1843.08s]  only does a total of O of N operations, because the first step is N, the next step is N over 2,
[1843.08s -> 1848.72s]  the next step is N over 4, and so on and so on. And that telescoping series, as you might know
[1848.72s -> 1855.44s]  from 161 or something like that, is O of N. Now there's an extra coefficient, and it's actually
[1855.44s -> 1860.36s]  two times that, because I take twice as many steps. So it's still log N in steps,
[1860.36s -> 1868.08s]  but it's actually, there's a constant there, which is 2. So this is something we're gonna
[1868.08s -> 1872.64s]  have you implement as your warm-up CUDA programming assignment, before we get into the meat of
[1872.68s -> 1882.64s]  assignment 3. So you'll be intimately familiar with this. Now, yeah, so I think some interesting
[1882.64s -> 1888.68s]  things to point out is, it's awesome if you're a theoretician. O of N work, log N steps,
[1888.68s -> 1899.56s]  but if you look closely, there's an extra, there's a factor at 2 here. There's a factor
[1899.56s -> 1905.96s]  at 2 here. And there's also some other things that are kind of not great, like you are not
[1905.96s -> 1910.00s]  using all your processors every single moment. So even though I said let's make use of an
[1910.00s -> 1914.44s]  infinite number of processors, you know, as the series goes down, you're using fewer and
[1914.44s -> 1918.24s]  fewer of them every step. And also data is kind of moving around all over the place.
[1918.24s -> 1924.32s]  So there's some reasons why, if I actually ask you to implement this in practice, like in
[1924.32s -> 1928.36s]  your homework, you're actually just gonna do this, this algorithm. You're not meant to like
[1928.36s -> 1932.28s]  make it really fast. It's just more of a programming exercise. If you were actually
[1932.28s -> 1937.20s]  working at NVIDIA and trying to make the library for scan, you would have to work
[1937.20s -> 1942.20s]  a little bit harder. Okay. And now let's talk about that just a little bit. So now
[1942.20s -> 1950.68s]  let's think about a simpler problem, actually, paralyzing this onto two cores. So, you know,
[1950.68s -> 1953.64s]  let's just say we cut it in half. We only had two processors and we were running this
[1953.64s -> 1958.12s]  algorithm. Well, first of all, you have perfect workload balance. That's not a problem.
[1958.88s -> 1963.16s]  You actually don't have a lot of communication between those processors, which is pretty cool.
[1963.16s -> 1968.32s]  If you look carefully, look at those red arrows, the only data that ever moves are two elements.
[1968.32s -> 1979.64s]  But you are bouncing around memory all over the place. So even though one processor is doing
[1979.64s -> 1982.64s]  this side and the other processor is doing the other side, you're bouncing around memory.
[1982.64s -> 1986.56s]  And I think as you're starting to get in this class, that's probably not such a great idea.
[1986.72s -> 1993.52s]  Okay. And so in the spirit of doing the simplest thing first, if you gave me two threads and said
[1993.52s -> 1999.60s]  compute a scan, I would probably do something like this. I would divide the array in half,
[1999.60s -> 2005.12s]  I would do two sequential scans, which is O of N, running straight through memory,
[2005.12s -> 2011.88s]  I would get the base for the first half, and then I would parallelize the application
[2012.04s -> 2017.72s]  of the base to the second half of the array. If we're on a shared memory multiprocessor,
[2017.72s -> 2023.12s]  the fact that information computed by P2 has to get over to P1 is not that big of a deal. I'm
[2023.12s -> 2029.76s]  just reading from the same memory system. So the thing that you would do probably in
[2029.76s -> 2032.80s]  one hour if I gave you this as an assignment would probably work pretty well.
[2032.80s -> 2046.84s]  Yeah. Oh, sorry, I didn't draw my line. Now it's clear. Okay. So let's think about
[2046.84s -> 2052.64s]  a different type of machine. Let's imagine that you were trying to compute a scan in
[2052.64s -> 2060.24s]  ISPC or actually a scan in CUDA where you kind of know that all of your threads,
[2060.40s -> 2066.80s]  your workers, are actually operating in SIMD. So instead of saying I have two threads and
[2066.80s -> 2069.16s]  they're going to be on different cores or eight threads are going to be on different cores,
[2069.16s -> 2075.40s]  just imagine if your workers were SIMD lanes. So let's take a look at this piece of code.
[2075.40s -> 2078.80s]  It's written in CUDA, but you can think about it just as ISPC if you want. It really
[2078.80s -> 2084.36s]  doesn't matter. This is pretty interesting. Look at this. So this is scan, it's called
[2084.36s -> 2090.24s]  scan warp because this is something that's called by every CUDA thread individually
[2090.24s -> 2094.92s]  in all the threads in a warp, which sort of share the same instruction stream. Or
[2094.92s -> 2099.84s]  you can think about this as if I wrote this as ISPC code, it would be scan of a program,
[2099.84s -> 2106.32s]  a gang. And so every instance in a gang would be calling this function. So for now,
[2106.32s -> 2111.92s]  just imagine that like a bunch of threads are calling this function. And so first of all,
[2111.96s -> 2119.56s]  every thread computes its lane, which is basically its program ID or your program index. And since,
[2119.56s -> 2124.76s]  remember last time, your thread index in CUDA could be anything in your thread block. You
[2124.76s -> 2130.32s]  can make a thread block of 2,000 threads or 256 threads, but those 2,000 threads were going
[2130.32s -> 2137.04s]  to be running in these little groups of 32 warps. So what I do here, and this is pretty low
[2137.08s -> 2145.88s]  level hacky code, is I first compute my ID in the warp, which is to take my thread ID, mod it by 32
[2145.88s -> 2152.80s]  to get my actual location in the warp. So now I'm making assumptions about the underlying
[2152.80s -> 2161.12s]  implementation. And so then I write basically this five lines of code, which says, if my lane
[2161.24s -> 2168.00s]  is zero, do this. If my lane is one, do this, and so on and so on. And can you confirm that
[2168.00s -> 2176.20s]  this code takes five steps? So first of all, why is there five steps? Log of 32, right?
[2176.20s -> 2183.30s]  So there's five steps, and every step is just an addition of two numbers. And based on my thread
[2183.34s -> 2194.22s]  ID or my index in this case, some threads stop participating after a while. So how much work
[2194.22s -> 2202.50s]  does this do? And log n, right? Like same thing, right? In other words, there are five
[2202.50s -> 2206.14s]  instructions. Let's ignore the if statements. Let's just say they're free for a second.
[2206.14s -> 2213.06s]  There's five instructions of actually doing math. That's log 32. And every single one of these
[2213.46s -> 2221.90s]  lines of code is doing how much work? In a warp. It's one thing per thread, and there's 32 threads,
[2221.90s -> 2226.50s]  so n, O of n, total, right? But every thread is doing one thing, but there are n threads.
[2226.50s -> 2235.38s]  So I'm doing overall n log n work, and my span is five. Or in other words, like if you
[2235.38s -> 2238.22s]  put a stopwatch on this program, you would say it gets done in five steps.
[2242.14s -> 2252.50s]  So what would this code look like if I changed it to use the more advanced O of n algorithm?
[2252.50s -> 2261.14s]  How many lines would it be? It would be five lines for the up sweep, and then another five
[2261.14s -> 2269.38s]  lines for the down sweep. So wait a minute. This is weird, right? So my n log n algorithm
[2269.38s -> 2278.82s]  took five cycles. My O of n algorithm takes how many cycles? Ten. How can that be the case?
[2278.82s -> 2288.78s]  Coefficient. Well, yeah, the coefficient's two. But what happened? I'm using the same
[2288.82s -> 2295.58s]  number of resources, right? In some sense, I have this SIMD block of execution units
[2295.58s -> 2304.66s]  that can do 32 things at once, and that 32 things at once takes five cycles to do
[2304.66s -> 2314.42s]  n log n work, and ten cycles to do O of n work. So the fact that the only thing this
[2314.46s -> 2320.10s]  processor can do is the same instruction. So the fact that I'm doing some redundant work
[2320.10s -> 2324.74s]  is actually kind of okay. The fact that I did better would mean that I would have all this
[2324.74s -> 2330.90s]  underutilization of my SIMD lanes. And so I do less work, but I spread it out over multiple
[2330.90s -> 2337.74s]  sort of highly incoherent instructions. So it's a loss. So you really have to think,
[2337.74s -> 2344.74s]  if you're implementing a library for scan, is depending on how the parallel work gets
[2344.74s -> 2350.06s]  mapped to a machine, different approaches may be very different. Like if I'm running
[2350.06s -> 2354.62s]  on a machine that has thousands of independent processors, I might use that work efficient
[2354.62s -> 2358.62s]  formulation. If I'm running on a machine with just two processors, I'd probably do
[2358.62s -> 2362.26s]  the simple thing that you all suggested, you know, like I divide it in half, compute a scan,
[2362.26s -> 2366.62s]  compute a scan, move the data over, and then finish this up. If I'm running on a SIMD processor,
[2366.62s -> 2369.66s]  this n log n thing is actually the best thing to do.
[2396.62s -> 2404.78s]  Well, I think that's... The problem that you're solving is not... Well,
[2404.78s -> 2409.74s]  the problem that I just discussed is a different problem. The problem is the following. Imagine
[2409.74s -> 2416.26s]  that like, okay, so this is 16 wide, right? Imagine you had a 16 wide SIMD unit. What
[2416.26s -> 2422.22s]  are the lanes doing here? There's nothing to do. That's the actual problem, right?
[2422.22s -> 2427.78s]  Whereas if I had a machine that only had two execution, like two threads or something like
[2427.78s -> 2432.66s]  that, the fact that we're not doing much in every step is great because like the processor
[2432.66s -> 2438.54s]  gets done with the step and then moves on really quickly. So your work efficiency actually,
[2438.54s -> 2442.34s]  like whether or not you care about work efficiency or raw parallelism can actually
[2442.34s -> 2446.90s]  kind of change based on how the machine works. And if I was to do this for real,
[2447.14s -> 2452.74s]  let me go back to... Where were we? If I was to do this for real on an NVIDIA chip,
[2452.74s -> 2457.54s]  for example, I would take that little piece of code that I gave you, this one right here,
[2457.54s -> 2464.66s]  that can compute the scan of 32 elements in five SIMD steps. And then if I had to do
[2464.66s -> 2472.02s]  something for, let's say, 128 elements, how would you do it?
[2472.02s -> 2484.86s]  We could do the N log N thing, but I'm actually... I would rather not take this N log N,
[2484.86s -> 2491.74s]  like I only want to take the N log N asymptotic if I don't have to pay for it in some sense.
[2491.74s -> 2496.26s]  So what I would do is like... Here's what I do. If I have a sequence that one
[2496.50s -> 2501.66s]  warp can get something done in five cycles and I get 128... Give me 128 things to do,
[2501.66s -> 2507.86s]  I'll just have four different warps through a 32-wide scan. And now I have four partials.
[2507.86s -> 2516.58s]  Let me just scan those. And then I push the bases back out, and then in parallel,
[2516.58s -> 2523.52s]  I update everything. So I did this for 128, but imagine you had a 32-squared scan,
[2523.56s -> 2531.68s]  1024 elements. You would do in parallel, a bunch of warps produced 32 scans in five cycles. Then
[2531.68s -> 2537.08s]  you stick those partials together into one 32-wide block, five more cycles to scan that,
[2537.08s -> 2542.16s]  and then take those 32 partials and put them back to the blocks, and then in parallel do that.
[2542.16s -> 2549.52s]  So it would take five cycles to do the original scan, five cycles to scan the scans,
[2549.76s -> 2556.88s]  and then one cycle for every thread to add the partial back to itself. So I could do it in 11
[2556.88s -> 2566.12s]  cycles for 1024 elements. And just for kicks, that's the code for doing it. I actually give
[2566.12s -> 2570.68s]  you this code. You call it as a subroutine if you wish in assignment three. You don't have
[2570.68s -> 2576.40s]  to understand how it works. And then if I wanted an even larger scan, I would take my blocks
[2576.40s -> 2583.88s]  of 32, then do 1024 by breaking it into blocks of 32, and then I distribute my blocks of 32
[2583.88s -> 2589.36s]  across all of the CUDA thread blocks, and I would do that in parallel and then scan those.
[2589.36s -> 2594.88s]  So I'm actually mixing this data parallel scan with a more conventional sequential algorithm.
[2594.88s -> 2599.72s]  So I'm doing data parallel when it helps, or the SIMD when it helps, and then when I have
[2599.72s -> 2603.52s]  far more parallelism than I have processors, I'm kind of backing off and doing the more
[2603.52s -> 2608.04s]  conventional thing. So I just wanted to kind of go through that sequence just to let people
[2608.04s -> 2612.08s]  know that there's some real sophistication in implementing these data parallel primitives
[2612.08s -> 2617.56s]  very efficiently on a machine. And I encourage you in assignment three to,
[2617.56s -> 2622.48s]  like basically we say, hey, learn how to program some CUDA by doing the O of N algorithm,
[2622.48s -> 2626.40s]  and then you move on to the actual assignment. And I say, by the way, if you have some extra
[2626.40s -> 2631.44s]  time, why don't you try and make your scan really fast, and here's the CUDA library for
[2631.44s -> 2635.40s]  scan and see how close you can get to it. And I have a feeling under the hood they're doing
[2635.40s -> 2638.64s]  this kind of stuff. I don't know, but it'd be interesting to see if anybody can beat CUDA's
[2638.64s -> 2642.48s]  standard library function for scan. But as a programmer, you just kind of think,
[2642.48s -> 2647.16s]  if I call scan, it'll be as fast as a good programmer can make it. You have a question?
[2647.16s -> 2659.16s]  Probably not so much within a tiny little SIMD block. Let me ask, do you think there's
[2659.16s -> 2664.64s]  any value to, if we were running a SIMD instruction and we knew a lane was dead,
[2664.64s -> 2671.96s]  do we gate it dynamically? I mean, according to Bill Dally these days with the sparse
[2671.96s -> 2674.76s]  tensor stuff, yes. You could.
[2674.76s -> 2681.20s]  So I think it's not astronomical savings, but if you're probably trying to say the factor of 1.5
[2681.20s -> 2685.16s]  of power or something like that, you probably do. I'm sure NVIDIA chases those types of
[2685.16s -> 2692.00s]  optimizations. Probably the bigger the SIMD with it is, probably the more you consider
[2692.00s -> 2693.00s]  doing something like that.
[2693.00s -> 2699.00s]  You're better off optimizing the, you know, taking advantage of the more capable
[2699.00s -> 2704.00s]  of that to optimize the use of the SIMD.
[2704.00s -> 2709.72s]  Okay. So let's just ramp up the complexity a little bit more because that's fun and we're
[2709.72s -> 2714.56s]  now in the back half of the class. So here's a more interesting primitive. So let's just say
[2714.56s -> 2722.24s]  we have scan. A very common sort of thing that you might do in applications is you don't
[2722.24s -> 2726.64s]  deal with sequences, but you might deal with sequences of sequences. So I'll give you some
[2726.64s -> 2732.44s]  examples like for every vertex in a graph for every edge of that vertex. So I have a
[2732.44s -> 2738.48s]  sequence of vertices and for every vertex I have a sequence of edges, standard graph
[2738.48s -> 2743.36s]  representation. Or like if you're in scientific computing, for every particle in a
[2743.36s -> 2749.20s]  simulation, for every particle within some range of it. So it's like a sequence of
[2749.20s -> 2755.44s]  sequences, right? For every document D in a collection for each word in D. So I have a
[2755.44s -> 2761.20s]  sequence of sequences and the sub-sequences that are all of different length.
[2761.20s -> 2764.04s]  So there's kind of two levels of parallelism in these problems. There's sort of the
[2764.04s -> 2767.76s]  parallelism over the outermost loop and then there's potentially, if you care, the
[2767.76s -> 2772.72s]  parallelism over the innermost loop. So imagine, for example, you're running on a GPU
[2772.76s -> 2780.20s]  and you're doing 200,000 things. Imagine if you had a graph with 500 vertices or
[2780.20s -> 2789.72s]  10,000 vertices, but each vertex had on average like 10 to 20 edges. So if you
[2789.72s -> 2794.96s]  parallelize just over the vertices, you don't have enough parallelism. You need to
[2794.96s -> 2799.96s]  actually find that parallelism over the edges. So these are the types of problems
[2799.96s -> 2806.56s]  that fit into this fold. So a segmented scan takes this input, a sequence of
[2806.56s -> 2813.96s]  sequences, and then applies the scan operator in parallel across all of the
[2813.96s -> 2820.40s]  outer sequences, but applies scan individually to each of them. So what's
[2820.40s -> 2822.88s]  going on here? This is the plus. So let's take a look. Let's confirm that
[2822.88s -> 2830.92s]  this is correct. My first sequence is one and two, so the scan exclusive should be
[2830.92s -> 2834.68s]  zero and one, and that's correct, right? Because the exclusive first element is
[2834.68s -> 2839.72s]  nothing, that's zero, and the exclusive second element is just everything up to
[2839.72s -> 2843.60s]  and excluding the second element, so it should just be one. The
[2843.60s -> 2849.00s]  exclusive scan here is zero, and then the exclusive scan here is zero, one, one
[2849.00s -> 2859.40s]  plus two is three, and three plus three is six. So that's segmented scan, and you
[2859.40s -> 2863.84s]  can think about implementing segmented scan often is you get this input, a
[2863.84s -> 2869.20s]  sequence of sequences might be encoded as two sequences. The first sequence is
[2869.20s -> 2874.08s]  just a list of, you know, a regular flat list of numbers, and the second
[2874.08s -> 2879.00s]  sequence might be some binary flags on which one to start, on where the
[2879.00s -> 2883.00s]  sub-sequences start. So in this case, the first element is the start of the
[2883.00s -> 2888.72s]  sequence, and then one, two, three, third, or the fourth element is the start of
[2888.72s -> 2893.08s]  the sequence. So imagine that this was just given to you as an array, and you
[2893.08s -> 2896.08s]  got that bit flag, you could say, oh, I know the first sub-sequence starts
[2896.08s -> 2900.60s]  here, and the second sub-sequence starts here. It's a very compact
[2900.60s -> 2904.44s]  representation of a sequence of sequences. If you care about performance,
[2904.44s -> 2909.92s]  you're doing any graph processing, this is how you implement a graph. It's like
[2909.92s -> 2914.32s]  these numbers would just be the list of edges, and then you just basically get
[2914.32s -> 2919.76s]  a number of edges per vertex here, and that's a very common thing. Okay, so we're
[2919.76s -> 2925.36s]  not going to go over how we implement it, but there is an algorithm, which is
[2925.36s -> 2931.48s]  really fun to read on your own, that adapts the work-efficient scan that I
[2931.48s -> 2936.96s]  just showed you, and makes it work on sequences of sequences. So you just give
[2936.96s -> 2941.74s]  it a sequence, and you give it these flags, and the output in O of n time and
[2941.74s -> 2947.48s]  log n span is that result, the sequences of sequences. And the
[2947.48s -> 2951.32s]  work-efficient scan looks a little bit like this, the segmented scan looks a
[2951.32s -> 2955.00s]  little bit like this, the way to understand the algorithm is this figure,
[2955.00s -> 2958.16s]  which you can think of, I mean it looks really complicated, but really all it is
[2958.16s -> 2962.28s]  is it's the old algorithm, except actually you're just propagating those
[2962.28s -> 2967.40s]  sub-sequence start flags along with the information. And what you'll see
[2967.40s -> 2971.24s]  here is that whenever you need to like have a back edge, if there is a
[2971.24s -> 2975.96s]  start flag, you just skip the propagation of the back edge. So if you go
[2975.96s -> 2980.96s]  into the code that I had, it basically is, there's some if statements which say
[2981.12s -> 2986.12s]  if the flag is one, do something else, don't do something. So if you
[2986.12s -> 2990.68s]  understand the segmented scan, sorry the regular scan, you actually will
[2990.68s -> 2993.80s]  understand the segmented scan if you just think about it as with the
[2993.80s -> 2998.20s]  information I need to pass these flags, and if I ever see a flag that
[2998.20s -> 3001.72s]  should stop me from propagating information over a boundary. So it's
[3001.72s -> 3006.40s]  actually not too big of a deal. Now why do we care about segmented scan?
[3006.72s -> 3011.48s]  Here's a great example. Sparse matrix multiplication, something that we do all
[3011.48s -> 3014.72s]  the time, and these guys are nodding their heads because they're a big fan of
[3014.72s -> 3020.72s]  sparse matrix multiplication. So here's a matrix, so here I have a dense
[3020.72s -> 3026.56s]  vector, my x's, and I have a sparse matrix. And by sparse I mean most of these
[3026.56s -> 3031.40s]  values are zeros. And this is pretty important, right? Like if this was an n
[3031.40s -> 3036.68s]  squared matrix and like 99% of the values are zeros, I can store it much
[3036.68s -> 3040.28s]  much more compactly. And sparse matrices appear everywhere in the world
[3040.28s -> 3044.56s]  whenever you have sparse connectivity, like for example the way Amazon might
[3044.56s -> 3048.44s]  store customer recommendation information, I have me and I have all the products
[3048.44s -> 3053.36s]  that I have bought, but like the cross product of all their users and all of
[3053.36s -> 3057.40s]  their products would be a very big matrix. But in practice very few people
[3057.60s -> 3062.36s]  buy very few products so they might implement it or represent it sparsely. So
[3062.36s -> 3067.48s]  one way we can express the sparse matrix is in a format called
[3067.48s -> 3072.32s]  compressed sparse row. So let's break this down. First of all it's compressed
[3072.32s -> 3076.80s]  because we're not going to store all the zeros explicitly, and it's sparse
[3076.80s -> 3080.68s]  row. So the way this is going to work is I'm going to store all my nonzero
[3080.68s -> 3086.92s]  values as a sequence of sequences. So if you look carefully the first row has
[3086.96s -> 3091.24s]  nonzeros three and one, that's my first subsequence. The next row has a
[3091.24s -> 3095.16s]  nonzero two, the next row has a nonzero four, the last row has three
[3095.16s -> 3102.00s]  nonzeros. So I have my rows are stored as sequences of sequences. Now
[3102.00s -> 3106.48s]  this is not a sufficient encoding of the matrix because for every nonzero I
[3106.48s -> 3111.88s]  need to know what column it's in, right? So I also have for every
[3111.88s -> 3116.04s]  nonzero I have another sequence of sequences that is what's column it's
[3116.08s -> 3121.52s]  in, right? So or equivalently I'm storing for every nonzero I'm
[3121.52s -> 3128.36s]  storing a value and a column. And then I have basically an array which
[3128.36s -> 3135.00s]  tells me in this array where is the, instead of a bit vector. So
[3135.00s -> 3139.04s]  before I gave you a, let me, let me, I'm showing a, I noticed I
[3139.04s -> 3144.12s]  just made a skip, a big jump. Okay. So here I use these Boolean
[3144.16s -> 3148.04s]  flags to tell you where the start of the sequences are. Okay. So
[3148.04s -> 3154.48s]  if you gave me a, if I just scan through this array I get all the
[3154.48s -> 3158.60s]  starting points. Another way to encode the same information would
[3158.60s -> 3163.52s]  be if it's a matrix for every row just give me the starting
[3163.52s -> 3168.76s]  point of the row. So here's what I did here was since this is
[3168.80s -> 3175.52s]  a 2D matrix where is the start of row zero in this big flat
[3175.52s -> 3179.84s]  array? Like the subsequence of row zero starts at zero. The
[3179.84s -> 3184.28s]  subsequence of row one starts at index two, subsequence of row
[3184.28s -> 3187.24s]  three starts at index four, subsequence of row four starts at
[3187.24s -> 3191.92s]  index five. Okay. So I'm encoding my sparse matrix as a list
[3191.92s -> 3196.76s]  of nonzeros, the column index of those nonzeros, and for
[3196.76s -> 3201.56s]  every row where does it start in these arrays? So this is
[3201.56s -> 3205.12s]  commonly called a compressed sparse row format. Compressed
[3205.12s -> 3207.72s]  sparse rows because it's row major, like all these data
[3207.72s -> 3212.64s]  structures are where the rows start. Okay. So if you had
[3212.64s -> 3215.96s]  segmented scan and map and some other things let's see how
[3215.96s -> 3218.36s]  we can do this in terms of our primitives that we have in
[3218.36s -> 3225.36s]  the lecture so far. Okay. So first of all given all the
[3225.40s -> 3230.96s]  nonzeros I need to multiply them by some x. So for every
[3230.96s -> 3236.08s]  nonzero I know the column of the nonzero, right, because
[3236.08s -> 3239.92s]  it was down in my columns array. And so I first need
[3239.92s -> 3244.08s]  to perform an operation that takes all of the x's and
[3244.08s -> 3249.92s]  produces a list or a sequence that's the same length as my
[3249.92s -> 3253.60s]  sequence of nonzeros. So I actually haven't told you about
[3253.64s -> 3256.84s]  an operation called gather but let's just assume for now
[3256.84s -> 3259.80s]  that I have the ability to basically for every column
[3259.80s -> 3264.64s]  here I need to make a new sequence that is pulling
[3264.64s -> 3269.64s]  elements given by this piece of data, the column index,
[3269.64s -> 3273.84s]  I pull x sub column index and I make a new array of
[3273.84s -> 3277.84s]  x values. So that means that I'm going to be duplicating
[3278.08s -> 3285.08s]  oops elements from x into this array. And then I just
[3285.08s -> 3288.68s]  do a map with the operation multiply. So I need to map
[3288.68s -> 3294.08s]  every nonzero in values against my gathered array from
[3294.08s -> 3298.44s]  x. So now I have a product of every nonvalue. Now I
[3298.44s -> 3301.72s]  need to sum across the rows, right? So that's a
[3301.72s -> 3305.44s]  segmented scan with a plus operation. So I have all my
[3305.44s -> 3310.12s]  products. I can create the flags array that we need to
[3310.12s -> 3313.04s]  say where's the start of all the rows and segmented
[3313.04s -> 3315.84s]  scan of this data with this starter array gives me
[3315.84s -> 3323.32s]  all these partials. And then the last element of
[3323.32s -> 3329.32s]  every segment is the sum of everything in the row.
[3329.32s -> 3332.32s]  So if I go grab the appropriate values out of this
[3332.40s -> 3335.92s]  array, I get the sum of everything in that row.
[3335.92s -> 3339.60s]  So I've done a sparse matrix multiplication that has
[3339.60s -> 3344.06s]  parallelism proportional to the number of nonzeros,
[3344.06s -> 3346.96s]  not proportional to the number of rows. And that
[3346.96s -> 3351.26s]  can be quite cool if my total number of nonzeros is
[3351.26s -> 3356.26s]  a lot higher than the total number of rows.
[3356.26s -> 3359.26s]  And this is going to basically parallelize about as
[3359.26s -> 3362.82s]  fast as map does, no problem, and about as fast as
[3362.82s -> 3365.22s]  segmented scan does, which is also going to be no
[3365.22s -> 3367.22s]  problem if we have good implementations of those
[3367.22s -> 3368.22s]  things. Yeah?
[3368.22s -> 3373.02s]  That could get you a little bit. That could get
[3373.02s -> 3376.06s]  you a little bit. But you have to do the gather
[3376.06s -> 3378.56s]  some like if you wrote this in like normal C code,
[3378.56s -> 3381.56s]  you'd have to go get that gather. The only
[3381.56s -> 3383.22s]  difference here is that we are actually
[3383.22s -> 3386.36s]  materializing that gather as a length and dense
[3386.46s -> 3389.96s]  array, and then iterating through the dense array,
[3389.96s -> 3391.72s]  whereas if we did it with a memory access, we would
[3391.72s -> 3393.66s]  never create a dense array. We would just go get the
[3393.66s -> 3395.66s]  data, go get the data again and again.
[3397.30s -> 3399.36s]  So this just hopefully gives you like a taste of how
[3399.36s -> 3402.80s]  fun like some of these primitives can be that you can
[3402.80s -> 3406.82s]  boil down irregular parallelism. Like now I can have
[3406.82s -> 3410.16s]  different rows with wildly different numbers of nonzeros
[3410.16s -> 3414.10s]  per row. I flatten the whole thing and treat it as a big
[3414.10s -> 3416.84s]  data parallel computation. And that can be pretty
[3416.84s -> 3422.84s]  cool. That can be pretty cool. Yeah, so I talked
[3422.84s -> 3425.86s]  about this fictitious gather, and really I should
[3425.86s -> 3428.26s]  have thrown this up earlier in the slide. Two other
[3428.26s -> 3431.10s]  really useful operations are gather and scatter. And
[3431.10s -> 3433.60s]  gather and scatter are your data movement operations
[3433.60s -> 3436.10s]  when you're thinking data parallel. So let me
[3436.10s -> 3439.94s]  illustrate these for you. So gather takes an index
[3439.94s -> 3443.94s]  sequence and a source data sequence and produces an
[3443.94s -> 3449.14s]  output for every input element in the input sequence,
[3449.14s -> 3452.58s]  uses that as the index value, and it grabs the data
[3452.58s -> 3455.50s]  in order to densify data all over the place. That's
[3455.50s -> 3458.64s]  what we did for that matrix, the vector x just a
[3458.64s -> 3461.14s]  second ago. And then scatter is just the opposite.
[3461.14s -> 3465.14s]  Given a dense set sequence in memory and a list of
[3465.14s -> 3469.68s]  indices on where to put it, scatter potentially sparsely
[3469.68s -> 3473.08s]  the data out to a bigger array. And these can be
[3473.12s -> 3476.12s]  very costly operations because on one end of the fence
[3476.12s -> 3479.08s]  you're definitely moving data to some unpredictable
[3479.08s -> 3485.84s]  data dependent place. These days, actually on CPUs,
[3485.84s -> 3488.92s]  you have an actual AVX2, a SIMD instruction which
[3488.92s -> 3492.42s]  performs a gather. So that SIMD instruction takes
[3492.42s -> 3495.42s]  a index vector, so here's a vector register with
[3495.42s -> 3500.18s]  indices in it. It takes a pointer, mem base, and the
[3500.18s -> 3503.02s]  result of executing the gather is that it will
[3503.02s -> 3508.18s]  dereference mem base sub index element and bring
[3508.18s -> 3513.68s]  that data into the result here in the vector
[3513.68s -> 3519.18s]  register. So in ispc, when your program just says
[3519.18s -> 3525.18s]  program index foo does a sub something, that's
[3525.18s -> 3528.68s]  potentially a gather operation. And if you think
[3528.68s -> 3532.68s]  about how this can be tricky, these are arbitrary
[3532.68s -> 3535.68s]  indices. It can be whatever they want. So it's very
[3535.68s -> 3538.68s]  possible for every single lane of the vector to
[3538.68s -> 3540.68s]  trigger a cache miss on a different cache line.
[3540.68s -> 3543.68s]  It's very possible for every lane of the vector
[3543.68s -> 3546.68s]  to trigger a different page fault handler. So this
[3546.68s -> 3550.68s]  can be a nasty little operation to get right.
[3550.68s -> 3553.68s]  So that's why it's very nice that when all of
[3553.68s -> 3557.68s]  those program instances in ispc or in CUDA access
[3557.68s -> 3561.68s]  the adjacent elements of the array, you don't need
[3561.68s -> 3564.68s]  a gather. You can have a vector load which just
[3564.68s -> 3566.68s]  says load the eight elements starting at this
[3566.68s -> 3569.68s]  base pointer. So there's a big difference between
[3569.68s -> 3571.68s]  vector loads and non-vector loads.
[3571.68s -> 3575.68s]  There are some fun tricks. When I learned all
[3575.68s -> 3577.68s]  this stuff, I would kind of go through a lot of
[3577.68s -> 3580.68s]  this stuff. If you have gather, can you turn it
[3580.68s -> 3586.68s]  into a scatter? Well, it's pretty easy if the
[3586.68s -> 3588.68s]  scatter is actually a permutation, right? So if the
[3588.68s -> 3590.68s]  scatter is a permutation, meaning that all of the
[3590.68s -> 3593.68s]  indices are unique and they cover everything and
[3593.68s -> 3596.68s]  they cover all elements of the array, well,
[3596.68s -> 3598.68s]  scattering according to an index is actually just
[3598.68s -> 3602.68s]  sorting the data according to the index array.
[3602.68s -> 3605.68s]  So you might find yourself on platforms where
[3605.68s -> 3608.68s]  you have a gather but no scatter. And so a
[3608.68s -> 3611.68s]  common trick is to turn one into the other.
[3611.68s -> 3614.68s]  Another version if you don't have scatter is
[3614.68s -> 3616.68s]  pretty fun. Let's say you don't have scatter,
[3616.68s -> 3622.68s]  but you do have sort, map, and segmented scan.
[3622.68s -> 3625.68s]  So here's a scatter op. So what it means is
[3625.68s -> 3628.68s]  scatter the value to this location and perform
[3628.68s -> 3630.68s]  an op. It's actually pretty common if you're
[3630.68s -> 3632.68s]  doing like a histogram, right? Like you compute
[3632.68s -> 3634.68s]  a bin that you need to add in and you
[3634.68s -> 3637.68s]  increment one to the bin. So look at this.
[3637.68s -> 3642.68s]  So for all elements in the sequence, the operation
[3642.68s -> 3646.68s]  we want to do is for all elements in the sequence,
[3646.68s -> 3650.68s]  take the index and I want to put the value
[3650.68s -> 3653.68s]  into that target location but not just store
[3653.68s -> 3655.68s]  it there, like do some op, like take the
[3655.68s -> 3657.68s]  current value and add the new value into it
[3657.68s -> 3660.68s]  or something like that. Okay, so the first
[3660.68s -> 3664.68s]  thing I decided to do here is I decided to
[3664.68s -> 3668.68s]  sort the index array. So I have the list
[3668.68s -> 3671.68s]  of indices where I need to put those things
[3671.68s -> 3673.68s]  and I decided to sort them. And notice
[3673.68s -> 3675.68s]  that those indices are not unique, so I'm
[3675.68s -> 3678.68s]  pushing multiple values to the same location.
[3678.68s -> 3680.68s]  So you can imagine we have a parallel sort.
[3680.68s -> 3682.68s]  We didn't talk about parallel sort today,
[3682.68s -> 3683.68s]  but you can imagine that the data
[3683.68s -> 3685.68s]  parallel library has sort. So now I have
[3685.68s -> 3689.68s]  the input sorted by the index. So I did
[3689.68s -> 3691.68s]  a sort of the input data according to those
[3691.68s -> 3695.68s]  indices. Now the next thing is I can
[3695.68s -> 3699.68s]  compute the start of each range of values
[3699.68s -> 3703.68s]  with the same index number. So this is
[3703.68s -> 3707.68s]  my sorted index array, and you can think
[3707.68s -> 3711.68s]  about doing some form of a map that said
[3711.68s -> 3713.68s]  okay, if I am the same as this one,
[3713.68s -> 3715.68s]  I'm not a start, but if I'm different
[3715.68s -> 3717.68s]  from this one, I am a start. So you
[3717.68s -> 3720.68s]  can think about in parallel, figuring out
[3720.68s -> 3722.68s]  every single element of the array says
[3722.68s -> 3725.68s]  am I the start of a new sequence?
[3725.68s -> 3729.68s]  I just made that bit vector completely
[3729.68s -> 3731.68s]  in parallel. So imagine a CUDA program
[3731.68s -> 3733.68s]  which for every element in the vector
[3733.68s -> 3737.68s]  or in the sequence says take my thread
[3737.68s -> 3739.68s]  ID, get the value in my position,
[3739.68s -> 3741.68s]  get the value in the previous position
[3741.68s -> 3743.68s]  and if they're different, I'm the
[3743.68s -> 3745.68s]  start of the sequence, so return 1,
[3745.68s -> 3747.68s]  else return 0. Perfectly parallelizable
[3747.68s -> 3750.68s]  operation. Now given this sequence
[3750.68s -> 3753.68s]  and given this sorted, now perform
[3753.68s -> 3757.68s]  a segmented scan on that sequence.
[3759.68s -> 3763.68s]  And now I have all of those operations
[3763.68s -> 3767.68s]  inserted in the appropriate memory address.
[3767.68s -> 3769.68s]  Interesting. Oh, and then I guess
[3769.68s -> 3771.68s]  the last thing I have to do is I would
[3771.68s -> 3773.68s]  have to scatter the results back out
[3773.68s -> 3775.68s]  to the target locations. So now I
[3775.68s -> 3777.68s]  decided to do some pretty interesting stuff
[3777.68s -> 3779.68s]  with just these basic parallel operations
[3779.68s -> 3781.68s]  and there are other parallel operations
[3781.68s -> 3784.68s]  like filter, which is take a sequence,
[3784.68s -> 3787.68s]  there's a function that's a predicate,
[3787.68s -> 3790.68s]  and remove everything from the sequence
[3790.68s -> 3793.68s]  that fit that property. Or a common
[3793.68s -> 3795.68s]  operation in a lot of database-y kind
[3795.68s -> 3797.68s]  of systems is given a sequence,
[3797.68s -> 3800.68s]  create a sequence of sequences,
[3800.68s -> 3803.68s]  so given a sequence of pairs,
[3803.68s -> 3807.68s]  create a sequence of sequences
[3807.68s -> 3810.68s]  where these sub-sequences are the key
[3810.68s -> 3813.68s]  followed by all values with the same key.
[3813.68s -> 3815.68s]  So imagine the key is a document
[3815.68s -> 3817.68s]  and the value is a bunch of words,
[3817.68s -> 3819.68s]  this would be like, sort all the words
[3819.68s -> 3822.68s]  by document. It's a very common operation
[3822.68s -> 3824.68s]  in a lot of data processing stuff.
[3824.68s -> 3827.68s]  Okay, so let's talk about why the heck I,
[3827.68s -> 3831.68s]  you know, why being able to think
[3831.68s -> 3834.68s]  in terms of reducing complex things
[3834.68s -> 3836.68s]  into these primitives can be quite useful.
[3836.68s -> 3838.68s]  And I'm going to give you an example
[3838.68s -> 3841.68s]  that's going to be shockingly similar
[3841.68s -> 3843.68s]  to your homework assignment.
[3843.68s -> 3846.68s]  So here's a problem that I took
[3846.68s -> 3848.68s]  from a physics application.
[3848.68s -> 3850.68s]  So imagine I have a particle,
[3850.68s -> 3852.68s]  I'm trying to do some galaxy simulation
[3852.68s -> 3854.68s]  or something like that, and I have all these particles
[3854.68s -> 3857.68s]  representing stars, or doing a fluid simulation,
[3857.68s -> 3859.68s]  I'm representing fluid by particles.
[3859.68s -> 3862.68s]  So in this case I have a bunch of particles
[3862.68s -> 3865.68s]  that are red dots, and the name of the game
[3865.68s -> 3868.68s]  is I want to create a data structure
[3868.68s -> 3871.68s]  where, okay, right, and then I've divided
[3871.68s -> 3874.68s]  space up into 16 cells.
[3874.68s -> 3876.68s]  So there's a grid over all of space,
[3876.68s -> 3879.68s]  16 cells, 4 by 4 grid,
[3879.68s -> 3881.68s]  and then I have all these particles
[3881.68s -> 3884.68s]  at arbitrary locations x, y.
[3884.68s -> 3886.68s]  And what I want to do is I want to create
[3886.68s -> 3890.68s]  a data structure where for every grid cell,
[3890.68s -> 3892.68s]  basically this table over here,
[3892.68s -> 3895.68s]  for every grid cell, I create a list
[3895.68s -> 3898.68s]  of particles that are in that cell.
[3898.68s -> 3901.68s]  So in some sense this is a sequence of sequences, right?
[3901.68s -> 3904.68s]  The outer sequence is a link 16 and has cells.
[3904.68s -> 3906.68s]  The inner sequences are the number
[3906.68s -> 3909.68s]  of the particle IDs in each cell.
[3909.68s -> 3911.68s]  So that's what we want to create.
[3911.68s -> 3913.68s]  I want to create a sequence of sequences
[3913.68s -> 3916.68s]  or a grid of lists, that's the data structure.
[3916.68s -> 3918.68s]  And I'd like to create this data structure
[3918.68s -> 3920.68s]  completely in parallel,
[3920.68s -> 3923.68s]  so that I have tons of processors,
[3923.68s -> 3927.68s]  a lot of particles, but maybe not a lot of grid cells.
[3929.68s -> 3931.68s]  The reason why this is a cool problem
[3931.68s -> 3933.68s]  is this data structure is super helpful
[3933.68s -> 3935.68s]  for any kind of physics simulation
[3935.68s -> 3936.68s]  when you want to say I want to compute
[3936.68s -> 3940.68s]  the forces on this particle based on nearby particles.
[3940.68s -> 3941.68s]  And then what you might do is you say,
[3941.68s -> 3943.68s]  oh, well, if I'm in this cell,
[3943.68s -> 3945.68s]  give me only the neighboring cells
[3945.68s -> 3947.68s]  and I'm only going to iterate over those particles.
[3947.68s -> 3950.68s]  It's a common end-body simulation kind of task.
[3950.68s -> 3953.68s]  So they have to make these data structures all the time.
[3953.68s -> 3956.68s]  Okay, so let's think about the dumbest,
[3956.68s -> 3958.68s]  well, not dumb, that's probably exactly
[3958.68s -> 3959.68s]  what I would start with, right?
[3959.68s -> 3961.68s]  Probably the smartest first implementation,
[3961.68s -> 3963.68s]  which is this.
[3963.68s -> 3965.68s]  For every particle in P,
[3965.68s -> 3967.68s]  let's assume there's millions of particles,
[3967.68s -> 3970.68s]  compute the cell containing P
[3970.68s -> 3973.68s]  and let's just say I have a lock or something like that.
[3973.68s -> 3975.68s]  Take the lock on the cell
[3976.68s -> 3981.68s]  or take the global lock and append P to the cell with C.
[3981.68s -> 3984.68s]  Pretty straightforward solution.
[3984.68s -> 3990.68s]  What are performance problems with my solution?
[3990.68s -> 3993.68s]  First of all, do I have a lot of parallelism?
[3993.68s -> 3995.68s]  I have tons of, well, no, I mean,
[3995.68s -> 3998.68s]  I have tons of parallelism over particles.
[3998.68s -> 4000.68s]  That's true, I can parallelize that for loop,
[4000.68s -> 4003.68s]  but do I really have a lot of parallelism?
[4003.68s -> 4004.68s]  People are saying no, why not?
[4005.68s -> 4007.68s]  Because this parallel loop is basically
[4007.68s -> 4009.68s]  going to synchronize on this lock.
[4009.68s -> 4011.68s]  Absolutely.
[4011.68s -> 4015.68s]  Okay, how do I make things a little better?
[4015.68s -> 4019.68s]  So I have contention on a shared thing,
[4019.68s -> 4021.68s]  it's the shared lock.
[4021.68s -> 4023.68s]  How do we alleviate contention?
[4023.68s -> 4026.68s]  You can just duplicate that cell list,
[4026.68s -> 4029.68s]  like, in dimension, that might cost you that kind of range.
[4029.68s -> 4032.68s]  Okay, so one suggestion would be
[4032.68s -> 4036.68s]  I'm going to make a different cell list for every particle,
[4036.68s -> 4038.68s]  for every thread, excuse me,
[4038.68s -> 4042.68s]  and then we'll just, in parallel,
[4042.68s -> 4045.68s]  deal with all the different cell lists.
[4045.68s -> 4048.68s]  Now, in this case, this is for each particle in parallel,
[4048.68s -> 4050.68s]  so are you going to make a different cell list per particle?
[4054.68s -> 4057.68s]  For every worker thread on the machine,
[4057.68s -> 4059.68s]  let's make a different cell list.
[4059.68s -> 4061.68s]  That's one, that's a good approach.
[4061.68s -> 4063.68s]  So what are the costs of that?
[4063.68s -> 4065.68s]  You have to, like, have all those cell lists,
[4065.68s -> 4066.68s]  and you single them.
[4066.68s -> 4067.68s]  I have to allocate all the steroids,
[4067.68s -> 4068.68s]  and then I'm going to have, like,
[4068.68s -> 4070.68s]  some merge process at the end.
[4070.68s -> 4071.68s]  And so if we were running on a machine
[4071.68s -> 4073.68s]  that only had a small number of processors
[4073.68s -> 4074.68s]  or a small number of threads,
[4074.68s -> 4075.68s]  I'd love your solution.
[4075.68s -> 4076.68s]  Looking good.
[4076.68s -> 4080.68s]  If we were on a machine that had a ton of processors,
[4080.68s -> 4083.68s]  maybe creating 10,000 cell lists is not tractable,
[4083.68s -> 4085.68s]  and then maybe actually even merging those
[4085.68s -> 4087.68s]  is going to be a good fraction of execution time.
[4087.68s -> 4089.68s]  Without dramatically changing the code,
[4089.68s -> 4091.68s]  are there any other options?
[4091.68s -> 4093.68s]  We have contention for a shared lock.
[4095.68s -> 4096.68s]  Okay, so one thing we could do
[4096.68s -> 4099.68s]  is have a lock for every individual cell, right?
[4099.68s -> 4100.68s]  Because, like, if I want to update
[4100.68s -> 4101.68s]  the data structure in the cell,
[4101.68s -> 4103.68s]  I should really, I'm not going to collide
[4103.68s -> 4105.68s]  with people touching other cells,
[4105.68s -> 4109.68s]  so I could at least get 16 times less contention here.
[4109.68s -> 4111.68s]  Maybe that will work, maybe it won't,
[4111.68s -> 4114.68s]  but it'll certainly work with a small number of processors.
[4114.68s -> 4116.68s]  For a large number of processors, it may not work
[4116.68s -> 4119.68s]  because there's still the contention for these locks.
[4119.68s -> 4121.68s]  Like, say I have 100,000 CUDA threads
[4121.68s -> 4123.68s]  trying to grab 16 locks.
[4123.68s -> 4125.68s]  That can be a problem.
[4125.68s -> 4129.68s]  Okay, so we have massive contention, which you saw,
[4129.68s -> 4132.68s]  and you told me already that we could actually
[4132.68s -> 4134.68s]  move to a list of different locks,
[4134.68s -> 4136.68s]  or start to multiple different locks.
[4136.68s -> 4139.68s]  You also suggested that if we have a small number of threads,
[4139.68s -> 4141.68s]  I don't quite have a,
[4141.68s -> 4143.68s]  well, I do have a slide on that, but it comes later,
[4143.68s -> 4145.68s]  is we could just duplicate the data structure,
[4145.68s -> 4147.68s]  but again, like, both of these solutions
[4147.68s -> 4150.68s]  are probably not going to scale to hundreds of thousands of threads.
[4150.68s -> 4154.68s]  If we wanted to scale to hundreds of thousands of threads,
[4154.68s -> 4155.68s]  any other ideas?
[4160.68s -> 4164.68s]  Yeah, we're going to have to go data parallel,
[4164.68s -> 4165.68s]  but I want to hear, yeah?
[4165.68s -> 4167.68s]  I was just saying, if they're all, like, equally,
[4167.68s -> 4171.68s]  can we just statically take up the cell list?
[4171.68s -> 4174.68s]  Well, there's only 16 cell lists, right?
[4175.68s -> 4178.68s]  So one interesting take here, by the way,
[4178.68s -> 4180.68s]  not a great solution, but I actually could change
[4180.68s -> 4184.68s]  the axis of parallelism.
[4184.68s -> 4187.68s]  I could say, I'm going to parallelize over all the cell lists,
[4187.68s -> 4190.68s]  and for each cell list, I'm going to iterate over all the particles,
[4190.68s -> 4192.68s]  and if that particle is within me,
[4192.68s -> 4195.68s]  I'm just going to add to my own cell list.
[4195.68s -> 4198.68s]  Notice how there's no contention at all now.
[4200.68s -> 4201.68s]  What's my problem here?
[4201.68s -> 4203.68s]  Well, one problem is, I've defined this thing to say
[4203.68s -> 4206.68s]  there's only 16 cells, so that's not enough parallelism,
[4206.68s -> 4209.68s]  but there's actually an even more frightening thing.
[4209.68s -> 4211.68s]  What if I actually had 10,000 cells?
[4211.68s -> 4213.68s]  I might be feeling good about myself,
[4213.68s -> 4216.68s]  but is there a problem?
[4219.68s -> 4223.68s]  Every thread on a unique cell is basically doing the whole problem,
[4223.68s -> 4225.68s]  is looping over all p particles.
[4227.68s -> 4229.68s]  So that's trouble.
[4229.68s -> 4232.68s]  And then a fourth answer was the one that you all gave me,
[4232.68s -> 4235.68s]  which was, we could duplicate the cell list into multiple cell lists
[4235.68s -> 4237.68s]  and merge them at the end,
[4237.68s -> 4239.68s]  which I also thought was a good solution,
[4239.68s -> 4241.68s]  but again, I don't think any of these,
[4241.68s -> 4243.68s]  the ones you're offering, the ones I offered,
[4243.68s -> 4245.68s]  none of us is going to get us to a good solution
[4245.68s -> 4247.68s]  with 100,000 threads.
[4248.68s -> 4250.68s]  So let's think about how to do this
[4250.68s -> 4253.68s]  in terms of data parallel concepts.
[4253.68s -> 4254.68s]  So let's start here,
[4254.68s -> 4258.68s]  and let me just show you some data representations.
[4258.68s -> 4261.68s]  Let's say that we have
[4261.68s -> 4264.68s]  particle indices, those are my red numbers,
[4264.68s -> 4267.68s]  so that's like particle 0, 1, 2, 3, 4, 5,
[4267.68s -> 4269.68s]  you know, that's what I'm, right here.
[4269.68s -> 4273.68s]  Now imagine I run a map on every particle
[4273.68s -> 4276.68s]  and compute what cell it lies in.
[4276.68s -> 4278.68s]  That's an easy computation, right?
[4278.68s -> 4280.68s]  If you give me the xy of a particle,
[4280.68s -> 4282.68s]  I can give you the cell,
[4282.68s -> 4285.68s]  and the results of that map are now represented here
[4285.68s -> 4287.68s]  in the sequence grid cell.
[4287.68s -> 4288.68s]  So now I know I have,
[4288.68s -> 4291.68s]  these particles need to go into these cells.
[4291.68s -> 4294.68s]  No communication, that's pretty trivial.
[4294.68s -> 4297.68s]  Yeah, so we're getting there, right?
[4297.68s -> 4299.68s]  Yeah, so how are we going to implement this group by?
[4299.68s -> 4301.68s]  And now I'm not going to give you group by,
[4301.68s -> 4302.68s]  I'm thinking about like,
[4302.68s -> 4304.68s]  well, you could just group by on this,
[4304.68s -> 4306.68s]  but now I want to do it really, really fast.
[4306.68s -> 4307.68s]  So let's think about
[4307.68s -> 4310.68s]  what is a really good implementation of group by, okay?
[4310.68s -> 4312.68s]  And to do a really good implementation of group by,
[4312.68s -> 4314.68s]  we need that to be parallel.
[4315.68s -> 4320.68s]  So, grouping and sorting are kind of the same thing,
[4320.68s -> 4324.68s]  so let's invoke a really high performance sort, okay?
[4324.68s -> 4328.68s]  So now we're going to sort all of the indices
[4328.68s -> 4330.68s]  by the grid cell.
[4330.68s -> 4334.68s]  So I did a sort here based on,
[4334.68s -> 4337.68s]  I sorted the array grid cells,
[4337.68s -> 4340.68s]  which, if you look at the grid cell, you get this.
[4340.68s -> 4343.68s]  But also notice that I kept the particle index with it,
[4343.68s -> 4346.68s]  because I need to know that this particle 3
[4346.68s -> 4348.68s]  goes into grid cell 4,
[4348.68s -> 4349.68s]  and particle 5 goes into grid cell...
[4349.68s -> 4354.68s]  So I've essentially done a group by via sort, right?
[4354.68s -> 4356.68s]  But my goal is to have this data structure,
[4356.68s -> 4360.68s]  not just kind of there, okay?
[4360.68s -> 4365.68s]  So now I need to figure out where are the starts of the bins,
[4365.68s -> 4367.68s]  because I don't have a sequence of sequences right now,
[4367.68s -> 4370.68s]  I just have a sorted sequence.
[4370.68s -> 4373.68s]  So I need to find the starts of the bins,
[4373.68s -> 4376.68s]  of the starts of the sub-sequences.
[4376.68s -> 4378.68s]  How do I do that?
[4378.68s -> 4381.68s]  I've told you.
[4381.68s -> 4384.68s]  Yeah, so I could run a map on this code,
[4384.68s -> 4388.68s]  and say if my value is different from my neighbors,
[4388.68s -> 4391.68s]  I'm the start of the sequence.
[4393.68s -> 4395.68s]  Alright, so I'm going to run that code,
[4395.68s -> 4397.68s]  and just trust me that that's what this is,
[4397.68s -> 4400.68s]  because it's starting to run out of time.
[4400.68s -> 4402.68s]  And so I'm writing basically,
[4402.68s -> 4406.68s]  if I at index foo,
[4406.68s -> 4409.68s]  if that value is different from the left,
[4409.68s -> 4413.68s]  then my position in the array
[4413.68s -> 4416.68s]  is where the start of bin foo is.
[4416.68s -> 4419.68s]  So I'll let you take a look at this maybe offline,
[4419.68s -> 4420.68s]  but at the end of the day,
[4420.68s -> 4425.68s]  I end up with these cell starts and cell ends.
[4425.68s -> 4430.68s]  So my goal here is this is an array of 16 values,
[4430.68s -> 4434.68s]  and this code updates some of those values
[4434.68s -> 4439.68s]  based on whether or not the current thread
[4439.68s -> 4442.68s]  is looking at an element that's the start of the bin or not.
[4442.68s -> 4444.68s]  So what I've done is I've done a map,
[4444.68s -> 4448.68s]  a sort, another map,
[4448.68s -> 4450.68s]  and I have my data structure.
[4450.68s -> 4452.68s]  So just confirm that this is the data structure.
[4452.68s -> 4454.68s]  So this is a data structure that says
[4454.68s -> 4457.68s]  if you give me a cell 0, 1, 2, 3, 4, 5, whatever,
[4457.68s -> 4459.68s]  if I look up there,
[4459.68s -> 4461.68s]  I look up the cell start and the cell end
[4461.68s -> 4464.68s]  into the original particle array.
[4464.68s -> 4465.68s]  And just from the diagram,
[4465.68s -> 4467.68s]  the reason why most of these cells are empty
[4467.68s -> 4469.68s]  are because in my problem,
[4469.68s -> 4473.68s]  most of the cells are empty.
[4473.68s -> 4475.68s]  So there's a completely data parallel way
[4475.68s -> 4479.68s]  of making a uniform grid
[4479.68s -> 4481.68s]  where your parallelism is proportional
[4481.68s -> 4483.68s]  to the number of particles
[4483.68s -> 4486.68s]  and not the number of cells.
[4486.68s -> 4488.68s]  Now Ron Fedka, who in the back
[4488.68s -> 4490.68s]  has probably had his students do this a bunch of times
[4490.68s -> 4493.68s]  because he loves to have particle lists hanging around.
[4493.68s -> 4495.68s]  Give me a particle, tell me what's next to it.
[4495.68s -> 4497.68s]  Okay, now I'm going to stop.
[4497.68s -> 4498.68s]  We're basically done.
[4498.68s -> 4499.68s]  I'll actually give you a couple of minutes back,
[4499.68s -> 4500.68s]  but the rest of this lecture,
[4500.68s -> 4502.68s]  which I did not plan to get to,
[4502.68s -> 4504.68s]  these slides are here only for your reference,
[4504.68s -> 4507.68s]  is some fun stuff of how do you make a histogram
[4507.68s -> 4510.68s]  with only these primitives.
[4510.68s -> 4514.68s]  So here's a way to make a histogram.
[4514.68s -> 4516.68s]  I think going over this histogram example
[4516.68s -> 4518.68s]  is pretty helpful, actually,
[4518.68s -> 4520.68s]  because you have to count the bins
[4520.68s -> 4524.68s]  and do other things.
[4524.68s -> 4526.68s]  Pretty close.
[4526.68s -> 4532.68s]  Yeah, histogram is about the same.
[4532.68s -> 4536.68s]  But you'll use this in your...
[4536.68s -> 4538.68s]  Well, the only difference between
[4538.68s -> 4539.68s]  the histogram and what I just did,
[4539.68s -> 4542.68s]  what I just did is I found all of the particles
[4542.68s -> 4544.68s]  that go into the same bin.
[4544.68s -> 4546.68s]  The histogram needs to actually finish it off
[4546.68s -> 4548.68s]  with a segmented sum to actually compute
[4548.68s -> 4549.68s]  the total sum of those things.
[4549.68s -> 4554.68s]  It's more like the sparse matrix multiply example I gave you.
[4554.68s -> 4559.68s]  And when you're doing the segmented scan,
[4559.68s -> 4560.68s]  you've got to be a little careful
[4560.68s -> 4562.68s]  because some of your bins can be empty.
[4562.68s -> 4564.68s]  So there's a special case in here
[4564.68s -> 4567.68s]  for what happens when you have empty bins.
[4567.68s -> 4570.68s]  Anyways, the point of the day is
[4570.68s -> 4572.68s]  there's just kind of a whole other way
[4572.68s -> 4574.68s]  to think about parallel algorithms,
[4574.68s -> 4577.68s]  which is how do I reduce irregular data structures,
[4577.68s -> 4582.68s]  irregular parallelism to regular parallelism
[4582.68s -> 4584.68s]  that's sort of embedded, encapsulated
[4584.68s -> 4588.68s]  in some library of these data parallel operators.
[4588.68s -> 4590.68s]  And depending on your platform,
[4590.68s -> 4592.68s]  you will have different libraries.
[4592.68s -> 4595.68s]  So, for example, if you're programming a GPU,
[4595.68s -> 4597.68s]  and everybody has implemented this library called Thrust,
[4597.68s -> 4600.68s]  and just go Google it and go look at the API documentation,
[4600.68s -> 4602.68s]  you will find things that look very familiar
[4602.68s -> 4603.68s]  given these lectures.
[4603.68s -> 4606.68s]  It's map, it's sort, it's scan, it's segmented scan,
[4606.68s -> 4609.68s]  it's all of these things.
[4609.68s -> 4612.68s]  And if you don't really want to do crazy CUDA hacking,
[4612.68s -> 4614.68s]  you can just express your algorithm
[4614.68s -> 4616.68s]  in terms of those high-level primitives.
[4616.68s -> 4618.68s]  All of Apache Spark, which you may have heard of
[4618.68s -> 4620.68s]  for distributed programming,
[4620.68s -> 4623.68s]  is based on the idea that all of your Spark programs
[4623.68s -> 4626.68s]  are done in terms of these sequence operators.
[4626.68s -> 4629.68s]  Their sequences are called these RDDs.
[4629.68s -> 4631.68s]  An RDD is a sequence.
[4631.68s -> 4633.68s]  And they only give you this set of operations
[4633.68s -> 4635.68s]  to do things on RDDs.
[4635.68s -> 4638.68s]  And the whole premise of Spark is if you use these RDDs,
[4638.68s -> 4640.68s]  you get parallelism across a cluster,
[4640.68s -> 4643.68s]  you get fault tolerance and redundancy,
[4643.68s -> 4645.68s]  and these operations are enough
[4645.68s -> 4647.68s]  to do a whole bunch of cool data analytics.
[4647.68s -> 4649.68s]  And so that's why Spark got so popular.
[4649.68s -> 4651.68s]  But just different implementation,
[4651.68s -> 4653.68s]  basically the same idea as this thrust.
[4653.68s -> 4655.68s]  So you'll get a little bit used to this
[4655.68s -> 4657.68s]  in programming assignment three.
[4657.68s -> 4658.68s]  All right?
[4658.68s -> 4660.68s]  Okay, good luck finishing up two,
[4660.68s -> 4663.68s]  and we'll be shipping you assignment on Monday.
