# Detected language: en (p=1.00)

[0.00s -> 9.00s]  Okay, let's just get started.
[9.00s -> 12.00s]  Welcome to Lecture 14 everyone.
[12.00s -> 19.00s]  Hope you've been doing well and managing all of the various deadlines.
[19.00s -> 26.00s]  So today we'll be looking at two interesting applications of language models.
[26.00s -> 30.00s]  In the first half I'll be talking about using language models to reason
[30.00s -> 36.00s]  in domains like math, geometry, doing things like spatial reasoning.
[36.00s -> 40.00s]  And then in the second half of the lecture I'll be talking about how you can use
[40.00s -> 44.00s]  language models to take actions in grounded environments.
[44.00s -> 49.00s]  Okay, so a little bit of a disclaimer.
[49.00s -> 54.00s]  A lot of the content today is research that was done in the last three, four years.
[54.00s -> 60.00s]  So there's plenty of questions, plenty of unanswered questions, and not a lot of answers.
[60.00s -> 68.00s]  So maybe we can have more of a discussion around these topics.
[68.00s -> 72.00s]  Okay, so let's get started with reasoning.
[72.00s -> 78.00s]  So experts like to start a lecture on reasoning by really talking about what are the various kinds of reasoning.
[78.00s -> 80.00s]  So I'm going to do that here.
[80.00s -> 85.00s]  But at a high level it's really about using facts and logic to arrive at an answer.
[85.00s -> 91.00s]  But more concretely there's three distinct categories of reasoning that we can talk about.
[91.00s -> 97.00s]  The first one, which is probably the one that most of you are familiar with, is deductive reasoning.
[97.00s -> 105.00s]  Where we go from rules of logic along with a premise to come with a firm conclusion.
[105.00s -> 113.00s]  So an example of that could be that we have the sentence, all mammals have kidneys, and all whales are mammals.
[113.00s -> 116.00s]  And then we can come up with the conclusion, all whales have kidneys.
[116.00s -> 120.00s]  And we could do multiple such steps of reasoning.
[120.00s -> 129.00s]  A second form of reasoning is inductive, where given observations we derive conclusions.
[129.00s -> 137.00s]  So maybe we've learned from experience that every time we see a creature with wings it is usually a bird.
[137.00s -> 144.00s]  And let's say we observe a state where we see a creature with wings.
[144.00s -> 150.00s]  And using our experience we can come up with this conclusion that the creature is likely to be a bird.
[150.00s -> 153.00s]  So that form of reasoning is inductive.
[153.00s -> 162.00s]  And finally we have abductive reasoning, where we're given an observation and then we start drawing possible explanations.
[162.00s -> 169.00s]  So maybe you see a car that cannot start and there's a puddle of liquid under the engine.
[169.00s -> 173.00s]  And then you start drawing inferences about the situation.
[173.00s -> 177.00s]  So one of them could be that the car has a leak in the radiator.
[178.00s -> 185.00s]  And apart from that taxonomy, we can also think of reasoning in formal and informal terms,
[185.00s -> 193.00s]  where formal reasoning involves using axioms and rules of formal logic to derive truth conditions.
[193.00s -> 198.00s]  There's also informal reasoning, which is what you and I probably do every day.
[198.00s -> 205.00s]  And here we just reason about everyday situations and use common sense to derive conclusions.
[205.00s -> 211.00s]  For most of the lecture when I say reasoning, I will mean informal deductive reasoning.
[211.00s -> 214.00s]  And that's often going to involve multiple steps.
[216.00s -> 220.00s]  So let's come back to language models.
[220.00s -> 232.00s]  So we've learnt in lectures 9, 10, 11 that large language models are really really good at coming up with plausible continuations of text
[232.00s -> 235.00s]  that reflect human preferences and constraints.
[235.00s -> 239.00s]  Today we'll try to answer if they can also reason.
[241.00s -> 248.00s]  So one of the most basic ways we can try to answer this question is via prompting.
[248.00s -> 252.00s]  And we've probably already seen this.
[252.00s -> 261.00s]  There's this popular method called chain of thought prompting, where you get a language model to produce a reasoning step before producing an answer.
[261.00s -> 271.00s]  And we could do this by providing some in-context examples with explicit reasoning steps that the language model can then mimic at test time.
[271.00s -> 274.00s]  So that's chain of thought prompting.
[274.00s -> 283.00s]  Another rather surprising property of language models is that sometimes you don't even have to show them these in-context examples.
[283.00s -> 287.00s]  And you could just prompt them with the sentence, let's think step by step.
[287.00s -> 292.00s]  And you can get these reasoning rationales before they produce an answer.
[292.00s -> 295.00s]  So that's pretty simple.
[295.00s -> 298.00s]  But let's keep going.
[298.00s -> 305.00s]  So another popular way to prompt language models to do reasoning is via self consistency.
[306.00s -> 313.00s]  So here what we do is instead of greedily sampling a rationale followed by an answer,
[313.00s -> 319.00s]  we're going to sample multiple reasoning steps and correspondingly multiple answers.
[319.00s -> 324.00s]  So what we see in the figure on the right, we have a question.
[324.00s -> 333.00s]  And then what you would normally do with chain of thought prompting is you would greedily decode a rationale and then condition on the rationale to generate an answer.
[333.00s -> 338.00s]  With self consistency, we are going to sample multiple times, so sample multiple rationales.
[338.00s -> 341.00s]  They are all going to lead to multiple answers.
[341.00s -> 344.00s]  And then we're going to pick the one that is the most common.
[344.00s -> 355.00s]  With the idea being that if an answer keeps appearing for multiple rationales as the majority of the rationales agree on, then it's more likely to be correct.
[355.00s -> 362.00s]  And the authors of self consistency find that on a variety of mathematical reasoning tasks,
[362.00s -> 369.00s]  if you add this simple idea of self consistency where you sample multiple times and sort of do majority voting,
[369.00s -> 374.00s]  that improves performance pretty drastically over a standard chain of thought.
[374.00s -> 381.00s]  And interestingly, when I saw this result the first time, I thought, okay, this is just like ensembling.
[381.00s -> 384.00s]  We learned this in CS229.
[384.00s -> 393.00s]  The idea is if you want to boost the performance of your system, I'm going to produce like 10 classifiers with different random seeds.
[393.00s -> 397.00s]  I'm going to produce a classification decision and I'm going to do a majority voting.
[397.00s -> 402.00s]  But it turns out that it's doing maybe a little bit more than just simple ensembling.
[402.00s -> 409.00s]  So the authors also compared an ensembling approach where it's the same language model with multiple different prompts.
[409.00s -> 412.00s]  And then you do majority voting there.
[412.00s -> 417.00s]  And then turns out that self consistency is better than just simple ensembling.
[419.00s -> 420.00s]  Okay.
[420.00s -> 425.00s]  So earlier today, I said that I'll be talking about multi-step reasoning.
[425.00s -> 432.00s]  So far we've looked at sort of math problems and like prompting, but not necessarily multi-step reasoning.
[432.00s -> 441.00s]  One of the main kind of aspects about multi-step reasoning is it involves breaking down a large problem into several sub-parts.
[441.00s -> 447.00s]  And answering each of the sub-parts and then combining everything into a solution.
[447.00s -> 448.00s]  Okay.
[448.00s -> 456.00s]  So there's this kind of decomposition strategy that was integrated into another prompting method called least to most prompting.
[456.00s -> 466.00s]  And the idea behind least to most prompting is, like I said, given a question, we're going to first break it down into sub-questions as shown here.
[467.00s -> 475.00s]  And then given these sub-questions, the language model will sort of answer each of the sub-questions.
[475.00s -> 480.00s]  And then condition on its answers to the sub-questions is going to generate the final answer.
[482.00s -> 488.00s]  And this is kind of how it looks like for sort of a math reasoning problem.
[488.00s -> 495.00s]  So in standard chain of thought prompting, you would have a question followed by a rationale and the answer.
[495.00s -> 504.00s]  With least to most prompting, which is this decomposition strategy, you would take the question and then instead of directly producing a rationale,
[504.00s -> 509.00s]  you sort of ask the language model to break it down into problems.
[509.00s -> 512.00s]  And then you have these two different sub-problems.
[512.00s -> 520.00s]  And then you start answering both of those sub-problems and then condition your final answer on the answers to those sub-problems.
[521.00s -> 525.00s]  So that's just like a prompting method, right?
[525.00s -> 536.00s]  One interesting experiment from least to most prompting was showing that you can sometimes generalize from a small number of reasoning steps to a much larger number of reasoning steps.
[536.00s -> 542.00s]  So here, in this sort of math word problem, there's two reasoning steps.
[543.00s -> 557.00s]  And if we show this prompt to the language model sort of as in-context example, we see that it continues to generalize even on examples that require more than five steps of reasoning.
[557.00s -> 561.00s]  And in a way, that's much better than standard chain of thought.
[562.00s -> 568.00s]  But it's not entirely clear if structuring inference in this manner is really fundamental.
[569.00s -> 583.00s]  One of the other results they reported was that with enough prompt engineering, so the rows corresponding to best, normal chain of thought is on par with least to most prompting.
[583.00s -> 593.00s]  But it's kind of an interesting idea of trying to break down problems into sub-problems, solving the sub-problems and then sort of building up a solution based on your answers to the sub-problems.
[594.00s -> 601.00s]  Okay, so all this was different sort of prompting methods to get reasoning behavior out of language models. Can we do something more?
[603.00s -> 617.00s]  So one of the things that we might be interested in is instead of trying to get really large language models to do reasoning, maybe we want to somehow get this kind of reasoning behavior in a smaller language model.
[618.00s -> 629.00s]  And one popular approach for doing that is distillation, where maybe you want to fine-tune a smaller llama model by teaching it to imitate a larger llama model.
[632.00s -> 634.00s]  And so that's what we're going to look at now.
[634.00s -> 648.00s]  So this model is called ORCA, and at a high level, ORCA is going to fine-tune a smaller 13 billion llama language model on explanations produced by GPT-4.
[650.00s -> 654.00s]  And to construct this data, it's pretty simple, it has these three steps.
[655.00s -> 662.00s]  So the first step is we get a wide variety of instructions from the FLAN V2 collection.
[663.00s -> 675.00s]  So FLAN V2 is basically a dataset, it kind of accumulates multiple datasets into one sort of collection, and it consists of instructions paired with questions and answers.
[676.00s -> 679.00s]  And I'll show an example of this in a moment.
[680.00s -> 689.00s]  And then we're going to prompt GPT-4 or ChatGPT with these instructions along with a system message.
[689.00s -> 699.00s]  And the objective of the system message is to get ChatGPT or GPT-4 to produce an informative explanation along with the answer.
[700.00s -> 714.00s]  So here we have a question about simple data processing, about calculating the median, and there's a system instruction that says please justify your steps and kind of answer step by step.
[714.00s -> 723.00s]  And in producing its output, the model sort of provides a fairly detailed explanation of how it got to the answer.
[724.00s -> 730.00s]  And what ORCA is going to do is use precisely this explanation to fine-tune a much smaller model.
[731.00s -> 733.00s]  So that's what's going to happen.
[734.00s -> 743.00s]  Once we have these explanations, we're going to fine-tune a much smaller 13 billion parameter llama model on these explanations.
[744.00s -> 754.00s]  So far we've looked at math reasoning and grade school math problems.
[755.00s -> 758.00s]  Let's turn to a different benchmark for reasoning.
[759.00s -> 765.00s]  So we're going to look at BigBenchHard, and this is another dataset for multi-step reasoning.
[766.00s -> 770.00s]  And let's look at some examples from BigBenchHard.
[771.00s -> 777.00s]  So it consists of multiple different subtasks, so there's a total of 23 different subtasks. I'm going to show a few examples.
[778.00s -> 781.00s]  So one of them is evaluating Boolean expressions.
[782.00s -> 786.00s]  So the question is true and false and not true and true is.
[787.00s -> 791.00s]  So that's basically evaluate this Boolean expression.
[791.00s -> 801.00s]  And with sort of chain of thought, the model can evaluate each of the sub-expressions and get to the final answer.
[802.00s -> 808.00s]  And another example of a task from BigBenchHard is data understanding.
[808.00s -> 824.00s]  So the question is, tomorrow is a given date, what is the date one year ago from today in a given format?
[825.00s -> 833.00s]  And it's paired with some options, and again the model can sort of think step by step, following basic chain of thought, and then come up with an answer.
[833.00s -> 839.00s]  So this is kind of the flavor of tasks in BigBench. Most of these involve multi-step reasoning.
[840.00s -> 846.00s]  They're fairly synthetic, but also reasonably hard for language models.
[847.00s -> 849.00s]  Another example is geometric shapes.
[850.00s -> 855.00s]  And this one is pretty surprising that language models can do anything here.
[855.00s -> 871.00s]  So you're given the SVG path element, and I have no idea what this renders as, but the question is, just given the SVG, what shape are you going to get?
[872.00s -> 878.00s]  And there's a bunch of options, and then again the model prompted with let's think step by step will produce some answer.
[879.00s -> 881.00s]  We don't know if it's correct, but it's going to produce some answer.
[882.00s -> 894.00s]  And so it's basically this data set covering different kinds of reasonings, spatial reasoning, data understanding, evaluating booleans, and it's sort of multi-choice.
[895.00s -> 899.00s]  So it's easier to get an accuracy number.
[900.00s -> 905.00s]  And so it covers a wide variety of different tasks.
[905.00s -> 910.00s]  On the left, we have performance from really large language models.
[911.00s -> 917.00s]  This is zero-shot chain of thought with just the prompt let's think step by step.
[918.00s -> 927.00s]  So GPT-4 has some potential contamination issues with BigBench hard, so maybe we can ignore that column.
[927.00s -> 938.00s]  Wikuna is, I think, a few months ago it was state of the art as an instruction-tuned llama 13b model.
[939.00s -> 955.00s]  And ORCA is again a llama 13b that's fine-tuned specifically on this explanation data where you have instructions, and then you have explanations from chat GPT or GPT-4, and you fine-tune on that.
[955.00s -> 972.00s]  And we see that overall it outperforms chat GPT, maybe because it's specialized to just these reasoning problems, and it outperforms Wikuna, which was not trained on these really extensive explanations.
[973.00s -> 979.00s]  So that's one way you can get a smaller language model to display some kind of reasoning behavior.
[980.00s -> 991.00s]  So this was all great, and we are very happy that you can just generate rationales from a big LM and then fine-tune a smaller language model on that.
[992.00s -> 997.00s]  But then someone could ask why not just fine-tune the big language model on its own rationales.
[998.00s -> 1003.00s]  So that's also been explored, and there's a bunch of different methods that do this.
[1003.00s -> 1007.00s]  I'm going to talk about one of them called reinforced self-training, or REST.
[1008.00s -> 1010.00s]  And I'm just going to alternate between two stages.
[1011.00s -> 1021.00s]  The first stage, given a reasoning problem, and perhaps the prompt let's think step-by-step, I'm going to have the language model generate multiple rationales.
[1022.00s -> 1028.00s]  And then I'm going to filter these rationales based on whether they give me the correct answer or not.
[1028.00s -> 1035.00s]  So think about the word algebra problems. Someone has three apples, someone else has four apples.
[1036.00s -> 1043.00s]  You generate a rationale, and the answer comes out to be seven. You keep that rationale. The answer is 12. You sort of leave that rationale out.
[1044.00s -> 1051.00s]  And then I'm going to do an update step where I'm going to take these rationales that I filtered in my first stage.
[1052.00s -> 1054.00s]  I'm going to fine-tune the language model on that.
[1054.00s -> 1066.00s]  And then I can do this iteratively. Now I have an updated language model, I can get hopefully better rationales, and then I can update the language model on better rationales to get an even better language model, and I can keep doing that.
[1066.00s -> 1092.00s]  And the results are promising, but what we find is on GSM8K, which is this grade school math dataset of algebraic word problems, as you increase the number of iterations of self-training, we see a slight improvement in performance, and then it starts degrading.
[1092.00s -> 1112.00s]  Math is another dataset that again focuses on multi-step reasoning, covering math problems. And again, on this dataset we see that as we do more iterations of this reinforced self-training paradigm, we see an improvement in the accuracy.
[1113.00s -> 1131.00s]  And the numbers in orange here are a much larger PALM model. The numbers in blue are a smaller model. And the dashed lines represent what you get if you did supervised fine-tuning on human-provided rationales.
[1131.00s -> 1145.00s]  So one of the promising things about this approach is when you do multiple iterations of self-training on your own rationales, you can outperform human-generated rationales.
[1145.00s -> 1163.00s]  And that is exemplified again in this graph, where what we find is the blue bar represents accuracy when you take the PALM model and you do supervised fine-tuning on human-provided rationales.
[1164.00s -> 1183.00s]  And then in green is if you controlled for theâ€”sorry, so blue is if you fine-tuned on all human-provided rationales. Orange is if you fine-tuned on one rationale per training example. And these are written by humans.
[1183.00s -> 1197.00s]  In green, it's what you get if you fine-tune on one rationale chosen at random per question, which is generated by the model. So it's controlling for the number of rationales.
[1197.00s -> 1209.00s]  And we see that it outperforms human-provided rationales. And then if you sort of do the full multi-step iterative procedure where you keep improving the model, we see again a boost in performance.
[1210.00s -> 1213.00s]  So that's super promising.
[1213.00s -> 1222.00s]  Let's kind of start revisiting the question that we asked in the beginning about reasoning in language models.
[1222.00s -> 1231.00s]  So one way of answering that question is we can apply all these methods and we can look at benchmarks.
[1231.00s -> 1243.00s]  But maybe the way to answer the question correctly is to be more systematic, come up with counterfactual tasks, and be very careful about possible data contamination.
[1243.00s -> 1248.00s]  And I'm going to show some results around that.
[1248.00s -> 1260.00s]  So we started the lecture with chain of thought, and maybe the first question to ask is, are the rationales that the model produces with chain of thought faithful?
[1260.00s -> 1270.00s]  What I mean by faithful is maybe the model produces some rationale, and then it produces an answer, but maybe the answer does not even depend on the rationale that it produced.
[1270.00s -> 1283.00s]  So maybe the question was, Tom has three apples and Jerry has four apples, and the rationale it produced was, Tom has three apples, Jerry has four, three plus four is seven, so the answer is 25.
[1283.00s -> 1288.00s]  So in a case like that, you'd say that the model was not faithful to its rationale.
[1288.00s -> 1304.00s]  And so what we see in this plot is a very careful experiment where on the x-axis, we have the number of reasoning samples.
[1304.00s -> 1306.00s]  So the setup is something like this.
[1306.00s -> 1310.00s]  So for every question, the model produces a rationale.
[1311.00s -> 1314.00s]  And the rationale here is multiple sentences.
[1314.00s -> 1324.00s]  And what we're going to do is we're going to force the model to sort of early exit from its rationalization and just force it to produce an answer.
[1324.00s -> 1331.00s]  So if it produced four rationales, I can early exit right after the first rationale and ask it to produce an answer.
[1331.00s -> 1335.00s]  I can exit after the second rationale, ask it to produce an answer, and so on.
[1335.00s -> 1343.00s]  And what I'm going to plot on the y-axis is the model's accuracy after early exiting in this procedure.
[1343.00s -> 1355.00s]  So let's say that I early exited after just one rationale, and the model produced exactly the same answer that it would if it had seen all four sentences in its rationale.
[1355.00s -> 1361.00s]  Then maybe we can conclude that the kind of reasoning is not faithful.
[1361.00s -> 1365.00s]  It doesn't matter if the model sees the full rationale or just the first sentence.
[1365.00s -> 1374.00s]  And if you take that to the extreme, maybe you terminate even without any rationale and it produces the same answer.
[1374.00s -> 1388.00s]  So the results here are somewhat mixed, but we see that there are enough data sets where it doesn't matter if the model sees the full rationale before answering or if you early exit.
[1388.00s -> 1398.00s]  You kind of get the same answer, which means that sometimes these rationales may be post-hoc explanations of the model's answer.
[1398.00s -> 1409.00s]  Another experiment that tries to answer this exact same question is you can take these rationales and then you can start corrupting them.
[1409.00s -> 1417.00s]  So maybe your rationale was length four, and then I generate the first rationale, the second rationale, and for the third rationale I just corrupt it.
[1417.00s -> 1421.00s]  And then the fourth rationale, and then I ask the model to generate my answer.
[1421.00s -> 1433.00s]  If it turns out that no matter how much I corrupt my rationale, the model produces the same answer, then I can conclude that, again, the answer kind of did not depend on my rationale.
[1433.00s -> 1445.00s]  So on the x-axis, we are looking at the percentage of reasoning steps before I add sort of a mistake in the rationale.
[1445.00s -> 1459.00s]  So what you should see is kind of a strictly increasing trend where if I add a mistake after the very first step, then that's probably going to change the answer a lot.
[1459.00s -> 1464.00s]  And then if I add a mistake after the last step, that maybe doesn't change the answer all that much.
[1464.00s -> 1475.00s]  But again, we find that for some data sets, it so happens that you can add a mistake in the first sentence in your rationale, and the answer is not going to change all that much.
[1475.00s -> 1484.00s]  And so that's also kind of an indicator that maybe these rationales are sort of post-hoc explanations of the model's behavior.
[1484.00s -> 1487.00s]  So yeah, so there's a lot of lines here.
[1487.00s -> 1499.00s]  So if anyone has questions, I see a few blank faces in the audience.
[1499.00s -> 1502.00s]  OK, so let's keep moving.
[1502.00s -> 1511.00s]  OK, so that was about whether the models, whether sort of chain of thought expresses kind of a reasoning that the model is faithful to.
[1512.00s -> 1517.00s]  Another question you could ask is, what if I changed my setting a little bit?
[1517.00s -> 1523.00s]  So my model, let's say I observe that it's able to do arithmetic in base 10.
[1523.00s -> 1526.00s]  So it's able to answer something like 12 plus 14.
[1526.00s -> 1529.00s]  Does that mean that my model knows how to do arithmetic?
[1529.00s -> 1536.00s]  Or maybe there was just this exact same example was present in the training data.
[1536.00s -> 1547.00s]  So one way you could test for this is by creating counterfactuals, which, based on our understanding of the data, you expect to not be present that frequently in the training data.
[1547.00s -> 1552.00s]  So instead of doing base 10 addition, you could do addition in base 9.
[1552.00s -> 1560.00s]  And then if the model has the same accuracy in base 9, then you can conclude that maybe this model has understood how to do addition.
[1560.00s -> 1571.00s]  Similarly for logic, maybe the reason why the model is so good at solving logic problems is because it's seen something very similar in its training data.
[1571.00s -> 1576.00s]  So what if I construct a world where, I don't know, corgis are reptiles?
[1576.00s -> 1580.00s]  Can it still do this logic problem?
[1581.00s -> 1590.00s]  And so what we find is there is sometimes a pretty significant drop when you move from...
[1590.00s -> 1591.00s]  There's a question?
[1591.00s -> 1593.00s]  Sorry, could you repeat the question?
[1593.00s -> 1597.00s]  Why is base 9 counterfactual in base 10?
[1597.00s -> 1608.00s]  So it's a counterfactual in the sense that the authors comment that base 10 addition is frequently observed in training data.
[1608.00s -> 1611.00s]  But very few people do base 9 addition.
[1611.00s -> 1615.00s]  And so there's going to be much fewer examples of this in training data.
[1615.00s -> 1618.00s]  So it's more so out of distribution rather than counterfactual, right?
[1618.00s -> 1619.00s]  Yeah.
[1619.00s -> 1626.00s]  So you can also call it out of distribution, for sure.
[1626.00s -> 1637.00s]  And yeah, so from results, what we see is there's this drop in performance, even for very simple logic problems that don't involve multiple steps of reasoning.
[1637.00s -> 1648.00s]  So kind of a significant drop in performance, which maybe suggests that there is not that much reasoning, there's more memorization.
[1648.00s -> 1650.00s]  Yeah.
[1650.00s -> 1662.00s]  So we could keep going with this paradigm of changing the problem setting so that it starts looking out of distribution to the training corpus.
[1663.00s -> 1669.00s]  And this is exactly what was done in this paper that looked at analogical reasoning.
[1669.00s -> 1672.00s]  So basically the setup is something like this.
[1672.00s -> 1677.00s]  I'm going to show certain examples of string transformations.
[1677.00s -> 1681.00s]  And I'm going to ask the model to generalize to new examples.
[1681.00s -> 1688.00s]  So in this extend sequence problem, I have ABCD and the output is ABCDE.
[1688.00s -> 1695.00s]  And then given IJKL, the model has to produce IJKLM, and so on.
[1695.00s -> 1709.00s]  Now the way you can make this into a counterfactual or something that is out of distribution is maybe you can change what the extend sequence task is.
[1709.00s -> 1716.00s]  So now instead of outputting ABCDE, maybe the model has to output ABCDF.
[1716.00s -> 1726.00s]  So instead of outputting the next character, it has to output one more, so the second character after the next, and so on.
[1726.00s -> 1736.00s]  The other kind of counterfactual you could add is instead of operating on the standard alphabet, you could modify the alphabet completely.
[1736.00s -> 1742.00s]  So instead of the alphabet being ABCD, maybe you start at XY and so on.
[1743.00s -> 1749.00s]  So what we find is, so we find two things.
[1749.00s -> 1760.00s]  The first thing that we find is that there's a significant drop in performance as we go from the standard sort of analogical reasoning problem to one of these counterfactuals,
[1760.00s -> 1766.00s]  where we either change the alphabet, we change the description of the task so that it becomes slightly unnatural.
[1767.00s -> 1777.00s]  On the other hand, the authors also did this exact same experiment on human subjects, where they find very little decrease in performance.
[1779.00s -> 1789.00s]  So overall, what this result suggests is maybe there's some reasoning, maybe there's some memorization, but there's nothing systematic.
[1790.00s -> 1799.00s]  So again, this is all emerging, so maybe someone will find that if you change your prompt a little bit, now models can do reasoning.
[1799.00s -> 1802.00s]  But this is kind of the current lay of the land.
[1806.00s -> 1809.00s]  So that was sort of the reasoning module of the lecture.
[1809.00s -> 1815.00s]  I'm going to now switch gears and talk about language model agents.
[1815.00s -> 1830.00s]  And this is kind of related to reasoning in the sense that reasoning involves sort of this multi-step inferences where given some facts, you have to arrive at completely new conclusions.
[1830.00s -> 1837.00s]  With agents, what we'll see is that there's some high level kind of objective a model has to accomplish,
[1837.00s -> 1846.00s]  and it has to reason about post-conditions, object affordances, kind of uncertainty in the world to carry out a sequence of steps.
[1848.00s -> 1850.00s]  So let's start with some terminology.
[1852.00s -> 1854.00s]  So we have our agent on the right.
[1855.00s -> 1858.00s]  That's going to be some neural network.
[1858.00s -> 1860.00s]  And then we have an environment.
[1862.00s -> 1866.00s]  I'll give some examples of what these environments could be.
[1868.00s -> 1876.00s]  The agent receives an observation from its environment, and based on the observation, it issues an action.
[1878.00s -> 1886.00s]  And along with that, it receives this second variable, g, and g represents a language instruction.
[1888.00s -> 1893.00s]  So there's many names for this setting and these models.
[1893.00s -> 1898.00s]  Digital agent, language condition policy, or an instruction following agent.
[1901.00s -> 1914.00s]  Some examples of environments are maybe it's sort of a web browser and sort of a browsing environment where the objective is to book a flight from San Francisco to New York.
[1915.00s -> 1925.00s]  And the observation could either be a raw pixel that the model sees, or it could be the HTML DOM representation.
[1927.00s -> 1940.00s]  And the action space, if you're looking at these web environments, could be typing on specific web elements, clicking on web elements, moving your mouse to a certain web element to interact with it, and so on.
[1941.00s -> 1951.00s]  And yeah, this has sort of a vast number of applications. I don't think I can cover all applications, but we can look at some.
[1951.00s -> 1955.00s]  So there's obviously digital assistance.
[1955.00s -> 1962.00s]  I'm not going to say the names because I know people might start popping up.
[1963.00s -> 1970.00s]  But you can give them natural language commands and set an alarm, set reminders, and so on.
[1971.00s -> 1981.00s]  You could also do natural language programming where you could, given natural language descriptions, get a model to write Python code.
[1982.00s -> 1990.00s]  Another example of this could be UI automation where maybe you want to do automated testing of UI elements.
[1991.00s -> 2002.00s]  And so instead of having a human verify whether a UI element works, maybe you can get a model to execute actions corresponding to a given instruction.
[2003.00s -> 2013.00s]  Or it could be something more user-facing where, given some kind of complex environment like Spotify, you could ask an agent to play some songs.
[2014.00s -> 2029.00s]  And then finally, there is this sort of emerging application where we want to add additional tools or plug-ins to language models so that they can control various different applications.
[2033.00s -> 2042.00s]  So before we look at how we can use language models to do instruction following, I think it's very helpful to look at how this was done before language models.
[2043.00s -> 2047.00s]  So there were basically three main ideas.
[2048.00s -> 2059.00s]  Sometimes the right thing to do was collect examples of utterances paired with logical forms.
[2060.00s -> 2071.00s]  So logical forms could be some kind of an executable representation that you could just execute against either a knowledge graph or a database to get an answer.
[2071.00s -> 2075.00s]  So maybe you have a query like, what state borders Texas?
[2076.00s -> 2087.00s]  And then there exists some sort of program description that you could execute against a knowledge graph to get an answer or a list here.
[2088.00s -> 2095.00s]  And idea number one that people looked at was to treat this as almost like machine translation.
[2095.00s -> 2108.00s]  So you have a source language, which is sort of English commands, and then you have a target language, which is sort of these meaning representations or logical forms.
[2109.00s -> 2115.00s]  And then you could apply the same machinery from assignment three to build kind of a natural language interface here.
[2116.00s -> 2123.00s]  So you directly maximize the probability of a sequence of actions given a goal or a command.
[2123.00s -> 2130.00s]  Idea number two was something a little bit more complex.
[2131.00s -> 2136.00s]  So here you have instructions paired with actions.
[2137.00s -> 2150.00s]  Instead of directly mapping instructions to actions, I'm going to infer an executable plan from these instructions and action sequences.
[2150.00s -> 2161.00s]  And I'm going to train a model to go from instructions to these plans and then define a very rich execution model that's going to directly execute these plans.
[2162.00s -> 2177.00s]  The advantage of this is maybe there is more sort of high level decisions you could encode in your plan, which would be harder to get into the model if you were to just train it to produce the action trajectories directly.
[2177.00s -> 2187.00s]  And I have an example of a system like that from 2011, which was basically an agent that could navigate in sort of grounded environments.
[2188.00s -> 2195.00s]  And yeah, the idea was something like this, that we kind of took an instruction and obtained a plan.
[2196.00s -> 2207.00s]  And then you would train a semantic parser, which is basically like this kind of machine translation system that would convert commands into sequences of into this plan.
[2207.00s -> 2218.00s]  And then once that's trained, at test time given a completely new instruction, you would run the semantic parser, get this plan, and then execute it in this execution model.
[2219.00s -> 2227.00s]  And I have an example of an instruction and a plan from this 2011 system.
[2228.00s -> 2239.00s]  The third idea, which is probably maybe the first one that comes to mind if you see a setting like that, is to use reinforcement learning directly.
[2240.00s -> 2246.00s]  And what people did there was to use RL to directly map instructions into actions.
[2246.00s -> 2258.00s]  So I'm going to learn a policy that outputs actions that maximize some reward, which is conditioned on my natural language instruction and the observation.
[2259.00s -> 2269.00s]  And this reward could be both sparse, which is I carry out the entire task and then my environment tells me if I achieve the task or not, or it could be something that I obtain after each step.
[2269.00s -> 2277.00s]  So I take an action and then the environment tells me if this action sort of completed some percentage of my task or not.
[2278.00s -> 2286.00s]  And on the top I've included an example of a system from 2009 that did this for automated Windows debugging.
[2287.00s -> 2300.00s]  And so you have some natural language instruction to click some UI elements and that gets mapped into kind of an API command that the model executes one after the other.
[2300.00s -> 2307.00s]  Okay, so these are basically the three main ideas that people had before language models.
[2308.00s -> 2322.00s]  You would either train semantic parsers, you would either infer these plans from instruction trajectory pairs and then learn to directly model plans and then have an execution model that can execute plans.
[2323.00s -> 2326.00s]  Or you would do reinforcement learning if you had a reward signal.
[2327.00s -> 2329.00s]  So how do we do things in 2024?
[2330.00s -> 2333.00s]  So there are a few ways to think about this.
[2334.00s -> 2339.00s]  I think maybe most instructive is to think about what we are trying to achieve.
[2340.00s -> 2345.00s]  So we are trying to model trajectories, so sequence of actions, conditioned on some goal.
[2346.00s -> 2355.00s]  So I want my model to book a flight from San Francisco to New York and I want it to produce a trajectory of maybe typing and clicking actions.
[2356.00s -> 2358.00s]  So let's look at how that factorizes.
[2359.00s -> 2372.00s]  So the probability of a trajectory conditioned on a goal or an instruction is just the probability of the state, action, next state and so on conditioned on the goal.
[2373.00s -> 2375.00s]  And you could factorize that into two terms.
[2376.00s -> 2380.00s]  So the first term is sort of the transition dynamics of the environment.
[2380.00s -> 2387.00s]  And that's just what happens if I take a certain action in a given state, how is my state going to change?
[2388.00s -> 2399.00s]  And the second object is sort of the agent policy, which is given my goal and the trajectory so far, what is the next action I should be taking?
[2400.00s -> 2408.00s]  And then people quickly realize that you could just treat this as kind of a generative problem.
[2408.00s -> 2416.00s]  So you could treat the problem of decision making in environments as sort of a generative trajectory modeling problem.
[2417.00s -> 2427.00s]  And what I have in sort of the top right is an example of a transformer that just takes the history of actions it's taken so far,
[2427.00s -> 2437.00s]  the current state, and some indication of what task it should achieve here based on reward, but it could be a natural language string.
[2438.00s -> 2440.00s]  And it's just trained to predict what's the next action.
[2441.00s -> 2444.00s]  And you could just train an autoregressive language model to do this.
[2445.00s -> 2449.00s]  And it turned out that this worked very well in sort of an offline RL case.
[2450.00s -> 2451.00s]  Question?
[2452.00s -> 2455.00s]  Why are we predicting one action for the current action?
[2456.00s -> 2462.00s]  So we are predicting one action for the current action.
[2463.00s -> 2471.00s]  So you predict an action, execute that, append that to your trajectory, and then you predict the next action and so on.
[2472.00s -> 2473.00s]  So we resolve it?
[2474.00s -> 2477.00s]  We input tokens into one output token and train an autoregressive.
[2477.00s -> 2482.00s]  And it turned out that this worked really well.
[2483.00s -> 2497.00s]  And so instead of getting these latent plans and training semantic parsers or trying to do reinforcement learning, we started using language models as policies.
[2498.00s -> 2504.00s]  And so a simple way to do all of that is to prompt a language model in a loop.
[2504.00s -> 2508.00s]  So we're going to specify the action space in text.
[2509.00s -> 2512.00s]  So this is like a simple sort of language model agent.
[2513.00s -> 2518.00s]  This is not going to work at all, but probably just illustrative of how agents can be built now.
[2519.00s -> 2521.00s]  So you provide an action space in text.
[2522.00s -> 2532.00s]  So maybe it's a digital environment, and maybe it can type, maybe it can click, maybe it can type characters, maybe it can move mouse somewhere.
[2532.00s -> 2543.00s]  You provided an instruction, and you provide it the sequence of actions and observations it's received so far.
[2544.00s -> 2550.00s]  And then conditioned on all that, you ask it to predict the next action.
[2551.00s -> 2556.00s]  And there's nothing deep going on here. This is just chain of thought prompting in a loop.
[2556.00s -> 2567.00s]  But the hope is that because we reduced the problem of decision making into just autoregressive modeling, this could work.
[2568.00s -> 2573.00s]  And indeed, a slightly more complex version of this can work in some environments.
[2574.00s -> 2585.00s]  So now I'm going to give a little flavor of what different environments look like now for evaluating language models as agents.
[2586.00s -> 2591.00s]  So the simplest environment that people consider is Miniwob.
[2592.00s -> 2598.00s]  So this is a sandbox environment that evaluates basic browser interactions.
[2599.00s -> 2604.00s]  Maybe on a mini Twitter environment, can you get a language model to retweet a given tweet?
[2605.00s -> 2610.00s]  Given a simulated email client, can the model forward someone's email?
[2611.00s -> 2615.00s]  Can it compose an email? Can it click on certain buttons or not?
[2616.00s -> 2619.00s]  It's not at all real world, so it's not real websites.
[2620.00s -> 2622.00s]  And it's a relatively short horizon.
[2623.00s -> 2629.00s]  So given any instruction, most tasks can be accomplished under three actions.
[2630.00s -> 2636.00s]  But zero short performance of even the best language models is still far from perfect, even on this very simple benchmark.
[2638.00s -> 2643.00s]  A second slightly more real world benchmark is Web Arena.
[2643.00s -> 2652.00s]  And this is also a sandbox environment, but it's kind of a pretty close approximation of real websites that span e-commerce.
[2653.00s -> 2659.00s]  So there is a website in Web Arena that resembles Amazon, social media, so something that resembles Twitter.
[2660.00s -> 2663.00s]  And additionally, there are utility tools like maps.
[2664.00s -> 2668.00s]  So an instruction could require a model to open up sort of a map application,
[2668.00s -> 2675.00s]  find the shortest path from point A to point B, and use that in its later sequence of actions.
[2676.00s -> 2679.00s]  And there's multi-tab browsing, like we kind of commonly do.
[2680.00s -> 2683.00s]  So with Minivop, there's only one single tab.
[2684.00s -> 2691.00s]  And with Web Arena, I think this was the first environment that introduced this idea where you kind of have multiple tabs,
[2692.00s -> 2695.00s]  and the agent can sort of switch between tabs.
[2695.00s -> 2699.00s]  And again, we are going to evaluate sort of functional correctness,
[2700.00s -> 2704.00s]  which is whether the model sort of gave the correct answer at the end,
[2705.00s -> 2709.00s]  whether the sequence of steps it took gave the intended behavior,
[2710.00s -> 2714.00s]  as opposed to whether it took a sequence of steps that maybe a user had pre-programmed.
[2715.00s -> 2723.00s]  So another popular kind of environment or data set is Web Links.
[2723.00s -> 2730.00s]  So Web Links also has multi-tab browsing, and it has web interactions on real websites.
[2731.00s -> 2735.00s]  So this is not sandboxed, approximations of real websites is not sandboxed,
[2736.00s -> 2741.00s]  kind of just like browser interactions, these are like actual real websites.
[2742.00s -> 2749.00s]  And it also introduced like a new action where the agent could communicate with the user.
[2749.00s -> 2758.00s]  So maybe there's some instruction which is to reserve kind of, I don't know, like a movie,
[2759.00s -> 2761.00s]  or like buy a movie ticket or something.
[2762.00s -> 2765.00s]  And then at some point, the model has to request credit card information.
[2766.00s -> 2773.00s]  And so there is this like additional action where a human could be involved in communicating with the agent.
[2774.00s -> 2778.00s]  And this is not an environment, but just a collection of interactions.
[2779.00s -> 2783.00s]  So you can't, for example, do any kind of exploration or online learning here,
[2784.00s -> 2786.00s]  but you could definitely use this for evaluating.
[2789.00s -> 2794.00s]  So this was just a taste of what some benchmarks look like for language model agents.
[2795.00s -> 2797.00s]  So how are we going to train these models?
[2798.00s -> 2806.00s]  So given that we are going to treat decision making as causal language modeling,
[2806.00s -> 2810.00s]  we're not going to use any of the ideas from pre-LLMs.
[2811.00s -> 2816.00s]  The standard practice is to do in-context learning with few short examples.
[2817.00s -> 2824.00s]  And in the few short examples, typically for any new kind of website or any new use case,
[2825.00s -> 2830.00s]  you're going to get humans to perform those tasks and sort of feed that into the language models prompt
[2830.00s -> 2839.00s]  as in-context demonstrations, which it could then use to solve similar looking tasks on very similar websites.
[2840.00s -> 2842.00s]  So obviously this is not scalable.
[2843.00s -> 2849.00s]  There's thousands of environments, on some environments there's like lots of different interactions that are possible.
[2850.00s -> 2858.00s]  And so maybe there's something better that we can do than just sort of getting humans to provide demonstrations for every new use case.
[2858.00s -> 2863.00s]  And so we're going to use something that we saw early on in the lecture.
[2864.00s -> 2870.00s]  It was to kind of use the language model to generate rationales and then fine-tune on that.
[2871.00s -> 2874.00s]  And here we don't have rationales, but we could produce action trajectories.
[2875.00s -> 2878.00s]  And then we're going to use that as supervision.
[2879.00s -> 2881.00s]  So the way that looks like is something like this.
[2882.00s -> 2885.00s]  So let's say I have some environment.
[2885.00s -> 2888.00s]  Let's say it's some mini web environment.
[2889.00s -> 2893.00s]  And I'm going to just get an agent that can randomly explore the environment.
[2894.00s -> 2900.00s]  So just execute a random sequence of clicks and types and scrolling operations.
[2901.00s -> 2903.00s]  And let's say it produces some trajectories.
[2904.00s -> 2907.00s]  And now I'm going to use these trajectories and somehow filter them.
[2908.00s -> 2909.00s]  So that was the idea from earlier.
[2910.00s -> 2912.00s]  So you're going to get a bunch of different outputs and then you're going to filter it somehow.
[2912.00s -> 2920.00s]  So here we're going to use a second language model because we don't know what a good trajectory looks like.
[2921.00s -> 2923.00s]  So not like a math problem where you know the correct answer.
[2924.00s -> 2928.00s]  We just had a language model interact with a website and generate trajectories.
[2929.00s -> 2931.00s]  And we want to somehow filter out what are good trajectories.
[2932.00s -> 2939.00s]  And so we're going to use a second model that will produce a description of these trajectories.
[2939.00s -> 2947.00s]  And the idea here is that if you can get a model to produce a description of what the sequence of actions corresponds to,
[2948.00s -> 2951.00s]  then maybe that's a good enough signal for a good trajectory.
[2952.00s -> 2960.00s]  And so maybe given the first trajectory it guesses that the instruction was to book a flight from San Francisco to New York.
[2961.00s -> 2966.00s]  For the second trajectory it said set the date to some given date.
[2966.00s -> 2973.00s]  And maybe it wasn't able to come up with any good sort of instruction for the third trajectory.
[2974.00s -> 2981.00s]  And then we're going to do something again that we saw earlier on which is to kind of do this iteratively.
[2982.00s -> 2987.00s]  So now we have a goal that we got for a trajectory.
[2988.00s -> 2992.00s]  And now I'm going to get the language model to condition its behavior on this goal.
[2992.00s -> 2996.00s]  So the goal is to set the date as some given date.
[2997.00s -> 3006.00s]  And now instead of doing random exploration, the model is going to produce a sequence of actions that have a better correspondence with some natural language instruction.
[3007.00s -> 3011.00s]  So it produced a trajectory based on that instruction.
[3012.00s -> 3025.00s]  And then I'm going to use some course filter that's just going to look at correspondences between the instruction and the sequence of actions and the states the language model visited.
[3026.00s -> 3031.00s]  And use that to decide if the trajectory was a good trajectory for the instruction.
[3032.00s -> 3039.00s]  And in this case, given the instruction, this seems like a pretty good trajectory for completing this task.
[3039.00s -> 3043.00s]  And so then we add it to a set of examples.
[3044.00s -> 3048.00s]  But maybe sometimes things are not so good.
[3049.00s -> 3055.00s]  So for that second instruction, the generator label was to book a flight from San Francisco to New York.
[3056.00s -> 3061.00s]  And let's say we run that again through the language model and it produced a second trajectory.
[3062.00s -> 3069.00s]  And clearly this does not seem like a successful trajectory corresponding to booking a flight.
[3070.00s -> 3072.00s]  And so what do we do here?
[3073.00s -> 3076.00s]  Maybe we can throw away this interaction, but interactions are pretty costly.
[3077.00s -> 3083.00s]  Specifically, if you're looking at real websites, then each interaction could take a few milliseconds.
[3084.00s -> 3086.00s]  And so maybe we don't want to throw away this interaction.
[3086.00s -> 3093.00s]  So what we're going to do here is again invoke the relabeler to take the trajectory and assign it a new label.
[3094.00s -> 3099.00s]  So the model was not successful at accomplishing the task it set out to do, but it accomplished something.
[3100.00s -> 3103.00s]  And we're going to come up with the best guess of what that was with the second language model.
[3104.00s -> 3112.00s]  And it's going to say that, okay, maybe the instruction you accomplished instead was to set the origin to SFO and the destination to New York City.
[3113.00s -> 3117.00s]  And so that's going to get fed back into the language model.
[3118.00s -> 3124.00s]  And we're going to keep doing this iteratively until our filter says that this is a good instruction trajectory pair.
[3125.00s -> 3137.00s]  So we have the same idea of using a language model to generate outputs and some iterative procedure that will give us a good set of training examples.
[3138.00s -> 3142.00s]  So overall the method looks something like this.
[3143.00s -> 3145.00s]  You have some environment.
[3146.00s -> 3154.00s]  We are going to use kind of an unconditioned language model to just randomly explore the environment and generate a sequence of trajectories.
[3155.00s -> 3165.00s]  And then we're going to convert these trajectories into synthetic training data by iteratively converting trajectories into natural language descriptions
[3165.00s -> 3170.00s]  and then taking natural language descriptions and converting them into even better trajectories and so on.
[3171.00s -> 3176.00s]  And once we have this collection of synthetic examples, there are two things we could do.
[3177.00s -> 3191.00s]  One could fine tune using this data, but the simplest thing you could do is kind of repeat the paradigm earlier of replace human provided demonstrations in context with these synthetic demonstrations.
[3192.00s -> 3199.00s]  And we find a reasonable boost in performance, a 13 point improvement on the Minivob benchmark.
[3200.00s -> 3207.00s]  And again, even though Minivob is very, very simple, zero short performance for even the best language models is far from perfect.
[3208.00s -> 3213.00s]  And we also see an improvement on second sort of multi-step tool use environment.
[3214.00s -> 3217.00s]  But so far we've only looked at text, right?
[3217.00s -> 3228.00s]  But maybe for real world applications it's kind of intractable to, for every environment, obtain the HTML and feed that into the language models context.
[3229.00s -> 3240.00s]  Sometimes there can be tens of thousands of DOM elements and then corresponding JavaScript and inputting all that into the language models context could be intractable.
[3240.00s -> 3246.00s]  And maybe that's also not the best way to kind of show the state of the environment.
[3247.00s -> 3253.00s]  Maybe the best way is to directly show the pixels corresponding to the environment.
[3254.00s -> 3261.00s]  And so now we're going to look at some examples of vision language models that people have used for building these agents.
[3264.00s -> 3268.00s]  So the first one that we're going to look at is lava.
[3268.00s -> 3275.00s]  And the idea here is again kind of similar to ORCA that we looked at in sort of the reasoning half of the lecture.
[3276.00s -> 3286.00s]  We're going to use GPT-4 to generate, this time, both instructions and responses for textual descriptions of images.
[3287.00s -> 3297.00s]  So maybe there's an image and we're going to sort of use metadata corresponding to that image to come up with a textual description.
[3298.00s -> 3304.00s]  Feed that into GPT-4 and ask it to generate possible questions and responses.
[3305.00s -> 3321.00s]  And then we're going to jointly fine-tune sort of an image encoder here clip along with a text-only decoder here, Vicuna, which is a llama model that is instruction-tuned.
[3322.00s -> 3330.00s]  And through this sort of joint fine-tuning, at the end we kind of get this image encoder that can output language responses.
[3331.00s -> 3340.00s]  And now we can sort of ask questions about images, maybe use that to directly input screenshots instead of HTML DOM elements.
[3341.00s -> 3354.00s]  So a second approach that looked at sort of building joint image language models that then people later adapted to agents was Pix2Struct.
[3355.00s -> 3360.00s]  And the idea is again very similar. There's an image encoder and a text decoder.
[3361.00s -> 3371.00s]  The image encoder will sort of take the image, convert them into patches, and assign each patch sort of a position ID, run that through a transformer.
[3372.00s -> 3375.00s]  And then there's a decoder that will decode out some text.
[3376.00s -> 3381.00s]  One of the new things that Pix2Struct introduced was a new pre-training task.
[3381.00s -> 3394.00s]  So for a lot of the pre-training was fairly simple. We're going to use GPT-4 to just generate sort of synthetic questions and responses based on textual descriptions of images.
[3395.00s -> 3397.00s]  But there's only so far you can go with textual descriptions of images.
[3398.00s -> 3413.00s]  What Pix2Struct did was to look at screenshots from websites and mask out screenshots and then ask the transformer decoder to produce HTML corresponding to the masked out elements.
[3414.00s -> 3420.00s]  So here there's like this list that has a corresponding HTML.
[3421.00s -> 3425.00s]  One of the data points in Pix2Struct looks something like this.
[3425.00s -> 3439.00s]  So you might mask out let's say the first answer corresponding to Python and ask the model to produce the HTML corresponding to just the patch that was masked out.
[3440.00s -> 3450.00s]  And so this seems like a more natural sort of pre-training objective that can maybe have better interactions between image and text.
[3450.00s -> 3455.00s]  And then this was also adapted for building these multi-modal agents.
[3456.00s -> 3463.00s]  So at this point I just want to kind of highlight that this is really an emerging application.
[3464.00s -> 3469.00s]  There's kind of this huge kind of prompting gap is what I like to call it.
[3470.00s -> 3479.00s]  So if you do not do extensive prompting and if you do not use bespoke few shot examples where for every different environment you have a different set of few shot examples,
[3480.00s -> 3486.00s]  even the best language models are very, very far from perfect even on very, very simple tasks like Minivob.
[3487.00s -> 3496.00s]  The goal was just to click on certain elements or respond to someone's email where in Minivob that just takes like five actions.
[3496.00s -> 3519.00s]  And then even for something as simple as Minivob, even after doing extensive prompting and few shot examples, there's this like drop in performance as you go from sort of the simplest task that involves mapping an instruction into a single action to mapping an instruction into maybe five or ten actions.
[3520.00s -> 3526.00s]  So long horizon planning is still very, very hard even on these very simple benchmarks.
[3527.00s -> 3538.00s]  And then if you look at something more complex like Web Arena, which tries to approximate real websites, has multi-tab browsing, has external tools that the model can use.
[3538.00s -> 3550.00s]  There's just a huge difference between sort of human level task success rate and what the best models get even after prompting, even with few shot examples.
[3553.00s -> 3559.00s]  And then the kinds of errors models make are also pretty weird.
[3559.00s -> 3569.00s]  So one of the examples from Web Links was the task was to just open Google Translate and sign in using credentials.
[3570.00s -> 3572.00s]  And it was an email and a password.
[3573.00s -> 3580.00s]  And then what GPT-4V did was instead of typing in the password, it just typed in the email into the password tab.
[3580.00s -> 3582.00s]  And it just couldn't recover from this error.
[3583.00s -> 3589.00s]  So it tried to sign in, there was an error, it tried to insert, tried to type in the email again and so on.
[3590.00s -> 3594.00s]  And I'm sure with extensive prompting you can fix this, and maybe that's besides the point, right?
[3594.00s -> 3611.00s]  And then again, there was a different example where the model had to issue a search, and then instead of issuing the search with the correct term, it sort of repeated the same term like three times.
[3612.00s -> 3617.00s]  And obviously that's not going to return any results.
[3617.00s -> 3625.00s]  So there's a lot of room for improvement, as you can see, and there's lots to be done in this space.
[3626.00s -> 3629.00s]  Okay, so I'm going to recap and take any questions.
[3630.00s -> 3633.00s]  So we kind of looked at two different things today.
[3634.00s -> 3636.00s]  We looked at reasoning and language models.
[3637.00s -> 3642.00s]  We saw that there's a few ways that you can get reasoning-like behavior in language models.
[3642.00s -> 3647.00s]  You can prompt them in various ways, so the simplest example of that is chain of thought prompting.
[3648.00s -> 3658.00s]  You can do chain of thought prompting, but generate multiple rationales and sort of try to reconcile them and pick the answer that was most frequent.
[3659.00s -> 3668.00s]  You can do sort of problem decomposition in your prompt, so ask the model to explicitly decompose a problem into multiple steps before answering.
[3669.00s -> 3670.00s]  So that was all prompting.
[3670.00s -> 3682.00s]  You could also try and train specialized small language models for reasoning by generating rationales from a big language model and then fine-tuning a smaller language model on these rationales.
[3683.00s -> 3694.00s]  Instead of fine-tuning a smaller language model on rationales from a big language model, you could just fine-tune the big language model on its own rationales and keep doing this iteratively.
[3694.00s -> 3704.00s]  And we saw that sometimes if you do multiple iterations of that, performance can keep improving and can even outperform human-provided rationales.
[3705.00s -> 3716.00s]  But on the flip side, we saw that while there are some initial reasons to be optimistic, if we go and do counterfactual evaluation,
[3716.00s -> 3728.00s]  we see that it's not clear if the models are good because the reasoning or if models are good because all of these problems are in some shape or form already in the training data.
[3729.00s -> 3731.00s]  And we saw that with counterfactual evaluation.
[3732.00s -> 3735.00s]  In the second part, we looked at language model agents.
[3736.00s -> 3742.00s]  We kind of talked about the historical perspective through which people build grounded agents.
[3742.00s -> 3750.00s]  And then we saw that you could recast the problem of decision-making as just causal language modeling.
[3751.00s -> 3757.00s]  And then we looked at various ways through which people have modeled decision-making with language models.
[3758.00s -> 3760.00s]  Most of it involves prompting and in-context learning.
[3761.00s -> 3769.00s]  And then we looked at a method similar to what we saw in the first module, generating synthetic demonstrations.
[3769.00s -> 3774.00s]  And here we looked at doing exploration and the same kind of iterative relabeling.
[3775.00s -> 3779.00s]  Most of the language models we looked at today were text-only.
[3780.00s -> 3786.00s]  We saw some examples of language models that can take both text and visual input.
[3787.00s -> 3791.00s]  And then we saw that benchmarks are very, very challenging.
[3792.00s -> 3793.00s]  Models make kind of trivial mistakes.
[3794.00s -> 3798.00s]  There's a huge gap between human performance and sort of what we get with models.
[3799.00s -> 3807.00s]  So there's a huge difference between human performance and where models are and a lot of room for driving further improvement.
[3808.00s -> 3812.00s]  And maybe some of you are doing it for your projects. Thank you.
