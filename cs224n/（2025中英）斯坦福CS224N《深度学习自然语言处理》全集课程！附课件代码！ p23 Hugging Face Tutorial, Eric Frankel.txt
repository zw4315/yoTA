# Detected language: en (p=1.00)

[0.00s -> 14.08s]  Hi, everyone. Welcome to the 224n Hugging Face Transformers tutorial. So this tutorial
[14.08s -> 19.66s]  is just going to be about using the Hugging Face library. It's really useful and a super
[19.66s -> 25.92s]  effective way of being able to use kind of some off-the-shelf NLP models, specifically
[25.92s -> 31.28s]  models that are kind of transformer-based, and being able to use those for either your
[31.28s -> 36.28s]  final project, your custom final project, or something like that, just using it in the
[36.28s -> 43.00s]  future. So it's a really helpful package to learn, and it interfaces really well with
[43.00s -> 45.84s]  PyTorch in particular, too.
[45.84s -> 51.64s]  Okay, so first things first is in case there's anything else that you are missing
[51.64s -> 56.84s]  from this kind of tutorial, the Hugging Face documentation is really good. They also have
[56.84s -> 62.36s]  lots of tutorials and walkthroughs, as well as other notebooks that you can play around
[62.36s -> 67.04s]  with as well. So if you're ever wondering about something else, that's a really good
[67.04s -> 68.56s]  place to look.
[68.56s -> 72.60s]  Okay, so in the Colab, the first thing we're going to do, that I already did but
[72.60s -> 77.80s]  can maybe run again, is just installing the Transformers Python package, and then
[77.80s -> 84.20s]  the Datasets Python package. So this corresponds to the Hugging Face transformers
[84.20s -> 89.52s]  and datasets, and so those are really helpful. The transformers is where we'll get a lot
[89.52s -> 93.94s]  of these kind of pre-trained models from, and the datasets will give us some helpful
[93.94s -> 100.76s]  datasets that we can potentially use for various tasks, so in this case sentiment analysis.
[100.76s -> 106.84s]  Okay, and so we'll use a bit of like a helper function for helping us understand what
[106.84s -> 112.60s]  encoding is, what encodings are actually happening as well. So we'll run this just
[112.60s -> 120.12s]  to kind of kick things off and import a few more things. Okay, so first what we'll
[120.12s -> 124.28s]  do is, this is generally kind of like the step-by-step for how to use something
[124.28s -> 130.28s]  off of Hugging Face. So first what we'll do is we'll find some model from like
[130.28s -> 135.64s]  the Hugging Face hub here, and note that there's like a ton of different models that
[135.64s -> 141.08s]  you're able to use. There's BERT, there's GPT-2, there's T5 Small, which is another
[141.08s -> 147.08s]  language model from Google. So there are a bunch of these different models that are
[147.08s -> 151.06s]  pre-trained, and all of these weights are up here in Hugging Face that are freely
[151.06s -> 155.88s]  available for you guys to download. So if there's a particular model you're interested
[155.88s -> 160.52s]  in, you can probably find a version of it here. You can also see kind of different
[160.52s -> 165.64s]  types of models on the side as well for specific tasks. So if we wanted to
[165.64s -> 171.12s]  do something like zero-shot classification, there are a couple models
[171.12s -> 175.84s]  that are specifically good at doing that particular task. Okay, so based off
[175.84s -> 179.16s]  of what task we're looking for, there's probably a Hugging Face model
[179.16s -> 184.36s]  for it that's available online for you to download. Okay, so that's what we'll do
[184.36s -> 189.44s]  first, is we'll go ahead and find a model in the Hugging Face hub, and then
[189.44s -> 193.56s]  you know, whatever you want to do. In this case, we'll do sentiment analysis,
[193.56s -> 198.64s]  and then there are two things that we need next. The first is a tokenizer for
[198.64s -> 203.08s]  actually, you know, splitting your input text into tokens that your model can
[203.08s -> 210.28s]  use, and the actual model itself. And so the tokenizer again kind of converts
[210.28s -> 214.36s]  this to some vocabulary IDs, these discrete IDs that your model can
[214.36s -> 218.60s]  actually take in, and the model will produce some prediction based off of
[218.60s -> 226.40s]  that. Okay, so first what we can do is again import this auto tokenizer and
[226.40s -> 232.64s]  this auto model for sequence classification. So what this will do
[232.64s -> 237.72s]  initially is download some of the, you know, key things that we need so that we
[237.72s -> 242.28s]  can actually initialize these. So what do each of these do? So first the
[242.28s -> 248.24s]  tokenizer, this auto tokenizer, is from some pre-trained tokenizer that is
[248.28s -> 253.00s]  already been used. So in general, there's a corresponding tokenizer for every
[253.00s -> 258.04s]  model you want to try and use. In this case, it's like CBERT, so like
[258.04s -> 263.16s]  something around sentiment and Roberta. And then the second is you can import
[263.16s -> 267.88s]  this model for sequence classification as well from something pre-trained on the
[267.88s -> 272.64s]  model hub again. So again, this corresponds to sentiment, Roberta, large
[272.68s -> 284.20s]  English. And if we want, we can even find this over here, we can find it as, I think
[284.20s -> 288.68s]  English, yeah, large English. So again, this is something we can easily find, you
[288.68s -> 293.84s]  just copy this string up here and then you can import that. Okay, we've
[293.84s -> 297.84s]  downloaded all of the kind of, all the things that we need, some kind of like
[297.84s -> 302.36s]  binary files as well, and then now we can go ahead and actually, you know, use
[302.36s -> 307.20s]  some of these inputs, right? So this gives you some set of an input, right? This
[307.20s -> 311.36s]  input string, I'm excited to learn about hugging face transformers. We'll get some
[311.36s -> 318.96s]  tokenized inputs here from the actual tokenized things here after we
[318.96s -> 322.88s]  pass it through the tokenizer. And then lastly, we'll get some notion of
[322.88s -> 328.32s]  the model output that we get. So this is kind of some legit's here over
[328.36s -> 333.00s]  whatever classification that we have. So in this case, good or bad, and then some
[333.00s -> 337.16s]  corresponding prediction. Okay, and we'll walk through what this kind of looks
[337.16s -> 340.92s]  like in just a second as well, a little more depth. This is broadly kind
[340.92s -> 345.12s]  of like how we can actually use these together. We'll tokenize some input
[345.12s -> 349.00s]  and then we'll pass these inputs through the model. So we'll talk about
[349.00s -> 354.92s]  tokenizers first. So tokenizers are used for basically just
[354.96s -> 358.32s]  pre-processing the inputs that you get for any model, and it takes some raw
[358.32s -> 365.40s]  string to like essentially a mapping to some number or ID that the model can
[365.40s -> 371.24s]  take in and actually kind of understand. So tokenizers are either kind of like
[371.24s -> 375.72s]  are specific to the model that you want to use, or you can use the
[375.72s -> 379.64s]  autotokenizer that will kind of conveniently import whatever
[379.72s -> 385.52s]  corresponding tokenizer you need for that model type. So that's kind of
[385.52s -> 388.68s]  like the helpfulness of the autotokenizer. It'll kind of make that
[388.68s -> 393.80s]  selection for you and make sure that you get the correct tokenizer for
[393.80s -> 397.48s]  whatever model you're using. So the question is, does it make sure that
[397.48s -> 400.56s]  everything is mapped to the correct index that the model is trained on? The
[400.56s -> 405.60s]  answer is yes. So that's why the autotokenizer is helpful. So there
[405.64s -> 412.08s]  are two types of tokenizers. There's the Python tokenizer, and there's also
[412.08s -> 417.92s]  like a tokenizer fast. The tokenizer fast is written in Rust. In
[417.92s -> 422.72s]  general, if you do the autotokenizer, it'll just default to the fast one. There's
[422.72s -> 425.68s]  not really a huge difference here. It's just about kind of like the
[425.68s -> 430.56s]  inference time for getting the model outputs. Yeah, so the question is, the
[430.56s -> 437.56s]  tokenizer creates dictionaries of the model inputs. So it's more
[437.56s -> 443.04s]  like I think the way to think about a tokenizer is like that
[443.04s -> 447.92s]  dictionary almost, right? So you want to kind of translate almost or have this
[447.92s -> 453.28s]  mapping from the tokens that you can get from like this string and then map
[453.28s -> 457.68s]  that into kind of some inputs that the model will actually use. So we'll see an
[457.68s -> 462.60s]  example of that in just a second. So for example, we can kind of call the
[462.60s -> 466.96s]  tokenizer in any way that we would for like a typical PyTorch model, but
[466.96s -> 470.80s]  we're just going to call it on like a string. So here we have our input
[470.80s -> 474.84s]  string is hugging face transformers is great. We pass that into the
[474.84s -> 479.00s]  tokenizer almost like it's like a function, right? And then we'll get out
[479.00s -> 485.04s]  some tokenization. So this gives us a set of input IDs. So to answer the
[485.04s -> 488.68s]  earlier question, these are basically the numbers that each of these tokens
[488.68s -> 493.76s]  represent, right? So that the model can actually use them. And then a
[493.76s -> 502.04s]  corresponding attention mask for the particular transformer. Okay, so there are
[502.04s -> 508.32s]  a couple ways of accessing the actual tokenized input IDs. You can treat it
[508.32s -> 511.44s]  like a dictionary, so hence kind of thinking about it almost as that
[511.48s -> 516.24s]  dictionary form, it's also just like a property of the output that you get. So
[516.24s -> 523.84s]  there are two ways of accessing this in like a pretty Pythonic way. Okay, so
[523.84s -> 529.56s]  what we can see as well is that we can look at the particular, the actual
[529.56s -> 533.76s]  kind of tokenization process almost. And so this can maybe give some insight
[533.76s -> 539.08s]  into what happens at each step, right? So our initial input string is going to
[539.12s -> 543.84s]  be hugging face transformers is great. Okay, the next step is that we actually
[543.84s -> 550.48s]  want to tokenize these individual kind of individual words that are passed in.
[550.48s -> 556.44s]  So here, this is the kind of output of this tokenization step, right? We get
[556.44s -> 563.56s]  kind of these individual split tokens, we'll convert them to IDs here, and then
[563.56s -> 568.72s]  we'll add any special tokens that our model might need for actually
[568.72s -> 575.40s]  performing inference on this. So there's a couple steps that happen kind of
[575.40s -> 580.84s]  like underneath when you use a tokenizer that
[580.84s -> 586.80s]  happens at a few things at a time. One thing to note is that for fast
[586.80s -> 593.00s]  tokenizers as well, there's another option that you're able to get to. So
[593.00s -> 598.12s]  you have essentially, right, you have this input string, you have the number of
[598.12s -> 603.24s]  tokens that you get, you might have some notion of like the special token mask
[603.24s -> 609.04s]  as well. So using char to word is going to give you like the word piece
[609.04s -> 613.16s]  of a particular character in the input. So here this is just giving you
[613.16s -> 617.04s]  additional options that you can use for the fast tokenizer as well for
[617.04s -> 624.40s]  understanding how the tokens are being used in the, from the input string.
[625.36s -> 631.52s]  Okay, so there are different ways of using the outputs of these tokenizers too.
[631.52s -> 637.92s]  So one is that, you know, you can pass this in and if you indicate that you
[637.92s -> 642.68s]  want it to return a tensor, you can also return a PyTorch tensor. So that's
[642.68s -> 649.92s]  great in case you need a PyTorch tensor, which you probably generally want. You
[649.92s -> 655.44s]  can also add multiple tokens into the tokenizer and then pad them as however
[655.44s -> 663.24s]  you need. So for here, for example, we can use the pad token as being this kind of
[663.24s -> 668.88s]  like pad bracket almost, and giving the token ID is going to correspond to zero.
[668.88s -> 673.48s]  So this is just going to add padding to whatever input that you give. So if
[673.48s -> 678.28s]  you need, you need your outputs to be the same length for a particular type of
[678.28s -> 682.00s]  model, right, this will add those padding tokens and then correspondingly
[682.00s -> 685.52s]  gives you like the zeros and the attention mask where you actually need
[685.52s -> 694.12s]  it. Okay, and so the way to do that here is you basically set padding to
[694.12s -> 698.24s]  be true and also set truncation to be true as well. And so if you ever have
[698.24s -> 704.96s]  kind of like more, any other kind of like features of the tokenizer that
[704.96s -> 708.80s]  you're interested in, again you can check the hugging face documentation,
[708.80s -> 713.44s]  which is pretty thorough for what each of these things do. Yeah, so the
[713.44s -> 718.56s]  question is kind of looking at, looking at the the hash hash at least
[718.56s -> 724.52s]  and whether that means that we should have like a space before or not. So, so
[724.52s -> 730.52s]  here in this case, yeah, so in this case we probably don't want like the space
[730.52s -> 737.20s]  before, right, just because we have like the hugging, like I don't know, hugging is
[737.20s -> 744.04s]  all one word in this case. Generally, like, generally the, for like the
[744.04s -> 748.80s]  tokenizers, generally the output that they give is still pretty consistent
[748.80s -> 752.88s]  though in terms of how the tokenization process works. So there
[752.88s -> 756.48s]  might be kind of these like, you know, instances where it might be contrary to
[756.64s -> 762.04s]  what you might expect for kind of how something is tokenized. In general, the
[762.04s -> 768.68s]  tokenization generally works fine. So in most cases, kind of like the direct
[768.68s -> 774.00s]  output that you get from the hugging face tokenizer is sufficient.
[777.28s -> 783.32s]  Okay, awesome. So one last thing past the adding kind of additional padding is
[783.36s -> 790.48s]  that you can also kind of decode like an entire batch at one, one given time. So
[790.48s -> 797.00s]  if we look again, we have like our tokenizer will additionally have this
[797.00s -> 802.16s]  method called like a batch decode. So if we have like the model inputs that we
[802.16s -> 806.72s]  get up here, this is the output of passing these sentences or these strings
[806.76s -> 814.28s]  into the tokenizer. We can go ahead and just pass like these input IDs that
[814.28s -> 819.00s]  correspond to that into the batch decode, and it'll give us kind of this
[819.00s -> 823.40s]  good, this decoding that corresponds to all the padding we added in, each of
[823.40s -> 830.24s]  the particular kind of like words and strings. And if you want to, you know,
[830.24s -> 835.60s]  ignore all the the presence of these padding tokens or anything like that, you
[835.60s -> 841.28s]  can also pass that in as skipping the special tokens. Gotcha. So this gives
[841.28s -> 845.24s]  like a, this is a pretty high-level overview of how you would want to
[845.24s -> 851.88s]  use tokenizers, I guess, in using Hugging Face. So now we can
[851.88s -> 857.04s]  talk about maybe how to use the Hugging Face models themselves. So again,
[857.04s -> 861.40s]  this is pretty similar to what we're seeing for something like
[861.44s -> 867.88s]  initially using a tokenizer. You just choose the specific model type for your
[867.88s -> 874.48s]  model, and then you can use that or the specific kind of auto model class, where
[874.48s -> 879.84s]  again this auto model kind of takes almost the like the initialization
[879.84s -> 884.68s]  process, it takes care of it for you in a pretty easy way without really any
[884.68s -> 891.96s]  too much overhead. So additionally, so for the pre-trained transformers that we
[891.96s -> 896.24s]  have, they generally have the same underlying architecture, but you'll have
[896.24s -> 901.16s]  different kind of heads associated with each transformer. So attention heads
[901.16s -> 904.84s]  that you might have to train if you're doing some sequence classification or
[904.84s -> 910.52s]  just some other task. So Hugging Face will do this for you, and so for this
[910.52s -> 917.44s]  I will walk through an example of how to do this for sentiment analysis. So if
[917.44s -> 921.36s]  there's a specific context like sequence classification we want to use,
[921.36s -> 927.36s]  we can use like this the very specific kind of like class a Hugging
[927.36s -> 932.76s]  Face provides, so distilbert for sequence classification. Alternatively, if we
[932.76s -> 937.20s]  were doing it using distilbert in like a mass language model setting, we
[937.20s -> 942.52s]  use distilbert for massed LM, and then lastly if we're just doing it purely for
[942.52s -> 946.06s]  the representations that we get out of distilbert, we just use like the
[946.06s -> 950.96s]  baseline model. So the key takeaway is that there are some
[950.96s -> 956.36s]  task specific classes that we can use from Hugging Face to initialize. So
[956.36s -> 962.24s]  auto model again is similar to kind of like the auto tokenizer. So for this
[962.28s -> 968.68s]  it's just going to kind of load by default that specific model, and so in
[968.68s -> 972.88s]  this case it's going to be just like kind of like the basic basic weights
[972.88s -> 982.36s]  that you need for that. Okay so here we'll have basically three
[982.36s -> 985.80s]  different types of models that we can look at. One is like an encoder type
[985.80s -> 993.30s]  model which is BERT, a decoder type model like GPT-2, it's like performing
[993.30s -> 998.08s]  these like you know generating some text potentially, and encoder decoder
[998.08s -> 1003.12s]  models so BART or T5 in this case. So again if you go back to kind of the
[1003.12s -> 1008.72s]  the Hugging Face hub, there's a whole sort of different different types of
[1008.72s -> 1012.52s]  models that that you could potentially use, and if we look in the
[1012.52s -> 1018.56s]  documentation as well. So here we can understand some notion of like the
[1018.56s -> 1022.86s]  different types of classes that we might want to use. Right so there's some
[1022.86s -> 1027.64s]  notion of like the auto tokenizer, different auto models for different
[1027.64s -> 1032.72s]  types of tasks. So here again if you have any kind of like specific use
[1032.72s -> 1037.56s]  cases that you're looking for, then you can check the documentation. Here again
[1037.56s -> 1041.76s]  if you use like an auto model from like pre-trained, you'll just create a
[1041.80s -> 1048.04s]  model that's an instance of that BERT model. In this case BERT model for BERT
[1048.04s -> 1056.60s]  based case. Okay let's, we can go ahead and start. One last thing to note is
[1056.60s -> 1062.52s]  that like again the particular choice of your model matches up with kind of
[1062.52s -> 1066.00s]  the type of architecture that you have to use. Right so there are
[1066.00s -> 1071.48s]  different, these different types of models can perform specific tasks. So you're not
[1071.52s -> 1076.84s]  going to be able to kind of load or use BERT for instance or distill BERT as like
[1076.84s -> 1080.88s]  a sequence-to-sequence model for instance, which requires the encoder and
[1080.88s -> 1086.88s]  decoder because distill BERT only consists of an encoder. So there's
[1086.88s -> 1090.52s]  a bit of like a limitation on how you can exactly use these, but it's
[1090.52s -> 1098.64s]  basically based on like the model architecture itself. Okay awesome so let's
[1098.68s -> 1105.68s]  go ahead and get started here. So similarly here we can import so auto
[1105.68s -> 1109.52s]  model for sequence classification. So again this is, we're going to perform
[1109.52s -> 1114.52s]  some classification task and we'll import this auto model here so that we
[1114.52s -> 1118.08s]  don't have to reference again something like distill BERT for
[1118.08s -> 1121.64s]  sequence classification. We'll be able to load it automatically and it'll be
[1121.64s -> 1127.22s]  all set. Alternatively we can do distill BERT for sequence classification
[1127.26s -> 1132.50s]  here and that specifically will require distill BERT to be the input there.
[1132.50s -> 1136.74s]  Okay so these are two different ways of basically getting the same model here.
[1136.74s -> 1143.74s]  One using the auto model, one using just explicitly distill BERT. And here
[1143.74s -> 1147.70s]  because it's classification we need to specify the number of labels or the
[1147.70s -> 1150.90s]  number of classes that we're actually going to classify for each of the
[1150.98s -> 1158.98s]  input sentences. Okay so here we'll get some like a warning here right if you
[1158.98s -> 1162.82s]  are following along and you print this out because some of the sequence
[1162.82s -> 1168.54s]  classification parameters aren't trained yet and so we'll go ahead and
[1168.54s -> 1174.58s]  take care of them. So here similarly we'll kind of like walk through how to
[1174.58s -> 1179.98s]  how to actually you know train some of these models. So the first is how do you
[1180.02s -> 1184.46s]  actually pass any of the inputs that you get from a tokenizer into the model?
[1184.46s -> 1191.66s]  Okay well if we get some model inputs from the tokenizer up here and we
[1191.66s -> 1198.10s]  pass this into the model by specifying that the input IDs are the input IDs
[1198.10s -> 1203.90s]  from the model inputs. And likewise we want to emphasize or we can you know
[1203.90s -> 1207.86s]  show here and specifically pass in that the attention mask is going to
[1207.86s -> 1212.18s]  correspond to the attention mask that we gave from these like these outputs of
[1212.18s -> 1217.38s]  the tokenizer. Okay so this is option one where you can specifically
[1217.38s -> 1224.34s]  identify which property goes to what. The second option is using kind of a
[1224.34s -> 1230.66s]  Pythonic hack almost which is where you can directly pass in the model
[1230.66s -> 1237.82s]  inputs and so this will basically unpack almost the keys of like the model
[1237.82s -> 1243.50s]  inputs here. So the model input keys, so the input IDs correspond to this. The
[1243.50s -> 1248.10s]  attention mask corresponds to the attention mask argument. So when we use
[1248.10s -> 1252.98s]  this star star kind of syntax this will go ahead and unpack our dictionary
[1252.98s -> 1257.26s]  and basically map the arguments to something of the same keys. So this is
[1257.26s -> 1262.06s]  an alternative way of passing it into the model. Both are going to be the
[1262.06s -> 1268.98s]  same. Okay so now what we can do is we can actually print out what the model
[1268.98s -> 1273.86s]  outputs look like. So again these are the inputs, the token IDs and the
[1273.86s -> 1279.98s]  attention mask. And then second we'll get the actual model outputs. So here
[1279.98s -> 1285.54s]  notice that the outputs are given by kind of these logits here. There's two
[1285.54s -> 1289.10s]  of them, we passed in one example and there's kind of two potential classes
[1289.10s -> 1293.82s]  that we're trying to classify. Okay and then lastly we have of course the
[1293.82s -> 1298.54s]  corresponding distribution over the labels here, right, since this is going
[1298.54s -> 1303.02s]  to be binary classification. Yes it's like a little bit weird that you're
[1303.02s -> 1307.36s]  gonna have like the two classes for the binary classification task and you
[1307.36s -> 1312.90s]  could basically just choose to classify one class or not. But we do this just
[1312.90s -> 1319.18s]  basically because of how Hugging Face models are set up. And so
[1319.18s -> 1324.14s]  additionally, you know, these are the models that we load in from Hugging
[1324.14s -> 1328.86s]  Face are basically just PyTorch modules. So these are the actual
[1328.86s -> 1332.84s]  models and we can use them in the same way that we've been using models
[1332.84s -> 1336.74s]  before. So that means things like loss.backward or something like that
[1336.74s -> 1342.58s]  actually will do this back propagation step corresponding to the loss of like
[1342.58s -> 1347.78s]  your inputs that you pass in. So it's really easy to train these guys
[1347.78s -> 1351.98s]  as long as you have like a label, you know, label for your data. You can
[1351.98s -> 1358.10s]  calculate your loss using, you know, the PyTorch cross entropy function. You get
[1358.10s -> 1362.70s]  some loss back and then you can go ahead and back propagate it. You can
[1362.70s -> 1367.58s]  actually even get kind of the parameters as well in the model that
[1367.58s -> 1372.34s]  you would probably get updated from this. This is just some big tensor of the
[1372.34s -> 1376.82s]  actual embedding weights that you have.
[1376.82s -> 1382.94s]  Okay, we also have like a pretty easy way for Hugging Face itself to be able
[1382.94s -> 1388.18s]  to calculate the loss that we get. So again, if we tokenize some input string,
[1388.18s -> 1393.82s]  we get our model inputs. We have two labels, positive and negative, and then
[1393.82s -> 1399.30s]  give some kind of corresponding label that we assign to the model inputs and
[1399.30s -> 1404.18s]  we pass this in. We can see here that the actual model outputs that are
[1404.18s -> 1410.02s]  given by a Hugging Face includes this loss here. Right, so it'll include the
[1410.02s -> 1414.34s]  loss corresponding to that input anyways. So it's a really easy way of
[1414.34s -> 1419.66s]  actually calculating the loss just natively in Hugging Face without having
[1419.66s -> 1424.94s]  to call any additional things from a PyTorch library. And lastly, we can
[1424.94s -> 1431.50s]  actually even use, if we have kind of like these two labels here, again for
[1431.50s -> 1436.58s]  positive or negative, what we can do is just take the model outputs, look at the
[1436.58s -> 1441.70s]  logits, and see which one is like the biggest again. We'll pass that and
[1441.70s -> 1446.14s]  take the argmax, so that'll give the index that's largest, and then that's
[1446.14s -> 1450.42s]  the output label that the model is actually predicting. So again, it gives a
[1450.42s -> 1453.66s]  really easy way of being able to do this sort of like classification,
[1453.70s -> 1458.22s]  getting the loss, getting what the actual labels are just from within Hugging Face.
[1458.22s -> 1469.50s]  Okay, awesome. So the last thing as well is that we can also kind of look inside
[1469.50s -> 1475.38s]  the model in a pretty cool way and also seeing what the attention weights
[1475.38s -> 1481.66s]  the model actually puts, the attention weights the model actually has. So this
[1481.70s -> 1485.66s]  is helpful if you're trying to understand like what's going on inside
[1485.66s -> 1492.98s]  of some NLP model. And so for here we can do again where we're importing our
[1492.98s -> 1499.42s]  model from some pre-trained kind of pre-trained model, model weights in the
[1499.42s -> 1504.50s]  Hugging Face hub. We want to set output attentions to true
[1504.50s -> 1508.22s]  and output hidden states to true. So these are going to be the key
[1508.22s -> 1512.58s]  arguments that we can use. We're actually kind of investigating what's
[1512.58s -> 1517.26s]  going on inside the model at each point in time. Again, we'll set the model to be
[1517.26s -> 1524.14s]  in eval mode. And lastly, we'll go ahead and tokenize our input string
[1524.14s -> 1531.10s]  again. We don't really care about any of the gradients here, again, so we don't
[1531.10s -> 1535.18s]  actually want to back propagate anything here. And finally, pass in the model
[1535.18s -> 1540.90s]  inputs. So now what we're able to do is when we print out the model hidden
[1540.90s -> 1545.42s]  states, so now this is a new kind of property in the output dictionary that
[1545.42s -> 1550.38s]  we get, we can look at what these actually look like here. And this is a
[1550.38s -> 1557.54s]  massive output. So you can actually look at the hidden state size per
[1557.54s -> 1561.22s]  layer, right, and so this kind of gives a notion of what we're going to be
[1561.26s -> 1566.22s]  looking at, like what the shape of this is at each given layer in our
[1566.22s -> 1571.66s]  model, as well as the attention head size per layer. So this gives you the
[1571.66s -> 1576.02s]  kind of shape of what you're looking at. And then if we actually look at the
[1576.02s -> 1581.06s]  model output itself, we'll get all of these different hidden states
[1581.06s -> 1587.30s]  basically. So we have tons and tons of these different hidden
[1587.30s -> 1592.98s]  states. We'll have the last hidden state here. So the model output is pretty
[1592.98s -> 1596.78s]  robust for kind of showing you what the hidden state looks like, as well as
[1596.78s -> 1601.82s]  what attention weights actually look like here. So in case you're trying to
[1601.82s -> 1606.22s]  analyze a particular model, this is a really helpful way of doing that. So
[1606.22s -> 1612.22s]  what model.eval does is it, sorry, question is what does the .eval do?
[1612.22s -> 1617.02s]  What it does is it basically sets your, and this is true for any PyTorch
[1617.02s -> 1622.78s]  module or model, is it sets it into quote-unquote eval mode. So again for this
[1622.78s -> 1627.30s]  like we're not really trying to calculate any of the gradients or
[1627.30s -> 1633.14s]  anything like that. That might correspond to like correspond to some
[1633.14s -> 1637.22s]  data that we pass in or try and update our model in any way. We just
[1637.22s -> 1642.66s]  care about evaluating it on that particular data point. So for that then
[1642.86s -> 1648.30s]  it's helpful to set the model into like eval mode essentially to help make sure
[1648.30s -> 1653.62s]  that that kind of like disables some of like that stuff that you'd use
[1653.62s -> 1657.74s]  during training time. So it just makes it a little more efficient. Yeah the
[1657.74s -> 1661.22s]  question was it's already pre-trained so can you go ahead and evaluate it?
[1661.22s -> 1665.66s]  Yeah you can. So yeah this is just the raw pre-trained model with no
[1665.66s -> 1671.38s]  fine-tuning. So the question is like how do you interpret these shapes
[1671.38s -> 1677.74s]  basically for the attention head size and then the hidden state size. So yeah
[1677.74s -> 1682.86s]  the key thing here is you'll probably want to look at kind of the shape given
[1682.86s -> 1686.66s]  on the side. It'll correspond to like the layer that you're actually kind of
[1686.66s -> 1692.46s]  like looking at. So here like when we call, we looked at the shape here,
[1692.46s -> 1697.18s]  we're specifically looking at like the first one in this list right. So
[1697.18s -> 1702.34s]  this will give us the first hidden layer, the second gives us a notion of
[1702.34s -> 1707.18s]  kind of like the batch that we're looking at, and then the last is like so
[1707.18s -> 1713.06s]  this is like some tensor right 768 dimensional, I don't know, representation
[1713.06s -> 1718.42s]  that corresponds there. And then for the attention head size it corresponds to
[1718.42s -> 1725.18s]  like the actual query word and the keyword for these last two here.
[1728.10s -> 1734.42s]  But yes so but for this you know we would expect this kind of initial index
[1734.42s -> 1738.82s]  here right the one to be bigger if we printed out all of the you know all of
[1738.82s -> 1743.26s]  the layers but we're just looking at the first one here. So we can also do
[1743.26s -> 1751.06s]  this for you know actually being able to get some notion of how these
[1751.06s -> 1755.98s]  different how this actually like looks and plot out these axes as well.
[1755.98s -> 1760.42s]  So again if we take this same kind of model input, which again is like this
[1760.42s -> 1764.78s]  hugging face transformers is great, we're actually trying to see like what do
[1764.78s -> 1770.02s]  these representations look like on like a per layer basis. So what we can do
[1770.02s -> 1774.70s]  here is basically we're looking at for each layer that we have in our
[1774.70s -> 1779.34s]  model right and again this is purely from the model output attentions or the
[1779.34s -> 1785.50s]  actual outputs the model. So what we can do is for each layer and then for
[1785.50s -> 1790.46s]  each head we can analyze essentially like what these representations look like
[1790.46s -> 1794.22s]  and in particular what the attention weights are across each of like the
[1794.22s -> 1798.34s]  tokens that we have. So this is like a good way of again understanding like
[1798.34s -> 1803.62s]  what your model is actually attending to within each layer. So on the side if
[1803.62s -> 1807.82s]  we look here maybe zoom in a bit we can see that this is going to be like
[1807.82s -> 1812.18s]  corresponds to the different layers and the top will correspond to these are
[1812.22s -> 1817.50s]  across the different attention heads. Okay this will just give you
[1817.50s -> 1823.58s]  some notion of like what the weights are. So again just to to clarify, so
[1823.58s -> 1826.98s]  again if we maybe look at the labels, sorry it's like a little cut off
[1826.98s -> 1832.42s]  and like zoomed out, but so this y-axis here like these different rows
[1832.42s -> 1839.58s]  corresponds to the different layers within the model. On the x-axis
[1839.82s -> 1845.50s]  here right we have like the like the different attention heads that are
[1845.50s -> 1851.10s]  present in the model as well. And so for each head we're able to for each at
[1851.10s -> 1856.50s]  each layer to basically get a sense of like what how the attention
[1856.50s -> 1860.38s]  distribution is actually being distributed, what's being attended to,
[1860.38s -> 1865.22s]  corresponding to each of like the tokens that you actually get here. So if
[1865.30s -> 1871.38s]  we look up again here as well right we're just trying to look at like
[1871.38s -> 1876.02s]  basically the model attentions that we get for each kind of corresponding
[1876.02s -> 1883.46s]  layer. The question is what's the the color key yellow is like higher
[1883.46s -> 1887.34s]  magnitude and higher value and then darker is like closer to zero. So
[1887.34s -> 1895.70s]  probably very nafy is like zero. So what we can do is now maybe walk through
[1895.70s -> 1901.14s]  like what a fine-tuning task looks like here. And so first like in a
[1901.14s -> 1905.14s]  project you know you're probably going to want to fine-tune a model, that's
[1905.14s -> 1909.26s]  fine. And we'll go ahead and walk through an example of what that looks
[1909.30s -> 1922.10s]  like here. Okay, so what we can do as well is, what we can do as well is use
[1922.10s -> 1927.86s]  some of the the data sets that we can get from Hugging Face as well, so it
[1927.86s -> 1931.86s]  doesn't just have models it has really nice data sets, and be able to
[1931.86s -> 1935.54s]  kind of like load that in as well. So here what we're going to be looking at
[1935.54s -> 1943.02s]  is looking at like the IMDB data set. And so here again is for sentiment
[1943.02s -> 1952.02s]  analysis, I will just look at only the first 50 tokens or so. And generally, so
[1952.02s -> 1955.78s]  this is this is like a you know helper function that we'll use for
[1955.78s -> 1961.34s]  truncating the output that we get. And then lastly for actually kind of
[1961.38s -> 1967.30s]  making this data set, we can use the data set dict class from Hugging Face
[1967.30s -> 1972.94s]  again, that will basically give us this smaller data set that we can get for the
[1972.94s -> 1977.90s]  for the train data set as well as specifying what we want for validation
[1977.90s -> 1982.22s]  as well. So here what we're going to do for our like mini data set for the
[1982.22s -> 1987.82s]  purpose of this demonstration is we'll use make train and val both from the
[1987.82s -> 1993.50s]  IMDB train data set. We'll shuffle it a bit, and then we're just going to select
[1993.50s -> 2000.18s]  here 128 examples, and then 32 from validation. So it'll shuffle it around,
[2000.18s -> 2006.90s]  it'll take the first 128, and it'll take the next 32. And then
[2006.90s -> 2010.74s]  we'll kind of truncate those particular inputs that we get. Again, just to kind
[2010.74s -> 2017.50s]  of make sure we're efficient, and we can actually run this on a CPU.
[2017.82s -> 2023.62s]  Okay, so next what we can do is just see kind of what does this look like.
[2023.62s -> 2027.46s]  It'll just again, this is kind of just like a dictionary, it's a wrapper class
[2027.46s -> 2031.70s]  almost, of giving you know your train data set and then your validation data
[2031.70s -> 2036.58s]  set. And in particular we can even look at like what the first ten of these
[2036.58s -> 2042.98s]  looks like. So first like the output, so we specify train, we want to look at
[2043.18s -> 2049.46s]  the first ten entries in our train data set, and the output of this is going to
[2049.46s -> 2055.22s]  be a dictionary as well, which is pretty cool. So we have some the first
[2055.22s -> 2062.22s]  ten text examples that give the actual movie reviews here. So this is
[2062.22s -> 2068.10s]  the given in a list, and then the second key that you get are the labels
[2068.10s -> 2072.62s]  corresponding to each of these. So whether it's positive or negative. So here one
[2072.66s -> 2077.46s]  is going to be a positive or a few, zero is negative. So it makes it really easy
[2077.46s -> 2085.50s]  to use this for something like sentiment analysis. Okay, so what we
[2085.50s -> 2091.26s]  can do is go ahead and prepare the data set and put it into batches of
[2091.26s -> 2095.98s]  16. Okay, so what does this look like? What we can do is we can call the map
[2095.98s -> 2102.98s]  function that this small data set dictionary has. So you can call map
[2102.98s -> 2108.74s]  and pass in a lambda function of what we want to actually do. So here the lambda
[2108.74s -> 2114.10s]  function is for each example that we have. We want to tokenize the text
[2114.10s -> 2119.98s]  basically. So this is basically saying how do we want to pre-process
[2119.98s -> 2125.02s]  this. And so here we're extracting the tokens, the input IDs that we'll pass as a
[2125.06s -> 2130.50s]  model. We're adding padding and truncation as well. We're gonna do this in a batch
[2130.50s -> 2139.42s]  and then the batch size will be 16. Okay, hopefully this makes sense. Okay, so next
[2139.42s -> 2146.22s]  we're basically just going to do like a little more modification on what the
[2146.22s -> 2149.78s]  data set actually looks like. So we're gonna remove the column that
[2149.82s -> 2156.38s]  corresponds to text and then we're gonna rename the column label to labels. So
[2156.38s -> 2160.14s]  again if we see this, this was called label, we're just gonna call it labels
[2160.14s -> 2163.78s]  and we're gonna remove the text column because we don't really need it
[2163.78s -> 2168.14s]  anymore. We just have gone ahead and pre-processed our data into the input
[2168.14s -> 2173.10s]  IDs that we need. Okay, and lastly we're gonna set it the format to torch so we
[2173.10s -> 2177.78s]  can go ahead and just pass this in, pass this into our model or a PyTorch
[2177.78s -> 2185.34s]  model. The question is what is labels? So, so label here corresponds to like again
[2185.34s -> 2189.30s]  the first in the context of sentiment analysis. It's like just but yeah
[2189.30s -> 2194.74s]  positive or negative. And so here we're just renaming the column. Okay, so now
[2194.74s -> 2197.86s]  we'll just go ahead and see what this looks like. Again we're gonna look
[2197.86s -> 2204.70s]  at the train set and only these first two things. And so, so here now we have
[2204.70s -> 2209.30s]  the two labels that correspond to each of the reviews and the input IDs that we
[2209.30s -> 2214.78s]  get corresponding for each of the reviews as well. Lastly we also get the
[2214.78s -> 2218.82s]  attention mask. So it's basically just taking the what you get out from the
[2218.82s -> 2222.22s]  tokenizer and it's just adding this back into the data set so it's really
[2222.22s -> 2228.30s]  easy to pass it. The question is we truncated which makes things easy but
[2228.30s -> 2236.54s]  how do you want to apply like padding evenly? So here if we do pass in, so
[2236.54s -> 2240.54s]  first is like you could either manually set some high truncation limit
[2240.54s -> 2247.20s]  like we did. The second is that you can just go ahead and set padding to
[2247.20s -> 2255.18s]  be true and then basically like the padding is basically added based off
[2255.18s -> 2260.34s]  of kind of like the longest like longest sequence that you have. Yeah so
[2260.34s -> 2266.34s]  question is I guess doing it for all of them all the text lists evenly. So
[2266.34s -> 2269.78s]  again it just like depends on like the size of like the data set you're
[2269.78s -> 2272.98s]  you're like you're loading it right. So if you're looking at particular
[2272.98s -> 2276.90s]  batches at a time you can just pad within that particular like batch
[2276.90s -> 2281.14s]  versus like yeah you don't need to like load all the data set into memory
[2281.18s -> 2285.98s]  pad the entire data set like port like in the same way. So it's fine to do it
[2285.98s -> 2292.22s]  within just batches. Yeah the question was how does how are the input IDs like
[2292.22s -> 2298.74s]  added and yeah the answer is yes it's basically done automatically. So we had
[2298.74s -> 2303.30s]  to manually remove the text column here and that kind of like this first
[2303.30s -> 2308.76s]  line here but like if you recall like the outputs of tokenize like at the
[2308.76s -> 2313.44s]  tokenizer it's basically just the input IDs and the and the attention mask. So
[2313.44s -> 2321.24s]  it just is smart enough to basically aggregate this together. Okay the last
[2321.24s -> 2325.64s]  thing we're gonna do is basically just put these so we have this like
[2325.64s -> 2331.76s]  data set now that looks great. We're just gonna import like a PyTorch data
[2331.76s -> 2336.64s]  loader typical normal data loader and then go ahead and load each of these
[2336.76s -> 2342.96s]  datasets that we just had. I mean specifying the batch size to be 16. Okay
[2342.96s -> 2350.72s]  so that's fine and great and so now for training the model it's basically
[2350.72s -> 2357.32s]  like exactly the same as what we would do in typical PyTorch. So again it's
[2357.32s -> 2361.04s]  like you still want to compute the loss you can back propagate the loss and
[2361.04s -> 2367.76s]  everything. Yeah so it's it's really up to your own design how you do how you
[2367.76s -> 2374.84s]  do the training. So here there's only like a few kind of asterisks I guess. One
[2374.84s -> 2379.88s]  is that you can import specific kind of optimizer types from the
[2379.88s -> 2385.82s]  transformers package. So you can do Adam with weight decay, you can get a
[2385.82s -> 2389.60s]  linear schedule for like the learning rate which will kind of decrease the
[2389.60s -> 2394.76s]  learning rate over time for each training step. So again it's
[2394.76s -> 2398.44s]  basically up to your choice. But if you look at the structure of like this code
[2398.44s -> 2402.84s]  right we load the model for classification, we set a number of epochs
[2402.84s -> 2407.56s]  and then however many training steps we actually want to do. We initialize our
[2407.56s -> 2412.88s]  optimizer and get some learning rate schedule and then from there it's
[2412.88s -> 2416.80s]  basically the same thing as what we would do for a typical kind of like
[2416.80s -> 2422.48s]  PyTorch model. We set the model to train mode, we go ahead and pass in
[2422.48s -> 2429.20s]  all these batches from like the data loader and then back propagate step the
[2429.20s -> 2435.20s]  optimizer and everything like that. So it's pretty similar from what
[2435.20s -> 2439.28s]  we're kind of like used to seeing essentially.
[2439.28s -> 2450.56s]  Awesome so that'll go do its thing at some point. Okay and so that's one
[2450.56s -> 2454.08s]  potential option is if you really like PyTorch you can just go ahead and do
[2454.08s -> 2460.92s]  that and it's really nice and easy. The second thing is that HuggingFace
[2460.92s -> 2465.44s]  actually has some sort of like a trainer class that you're able to use
[2465.48s -> 2470.60s]  that can handle most of these things. So again if we do that kind of
[2470.60s -> 2474.56s]  like the same thing here this will actually run once our model is done
[2474.56s -> 2480.68s]  training. Like we can create the our you know our data set in the same way
[2480.68s -> 2487.16s]  as before. Now what we need to use is like this import of like a
[2487.16s -> 2491.76s]  training arguments class. This is going to be basically a dictionary of all the
[2491.76s -> 2495.84s]  things that we want to use when we're going to actually train our model and
[2495.84s -> 2500.84s]  then this kind of like additional trainer class which will handle the
[2500.84s -> 2506.56s]  training kind of like magically for us and kind of wrap around in that way.
[2506.56s -> 2513.40s]  Okay so if you can okay I think we're missing a directory but I think yeah
[2513.40s -> 2518.56s]  pretty straightforward for how you want to train. Yeah so for here at
[2518.60s -> 2523.48s]  least again there are kind of the two key arguments. The first is training
[2523.48s -> 2527.68s]  arguments. So this will specify how a number of specifications that you can
[2527.68s -> 2532.36s]  actually pass through to it. It's where you want to log things for each
[2532.36s -> 2536.52s]  kind of like device. In this case like we're just using one GPU but
[2536.52s -> 2540.84s]  potentially if you're using multiple GPUs what the batch size is during
[2540.84s -> 2545.96s]  training, what the batch size is during evaluation time, how long you want to
[2545.96s -> 2551.72s]  train it for, how you want to evaluate it. So this is kind of like evaluating on
[2551.72s -> 2557.32s]  an epoch level, what the learning rate is and so on so on. So again if you
[2557.32s -> 2562.24s]  want to check the documentation you can see that here. There's a bunch of
[2562.24s -> 2566.64s]  different arguments that you can give. There's like warm-up steps, warm-up ratio,
[2566.64s -> 2572.22s]  like weight decay. There's like so many things. So again it's basically like a
[2572.30s -> 2575.90s]  dictionary. Feel free to kind of like look at these different arguments you
[2575.90s -> 2580.58s]  can pass in but there's a couple key ones here. And this is basically, this
[2580.58s -> 2584.46s]  basically mimics the same arguments that we used before in our like
[2584.46s -> 2593.14s]  explicit PyTorch method here for Hugging Face. Okay similarly what we do
[2593.14s -> 2596.50s]  is we can just pass this into the trainer and that will take care of
[2596.50s -> 2600.74s]  basically everything for us. So that whole training loop that we did before
[2600.78s -> 2606.06s]  is kind of condensed into this one class function for actually just doing
[2606.06s -> 2610.38s]  the training. So we pass in the model, the arguments, the trained dataset, eval
[2610.38s -> 2615.18s]  dataset, what tokenizer we want to use, and then some function for computing
[2615.18s -> 2622.38s]  metrics. So for here we pass in this function eval and it takes eval
[2622.38s -> 2626.74s]  predictions as input. Basically what this does is these predictions are given
[2626.78s -> 2630.94s]  from the trainer passed into this function and we just can split it into
[2630.94s -> 2635.46s]  the actual logits and the labels that are predicted. Or sorry, the ground truth
[2635.46s -> 2639.18s]  labels that we have. And then from here we can just calculate any sort of
[2639.18s -> 2644.54s]  additional metrics we want like accuracy, F1 score, recall, or whatever you
[2644.54s -> 2650.66s]  want. Okay so this is like an alternative way of formulating that
[2650.70s -> 2658.58s]  training loop. Okay the last thing here as well is that we can have some sort of
[2658.58s -> 2663.46s]  callback as well if you want to do things during the training process. So
[2663.46s -> 2668.02s]  after every epoch or something like that you want to evaluate your model on
[2668.02s -> 2672.34s]  the validation set or something like that. Or just go ahead and like dump
[2672.34s -> 2678.26s]  some sort of output. That's what you can use a callback for. And so here
[2678.74s -> 2684.42s]  this is just a logging callback. It's just gonna log kind of like that
[2684.42s -> 2690.74s]  information about the process itself. Again not super important but in case
[2690.74s -> 2696.50s]  that you're looking to try and do any sort of callback during training it's an
[2696.50s -> 2700.30s]  easy way to add it in. The second is if you want to do early stopping as
[2700.30s -> 2707.30s]  well. So early stopping will basically stop your model early as it sounds. If
[2707.34s -> 2712.06s]  it's not learning anything and a bunch of epochs are going by and so you can
[2712.06s -> 2714.90s]  set that so that you don't waste kind of like compute time or you can see
[2714.90s -> 2718.70s]  the results more easily. The question is is there a good choice for the
[2718.70s -> 2724.14s]  patient's value? It just depends on the model architecture. Not really I
[2724.14s -> 2733.42s]  guess. It's yeah pretty up to your discretion. Okay awesome. And so the
[2733.42s -> 2738.98s]  last thing that we do is just do call trainer.train. So if you recall this
[2738.98s -> 2743.46s]  is just the instantiation of this trainer class called trainer.train and
[2743.46s -> 2749.70s]  it'll just kind of go. So now it's training which is great. It gives us a
[2749.70s -> 2754.62s]  nice kind of estimate of how long things are taking, what's going on, what
[2754.62s -> 2761.30s]  arguments that we actually passed in. So that's just gonna run and then
[2761.30s -> 2766.78s]  likewise hopefully it'll train relatively quickly. Okay it'll take two
[2766.78s -> 2772.54s]  minutes. We can also evaluate the model pretty easily as well. So we just call
[2772.54s -> 2777.30s]  trainer.predict on whatever data set that we're interested in. So here
[2777.30s -> 2782.34s]  it's the tokenized data set corresponding to the validation data set.
[2782.34s -> 2789.34s]  Okay hopefully we can pop that out soon. And lastly so if we saved
[2789.38s -> 2797.82s]  anything to our model checkpoints. So hopefully this is saving stuff right now.
[2799.18s -> 2803.30s]  Yeah so this is going to be is continuing to save stuff to the folder
[2803.30s -> 2809.10s]  that we specified. And so here in case we ever want to kind of like load our
[2809.10s -> 2813.78s]  model again from the weights that we've actually saved, we just pass in the
[2813.78s -> 2818.22s]  name of the checkpoint like the relative path here to our checkpoint.
[2818.22s -> 2823.38s]  So notice how we have some checkpoint 8 here. Alright we just pass in the
[2823.38s -> 2827.90s]  path to that folder, we load it back in, we tokenize, and it's the same thing
[2827.90s -> 2834.46s]  as we did before. There are a few kind of additional
[2834.46s -> 2838.90s]  appendices for how to do like different tasks as well. So there's
[2838.90s -> 2845.10s]  appendix on generation, how to define a custom data set as well, how it's a
[2845.10s -> 2853.74s]  kind of like pipeline, different kind of like tasks together. So this is
[2853.74s -> 2858.62s]  kind of like using some a pre-trained model that you can just use through
[2858.62s -> 2864.54s]  kind of like the pipeline interface really easily. There's like in different
[2864.54s -> 2868.66s]  types of tasks like mass language modeling. But feel free to look at
[2868.66s -> 2873.14s]  through those at your own time and yeah thanks a bunch.
[2875.10s -> 2877.16s]  you
