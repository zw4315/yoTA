# Detected language: en (p=1.00)

[0.00s -> 11.68s]  Okay, hi everyone, I'll get started, the last class.
[11.68s -> 19.08s]  Okay, yeah, well, welcome, congratulations and thank you to making it to the last real
[19.08s -> 22.68s]  lecture of CS224N.
[22.68s -> 28.52s]  So this is the plan for today, the lecture's titled NLP Linguistics and Philosophy,
[28.52s -> 34.00s]  which I took as meaning that I could talk about anything I wanted to, so that is what
[34.00s -> 35.60s]  I'm going to do.
[35.60s -> 39.40s]  So this is sort of this, what we're going to go through, talk a bit about the major
[39.40s -> 47.50s]  ideas of CS224N and open problems, some of the more foundational questions of where
[47.50s -> 54.48s]  are we with LLM, symbolic versus neural systems, meaning and linguistics in NLP, and
[54.48s -> 59.32s]  then I'll close with some slides on the future risks of AI in the world.
[59.32s -> 65.68s]  Okay, so here is an attempt to sort of lay out the most major things that we looked
[65.68s -> 69.52s]  at in CS224N.
[69.52s -> 74.88s]  We started with word vectors and we developed the idea of neural NLP systems.
[74.88s -> 81.08s]  We expanded from a simple feed-forward network into doing sequence models, language models,
[81.08s -> 87.40s]  RNNs, LSTMs, and then we introduced this powerful new model that's been very influential,
[87.40s -> 88.92s]  the transformer.
[88.92s -> 94.16s]  And then we built from there to the kind of, it's not exactly an architecture but
[94.16s -> 99.84s]  model that's been built up in recent years to produce high-performance NLP systems
[99.84s -> 106.20s]  where we're first doing pre-training and then a post-training phase of various techniques
[106.20s -> 111.56s]  that we talked about to produce these general foundation models that understand language
[111.56s -> 112.64s]  so well.
[112.64s -> 116.68s]  And then we went on from there and talked about various particular topics like benchmarking
[116.68s -> 118.36s]  and reasoning.
[118.36s -> 124.92s]  So a few of the major ideas that we looked at were this idea that you could get a long
[124.92s -> 132.36s]  way by having dense representations, those are our hidden representations in neural networks,
[132.36s -> 138.08s]  and then looking at distributional semantics, representing words by their context, first
[138.08s -> 141.32s]  slogan of usual, know a word by the company it keeps.
[141.32s -> 147.32s]  And I'll come back to that a bit later in talking about ideas of meaning, but that's
[147.32s -> 153.68s]  essentially been the idea that has driven most of the successful ideas of modern NLP,
[153.68s -> 159.16s]  whether it's the earliest statistical NLP phase or more modern neural NLP phase.
[159.16s -> 165.24s]  And in this world we start instantiating that as these models of word vectors, but
[165.24s -> 171.20s]  the same contextual idea is then used in all the models up through transformers.
[171.20s -> 178.64s]  We looked at both the challenges and opportunities of training large deep neural networks and
[178.64s -> 183.84s]  how gradually people developed ideas and tricks such as having residual connections
[183.84s -> 190.20s]  which made it much more possible and stable to do successfully, which took us from a place
[190.20s -> 195.38s]  where a lot of this seemed black magic that was hard to get right to people being
[195.38s -> 200.64s]  able to very reliably train high performance transformer models.
[200.64s -> 206.24s]  We talked about sequence models, what's good about them and some of their problems and
[206.24s -> 210.96s]  how those problems have been addressed in large measure by adopting this different
[210.96s -> 215.52s]  architecture of transformers which give a form of parallelization.
[215.52s -> 221.84s]  And then we moved into the modern form of pre-training by language modeling where
[221.84s -> 228.72s]  language modeling seems a simple thing predicting words in context, but it emerges as what
[228.72s -> 235.92s]  we think of as a universal pre-training task that all kinds of both linguistic and world
[235.92s -> 240.78s]  knowledge help you to do this task of predicting words better.
[240.78s -> 248.52s]  And so this has ended up as just a general method to produce the kind of powerful knowledgeable
[248.52s -> 250.76s]  models that we have today.
[250.76s -> 256.12s]  And up until now there's been this amazing property that we see this empirical fact
[256.12s -> 263.66s]  that we seem to just get this basically, well not basically, it's extremely linear
[263.66s -> 270.70s]  improvements as performance as we continue to scale data and compute and model size
[270.70s -> 275.34s]  up by orders of magnitude.
[275.34s -> 278.58s]  That doesn't mean that all problems in NLP are solved.
[278.58s -> 283.98s]  There are lots of things that people still work on and see opportunities to try and
[283.98s -> 289.84s]  make things better and a few of these are mentioned on the next few slides.
[289.84s -> 299.76s]  So there's a real question of how much these models are good at actually learning
[299.76s -> 307.68s]  to be able to do things generally rather than just being very good at memorization.
[307.68s -> 312.12s]  That a lot of the benefits of what we're getting from these large pre-trained language
[312.12s -> 319.36s]  models is that they've seen a huge amount of stuff and therefore they know everything.
[319.36s -> 322.92s]  They've seen every pattern before and they know how to use things.
[322.92s -> 329.42s]  I've occasionally used the analogy that large language models are sort of like a talking
[329.42s -> 336.38s]  encyclopedia that they're really in many ways more like a huge knowledge store than
[336.38s -> 342.16s]  necessarily something that is intelligent in the sense of being able to work out how
[342.16s -> 346.46s]  to solve new problems and generalize as human beings do.
[346.46s -> 353.08s]  A kind of interesting fact actually is that in some ways transformer models are actually
[353.08s -> 358.94s]  worse at generalizing than the older LSTMs that preceded them.
[358.94s -> 363.90s]  So here's just one little graph I'm not going to spend a lot of time on, but this
[363.90s -> 372.54s]  was looking at data that's being generated by a finite automata and then trying to learn
[372.54s -> 380.30s]  it from a limited amount of data with either an LSTM or a transformer.
[380.30s -> 385.74s]  And the observation is that at the scales that they're working, even having seen quite
[385.74s -> 391.82s]  limited exemplification, the LSTM is basically sealing the entire of this graph, right?
[391.82s -> 399.98s]  It's just at the one line because it generalizes in good ways because of its LSTM architecture,
[399.98s -> 405.42s]  whereas the transformer needs to see a ton more data before it actually learns the patterns
[405.42s -> 406.42s]  well.
[406.42s -> 414.26s]  And so if we think of one of the prime attributes of humans' intelligence is actually
[414.26s -> 422.24s]  we're amazing at figuring out and learning things from very limited exposure, right?
[422.24s -> 427.30s]  You know, there's something that you don't know how to do and a friend shows you once
[427.30s -> 429.38s]  what you do to make it work.
[429.38s -> 434.86s]  And by and large, you know, you'll improve a few times with practice, but you can learn
[434.86s -> 441.22s]  effectively new skills from these kind of single shot examples.
[441.22s -> 446.62s]  And that's not always what we seem to be seeing in our models.
[446.62s -> 451.38s]  There's a lot of interest in what's going on inside neural networks, that a lot of
[451.38s -> 457.14s]  the time neural networks still appear as black boxes where we have no real idea of
[457.14s -> 459.34s]  how they're doing what they're doing.
[459.34s -> 464.86s]  And as perhaps for your final projects, the main thing you're doing is measuring the
[464.86s -> 470.54s]  final performance number and seeing if it goes up or not.
[470.54s -> 474.68s]  So there's a lot of interest in better understanding what do they learn?
[474.68s -> 475.86s]  How do they learn it?
[475.86s -> 477.88s]  Why do they succeed and fail?
[477.88s -> 482.96s]  And a lot of that work is starting to look more closely into what's happening inside
[482.96s -> 485.82s]  neural network computations.
[485.82s -> 490.52s]  There is some work of that sort that actually goes back quite a fair way.
[490.52s -> 499.00s]  So here's an old blog post by Andre Karpathy while he was a grad student here in 2016.
[499.00s -> 503.78s]  And he was looking at LSTMs and how do they learn.
[503.78s -> 510.38s]  And he found that one of the neurons in an LSTM cell was effectively sort of measuring
[510.38s -> 512.74s]  position along a line of text.
[512.74s -> 518.96s]  And as the line of text got long, its sort of value started to change because the
[518.96s -> 523.88s]  model was learning that there was sort of a line length of this text and that the line
[523.88s -> 526.74s]  was likely to be ending at that point.
[526.74s -> 530.44s]  And in recent times there's started to be with transformers as well, a lot of work
[530.44s -> 535.42s]  looking at mechanistic interpretability or causal abstraction, trying to understand the
[535.42s -> 538.58s]  internals of models.
[538.58s -> 545.86s]  A problem that's far from solved and in many respects probably unsolvable is the multilingual
[545.86s -> 551.94s]  question of dealing with all the other languages of the world.
[551.94s -> 558.14s]  But you do have to keep in your head that whatever you see for English, it's worse for
[558.14s -> 563.54s]  every other language and what they're getting out of modern language models.
[563.54s -> 565.82s]  Now you know, there is a good news story here.
[565.82s -> 568.98s]  I don't want us to claim that everything is terrible.
[568.98s -> 577.30s]  So in this graph, which is kind of small, the blue line was the performance of GPT-3.5
[577.30s -> 586.30s]  for English, and then all of the green bars are then the performance of GPT-4.
[586.30s -> 592.06s]  And so there's a genuine good news story here, which is, look, not just for English,
[592.06s -> 598.90s]  but for a lot of other languages, for Greek, Latvian, Arabic, Turkish, all of them
[598.90s -> 605.18s]  in GPT-4 are better than English was in GPT-3.5.
[605.18s -> 614.42s]  So that's the good news argument that building these models big is in some sense raising
[614.42s -> 615.78s]  all boats.
[615.78s -> 619.78s]  But these are still all huge languages.
[619.78s -> 627.66s]  And things are starting to drop off at the bottom of this table for languages where
[627.66s -> 631.90s]  the performance is worse than English in GPT-3.5.
[631.94s -> 638.90s]  So even for those languages, they're languages for which much less written data is available,
[638.90s -> 640.62s]  but they're still large languages.
[640.62s -> 644.50s]  So the ones that the three at the bottom are actually all Indian languages.
[644.50s -> 650.18s]  They're Punjabi, Marathi, and Telugu, which are languages that are each spoken
[650.18s -> 651.30s]  by millions of people.
[651.30s -> 653.90s]  They're not small languages.
[653.90s -> 659.18s]  So the real question is, what happens when you actually get to the small,
[659.18s -> 660.62s]  low-resource languages?
[660.62s -> 666.54s]  So the vast majority of languages around the world don't have millions of speakers.
[666.54s -> 672.22s]  They vary from having hundreds of speakers to hundreds of thousands of speakers.
[672.22s -> 674.34s]  And there are thousands of such languages.
[674.34s -> 679.46s]  A lot of those languages are primarily oral and have very limited amounts
[679.46s -> 681.66s]  of written text.
[681.66s -> 686.22s]  Now, some of those languages are likely, or many of those languages are likely to go
[686.22s -> 689.38s]  extinct in the coming decades.
[689.38s -> 694.38s]  But many of those language communities would like to preserve their languages.
[694.38s -> 699.50s]  And it's very unclear how the kind of language technologies that we've been
[699.50s -> 705.14s]  talking about in the later parts of the course can be extended to those languages
[705.14s -> 709.34s]  because there just isn't sufficient data to build the kind of models that we've
[709.34s -> 712.06s]  been looking at.
[712.06s -> 719.14s]  So I imagine you've gotten some idea in this course of how evaluation is a huge part
[719.18s -> 724.22s]  of what we do, that effectively a lot of the way that progress is being driven is
[724.22s -> 729.26s]  by defining evaluations of what models should be able to achieve and then people
[729.26s -> 736.46s]  working to measure systems and improve systems so that they do better on what we
[736.46s -> 741.82s]  see as good language understanding or other properties.
[741.82s -> 747.82s]  One of the concerns that many people have about what's happened with the large
[747.86s -> 754.86s]  recent closed models from large companies is a concern that all of the benchmarks are
[754.86s -> 757.82s]  being sullied and not to be trusted.
[757.82s -> 765.46s]  So here's one example that comes from a tweet of Horace Hurst.
[765.46s -> 769.98s]  And he's noting, I suspect GPT-4's performance is influenced by data
[769.98s -> 774.78s]  contamination, at least on code forces, one of the coding benchmarks.
[774.78s -> 781.82s]  Of the easiest problems on code forces, it solved 10 out of 10 pre-2021 problems,
[781.82s -> 784.46s]  but zero out of 10 recent problems.
[784.46s -> 787.46s]  This strongly points to contamination.
[787.46s -> 792.30s]  And the worry is that every time you're seeing these fantastic results of how
[792.30s -> 797.90s]  well the latest best language model is performing, that at this point,
[797.90s -> 804.38s]  so much data is on the web that gets included in the pre-training data for
[804.38s -> 809.82s]  these large language models, that essentially they're memorizing at least a good share of
[809.82s -> 812.78s]  the questions that are appearing in these challenges.
[812.78s -> 818.26s]  So they're not actually solving them in a fair way as an independent test set at all.
[818.26s -> 820.06s]  They're just memorizing them.
[820.06s -> 825.46s]  And so there are sort of issues then as to, you know, what kind of thoroughly hidden test sets
[825.46s -> 832.38s]  we can have or dynamic evaluation mechanisms so we can actually have benchmark integrity.
[832.38s -> 836.14s]  Another huge area that a number of us are involved in at Stanford
[836.14s -> 841.18s]  elsewhere is making NLP work in different technical domains.
[841.18s -> 847.10s]  So domains including biomedical or clinical medical NLP have a lot of differences of
[847.10s -> 849.82s]  vocabulary and usage.
[849.82s -> 855.58s]  They have a lot of potential good uses, but they also have a lot of potential risks of
[855.58s -> 859.54s]  doing harm if the language understanding is incomplete.
[859.54s -> 866.82s]  I myself have been more involved doing things in the legal NLP, working with other people
[866.82s -> 872.78s]  at the RegLab with Dan Ho in building foundation models for law.
[872.78s -> 879.70s]  And there are all kinds of ways, again, in which this kind of technology could be
[879.70s -> 880.98s]  really useful, right?
[880.98s -> 887.42s]  The biggest problem in most countries, it's bad in the United States, but it's way worse
[887.42s -> 893.42s]  in a place like India is that most people can't get access to the kind of legal help
[893.42s -> 899.98s]  that they need to solve their problems because of the cost of it and the lack of trained lawyers.
[899.98s -> 908.14s]  So if more could be done to be able to help people via NLP tools, you know, in principle,
[908.14s -> 909.38s]  that would be great.
[909.38s -> 914.66s]  But in practice, the tools still don't have good enough language understanding.
[914.70s -> 922.34s]  So in the RegLab, there's a just completed study out at the moment looking at legal NLP systems.
[922.34s -> 928.22s]  And we were finding that the hallucination rate, the rate in which there was made up stuff
[928.22s -> 934.38s]  in their legal answers was effectively for one question in six, which is in a very good
[934.38s -> 941.94s]  accuracy rate if you're someone who's wanting to rely on these systems for legal advice.
[942.02s -> 946.14s]  There are lots of things also to work out dealing with the social and cultural aspects
[946.14s -> 947.14s]  of NLP.
[947.14s -> 957.66s]  NLP systems remain very biased against various cultures and religions.
[957.66s -> 962.38s]  They have certain social norms, you could say, that they pick up from somewhere.
[962.38s -> 965.02s]  But those social norms are very biased.
[965.02s -> 971.56s]  It's against certain groups and related to there being small languages that I mentioned
[971.56s -> 976.44s]  before that there are lots of issues with underrepresented groups in having the kind
[976.44s -> 979.32s]  of NLP that they'd like to have.
[979.32s -> 984.04s]  Okay, so that's sort of the summary of that bit.
[984.04s -> 988.96s]  So for the next bit, I thought I'd just sort of give one more bit of perspective
[988.96s -> 996.92s]  on where are we with the best language models like GPT-4.
[996.92s -> 1003.08s]  I mean, I think it's really interesting at this moment of where we are because, you
[1003.08s -> 1010.60s]  know, on the one hand, the performance of these models is just amazing.
[1010.60s -> 1016.88s]  And you know, even as someone who works in NLP and has worked in it for many, many
[1016.88s -> 1025.08s]  years now, I mean, you know, I can tell a sort of story that these models, you know,
[1025.08s -> 1029.96s]  that we do this training to predict the next word and it's conditioning on a lot of text
[1029.96s -> 1032.84s]  and it knows about things and it does.
[1032.84s -> 1037.24s]  But, you know, in some sense, these things still seem like magic, right?
[1037.24s -> 1041.12s]  It's just kind of hard to believe how this could possibly work.
[1041.12s -> 1050.30s]  So in this example, I asked ChatGPT 4.0, I did this this morning, I asked it to write
[1050.30s -> 1056.78s]  a sonnet explaining the transformer neural net architecture in which every line begins
[1056.78s -> 1063.82s]  with the letter T. And it sort of still frankly blows my mind and I don't actually
[1063.82s -> 1071.94s]  feel I can really explain even to myself in a way that's convincing how this large
[1071.94s -> 1079.74s]  transformer is able to take all its pre-training text and reading that instruction and as
[1079.74s -> 1088.40s]  a next token prediction machine, it successfully produces something that is a sonnet and every
[1088.40s -> 1093.74s]  line begins with the letter T. I hope you remember from your high school English classes,
[1093.74s -> 1100.34s]  sonnets are meant to have A, B rhyming, it's a little bit imperfect here, net set,
[1100.34s -> 1105.10s]  some people will complain about that, but it's basically a rhyme, behold, unfold,
[1105.10s -> 1106.10s]  grasp, clasp, right?
[1106.46s -> 1112.46s]  So it's basically doing the rhyming right and the lines are meant to be in iambic pentameter
[1112.46s -> 1118.62s]  and it's basically produced lines in iambic pentameter to delve into the world of neural nets,
[1118.62s -> 1122.98s]  you know, iambic pentameter, yeah, it's incredible.
[1122.98s -> 1127.70s]  So here's its sonnet it came up with, to delve into the world of neural nets,
[1127.70s -> 1133.62s]  transformers rise and marvel to behold, through layers deep the network's path is set,
[1133.66s -> 1139.58s]  to learn from data patterns to unfold, the tokens feed into attention's grasp,
[1139.58s -> 1145.50s]  to weigh their import context to align, the queries keys and values in a clasp,
[1145.50s -> 1152.06s]  to process sequences with power fine, through heads of many multifaceted site,
[1152.06s -> 1157.98s]  to parallelize capturing the span, transforming each a matrix math delight,
[1158.58s -> 1164.42s]  to synthesize with elegance and plan, the models might in every task it shows,
[1164.42s -> 1167.54s]  to turn more data into wisdom's prose.
[1167.54s -> 1174.86s]  Now, you could object that I'm not sure this exactly explained
[1174.86s -> 1180.46s]  the transformer neural net architecture, it's a little bit abstract, I'll give it that,
[1180.46s -> 1190.70s]  but you know, in another sense, it did in one place or another evoke quite a bit of stuff
[1190.70s -> 1197.10s]  about transformers with queries, keys and values and multi-headed stuff,
[1197.10s -> 1204.66s]  parallelized with matrix math, whatever else, yeah, still kind of blows my mind
[1204.98s -> 1212.06s]  how well that works and, you know, indeed, as natural language understanding
[1212.06s -> 1220.06s]  and sort of world understanding devices, I mean, these devices have clearly crossed
[1220.06s -> 1226.18s]  the threshold in which they're very usable in many contexts.
[1226.18s -> 1232.42s]  So, here's, there's now started to be some fairly good studies that have been done on,
[1233.42s -> 1240.18s]  you know, how much value people can get out of using LLMs like GPT-4.
[1240.18s -> 1247.22s]  So, this study by Delacroix and a whole lot of colleagues, including Ethan Moloch,
[1247.22s -> 1252.10s]  they took a bunch of consultants from the Boston Consulting Group,
[1252.10s -> 1256.30s]  and so, you know, what that's like, that means, you know, 23-year-olds
[1256.30s -> 1260.22s]  graduating from universities like this one, but more on the East Coast,
[1260.22s -> 1267.82s]  they've come, you know, Boston consultants, you know, not exactly dummies.
[1267.82s -> 1273.62s]  And so, they found in this study, so, you know, controlled task,
[1273.62s -> 1278.46s]  there are actually sort of three groups, but the big contrast is that,
[1278.46s -> 1283.02s]  you know, two of the groups were using GPT-4 to do consulting tasks
[1283.02s -> 1288.46s]  and one of the groups wasn't using GPT-4 to do tasks.
[1288.50s -> 1293.10s]  The difference between the two that were was one of them was given more training
[1293.10s -> 1297.46s]  on how to use GPT-4, but that didn't seem to make much of a difference.
[1297.46s -> 1304.50s]  But their result was that the groups using GPT-4 in their study
[1304.50s -> 1311.62s]  completed 12% more tasks on average, they did the task 25% more quickly,
[1311.62s -> 1318.82s]  and the results were judged 40% higher quality than those not using AI,
[1318.82s -> 1326.58s]  which I think is a pretty stunning success of how GPT-4 or similar LLMs
[1326.58s -> 1330.94s]  are good enough to actually help people get real work done,
[1330.94s -> 1334.30s]  you know, with whatever asterisks you want to put about the quality
[1334.30s -> 1339.14s]  of management consultant work in various instances.
[1339.14s -> 1343.06s]  Yeah, I mean, and the interesting result is that, you know,
[1343.06s -> 1346.94s]  using these LLMs seemed to be a big leveler,
[1346.94s -> 1354.14s]  and actually you see exactly the same thing for people using coding LLMs,
[1354.14s -> 1358.70s]  that they're a huge assistance for people whose own skills are weaker
[1358.70s -> 1364.22s]  and they're much less of an assistance for people whose own skills are strong.
[1364.22s -> 1368.62s]  Okay, so that's a good news story, but, you know, if on the other hand,
[1368.62s -> 1372.46s]  you're more like the good news story for human beings,
[1372.46s -> 1376.18s]  here's a study that goes in the other direction.
[1376.18s -> 1384.22s]  Can GPT-4 write fiction that matches the quality of New Yorker fiction writers?
[1384.22s -> 1388.98s]  And the result of that study was not even close,
[1388.98s -> 1395.38s]  that GPT-4 was measured as three to 10 times worse at creative writing
[1395.38s -> 1397.22s]  than a New Yorker fiction writer.
[1397.26s -> 1400.54s]  So, there's still hope for human beings. Hang on there.
[1400.54s -> 1406.30s]  And so, you know, I think that's kind of the, you know,
[1406.30s -> 1409.38s]  the dual screen picture that we have at the moment.
[1409.38s -> 1412.34s]  In some ways, these things are great and useful.
[1412.34s -> 1415.70s]  In other ways, they're not so great.
[1415.70s -> 1422.94s]  And I think that's something that we're still sort of going to be seeing playing out
[1422.94s -> 1424.90s]  in the future years.
[1424.90s -> 1430.62s]  I think living in Silicon Valley, we see a lot of the positive hype.
[1430.62s -> 1435.66s]  So, if you just want to see a little bit of the negative on the other side,
[1435.66s -> 1440.58s]  late last year, there was a piece in the Financial Times,
[1440.58s -> 1444.98s]  which was titled, Generative AI, Hypely Intelligent.
[1444.98s -> 1451.14s]  And I won't read all of this, but basically, they were wanting to express
[1451.18s -> 1456.18s]  considerable skepticism of the current AI boom.
[1456.18s -> 1458.10s]  Investors should keep their heads.
[1458.10s -> 1463.94s]  Expectations for generative AI are running way ahead of the limitations that apply to it.
[1463.94s -> 1468.86s]  As investment in generative AI grows, so does pressure to create new use cases.
[1468.86s -> 1475.30s]  By 2027, IDC thinks enterprise spending on generative AI will reach $143 billion,
[1475.30s -> 1478.94s]  up from $16 billion this year, so 10 times up.
[1478.94s -> 1483.06s]  Open AI hopes for more funding to pursue human-like AI.
[1483.06s -> 1487.18s]  It is worth remembering that when examining Altman's plan for superintelligence.
[1487.18s -> 1489.66s]  Models predict they do not comprehend.
[1489.66s -> 1495.62s]  That limitation casts doubt on AI achieving even human-like intelligence.
[1495.62s -> 1502.38s]  And then they sort of start talking about some of the problems with limited gains
[1502.38s -> 1508.66s]  for lower-skilled workers, inaccuracies in the work they produce.
[1508.66s -> 1514.18s]  And suggest that the limitations will become more obvious as generative AI tools roll out.
[1514.18s -> 1517.30s]  That will put pressure on providers to address costs.
[1517.30s -> 1523.54s]  AI could add $4 trillion to profits, says McKinsey, but price and clarity is lacking.
[1523.54s -> 1528.90s]  Without it, companies cannot predict what financial gains AI can accomplish.
[1528.90s -> 1533.50s]  And AI cannot predict that either.
[1533.50s -> 1536.10s]  OK, that's that topic.
[1536.10s -> 1538.50s]  I'm chugging through my topics.
[1538.50s -> 1546.06s]  The next topic is I wanted to return and say a bit more about symbolic methods that
[1546.06s -> 1555.98s]  dominated AI from the 60s until about 2010, versus what I termed here as cybernetics.
[1555.98s -> 1563.42s]  Because the original alternative, going back to the 50s and 60s, was called cybernetics.
[1563.42s -> 1572.06s]  And in a very real sense, neural networks is a continuation of the cybernetics tradition
[1572.06s -> 1579.22s]  rather than the AI tradition that started in the 50s and 60s.
[1579.22s -> 1584.98s]  In this context, Stanford is the home of the symbolic systems program.
[1584.98s -> 1590.86s]  So at the moment, we are unique in having a symbolic systems program.
[1590.86s -> 1599.58s]  So the name symbolic systems came about because at the time it was started, so I guess philosophy
[1599.58s -> 1603.00s]  was an active part of the symbolic systems program.
[1603.00s -> 1608.42s]  And John Barwise, shown in this picture, he actually died young, so he actually died
[1608.42s -> 1610.14s]  in 2000.
[1610.34s -> 1622.30s]  John Barwise had a very strong belief that you are meant to be dealing with meaning
[1622.30s -> 1629.24s]  in the world and the connection between people's thinking and the world.
[1629.24s -> 1635.68s]  And so he refused to allow the program to be called cognitive science, as it's called
[1635.68s -> 1637.82s]  at most other places.
[1637.94s -> 1641.90s]  And it ended up being called symbolic systems.
[1641.90s -> 1647.10s]  Now at one point, there were two universities that had symbolic systems because John Barwise
[1647.10s -> 1652.70s]  actually moved away from Stanford and went to Indiana, which is where he originally
[1652.70s -> 1654.22s]  was from.
[1654.22s -> 1658.22s]  And so Indiana also had a symbolic systems program for a number of years.
[1658.22s -> 1663.78s]  But they've actually changed theirs to cognitive science now since he died.
[1663.78s -> 1667.26s]  We are unique in having symbolic systems.
[1667.26s -> 1673.76s]  And so the idea of symbolic systems, this is sort of what's on the website with a bit
[1673.76s -> 1675.18s]  of interpretation, right?
[1675.18s -> 1680.70s]  So symbolic systems study systems and meaningful symbols that represent the world about us,
[1680.70s -> 1685.34s]  like human languages, logics, and programming languages, and the systems that work with
[1685.34s -> 1691.74s]  these symbols, like brains, computers, and complex social systems, contrasting that
[1691.74s -> 1696.78s]  to the sort of typical view of cognitive science, which is focusing on the mind and
[1696.78s -> 1700.66s]  intelligence as a naturally occurring phenomenon.
[1700.66s -> 1706.12s]  Symbolic systems gives equal focus to human constructed systems that use symbols to communicate
[1706.12s -> 1709.38s]  and to represent information.
[1709.38s -> 1721.68s]  So in AI terms, you know, AI as a field and the name AI arose around arguing for
[1721.68s -> 1723.56s]  a symbolic approach, right?
[1723.56s -> 1731.14s]  That John McCarthy, who's the color photo there and who founded Stanford's artificial
[1731.14s -> 1737.98s]  intelligence and the original famous Stanford AI lab.
[1737.98s -> 1742.10s]  So John McCarthy came up with the name artificial intelligence.
[1742.10s -> 1751.44s]  And he very, very explicitly chose a new name to disassociate what he was doing from
[1751.44s -> 1758.52s]  a cybernetics approach, which had been pursued by people, including Norbert Wiener at MIT,
[1758.52s -> 1761.28s]  who's shown on the right side.
[1761.28s -> 1769.28s]  So Marvin Minsky, the teeny photo down here, sort of founded artificial intelligence at
[1769.28s -> 1771.44s]  MIT.
[1771.44s -> 1776.32s]  McCarthy worked with him for a few years, and then McCarthy came to Stanford.
[1776.44s -> 1782.88s]  Two of the other most prominent early AI people are Newell and Simon, who were at CMU, and
[1782.88s -> 1785.46s]  then the other two people on the right side.
[1785.46s -> 1794.96s]  And so in particular, Newell and Simon develop, well, actually no, let me say a
[1794.96s -> 1795.96s]  sentence first.
[1795.96s -> 1800.72s]  Yeah, so I mean McCarthy's own background was a mathematician and a logician, right?
[1800.72s -> 1808.72s]  So that he wanted to construct an artificial intelligence that looked like math and logic
[1808.72s -> 1809.72s]  effectively, right?
[1809.72s -> 1814.28s]  And that sort of most AI as a symbolic system.
[1814.28s -> 1819.04s]  And that was developed as a position in the philosophy of artificial intelligence by
[1819.04s -> 1820.76s]  Newell and Simon.
[1820.76s -> 1826.32s]  And so they developed what they called the physical symbol system hypothesis.
[1826.32s -> 1833.24s]  So that said, a physical symbol system has the necessary and sufficient means for general
[1833.24s -> 1835.08s]  intelligent action.
[1835.08s -> 1838.40s]  And so that's a super strong claim.
[1838.40s -> 1845.90s]  It's not only claiming that having a symbol system allows you to produce artificial general
[1845.90s -> 1852.80s]  intelligence, but through the necessary clause that you can't have artificial general
[1852.80s -> 1856.78s]  intelligence without having a symbol system.
[1856.78s -> 1863.52s]  So that was sort of the basis of classical AI, right?
[1863.52s -> 1871.56s]  So and that kind of contrasts a bit with the, you know, so cybernetics, you know,
[1871.56s -> 1876.44s]  had its origins in sort of control and communication.
[1876.44s -> 1882.56s]  So it's much nearer to sort of an electrical engineering kind of background and was wanting
[1882.56s -> 1891.76s]  to sort of unify ideas of control and communication between animals, maybe perhaps more than humans
[1891.76s -> 1893.08s]  and machines.
[1893.08s -> 1901.94s]  Yeah, so I mean, you know, sort of in, yes, cybernetics comes from a Greek word
[1901.94s -> 1906.56s]  kubernetes, which is sort of interesting all the uses it has.
[1906.56s -> 1912.32s]  So it's exactly the same route that occurs both in kubernetes, if you are familiar with
[1912.32s -> 1919.08s]  that as, you know, distributed containers on modern systems, but also it's actually
[1919.08s -> 1926.24s]  the same route that the word government comes from, because it's a control system as well.
[1926.24s -> 1929.48s]  Yeah.
[1929.48s -> 1936.08s]  So under the cybernetics tradition was when neural nets first started being explored.
[1936.08s -> 1940.68s]  The very earliest neural nets, some of the most famous ones are Frank Rosenblatt,
[1940.68s -> 1942.24s]  which we use for vision.
[1942.24s -> 1945.84s]  The neural net was actually wired.
[1945.84s -> 1952.36s]  To say just a teeny bit about this, in case you think that AI hype is only a thing of
[1952.36s -> 1961.38s]  the 2020s, there was just as much AI hype in the 1950s when Rosenblatt unveiled his
[1961.38s -> 1963.38s]  Perceptron.
[1963.38s -> 1970.82s]  Also in the New York Times article about it, New Navy Device Learns By Doing, Psychologist
[1970.82s -> 1975.26s]  Shows Embryo of Computer Design to Read and Grow Wiser.
[1975.26s -> 1981.46s]  The Navy revealed the embryo of an electronic computer today that it expects will be able
[1981.46s -> 1990.26s]  to walk, talk, see, write, reproduce itself, and be conscious of its existence.
[1990.26s -> 1996.24s]  And this hype is all the more incredible when you get to the later paragraph of the
[1996.24s -> 2001.08s]  article and you find out what the demonstration was actually of.
[2001.08s -> 2009.34s]  And the demonstration that people were shown was that this device learned to differentiate
[2009.34s -> 2016.04s]  between right arrow and left arrow pictures after 50 exposures.
[2016.04s -> 2021.04s]  So there you go.
[2021.04s -> 2022.04s]  Okay.
[2022.04s -> 2023.04s]  Yeah.
[2023.04s -> 2032.02s]  So, what do we make of this in the case of NLP and language?
[2032.02s -> 2041.04s]  And you know, the position I would like to suggest is, you know, there's just no
[2041.04s -> 2047.28s]  doubt that language is a symbolic system, right?
[2047.28s -> 2052.88s]  But humans developed language as a symbolic system.
[2052.88s -> 2060.06s]  It's perhaps most obvious that if you think about it in writing, we have symbols of
[2060.06s -> 2062.52s]  the letters and words that we use.
[2062.52s -> 2067.28s]  But even if there's no writing and, you know, the majority of human language use
[2067.28s -> 2074.24s]  over time has been verbal human language use, that even though the substrate is carried
[2074.24s -> 2079.72s]  on where the sound waves or in sign languages, movements of hands, even though that's
[2079.72s -> 2085.64s]  a continuous substrate, the structure of human languages is a symbol system.
[2085.64s -> 2089.56s]  We have symbols, which are the sounds of human languages.
[2089.56s -> 2094.64s]  The cat, we have a k, an a, and a t, those are symbols, and they're recognized
[2094.68s -> 2101.88s]  in a symbolic way by language users, and indeed all the pioneering work and categorical
[2101.88s -> 2109.40s]  perception in cognitive psychology is done with the sounds of human languages, the phonemes
[2109.40s -> 2111.40s]  as linguists call them.
[2111.40s -> 2116.68s]  So spoken language also has a symbolic structure.
[2116.72s -> 2124.40s]  But, you know, going against Newell and Simon, the fact that humans use a symbol system
[2124.40s -> 2130.60s]  for communication doesn't mean that the processor of the symbols, the human brain,
[2130.60s -> 2133.28s]  has to be a physical symbol system.
[2133.28s -> 2139.88s]  And so similarly, we don't have to design NLP, our computer processes, those physical
[2139.88s -> 2142.80s]  symbol systems either.
[2142.84s -> 2148.52s]  The brain is, you know, clearly much more like a neural network model, and probably
[2148.52s -> 2154.12s]  neural models will scale better and capture language processing better than something
[2154.12s -> 2157.92s]  that is a symbolic processor in the same way.
[2157.92s -> 2163.48s]  I mean, that sort of leaves behind the question of, well, why did humans come up
[2163.48s -> 2166.56s]  with a symbol system for communication?
[2166.56s -> 2172.24s]  I mean, after all, you know, we could have just sort of hummed at different frequencies
[2172.28s -> 2176.12s]  and that could have been used as our system of communication.
[2176.12s -> 2181.08s]  I mean, I think the dominant idea, which seems reasonable to me, but who knows, is
[2181.08s -> 2186.36s]  that having a symbolic system gives signaling reliability, right?
[2186.36s -> 2192.12s]  That if you have discrete target points that are separated, then that gives you an
[2192.12s -> 2196.20s]  ability when there's degradation of the signal to recover it well.
[2197.04s -> 2203.60s]  Yeah, so where does that leave linguistics, which has mainly been developed in terms
[2203.60s -> 2208.32s]  of describing a symbolic system?
[2208.32s -> 2213.72s]  I think the right way to think about it is linguistics is good for giving us questions,
[2213.72s -> 2219.24s]  concepts, and distinctions when thinking about language acquisition, processing,
[2219.24s -> 2220.80s]  and understanding.
[2220.80s -> 2225.68s]  And indeed, one of the interesting things that's come about is that, you know,
[2225.68s -> 2233.92s]  is that sort of as NLP and AI have been developed further and as able to do a lot of
[2233.92s -> 2239.36s]  low-level stuff, that there's actually the sort of higher-level concepts that linguists
[2239.36s -> 2244.84s]  often talk about a lot, things like compositionality and systematic generalization,
[2244.84s -> 2251.04s]  which I'll come back to in a few minutes, the mapping of stable meanings for symbols,
[2251.04s -> 2258.04s]  the reference of linguistic expressions in the world, that they get talked about more
[2258.04s -> 2263.20s]  and more in artificial intelligence context, building neural systems.
[2263.20s -> 2268.44s]  And I mean, I think one way to think about it is that, you know, a lot of the early
[2268.44s -> 2280.44s]  neural network of most notably visual processing, but also other kinds of sensory stuff like sounds.
[2280.44s -> 2285.60s]  I mean, doing that is sort of what gets you to insect-level intelligence.
[2285.60s -> 2289.60s]  And if you want to get higher up the chain than insect-level intelligence,
[2289.60s -> 2294.12s]  then a lot of the kind of questions and properties of linguistic systems
[2294.12s -> 2297.64s]  become increasingly relevant.
[2297.64s -> 2307.16s]  At a slightly more prosaic level, that I don't think one necessarily wants to believe
[2307.20s -> 2311.16s]  all the fine details of different linguistic theories.
[2311.16s -> 2315.92s]  But, you know, for how human languages are structured and how they behave,
[2315.92s -> 2321.36s]  I think, yeah, most of our broad understanding of linguistics is right.
[2321.36s -> 2326.56s]  And so therefore, when we're thinking about NLP systems and we're thinking about, you know,
[2326.56s -> 2331.20s]  understanding how they behave, wanting to know whether they have certain properties,
[2331.20s -> 2337.04s]  thinking up ways to evaluate them, a lot of that is done in terms of linguistic
[2337.04s -> 2341.84s]  understanding, wanting to see whether they capture facts about sentence structure,
[2341.84s -> 2347.72s]  discourse structure, semantic properties like natural language inference,
[2347.72s -> 2351.92s]  whether you can do things like bridging an afro, which I did not cover this year's class,
[2351.92s -> 2356.00s]  because we skipped the coreference lecture when we sliced one lecture off the class.
[2356.00s -> 2360.52s]  Metaphors, presuppositions, all of these things are linguistic notions
[2360.52s -> 2365.04s]  that we try and get our NLP models to capture.
[2365.04s -> 2369.96s]  So I just want to say a couple more remarks about, you know,
[2369.96s -> 2374.24s]  the role of human language in human intelligence.
[2374.24s -> 2377.52s]  I think this is kind of interesting.
[2377.52s -> 2382.44s]  So an interesting person in the history of linguistics is this guy,
[2382.44s -> 2389.96s]  Wilhelm von Humboldt, who was a prominent German academic.
[2389.96s -> 2398.44s]  So really, the American education system was borrowed from Germany, right?
[2398.44s -> 2406.52s]  So up until the Second World War, the preeminent place of science and learning was Germany.
[2406.52s -> 2414.08s]  And Germany, essentially via von Humboldt's work, developed the idea of having graduate education.
[2414.08s -> 2421.04s]  And the US copied graduate education from Germany and started doing its own.
[2421.04s -> 2428.00s]  But, you know, in that context, it was still the case that for people in the United States
[2428.00s -> 2437.20s]  prior to the 1930s, that generally people would go to Germany to finish their education,
[2437.20s -> 2442.28s]  either to get their PhD or to do a postdoc or something like that, right?
[2442.28s -> 2449.48s]  So, you know, if you trace back my own academic tree or most other academic trees of people
[2449.48s -> 2456.96s]  who got PhDs in the US, they actually go back a few generations and then they go back to Germany.
[2456.96s -> 2462.48s]  So we don't think of that as much in the modern world.
[2462.48s -> 2467.68s]  Yeah, so Humboldt was influential in developing the university system,
[2467.68s -> 2472.32s]  but he also worked a lot on language.
[2472.32s -> 2480.00s]  And he's someone that Chomsky always cites because he's known for this famous statement
[2480.00s -> 2484.52s]  about that human language must make infinite use of finite means.
[2484.52s -> 2489.24s]  So the fact that we have a limited supply of words and sentence structures,
[2489.24s -> 2494.68s]  but out of those, we can recursively build up an infinite number of sentences.
[2494.68s -> 2501.36s]  And that's, in Chomsky's view, supporting the kind of symbolic, structured view of language
[2501.36s -> 2503.16s]  that he's been advocating.
[2503.16s -> 2508.28s]  But I think that's sort of another interesting take of von Humboldt's,
[2508.28s -> 2515.92s]  which we can argue whether it's right or not, but I think is kind of interesting.
[2515.92s -> 2523.72s]  And one of the things he wants to stress is that language isn't just something
[2523.76s -> 2533.52s]  used for the purpose of communication, that he should actually introduce something here.
[2533.52s -> 2539.64s]  So Kahneman and Tversky are two well-known cognitive psychologists,
[2539.64s -> 2542.92s]  and they introduced this idea that there are two kinds of thinking,
[2542.92s -> 2546.84s]  system one cognition and system two cognition.
[2546.84s -> 2552.44s]  And system one is the kind of subconscious thinking that you're not really thinking
[2552.44s -> 2559.40s]  of just we process stuff when it comes into our heads, whether visual signals or speech.
[2559.40s -> 2564.32s]  And system two thinking is the conscious, let me think about this
[2564.32s -> 2566.60s]  and try and figure out what's going on.
[2566.60s -> 2569.60s]  I'm solving a math problem style of thinking.
[2569.60s -> 2576.12s]  And, you know, I think you can see in von Humboldt's writings
[2576.12s -> 2581.92s]  essentially the same kind of distinction between system one and system two cognition,
[2581.92s -> 2586.88s]  although he refers to system one cognition as acts of the spirit
[2586.88s -> 2591.04s]  and system two cognition as thinking.
[2591.04s -> 2598.88s]  Yeah, so basically he argues for a version of the philosophical position
[2598.88s -> 2606.00s]  of the language of thought, of suggesting that effective system two thinking
[2606.00s -> 2611.40s]  requires extension of the mind through the symbols of language.
[2611.40s -> 2618.08s]  And so he argued that having language is absolutely a necessary foundation
[2618.08s -> 2620.60s]  for the progress of the human mind.
[2620.60s -> 2625.16s]  And I think that's actually an interesting perspective, which I have some sympathy with.
[2625.16s -> 2628.12s]  I mean, you know, obviously we can think without language.
[2628.12s -> 2630.24s]  You know, we can feel afraid.
[2630.24s -> 2635.12s]  We can think visually and about how things that fit together.
[2635.12s -> 2642.28s]  But I think it's fairly plausible that for the sort of more abstract,
[2642.28s -> 2650.08s]  larger scale thinking that humans engage in and has led them to sort of higher levels
[2650.08s -> 2656.72s]  of thought than a chimpanzee gets to, that language gives a scaffolding inside the mind
[2656.72s -> 2658.88s]  that makes that possible.
[2658.88s -> 2663.52s]  Another version of that is from the philosopher Daniel Dennett,
[2663.56s -> 2666.60s]  who just actually died a couple of months ago.
[2666.60s -> 2671.72s]  So Dennett wrote this book called From Bacteria to Bark and Back.
[2671.72s -> 2676.00s]  And the main thing this book was about was the origin of human consciousness.
[2676.00s -> 2680.32s]  And I'm not going to talk about human consciousness today.
[2680.32s -> 2687.08s]  But he introduced this model of four grades of progressively more
[2687.08s -> 2690.08s]  competent intelligences.
[2690.08s -> 2697.60s]  And so the four levels he outlined was that the bottom one was Darwinian.
[2697.60s -> 2703.20s]  So a Darwinian intelligence was something that was pre-designed and fixed.
[2703.20s -> 2706.16s]  It doesn't improve during its lifetime.
[2706.16s -> 2712.44s]  Improvement only happens by evolution through genetic selection.
[2712.44s -> 2718.72s]  So things like bacteria and viruses are Darwinian intelligences.
[2718.72s -> 2723.08s]  So then after that was Skinnerian intelligences.
[2723.08s -> 2729.40s]  And so they improved behavior by learning to respond to reinforcement.
[2729.40s -> 2734.24s]  So something like a lizard or perhaps a dog,
[2734.24s -> 2741.64s]  we could argue about how intelligent dogs are, has Skinnerian intelligence.
[2741.64s -> 2746.76s]  And so then the third level up, Popperian intelligence,
[2746.76s -> 2750.04s]  is things that learn models of the environment.
[2750.04s -> 2755.08s]  So they can improve performance by thinking through plans,
[2755.08s -> 2759.72s]  and then executing them and seeing how they behave.
[2759.72s -> 2765.96s]  So in a computational sense, Popperian intelligence kind of
[2765.96s -> 2770.64s]  means that you can do model-based reinforcement learning.
[2770.64s -> 2776.04s]  And so primates like chimpanzees can definitely
[2776.04s -> 2781.20s]  do the kind of planning and model-based reinforcement learning
[2781.20s -> 2783.16s]  that gives you a Popperian intelligence.
[2783.16s -> 2785.88s]  But actually, a lot of recent evidence shows
[2785.88s -> 2789.16s]  that a lot of simpler creatures can also do it.
[2789.16s -> 2793.64s]  So I'm not sure the facts here.
[2793.64s -> 2802.64s]  So all these studies you see are about crows from the South Pacific,
[2802.80s -> 2806.04s]  Australia, and Fiji, and places like that.
[2806.04s -> 2808.96s]  So I'm not sure if Northern Hemisphere crows are dumber.
[2808.96s -> 2814.76s]  But at least Southern Hemisphere crows can learn plans
[2814.76s -> 2819.52s]  so that they can do multi-stage planning to work out ways
[2819.52s -> 2823.32s]  to get a piece of meat that's down the hole by learning to pick up a stick
[2823.32s -> 2824.56s]  and poke it in.
[2824.56s -> 2830.32s]  And so that even crows can be Popperian intelligences.
[2830.32s -> 2835.96s]  But what Dennis suggests is that there's a stage beyond Popperian
[2835.96s -> 2839.96s]  intelligence, which he calls Gregorian intelligence.
[2839.96s -> 2843.00s]  And the idea of Gregorian intelligence
[2843.00s -> 2847.04s]  is that you can build thinking tools which
[2847.04s -> 2853.64s]  allow you to do a higher level of control of mental searches.
[2853.64s -> 2859.72s]  And so he suggests that things like, well, mathematics
[2859.72s -> 2861.44s]  is a thinking tool.
[2861.44s -> 2864.56s]  But well, also democracy is a thinking tool.
[2864.56s -> 2868.80s]  But nevertheless, out of the space of thinking tools,
[2868.80s -> 2872.44s]  the human language is the preeminent thinking tool
[2872.44s -> 2873.76s]  that we have.
[2873.76s -> 2877.76s]  And so he suggests that the only biological example
[2877.76s -> 2883.04s]  we have of a Gregorian intelligence is human beings.
[2883.04s -> 2885.72s]  And so I think in that kind of sense,
[2885.80s -> 2890.64s]  you can say that's a very important role for language.
[2890.64s -> 2895.28s]  OK, two parts to go in my summary.
[2895.28s -> 2898.56s]  OK, so the next one is, what kind of semantics
[2898.56s -> 2901.20s]  should we use for language?
[2901.20s -> 2903.16s]  And so this is getting back to the question
[2903.16s -> 2905.68s]  I mentioned for word vectors.
[2905.68s -> 2907.68s]  And this is kind of interesting.
[2907.68s -> 2911.92s]  So the semantics that's been dominant in philosophy
[2911.92s -> 2914.84s]  of language or in linguistic semantics
[2914.84s -> 2917.92s]  is a notion of model theoretic semantics,
[2917.92s -> 2922.32s]  where the meaning of words is their denotation, what
[2922.32s -> 2923.72s]  they represent in the world.
[2923.72s -> 2926.20s]  I mentioned this, I think, in an early lecture, right?
[2926.20s -> 2929.60s]  So that if you have a word like computer,
[2929.60s -> 2932.40s]  the meaning of computer is the set of computers.
[2932.40s -> 2934.44s]  This one, that one, that one, all the other computers
[2934.44s -> 2935.24s]  are out, right?
[2935.24s -> 2937.88s]  So it's a denotational relationship
[2937.88s -> 2941.32s]  between a word and its denotation in the world
[2941.32s -> 2943.16s]  or in a model of the world.
[2943.16s -> 2945.68s]  And that was the notion that was
[2945.68s -> 2950.36s]  used in most of the history of AI for doing symbolic AI.
[2950.36s -> 2952.56s]  And that then contrasts with this sort
[2952.56s -> 2956.56s]  of distributional semantics, that the meaning of a word
[2956.56s -> 2959.68s]  is understanding the context in which it's used,
[2959.68s -> 2964.88s]  which is effectively what we're using for our neural models.
[2964.88s -> 2966.08s]  Yeah.
[2966.08s -> 2969.52s]  So if you look at the traditional view
[2969.52s -> 2974.28s]  of interpreting the meaning of human language,
[2974.28s -> 2976.16s]  and this is what you'll have seen
[2976.16s -> 2980.20s]  if you did an intro logic class at some point, right?
[2980.20s -> 2984.16s]  That we have a sentence, the red apple is on the table,
[2984.16s -> 2987.92s]  and you get to write in some logical representation,
[2987.92s -> 2990.64s]  first order predicate calculus or whatever.
[2990.64s -> 2992.96s]  This one's a bit different to allow in thus,
[2992.96s -> 2995.80s]  where normally for first order predicate calculus,
[2995.80s -> 2999.00s]  you only do for all and if there exists.
[2999.00s -> 3002.16s]  But you have a sort of a formal logic.
[3002.16s -> 3005.56s]  And in the early week, in weeks one and two
[3005.56s -> 3008.20s]  of the logic class, you have some English sentences
[3008.20s -> 3010.60s]  for which you translate into formal logic.
[3010.60s -> 3014.56s]  And then after that, you forget about human languages
[3014.56s -> 3016.48s]  and you just sort of start proving stuff
[3016.48s -> 3019.68s]  about formal logical systems.
[3019.68s -> 3024.16s]  And so to some extent, what you get in a philosophy class
[3024.16s -> 3028.28s]  represents the tradition of Alfred Tarski.
[3028.28s -> 3032.92s]  So Tarski believed that you couldn't talk about meaning
[3032.92s -> 3036.08s]  in terms of talking about human languages
[3036.08s -> 3038.88s]  because human languages were quote,
[3038.88s -> 3042.04s]  impossibly incoherent.
[3042.04s -> 3049.00s]  Yeah, and so from about the 1940s until 1980,
[3049.00s -> 3053.16s]  Tarski was the preeminent logician in the US.
[3053.16s -> 3055.08s]  He was in Berkeley.
[3055.08s -> 3058.96s]  And so that was very much the view
[3058.96s -> 3061.16s]  of the logicians of the world.
[3061.16s -> 3064.92s]  But during that period, one of his students
[3064.92s -> 3068.12s]  was this guy, Richard Montague.
[3068.12s -> 3074.12s]  So Richard Montague sort of rebelled against that picture
[3074.12s -> 3077.84s]  saying, I reject the contention that an important theoretical
[3077.84s -> 3081.40s]  difference exists between formal and natural languages.
[3081.40s -> 3087.56s]  And so he then set about showing that, well,
[3087.56s -> 3092.48s]  you could start building up a formal semantics
[3092.48s -> 3096.08s]  for describing the meaning of natural language sentences.
[3096.08s -> 3100.04s]  And so Richard Montague's work became the foundation
[3100.04s -> 3104.80s]  of the work that's used in semantics in linguistics
[3104.80s -> 3105.36s]  as well.
[3105.36s -> 3109.36s]  For anyone who's done Ling 130 or 230,
[3109.36s -> 3115.00s]  the picture you saw is sort of a Montague picture of semantics.
[3115.00s -> 3118.08s]  And so that was the semantics that
[3118.08s -> 3123.16s]  was taken over and essentially used
[3123.16s -> 3125.32s]  as the model of doing natural language
[3125.32s -> 3130.84s]  understanding for most of the history of NLP,
[3130.84s -> 3137.20s]  roughly 1960 to 2015, 2017.
[3137.44s -> 3140.68s]  So the picture essentially was that if we
[3140.68s -> 3144.28s]  wanted to have a sentence that we interpreted
[3144.28s -> 3148.24s]  like the red apple is on the table, what we would do
[3148.24s -> 3151.76s]  is we'd first produce a syntactic structure
[3151.76s -> 3152.72s]  for the sentence.
[3152.72s -> 3155.40s]  So we would parse it.
[3155.40s -> 3159.76s]  And then using ideas roughly along the lines
[3159.76s -> 3164.60s]  that Montague suggested, we would construct its meaning
[3164.60s -> 3169.04s]  by looking up meanings of words in a lexicon
[3169.04s -> 3171.84s]  and then using the compositionality
[3171.84s -> 3175.92s]  of human languages to work out the meanings of progressively
[3175.92s -> 3178.84s]  larger phrases and clauses in terms
[3178.84s -> 3180.96s]  of the meanings of those words
[3180.96s -> 3182.92s]  and the way that they are combined,
[3182.92s -> 3186.96s]  slightly reminiscent of my discussion of tree structures
[3186.96s -> 3190.04s]  to meanings in the last lecture I gave.
[3190.04s -> 3194.56s]  And so you would build up a meaning representation
[3194.56s -> 3196.80s]  of a sentence.
[3196.80s -> 3199.60s]  And so this could then give you a semantic meaning
[3199.60s -> 3203.56s]  of a sentence that you could use in a system.
[3203.56s -> 3207.12s]  This is approximately a slide, except retitled,
[3207.12s -> 3214.44s]  that I actually used to use in CS224n in the 2000s decade.
[3214.44s -> 3220.00s]  So we have a part of a sentence.
[3220.00s -> 3221.32s]  I get on that whole sentence.
[3221.32s -> 3222.00s]  Here it is.
[3222.00s -> 3227.04s]  How many red cars, what, can I get this sentence?
[3227.04s -> 3228.80s]  I think there's a sentence here.
[3228.80s -> 3234.00s]  How many red cars in Palo Alto does Kathy like?
[3234.00s -> 3238.08s]  How many red cars in Palo Alto does Kathy like?
[3238.08s -> 3241.80s]  And sorry, yeah, the cars got hidden underneath here.
[3241.80s -> 3243.28s]  Yeah, so we have a sentence.
[3243.28s -> 3244.60s]  We parse it.
[3244.60s -> 3247.00s]  We look up meanings of words in a lexicon.
[3247.00s -> 3248.96s]  We start composing them up.
[3248.96s -> 3252.44s]  We get a semantic form for the whole sentence, which
[3252.44s -> 3255.00s]  we can then convert into SQL, and we
[3255.00s -> 3258.44s]  can run against a database, and we can get the answer.
[3258.44s -> 3262.80s]  And this was outlined the kind of technology
[3262.80s -> 3265.48s]  that was widely used for natural language understanding
[3265.48s -> 3269.84s]  systems that were built anywhere from the 1960s
[3269.84s -> 3272.32s]  to the 2010s.
[3272.32s -> 3277.56s]  And in particular, they were used not only
[3277.56s -> 3281.20s]  in a purely kind of rule-based grammar and lexicon way.
[3281.20s -> 3284.24s]  This same basic technology was incorporated
[3284.24s -> 3286.40s]  into a machine learning context, where
[3286.40s -> 3290.04s]  your goal was to start to learn various of these parts.
[3290.04s -> 3292.36s]  You could not only learn the parser,
[3292.36s -> 3296.84s]  but you could also learn semantic meanings of words
[3296.84s -> 3298.84s]  and learn composition rules.
[3298.84s -> 3301.20s]  And so the acme of that work was then
[3301.20s -> 3303.20s]  what was called semantic parsing that
[3303.20s -> 3306.08s]  was pioneered by Luke Zettlmoyer and Mike
[3306.08s -> 3309.44s]  Collins in the 2000s decade, and then taken up
[3309.44s -> 3313.76s]  by others, including Percy Liang, so Percy Liang's PhD
[3313.76s -> 3317.84s]  thesis, but also actually his early work at Stanford
[3317.84s -> 3321.28s]  before he was convinced to do neural networks,
[3321.28s -> 3325.96s]  was doing semantic parsing work.
[3325.96s -> 3328.68s]  So these systems could actually work
[3328.68s -> 3331.20s]  and were used in limited domains,
[3331.20s -> 3334.48s]  but they're always extremely brittle.
[3334.48s -> 3338.00s]  And yeah, the interesting thing is sort of what of humans.
[3338.00s -> 3342.36s]  I mean, there is some evidence that humans do something
[3342.36s -> 3346.76s]  like this, that they work out the structure of sentences
[3346.76s -> 3353.20s]  and compute meanings in a bottom-up, mostly
[3353.20s -> 3355.00s]  projective way.
[3355.00s -> 3357.60s]  There's a lot of controversy as to exactly how
[3357.60s -> 3361.20s]  human understanding of sentences still works,
[3361.20s -> 3362.84s]  but there are certainly people who've
[3362.84s -> 3367.68s]  argued in support of human brains doing something similar.
[3367.68s -> 3369.80s]  That's obviously not what we're getting
[3369.80s -> 3372.28s]  with current day transformers.
[3372.28s -> 3379.60s]  And so the question is, do our current day neural language
[3379.60s -> 3383.00s]  models provide suitable meaning functions?
[3383.00s -> 3388.00s]  And that's a complex question, because in many ways,
[3388.00s -> 3389.52s]  yeah, they seem to.
[3389.52s -> 3392.60s]  They do an amazing job at understanding whatever sentences
[3392.60s -> 3394.60s]  you put into them, but there are still
[3394.60s -> 3399.20s]  some genuine concerns as to whether they are making
[3399.20s -> 3402.64s]  shortcuts or work to a certain extent
[3402.64s -> 3404.28s]  and don't actually have the same kind
[3404.28s -> 3409.00s]  of compositional understanding with systematic generalization
[3409.00s -> 3412.40s]  that human beings do.
[3412.40s -> 3416.40s]  So that's the traditional denotational semantics view,
[3416.40s -> 3421.72s]  and that contrasts with the kind of use theory of meaning.
[3421.76s -> 3424.80s]  And in the first or second lecture
[3424.80s -> 3426.40s]  and at the beginning of this one,
[3426.40s -> 3430.64s]  I attributed that to the British linguist J.R. Firth.
[3430.64s -> 3433.64s]  You shall know a word by the company it keeps.
[3433.64s -> 3436.12s]  But it's not only a position of Firth.
[3436.12s -> 3439.80s]  It's also been a minority position of philosophers.
[3439.80s -> 3443.44s]  In particular, it was advanced by Wittgenstein
[3443.44s -> 3446.96s]  in his later work, in his work Philosophical Investigation.
[3446.96s -> 3450.96s]  So in that work, he writes, when I talk about language,
[3450.96s -> 3453.24s]  words, sentences, et cetera, I must
[3453.24s -> 3455.44s]  speak the language of every day.
[3455.44s -> 3458.16s]  Is this language somehow too coarse and material
[3458.16s -> 3459.84s]  for what we want to say?
[3459.84s -> 3462.36s]  Then how is another one to be constructed?
[3462.36s -> 3464.80s]  And how strange that we should be able to do anything
[3464.80s -> 3467.40s]  at all with the one we have.
[3467.40s -> 3469.44s]  Philosophical Investigations is written
[3469.44s -> 3472.32s]  in this sort of vaguely poetical literary style,
[3472.32s -> 3476.32s]  but the point of it is meant to be saying, look,
[3476.32s -> 3478.00s]  these logician people are claiming
[3478.00s -> 3482.32s]  you can't use natural human languages to express meaning,
[3482.32s -> 3486.12s]  and you have to translate into this symbol system.
[3486.12s -> 3487.84s]  But isn't that a weird concept
[3487.84s -> 3490.28s]  that one symbol system is no good,
[3490.28s -> 3495.04s]  but this other symbol system somehow fixes things?
[3495.04s -> 3498.88s]  And then about denotational semantics, he writes,
[3498.88s -> 3501.96s]  you say the point isn't the word, but it's meaning.
[3501.96s -> 3504.64s]  And you think of the meaning as a thing of the same kind
[3504.64s -> 3507.56s]  as the word, though also different from the word.
[3507.56s -> 3509.68s]  Here the word, there the meaning.
[3509.68s -> 3512.84s]  So that's the symbol and its denotation.
[3512.84s -> 3516.28s]  The money and the cow that you can buy with it.
[3516.28s -> 3518.80s]  But contrast money and its use.
[3518.80s -> 3522.16s]  And he goes on from there to argue for the kind of,
[3522.16s -> 3524.48s]  you know, the meaning of money is the way
[3524.48s -> 3527.44s]  that money can be used in the world.
[3527.44s -> 3530.68s]  The meaning of money isn't pointing at pieces of money.
[3532.56s -> 3535.84s]  Okay, so this is what's referred to
[3535.92s -> 3537.88s]  as a use theory of meaning.
[3537.88s -> 3542.44s]  And so the question is, is that a good theory of meaning?
[3542.44s -> 3546.08s]  So some people just don't accept
[3548.28s -> 3553.20s]  this kind of distributional semantics use theories of meaning
[3553.20s -> 3556.36s]  as a theory of meaning or semantics.
[3556.36s -> 3558.80s]  Most prominently in recent NLP work,
[3558.80s -> 3561.76s]  that's the position of Bender and Kohler.
[3561.76s -> 3564.52s]  They just take as axiomatic the only thing
[3564.52s -> 3567.32s]  that counts as having a meaning
[3567.32s -> 3571.32s]  is that you've got form over here and meaning over there.
[3573.32s -> 3576.28s]  But I think that that's too narrow.
[3576.28s -> 3581.28s]  I think we have to argue that meaning arises from connect,
[3582.04s -> 3584.96s]  meaning of words arises from connecting words
[3584.96s -> 3586.48s]  to other things.
[3586.48s -> 3591.48s]  And although in some sense you could say connecting words
[3591.68s -> 3594.12s]  to things in the real world is privileged,
[3594.60s -> 3598.00s]  it's not the only way that you can ground meanings.
[3598.00s -> 3600.84s]  You can have meanings in a virtual world,
[3600.84s -> 3604.20s]  but you can also have meanings by connecting one word
[3604.20s -> 3606.96s]  to other things in human language.
[3606.96s -> 3610.80s]  And the other thing that I think you need to say is,
[3610.80s -> 3614.36s]  you know, meaning isn't a sort of a zero one thing
[3614.36s -> 3618.60s]  that you know the denotation of a word or you don't.
[3618.60s -> 3620.40s]  I think meaning is a gradient thing
[3620.40s -> 3624.20s]  and you can understand meanings of words and phrases
[3624.20s -> 3626.12s]  either more or less.
[3626.12s -> 3628.84s]  And so this is an example I gave in a piece
[3628.84s -> 3631.52s]  that I wrote a couple of years ago.
[3631.52s -> 3635.36s]  Okay, what is the meaning of the word shinai?
[3636.76s -> 3639.92s]  Well, maybe a few of you know it,
[3639.92s -> 3643.88s]  but if you don't, well, what could I do?
[3643.88s -> 3646.56s]  Well, you know, if you'd seen or held one,
[3646.56s -> 3649.52s]  you'd have classic grounded meaning.
[3649.52s -> 3652.48s]  Know something about the denotation.
[3652.48s -> 3654.44s]  Well, if that's not the case,
[3654.44s -> 3655.96s]  well, you know, I could at least show you
[3655.96s -> 3656.80s]  a picture of one.
[3656.80s -> 3658.40s]  Here's a picture of one.
[3658.40s -> 3662.52s]  So that gives you some information about what a shinai is.
[3662.52s -> 3666.36s]  But, you know, is that the only thing I can do?
[3666.36s -> 3671.36s]  I mean, suppose, well, sorry, I left out a bullet point.
[3671.36s -> 3674.52s]  So this gives you a partial meaning of a shinai,
[3674.52s -> 3677.32s]  but surely you have a richer meaning
[3677.36s -> 3679.44s]  if you'd heard one being played.
[3680.64s -> 3683.44s]  And, well, is showing you a picture of one
[3683.44s -> 3685.84s]  the only thing I can do?
[3685.84s -> 3689.72s]  Suppose you'd never, you know, seen, felt, or heard one,
[3689.72s -> 3692.00s]  but, you know, I told you
[3692.00s -> 3695.84s]  it's a traditional Indian instrument, a bit like an oboe.
[3695.84s -> 3698.24s]  Well, I think you understand something
[3698.24s -> 3700.56s]  about the meaning of the word at that point.
[3701.92s -> 3704.84s]  That, you know, it's sort of connected to India.
[3704.84s -> 3708.68s]  It's a wind instrument using reeds
[3708.68s -> 3710.52s]  that's used for playing music.
[3710.52s -> 3712.60s]  You know, I could tell you some other things about it.
[3712.60s -> 3715.76s]  I could say it has holes sort of like a recorder,
[3715.76s -> 3718.80s]  but it has multiple reeds and a flared end
[3718.80s -> 3720.80s]  more like an oboe.
[3720.80s -> 3723.00s]  Then maybe you know a bit more about a shinai
[3723.00s -> 3725.16s]  even though you've never seen one.
[3726.48s -> 3731.12s]  And if you then extend to what we do more
[3731.12s -> 3736.12s]  in our sort of corpus-based linguistic learning,
[3736.32s -> 3737.52s]  you know, you could imagine
[3737.52s -> 3740.40s]  it's not that I tried to define one for you.
[3740.40s -> 3743.96s]  Instead, I've just shown you a textual use example.
[3743.96s -> 3746.48s]  So, here are several of those.
[3746.48s -> 3749.40s]  So, here's one textual use example.
[3749.40s -> 3753.96s]  From a week before, shinai players sat in bamboo matchans
[3753.96s -> 3756.96s]  at the entrance to the house playing their pipes.
[3756.96s -> 3760.24s]  Bikash Babu disliked the shinai's wail,
[3760.24s -> 3762.04s]  but was determined to fulfill
[3762.04s -> 3764.20s]  every conventional expectation
[3764.20s -> 3766.24s]  the groom's family might have.
[3767.60s -> 3772.32s]  So, if that's all you know about a shinai,
[3772.32s -> 3774.20s]  you know, in some ways,
[3774.20s -> 3776.72s]  you understand less of the meaning of the word
[3776.72s -> 3778.64s]  than if you'd seen one.
[3778.64s -> 3781.56s]  But actually, in other ways,
[3781.56s -> 3784.76s]  you understand more of the meaning of the word
[3784.76s -> 3786.48s]  than if you'd just seen one
[3786.48s -> 3790.00s]  because, you know, from that one textual example,
[3790.00s -> 3791.56s]  you know some things.
[3791.56s -> 3796.24s]  You have heard a characterization of the sound as wailing,
[3797.16s -> 3801.72s]  and you know that it's connected with weddings,
[3801.72s -> 3804.24s]  which you don't get from just having held
[3804.24s -> 3806.96s]  or looked at one, or even, you know,
[3806.96s -> 3809.72s]  having had someone stand in front of you and play it.
[3809.72s -> 3811.64s]  And, you know, that's an important part
[3811.64s -> 3815.20s]  of the meaning of a shinai to people.
[3815.20s -> 3817.32s]  And so, that's the sense in which I think
[3817.40s -> 3820.36s]  meaning comes from various kinds of connections.
[3821.36s -> 3824.80s]  Okay, last topic, our AI future.
[3826.16s -> 3829.12s]  Yeah, so there are different senses of our AI future
[3829.12s -> 3833.24s]  and lots of things that we can be worried about.
[3833.24s -> 3834.64s]  One thing we can be worried about
[3834.64s -> 3837.00s]  is whether we're all gonna lose our jobs.
[3838.36s -> 3840.20s]  Interesting question.
[3841.92s -> 3845.84s]  Here's a newspaper article from the New York Times.
[3845.92s -> 3848.76s]  March of the machine makes idle hands,
[3848.76s -> 3850.32s]  prevalence of unemployment
[3850.32s -> 3852.80s]  with greatly increased industrial output
[3852.80s -> 3856.08s]  points to the influence of labor saving devices
[3856.08s -> 3858.40s]  as an underlying cause.
[3858.40s -> 3863.40s]  This was published in the New York Times in 1928.
[3863.88s -> 3866.72s]  But, you know, it turns out that quite a few people
[3866.72s -> 3871.32s]  like labor saving machines like washing machines
[3871.32s -> 3875.56s]  and dishwashers and sewing machines.
[3875.56s -> 3878.28s]  Lots of useful labor saving machines.
[3879.20s -> 3882.76s]  And, well, you know, this was published in 1928
[3882.76s -> 3887.76s]  just before, you know, at a time when a small group
[3888.52s -> 3891.88s]  of immensely powerful and rich men
[3891.88s -> 3894.80s]  dominated the United States
[3894.80s -> 3897.56s]  just before the Great Depression.
[3897.56s -> 3901.04s]  But what happened in the decades after that
[3901.88s -> 3905.68s]  is greatly changed policies in the United States
[3905.68s -> 3909.84s]  led to boom years that distributed wealth
[3909.84s -> 3914.28s]  and work much more evenly across the country
[3914.28s -> 3915.84s]  and the country boomed.
[3916.76s -> 3918.56s]  You know, here's another one.
[3918.56s -> 3921.92s]  In the past, new industries hired far more people
[3921.92s -> 3923.92s]  than those they put out of business.
[3923.92s -> 3927.48s]  But this is not true of many of today's new industries.
[3927.48s -> 3930.60s]  Today's new industries have comparatively few jobs
[3931.12s -> 3932.52s]  either the unskilled or semi-skilled,
[3932.52s -> 3934.76s]  just the class of workers whose jobs
[3934.76s -> 3937.20s]  are being eliminated by automation.
[3937.20s -> 3941.68s]  You know, this was Time Magazine in 1961.
[3941.68s -> 3944.84s]  So this is a long standing fear
[3944.84s -> 3947.64s]  which at least so far has not been realized.
[3947.64s -> 3951.96s]  You know, here we are in a country in which
[3951.96s -> 3955.64s]  not everyone might have the work that they wish they had
[3955.64s -> 3959.64s]  but that overall almost everybody has a job
[3959.68s -> 3963.60s]  and many people are working a lot of hours a week
[3963.60s -> 3966.12s]  whereas once upon a time the claim was
[3966.12s -> 3968.08s]  that before the end of the 20th century
[3968.08s -> 3970.28s]  we'd only have to do a three day work week
[3970.28s -> 3973.44s]  because there wouldn't be much work to go around, imagine.
[3975.20s -> 3979.12s]  Yeah, so another fear is will almost all the money
[3979.12s -> 3983.92s]  go to five to 10 enormous technology giants?
[3983.92s -> 3986.44s]  I actually think this is a more serious worry.
[3986.44s -> 3988.32s]  This seems to be the direction
[3988.36s -> 3990.56s]  that we're headed in at the moment.
[3990.56s -> 3993.88s]  I think there's no doubt that modern networks
[3993.88s -> 3996.12s]  and a concentration of AI talent
[3996.12s -> 3998.28s]  tend to encourage this outcome.
[3999.28s -> 4002.12s]  But you know, essentially this is the modern analog
[4002.12s -> 4006.20s]  of what happened in the early decades of the 20th century.
[4006.20s -> 4009.92s]  You know, the equivalent then was transportation networks
[4009.92s -> 4012.96s]  and it was domination of the new transportation networks
[4012.96s -> 4016.08s]  like railways that led to a few people
[4016.08s -> 4018.92s]  dominating the economic system.
[4018.92s -> 4023.92s]  But what happens there would be, you know,
[4024.24s -> 4027.56s]  essentially comes down to a political and social question.
[4027.56s -> 4030.40s]  So as I was mentioning before,
[4030.40s -> 4034.48s]  after the Great Depression country successfully dealt
[4034.48s -> 4038.48s]  with the monopolistic power of a small number of companies
[4039.56s -> 4044.12s]  and with political leadership we could do that again.
[4044.12s -> 4046.16s]  The problem is that there's not much sign
[4046.16s -> 4048.96s]  of political leadership right at the moment.
[4048.96s -> 4051.88s]  But that's a political problem to solve
[4051.88s -> 4053.24s]  rather than it actually being
[4053.24s -> 4055.56s]  a technological problem to solve.
[4057.12s -> 4060.04s]  So the next problem is should we be afraid
[4060.04s -> 4063.80s]  of an imminent singularity, i.e. when machines
[4063.80s -> 4068.40s]  have artificial general intelligence beyond the human level?
[4068.40s -> 4071.24s]  In particular, would such an event
[4071.24s -> 4073.88s]  threaten human survival?
[4073.88s -> 4078.88s]  So this is a concern that has increasingly
[4079.76s -> 4083.52s]  exploded into the mainstream with discussions
[4083.52s -> 4086.96s]  of AI existential risk and in quite a few
[4086.96s -> 4089.28s]  of the discussions that have been leading
[4089.28s -> 4092.92s]  to the setting up of things like AI safety institutes
[4092.92s -> 4097.48s]  in the US, UK, are motivated by maybe
[4097.48s -> 4100.40s]  there are these worries of out of control
[4100.40s -> 4104.04s]  artificial intelligence taking over
[4104.04s -> 4106.96s]  and deciding to eliminate humanity.
[4106.96s -> 4109.28s]  So we get these sort of article headlines
[4109.28s -> 4111.96s]  like pausing AI developments isn't enough.
[4111.96s -> 4114.48s]  We need to shut it all down.
[4114.48s -> 4117.60s]  How rogue AIs may arise.
[4117.60s -> 4120.96s]  AI godfather Jeffrey Hinton warns of dangers
[4120.96s -> 4122.56s]  as he quits Google.
[4122.56s -> 4125.68s]  We must slow down the race to godlike AI.
[4126.04s -> 4131.04s]  I don't personally give these concerns
[4132.96s -> 4136.28s]  too much credence.
[4136.28s -> 4138.48s]  And I think there's started to be increasing
[4138.48s -> 4141.00s]  pushback against them.
[4141.00s -> 4144.88s]  So in the other direction, Francois Chollet
[4144.88s -> 4148.68s]  who is the architect of Keras sort of argues
[4148.68s -> 4151.32s]  there does not exist any AI model or technique
[4151.32s -> 4154.32s]  that could represent an extinction risk for humanity,
[4154.32s -> 4157.00s]  not even if you extrapolate capabilities
[4157.00s -> 4159.84s]  far into the future via scaling laws.
[4159.84s -> 4163.08s]  Most arguments boil down to this is a new type
[4163.08s -> 4165.16s]  of technology, it could happen.
[4166.84s -> 4171.20s]  Joelle Pinot who's meta AI leader
[4171.20s -> 4175.08s]  refers to existential risk discourses unhinged
[4175.08s -> 4178.36s]  and points out the flaw of a lot
[4178.36s -> 4180.72s]  of the utilitarian argumentation
[4180.72s -> 4183.80s]  that goes along with discussions of these risks
[4184.24s -> 4189.24s]  which is if you say the elimination of humanity
[4190.64s -> 4195.64s]  is infinitely bad, that means any non-zero chance
[4196.88s -> 4199.44s]  multiplied by infinity will be bigger
[4199.44s -> 4201.28s]  than the badness of anything else
[4201.28s -> 4203.52s]  that could happen in the world.
[4203.52s -> 4206.40s]  But that isn't actually a sensible way
[4206.40s -> 4209.28s]  to have rational discussion about the outcomes.
[4209.28s -> 4211.92s]  And many people including Timny Gebru
[4211.96s -> 4214.64s]  have argued that a lot of the,
[4216.24s -> 4220.96s]  well, a lot of the outcome of this focus
[4220.96s -> 4224.28s]  on existential risk and if you're more cynical,
[4224.28s -> 4227.32s]  a lot of the purpose of this focus
[4227.32s -> 4230.52s]  on existential risk is to distract away
[4230.52s -> 4233.16s]  from the immediate harms that are arising
[4233.16s -> 4235.88s]  from companies deploying automated systems
[4235.88s -> 4239.00s]  including their biases, worker exploitation,
[4239.00s -> 4242.00s]  copyright violation, disinformation,
[4242.00s -> 4245.68s]  growing concentration of power and regulatory capture
[4245.68s -> 4247.84s]  by leading AI companies.
[4247.84s -> 4252.20s]  And that's something that is worth thinking about
[4252.20s -> 4256.12s]  that behind all the discussions about amazing AIs
[4256.12s -> 4258.16s]  and all the things we can do with them
[4258.16s -> 4262.16s]  like get our homework done or generate wonderful images,
[4262.16s -> 4264.44s]  that there are lots of things underneath
[4264.44s -> 4269.44s]  about disinformation, deception, hallucinations,
[4269.76s -> 4272.80s]  problems of homogeneity of decision making,
[4272.80s -> 4277.12s]  violation of copyrights and people's creativity,
[4277.12s -> 4279.04s]  lots of carbon emissions,
[4280.16s -> 4282.60s]  erosion of rich human practices.
[4282.60s -> 4285.76s]  So we need to be conscious of the present day harms
[4285.76s -> 4288.72s]  that can come about from AI.
[4288.72s -> 4292.04s]  And for NLP as well, there are various kinds of harms
[4292.08s -> 4293.32s]  that we've touched on,
[4293.32s -> 4296.36s]  which include generating offensive content,
[4296.36s -> 4300.76s]  generating untruthful content and enabling disinformation.
[4300.76s -> 4304.24s]  So the disinformation one is an interesting one
[4304.24s -> 4308.28s]  that if models can reason well about texts,
[4308.28s -> 4311.20s]  can they also be persuasive
[4311.20s -> 4315.32s]  in communicating incorrect information or opinions to users?
[4315.32s -> 4317.48s]  Perhaps there are new possibilities
[4317.48s -> 4322.40s]  for doing very personalized misinformation propagation
[4322.40s -> 4325.12s]  that easily persuades human beings
[4325.12s -> 4329.52s]  better than traditional methods of political advertising.
[4329.52s -> 4332.44s]  And there's starting to be evidence that that's true.
[4332.44s -> 4335.36s]  It's still being debated in the literature,
[4335.36s -> 4338.84s]  but there's now multiple studies suggesting
[4338.84s -> 4341.92s]  that humans can be influenced by disinformation
[4341.92s -> 4343.92s]  generated by AIs.
[4343.92s -> 4346.00s]  And it seems reasonable to think
[4346.04s -> 4348.76s]  that we're going to start seeing more use of that
[4348.76s -> 4351.24s]  in political systems and elsewhere,
[4351.24s -> 4354.48s]  which is potentially quite scary.
[4354.48s -> 4358.72s]  And perhaps the worst of it isn't gonna be text-based.
[4358.72s -> 4363.72s]  It's likely that visual fakes
[4363.80s -> 4367.16s]  are going to be even more compelling in political contexts.
[4367.16s -> 4370.20s]  And this sort of seems like
[4370.20s -> 4373.44s]  whether it happens in the US for this election
[4373.44s -> 4375.84s]  or in other countries in their elections,
[4375.84s -> 4379.12s]  that we're likely to see some major incidents
[4379.12s -> 4382.60s]  where AI-generated fakes can be seen
[4382.60s -> 4385.84s]  of having a major impacts on political systems.
[4386.96s -> 4390.64s]  So I sort of think really what we should be doing
[4390.64s -> 4394.32s]  is worrying not about existential risks,
[4394.32s -> 4398.08s]  but worrying about what people and organizations with power
[4398.08s -> 4399.96s]  will use AI to do.
[4401.12s -> 4404.96s]  That this is a pattern that we've noticed multiple times
[4405.00s -> 4407.16s]  also with social media, right?
[4407.16s -> 4409.44s]  In the early days of social media,
[4409.44s -> 4411.92s]  there was the idea that this was meant to lead
[4411.92s -> 4415.04s]  to new freedoms for people across the globe,
[4415.04s -> 4418.68s]  bringing the positives of free political thought
[4418.68s -> 4420.52s]  and improved human lives.
[4420.52s -> 4423.56s]  In last measure, that isn't what's happened.
[4423.56s -> 4426.84s]  The new technologies get captured by powerful people
[4426.84s -> 4431.52s]  and organizations who master the new technological options
[4431.52s -> 4435.84s]  and AI and machine learning is being increasingly used
[4435.84s -> 4437.64s]  for surveillance and control.
[4437.64s -> 4441.08s]  And we're seeing that around the world at the moment.
[4442.48s -> 4444.76s]  So my final thought to end with
[4445.72s -> 4448.48s]  is a thought about Carl Sagan.
[4448.48s -> 4452.28s]  So when I was young many decades ago,
[4452.28s -> 4456.84s]  Carl Sagan did the series Cosmos on television
[4456.84s -> 4460.00s]  explaining the miracles of the universe.
[4460.00s -> 4464.36s]  And at the time when I was a teenager, I loved Cosmos.
[4464.36s -> 4467.44s]  Now, this was a long time ago.
[4467.44s -> 4470.40s]  So much more recently, there's now a new generation
[4470.40s -> 4475.00s]  of Cosmos and the book is advertised on the basis of
[4475.00s -> 4478.20s]  With a New Forward by Neil deGrasse Tyson.
[4479.60s -> 4483.92s]  I think Carl Sagan was a good guy
[4483.92s -> 4486.24s]  and he didn't only write Cosmos,
[4486.24s -> 4488.48s]  he wrote a number of other books
[4488.52s -> 4490.52s]  and another of the books he wrote
[4490.52s -> 4492.96s]  was The Demon-Haunted World,
[4492.96s -> 4497.24s]  which has a theme that's a little bit closer
[4497.24s -> 4499.96s]  to some of the things that connect with
[4500.88s -> 4502.80s]  what we're dealing with here.
[4502.80s -> 4506.00s]  So in that book, he writes,
[4506.00s -> 4507.88s]  I have a foreboding of a world
[4507.88s -> 4510.72s]  in my children's or grandchildren's time
[4510.72s -> 4513.08s]  when awesome technological powers
[4513.08s -> 4515.44s]  are in the hands of a very few
[4515.44s -> 4518.08s]  and no one representing the public interest
[4518.08s -> 4520.24s]  can even grasp the issues.
[4520.24s -> 4521.96s]  When the people have lost the ability
[4521.96s -> 4523.68s]  to set their own agendas
[4523.68s -> 4526.60s]  or knowledgeably question those in authority,
[4526.60s -> 4528.20s]  when clutching our crystals
[4528.20s -> 4530.84s]  and nervously consulting our horoscopes,
[4530.84s -> 4533.40s]  our critical faculties in decline,
[4533.40s -> 4536.44s]  unable to distinguish between what feels good
[4536.44s -> 4537.84s]  and what's true,
[4537.84s -> 4540.44s]  we slide almost without noticing
[4540.44s -> 4543.48s]  back into superstition and darkness.
[4543.48s -> 4545.60s]  I think if you look around the US
[4545.60s -> 4548.56s]  and many other parts of the world today,
[4548.56s -> 4551.76s]  this is actually much more the risk
[4551.76s -> 4554.04s]  that humanity is facing
[4554.04s -> 4556.40s]  and why education,
[4556.40s -> 4558.48s]  which we try to provide at Stanford
[4558.48s -> 4562.32s]  and other places is an important thing
[4562.32s -> 4564.08s]  that should be valued
[4564.08s -> 4567.36s]  and all the other things that go along with this
[4567.36s -> 4569.32s]  of having things like open source
[4569.32s -> 4573.24s]  that supports the broad dissemination of learning.
[4574.36s -> 4575.20s]  Thank you.
[4575.60s -> 4576.44s]  Thank you.
[4576.44s -> 4577.28s]  Thank you.
[4577.28s -> 4578.12s]  Thank you.
[4578.12s -> 4579.12s]  Thank you.
