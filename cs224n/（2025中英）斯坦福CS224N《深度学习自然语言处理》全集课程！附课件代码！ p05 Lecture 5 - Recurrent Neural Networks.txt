# Detected language: en (p=1.00)

[0.00s -> 9.46s]  Okay, let me get started for today.
[9.46s -> 16.70s]  So for today, first of all, I'm going to spend a few minutes talking about a couple
[16.70s -> 22.12s]  more neural net concepts, including actually a couple of the concepts that turn up in
[22.12s -> 31.32s]  assignment two. Then the bulk of today is then going to be moving on to introducing
[31.32s -> 38.20s]  what our language models. And then after introducing language models, we're going
[38.20s -> 43.44s]  to introduce a new kind of neural network, which is one way to build
[43.44s -> 48.96s]  language models, which is recurrent neural networks. They're important thing
[48.96s -> 54.16s]  to know about, and we use them in assignment three, but they're certainly not the only
[54.16s -> 58.96s]  way to build language models. In fact, probably a lot of you already know that there's this
[58.96s -> 64.56s]  other kind of neural network called transformers, and we'll get onto those after we've done
[64.56s -> 70.12s]  recurrent neural nets. Talk a bit about problems with recurrent neural networks, and
[70.12s -> 74.96s]  well, if I have time, I'll get onto the recap. Before getting into the content
[74.96s -> 79.42s]  of the class, I thought I could just spend a minute on giving you the stats of who is
[79.42s -> 88.72s]  in CS224N. Who's in CS224N kind of looks like the pie charts they show in CS106A
[88.72s -> 96.48s]  these days, except more grad students, I guess. So the four big groups are the computer science
[96.48s -> 104.28s]  undergrads, the computer science grads, the undeclared undergraduates, and the NDO grads.
[104.28s -> 110.04s]  So this is a large portion of the SCPD students, so some of them are under computer science
[110.04s -> 117.20s]  grads. So that makes up about 60% of the audience, and if you're not in one of those four big
[117.20s -> 122.40s]  groups, you're in the other 40%, and everybody is somewhere. So there are lots of other
[122.40s -> 129.12s]  interesting groups down here. So the bright orange down here, that's where the math and
[129.12s -> 137.28s]  physics PhDs are. Up here, I mean interestingly, we now have more statistics grad students than
[137.28s -> 144.08s]  there are synthesis undergrads. It didn't used to be that way around in NLP classes. And you
[144.08s -> 152.00s]  know, one of my favorite groups, the little magenta group down here, these are the humanities
[152.00s -> 160.72s]  undergrads. Yeah, humanities undergrads. In terms of years, it breaks down like this. First
[160.72s -> 166.60s]  year grad students are the biggest group, tons of juniors and seniors, and a couple of brave
[166.60s -> 181.00s]  frosh. Are any brave frosh here today? Yeah. Okay, welcome. Yeah, so modern neural networks,
[181.00s -> 189.04s]  especially language models, are enormous. This chart's sort of out of date, because it only goes
[189.04s -> 196.76s]  up to 2022, but it's sort of actually hard to make an accurate chart for 2024, because in the
[196.76s -> 201.72s]  last couple of years, the biggest language model makers have in general stopped saying how
[201.72s -> 206.52s]  large their language models are in terms of parameters. But at any rate, they're clearly
[207.04s -> 217.28s]  huge models which have over a hundred billion parameters. And so large and then deep in terms
[217.28s -> 225.80s]  of very many layers, neural nets, are a cornerstone of modern NLP systems. We're going to be pretty
[225.80s -> 233.04s]  quickly working our way up to look at those kind of deep models. But I just sort of, for
[233.04s -> 239.92s]  starting off with something simpler, you know, I did just want to kind of key you in for a few
[239.92s -> 248.40s]  minutes into a little bit of history, right? So the last time neural nets were popular was in the
[248.40s -> 254.52s]  80s and 90s, and that was when people worked out the backpropagation algorithm. Jeff Hinton
[254.52s -> 260.16s]  and colleagues made famous the backpropagation algorithm that we've looked at, and that allowed
[260.28s -> 269.64s]  the training of neural nets with hidden layers. And so, but in those days, pretty much all the
[269.64s -> 274.44s]  neural nets with hidden layers that were trained were trained with one hidden layer. You had the
[274.44s -> 281.28s]  input, the hidden layer, and the output, and that's all that there was. And the reason for
[281.28s -> 289.84s]  that was for a very, very long time, people couldn't really get things to work with more
[289.84s -> 296.96s]  hidden layers. So that only started to change, and the resurgence of what often got called deep
[296.96s -> 304.80s]  learning, but anyway, back to neural nets, started around 2006. And this was one of the
[304.80s -> 310.68s]  influential papers at the time, Greedy Layerwise Training of Deep Neural Networks by Yoshua
[310.68s -> 315.92s]  Abengio and colleagues. And so right at the beginning of that paper, they observed this,
[316.48s -> 322.32s]  the problem. However, until recently, it was believed too difficult to train deep multi-layer
[322.32s -> 328.52s]  neural networks. Empirically, deep networks were generally found to be not better, and often
[328.52s -> 335.64s]  worse, than neural networks with one or two hidden layers. Jerry Tesoro was actually a faculty
[335.64s -> 341.28s]  member who worked very early on autonomous driving with neural networks. As this is a negative
[341.28s -> 348.26s]  result, there's not been much report in the machine learning literature. So that really,
[348.26s -> 354.16s]  you know, although people had neural networks and back propagation and recurrent neural networks
[354.16s -> 361.36s]  we're going to talk about today, that for a very long period of time, you know, 15 years or
[361.36s -> 368.12s]  so, things seemed completely stuck in that you could, although in theory it seemed like deep
[368.12s -> 376.32s]  neural networks should be promising, in practice they didn't work. And so it really then took some
[376.32s -> 381.72s]  new developments that happened in the late 2000s decade and then more profoundly in the
[381.72s -> 389.72s]  2010s decade to actually figure out how we could have deep neural networks that actually
[389.72s -> 394.64s]  worked, working far better than the shallow neural networks and leading into the networks
[394.80s -> 401.00s]  that we have today. And, you know, we're going to be starting to talk about some of those
[401.00s -> 411.00s]  things in this class and coming up with classes. And I mean, I think, you know, the tendency when
[411.00s -> 420.16s]  you see the things that got neural networks to work much better, like the natural reaction
[420.20s -> 426.72s]  is to sort of shrug and be underwhelmed and think, oh, is this all there is to it? This
[426.72s -> 434.32s]  doesn't exactly seem like difficult science. And in some sense that's true. There are fairly
[434.32s -> 443.16s]  little introductions of new ideas and tweaks of things, but nevertheless a handful of little
[443.16s -> 450.20s]  ideas and tweaks of things turn things around from a field that was sort of stuck for 15
[450.20s -> 457.08s]  years going nowhere and which nearly everyone had abandoned because of that to suddenly turning
[457.08s -> 463.28s]  around and there being the ability to train these deeper neural networks, which then behaved
[463.28s -> 469.60s]  amazingly better as machine learning systems than other things that had preceded them and
[469.64s -> 478.40s]  dominated for the intervening time. So that took a lot of time. So what are these things? One of
[478.40s -> 485.68s]  them, which you can greet with a bit of a yawn in some sense, is doing better regularization of
[485.68s -> 494.60s]  neural nets. So regularization is the idea that beyond just having a loss that we want to
[494.60s -> 503.00s]  minimize in terms of describing the data, we want to in some other ways manipulate what parameters
[503.00s -> 511.62s]  we learn so that our models work better. And so normally we have some more complex loss
[511.62s -> 519.28s]  function that does some regularization. The most common way of doing this is what's called L2 loss,
[519.40s -> 527.40s]  where you add on this parameter squared term at the end and this regularization says,
[527.40s -> 534.80s]  you know, it'd be kind of good to find a model with small parameter weights. So you
[534.80s -> 540.96s]  should be finding the smallest parameter weights that will explain your data well. And there's
[540.96s -> 548.80s]  a lot you can say about regularization, these kind of losses. They get talked about a lot more
[548.84s -> 556.60s]  in other classes like CS229 machine learning. And so I'm not going to say very much about it.
[556.60s -> 563.84s]  This isn't a machine learning theory class. But I do just want to sort of put in one note
[563.84s -> 574.12s]  that's sort of very relevant to what's happened in recent neural networks work. So the classic
[574.44s -> 580.52s]  regularization was we needed this kind of regularization to prevent our networks
[580.52s -> 587.84s]  from overfitting, meaning that they would do a very good job at modeling the training data,
[587.84s -> 595.08s]  but then they would generalize badly to new data that was shown. And so the picture that you
[595.08s -> 603.60s]  got shown was this, that as you train on some training data, your error necessarily goes down.
[604.16s -> 612.28s]  However, after some point, you start learning specific properties of things that happen to
[612.28s -> 617.52s]  turn up in those training examples and that you're learning things that are only good for
[617.52s -> 624.16s]  the training examples. And so they won't generalize well to different pieces of data you see at test
[624.16s -> 631.04s]  time. So if you have a separate validation set or a final test set, you would, and you've
[631.04s -> 639.88s]  traced out the error or loss on that validation or test set, that after some point it would start
[639.88s -> 646.40s]  to go up again. This is a quirk in my bad PowerPoint. It's just meant to go up. And the
[646.40s -> 653.84s]  fact that it goes up is then you have overfit your training data. And making the parameters
[653.84s -> 658.92s]  numerically small is meant to lessen the extent to which you overfit on your training data.
[658.92s -> 668.40s]  This is not a picture that modern neural network people believe at all. Instead,
[668.40s -> 676.52s]  the picture has changed like this. We don't believe that overfitting exists anymore,
[676.52s -> 684.76s]  but what we are concerned about is models that will generalize well to different data.
[685.72s -> 694.60s]  So that when we train, you know, so in classical statistics the idea that you could train billions
[694.60s -> 701.64s]  of parameters like large neural nets now have would be seen as ridiculous because you could not
[701.64s -> 709.52s]  possibly estimate those parameters well. And so you just have all of this noisy mess. But what's
[709.56s -> 714.60s]  actually been found is that, yeah, it's true you can't estimate the numbers well,
[714.60s -> 720.20s]  but what you get is a kind of interesting averaging function from all these myriad
[720.20s -> 729.88s]  numbers. And if you do it right, what happens is as you go on training, that for a while it
[729.88s -> 734.64s]  might look like you're starting to overfit, but if you keep on training in a huge network,
[735.48s -> 742.60s]  not only will your training loss continue to go down very infinitesimally, but your validation
[742.60s -> 752.52s]  loss will go down as well. And so that huge, on huge networks these days, we train our models
[752.52s -> 758.68s]  so that they overfit to the training data almost completely, right? So that if you train
[758.72s -> 765.60s]  a huge network now on a training set, you can essentially train them to get zero loss,
[765.60s -> 772.60s]  you know, maybe it's 0.00007 loss or something, but you can train them to get zero loss because
[772.60s -> 779.28s]  you've got such rich models, you can perfectly fit, memorize the entire training set. Now,
[779.28s -> 783.16s]  classically that would have been seen as a disaster because you've overfit the training
[783.16s -> 789.16s]  data. With modern large neural networks, it's not seen as a disaster because providing you've
[789.16s -> 796.60s]  done regularization well, that your model will also generalize well to different data. However,
[796.60s -> 803.80s]  the flip side of that is normally this kind of L2 regularization or similar ones like L1
[803.80s -> 810.28s]  regularization aren't strong enough regularization to achieve that effect. And so neural network
[810.36s -> 817.56s]  people have turned to other methods of regularization of which everyone's favorite is dropout. So this
[817.56s -> 827.56s]  is one of the things that's on the assignment. And at this point I should apologize or something
[827.56s -> 835.64s]  because the way dropout is done, the way dropout is presented here is sort of the original
[835.64s -> 840.24s]  formulation. The way dropout is presented on the assignment is the way it's now normally done,
[840.24s -> 847.08s]  in deep learning packages. So there are a couple of details that vary a bit. And let me just
[847.08s -> 853.80s]  present the main idea here and not worry too much about the details of the math. So the idea
[853.80s -> 862.72s]  of dropout is at training time, every time you are doing a piece of training with an example,
[862.72s -> 868.96s]  what you're going to do is inside the middle layers of the neural network, you're just going
[868.96s -> 874.68s]  to throw away some of the inputs. And so technically the way you do this is you have
[874.68s -> 882.28s]  a random mask that you sample each time of zeros and ones. You do a Hadamard product of that with
[882.28s -> 889.80s]  the data. So some of the data items go to zero and you have different masks each time. So for
[889.80s -> 897.96s]  the next thing, you know, I've now masked out something different this time. And so you're
[898.00s -> 904.76s]  just sort of randomly throwing away the inputs. And the effect of this is that you're training
[904.76s -> 912.92s]  the model that it has to be robust and work well and make as much use of every input as it
[912.92s -> 920.28s]  can. It can't decide that can be extremely reliant on, you know, component 17 of the vector,
[920.28s -> 924.92s]  because sometimes it's just going to randomly disappear. So if there are other features that
[924.92s -> 930.12s]  you could use instead that would let you work out what to do next, you should also know how
[930.12s -> 936.80s]  to make use of those features. So at training time, you randomly delete things. At test time,
[936.80s -> 943.00s]  so for efficiency, but also quality of the answer, you don't delete anything. You keep
[943.00s -> 947.88s]  all of your weights, but you just rescale things to make up for the fact that you
[947.88s -> 955.84s]  used to be dropping things. Okay, so there are several ways that you can think of explaining
[955.84s -> 962.96s]  this. One motivation that's often given is that this prevents feature co-adaptation. So
[962.96s -> 970.56s]  rather than a model being able to learn complex functions of feature 7, 8, and 11 can help me
[970.56s -> 976.92s]  predict this, it knows that some of the features might be missing. So it has to sort of make use
[976.96s -> 983.16s]  of things in a more flexible way. Another way of thinking of it is that there's been a lot of work
[983.16s -> 989.04s]  on model ensembles where you can sort of mix together different models and improve your
[989.04s -> 994.88s]  results. If you're training with dropout, it's kind of like you're training with a huge model
[994.88s -> 1000.84s]  ensemble because you're training with the ensemble of the power set, the exponential number
[1000.88s -> 1007.48s]  of every possible dropout of features all at once, and that gives you a very good model.
[1007.48s -> 1013.68s]  So there are different ways of thinking about it. I mean, if you've seen Naive Bayes and
[1013.68s -> 1018.76s]  logistic regression models before, you know, I kind of think a nice way to think of it is
[1018.76s -> 1023.96s]  that it gives us sort of a middle ground between the two because for Naive Bayes models,
[1023.96s -> 1028.48s]  you're weighting each feature independently, just based on the data statistics,
[1028.48s -> 1033.20s]  doesn't matter what other features are there. In a logistic regression, weights are set in the
[1033.20s -> 1039.32s]  context of all the other features, and with dropout, you're somewhere in between. You're
[1039.32s -> 1043.28s]  seeing the weights in the context of some of the other features, but different ones will
[1043.28s -> 1049.92s]  disappear at different times. But, you know, following work that was done at Stanford by
[1049.92s -> 1057.32s]  Stefan Barger and others, generally these days people regard dropout as a form of feature
[1057.52s -> 1062.28s]  dependent regularization, and he shows some theoretical results as to why to think of
[1062.28s -> 1071.44s]  it that way. Okay, I think we've implicitly seen this one, but vectorization is the idea
[1071.44s -> 1080.96s]  no for loops always use vectors, matrices, and tensors, right? The entire success and speed
[1081.04s -> 1088.44s]  of deep learning works from the fact that we can do things with vectors, matrices, and tensors. So,
[1088.44s -> 1093.08s]  you know, if you're writing for loops in any language, but especially in Python,
[1093.08s -> 1100.36s]  things run really slowly. If you can do things with vectors and matrices, even on CPU,
[1100.36s -> 1106.36s]  things run at least an order of magnitude faster. And, well, what everyone really wants to move
[1106.40s -> 1113.20s]  to doing in deep learning is running things on GPUs, or sometimes now neural processing units,
[1113.20s -> 1117.36s]  and then you're getting, you know, two, three orders of magnitude of speed up. So,
[1117.36s -> 1123.48s]  do always think about I should be doing things with vectors and matrices. If I'm
[1123.48s -> 1129.24s]  writing a for loop for anything that isn't some very superficial bit of input processing,
[1129.24s -> 1135.36s]  I've almost certainly made a mistake, and I should be working out how to do things with
[1135.40s -> 1140.60s]  vectors and matrices. And, you know, that's kind of things like dropout. You don't want to write
[1140.60s -> 1146.64s]  a for loop that goes through all the positions and set some of them to zero. You want to be sort
[1146.64s -> 1155.36s]  of using a vector operation with your mask. Two more, I think. Parameter initialization. I
[1155.36s -> 1162.92s]  mean, this one might not be obvious, but when we start training our neural networks,
[1163.36s -> 1174.80s]  in almost all cases, it's vital that we initialize the parameters of our matrices to some random
[1174.80s -> 1182.48s]  numbers. And the reason for this is if we just start with the, if we just start with our
[1182.48s -> 1191.80s]  matrices all zero or some other constant, normally the case is that we have symmetry.
[1192.32s -> 1198.52s]  It's sort of like in this picture when you're starting on this saddle point that, you know,
[1198.52s -> 1204.40s]  it's symmetric to the left and the right and, whatever, forward and backwards and left and
[1204.40s -> 1210.40s]  right. And so, you sort of don't know where to go and you might be sort of stuck and stay
[1210.40s -> 1216.56s]  in the one place. I mean, normally a way to think about is the operations that you're doing
[1216.56s -> 1222.20s]  to all the elements in the matrix are sort of the same. So, rather than having, you know,
[1222.20s -> 1227.80s]  a whole vector of features, if all of them have the same value initially, often it's sort
[1227.80s -> 1234.12s]  of like you only have one feature and you've just got a lot of copies of it. So, to initialize
[1234.12s -> 1240.52s]  learning and have things work well, we almost always want to set all the weights to very small
[1240.56s -> 1251.16s]  random numbers. And so, at that point, you know, when I say very small, we sort of want to make
[1251.16s -> 1257.32s]  them in a range so that they don't disappear to zero if we make them a bit smaller and they
[1257.32s -> 1262.32s]  don't sort of start blowing up into huge numbers when we multiply them by things. And
[1262.32s -> 1268.92s]  doing this initialization at the right scale was, used to be seen as something pretty important
[1269.16s -> 1273.32s]  and there are particular methods that had a basis of sort of thinking of what happens
[1273.32s -> 1278.68s]  once you do matrix multiplies that people have worked out and often used. One of these was
[1278.68s -> 1285.88s]  this Javier initialization, which was sort of working out what variance of your uniform
[1285.88s -> 1292.64s]  distribution to be, variance of your distribution to be using based on the sort of number of
[1292.68s -> 1299.16s]  inputs and outputs of a layer and things like that. The specifics of that, you know, I think
[1299.16s -> 1304.60s]  we still use to initialize things in this assignment too, but we'll see later that they
[1304.60s -> 1310.76s]  go away because people have come up with clever methods, in particular doing layer normalization,
[1310.76s -> 1315.48s]  which sort of obviates the need to be so careful on the initialization, but you still
[1315.52s -> 1324.44s]  need to initialize things to something. OK, then the final one, which is also something
[1324.44s -> 1330.04s]  that appears in the second assignment that I just want to say a word about, was optimizers.
[1330.04s -> 1338.12s]  So we talked about in class stochastic gradient descent and did the basic equations for stochastic
[1338.12s -> 1343.64s]  gradient descent. And, you know, to a first approximation, there's nothing wrong with
[1343.68s -> 1348.68s]  stochastic gradient descent. And if you fiddle around enough, you can usually get stochastic
[1348.68s -> 1354.56s]  gradient descent actually to work well for almost any problem. But getting it to work
[1354.56s -> 1360.60s]  well is very dependent on getting the scales of things right, of sort of having the right
[1360.60s -> 1365.12s]  step size. And often you have to have a learning rate schedule with decreasing step sizes
[1365.12s -> 1373.40s]  and various other complications. So people have come up with more sophisticated optimizers
[1373.40s -> 1379.96s]  for neural networks. And for complex nets, sometimes these seem kind of necessary to
[1379.96s -> 1385.44s]  get them to learn well. And at any rate, they give you sort of lots of margins of
[1385.44s -> 1391.44s]  safety since they're much less dependent on you setting different hyperparameters right.
[1391.44s -> 1398.08s]  And the idea of all of the methods I mentioned and the most commonly used methods is that
[1398.08s -> 1406.44s]  for each parameter, they're accumulating a measure of what the gradient has been in the past.
[1406.44s -> 1412.72s]  And they've got some idea of the scale of the gradient, the slope for a particular parameter.
[1412.72s -> 1418.64s]  And then they're using that to decide how much you move the learning rate at each time step.
[1418.64s -> 1423.00s]  So the simplest method that was come up was this one called AdaGrad.
[1423.00s -> 1427.52s]  If you know John Ducey in EE, he was one of the co-inventors of this.
[1427.60s -> 1431.56s]  You know, it's simple and nice enough, but it tends to stall early.
[1431.56s -> 1434.08s]  Then people came up with different methods.
[1434.08s -> 1436.44s]  Adam's the one that's on assignment two.
[1436.44s -> 1439.88s]  It's a really good safe place to start.
[1439.88s -> 1446.40s]  But in a way, sort of our word vectors have a special property because of this sparseness
[1446.40s -> 1452.56s]  that you know you're very sparsely updating them because particular words only turn up occasionally.
[1452.60s -> 1459.12s]  So people have actually come up with particular optimizers that sort of have special properties
[1459.12s -> 1461.16s]  for things like word vectors.
[1461.16s -> 1465.16s]  And so these ones with a W at the end can sometimes be good to try.
[1465.16s -> 1472.04s]  And then, you know, again, there's a whole family of extra ideas that people have used to improve optimizers.
[1472.04s -> 1477.92s]  And if you want to learn about that, you can go off and do an optimization class like convex optimization.
[1477.92s -> 1482.36s]  But there are ideas like momentum and Nesterov acceleration and things like that.
[1482.36s -> 1485.96s]  And all of those things people also variously try to use.
[1485.96s -> 1491.00s]  But Adam is a good name to remember if you remember nothing else.
[1491.00s -> 1493.52s]  Okay, that took longer than I hoped.
[1493.52s -> 1496.16s]  But I'll get on now to language models.
[1496.16s -> 1498.28s]  Okay, language models.
[1498.28s -> 1505.56s]  So, you know, in some sense language model is just two English words.
[1505.56s -> 1511.04s]  But when in NLP we say language models, we mean it as a technical term
[1511.08s -> 1513.48s]  that has a particular meaning.
[1513.48s -> 1522.76s]  So the idea of a language model is something that can predict, well, what word is going to come next.
[1522.76s -> 1528.80s]  Or more precisely, it's going to put a probability distribution over what words come next.
[1528.80s -> 1532.88s]  So the students open their, what words are likely to come next?
[1536.36s -> 1537.12s]  Bags.
[1538.88s -> 1539.92s]  Laptops.
[1541.16s -> 1541.68s]  Notebooks.
[1541.68s -> 1543.40s]  Notebooks, yes.
[1543.40s -> 1545.52s]  I have some of those at least.
[1545.52s -> 1553.52s]  Okay, yeah, I mean, so, right, so these are kind of likely words.
[1553.52s -> 1560.20s]  And if on top of those we put a probability on each one, then we have a language model.
[1560.20s -> 1566.00s]  So formally we've got a context of preceding items.
[1566.04s -> 1570.72s]  We're putting a probability distribution over the next item,
[1570.72s -> 1577.36s]  which means that the sum of the estimates of this items in the vocabulary will sum to one.
[1577.36s -> 1582.88s]  And if we've defined a P like this that predicts probabilities of next words,
[1582.88s -> 1589.00s]  that is called a language model, as it says here.
[1589.00s -> 1595.04s]  An alternative way that you can think of a language model is that a language model
[1595.08s -> 1600.20s]  is a system that assigns a probability to a piece of text.
[1600.20s -> 1606.88s]  And so we can say that a language model can take any piece of text and
[1606.88s -> 1608.36s]  give it a probability.
[1608.36s -> 1611.84s]  And the reason we can do that is we can use the chain rule.
[1611.84s -> 1616.60s]  So if I want to know the probability of any stretch of text,
[1616.60s -> 1621.56s]  I say, given my previous definition of language model, easy, I can do that.
[1621.56s -> 1626.40s]  Probability of x1 with a null preceding context times
[1626.40s -> 1630.36s]  the probability of x2 given x1, etc., along.
[1630.36s -> 1633.68s]  I can do this chain rule decomposition.
[1633.68s -> 1636.76s]  And then the terms of that decomposition
[1636.76s -> 1641.80s]  are precisely what the language model, as I defined it, previously provides.
[1641.80s -> 1648.08s]  Okay, so language models are this essential technology for NLP.
[1648.08s -> 1654.64s]  Just about everything from the simplest places forward where people do things
[1654.64s -> 1659.28s]  with human language and computers, people use language models.
[1659.28s -> 1665.36s]  In particular, they weren't something that got invented in 2022 with ChatGPT.
[1665.36s -> 1671.08s]  Language models have been central to NLP at least since the 80s.
[1671.08s -> 1674.56s]  The idea of them goes back to at least the 50s.
[1674.60s -> 1680.16s]  So anytime you're typing on your phone and it's making suggestions of next
[1680.16s -> 1684.16s]  words, regardless of whether you like those suggestions or not,
[1684.16s -> 1688.12s]  those suggestions are being generated by a language model.
[1689.40s -> 1693.68s]  Traditionally, a compact, not very good language model, so
[1693.68s -> 1698.80s]  it can run sort of quickly in very little memory in your keyboard application.
[1700.00s -> 1703.96s]  If you go on Google and you start typing some stuff, and
[1703.96s -> 1708.56s]  it's telling you stuff that could come after it to complete your query,
[1708.56s -> 1712.04s]  well again, that's being generated by a language model.
[1713.04s -> 1715.48s]  So how can you build a language model?
[1715.48s -> 1718.84s]  So before getting into neural language models,
[1718.84s -> 1724.04s]  I've got just a few slides to tell you about the old days of language modeling.
[1724.04s -> 1732.40s]  So this is sort of how language models were built from 1975 until
[1732.44s -> 1737.04s]  effectively around about 2012.
[1737.04s -> 1742.44s]  So we want to put probabilities on these sequences.
[1742.44s -> 1747.64s]  And the way we're gonna do it is we're gonna build
[1747.64s -> 1750.00s]  what's called an n-gram language model.
[1751.20s -> 1756.08s]  And so this is meaning we're going to look at short word sub-sequences and
[1756.08s -> 1757.20s]  use them to predict.
[1757.24s -> 1763.04s]  So n is a variable describing how short are the word sequences that we're gonna
[1763.04s -> 1764.96s]  use to predict.
[1764.96s -> 1769.08s]  So if we just look at the probabilities of individual words,
[1769.08s -> 1771.76s]  we have a unigram language model.
[1771.76s -> 1775.96s]  If we look at probabilities of pairs of words by gram language model,
[1777.60s -> 1781.96s]  probabilities of three words, trigram language models, probabilities of more
[1781.96s -> 1785.92s]  than three words, they get called four gram language models,
[1785.96s -> 1789.68s]  five gram language models, six gram language models.
[1789.68s -> 1794.08s]  So for people with a classics education, this is horrific, of course.
[1794.08s -> 1798.96s]  In particular, not even these ones are correct because
[1798.96s -> 1804.24s]  gram is a Greek root, so it should really have Greek numbers in front here.
[1804.24s -> 1807.80s]  So you should have monograms and diagrams.
[1807.80s -> 1811.56s]  And actually, so the first person who introduced the idea of n-gram models
[1811.56s -> 1815.52s]  was actually Claude Shannon when he was working at information theory,
[1815.52s -> 1818.44s]  the same guy that did cross-entropy and all of that.
[1818.44s -> 1823.16s]  And if you look at his 1951 paper, he uses diagrams.
[1823.16s -> 1829.12s]  But the idea died about there, and everyone else, this is what people say in practice.
[1830.48s -> 1834.16s]  It's kind of cute, I like it, a nice practical notation.
[1835.20s -> 1840.68s]  So to build these models, the idea is, look, we're just gonna
[1840.68s -> 1845.52s]  count how often different n-grams appear in text,
[1845.52s -> 1849.24s]  and use those to build our probability estimates.
[1849.24s -> 1855.08s]  And in particular, our trick is that we make a Markov assumption,
[1855.08s -> 1860.28s]  so that if we're predicting the next word based on a long context,
[1860.28s -> 1863.92s]  we say, tell you what, we're not gonna use all of that.
[1863.92s -> 1869.32s]  We're only gonna use the most recent n-1 words.
[1869.32s -> 1873.64s]  So we have this big context, and we throw most of it away.
[1873.64s -> 1880.96s]  And so if we're predicting word xt plus 1 based on simply the preceding n-1 words,
[1880.96s -> 1886.00s]  well, then we can make the prediction using n-grams.
[1887.68s -> 1893.32s]  Why, let's, whatever it is, if we use n is 3,
[1893.36s -> 1899.60s]  We would have a trigram here, and normalized by a bigram down here,
[1899.60s -> 1905.24s]  and that that would give us relative frequencies of the different terms.
[1906.96s -> 1912.16s]  So we can do that simply by counting how often
[1912.16s -> 1916.80s]  n-grams occur in a large amount of text, and
[1916.80s -> 1919.84s]  simply dividing through by the counts, and
[1919.88s -> 1924.40s]  that gives us a relative frequency estimate of the probability
[1924.40s -> 1928.28s]  of different continuations, does that make sense?
[1928.28s -> 1929.92s]  Yeah, that's the way to do it?
[1929.92s -> 1936.96s]  Okay, so suppose we're learning a four gram language model, right?
[1936.96s -> 1940.40s]  And we've got a piece of text, as the practice started the clock,
[1940.40s -> 1942.60s]  the students opened there.
[1942.60s -> 1948.76s]  So well, to estimate things, we are going to throw away all but
[1948.76s -> 1950.52s]  the preceding three words.
[1950.52s -> 1954.20s]  So we're going to estimate based on students opened there.
[1954.20s -> 1958.28s]  And so we're going to work out the probabilities by looking for
[1958.28s -> 1964.40s]  counts of students opened there, w, and counts of students opened there.
[1964.40s -> 1969.76s]  So we might have in a corpus that students opened there occurred 1,000 times.
[1969.76s -> 1973.20s]  Students opened their books occurred 400 times.
[1973.20s -> 1978.64s]  And so we'd say the probability estimate is simply 0.4 for books.
[1979.56s -> 1986.24s]  If exams occurred 100 times, the probability estimate is 0.1 for exams.
[1989.24s -> 1992.48s]  And well, you can sort of see that this is bad.
[1992.48s -> 1995.44s]  It's not terrible, cuz if you are going to try and
[1995.44s -> 2000.32s]  predict the next word in a simple way, looking at the immediately prior words,
[2000.32s -> 2003.08s]  these are the most helpful words to look at.
[2003.08s -> 2007.92s]  But it's clearly sort of primitive, because if you'd known that
[2008.24s -> 2011.08s]  the prior text was as the proctor started the clock,
[2011.08s -> 2014.60s]  that makes it sound likely that the word should have been exams.
[2014.60s -> 2018.52s]  Where since you're estimating just based on students opened theirs,
[2018.52s -> 2023.28s]  well you'd be more likely to choose books because it's more common.
[2023.28s -> 2028.80s]  So it's a kind of a crude estimate, but it's a decent enough place to start.
[2030.32s -> 2034.24s]  It's a crude estimate that could be problematic in other ways.
[2034.24s -> 2038.32s]  I mean, why else might we kind of get into
[2038.32s -> 2041.12s]  troubles by using this probability estimate?
[2043.82s -> 2044.32s]  Yeah?
[2046.32s -> 2048.56s]  So there are a lot of n-grams.
[2048.56s -> 2050.80s]  Yeah, so there are a lot of words.
[2050.80s -> 2053.64s]  And therefore, there are a lot of n-grams.
[2053.64s -> 2054.72s]  Yeah, so that's a problem.
[2054.72s -> 2056.52s]  We'll come to it later.
[2056.52s -> 2058.68s]  Anything else maybe up the back?
[2058.68s -> 2061.36s]  Like the word w might not even show up in the training data.
[2061.36s -> 2063.40s]  So you might just have to count zero for that.
[2063.64s -> 2071.24s]  Yeah, so if we're counting over any reasonable size corpus,
[2071.24s -> 2078.24s]  there are lots of words that we just are not going to have seen, right?
[2078.24s -> 2082.72s]  That they never happen to occur in the text that we counted over.
[2082.72s -> 2085.96s]  So if you start thinking students open there,
[2085.96s -> 2089.40s]  there are lots of things that you could put there.
[2089.40s -> 2091.64s]  Students open their accounts.
[2091.64s -> 2095.88s]  Or if the students are doing dissections in a biology class,
[2095.88s -> 2098.72s]  maybe students open their frogs, I don't know.
[2098.72s -> 2105.16s]  There are lots of words that in some context would actually be possible.
[2105.16s -> 2108.24s]  And lots of them that we won't have seen.
[2108.24s -> 2111.24s]  And so it gives them a probability estimate of zero.
[2111.24s -> 2114.68s]  And that tends to be an especially bad thing to do with probabilities.
[2114.68s -> 2118.92s]  Cuz once we have a probability estimate of zero, any computations that we do
[2118.92s -> 2122.16s]  that involve that will instantly go to zero.
[2122.16s -> 2124.72s]  So we have to deal with some of these problems.
[2124.72s -> 2128.00s]  So for that sparsity problem, right, yeah,
[2128.00s -> 2134.04s]  that we could have the word never occurred in the numerator.
[2134.04s -> 2139.20s]  And so simply done, we get a probability estimate of zero.
[2139.20s -> 2141.00s]  The way that was dealt with
[2141.00s -> 2145.00s]  was that people just hacked the counts a little to make it non-zero.
[2145.00s -> 2147.44s]  So there are lots of ways that are explored.
[2147.44s -> 2151.88s]  But the easiest way is you just sort of added a little delta,
[2151.88s -> 2155.12s]  like 0.25 to counts.
[2155.12s -> 2159.76s]  So things that you never saw got a count of 0.25 in total.
[2159.76s -> 2163.04s]  And things you saw once got a count of 1.25.
[2163.04s -> 2166.08s]  And then there are no zeros anymore, everything is possible.
[2167.28s -> 2170.72s]  You could think then there's a second problem that, wait,
[2170.72s -> 2175.08s]  you might never have seen students open there before.
[2175.08s -> 2180.80s]  Hence, that means your denominator is just undefined,
[2180.80s -> 2183.52s]  and you don't have any counts in the numerator either.
[2183.52s -> 2186.36s]  So you sort of need to do something different there.
[2186.36s -> 2191.68s]  And the standard trick that was used then was that you did back off.
[2191.68s -> 2196.28s]  So if you couldn't estimate words coming after students open there,
[2196.28s -> 2201.84s]  you just worked out the estimates for words coming after open there.
[2201.84s -> 2203.56s]  And if you couldn't estimate that,
[2203.56s -> 2207.28s]  you just used the estimate of words coming after there.
[2207.28s -> 2208.20s]  So you used less and
[2208.20s -> 2212.08s]  less context until you could get an estimate that you could use.
[2213.56s -> 2218.40s]  But something to note is that we've got these conflicting pressures now.
[2218.40s -> 2224.44s]  So that on the one hand, if you want to come up with a better estimate
[2224.44s -> 2229.88s]  that you would like to use more context, i.e., to have a larger n-gram.
[2229.88s -> 2234.60s]  But on the other hand, as you use more
[2236.12s -> 2242.08s]  conditioning words, well, the storage size problem someone mentioned
[2242.08s -> 2246.48s]  gets worse and worse because the number of n-grams that you have to know about
[2246.48s -> 2250.04s]  is going up exponentially with the size of the context.
[2250.04s -> 2254.44s]  But also, your sparseness problems are getting way, way worse.
[2254.44s -> 2258.16s]  And you're almost necessarily gonna be ending up seeing zeros.
[2258.16s -> 2261.32s]  And so, because of that, in practice,
[2261.32s -> 2266.92s]  where things tended to sort of max out was five.
[2266.92s -> 2270.28s]  And occasionally, people use six grams and seven grams.
[2270.28s -> 2274.44s]  But most of the time, between the sort of sparseness and
[2274.44s -> 2279.52s]  the cost of storage, five grams was the largest thing people dealt with.
[2279.52s -> 2287.08s]  So a famous resource from back in the 2000s decade that Google released
[2287.16s -> 2293.28s]  was Google Ngrams, which was built on a trillion word web corpus and
[2293.28s -> 2295.36s]  had counts of n-grams.
[2295.36s -> 2298.92s]  And it gave counts of n-grams up to n equals five, and
[2298.92s -> 2300.04s]  that is where they stopped.
[2301.72s -> 2304.60s]  Okay, well, we sort of said the storage problem.
[2304.60s -> 2308.68s]  The storage problem is, well, to do this, you need to store these counts.
[2308.68s -> 2312.48s]  The number of counts is going up exponentially in the amount of
[2312.48s -> 2313.64s]  context size.
[2314.20s -> 2319.88s]  Okay, but what's good about Ngram language models?
[2319.88s -> 2322.20s]  They're really easy to build.
[2322.20s -> 2326.64s]  You can build one yourself in a few minutes when you wanna have
[2326.64s -> 2328.16s]  a bit of fun on the weekend.
[2329.16s -> 2334.32s]  All you have to do is start sort of storing these counts for
[2334.32s -> 2337.52s]  n-grams, and you can use them to predict things.
[2337.52s -> 2341.12s]  So at least if you do it over a small corpus,
[2341.16s -> 2346.96s]  like a couple of million words of text, you can build an n-gram language
[2346.96s -> 2351.12s]  model in seconds on your laptop, or you have to write the software.
[2351.12s -> 2353.32s]  Okay, a few minutes to write the software, but
[2353.32s -> 2357.68s]  building the model takes seconds because there's no training in your network.
[2357.68s -> 2361.92s]  All you do is count how often n-grams occur.
[2361.92s -> 2364.24s]  And so once you've done that,
[2364.24s -> 2368.96s]  you can then run an n-gram language model to generate text.
[2368.96s -> 2372.48s]  We could do text generation before chat GPT, right?
[2372.48s -> 2377.60s]  So if I have a trigram language model, I can start off with some words,
[2377.60s -> 2383.16s]  today the, and I could look at my stored n-grams and
[2383.16s -> 2386.68s]  get a probability distribution over next words.
[2386.68s -> 2389.12s]  And here they are.
[2389.12s -> 2396.04s]  Note the strong patterning of these probabilities,
[2396.04s -> 2398.96s]  because remember they're all derived from counts, right,
[2398.96s -> 2400.60s]  that are being normalized.
[2400.60s -> 2403.68s]  So really, these are words that occurred once.
[2403.68s -> 2405.84s]  These are words that occurred twice.
[2405.84s -> 2409.04s]  These are words that occurred four times in this context, right?
[2409.04s -> 2413.12s]  So they're sort of in some sense crude when you look at them more carefully.
[2413.12s -> 2419.32s]  But so what we could do is then at this point, we roll a die and
[2419.32s -> 2424.00s]  get a random number from 0 to 1, and we can use that sample from this
[2424.00s -> 2428.40s]  distribution, sorry, yeah.
[2430.52s -> 2435.52s]  So we sample from this distribution, and so that if we sort of
[2436.68s -> 2443.84s]  generate sort of as our random number something like 0.35,
[2443.84s -> 2448.56s]  if we go down from the top, we'd say, okay, we've sampled the word price.
[2448.56s -> 2452.80s]  Today the price, and then we repeat over, we condition on that,
[2452.80s -> 2456.12s]  we probability distribution of the next word.
[2456.12s -> 2460.52s]  We generate a random number and use it to sample from the distribution.
[2460.52s -> 2465.24s]  We say generate 0.2, and so we choose of.
[2465.24s -> 2467.04s]  We now condition on that.
[2467.04s -> 2469.56s]  We get a probability distribution.
[2469.56s -> 2473.64s]  We generate a random number which is 0.5 or something, and so
[2473.64s -> 2478.32s]  we get gold coming out, and we can say today the price of gold.
[2478.32s -> 2482.16s]  And we can keep on doing this and generate some text.
[2482.16s -> 2487.32s]  And so here's some text generated from 2 million words
[2487.32s -> 2492.12s]  of training data using a trigram language model.
[2492.12s -> 2496.44s]  Today the price of gold per ton while production of shoe lasts and
[2496.44s -> 2500.00s]  shoe industry, the bank intervened just after it considered and
[2500.00s -> 2504.60s]  rejected an IMF demand to rebuild depleted European stocks.
[2504.60s -> 2509.36s]  September 3rd in primary 76 cents a share.
[2509.36s -> 2513.76s]  Now, okay, that text isn't great.
[2513.76s -> 2519.48s]  But I actually want people to be in a positive mood today.
[2519.48s -> 2524.32s]  And actually, it's not so bad, right?
[2524.32s -> 2527.04s]  It's sort of surprisingly grammatical.
[2527.04s -> 2530.80s]  I mean, in particular, like I lower cased everything, so
[2530.80s -> 2536.00s]  this is the IMF that should be capitalized of the International Monetary Fund, right?
[2536.08s -> 2539.88s]  There are big pieces of this that even make sense, right?
[2539.88s -> 2545.16s]  The bank intervened just after it considered and rejected an IMF demand.
[2545.16s -> 2550.40s]  That's pretty much making sense as a piece of text, right?
[2550.40s -> 2554.16s]  So it's mostly grammatical.
[2554.16s -> 2556.60s]  It looks like English text.
[2556.60s -> 2558.76s]  I mean, it makes no sense, right?
[2558.76s -> 2560.44s]  It's sort of really incoherent.
[2560.44s -> 2562.48s]  So there's work to do.
[2562.48s -> 2569.04s]  But what was already you could see, there's even these simple n-gram models.
[2569.04s -> 2574.44s]  You could, from a very low level, you could kind of approach
[2574.44s -> 2579.80s]  what text and human language worked like in from below.
[2579.80s -> 2583.32s]  And I could easily make this better even with the n-gram language model,
[2583.32s -> 2585.56s]  because rather than two million words of text,
[2585.56s -> 2588.52s]  if I trained on ten million words of text, it'd be better.
[2588.52s -> 2592.12s]  If I then, rather than a trigram model, could go to a four gram model,
[2593.00s -> 2596.88s]  it would get better, and you'd sort of start getting better and
[2596.88s -> 2599.76s]  better approximations of text.
[2599.76s -> 2607.12s]  And so this is essentially what people did until about 2012.
[2607.12s -> 2613.40s]  And really the same story that people tell today,
[2613.40s -> 2618.36s]  that scale will solve everything, is exactly the same story that people used
[2618.36s -> 2623.64s]  to tell in the early 2010s with these n-gram language models.
[2623.64s -> 2627.32s]  If you weren't getting a good enough results with your ten million words of
[2627.32s -> 2632.04s]  text in a trigram language model, the answer was that if you had
[2632.04s -> 2636.00s]  a hundred million words of text in a four gram language model, you'd do better.
[2636.00s -> 2640.04s]  And then if you had a trillion words of text in a five gram language model,
[2640.04s -> 2641.16s]  you'd do better.
[2641.16s -> 2644.56s]  And gee, wouldn't it be good if we could collect ten trillion words of text,
[2644.56s -> 2648.08s]  so we could train an even better n-gram language model.
[2648.08s -> 2649.48s]  Same strategy.
[2649.48s -> 2654.36s]  But it turns out that sometimes you can do better with better models,
[2654.36s -> 2656.84s]  as well as simply scale.
[2656.84s -> 2659.40s]  And so things got reinvented and
[2659.40s -> 2663.36s]  started again with building neural language models.
[2664.48s -> 2667.76s]  So how can we build a neural language model?
[2669.12s -> 2674.12s]  So we've got the same task of having a sequence of words, and
[2674.12s -> 2678.84s]  we want to put a probability estimate over what word comes next.
[2678.84s -> 2682.44s]  And so the simplest way you could do that,
[2682.44s -> 2686.68s]  which you'll hopefully all have thought of because it connects what we did
[2686.68s -> 2688.44s]  in the earlier classes.
[2688.44s -> 2692.60s]  Look, we already had this idea that we could have
[2692.60s -> 2697.36s]  represented context by the concatenation of some word vectors.
[2697.36s -> 2702.08s]  And we could put that into a neural network, and
[2702.08s -> 2704.76s]  we could use that to predict something.
[2704.76s -> 2707.96s]  And in the example I did in the last couple of classes,
[2707.96s -> 2713.24s]  what we used it to predict was, is the center word a location or
[2713.24s -> 2716.64s]  not a location, just a binary choice?
[2716.64s -> 2718.60s]  But that's not the only thing we could predict.
[2718.60s -> 2721.12s]  We could have predicted lots of things with this neural network.
[2721.12s -> 2725.68s]  We could have predicted whether the piece of text was positive or negative.
[2725.68s -> 2729.48s]  We could have predicted whether it was written in English or Japanese.
[2729.48s -> 2731.32s]  We could predict lots of things.
[2731.32s -> 2735.48s]  So one thing we could choose to predict is we could choose to predict
[2735.48s -> 2738.96s]  what word is gonna come next after this window of text.
[2738.96s -> 2742.48s]  We'd have a model just like this one, apart from up the top,
[2742.48s -> 2746.80s]  instead of doing this binary classification, we'd do a many,
[2746.80s -> 2750.84s]  many way classification over what is the next word
[2750.84s -> 2753.88s]  that is going to appear in the piece of text.
[2753.88s -> 2757.80s]  And that would then give us a neural language model.
[2757.84s -> 2762.04s]  In particular, it'd give us a fixed window neural language model.
[2763.04s -> 2766.96s]  So that we'd do the same Markov assumption trick
[2766.96s -> 2770.18s]  of throwing away the further back context.
[2770.18s -> 2777.60s]  And so for the fixed window, we'll use word embeddings,
[2777.60s -> 2779.48s]  which you can concatenate.
[2779.48s -> 2781.92s]  We'll put it through a hidden layer, and
[2781.92s -> 2784.28s]  then we'll take the output of that hidden layer,
[2784.28s -> 2789.24s]  multiply it by another layer, say, and
[2789.24s -> 2793.68s]  then put that through a softmax and get an output distribution.
[2793.68s -> 2799.12s]  And so this gives us sort of a fixed window neural language model.
[2799.12s -> 2804.68s]  And apart from the fact that we're now doing a classification over many,
[2804.68s -> 2809.68s]  many, many classes, this is exactly like what we did last week.
[2809.68s -> 2812.56s]  So it should look kind of familiar.
[2812.56s -> 2815.84s]  It's also kind of like what you're doing for assignment two.
[2815.84s -> 2821.00s]  And so this is essentially the first kind of neural language
[2821.00s -> 2824.32s]  model that was proposed.
[2824.32s -> 2829.84s]  So in particular, Yoshua Bengio, really sort of right at the beginning
[2829.84s -> 2833.48s]  of the 21st century, suggested that you could do this.
[2833.48s -> 2836.24s]  That rather than using an n-gram language model,
[2836.24s -> 2840.38s]  you could use a fixed window neural language model.
[2840.42s -> 2844.06s]  And even at that point, he and
[2844.06s -> 2849.06s]  colleagues were able to get some positive results from this model.
[2849.06s -> 2852.74s]  But at the time, it wasn't widely noticed.
[2852.74s -> 2854.90s]  It didn't really take off that much.
[2854.90s -> 2857.66s]  And it was sort of for a combination of reasons.
[2857.66s -> 2860.14s]  When it was only a fixed window,
[2860.14s -> 2864.38s]  it was sort of not that different to n-grams in some sense.
[2864.38s -> 2868.34s]  And although the neural network could give better generalization,
[2868.34s -> 2871.18s]  it could be argued rather than using counts.
[2871.18s -> 2878.22s]  I mean, in practice, neural nets were still hard to run without GPUs.
[2878.22s -> 2882.72s]  And people felt, and I think in general this was the case,
[2882.72s -> 2886.98s]  that you could get more oomph by doing the scale story.
[2886.98s -> 2893.46s]  And collecting your n-gram counts on hundreds of billions of words of text,
[2893.46s -> 2896.46s]  rather than trying to make a neural network out of it.
[2896.46s -> 2900.86s]  And so it didn't really sort of especially take off at that time.
[2900.86s -> 2903.54s]  But in principle, it seemed a nice thing.
[2903.54s -> 2906.46s]  It got rid of the sparsity problem.
[2906.46s -> 2908.54s]  It got rid of the storage costs.
[2908.54s -> 2911.90s]  You no longer have to store all observed n-grams.
[2911.90s -> 2915.58s]  You just have to store the parameters of your neural network.
[2915.58s -> 2919.54s]  But it didn't solve all the problems that we'd like to solve.
[2919.54s -> 2923.98s]  So in particular, we still have this problem of the Markov assumption that
[2923.98s -> 2929.46s]  we're just using a small fixed context beforehand to predict from.
[2931.10s -> 2935.86s]  And there are some disadvantages to enlarging that window.
[2935.86s -> 2938.78s]  And there's no fixed window that's ever big enough.
[2940.18s -> 2945.18s]  There's another thing that if you look technically at this
[2945.18s -> 2949.62s]  model that might sort of make you suspicious of it.
[2949.62s -> 2955.18s]  Which is, when we have words in different positions,
[2955.18s -> 2959.86s]  that those words in different positions will be treated by
[2959.86s -> 2964.46s]  completely different subparts of this matrix W.
[2964.46s -> 2968.38s]  So you might think that, okay, for
[2968.38s -> 2976.34s]  predicting that books comes next, the fact that this is a student is important.
[2976.34s -> 2981.90s]  But it doesn't matter so much exactly where the word student occurs, right?
[2981.90s -> 2987.58s]  The context could have been the students slowly open there.
[2987.58s -> 2989.70s]  And it's still the same students.
[2989.70s -> 2993.98s]  We've just got a bit of different linguistic structure where this W matrix
[2993.98s -> 2998.06s]  would be using completely separate parameters to be learning stuff about
[2998.06s -> 3001.18s]  student here versus student in this position.
[3001.18s -> 3003.70s]  So that seems kind of inefficient and wrong.
[3004.10s -> 3009.78s]  And so that suggested that we kind of need a different kind of
[3009.78s -> 3015.58s]  neural architecture that can process any length of import and
[3015.58s -> 3021.02s]  can use the same parameters to say, hey, I saw the word student.
[3021.02s -> 3023.70s]  That's evidence that things like books, exams,
[3023.70s -> 3027.74s]  homework will be turning up regardless of where it occurs.
[3027.74s -> 3032.38s]  And so that then led to exploration of this different neural
[3032.42s -> 3036.66s]  network architecture called recurrent neural networks,
[3036.66s -> 3038.58s]  which is what I'll go on to next.
[3038.58s -> 3043.78s]  But before I do, is everyone basically okay with what a language model is?
[3043.78s -> 3045.58s]  Yeah, no questions?
[3048.58s -> 3051.82s]  Okay, recurrent neural networks.
[3055.86s -> 3061.50s]  So recurrent neural networks is a different family
[3061.58s -> 3062.54s]  of neural networks.
[3062.54s -> 3068.14s]  So effectively in this class, we see several neural network architectures.
[3069.70s -> 3074.34s]  So in some sense, the first architecture we saw was Word2Vec.
[3074.34s -> 3081.22s]  It's a sort of a very simple encoder-decoder architecture.
[3081.22s -> 3086.98s]  The second family we saw was feed-forward networks or
[3086.98s -> 3090.82s]  fully connected layer classic neural networks.
[3090.82s -> 3094.18s]  And the third family we're gonna see is recurrent neural networks,
[3094.18s -> 3095.78s]  which have different kinds.
[3095.78s -> 3099.54s]  And then we'll go on and go on to transformer models.
[3099.54s -> 3105.70s]  Okay, so the idea of a recurrent neural network is that you've got
[3105.70s -> 3110.38s]  one set of weights that are going to be applied through
[3110.38s -> 3116.22s]  successive moments in time, successive positions in the text.
[3116.22s -> 3121.46s]  And as you do that, you're going to update the parameters as you go.
[3121.46s -> 3124.46s]  We'll go through this in quite a bit of detail.
[3124.46s -> 3126.78s]  But here's the idea of it.
[3126.78s -> 3131.38s]  So we've got the students open there, and we want to predict with that.
[3131.38s -> 3136.42s]  And the way that we're going to do it, okay,
[3136.42s -> 3138.62s]  I've still got four words in my example, so
[3138.62s -> 3141.18s]  I can put stuff down the left side of the slide.
[3141.18s -> 3145.38s]  But there could have been 24 words with recurrent neural networks,
[3145.38s -> 3148.46s]  because they can deal with any length of context.
[3148.46s -> 3153.62s]  Okay, so as before, our words start off as just words or
[3153.62s -> 3159.58s]  one-hot vectors, and we can look up their word embeddings just like before.
[3159.58s -> 3163.74s]  Okay, but now to compute probabilities for
[3163.74s -> 3167.22s]  the next word, we're gonna do something different.
[3167.22s -> 3171.34s]  So our hidden layer is going to be recurrent.
[3171.34s -> 3176.42s]  And by recurrent, it means we're going to sort of change a hidden
[3176.42s -> 3181.18s]  state at each time step as we proceed through the text from left to right.
[3182.30s -> 3187.66s]  So we're gonna start off with an h0, which is the initial hidden state,
[3187.66s -> 3190.82s]  which can actually just be all zeros.
[3190.82s -> 3196.30s]  And then at each time step, what we're gonna do is we're going to
[3196.30s -> 3201.66s]  multiply the previous hidden state by a weight matrix.
[3201.66s -> 3207.06s]  We're going to take a word embedding and multiply it by a weight matrix.
[3207.06s -> 3210.50s]  And then we're going to sum the results of those two things, and
[3210.50s -> 3213.10s]  that's gonna give us a new hidden state.
[3213.10s -> 3216.98s]  So that hidden state will then sort of store
[3216.98s -> 3220.62s]  a memory of everything that's been seen so far.
[3220.62s -> 3224.14s]  So we'll do that, and then we'll continue along.
[3224.14s -> 3230.66s]  So we'll multiply the next word vector by the same weight matrix We.
[3230.66s -> 3237.14s]  We multiply the previous hidden state by the same weight matrix Wh.
[3237.14s -> 3240.90s]  And we add them together and get a new representation.
[3243.10s -> 3245.82s]  I've only sort of said this bit, so I've left out a bit.
[3245.82s -> 3248.26s]  Commonly, there are two other things you're doing.
[3248.26s -> 3252.78s]  You're adding on a bias term, cuz we usually separate out a bias term.
[3252.90s -> 3255.82s]  And you're putting things through a non-linearity, so
[3255.82s -> 3257.62s]  I should make sure I mention that.
[3257.62s -> 3261.66s]  And for recurrent neural networks, most commonly,
[3261.66s -> 3264.70s]  this non-linearity has actually been the tanh function.
[3264.70s -> 3267.78s]  So it's sort of balanced on the positive and negative side.
[3267.78s -> 3271.70s]  And so you keep on doing that through each step.
[3271.70s -> 3277.50s]  And so the idea is once we've gotten to here, this h4's hidden state
[3277.50s -> 3281.46s]  is a hidden state that in some sense has read the text up until now.
[3281.46s -> 3284.42s]  It's seen all of the students open there.
[3284.42s -> 3288.94s]  And if the word students occurred in any of these positions,
[3288.94s -> 3293.22s]  it will have been multiplied by the same We matrix and
[3293.22s -> 3295.06s]  added into the hidden state.
[3295.06s -> 3296.94s]  So it's kind of got a cleaner,
[3296.94s -> 3301.66s]  low parameter way of incorporating in the information that's seen.
[3301.66s -> 3304.98s]  So now I want to predict the next word.
[3304.98s -> 3308.58s]  And to predict the next word, I'm then going to do,
[3308.58s -> 3314.10s]  based on the final hidden state, the same kind of thing I did before.
[3314.10s -> 3317.94s]  So I'm going to multiply that hidden state by matrix and
[3317.94s -> 3322.26s]  add another bias and stick that through a softmax and
[3322.26s -> 3326.34s]  use that to sample from that softmax.
[3326.34s -> 3328.54s]  Well, the softmax will give me a language model,
[3328.54s -> 3331.54s]  a probability over all next words, and
[3331.54s -> 3333.98s]  I can sample from it to generate the next word.
[3334.02s -> 3338.02s]  That make sense?
[3339.74s -> 3343.10s]  Okay, recurrent neural networks.
[3349.22s -> 3352.90s]  Okay, so for recurrent neural networks,
[3352.90s -> 3357.06s]  we can now process any length of preceding context, and
[3357.06s -> 3360.74s]  we'll just put more and more stuff in our hidden state.
[3361.34s -> 3368.10s]  So our computation is using information from many steps back.
[3370.10s -> 3375.06s]  Our model size doesn't increase for having a long context, right?
[3375.06s -> 3379.30s]  We have to do more computation for a long context, but
[3379.30s -> 3383.94s]  our representation of that long context just remains this fixed size
[3383.94s -> 3389.66s]  in vector h of whatever dimension it is, so there's no exponential blowout anymore.
[3390.82s -> 3393.42s]  There's the same way it's applied in every time step, so
[3393.42s -> 3396.86s]  there's a symmetry in how inputs are processed.
[3396.86s -> 3398.78s]  There are some catches.
[3399.86s -> 3405.34s]  The biggest catch in practice is that recurrent computation is slow.
[3405.34s -> 3410.42s]  So for the feed forward layer, we just had our input vector,
[3410.42s -> 3414.90s]  we multiply it by matrix, we multiply it by matrix however many times, and
[3414.90s -> 3416.46s]  then at the end we're done.
[3416.46s -> 3420.66s]  Whereas here, we've sort of stuck with this sequentiality that you have
[3420.66s -> 3424.66s]  to be doing one hidden vector at a time.
[3424.66s -> 3428.34s]  In fact, this is going against what I said at the beginning of class,
[3428.34s -> 3431.46s]  cuz essentially here you're doing a for loop.
[3431.46s -> 3434.66s]  You're going through for time equals one to t, and
[3434.66s -> 3438.26s]  then you're generating a term each hidden vector.
[3438.26s -> 3442.54s]  And that's one of the big problems with RNNs that have led them to fall out
[3442.54s -> 3443.14s]  of favor.
[3445.14s -> 3450.22s]  There's another problem that we'll look at more is
[3450.94s -> 3452.94s]  that in theory this is perfect.
[3452.94s -> 3458.18s]  You're just incorporating all of the past context in your hidden vector.
[3458.18s -> 3463.78s]  In practice, it tends not to work perfectly because although stuff
[3463.78s -> 3469.42s]  you saw back here is in some sense still alive in the hidden vector
[3469.42s -> 3475.34s]  as you come across here, that your memory of it gets more and more distant.
[3475.34s -> 3479.82s]  And it's the words that you saw recently that dominate the hidden state.
[3479.82s -> 3483.58s]  Now in some sense that's right because the recent stuff is the most important
[3483.58s -> 3485.78s]  stuff that's freshest in your mind.
[3485.78s -> 3488.18s]  It's the same with human beings.
[3488.18s -> 3491.46s]  They tend to forget stuff from further back as well.
[3491.46s -> 3495.90s]  But RNNs, especially in the simple form that I've just explained,
[3495.90s -> 3500.46s]  forget stuff from further back rather too quickly.
[3500.46s -> 3505.90s]  And we'll come back to that again in Thursday's class.
[3506.90s -> 3511.14s]  Okay, so for training an RNN language model,
[3511.14s -> 3517.06s]  the starting off point is we get a big corpus of text again.
[3517.06s -> 3521.10s]  And then we're gonna compute for
[3521.10s -> 3527.98s]  each time step a prediction of the probability of next words.
[3527.98s -> 3531.42s]  And then there's gonna be an actual next word.
[3531.42s -> 3536.74s]  And we're gonna use that as the basis of our loss.
[3536.74s -> 3541.18s]  So our loss function is the cross entropy between the predicted
[3541.18s -> 3545.70s]  probability and what the actual next word that we saw is.
[3545.70s -> 3548.50s]  Which again, as in the example I showed before,
[3548.50s -> 3553.66s]  is just the negative log likelihood of the actual next word.
[3553.66s -> 3558.58s]  Ideally, you'd like to predict the actual next word with probability one,
[3558.58s -> 3564.66s]  which means the negative log of one would be zero, and there'd be no loss.
[3564.66s -> 3567.86s]  But in practice, if you give it an estimate of 0.5,
[3567.86s -> 3571.14s]  there's only a little bit of loss, and so on.
[3571.14s -> 3575.86s]  And so to get our overall objective function,
[3575.86s -> 3578.86s]  we work out the average loss,
[3578.86s -> 3583.86s]  the average negative log likelihood of predicting each word in turn.
[3583.86s -> 3586.10s]  So showing that as pictures,
[3586.10s -> 3589.62s]  if our corpus is the students open their exams,
[3589.62s -> 3597.18s]  we're first of all gonna be trying to predict what comes after the.
[3597.18s -> 3603.34s]  And we will predict some word with different probabilities.
[3603.34s -> 3606.46s]  And then we'll say, the actual next word is students.
[3606.46s -> 3610.66s]  Okay, you gave that a probability of 0.05, say,
[3610.66s -> 3613.10s]  cuz all we know was the first word was the.
[3613.10s -> 3615.58s]  Okay, there's a loss for that.
[3616.10s -> 3619.30s]  The negative log prob given to students.
[3619.30s -> 3625.74s]  We then go on and generate the probability estimate over the next words.
[3625.74s -> 3628.34s]  And then we say, well, the actual word is opened.
[3628.34s -> 3631.30s]  What probability estimate did you give to that?
[3631.30s -> 3633.82s]  We get a negative probability loss.
[3633.82s -> 3636.06s]  Keep on running this along.
[3636.06s -> 3642.06s]  And then we sum all of those losses, and we average them per word.
[3642.06s -> 3645.50s]  And that's our sort of average per word loss.
[3645.50s -> 3649.90s]  And we want to make that as small as possible.
[3649.90s -> 3652.54s]  And so that's our training mechanism.
[3652.54s -> 3658.54s]  And it's important to notice that for
[3658.54s -> 3661.98s]  generating this loss, we're not just doing free generation.
[3661.98s -> 3666.02s]  We're not just saying to the model, go off and generate a sentence.
[3666.02s -> 3671.34s]  What we're actually doing is that each step we're effectively saying, okay,
[3671.38s -> 3674.02s]  the prefix is the student's open.
[3674.02s -> 3678.70s]  What probability distribution do you put on next words after that?
[3678.70s -> 3681.42s]  Generate it with our current neural network.
[3681.42s -> 3684.82s]  And then say ask for the actual next word.
[3684.82s -> 3687.34s]  What probability estimate did you give to there?
[3687.34s -> 3688.90s]  And that's our loss.
[3688.90s -> 3693.78s]  But then what we do is stick there into our current neural network,
[3693.78s -> 3695.54s]  the right answer.
[3695.54s -> 3698.14s]  So we always go back to the right answer.
[3698.18s -> 3702.38s]  Generate probability distribution for next words.
[3702.38s -> 3707.54s]  And then ask, okay, what probability did you give to the actual next word exams?
[3707.54s -> 3710.34s]  And then again, we use the actual next word.
[3710.34s -> 3713.38s]  So we do one step of generation,
[3713.38s -> 3719.26s]  then we pull it back to what was actually generated, what was actually in the text.
[3719.26s -> 3723.90s]  And then we ask it for guesses over the next word and repeat forever.
[3723.90s -> 3727.06s]  And so the fact that we don't do free generation, but
[3727.10s -> 3733.10s]  we pull it back to the actual piece of text each time, makes things simple.
[3733.10s -> 3739.78s]  Because we sort of know what the actual author used for the next word.
[3739.78s -> 3742.86s]  And that process is called teacher forcing.
[3742.86s -> 3746.46s]  And so the most common way to train
[3746.46s -> 3750.18s]  language models is using this kind of teacher forcing method.
[3750.18s -> 3755.50s]  I mean, it's not perfect in all respects cuz we're not actually exploring
[3755.50s -> 3758.38s]  different things the model might want to generate on its own and
[3758.38s -> 3760.18s]  seeing what comes after them.
[3760.18s -> 3764.50s]  We're only doing the tell me the next word from some human generated
[3764.50s -> 3765.34s]  piece of text.
[3772.06s -> 3776.46s]  Okay, so that's how we get losses.
[3776.46s -> 3781.66s]  And then after that, we want to, as before,
[3781.66s -> 3786.02s]  use these losses to update the parameters of a neural network.
[3787.46s -> 3790.82s]  Okay, and how do we do that?
[3792.10s -> 3796.98s]  Well, in principle, we just have all of the texts that we've collected,
[3796.98s -> 3801.10s]  which you could think of as just a really long sequence of, okay,
[3801.10s -> 3804.46s]  we've got a billion words of text, here it is, right?
[3804.46s -> 3809.74s]  So in theory, you could just run your recurrent neural network over
[3809.74s -> 3813.98s]  your billion words of text, updating the context as you go.
[3815.06s -> 3820.70s]  But that would make it very difficult to train a model
[3820.70s -> 3824.90s]  because you'd be accumulating these losses for a billion steps and
[3824.90s -> 3826.26s]  you'd have to store them.
[3827.42s -> 3831.30s]  And then you'd have to store hidden states so
[3831.30s -> 3834.54s]  you could update parameters, and it just wouldn't work.
[3834.54s -> 3840.30s]  So what we actually do is we cut our training data into segments of
[3840.30s -> 3845.02s]  a reasonable length, and then we're going to sort of run
[3845.02s -> 3848.86s]  our recurrent neural network on those segments.
[3848.86s -> 3852.38s]  And then we're going to compute a loss for each segment.
[3852.38s -> 3857.54s]  And then we're going to update the parameters of the recurrent
[3857.54s -> 3861.26s]  neural network based on the losses that we found for that segment.
[3862.26s -> 3867.30s]  I describe it here as the segments being sentences of documents,
[3867.30s -> 3870.90s]  which seems a linguistically nice thing.
[3870.90s -> 3874.42s]  It turns out that in recent practice,
[3874.42s -> 3878.98s]  when you're wanting to scale most efficiently on GPUs,
[3878.98s -> 3882.14s]  people don't bother with those linguistic niceties.
[3882.14s -> 3887.66s]  They just say, a segment is 100 words, just cut every 100 words.
[3887.66s -> 3891.22s]  And the reason why that's really convenient is you can then create
[3891.22s -> 3895.62s]  a batch of segments, all of which are 100 words long, and
[3895.62s -> 3902.78s]  stick those in a matrix and do vectorized training more efficiently.
[3902.78s -> 3904.30s]  And things go great for you.
[3905.30s -> 3908.62s]  Okay, but there's still a few more things that we need to know
[3908.62s -> 3910.30s]  to get things to work great for you.
[3910.30s -> 3914.94s]  I'll try and get a bit more through this before today ends.
[3914.94s -> 3919.46s]  So we sort of need to know about how to work out
[3919.46s -> 3925.34s]  the derivative of our loss with respect to
[3925.34s -> 3930.58s]  the parameters of our recurrent neural network.
[3930.58s -> 3935.94s]  And the interesting case here is these WH parameters
[3935.94s -> 3941.70s]  are sort of being used everywhere through the neural network at each stage,
[3941.70s -> 3943.26s]  as are the WE ones.
[3943.26s -> 3946.78s]  So they appear at many places in the network.
[3946.78s -> 3952.34s]  So how do we work out the partial derivatives of the loss with
[3952.34s -> 3955.54s]  respect to the repeated weight matrices?
[3955.54s -> 3959.26s]  And the answer to that is, it's really simple.
[3960.58s -> 3966.18s]  You can just sort of pretend that those WHs in each position
[3966.18s -> 3972.82s]  are different and work out the partials with respect to them at one position.
[3972.82s -> 3976.42s]  And then to get the partials with respect to WH,
[3976.42s -> 3980.18s]  you just sum whatever you found in the different positions.
[3981.50s -> 3987.18s]  And so that is sort of, okay, the gradient with respect to
[3987.18s -> 3992.78s]  repeated weight is the sum of the gradient with respect to each time it appears.
[3992.78s -> 3995.54s]  And the reason why that is,
[3995.54s -> 4000.70s]  it sort of follows what I talked about in lecture three.
[4000.70s -> 4005.18s]  That we talk, or you can also think about it
[4005.18s -> 4011.22s]  in terms of what you might remember from multivariable chain rules.
[4011.22s -> 4014.22s]  But the way I introduced in lecture three
[4014.22s -> 4017.90s]  is the gradient sum at outward branches.
[4017.90s -> 4022.94s]  And so what you can think about it in a case like this is that you've
[4022.94s -> 4030.06s]  got a WH matrix which is being copied by identity to WH1,
[4030.06s -> 4035.50s]  WH2, WH3, WH4, etc., at each time step.
[4035.50s -> 4039.50s]  And so since those are identity copies,
[4039.50s -> 4046.30s]  they have partial derivative with respect to each other of one.
[4046.30s -> 4053.42s]  And so then we apply the multivariable chain rule to these copies.
[4053.42s -> 4057.78s]  And so we've then got an outward branching node, and
[4057.78s -> 4064.62s]  you're just summing the gradients to get the total gradient of each time for the matrix.
[4069.62s -> 4077.90s]  Okay, yeah, I mean, there's one other trick that's perhaps worth knowing.
[4077.90s -> 4081.70s]  I mean, if you've got sort of segments that are 100 long,
[4081.70s -> 4084.94s]  a common speed up is to say,
[4084.94s -> 4089.46s]  maybe we don't actually have to run back propagation for 100 time steps.
[4089.46s -> 4093.14s]  Maybe we could just run it for 20 time steps and stop,
[4093.14s -> 4097.10s]  which is referred to as truncated back propagation through time.
[4097.10s -> 4100.22s]  I mean, in practice, that tends to be sufficient.
[4100.22s -> 4103.70s]  Note in particular, you're still on the forward pass,
[4103.70s -> 4107.94s]  updating your hidden state using your full context.
[4107.94s -> 4113.54s]  But in the back propagation, you're just sort of cutting it short to speed up training.
[4115.50s -> 4120.82s]  Okay, so just as I did before with an n-gram language model,
[4120.82s -> 4126.02s]  we can use RNN language model to generate text.
[4126.02s -> 4131.14s]  And it's pretty much the same idea, except now we're sort of,
[4131.14s -> 4137.10s]  rather than just using counts of n-grams, we're using the hidden state of our neural network
[4137.10s -> 4143.70s]  to give us the input to a probability distribution that we can then sample from.
[4143.74s -> 4147.02s]  So I can start with the initial hidden state.
[4147.02s -> 4150.50s]  I can use the start of sentence symbol.
[4150.50s -> 4155.74s]  I mean, the example I had before, I started immediately with the,
[4155.74s -> 4158.34s]  hoping that I was less confusing the first time.
[4158.34s -> 4163.14s]  But what you should have asked is, wait a minute, where did the the come from?
[4163.14s -> 4167.98s]  So normally what we actually do is use a special start
[4167.98s -> 4172.98s]  of sequence symbol like this angle bracketed s.
[4172.98s -> 4178.06s]  And so we sort of feed it in as a pseudo word, which has a word embedding.
[4178.06s -> 4183.54s]  And then we, based on this, will be generating first words of the text.
[4183.54s -> 4189.34s]  So we end up with some representation from which we can sample and
[4189.34s -> 4191.58s]  get the first word.
[4191.58s -> 4194.34s]  So now we don't have any actual text.
[4194.34s -> 4199.78s]  So what we're gonna do is take that generated word that we generated and
[4199.78s -> 4202.90s]  copy it down as the next input.
[4202.90s -> 4206.74s]  And then we're going to run a next stage of neural network.
[4206.74s -> 4211.82s]  Sample from the probability distribution, the next word, favorite.
[4211.82s -> 4217.38s]  Copy it down as the next word of the input and keep on generating.
[4217.38s -> 4222.34s]  And so this is referred to as a roll out that you're kind of continuing to roll
[4222.34s -> 4226.54s]  the dice and generate forward and generate a piece of text.
[4226.54s -> 4232.22s]  And so, and normally you want to stop at some point.
[4232.22s -> 4237.26s]  And the way we can stop at some point is we can have a second special symbol.
[4237.26s -> 4244.06s]  The angle brackets slash s, which says end of your sequence.
[4244.06s -> 4249.50s]  So we can generate an end of sequence symbol and then we can stop.
[4249.50s -> 4253.86s]  And so using this, we can sort of generate pieces of text.
[4253.86s -> 4258.26s]  And essentially, this is exactly what's happening if you use something like
[4258.26s -> 4259.94s]  ChatGPT, right?
[4259.94s -> 4264.58s]  That the model is a more complicated model that we haven't yet gotten to.
[4264.58s -> 4269.30s]  But it's generating the response to you by doing this kind of process
[4269.30s -> 4273.86s]  of generating a word at the time, treating it as an input, and
[4273.86s -> 4277.74s]  generating the next word, and generating this sort of roll out.
[4277.74s -> 4280.50s]  And that's why, and it's done probabilistically.
[4280.50s -> 4285.34s]  So if you do it multiple times, you can get different answers.
[4285.42s -> 4289.46s]  We haven't yet gone to ChatGPT, but we can have a little bit of fun.
[4289.46s -> 4294.86s]  So you can take this simple recurrent neural network that we've just built here.
[4294.86s -> 4299.46s]  And you can train it on any piece of text and get it to generate stuff.
[4300.62s -> 4305.06s]  So for example, I can train it on Barack Obama's speeches.
[4305.06s -> 4306.86s]  So that's a small corpus, right?
[4306.86s -> 4308.98s]  He didn't talk that much, right?
[4308.98s -> 4311.66s]  I've only got a few hundred thousand words of text.
[4311.66s -> 4313.26s]  It's not a huge corpus.
[4313.34s -> 4316.14s]  I'll just show you this, and then I can answer the question.
[4316.14s -> 4319.58s]  But I can generate from it, and I get something like,
[4319.58s -> 4323.54s]  the United States will step up to the cost of a new challenges of
[4323.54s -> 4327.62s]  the American people that will share the fact that we created the problem.
[4327.62s -> 4330.82s]  They were attacked, and so that they have to say that all the tasks of
[4330.82s -> 4333.98s]  the final days of war, that I will not be able to get this done.
[4335.62s -> 4339.78s]  Yeah, well, maybe that's slightly better than my n-gram language model.
[4339.78s -> 4342.94s]  Still not perfect, you might say, but somewhat better.
[4343.02s -> 4344.02s]  Maybe?
[4344.02s -> 4345.02s]  Did you have a question?
[4345.02s -> 4346.02s]  Yeah.
[4346.02s -> 4351.02s]  Since we're creating the model, I've truncated some of the corpus,
[4351.02s -> 4356.02s]  does that impose some kind of limitation on how much we can produce
[4356.02s -> 4362.02s]  and still have some coherency in the meaning of the sentence for creating or?
[4364.02s -> 4365.02s]  So yeah.
[4365.02s -> 4370.02s]  So I suggested we're going to chunk the text into 100 word units.
[4370.10s -> 4375.10s]  So that's the limit of the amount of prior context that we're going to use.
[4375.10s -> 4377.10s]  So I mean, that's a fair amount, 100 words.
[4377.10s -> 4380.10s]  That's typically several sentences.
[4380.10s -> 4384.10s]  But to the extent that you wanted to know even more about the
[4384.10s -> 4387.10s]  further back context, you wouldn't be able to.
[4387.10s -> 4392.10s]  And certainly that's one of the ways in which modern large language
[4392.10s -> 4395.10s]  models are using far bigger context than that.
[4395.10s -> 4398.10s]  They're now using thousands of words of prior context.
[4398.18s -> 4399.18s]  Yeah.
[4399.18s -> 4400.18s]  Absolutely.
[4400.18s -> 4403.18s]  It's a limit on how much far back context.
[4403.18s -> 4408.18s]  So in some sense, even though in theory a recurrent neural
[4408.18s -> 4411.18s]  network can feed in an arbitrary length context,
[4411.18s -> 4415.18s]  as soon as I say, oh, practically we cut it into segments,
[4415.18s -> 4418.18s]  you know, actually that means we are making a Markov assumption
[4418.18s -> 4421.18s]  again and we're saying the further back context doesn't matter.
[4423.18s -> 4424.18s]  Yeah.
[4424.18s -> 4425.18s]  Okay.
[4425.26s -> 4428.26s]  A couple more examples.
[4428.26s -> 4432.26s]  So instead of Barack Obama, I can feed in Harry Potter,
[4432.26s -> 4435.26s]  which is a somewhat bigger corpus of text actually,
[4435.26s -> 4436.26s]  and generate from that.
[4436.26s -> 4440.26s]  And so I can get, sorry, Harry, shouted, panicking.
[4440.26s -> 4443.26s]  I'll leave those brooms in London, are they?
[4443.26s -> 4447.26s]  No idea, said nearly headless Nick, casting low cliffs by Cedric,
[4447.26s -> 4450.26s]  carrying the last bit of treacle charms from Harry's shoulder.
[4450.26s -> 4453.26s]  And to answer him, the common room perched upon it,
[4453.34s -> 4456.34s]  four arms held a shining knob from when the spider hadn't
[4456.34s -> 4458.34s]  felt it seemed.
[4458.34s -> 4460.34s]  He reached the teams too.
[4460.34s -> 4462.34s]  Well, there you are.
[4462.34s -> 4464.34s]  You can do other things as well.
[4464.34s -> 4470.34s]  So you can train it on recipes and generate a recipe.
[4470.34s -> 4474.34s]  This one's a recipe I don't suggest you try and cook,
[4474.34s -> 4479.34s]  but it looks sort of like a recipe if you don't look very hard.
[4479.42s -> 4482.42s]  Chocolate ranch barbecue.
[4482.42s -> 4486.42s]  Categories, game casseroles, cookies, cookies.
[4486.42s -> 4489.42s]  Yields six servings.
[4489.42s -> 4493.42s]  Two tablespoons of Parmesan cheese chopped.
[4493.42s -> 4497.42s]  One cup of coconut milk and three eggs beaten.
[4497.42s -> 4500.42s]  Place each pasta over layers of lumps.
[4500.42s -> 4506.42s]  Shape mixture into the moderate oven and simmer until firm.
[4506.50s -> 4511.50s]  Serve hot in bodied fresh mustard, orange, and cheese.
[4511.50s -> 4514.50s]  Combine the cheese and salt together,
[4514.50s -> 4518.50s]  the dough in a large skillet, and the ingredients
[4518.50s -> 4521.50s]  and stir in the chocolate and pepper.
[4521.50s -> 4525.50s]  Yeah, it's not exactly a very consistent recipe
[4525.50s -> 4527.50s]  when it comes down to it.
[4527.50s -> 4531.50s]  It sort of has a language of a recipe that it's absolutely,
[4531.50s -> 4534.50s]  maybe if I had scaled it more and had a bigger corpus,
[4534.58s -> 4536.58s]  it would have done a bit better.
[4536.58s -> 4540.58s]  But it's definitely not using the ingredients there are.
[4540.58s -> 4543.58s]  Let's see, it's almost time today,
[4543.58s -> 4549.58s]  so maybe about all I can do is do,
[4549.58s -> 4551.58s]  I can do one more fun example.
[4551.58s -> 4554.58s]  And then after that, oh yeah,
[4554.58s -> 4556.58s]  I probably should do that bit at the start next time.
[4556.58s -> 4561.58s]  So as a variant of building RNN language models,
[4561.66s -> 4565.66s]  I mean, so far we've been building them over words.
[4565.66s -> 4569.66s]  So our, you know, token time steps
[4569.66s -> 4572.66s]  over which you build it as words.
[4572.66s -> 4574.66s]  I mean, actually you can use the idea
[4574.66s -> 4578.66s]  of recurrent neural networks over any other size unit.
[4578.66s -> 4580.66s]  And people have used them for other things.
[4580.66s -> 4582.66s]  So people have used them in bioinformatics
[4582.66s -> 4587.66s]  for things like DNA for sort of having gene sequencing
[4587.66s -> 4590.66s]  or protein sequencing or anything like that.
[4590.74s -> 4592.74s]  But even staying with language,
[4592.74s -> 4595.74s]  instead of building them over words,
[4595.74s -> 4598.74s]  you can build them over characters.
[4598.74s -> 4602.74s]  So that my, I'm generating a letter at a time
[4602.74s -> 4604.74s]  rather than a word at a time.
[4605.74s -> 4608.74s]  And so that can sometimes be useful
[4608.74s -> 4611.74s]  because it allows us to sort of generate things
[4611.74s -> 4614.74s]  that sort of look like words
[4614.74s -> 4617.74s]  and perhaps have the structure of English words.
[4617.82s -> 4621.82s]  And so similarly, there are other things that you can do.
[4621.82s -> 4626.82s]  So before I initialized the hidden state,
[4626.82s -> 4629.82s]  I said, oh, you just have an initial hidden state.
[4629.82s -> 4631.82s]  You can make it zeros if you want.
[4631.82s -> 4636.82s]  Well, sometimes we're going to build a contextual RNN
[4636.82s -> 4638.82s]  where we can initialize the hidden state
[4638.82s -> 4640.82s]  with something else.
[4640.82s -> 4644.82s]  So in particular, I can initialize the hidden state
[4644.90s -> 4647.90s]  with the RGB values of a color.
[4647.90s -> 4651.90s]  And so I can have initialized the hidden state with the color
[4651.90s -> 4654.90s]  and generate, character at a time,
[4654.90s -> 4656.90s]  the name of paint colors.
[4656.90s -> 4660.90s]  And I can train a model based on
[4660.90s -> 4664.90s]  a paint company's catalog of names of colors
[4664.90s -> 4667.90s]  and their RGB of their colors.
[4667.90s -> 4671.90s]  And then I can give it different paint colors
[4671.90s -> 4673.90s]  and it'll come up with names for them.
[4673.98s -> 4675.98s]  And it actually does an excellent job.
[4675.98s -> 4676.98s]  This one worked really well.
[4676.98s -> 4678.98s]  Look at this.
[4678.98s -> 4683.98s]  This one here is ghastly pink, power gray,
[4683.98s -> 4689.98s]  navel tan, bochco white, horrible gray,
[4689.98s -> 4690.98s]  homestar brown.
[4690.98s -> 4692.98s]  Now, couldn't you just imagine finding all of these
[4692.98s -> 4694.98s]  in a paint catalog?
[4694.98s -> 4696.98s]  I mean, some of them are...
[4696.98s -> 4699.98s]  There's some really good ones over here in the bottom, right?
[4699.98s -> 4701.98s]  This color here is dope.
[4702.06s -> 4707.06s]  And then this stoner blue,
[4707.06s -> 4713.06s]  burple simp, stinky bean and turdly.
[4713.06s -> 4717.06s]  Now, I think I've got a real business opportunity here
[4717.06s -> 4719.06s]  in the paint company market
[4719.06s -> 4721.06s]  for my recurrent neural network.
[4721.06s -> 4723.06s]  Okay, I'll stop there for today
[4723.06s -> 4726.06s]  and do more of the science of neural networks next time.
