# Detected language: en (p=1.00)

[0.00s -> 12.64s]  Okay, hi everyone! I'll be getting started. Okay, so it's Tuesday of week two, so hopefully
[12.64s -> 21.30s]  that means everyone has done the assignment one. Everyone done assignment one? You know,
[21.30s -> 25.74s]  if I'm saying this, I'm probably saying it to the wrong people, but it seems like
[25.74s -> 34.02s]  every year some people blow some of their late days on assignment one and it's really just the
[34.02s -> 40.90s]  wrong place to use them. So yeah, hopefully you've all done assignment one and did not,
[40.90s -> 47.38s]  that this is meant to be the easy on-ramp and then we go straight on from that so that out
[47.38s -> 57.70s]  today we have assignment two. So assignment two has two purposes. Purpose one is to make you do
[57.70s -> 65.98s]  some math to some understanding of what neural networks really compute and how they compute it,
[65.98s -> 70.82s]  and that's what I'm going to talk about today is also going through that math. But then
[70.82s -> 77.02s]  simultaneously, maybe it does three things in assignment two, so we're going to be learning
[77.18s -> 82.58s]  something about dependency parsing which will be actually something about language structure
[82.58s -> 89.78s]  and linguistics. But then thirdly, for assignment two, we're going to start using PyTorch. So PyTorch
[89.78s -> 95.86s]  is one of the leading software frameworks for deep learning and the one that we're going to use
[95.86s -> 107.42s]  for this class. So for PyTorch, I mean, the assignment three PyTorch is exceedingly scaffolded,
[107.42s -> 111.90s]  so it's sort of, you know, here's this thing and you have to write these two lines,
[111.90s -> 119.86s]  use these two functions. But nevertheless, for help people get up to speed and get started
[119.86s -> 126.46s]  using PyTorch, on Friday at 3.30 in Gates V01, or it will also again be recorded,
[126.46s -> 131.50s]  we have a tutorial on PyTorch and so that's a great way to get more of a
[131.50s -> 141.34s]  sense of PyTorch and how it works before doing assignment two. The other things, yeah,
[141.34s -> 148.22s]  so for nearly all the lectures we've got further reading of places that you can look. Of all the
[148.22s -> 156.46s]  classes in the entire quarter, this for many people might be a really good one to look at
[156.46s -> 162.74s]  the suggested readings. We have several readings which are sort of shorter tutorials and reviews
[162.74s -> 170.78s]  of the kind of matrix calculus and linear algebra that we need for this class. So
[170.78s -> 176.30s]  really encourage you to look at those. If you decide that one is your favorite,
[176.30s -> 180.98s]  you can tell us on Ed which one you think is the best one to choose between them. I kind of
[180.98s -> 186.82s]  like the one that's first on the list, but maybe you'll feel differently. Yeah, conversely,
[186.82s -> 195.54s]  yeah, so today will be sort of all math and then Thursday will be kind of all language
[195.54s -> 200.70s]  and linguistics. Some people find the language and linguistics hard as well, so I guess
[200.82s -> 208.82s]  different kinds of people. Okay, so getting straight into it. So where we started last time,
[208.82s -> 215.62s]  I'd sort of shown these baby neural networks and sort of said well we can think of each of
[215.62s -> 222.10s]  those orange things as basically like a little logistic regression unit. And the crucial
[222.10s -> 228.14s]  difference from then the kind of statistics machine learning you see in the stats class,
[228.34s -> 236.10s]  109 or wherever, is that in those you have one logistic regression and you're defining the input
[236.10s -> 243.10s]  features to it and you've got some decision variable that you want to have at the output.
[243.10s -> 249.14s]  Here you're sort of building these cascades of little logistic regressions and so the idea is
[249.14s -> 253.66s]  right at the end we're going to define what we want. We're going to capture that by
[253.98s -> 260.62s]  our objective function or loss function, but for the stuff in the middle, that stuff in the middle
[260.62s -> 269.30s]  is going to be a chance for the neural network to learn by itself what would be useful inputs to
[269.30s -> 278.02s]  further downstream neurons, what kind of functions should I come up with in terms of my inputs that
[278.02s -> 287.58s]  will help me provide useful outputs to help the final computation down the track. And you know
[287.58s -> 293.74s]  if you haven't sort of seen and thought about this much before, I mean I think you know it's
[293.74s -> 300.02s]  worth sitting with that idea for a moment because this is really a super powerful idea
[300.02s -> 306.46s]  which is what's made neural networks more powerful in most circumstances than other
[306.50s -> 313.42s]  forms of machine learning. The fact that you have this self-organization of intermediate levels
[313.42s -> 319.34s]  of representation that you use to compute things that will be useful downstream for what
[319.34s -> 325.30s]  you eventually want to do. The other reason I was bringing back up this picture is I've sort
[325.30s -> 334.94s]  of wanted to go straight from here to matrices. So while you could sort of wire together neurons
[335.02s -> 342.14s]  however you wanted to, and arguably if you look at human brains they look more like neurons wired
[342.14s -> 347.30s]  together however you wanted to, but for what's done with neural networks basically there's
[347.30s -> 353.46s]  always this kind of regular structure of layers. So once we have this regular structure of layers
[353.46s -> 363.50s]  we are taking the output of one of our neurons at one layer and we're feeding them together
[363.50s -> 373.42s]  with weights to produce the inputs to the next layer. So we're taking the x1, x2, x3 outputs,
[373.42s -> 380.66s]  we're multiplying them all by weights, we're adding a bias term, and then we're going to
[380.66s -> 385.02s]  put it through a non-linearity and that will give us the value at the next layer. So if
[385.02s -> 391.62s]  we then kind of collapse that to a vector and this to a vector that then collapses into a
[391.66s -> 400.22s]  computation that first of all we're doing a matrix multiplication, we're calculating wx of
[400.22s -> 406.10s]  the inputs and then we're adding on the biases as a vector of biases which gives us this
[406.10s -> 414.50s]  intermediate value z, and then we have this non-linearity or activation function which is
[414.50s -> 420.42s]  applied to that which gives us the values in the next layer of the neural network. And
[420.46s -> 426.54s]  the activation function is applied to a vector and produces a vector but it's operating on each
[426.54s -> 432.72s]  of the individual components of that vector one at a time. So we've got some scalar function
[432.72s -> 439.94s]  that we're just applying to each element of the vector. And so that's the kind of picture
[439.94s -> 446.30s]  we saw when I did this example and I'm going to continue to use this example in today's
[446.38s -> 452.18s]  class. Remember we were going to decide whether the word in the middle of the input window was
[452.18s -> 458.58s]  a location or not. And so we were doing the matrix multiplication, putting it through the
[458.58s -> 464.34s]  non-linearity, we're then just doing a dot product here and then we can- that got stuck
[464.34s -> 475.06s]  into sigmoid to predict yes or no. And the final thing I wanted to say a little bit about is
[475.06s -> 482.38s]  these Fs, the non-linearity or the activation function. Where did they come in? Well the
[482.38s -> 488.54s]  starting point of where they came in in the history of neural networks is when people came
[488.54s -> 496.74s]  up with this idea that well you could represent the operation of a basic neuron by doing a matrix
[496.74s -> 504.98s]  multiplication of the inputs and then having a bias term or here as represents the threshold
[504.98s -> 511.38s]  term to see whether the neurons should fire or not. That was actually in the very first
[511.38s -> 520.10s]  implementation which dates back to the 1940s as done as a threshold, right? So that if the
[520.14s -> 528.22s]  activation was greater than theta you output one, otherwise you output zero. And well if you
[528.22s -> 537.02s]  have a threshold the two lines are flat, right? So there is no slope, there is no gradient. So
[537.02s -> 544.22s]  that's actually makes learning much harder. So the whole secret of what we build with neural
[544.22s -> 550.58s]  networks and in particular an alternative name that's popular in some circles these days is
[550.58s -> 557.70s]  gradient-based learning. And the entire idea of gradient-based learning is if we actually have
[557.70s -> 564.34s]  some slopes then it's like going skiing during spring break. You can work out where it's steeper
[564.34s -> 571.06s]  and you can head down where it's steeper and that will allow us to optimize our function to
[571.06s -> 578.34s]  learn much more quickly. And so that's one reason that we don't just want to have threshold units,
[578.34s -> 585.94s]  we want to have things with slopes so we have gradients. So in subsequent work people started
[585.94s -> 594.34s]  using activation functions with slopes. And so the first popular one was this sigmoidal
[594.58s -> 600.46s]  logistic that we've seen from mapping to probabilities. But you know it's sort of
[600.46s -> 606.54s]  imperfect it seemed because you know the output was always non-negative so that sort of tends to
[606.54s -> 615.66s]  push things towards bigger numbers. So there was quite a bit of use then of this tanh function
[615.66s -> 621.78s]  and you'll actually see tanh when we do assignment three we'll be using tanh as in our
[621.86s -> 632.14s]  current neural networks. And so I've written there the formula usually give for tanh in terms of
[632.14s -> 638.34s]  exponentials. Yeah if your math is rusty it's not obvious that tanh and logistic have much to
[638.34s -> 646.22s]  do with each other but if you want to treat this as a math problem that a tanh is literally
[646.22s -> 652.46s]  just a rescaled logistic. You're stretching it by two and moving it down by one it's the same function.
[652.46s -> 661.26s]  Okay but you know so that's nice but if you calculate tanh's you have to do all of these
[661.26s -> 667.18s]  exponentials and you know exponentials are kind of slow on your computer and things like that.
[667.18s -> 673.04s]  You might wonder whether you couldn't get away with something much cheaper. And so people
[673.08s -> 679.48s]  thought about that and thought oh maybe we could just use a so-called hard tanh where it has a
[679.48s -> 685.44s]  slope of one in the middle and is then just flat outside that area. And you know that seemed
[685.44s -> 693.60s]  to work in many cases and so that then led to the popularity of the rectified linear unit. So
[693.60s -> 700.60s]  the rectified linear unit is simply zero in the negative region and then as y equals x in
[700.60s -> 707.32s]  the positive region. Now this seems kind of wonky and goes against what I was saying
[707.32s -> 712.84s]  about gradient based learning because once you're in the negative region there's no gradient you're
[712.84s -> 720.52s]  just dead. But in the positive region there is gradient and the gradient is particularly simple
[720.52s -> 729.28s]  right the slope is always one. And so you know this still feels slightly perverse to me but
[729.84s -> 737.04s]  this really became the norm of what people use for a number of years because people found that
[737.04s -> 742.32s]  although it for an individual neuron it was dead half the time anytime it went negative that
[742.32s -> 748.32s]  overall for your neural network some things would be alive. So it kind of gave sort of a form
[748.32s -> 756.04s]  of specialization and the fact that the slope was always one meant that you got really easy
[756.28s -> 763.32s]  productive backward flow of gradients in a way we'll talk about later. And so learning with
[763.32s -> 770.60s]  the ReLU turned out to be very effective and people started using the ReLU nonlinearity
[770.60s -> 775.96s]  everywhere and it sort of became the default in the norm and you'll see us using it in the
[775.96s -> 780.28s]  assignments in particular we use an assignment too and so you'll get to see that it works.
[780.84s -> 787.80s]  But nevertheless at some point people sort of had second thoughts and decided you know having
[787.80s -> 792.76s]  a dead over half of its range maybe isn't such a good idea after all even though it seemed to
[792.76s -> 798.68s]  work great for a few years. And so a lot of what's happened since then is then to come up
[798.68s -> 805.88s]  with other functions which are in some sense ReLU-like but not actually dead. So
[806.04s -> 817.08s]  okay I don't really yeah here we go. All right so one version of that is the so-called
[817.08s -> 823.72s]  Leaky ReLU. So for the Leaky ReLU you make the negative half a straight line as well with
[823.72s -> 828.76s]  a very minor slope but still it's got a little bit of slope. There was then a variant of that
[828.76s -> 834.28s]  called the parametric ReLU where you have one extra parameter which is actually what the slope
[834.28s -> 842.20s]  of the negative part is and people showed some positive results with that. More recently again
[842.20s -> 850.20s]  and this is what you often see in recent transformer models is you see non-linearities
[850.20s -> 858.20s]  like SWISH and JELU. So both of these are sort of fancy functions but kind of what they both
[858.20s -> 865.48s]  look like is basically this is y equals x to all intents and purposes not quite but approximately
[865.48s -> 869.88s]  and then you've got sort of some funky bit of curve down here which again gives you a bit of
[869.88s -> 874.52s]  slope. It's sort of the curve is going the opposite way that's sort of a bit funny but
[874.52s -> 882.52s]  they seem to work well commonly used in recent transformer models. So you know that's a bit of
[882.52s -> 889.80s]  a dump of all the non-linearities people use. I mean the details of that aren't super important
[889.80s -> 897.88s]  right now but the important thing to have in your head is why do we need non-linearities
[897.88s -> 903.88s]  and the way to think about that is that what we're doing with neural networks is
[903.88s -> 909.32s]  function approximation. There's some very complex function that we want to learn you know like
[909.32s -> 914.44s]  maybe we want to go from a piece of text to its meaning or we want to be interpreting visual
[914.44s -> 921.00s]  scenes or something like that and so we want to build really good function approximators
[921.56s -> 927.64s]  and well if you're just doing matrix multiplies a matrix multiply of a vector is a linear
[927.64s -> 933.56s]  transform so that doesn't let you multiply complex functions. I guess strictly if you put a
[933.56s -> 938.68s]  bias on the end it's then an affine transform but let's keep it simple. Linear transforms
[938.68s -> 945.16s]  right so if you're doing multiple if you're doing multiple matrix multiplies you're doing multiple
[945.16s -> 950.44s]  linear transforms but they compose so you could have just multiplied these two matrices together
[950.44s -> 956.36s]  and you'd have a single linear transform so you get no power in terms of representation
[956.92s -> 964.76s]  by having multi-layer networks that are just matrix multiplies. You know as of a little aside
[964.76s -> 971.40s]  in terms of representational power having multi-layer matrix multiplies gives you no
[971.40s -> 976.68s]  power but if you think about in terms of learning actually it does give you some power.
[976.68s -> 981.32s]  So in the theoretical community looking at neural networks there are actually quite a few papers
[981.32s -> 986.92s]  that look at linear neural networks meaning that they're just sequences of the multiplies with no
[987.48s -> 991.96s]  non-linearities because they have interesting learning properties even though they give you
[991.96s -> 998.84s]  no representational power. Okay but we'd like to be able to learn functions like this not only
[998.84s -> 1003.88s]  functions like this and to be able to learn functions like this we need more than linear
[1003.88s -> 1011.32s]  transforms and we achieve those by having something that makes us be calculating a non-linear
[1011.32s -> 1018.60s]  function and it's these activation functions that give us non-linear functions. Okay cool.
[1019.32s -> 1027.40s]  Um okay so then for getting on to today so the whole thing we want to do now is gradient based
[1027.40s -> 1034.28s]  learning right this is our stochastic gradient descent equation where here you know that upside
[1034.28s -> 1040.60s]  down triangle symbol right that's our gradient we're wanting to work out the slope of our
[1040.60s -> 1046.60s]  objective function and so this is how we're going to learn by calculating gradients. So what
[1046.60s -> 1052.52s]  we want to know is how do we calculate the gradients for an arbitrary function and so what
[1052.52s -> 1061.96s]  I want to do today is first of all do this by hand for math and then discuss you know how do
[1061.96s -> 1068.36s]  we do it computationally which is effectively the famous thing that's taken as powering
[1069.40s -> 1073.08s]  under powering all of neural nets which is the back propagation algorithm.
[1073.08s -> 1080.04s]  But the back propagation algorithm is just automating the math. Okay and so for the math
[1080.04s -> 1086.04s]  it's matrix calculus and at this point then there's a huge spectrum between people who
[1086.04s -> 1091.64s]  know much more math than me and people who barely ever learned this but you know
[1092.28s -> 1099.96s]  I hope to explain the essentials or remind people of them enough that you're at least
[1099.96s -> 1106.44s]  at a starting point for reading some other stuff and doing homework too so let's get into
[1106.44s -> 1112.84s]  that and so I'm going to spend about half the time on those two halves and you know the hope
[1112.84s -> 1117.48s]  is that after this you'll feel like oh I actually understand how neural networks work under
[1117.48s -> 1126.60s]  the hood fingers crossed. Okay so here we go so if you're a Stanford student you maybe did math
[1126.60s -> 1134.84s]  51 or else you could have done math 51 which teaches linear algebra multivariate calculus
[1134.84s -> 1141.88s]  and modern applications. Math 51 covers everything I'm going to talk about and way more stuff so if
[1141.88s -> 1147.56s]  you actually know that and remember it you can look at Instagram for the next 35 minutes.
[1148.36s -> 1154.84s]  But I think the problem is that you know quite a part of the fact a lot of people do it as frosh
[1155.80s -> 1161.32s]  you know this is a lot to get through in 10 weeks and I think that a lot of the people who do this
[1161.32s -> 1167.00s]  class sort of by two years later don't really have much ability to use any of it but you know
[1167.00s -> 1171.96s]  if you actually looked at this book really hard and for a very really long time you would
[1171.96s -> 1178.44s]  have discovered that actually right towards the end of the book in appendix G there's actually an
[1178.44s -> 1184.44s]  appendix on neural networks and the multi-variable chain rule which is precisely what we're going
[1184.44s -> 1191.96s]  to be using for doing our neural networks but there are only two problems one problem is that
[1191.96s -> 1199.64s]  this is page 697 of the book and I'm not sure anyone ever gets that far and the other problem
[1199.64s -> 1205.56s]  is even if you do get that far you know I know I find these pages that they're really
[1205.56s -> 1211.00s]  dense texty pages it's not even easy to understand them if you have gone there so here's
[1211.08s -> 1219.32s]  my attempt on that um so the mantra to have in your head is g if I can remember basic single
[1219.32s -> 1225.24s]  variable calculus you know that I've got 3x squared and the derivative of that is 6x
[1225.88s -> 1232.76s]  that's all you sort of need to know right the mantra is multi-variable calculus is just like
[1232.76s -> 1239.48s]  single variable calculus except you're using matrices okay so that's our article of faith
[1239.48s -> 1244.92s]  and we're going to do that and so what we're wanting to do is to do matrix calculus or
[1244.92s -> 1251.16s]  and the generalization of that tensor calculus sort of using vectors matrices and higher order
[1251.16s -> 1256.92s]  tensors because if we can do things in what's referred to as vectorized gradients in the
[1256.92s -> 1264.20s]  neural network world that that will be sort of the fast efficient way to do our operations
[1264.20s -> 1269.96s]  you know so if you want to think it all through you can do it single variable at a time and check
[1269.96s -> 1274.60s]  that you're doing the right thing and I sort of tried to indicate that in the first lecture
[1274.60s -> 1281.32s]  but if we want to have our networks go room we want to be doing matrix calculus okay
[1282.36s -> 1289.56s]  so let's work up to doing that okay so this is the part that I I trust everyone can remember
[1289.56s -> 1300.92s]  right so we have f of x equals x cubed and we can do single variable derivative and the derivative
[1300.92s -> 1308.04s]  is 3x squared everyone remember that one okay that's something we can all start from and
[1308.04s -> 1314.84s]  remember this derivative is saying the slope of things right so the slope of things lets us
[1314.84s -> 1321.96s]  work out where is something steep so we'll be able to go skiing right that's our goal right and
[1321.96s -> 1328.84s]  so you can think of the slope of things as how much the output will change if we change
[1328.84s -> 1335.40s]  the input a bit right that's our measure of um steepness right so um that so since the
[1335.40s -> 1343.24s]  derivative is 3x squared if we're at x equals 1 that means the slope is about 3 times 1 squared
[1343.24s -> 1351.08s]  3 so if I work out the value of the function for 1.01 it's gone up by about three times point
[1351.08s -> 1359.48s]  I move the x by 0.01 and the output moved by 0.03 where if I go to x equals 4 the derivative
[1359.48s -> 1366.20s]  is 3 times 4 squared is 48 and so if I work out the value of the function at 4.01 I get
[1366.20s -> 1374.04s]  approximately 64.48 versus 64 right that the small difference from 4 to 4.01
[1374.04s -> 1382.84s]  has been magnified 48 times in the output okay so now we just uh remember the mantra
[1382.84s -> 1389.32s]  it's going to be exactly the same single value calculus um but with more stuff so if we have
[1389.32s -> 1397.88s]  a function with n inputs we then go to work out its gradient um which is its partial derivative
[1397.88s -> 1405.24s]  with respect to each input so its gradient will now be a vector of the same size as the
[1405.24s -> 1411.72s]  number of inputs um and there's this funky symbol um which people pronounce various ways
[1411.72s -> 1416.68s]  I mean you know this kind of originated some kind of someone's weird way of drawing a
[1416.68s -> 1421.88s]  calligraphic d right so it is really a d um so I think I'll mainly just call it d
[1421.88s -> 1427.64s]  but sometimes people call it partial or funky d or some some other name right so you have
[1427.64s -> 1436.36s]  df dx 1 df dx 2 for each of the variables okay so if we go beyond that um and then have
[1436.36s -> 1447.24s]  a function with um n inputs and m outputs what we then get for um the gradient is what's
[1447.24s -> 1454.76s]  referred to as the Jacobian now actually um the dude this is named after was a German Jew
[1454.76s -> 1462.28s]  so it should really be Jacobi um but no one says that in this country um Jacobian um
[1463.08s -> 1471.08s]  okay so the Jacobian is then a matrix of partial derivatives um where you're working
[1471.08s -> 1478.76s]  out for each output and each input the partial derivative between the component of the input
[1478.76s -> 1484.28s]  and the output so this looks like the kind of thing that we're going to have when we have a
[1484.28s -> 1491.00s]  neural network layer because we're going to have um n inputs and m outputs for the two layers
[1491.32s -> 1499.64s]  of our neural network so we'll be using these kind of Jacobians okay um so then you know the
[1499.64s -> 1507.24s]  whole idea of neural networks is we've got these multi-level computations and they're going
[1507.24s -> 1514.92s]  to correspond to composition of functions so we need to know how to compose things both
[1514.92s -> 1521.32s]  for calculating functions and for calculating their gradients so if we have a one variable
[1521.32s -> 1529.96s]  function and we want to um work out um its derivative in terms of a composition of two
[1529.96s -> 1539.40s]  functions what we're doing is multiplying um our computations okay so um if you compose together
[1539.96s -> 1547.32s]  um z of y um that's the function that we did at the beginning that gives you three oh was it
[1547.32s -> 1553.48s]  no it's not sorry it's different okay z of y gives you three x squared right and so we know
[1553.48s -> 1562.04s]  that the derivative of that is um six x okay but if we do it in terms of the pieces we can
[1562.04s -> 1573.16s]  work out um dz dy um which is just going to be three and um dy dx is 2x and we can work out
[1573.16s -> 1580.04s]  the total derivative by multiplying these two pieces and we get 6x the same answer right um
[1580.60s -> 1589.16s]  so um matrix calculus is exactly like single variable calculus except we're using um tensors
[1589.16s -> 1596.20s]  of different um so the word tensor is used to mean as you go up that spectrum in its size so
[1596.20s -> 1602.12s]  from sort of scalar to vector the matrix to then you know what in computer science is
[1602.12s -> 1609.24s]  normally still uh is multi-dimensional arrays um that spectrum is then tensors of different
[1610.44s -> 1617.32s]  dimensions okay so um when we have multiple variable functions we're going to multiply
[1617.32s -> 1627.48s]  jacobians so here we have a function wx plus b and then we compose um the non-linearity f
[1627.48s -> 1634.04s]  to get h and so we're going to be able to compute that in the same way as a product of
[1634.04s -> 1642.60s]  these partial derivatives which are jacobians okay so let's start looking at a few examples of
[1642.60s -> 1648.04s]  what we get so let's count with start with an element wise activation function
[1648.60s -> 1655.40s]  so when we have a a vector that's being calculated as the activation function of
[1655.40s -> 1661.88s]  a previously computed quantity well we're computing that component wise as i explained
[1661.88s -> 1669.08s]  before so hi equals f of zi and where this sort of f is our activation function that actually
[1669.08s -> 1676.52s]  applies to a scalar but you know overall this layer is a function with n outputs and n inputs
[1676.52s -> 1681.88s]  and so it's going to have an n by n jacobian and well what that's going to so this is
[1681.88s -> 1688.92s]  our definition of the jacobian um but in this case this is sort of a special case
[1689.56s -> 1698.84s]  because if i equals j um then we're going to have um the output um j
[1700.12s -> 1707.88s]  the hj depending on zi and otherwise it's going to be zero because for the off diagonal entries
[1707.88s -> 1712.84s]  it doesn't matter how you change the value it's not changing the output because the output
[1712.84s -> 1718.52s]  only depends on the corresponding index and so what we're going to get for this jacobian
[1718.52s -> 1725.16s]  of activation functions is a matrix where everything is zero apart from the diagonal terms
[1725.88s -> 1733.00s]  that correspond to where we're calculating the activation function and for those ones we're
[1733.00s -> 1738.52s]  going to have to work out how to compute the derivative of our activation function
[1739.08s -> 1744.04s]  that was on assignment one one of the questions on assignment one i do believe
[1744.04s -> 1748.52s]  always always on assignment two no no it's assignment two one of the questions on
[1748.52s -> 1753.96s]  assignment two i got that wrong one of the ones on the new assignment is say hey um can you work
[1753.96s -> 1759.48s]  out um the derivative of a logistic function well then we'd be able to plug that straight into
[1760.12s -> 1767.00s]  f prime so i'm not going to give that answer away today um okay so um other things that we
[1767.00s -> 1775.48s]  want to do um with uh jacobian is well we have this um layer of our neural network where we're
[1776.36s -> 1782.12s]  calculating w x plus b and we can want to work out the partial derivative of that with respect
[1782.12s -> 1789.56s]  to x um you know this is the kind of place where it actually works to remember the mantra
[1789.56s -> 1796.52s]  and say matrix calculus is just like single variable calculus but with matrices
[1796.52s -> 1801.16s]  so if you just don't use your brain too hard and think oh it's just like single variable calculus
[1801.16s -> 1807.48s]  so what should the answer be it's obviously going to be w right and indeed it is um similarly
[1807.48s -> 1813.00s]  if we want to do the same thing for w x b and work out the partial derivative with respect to b
[1813.80s -> 1820.76s]  well that would be one in terms of single variable calculus and so in matrix calculus that becomes
[1820.84s -> 1826.92s]  an identity matrix okay slightly different but same idea um but that's reflecting the
[1826.92s -> 1833.08s]  fact that b is actually a vector so we need we need it to be coming out um as an identity
[1833.08s -> 1843.56s]  matrix um okay so um higher up in my example picture i did this sort of vector um dot product
[1843.56s -> 1852.36s]  of ut h um and well what happens if we work out the um the um
[1854.44s -> 1863.48s]  the cobian of that what we end up with strictly is we come out with ht um and you know this is
[1863.48s -> 1869.16s]  sort of like when you're working out um well we did this in the first class right when we did
[1869.56s -> 1876.76s]  a dot product calculation that you kind of get for each individual element you get the opposite
[1876.76s -> 1883.56s]  term and so you get um the other vector coming out um these are sort of good ones to compute
[1883.56s -> 1888.76s]  it at home for practice to make sure um you really do know the answers and why they
[1888.76s -> 1895.40s]  work out the way they do okay um so let's go back to our little neural net
[1896.12s -> 1903.16s]  this was most of our neural net up above our neural net um there was the non-linearity
[1903.16s -> 1909.08s]  now i'm gonna uh leave that out of this time oh see i got it wrong it's on assignment two
[1909.88s -> 1916.52s]  but you know normally you'd be calculating the partials of the output the loss function with
[1916.52s -> 1921.88s]  respect to the inputs but since the loss function is on assignment two i'm going to
[1921.88s -> 1927.80s]  leave that out and i'm just going to calculate derivatives with respect to this score that
[1927.80s -> 1935.24s]  feeds into the loss function so we first of all got um the neural network layer the non-linearity
[1935.24s -> 1940.44s]  and then we're doing this dot product to work out a score for each position which feeds into
[1940.44s -> 1947.72s]  the logistic function so if you want to work out ds db um so that's with respect to the bias
[1947.72s -> 1955.16s]  first so the way we do it is you know we break up our equations into our individual pieces that
[1955.16s -> 1960.60s]  are composed together and so that means whether we break this up so we first calculate the z
[1960.60s -> 1968.12s]  equals w x plus b then we apply the activation function to the different components okay then
[1968.12s -> 1979.88s]  after that um well what we remember to do is okay to work out um our um partial derivatives
[1979.88s -> 1988.92s]  of b of s with respect to b that what we're going to be doing is doing the product of um
[1988.92s -> 1995.56s]  the partial derivatives of the component pieces so we're applying um the matrix calculus version
[1995.56s -> 2006.84s]  of the chain rule so ds db equals ds dh times dh dz times dz db um and which corresponds these
[2006.84s -> 2013.40s]  three layers that are composed together and so at that point um we remember our useful
[2013.40s -> 2022.04s]  jacobians from the previous slide and we can just apply them so the top one um ds dh is
[2022.04s -> 2030.04s]  um the u transpose or else well maybe it's u let's come back to that but
[2030.04s -> 2035.32s]  i've got there's a fine point on that but i will explain more about later um um
[2036.60s -> 2043.56s]  okay um then for the dh dz that was the activation function where we got the diagonal
[2043.56s -> 2052.44s]  of um the derivative of f of z and then for dz db that's where we got the identity function
[2052.44s -> 2059.80s]  okay um so we can simplify that down and so what that's going to end up as
[2060.36s -> 2069.96s]  is the u transpose that funny symbol there times the um l the vector element wise um
[2070.04s -> 2077.72s]  derivative of f um this symbol which doesn't normally turn up um in your regular math course
[2077.72s -> 2083.80s]  but turns up all the time in neural networks is referred to as the Hadamard product and the
[2083.80s -> 2089.40s]  Hadamard product is meaning element wise multiplication so it's not like a cross product
[2089.40s -> 2094.60s]  where you put two vectors together and you get out one number of scalar you put two vectors
[2094.60s -> 2100.76s]  together you element wise multiply them all and you're left with another vector of the same type
[2102.36s -> 2109.72s]  okay so that so now this gave us a working out of the partials of ds db and for a neural network
[2110.36s -> 2118.04s]  we want to work out all the other partials as well so overall here in the picture right
[2118.04s -> 2129.48s]  we had the x the w the b and the u and we'd like to work out partials with respect to all
[2129.48s -> 2136.68s]  of those variables so we can change their values and learn so that our model predicts better
[2137.32s -> 2146.52s]  um so um so suppose we now want to calculate ds dw so again we can split it up with the
[2146.52s -> 2153.48s]  same chain rule and say ds dw equals the product of these three things and the important thing to
[2153.48s -> 2159.08s]  notice is that two of those three things were exactly the same ones that we calculated before
[2159.64s -> 2166.68s]  the only bit that's different is that at the end we're now doing dz dw rather than dz db
[2167.32s -> 2173.64s]  and so the first central idea that we'll come back to when we do computation graphs is
[2173.64s -> 2180.36s]  oh we really want to avoid doing repeated work so we want to realize that those two parts of
[2180.36s -> 2185.88s]  things are the same and since we're just multiplying these um partial derivatives
[2185.88s -> 2193.40s]  together right we can just compute what that part is and reuse it and so if we want to um
[2194.20s -> 2201.64s]  wait yeah okay so if we're wanting to calculate dsdw the part that's the same
[2202.44s -> 2209.24s]  this part here we can refer to as delta so delta is sort of the upstream gradient or the
[2209.24s -> 2215.40s]  error signal the part that you've got from sort of starting at the beginning dsdh dhdz
[2215.40s -> 2222.04s]  this sort of shared upstream part we can calculate that once and then we can use it
[2223.00s -> 2231.08s]  to calculate both of these two things and for dsdb because the dzdb just comes out
[2231.08s -> 2240.36s]  as the identity matrix um the answer is just delta but for dsdw we need to work out
[2240.36s -> 2251.96s]  the dz dw before we're finished okay so what um does what do we get for that last piece so
[2251.96s -> 2257.00s]  one question you might start off with and it's normally a good thing to think about when you're
[2257.00s -> 2262.84s]  doing assignment problems on this and other things is the first thing to think about is you know
[2262.84s -> 2269.88s]  what do things look like like am i should the answer be a vector a matrix uh what size
[2269.88s -> 2282.12s]  should it be and things like that so for dsdw um w is an n by m matrix um and s is a scalar
[2282.12s -> 2291.32s]  so therefore since we have one output and n times m imports the answer according to math
[2291.32s -> 2301.96s]  should be that we've got a one by n times m jacobian a big long row vector um but here's where
[2301.96s -> 2308.44s]  um things get a teeny bit tricky um and there's sort of we end up with this weird
[2310.76s -> 2318.12s]  mess of math and engineering convenience because you know immediately what we're wanting to do
[2318.20s -> 2325.56s]  is we're wanting to take our old parameters which will be in stored in the form of matrices
[2325.56s -> 2330.92s]  vectors and so on that we're using as coefficients and we're going to want to subtract from
[2332.52s -> 2339.56s]  our you know a fraction of our calculated gradient so what we'd like to do is have our
[2340.84s -> 2346.84s]  calculated gradients in the same shapes as our parameters because then we can just do
[2346.84s -> 2352.36s]  subtraction whereas if they've turned into a god almighty row vector um that's not quite so
[2352.36s -> 2360.20s]  convenient um so it turns out that what we end up doing um is using something that gets
[2360.20s -> 2373.24s]  referred to as the shape convention that um we uh uh reshape our um jacobians so they fit
[2373.24s -> 2380.52s]  into things that are of the same shape as the parameters that we are using so we're going to
[2380.52s -> 2390.20s]  represent um dsdw as an n by m matrix laid out as follows and that's a place that one people
[2390.20s -> 2397.48s]  can get confused okay so that's what we want to calculate that kind of matrix but um and so
[2397.48s -> 2403.88s]  that matrix is going to be delta times dz dw so delta is going to be part of the answer
[2403.88s -> 2411.40s]  and then we want to know um what dz dw is um and the answer is going to be it's going to come
[2411.40s -> 2419.64s]  out like this so dsdw is going to be um delta t times xt so it's going to be the product of
[2419.64s -> 2425.40s]  the upstream gradient which was the same thing we calculated before for the other two quantities
[2425.40s -> 2433.48s]  and then a local input symbol which um is input signal which is here coming out to
[2433.48s -> 2442.28s]  xt okay um and you know so we're taking the transposes of those two vectors which it means
[2442.28s -> 2450.12s]  that we end up calculating an outer product of those two vectors um which gives us our gradient
[2450.84s -> 2456.28s]  and so why is that the right answer well you know it kind of looks convenient because that's
[2456.28s -> 2461.32s]  giving us something of the right shape um for what i was arguing we want to find out and we
[2461.32s -> 2468.36s]  have the right number of terms um now i i'm going to rush through this so i encourage you to read
[2468.36s -> 2475.08s]  um the uh lecture notes and do this more carefully but um let me at least a little bit
[2475.08s -> 2483.16s]  explain why it makes sense right so um if you think of one weight um in so all of these
[2483.16s -> 2487.96s]  connections are our matrix right the matrix is being represented by all these lines in a neural
[2487.96s -> 2494.52s]  network so if you think of one number in the matrix so here is w23 so it's connecting from
[2494.52s -> 2503.00s]  input three or it's multiplying input three to give part of the answer of h2 it right so it's
[2503.00s -> 2512.76s]  this line here um so for this line here um this weight is being used only in the calculation of h2
[2513.72s -> 2520.76s]  and the only thing that's dependent on is x3 um so that if you're then wanting to work out
[2520.76s -> 2536.44s]  the partial of um h2 um or z2 sorry yeah um sorry yeah sorry z2 the partial of z2
[2537.00s -> 2543.48s]  with respect to x3 it's sort of depending on these two pieces only and that's what you're
[2543.48s -> 2550.20s]  achieving um by working out um the sort of outer product like that
[2552.20s -> 2560.28s]  okay um yeah so let me just come back um one more time to um these these sort of question
[2560.28s -> 2569.88s]  of the shape of derivatives um you know so i already sort of fudged it um when i was sort of
[2570.36s -> 2577.32s]  um talking about oh should i put the the transpose there or should i not and get a row vector versus
[2577.32s -> 2585.96s]  a column vector um so there's sort of this disagreement between whether you kind of have
[2585.96s -> 2592.20s]  the jacobean form which is what actually makes the chain rule work right in terms of
[2592.20s -> 2599.08s]  doing multiplication versus the shape convention which is how we store everything for our
[2599.08s -> 2606.36s]  computations and makes doing stochastic gradient descent where you're subtracting
[2606.36s -> 2615.64s]  um whatever kind of tensor you have easy um so um you know this can be a source of confusion
[2615.64s -> 2621.16s]  um since we're doing a computer science course for the answers in the assignment we expect you
[2621.16s -> 2627.48s]  to follow the shape convention so you know if you're working out the derivatives with respect
[2627.48s -> 2634.76s]  to some matrix it should be shaped like a matrix with the same parameters um but you know you may
[2634.76s -> 2639.64s]  well want to think about jacobean forms and computing your answers i mean there are sort of
[2639.64s -> 2645.80s]  two ways to go about doing this one way of doing it is to sort of work out all the math
[2645.88s -> 2654.20s]  using jacobeans our math 51 and at the end just to reshape it so it fits into the same shape as the
[2654.20s -> 2660.52s]  parameters according to our shape convention i mean the other way is to sort of do each
[2660.52s -> 2665.48s]  stage following the shape convention but then you sort of have to be game to sort of
[2665.48s -> 2671.72s]  reshape things as needed by sort of doing um transposing to have things work out at the
[2671.72s -> 2677.24s]  different stages okay that was my attempt to quickly review the math
[2680.60s -> 2688.20s]  most people are still here um i will now go on to the second half and um go on to the um
[2688.76s -> 2696.76s]  how we do the computation right so you know so most of um yeah so the famous thing that powers
[2696.76s -> 2703.24s]  neural networks is the back propagation algorithm so the back propagation algorithm
[2703.24s -> 2709.56s]  is really only two things you know its invention made people famous because it gave
[2709.56s -> 2716.28s]  an effective learning algorithm but you know at a fundamental level the back propagation algorithm
[2716.28s -> 2724.28s]  is only two things thing one is you use the chain rule you do calculus of complex functions
[2724.28s -> 2732.28s]  and thing two is um you store intermediate results so you never recompute the same stuff
[2732.28s -> 2740.04s]  again that's all there is to the um back propagation algorithm and so let's just go through
[2740.04s -> 2748.12s]  that so if we're computationally wanting to deal um with you know functions and doing back
[2748.12s -> 2754.92s]  propagation we can think of them as being represented as a graph and in some way or
[2754.92s -> 2763.08s]  another um this kind of graph is being used inside your neural network framework so here is
[2763.08s -> 2767.96s]  here is a re-representation of my little neural network for finding whether the word at
[2767.96s -> 2775.40s]  the center is a location so i'm taking the x vector input i'm multiplying it by w
[2775.40s -> 2781.80s]  i'm adding b to it i'm putting it through the non-linearity and then i'm doing the dot product
[2781.80s -> 2788.92s]  with my vector u right so that was my computation and so the source nodes are the
[2788.92s -> 2798.76s]  inputs in this graph the interior nodes then the operations i do and so then the edges that
[2798.76s -> 2807.32s]  connect those together then pass along the results of each operation so i pass along w x to the
[2807.32s -> 2813.72s]  addition function with b then i that gives me z that i pass through the non-linearity which
[2813.72s -> 2821.40s]  gives me h which i then um dot product with the u to get s okay so i do precisely this
[2821.40s -> 2828.36s]  computation and this is referred to as forward propagation or the forward pass of a neural network
[2828.44s -> 2837.40s]  so um the forward pass just calculates functions okay but then once we've done that what we want
[2837.40s -> 2846.28s]  to do is then work out gradients so we can do gradient-based learning and so that part
[2846.28s -> 2854.52s]  is then referred to as back propagation or the backward pass and then we run things backward
[2854.52s -> 2859.88s]  so for running things backward we're going to use the same graph and we're going to backwards
[2859.88s -> 2868.20s]  pass along at gradients and so we start at the right hand side and we have ds ds so ds ds is
[2868.20s -> 2876.52s]  just one because you know um you change s you've changed s and then what we want to do is sort
[2876.52s -> 2888.44s]  of then work further back so we can work out dsdh dsdz dsdb dsdw dsdx as we work back um and so
[2888.44s -> 2896.92s]  this is um the what we want to work out with gradients um and so how are we going to do that
[2896.92s -> 2903.80s]  well if we look at a single node so for example our um our non-linearity node but any
[2903.80s -> 2912.36s]  node where h equals f of x what we're going to have is an upstream gradient um dsdh and
[2913.00s -> 2920.20s]  what we want to do is calculate the downstream gradient of the next variable down the dsdz
[2921.00s -> 2928.12s]  and the way that we're going to do that is we're going to say well let's look at f what is
[2928.20s -> 2935.56s]  f's gradient and that's going to be our local gradient and then this is immediately what gives
[2935.56s -> 2945.00s]  us the chain rule that dsdz is going to be the product of our upstream gradient dsdh times
[2945.00s -> 2952.44s]  the dh dz the local gradient that we calculate at that node so downstream gradient
[2952.44s -> 2956.92s]  equals upstream gradient times local gradient
[2962.12s -> 2968.60s]  oh yeah that's what that's what it says when i press that again okay so this is the sort of
[2968.60s -> 2977.00s]  the single the single input single output case though those inputs might be vectors or matrices
[2977.08s -> 2983.72s]  or something like that um we then have sort of more complex graph cases um so
[2985.08s -> 2990.28s]  i think i should have retitled this slot oh yeah so it's still so sorry so the next
[2990.28s -> 2998.20s]  case is for our node it might have multiple inputs so this is where we're calculating um wx
[2998.20s -> 3004.60s]  so in that case we still have an ups we have a single upstream gradient and then what we're
[3004.60s -> 3011.32s]  going to do is we want to calculate the downstream gradient with respect to each
[3011.32s -> 3017.64s]  input and the way we're going to do that is we're going to work out the local gradient with
[3017.64s -> 3024.04s]  respect to each input and then we're going to do the same kind of multiplication of upstream
[3024.04s -> 3031.08s]  gradient times local gradient with respect to each input again um chain rule
[3031.96s -> 3040.04s]  okay um so here's a little example of this so i'm this isn't really uh the kind of thing you
[3040.04s -> 3045.96s]  normally see in a neural network but it's an easy example so f of x y z is going to be
[3045.96s -> 3055.48s]  x plus y times the max of y z and we've got current values of x y and z of one two and
[3056.12s -> 3062.44s]  zero respectively so here's our little computation graph um and so for forward
[3062.44s -> 3067.64s]  propagation you know we're going to do this addition we're going to do this max function
[3067.64s -> 3074.04s]  and then we're going to multiply the two and that gives us the value of f um so we can run
[3074.04s -> 3080.68s]  that with the current values of x y and z and this is what we get so the max of two and zero
[3080.68s -> 3088.76s]  is two addition is three the answer is six okay so then after having done that we run
[3088.76s -> 3094.76s]  the backward propagation and yeah so this procedure you know it's not actually special
[3094.76s -> 3099.56s]  to neural networks right you can use it for any piece of math if you want to just run your math
[3099.56s -> 3106.68s]  um on pi torch rather than um working it out in your head or with Mathematica um okay so now
[3106.68s -> 3116.60s]  we work out um backwards so we want to know the local gradient so da dz is going to be one
[3118.28s -> 3125.32s]  sorry i said that one da dx is going to be one so a equals x plus y da dy equals one um
[3125.32s -> 3131.24s]  for the max function that's going to depend on which of the two is larger because it's going
[3131.24s -> 3137.56s]  to have a slope of one for the one that's the biggest and zero for the one that's the smallest
[3138.68s -> 3146.12s]  and then for the product that's like what we saw with vectors that df da is going to be b
[3146.12s -> 3154.20s]  and df db is going to be a so those are all our local gradients and so then we can use those
[3154.20s -> 3163.88s]  to calculate out the derivatives so df df is one we then multiply that by the two um local
[3163.88s -> 3176.52s]  gradients um that are calculated um for a and b so that gives us um two and three where you're
[3176.52s -> 3184.68s]  swapping over the numbers then for the max um that we're having the one that is biggest
[3185.24s -> 3193.16s]  um we're taking the upstream times one so it gets three the other one gets zero and then
[3193.16s -> 3199.24s]  for the plus we're just sending the gradient down in both directions and so both of them come
[3199.24s -> 3211.08s]  out as two um and so that gives us the fdx so the final function value is two the fdy we're taking
[3211.08s -> 3217.64s]  the three and adding the two i'll mention that again in a minute which gives us five and then
[3217.64s -> 3225.24s]  df dz is zero um and we should be able to again be able to quickly check that we've got
[3225.24s -> 3236.04s]  this right right so um if we consider you know the slope around um z as you change z a little so
[3236.04s -> 3243.40s]  z is zero if we make z 0.1 that makes absolutely no difference to what the computed
[3243.40s -> 3252.68s]  function value is um so the gradient there is zero that's correct um so if i change up the top
[3252.68s -> 3262.52s]  if i change x a little bit right if i change x to 1.1 then i'll be calculating um 1.1 plus 2
[3262.52s -> 3274.04s]  is 3.1 um and then i'll be taking the max which is 2 and i'll be calculating 5.1 um
[3275.00s -> 3285.00s]  and so wait no i didn't go wrong oh times two right i i didn't do the multiplication right
[3286.28s -> 3294.12s]  um sorry yeah so we get the 3.1 that's multiplied by two that gives us 6.2
[3294.12s -> 3301.56s]  so a change of 0.1 in the x has moved things up by 0.2 so that corresponds to the gradient
[3301.64s -> 3308.68s]  being two and so then the final case is well what if we change y to um
[3309.48s -> 3320.76s]  so y started off as a two and made it 2.1 then we're going to get 2.1 multiplied by one is 2.1
[3320.76s -> 3327.80s]  6.1 and right and then we've got the 2.1 here
[3329.56s -> 3336.76s]  that oh sorry i keep doing this wrong 2.1 plus 1 equals 3.1 and then we've got 2.1 is the max
[3336.76s -> 3348.44s]  so we've got 2.1 times 3.1 and that comes out to be 6.51 so it's approximately gone up by
[3348.44s -> 3355.72s]  so our 0.1 difference has gone up to approximately 0.5 it's just an estimate um and so that
[3355.72s -> 3362.44s]  corresponds to the gradient being five right we get this five times multiplication of our changes
[3363.64s -> 3369.88s]  okay um and so that the fact that illustrates the fact that the right thing to do is when you
[3369.88s -> 3379.00s]  have outward branches in your um computation graph and you're running the um back propagation
[3379.00s -> 3388.52s]  that what you do is you sum the gradients right um so that for this case we had y being
[3388.52s -> 3395.72s]  um the y is sort of going into these two different things in our previous chart so once
[3395.72s -> 3401.64s]  we've worked out the upstream gradients we sum them to get the total gradient and so that's what
[3401.64s -> 3407.56s]  we did back here we had two outward things and we sort of took these calculated upstream
[3407.56s -> 3413.48s]  gradients and two and three and we just summed them to get five and that gave the right answer
[3413.48s -> 3427.96s]  um okay um and so you can think about that um for the sort of just generally how the sort
[3427.96s -> 3434.68s]  of things to think about as sort of gradients move around in these pictures so that when we have
[3434.68s -> 3442.68s]  a plus operation that um plus just sort of distributes gradient so the same gradient
[3442.68s -> 3450.20s]  that's the upstream gradient goes to each input um when you have a max it's kind of like a router
[3450.20s -> 3457.64s]  of gradient so the max is going to send the gradient to one of the inputs and send nothing
[3457.64s -> 3463.80s]  at all the other inputs um and when you have a multiplication it's a little bit funky
[3463.80s -> 3469.80s]  because you're sort of um doing this sort of switching of the forward coefficient so you're
[3469.80s -> 3478.04s]  taking the upstream gradient multiplied by the opposite um forward coefficient gives you your
[3478.04s -> 3487.08s]  um downstream gradient okay um so we kind of have this systematic way of being able to sort
[3487.08s -> 3496.76s]  of forward pass calculate the values of functions then run this backward to work out the gradients
[3496.76s -> 3504.36s]  heading down the network and so the main other thing of the back propagation algorithm
[3504.36s -> 3510.44s]  is just that we want to do this efficiently so the wrong way to do it would be to say well
[3510.44s -> 3519.88s]  gee i want to calculate dsdb dsdw dsdx dsdu so let me start doing those one at a time
[3519.88s -> 3526.12s]  and when i've done them all i will stop because that means if you first calculate dsdb
[3526.12s -> 3533.48s]  you do all of the part that's in blue um but then if you went on to dsdw
[3534.20s -> 3540.68s]  um you'd be calculating all the part in red and well just as we saw in the math part
[3540.68s -> 3547.00s]  when we were doing it as math um these parts are exactly the same you're doing exactly the
[3547.00s -> 3554.52s]  same computations so you only want to do those that part once and work out this upstream
[3554.52s -> 3561.00s]  gradient or error signal that is being then sort of calculated and is then being shared
[3561.00s -> 3567.56s]  so the picture that we want to have is you're doing together the shared part and then you're
[3567.56s -> 3576.12s]  only sort of doing separately the little bits um that you need to do okay um boy i seem to have
[3576.12s -> 3581.00s]  been rushing through today and i'm going to actually end early unless anyone is going to
[3581.00s -> 3587.40s]  slow me down but i do have uh just a few more slides um to go through um yeah so
[3587.96s -> 3595.40s]  the sort of generalization of this as an algorithm is you know in the general case
[3595.40s -> 3601.88s]  you know so we normally have these sort of neural network layers and matrices which you
[3601.88s -> 3608.76s]  represent as vectors and matrices um and you know it's sort of nice and clean and it
[3608.76s -> 3616.12s]  looks like um doing that in calculus class i mean strictly speaking that isn't necessary
[3616.12s -> 3622.04s]  so the algorithm for forward propagation backward propagation that i've outlined that
[3622.04s -> 3628.60s]  you can have it work in a completely arbitrary computation graph providing it's a dag that
[3628.60s -> 3635.24s]  doesn't have cycles in it um so the general algorithm is well you've got a whole bunch of
[3635.24s -> 3643.24s]  variables that depend on other variables there's some way in which we can sort them so that each
[3643.24s -> 3648.92s]  variable only depends on variables to the left of it so that's referred to as a topological
[3648.92s -> 3655.88s]  sort of the outputs and so that means there's a way we can do a forward pass where we're
[3655.88s -> 3662.52s]  calculating um variables in terms of ones that have already been calculated but you know if you
[3662.52s -> 3668.44s]  want to have some extra wonky arcs so it's not like nice matrix multiplies or anything we're
[3668.44s -> 3673.88s]  totally allowed to um do that or we can have things not fully connected right so there's no
[3673.88s -> 3680.52s]  connections across here right we can have an arbitrary computation graph um and so that gives
[3680.52s -> 3687.64s]  us our forward propagation and then once we've done the forward propagation we can initialize
[3687.64s -> 3697.40s]  the output gradient as as one and then we're going to visit the nodes in reverse order and
[3697.40s -> 3703.32s]  at for each node we're going to compute a gradient by using the upstream gradient and
[3703.32s -> 3708.68s]  the local gradient to compute the downstream gradient and so then we can head back down the
[3708.68s -> 3715.88s]  computation graph and work out all of the downstream gradients and so the crucial thing
[3715.88s -> 3728.20s]  to notice is that if you do it correctly um that working out um the the gradients has the same
[3728.20s -> 3735.40s]  big O complexity as working out the forward calculation right so that if you're doing more
[3735.40s -> 3740.44s]  you know in if in terms of big O terms right you might have different functions depending
[3740.44s -> 3746.12s]  on what the derivatives are but in big O terms if you're doing more work in the backward pass
[3746.12s -> 3751.24s]  than you're doing in the forward pass that means that you're somehow failing to do this
[3752.84s -> 3757.00s]  efficient computation and that you're recomputing some of your work
[3758.52s -> 3767.16s]  okay um so because we have such a good algorithm here you should be able to just work out the
[3767.16s -> 3773.16s]  backward pass automatically and that that gets referred to as automatic differentiation
[3773.16s -> 3781.48s]  so if you had the symbolic form of what you're calculating with your forward pass
[3782.76s -> 3789.56s]  you should just be able to say yo computer can you work out the backward pass for me
[3789.56s -> 3795.48s]  and you know kind of mathematical like it could look at the symbolic form of all of your functions
[3795.96s -> 3805.72s]  um work out their derivatives and do the entire thing for you um so early on there was a
[3805.72s -> 3813.88s]  pioneering um deep learning framework theano principally from the um university of montreal
[3813.88s -> 3820.84s]  which attempted to do precisely that that you had the entire forward pass computation started
[3820.84s -> 3828.44s]  in symbolic form and it just did the entire thing for you and worked out the backward pass
[3828.44s -> 3838.28s]  automatically um but you know somehow that sort of proved to be um too heavyweight or um
[3838.28s -> 3843.96s]  hard to deal with different things or people just like to write their own python or whatever
[3843.96s -> 3852.84s]  it is um so that idea did not fully succeed and so what in practice all of the current main
[3852.84s -> 3859.80s]  frameworks have fallen back on is something that's actually less automated than that so it's sort
[3859.80s -> 3864.36s]  of like we've gone backwards in time but the software has gone a lot better really it's a lot
[3864.92s -> 3873.16s]  stabler and faster um so all of the modern deep learning frameworks sort of say look i will
[3873.16s -> 3878.76s]  manage the computation graph for you and i can run the forward propagation pass and the backward
[3878.76s -> 3886.12s]  propagation path but you're going to have to work out the local um derivatives yourself um so
[3886.12s -> 3893.64s]  if you're if you're putting in a layer or putting in um you know a function like an
[3893.64s -> 3901.80s]  activation function in the in a neural network your class your python class that represents that
[3901.80s -> 3909.72s]  you're going to have to tell me what the forward um computation is and what the local gradient is
[3909.72s -> 3915.40s]  and i'm just going to call your local gradient and assume it's correct um so there's
[3915.40s -> 3923.64s]  a bit more that has to be done manually so so the path is automated then um is that you know
[3923.64s -> 3931.00s]  when you know not precisely this code obviously but roughly you know inside the deep learning
[3931.00s -> 3937.88s]  software um it's computing with a computation graph and it's got a forward and a backward and
[3937.88s -> 3944.84s]  it's doing what i presented on the pictures before so for the forward um pass it's
[3944.84s -> 3951.16s]  topologically sorting all the nodes of the graph and then it's going through them and for
[3951.16s -> 3958.68s]  each node in the graph it's calling its forward function which will be able to compute its local
[3958.68s -> 3964.92s]  value in terms of its inputs which have already been calculated because it's topologically sorted
[3964.92s -> 3970.60s]  and then it's um running the backward path and the backward path you're reversing your
[3970.60s -> 3977.48s]  topological sort and then you're working out um the gradient which is going to be the
[3977.48s -> 3983.64s]  multiplication of the upstream error signal times your local gradient and so what a human
[3983.64s -> 3992.12s]  being has to implement um is that for anything whether it's a single gate here's a multiply gate
[3992.12s -> 3999.24s]  or a neural network layer you have to implement a forward pass and a backward pass so here for
[3999.24s -> 4005.64s]  my baby example since we're just doing multiplication my forward pass is that i just
[4005.64s -> 4012.92s]  multiply the two numbers and return it so i'm specifying that for the local node and then
[4012.92s -> 4020.60s]  the other part is that i have to work out those gradients and well we sort of know how to do that
[4020.60s -> 4026.20s]  because that's the examples that we've been doing here um but notice that there's sort of a trick
[4026.20s -> 4035.00s]  right for what i've got now you kind of can't write down what the gradients are because
[4035.00s -> 4041.64s]  you know what these because you know backward is just taking as an input the upstream gradient
[4041.64s -> 4047.72s]  and you can't work out what the downstream gradients are going to be unless you know what
[4048.60s -> 4055.40s]  function values you're calculating it at so the standard trick that all which is how everyone
[4055.40s -> 4061.24s]  writes this code is you're relying on the fact that the forward is being calculated before
[4061.24s -> 4069.40s]  the backward and so your forward method um shoves into some local variables of the class
[4069.40s -> 4075.00s]  what the values of the inputs are and then you have them available um so when you get
[4075.00s -> 4083.08s]  to the backward pass you can do what we did before um that um the dx is going to be
[4083.08s -> 4091.00s]  the upstream error signal times the opposite input and um and similarly for dy and that's
[4091.00s -> 4100.68s]  going to give us the answer okay um just two last things then to mention yeah so doing this
[4100.68s -> 4108.60s]  um your you need to write um you need to get the math right for what's the derivative
[4108.60s -> 4116.52s]  of your function so you get the right backward calculation so the standard way to check that
[4116.52s -> 4124.84s]  you've got the right backward calculation is to do manual gradient checking with numeric gradients
[4125.40s -> 4133.48s]  so the way you do that is you sort of like for the couple of examples i did when i said oh
[4133.48s -> 4140.04s]  let's check it by for going from one to one point one what should the slope be approximately
[4140.04s -> 4145.24s]  we're going to do that in an automated way and so we're going to say at the value x
[4145.24s -> 4153.00s]  let's estimate what the gradient should be and the way to do that is to pick a small h there isn't
[4153.00s -> 4156.92s]  a magical number because it depends on the function but typically you know if neural
[4156.92s -> 4164.20s]  networks around 10 to the minus 4 is good um a small h and work out the function value
[4164.20s -> 4174.28s]  i the forward pass at x plus h and x minus h divided by the run which is 2h and that should
[4174.28s -> 4180.36s]  give you an estimate of the slope what the backward pass is calculating and you want those
[4180.36s -> 4186.84s]  two numbers to be approximately equal you know within some 10 to the minus 2 of each other
[4186.84s -> 4194.20s]  and then probably you're calculating the gradient right and if they aren't equal um that um
[4195.32s -> 4202.60s]  you probably have made a mistake um yeah so um note that this formula for the version i did
[4202.60s -> 4211.72s]  for my examples i just compared to x with x plus h right i did a one-sided estimate which is
[4211.72s -> 4217.56s]  normally what you get taught in a math class if you're doing this to check your gradients
[4217.56s -> 4223.32s]  numerically you're far far better off doing this two-sided estimate because it's much more
[4223.32s -> 4230.84s]  accurate and stable when you're doing it equally around both sides of your h um yeah so this
[4230.84s -> 4238.44s]  looks easy to do um if if this was just so good why doesn't everyone do this all the time forget
[4238.44s -> 4246.04s]  about calculus um you know the reason you don't want to do this is that doing this is incredibly
[4246.04s -> 4252.44s]  slow right because you have to repeat this computation for every parameter of your model
[4252.44s -> 4259.32s]  that you're not getting the kind of speed ups you're getting from the back propagation algorithm
[4259.32s -> 4263.00s]  but you know it's useful for checking your implementation is correct you know in the old
[4263.00s -> 4269.48s]  days before frameworks like pytorch um you know we used to write everything by hand and people
[4269.48s -> 4275.24s]  often got things wrong um but nowadays you know it's less needed but it's good to check that if
[4275.24s -> 4282.76s]  you've implemented your own new layer that it's doing the right thing okay um yeah so that's
[4282.76s -> 4287.96s]  everything that we need to know about neural nets back propagation is the chain rule applied
[4287.96s -> 4294.76s]  efficiently forward pass is just function application backward class is chain rule
[4294.76s -> 4303.72s]  applied inefficiently um so you know uh we're going to inflict pain on our students by making
[4303.72s -> 4310.52s]  them do some math and calculate some of these things and um do the homework and i know that'll
[4310.52s -> 4317.96s]  be harder for some of you than others um you know that in some sense you don't actually need
[4317.96s -> 4321.96s]  to know how to do this the beauty of these modern deep learning frameworks is they'll do
[4321.96s -> 4327.08s]  it all for you they predefine common layer types and you can just plug them together like
[4327.08s -> 4333.00s]  pieces of lego and they'll be computed right and this is precisely the reason that high school
[4333.00s -> 4338.68s]  students across the country in the world can now do deep learning projects for their science
[4338.68s -> 4342.92s]  fairs because you don't actually have to understand any of this math um you can just
[4342.92s -> 4349.80s]  use what's given to you um but you know um we kind of uh want to hope that you actually do
[4349.80s -> 4356.12s]  understand something about what's going on under the hood and how neural networks work so
[4356.12s -> 4362.36s]  therefore you make you suffer a little bit and of course you know if you sort of wanting to
[4362.36s -> 4368.28s]  look at and understand more complex things you need to have some sense of what's going on
[4368.28s -> 4373.64s]  so later on when we get on to a current neural networks we'll talk a bit about things like
[4373.64s -> 4378.60s]  exploding and vanishing gradients and if you want to have some understanding about well why things
[4378.60s -> 4383.88s]  aren't working and things are going wrong um then you sort of want to know what it's actually
[4383.88s -> 4389.32s]  calculating rather than just thinking it's all a black box magic and so that's why we hope to
[4389.32s -> 4396.68s]  have uh taught something about that okay i think i'm done if the audience is sufficiently stunned
[4396.68s -> 4407.40s]  um and we can stop for the day okay thank you
