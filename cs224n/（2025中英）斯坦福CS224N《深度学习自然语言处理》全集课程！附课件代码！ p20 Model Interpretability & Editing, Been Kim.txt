# Detected language: en (p=1.00)

[0.00s -> 13.12s]  Today, I'm delighted to introduce our final guest speaker, Bean Kim. Bean Kim is a staff
[13.12s -> 18.44s]  research scientist at Google Brain. If you're really into Googleology, you know those
[18.44s -> 23.08s]  funny words at the beginning like staff sort of says how senior you are, and that
[23.08s -> 31.12s]  means that Bean's a good research scientist. So I discovered at lunch today that Bean
[31.12s -> 37.56s]  started out studying mechanical engineering at Seoul National University, but she moved
[37.56s -> 41.64s]  on to, I don't know if it's better things or not, but she moved on to computer
[41.64s -> 48.12s]  science and did her PhD at MIT. And there she started working on the
[48.12s -> 53.56s]  interpretability and explainability of machine learning models. I think she'll be talking
[53.56s -> 59.84s]  about some different parts of her work, but a theme that she's had in some of her recent
[59.84s -> 66.68s]  work that I find especially appealing as an NLP person is the idea that we should
[66.68s -> 73.82s]  be using higher level human interpretable languages for communication between people
[73.82s -> 80.46s]  and machines. So welcome Bean, looking forward to your talk, and go for it.
[80.46s -> 90.38s]  Thank you. Thanks for having me. It's an honor to be here. It's the rainiest
[90.38s -> 95.34s]  Stanford I've ever seen. Last night, I got here last night, but then I live in Seattle,
[95.34s -> 99.92s]  so this is pretty common. So I still was able to see the blue sky today. I was like,
[99.92s -> 105.52s]  this works. I really like it here. So today I'm going to share some of my dreams, chasing
[105.52s -> 112.32s]  my dreams to communicate with machines. So if you're in this class, you probably agree,
[112.32s -> 117.64s]  you don't have to, that large language models and generator models are pretty cool.
[117.64s -> 123.16s]  They're impressive, but you may also agree that they're a little bit frightening.
[123.16s -> 127.96s]  Not just because they're impressive, they're doing a really good job, but also we're
[128.00s -> 133.88s]  not quite sure where we're going with this technology. In 10 years out, will we look
[133.88s -> 139.48s]  back and say that technology was net positive, or we will say, ah, that was catastrophic.
[139.48s -> 146.40s]  We didn't know that that would happen. Ultimately, what I would like to do, or maybe
[146.40s -> 152.52s]  hopefully what we all want to do is to have this technology benefit us humans.
[152.52s -> 157.76s]  I know in 10 years time, or maybe, well, 20 years or earlier, he's going to ask me,
[157.76s -> 162.08s]  he's going to be like, mom, did you work on this AI stuff? I watched some of your
[162.08s -> 169.24s]  talks. And did you know that how this will profoundly change our lives? And what did
[169.24s -> 174.72s]  you do about that? And I have to answer that question, and I really hope that I have
[174.72s -> 184.44s]  some good things to say to him. So my initial thought, and still so, or current
[184.44s -> 189.92s]  thought, is that if we want our ultimate goal to benefit humanity, why not directly
[189.92s -> 196.96s]  optimize for it? Why wait? So how can we benefit? There's lots of different ways
[196.96s -> 202.92s]  we can benefit, but one way we can benefit is to treat this like a colleague, you know,
[202.92s -> 208.60s]  a colleague who are really good at something. This colleague is not perfect, but it's good
[208.60s -> 213.08s]  at something enough that you want to learn something from them.
[213.12s -> 217.68s]  One of the differences though, in this case, is that this colleague is kind of weird. This
[217.68s -> 223.40s]  colleague might have very different values. It might have very different experiences in
[223.40s -> 229.76s]  the world. It may not care about surviving as much as we do. Maybe mortality isn't
[229.76s -> 237.36s]  really a thing for this colleague. So you have to navigate that in our conversation.
[237.36s -> 240.80s]  So what do you do when you first meet somebody? There's someone so different. What
[240.80s -> 246.44s]  do you do? You try to have a conversation to figure out what, how do you do what you
[246.44s -> 253.64s]  do? How are you solving decades old protein folding problem? How are you beating the world
[253.64s -> 259.72s]  gold champion so easily, what it means? Are you using the same language, the science
[259.72s -> 265.18s]  knowledge, the language that we use, atoms, molecules, or do you think about the world
[265.18s -> 273.66s]  in a very different way? And more importantly, how can we work together? I have one alien
[273.66s -> 279.18s]  that I really want to talk to, and it's AlphaGo. So AlphaGo beat world gold champion
[279.18s -> 283.86s]  Isidoro in 2016. Isidoro is from South Korea. I'm from South Korea. I watched
[283.86s -> 288.78s]  every single match. It was such a big deal in South Korea and worldwide, I hope.
[288.78s -> 294.34s]  And in one of the matches, AlphaGo played this move called move 37. How many people
[294.34s -> 301.38s]  watched AlphaGo match, matches? And how many people remember move 37? Yeah, a few
[301.38s -> 306.62s]  people, right? And I remember the Naidan commentator who's been like talking a lot throughout
[306.62s -> 314.78s]  the matches suddenly got really quiet. And he said, Hmm, that's a very strange move.
[314.78s -> 320.50s]  And I knew then that something really interesting has just happened in front of my eyes, that
[320.50s -> 323.94s]  this is going to change something this AlphaGo has made, something that we're going
[323.94s -> 329.02s]  to remember forever. And sure enough, this move turned around the game for AlphaGo and
[329.02s -> 335.66s]  leading AlphaGo to win one of the matches. So gold players today continue to analyze
[335.66s -> 340.54s]  this move and still discuss, and people talk about this is not the move a human would
[340.54s -> 351.14s]  phantom. So the question is, how did AlphaGo know this is a good move? My dream is to
[351.14s -> 357.22s]  learn something new by communicating with machines and having a conversation, and such
[357.22s -> 362.34s]  that humanity will gain some new angle to our important problems like medicine and
[362.34s -> 368.66s]  science and many others. And this is not just about discovering new things. If you
[368.66s -> 374.78s]  think about reward hacking, you have to have a meaningful conversation with somebody
[374.78s -> 380.70s]  to truly figure out what their true goal is. So in a way, solving this problem is
[380.70s -> 389.58s]  a superset of solving AI safety too. So how do we have this conversation? Conversation
[389.58s -> 395.06s]  assumes that we share some common vocabulary between that exchange to exchange meaning
[395.06s -> 399.58s]  and ultimately the knowledge. And naturally, a representation plays a key role in this
[399.58s -> 405.94s]  conversation. On the left, and we can visualize this, on the left, we say this is a representational
[405.94s -> 411.70s]  space of what humans know, on the right, what machines know. Here in the left circle,
[411.70s -> 415.30s]  there will be something like this dog is fluffy. And you know what that means because
[415.30s -> 421.86s]  we all share somewhat similar vocabulary. But on the right, we have something like
[421.86s -> 431.18s]  move 37, where we humans yet to have a representation for. So how do we have this
[431.18s -> 436.50s]  conversation? Our representational space needs overlap. And the more overlap we have,
[436.50s -> 441.74s]  the better conversation we're going to have. Humans are all good at learning new things,
[441.74s -> 447.22s]  like here everyone is learning something new. So we can expand what we know by learning new
[447.22s -> 453.66s]  concepts and vocabularies. And doing so, I believe, will help us to build machines that
[453.66s -> 461.34s]  can better align with our values and our goals. So this is the talk that I gave. If
[461.34s -> 464.46s]  you're curious about some of the work we're doing towards this direction, I highly
[464.46s -> 469.10s]  recommend it. It's a YouTube video. I clear a keynote, half an hour. You can fast, do
[469.10s -> 474.70s]  a fast feed. But today I'm going to talk more about my hopes and dreams. And hopefully
[474.70s -> 481.98s]  at the end of the day, your hopes and dreams too. So first of all, I'm just going to
[481.98s -> 486.74s]  set the expectation. So at the end of this talk, we still don't know how the
[486.74s -> 494.66s]  move 37 is made. Sorry. That's going to take a while. In fact, the first part
[494.66s -> 501.94s]  of this talk is going to be about how we move backwards in this progress, in terms
[501.98s -> 507.54s]  of making this progress in our journey. And still very, very small portion of our entire
[507.54s -> 513.66s]  journey towards understanding move 37. And of course, this journey wouldn't be like
[513.66s -> 519.34s]  a singular path. There will be lots of different branches coming in. Core ideas like
[519.34s -> 524.34s]  transformer helped many domains across. They will be similar here. So I'm going
[524.34s -> 530.18s]  to talk in the part two, some of our work on understanding emerging behaviors and reinforcement
[530.18s -> 535.82s]  learning. And all the techniques that I'm going to talk about is going to be in principle
[535.82s -> 545.70s]  applicable to NLP. So coming back to our hopes and dreams, move 37. So let's first
[545.70s -> 552.22s]  think about how we might realize this dream. And taking a step back, we have to ask, do
[552.22s -> 558.74s]  we have tools to first estimate what even machines know? There has been many development
[558.78s -> 565.54s]  in machine learning last decade now to develop tools to understand and estimate this purple
[565.54s -> 571.98s]  circle. So is that accurate? Unfortunately, many recent research showed that there's
[571.98s -> 580.42s]  a huge gap between what machines actually know and what we think the machines know.
[580.42s -> 585.88s]  And identifying and bridging this gap is important because these tools will form bases
[585.88s -> 592.92s]  for understanding that move 37. So what are these tools? How many people familiar with
[592.92s -> 598.60s]  saliency maps? A lot, but you don't have to explain what it is. So saliency map is
[598.60s -> 605.00s]  one of the popular interpretability methods. For simplicity, let's say on ImageNet, you
[605.00s -> 609.52s]  have an image like this. You have a bird. The explanation is going to take a form of
[609.52s -> 616.32s]  the same image, but where each pixel is associated with a number that is supposed
[616.32s -> 624.68s]  to imply some importance of that pixel for prediction of this image. And one definition
[624.68s -> 630.24s]  of that importance is that that number indicates how the function looks like around
[630.24s -> 636.60s]  this pixel. So for example, if I have a pixel xj, maybe around xj, the function moves
[636.60s -> 640.88s]  up like the yellow curve, or function is flat or function is going down like the green
[640.88s -> 648.76s]  curve. And so if it's flat like a blue curve or red curve, maybe that feature is
[648.76s -> 653.48s]  irrelevant to predicting bird. Maybe it's going up, then it's maybe more important
[653.48s -> 657.16s]  because the value of x increases and the function value goes up. Function value here
[657.16s -> 665.64s]  like a prediction value. So let's think about what are the few ways why this gap
[665.68s -> 671.36s]  might exist? There are a few ways. Not exhaustive, they overlap a little bit, but helpful for us to think about.
[671.36s -> 676.60s]  Maybe assumptions are wrong. So this alien, again, these machines that we train, works
[676.60s -> 680.92s]  in a completely different, perhaps completely different representational space, very different
[680.92s -> 686.28s]  experiences about the world. So assuming that it sees the world that we do, just
[686.28s -> 691.60s]  like we do, like having the gestalt phenomenon, there's few thoughts, humans have tendency
[691.60s -> 696.52s]  to connect them. Maybe machines have the tool, maybe not. So maybe our assumptions
[696.52s -> 702.48s]  about these machines are wrong. Maybe our expectations are mismatched. We thought it
[702.48s -> 709.60s]  was doing x, but it was actually doing y. Or maybe it's beyond us. Maybe it's
[709.60s -> 716.58s]  showing something superhuman that humans just can't understand. I'm going to dig deeper
[716.58s -> 723.22s]  into some of our work. This is more recent work. So again, coming back to the earlier
[723.22s -> 729.10s]  story about SalienceMed, we're going to play with some of these methods.
[729.10s -> 737.14s]  Now in 2018, we stumbled upon this phenomenon that was quite shocking, which was that we
[737.14s -> 741.30s]  were actually trying to write some different paper, GAN paper, of course, it ends here.
[741.30s -> 746.42s]  But we were testing something and we realized that train network and untrained network
[746.42s -> 752.46s]  has the same, very similar SalienceMed. In other words, random prediction and meaningful
[752.46s -> 758.14s]  prediction were giving me the same explanation. So that was puzzling. We thought we had a
[758.14s -> 764.22s]  bug, but it turned out we didn't. It actually is indistinguishable qualitatively
[764.22s -> 771.30s]  and quantitatively. So that was shocking. But then we wondered, maybe this is a one-off
[771.30s -> 779.06s]  error. Maybe it still works somehow in practice. So we tested that in a follow-up paper.
[779.06s -> 783.98s]  Okay, what if the model had an error, one of these errors, maybe it has a labeling
[783.98s -> 789.98s]  error, maybe it has a spurious correlation, maybe it had auto-distribution at test time.
[789.98s -> 795.06s]  If we intentionally insert these bugs, can explanation tell us that there's something
[795.06s -> 802.30s]  wrong with the model? It turns out that that's also not quite true. You might think
[802.30s -> 806.38s]  that, oh, maybe a superior correlation. Another follow-up work also showed that this
[806.38s -> 816.38s]  is also not the case. So we were disappointed. But then still we say, you know, maybe there's
[816.38s -> 822.32s]  no theoretical proof of this. Maybe this is, again, a lab setting test. We had grad
[822.36s -> 829.60s]  students to test this system. Maybe there's still some hope. So this is more recent work
[829.60s -> 835.16s]  where we theoretically prove that some of these methods, very popular methods, cannot
[835.16s -> 842.36s]  do better than random. So I'm going to talk a little bit about that. I'm missing
[842.36s -> 847.60s]  Pang Wei in the author list, I just realized. This is also work with Pang Wei.
[847.60s -> 853.48s]  So let's first talk about our expectation. What is our expectation about this tool?
[853.48s -> 860.88s]  Now the original paper that developed this method, IG and Shop, talks about how IG
[860.88s -> 866.64s]  can be used for accounting the contributions of each feature. So what that means is that
[866.64s -> 871.36s]  when the tool assigns zero attribution to a pixel, we're going to say, OK, well, pixel
[871.36s -> 877.64s]  is unused by the function. And that means that f will be insensitive if I perturb this
[877.64s -> 884.80s]  x. And in fact, this is how it's been used in practice. This is a paper published
[884.80s -> 893.84s]  in Nature that uses Shop to figure out the eligibility criteria in a medical trial.
[893.84s -> 899.36s]  What we show in this work is that none of these inferences that seemed pretty natural
[899.36s -> 905.32s]  were true. And in fact, just because popular attribution methods tell you anything about
[905.32s -> 912.60s]  attribution is x, you cannot conclude anything about the actual model behavior.
[912.60s -> 921.80s]  So how does that work? How many people here do theory proof? Well, a few. Great.
[921.80s -> 926.00s]  I'll tell you, I learned about theory proving from this project as well. So I'll tell you
[926.00s -> 931.96s]  like the way that we pursued this particular work is that first think about this problem
[931.96s -> 937.24s]  and we're going to formulate into some other problem that we know how to solve. So in
[937.24s -> 942.24s]  this case, we formulate this as hypothesis testing, because once you formulate in the
[942.24s -> 946.92s]  hypothesis testing, yes or no, there are lots of tools and statistics you can use to
[946.92s -> 953.66s]  prove this. So what is hypothesis? The hypothesis is that I'm a user, I got an attribution
[953.66s -> 959.90s]  value from one of these tools, and I have a mental model of, ah, this feature is important
[959.90s -> 966.34s]  or maybe not important. Then the hypothesis is that whether that's true or not. And
[966.34s -> 972.94s]  what we showed is that given whatever hypothesis you may have, you cannot do better than
[972.94s -> 979.14s]  random guessing, invalidating or invalidating this hypothesis testing. And that means yes,
[979.14s -> 984.82s]  sometimes it's right, but you don't do hypothesis testing if you cannot validate yes or no. You
[984.82s -> 988.58s]  just don't, because like, what's the point of doing it if you just don't know if it's
[988.58s -> 996.78s]  as good as random guessing, right? And the result is that, yes, for this graph, it's
[996.78s -> 1001.38s]  just a visualization of our results. If you plot true negative and true positive,
[1001.38s -> 1005.24s]  and line is random guessing, because this is the worst method, that's the best method,
[1005.24s -> 1011.48s]  all the equal distance is this line, methods that we know, SHOP and IG, all falls under
[1011.48s -> 1020.64s]  this line of random guessing. That's bad news. But maybe, maybe this still works in
[1020.64s -> 1025.54s]  practice for some reason. Maybe there were some assumptions that we had that didn't
[1025.54s -> 1031.88s]  quite meet in the practice. So does this phenomenon hold in practice? The answer is
[1031.88s -> 1038.38s]  yes. We did, we now have more image graphs and more bigger models. But here we test two
[1038.38s -> 1042.88s]  concrete end tasks that people care about in interpretability, or use these methods
[1042.88s -> 1048.12s]  to do recourse or spurious correlation. So recourse, for those who are not familiar,
[1048.12s -> 1053.20s]  is you're getting alone, and you wonder whether if I'm older, I would have a high
[1053.20s -> 1059.32s]  chance of getting alone. So I tweak this one feature and see if my value goes up or
[1059.32s -> 1064.04s]  down. Very reasonable task that people do all the time, pretty significant implication
[1064.04s -> 1071.40s]  socially. So for two of these concrete end tasks, both of them boil down to this
[1071.40s -> 1076.84s]  hypothesis testing framework that I talked about. They're all around the random
[1076.84s -> 1082.96s]  guessing line or worse than random guessing. So you might say, oh, no,
[1082.96s -> 1086.92s]  this is not good. A lot of people are using these tools, so what do we do?
[1086.96s -> 1095.64s]  We have a very simple idea about this. So people like developing complex tools,
[1095.64s -> 1100.44s]  and I really hope you're not one of those people, because a lot of times,
[1100.44s -> 1105.56s]  simple methods work. Who comes raiser? But also, simple methods are elegant.
[1105.56s -> 1110.40s]  There's a reason, perhaps a lot of times, why they work. They're simple.
[1110.40s -> 1115.48s]  You can understand them. They make sense. So let's try that idea here.
[1115.52s -> 1119.72s]  So again, your goal is to estimate a function shape. What do you do?
[1119.72s -> 1123.96s]  Well, the simplest thing you do is you have a point of interest,
[1123.96s -> 1128.36s]  you sample around that point, and evaluate the function around that point.
[1128.36s -> 1131.88s]  If it goes up, maybe function's going up. If it goes down,
[1131.88s -> 1136.48s]  maybe function's going down, right? So that's the simplest way you can
[1136.48s -> 1141.80s]  kind of brute force it. But then the question is, how many samples do we need?
[1141.80s -> 1147.84s]  So here, this is the equation that you're boosting, you're lifting this line upwards that way
[1147.84s -> 1152.80s]  by adding that additional term. It's proportional to the number of samples.
[1152.80s -> 1155.72s]  The more samples you have, the better estimation you have, makes sense.
[1155.72s -> 1158.76s]  And differences in output, how much resolution do you care?
[1158.76s -> 1166.48s]  Do you care 0.1 to 0.2, or do you only care zero slope to, like, slope one?
[1166.48s -> 1171.20s]  That's resolution that you care about, and number of features, of course.
[1171.20s -> 1182.32s]  So if you worry about making some conclusion based on function shape, sample. Easy.
[1182.32s -> 1187.52s]  So can we infer the model behavior using these popular methods?
[1187.52s -> 1193.52s]  The answer is no. And this holds both theory and practice.
[1193.52s -> 1198.32s]  We're currently working on even bigger models to show just, like, again and again,
[1198.32s -> 1201.48s]  empirical evidence that, yes, it just really doesn't work.
[1201.48s -> 1206.16s]  Please think twice and three times before using these methods.
[1206.16s -> 1209.28s]  And also, model-dependent sample complexity.
[1209.28s -> 1213.36s]  If your function is kind of crazy, of course you're going to need more samples.
[1213.36s -> 1217.84s]  So what is the definition? How do we characterize these functions?
[1217.84s -> 1223.64s]  And finally, we haven't quite given up yet, because these methods have a pretty good root in economics
[1223.64s -> 1225.88s]  and Sharpley values and all that.
[1225.88s -> 1231.92s]  So maybe there are a lot narrower conditions where these methods work.
[1231.92s -> 1235.08s]  And we believe such conditions does exist.
[1235.08s -> 1237.12s]  We just have to figure out when.
[1237.12s -> 1242.84s]  Once we figure out what that condition is, then in given function I can test it and say,
[1242.84s -> 1247.44s]  yes, I can use shop here, yes, I can use IG here, or no, I can't.
[1247.44s -> 1251.84s]  That would be still very useful ongoing work.
[1251.84s -> 1255.00s]  Before I go to the next one, any questions?
[1255.00s -> 1256.00s]  Yes?
[1256.00s -> 1264.28s]  Do the findings you have about the J-piece models, like, does it only apply to computer-driven models, or does it apply to NLP-only models?
[1264.28s -> 1269.08s]  Any model that has a function.
[1269.08s -> 1270.24s]  Yeah, very simple.
[1270.24s -> 1276.88s]  Simple, actually, a simple-ish proof that can show simply any function, this holds.
[1276.88s -> 1281.08s]  Any other questions?
[1281.32s -> 1282.32s]  Wonderful.
[1282.32s -> 1283.32s]  Yeah, Chris?
[1283.32s -> 1295.32s]  This may just relate to the last bullet, but it seems like for the last couple of years there have been at least dozens, maybe hundreds of people writing paper using Shapley values.
[1295.32s -> 1310.32s]  I mean, is your guess that most of that work gets invalid, or that a lot of it might be okay because the whatever conditions and where it's all right might often be being there?
[1311.32s -> 1313.80s]  So two answers to that question.
[1313.80s -> 1317.84s]  My hypothesis testing results shows that it's random, right?
[1317.84s -> 1326.64s]  So maybe in the optimistic case, 50% of those papers, you hit it.
[1326.64s -> 1338.24s]  And on the other side, on the second note, even if maybe shop wasn't perfect, maybe it was kind of wrong, but even if it helped human at the end task, whatever that might be,
[1338.28s -> 1347.96s]  doctors to be more efficient, identifying bugs and whatnot, and if they did the validation correctly with the right control testing setup, then I think it's good.
[1347.96s -> 1354.60s]  You know, you figured out somehow how to make this noisy tools together work with human in the loop maybe, and that's also good.
[1354.60s -> 1361.16s]  And I personally really like shop paper, and I'm a good friend with Scott, and I love all his work.
[1361.16s -> 1367.16s]  It's just that I think we need to narrow down our expectations so that our expectations are better aligned.
[1369.16s -> 1374.08s]  All right, I'm going to talk about another work that's kind of similar flavor.
[1374.08s -> 1376.76s]  Now it's an NLP.
[1376.76s -> 1384.44s]  So this is one of those papers, just like the many other papers that we ended up writing, one of those Serendipity papers.
[1384.44s -> 1395.76s]  So initially, Peter came up as an intern, and we thought, we're going to locate ethical knowledge in these large language models, and then maybe we're going to edit them to make them a little more ethical.
[1395.76s -> 1396.96s]  So that was the goal.
[1397.00s -> 1402.84s]  And then we thought, oh, the Rome paper from David Bao, and I also love David's work, and let's use that.
[1402.84s -> 1405.00s]  That's the start of this work.
[1405.00s -> 1411.00s]  But then we start digging into and implementing the Rome, and like things didn't quite line up.
[1411.00s -> 1419.96s]  So we do like sanity check, experiment after sanity check, and we ended up writing completely different paper, which I'm going to about to talk to you about.
[1420.00s -> 1429.24s]  So this paper, the Rome, for those who are not familiar, which I'm going into a little more detail in a bit, is about editing a model.
[1429.24s -> 1436.48s]  So you first locate a knowledge in a model, like the Space Needle is in Seattle, that's a fact, your knowledge.
[1436.48s -> 1439.00s]  You locate them, you edit them.
[1439.00s -> 1443.32s]  Because you can locate them, you can mess with it to edit that fact.
[1443.32s -> 1445.00s]  That's like the whole promise of it.
[1445.00s -> 1451.48s]  In fact, that's a lot of times how localization or editing methods were motivated in the literature.
[1451.48s -> 1456.52s]  But what we show is that this assumption is actually not true.
[1456.52s -> 1462.84s]  And to be quite honest with you, I still don't quite get why this is not related.
[1462.84s -> 1466.48s]  And I'll talk more about this because it's like a big question to us.
[1466.48s -> 1469.52s]  This is a pretty active work.
[1469.52s -> 1479.52s]  So substantial fraction of factual knowledge is stored outside of layers that are identified as having the knowledge.
[1479.52s -> 1484.52s]  And you will see this a little more detail in a bit.
[1484.52s -> 1490.36s]  In fact, the correlation between where the location, where the facts are located,
[1490.36s -> 1497.28s]  and how well you will edit if you edit that location is completely uncorrelated.
[1497.28s -> 1500.80s]  So they have nothing to do with each other.
[1500.80s -> 1506.16s]  So we thought, well, maybe it's the problem with the definition of editing.
[1506.16s -> 1508.96s]  What we mean by editing can mean a lot of different things.
[1508.96s -> 1513.04s]  So let's think about different ways to edit a thing.
[1513.04s -> 1516.28s]  So we tried a bunch of things with a little success.
[1516.28s -> 1522.24s]  We couldn't find an editing definition that actually relates really well with localization methods,
[1522.24s -> 1523.96s]  like in particular with ROM.
[1523.96s -> 1530.80s]  So let's talk a little bit about ROM, how ROM works super briefly.
[1530.80s -> 1534.96s]  There's a lot of details missed out on the slide, but roughly you will get the idea.
[1534.96s -> 1538.52s]  So ROM is Mang et al. 2022.
[1538.52s -> 1541.92s]  They have what's called causal tracing algorithm.
[1541.92s -> 1548.12s]  And the way it works is that you're going to run a model on this particular data set,
[1548.12s -> 1552.80s]  counterfact data set that has this tuple, subject, relation, and object.
[1552.80s -> 1556.28s]  The space needle is located in Seattle.
[1556.28s -> 1561.60s]  And so you're going to have a clean run of the space needle is in Seattle one time.
[1561.60s -> 1565.96s]  You stole every single module, every single value, activations.
[1565.96s -> 1569.68s]  And then in the second run, which they call corrupted run,
[1569.68s -> 1575.32s]  you're going to add noise in the space needle is or the space.
[1575.32s -> 1581.84s]  Then you're going to intervene at every single one of those modules
[1581.84s -> 1586.32s]  as if by copying this module to the corrupted run.
[1586.32s -> 1594.36s]  So as if that particular model was never interrupted, noise was never added to that module.
[1594.36s -> 1600.72s]  So it's a typical intervention case where you pretend everything else being equal.
[1600.72s -> 1606.32s]  If I change just this one module, what is the probability of having the right answer?
[1606.32s -> 1614.96s]  So in this case, probability of the right answer Seattle given that I know it's the model and I intervened on it.
[1614.96s -> 1622.56s]  So at the end of the day, you'll find graph like that where each layer and each token has a score.
[1622.56s -> 1630.96s]  How likely it is if I intervene on that token in that layer, how likely is it that I will recover the right answer?
[1630.96s -> 1636.20s]  Because if I recover the right answer, that's the module, that's the module that stores the knowledge.
[1636.20s -> 1644.28s]  Really reasonable algorithm. I couldn't find technical flaw in this algorithm. I quite like it, actually.
[1644.28s -> 1650.96s]  So, but when we start looking at this using the same model that they use, GPTJ,
[1650.96s -> 1658.08s]  we realize that a lot of these facts, so Rome uses just layer six to edit,
[1658.08s -> 1662.68s]  because that was supposedly the best layer across this data set to edit.
[1662.68s -> 1669.12s]  Most of the factional knowledge is stored in layer six, and they showed editing success and whatnot.
[1669.12s -> 1672.52s]  But we realized the truth looks like the graph on the right.
[1672.52s -> 1675.28s]  So the red line is the layer six.
[1675.28s -> 1681.32s]  Their extension paper called Memit edits multiple layers at the blue region.
[1681.32s -> 1688.56s]  The black bars are histogram of where the knowledge was actually peaked if you test every single layer.
[1688.56s -> 1691.88s]  And as you can see, not a lot of facts fall into that region.
[1691.88s -> 1696.12s]  So in fact, every single fact has different regions where it peaked.
[1696.12s -> 1700.36s]  So layer six, for a lot of facts, weren't the best layer.
[1700.36s -> 1703.16s]  But the editing really worked. It really works.
[1703.16s -> 1705.92s]  And we were able to duplicate the results.
[1705.92s -> 1710.68s]  So we thought, what do we do to find this ethical knowledge?
[1710.68s -> 1713.08s]  How do we find the best layer to edit?
[1713.08s -> 1714.72s]  So that's where we started.
[1714.72s -> 1717.60s]  But then we thought, you know what, take a step back.
[1717.60s -> 1722.52s]  We're going to actually do a sanity check first to make sure that tracing effect,
[1722.52s -> 1729.52s]  the tracing effect is the localization implies better editing results.
[1729.52s -> 1733.72s]  And that's when everything started falling apart.
[1733.72s -> 1736.08s]  So let's define some metrics first.
[1736.08s -> 1741.56s]  The edit success, this is the rewrite score, same score as Rome paper used.
[1741.56s -> 1742.92s]  That's what we used.
[1742.92s -> 1750.00s]  And the tracing effect, this is localization, is probably, you can read the slide.
[1750.00s -> 1758.24s]  So when we plotted the relation between tracing effect and rewrite score, the editing method,
[1758.24s -> 1762.12s]  red line implies the perfect correlation.
[1762.12s -> 1765.28s]  And that was our assumption, that there will be perfectly correlated,
[1765.28s -> 1768.48s]  which is why we do localization to begin with.
[1768.48s -> 1770.88s]  The actual line was yellow.
[1770.92s -> 1772.12s]  It's close to zero.
[1772.12s -> 1776.44s]  It's actually negative in this particular data set.
[1776.44s -> 1777.72s]  That is not even uncorrelated.
[1777.72s -> 1779.96s]  It's anti-correlated.
[1779.96s -> 1781.16s]  And we didn't stop there.
[1781.16s -> 1782.68s]  We were so puzzled.
[1782.68s -> 1784.76s]  We're going to do this for every single layer.
[1784.76s -> 1787.04s]  And we're going to find our square value.
[1787.04s -> 1793.04s]  So how much of the choice of layer versus the localization, the tracing effect,
[1793.04s -> 1797.12s]  explains the variance of successful edits.
[1797.16s -> 1800.00s]  If you're not familiar with R-squared, R-squared is like a,
[1800.00s -> 1803.24s]  think about it as the importance of a factor.
[1803.24s -> 1806.96s]  And it turns out that layer takes 94%.
[1806.96s -> 1810.76s]  Tracing effect is zero, zero, one, six.
[1810.76s -> 1812.00s]  And so we were really puzzled.
[1812.00s -> 1813.20s]  We were scratching our head.
[1813.20s -> 1815.64s]  Why is this true?
[1815.64s -> 1817.52s]  But it was a cross-layer.
[1817.52s -> 1819.24s]  We tried all sorts of different things.
[1819.24s -> 1820.84s]  We tried different model.
[1820.84s -> 1822.12s]  We tried different data set.
[1822.12s -> 1824.60s]  It was all roughly the case.
[1824.60s -> 1828.16s]  So at this point, we contacted David.
[1828.16s -> 1829.56s]  And we started talking about it.
[1829.56s -> 1830.96s]  And we resolved them.
[1830.96s -> 1834.64s]  They acknowledged that this is a phenomenon that exists.
[1834.64s -> 1835.96s]  Yeah, John?
[1835.96s -> 1838.80s]  So apart from the layer, the other way
[1838.80s -> 1841.32s]  which localization can happen is, are you
[1841.32s -> 1842.68s]  looking at the correct token?
[1842.68s -> 1844.88s]  Is that the other corresponding?
[1844.88s -> 1845.68s]  Yeah.
[1845.68s -> 1848.96s]  Yeah, in this graph, the token is in.
[1848.96s -> 1852.80s]  So the added benefit of the rest of the localization
[1852.80s -> 1854.12s]  could only help you look at which
[1854.12s -> 1855.36s]  is the correct subgroup token.
[1855.36s -> 1855.84s]  Is that it?
[1855.84s -> 1856.40s]  Yeah, yeah.
[1856.40s -> 1858.28s]  And so looking at any of the subgroup tokens
[1858.28s -> 1859.92s]  is sort of fine is what I should think of.
[1859.92s -> 1861.68s]  Yeah, yeah, just layer.
[1861.68s -> 1863.52s]  Layer is the most biggest thing.
[1863.52s -> 1865.16s]  That's the only thing you should care.
[1865.16s -> 1866.88s]  If you care about editing, layers.
[1866.88s -> 1868.92s]  In fact, don't worry about localization at all.
[1868.92s -> 1873.28s]  It's extra wasted carbon climate effect.
[1873.28s -> 1876.24s]  So that was our conclusion.
[1876.24s -> 1878.44s]  But then we thought, you know, maybe
[1878.44s -> 1882.52s]  the particular definition of edit that they used in the room
[1882.88s -> 1883.88s]  was maybe different.
[1883.88s -> 1886.92s]  Maybe there exists a definition of editing
[1886.92s -> 1890.48s]  that correlates a lot better with localization.
[1890.48s -> 1891.48s]  Because there must be.
[1891.48s -> 1892.36s]  I'm still puzzled.
[1892.36s -> 1894.56s]  Why is this not correlated?
[1894.56s -> 1898.80s]  So we tried a bunch of different definitions of edits.
[1898.80s -> 1901.04s]  You might inject an error.
[1901.04s -> 1906.28s]  You might reverse the tracing.
[1906.28s -> 1907.72s]  You might want to erase a fact.
[1907.72s -> 1908.96s]  We might want to amplify the fact.
[1908.96s -> 1909.80s]  All these things.
[1909.80s -> 1912.48s]  Like maybe one of these will work.
[1912.48s -> 1913.96s]  It didn't.
[1913.96s -> 1916.00s]  So the graph that you're seeing down here
[1916.00s -> 1919.40s]  is r square value for four different methods.
[1919.40s -> 1921.36s]  And this wasn't just a case for room and memory.
[1921.36s -> 1924.44s]  It was also a case for fine-tuning methods.
[1924.44s -> 1927.52s]  That you want to look at the difference between blue
[1927.52s -> 1931.76s]  and orange bar represents how much the tracing effect
[1931.76s -> 1934.36s]  influenced r square value of the tracing effect.
[1934.36s -> 1936.12s]  As you can see, it's ignorable.
[1936.12s -> 1937.88s]  They're all the same.
[1937.88s -> 1940.72s]  You might feel that, oh, fact forcing, the last one,
[1940.72s -> 1942.20s]  has a little bit of hope.
[1942.20s -> 1945.76s]  But still, compared to the impact of layer,
[1945.76s -> 1948.64s]  choice of layer, it's ignorable.
[1948.64s -> 1952.44s]  So at this point, we said, OK, well,
[1952.44s -> 1955.88s]  we can't locate the ethical knowledge at this project.
[1955.88s -> 1957.84s]  We're going to have to switch the direction.
[1957.84s -> 1961.56s]  And we ended up doing a lot more in-depth analysis on this.
[1964.20s -> 1968.12s]  So in summary, does localization help editing?
[1968.12s -> 1969.52s]  No.
[1969.52s -> 1971.60s]  The relationship is actually zero.
[1971.60s -> 1974.40s]  For this particular editing method,
[1974.40s -> 1976.88s]  from what I know, is pretty state of the art.
[1976.88s -> 1980.52s]  And the counterfact data, it's not true.
[1980.52s -> 1982.88s]  Are there any other editing methods that correlate better?
[1982.88s -> 1983.48s]  No.
[1983.48s -> 1985.76s]  But if somebody can answer this question for me,
[1985.76s -> 1987.40s]  that would be very satisfying.
[1987.40s -> 1990.56s]  I feel like there should still be something there
[1990.56s -> 1992.48s]  that we're missing.
[1992.48s -> 1995.00s]  But causal tracing, I think what it does
[1995.00s -> 1999.00s]  is it reveals the factual information when
[1999.04s -> 2002.00s]  the transformer is passing forward.
[2002.00s -> 2004.92s]  I think it represents where is the fact when
[2004.92s -> 2006.52s]  you are doing that.
[2006.52s -> 2008.56s]  But what we found here is that it has nothing
[2008.56s -> 2010.76s]  to do with editing success.
[2010.76s -> 2012.08s]  Those two things are different.
[2012.08s -> 2015.12s]  And we have to resolve that somehow.
[2015.12s -> 2017.88s]  But a lot of insights that they found in their paper
[2017.88s -> 2020.52s]  is still useful, like the early to mid-range NLP
[2020.52s -> 2022.40s]  representation, last token.
[2022.40s -> 2024.28s]  They represent the factual, something
[2024.28s -> 2026.16s]  we didn't know before.
[2026.20s -> 2030.48s]  But it is important not to validate localization methods
[2030.48s -> 2031.76s]  using the editing method.
[2031.76s -> 2033.04s]  Now we know.
[2033.04s -> 2036.12s]  And maybe not to motivate editing methods
[2036.12s -> 2038.60s]  using via localization.
[2038.60s -> 2041.88s]  Those are the two things now we know that we shouldn't do,
[2041.88s -> 2045.20s]  because we couldn't find a relationship.
[2045.20s -> 2047.84s]  Any questions on this one before I move on to the next
[2047.84s -> 2048.36s]  one?
[2048.36s -> 2057.48s]  You're not shocked by this.
[2057.48s -> 2058.68s]  I'm shocked by this.
[2058.68s -> 2061.64s]  I'm still so puzzled.
[2061.64s -> 2062.80s]  There should be something.
[2062.80s -> 2063.44s]  I don't know.
[2066.56s -> 2068.88s]  All right.
[2068.88s -> 2072.24s]  So in summary of this first part,
[2072.24s -> 2075.32s]  we talked about why the gap might exist,
[2075.32s -> 2078.84s]  what machines know versus what we think machines know.
[2078.84s -> 2081.12s]  There are three ideas.
[2081.12s -> 2082.08s]  Assumptions are wrong.
[2082.08s -> 2083.52s]  Maybe our expectations are wrong.
[2083.52s -> 2085.28s]  Maybe it's beyond us.
[2085.28s -> 2088.48s]  There's a good quote that says, good artists still.
[2088.48s -> 2090.52s]  I think good researchers doubt.
[2090.52s -> 2094.08s]  We have to be really suspicious of everything that we do.
[2094.08s -> 2095.56s]  And that's maybe the biggest lesson
[2095.56s -> 2097.88s]  that I've learned over many years,
[2097.88s -> 2100.60s]  that once you like your results so much,
[2100.60s -> 2102.36s]  that's a bad sign.
[2102.36s -> 2104.68s]  Come back, go home, have a beer,
[2104.68s -> 2105.72s]  go to sleep.
[2105.72s -> 2109.24s]  And next day, you come back and put your paper on your desk
[2109.24s -> 2112.48s]  and think, OK, now I'm going to review this paper.
[2112.48s -> 2113.64s]  How do I criticize this?
[2113.64s -> 2116.32s]  What do I not like about this paper?
[2116.32s -> 2119.36s]  That's one way to criticize your own research,
[2119.36s -> 2122.12s]  and that will improve your thinking a lot.
[2126.28s -> 2129.24s]  So let's bring our attention back to our hopes and dreams.
[2129.24s -> 2131.48s]  It keeps coming back.
[2131.52s -> 2136.56s]  So here, I came to realize maybe instead of just building
[2136.56s -> 2140.84s]  tools to understand, perhaps we need to do some groundwork.
[2140.84s -> 2141.84s]  What do I mean?
[2141.84s -> 2144.64s]  Well, this alien that we've been dealing with,
[2144.64s -> 2149.08s]  trying to generate explanations, seems to be a different kind.
[2149.08s -> 2151.92s]  So maybe we should study them as if they're
[2151.92s -> 2154.80s]  like new species in the wild.
[2154.80s -> 2157.00s]  So what do you do when you observe a new species
[2157.00s -> 2157.88s]  in the wild?
[2157.88s -> 2159.16s]  You have a couple of ways.
[2159.20s -> 2162.20s]  But one of the ways is do observational study.
[2162.20s -> 2165.20s]  So you saw some species in the wild far away.
[2165.20s -> 2167.32s]  First, you just kind of watch them.
[2167.32s -> 2169.60s]  You watch them and see what are they like,
[2169.60s -> 2174.56s]  what are their habitat, what are their values, and whatnot.
[2174.56s -> 2177.48s]  And second way, you can actually intervene and do
[2177.48s -> 2178.76s]  a controlled study.
[2178.76s -> 2182.92s]  So we did something like this with reinforcement learning
[2182.92s -> 2185.44s]  setup.
[2185.44s -> 2189.08s]  I'm going to talk about these two papers, first paper.
[2189.08s -> 2191.36s]  Emergent behaviors in multi-agent systems
[2191.36s -> 2192.72s]  has been so cool.
[2192.72s -> 2196.40s]  Who saw this hide-and-seek video by OpenAI?
[2196.40s -> 2197.28s]  Yeah, it's so cool.
[2197.28s -> 2199.48s]  If you haven't seen it, just Google it and watch it.
[2199.48s -> 2200.48s]  It's so fascinating.
[2200.48s -> 2202.96s]  I'm only covering the tip of an iceberg in this.
[2202.96s -> 2206.40s]  But at the end of this hide-and-seek episode,
[2206.40s -> 2210.84s]  at some point, the agents discover a bug
[2210.84s -> 2212.96s]  in this physical system and start
[2212.96s -> 2215.68s]  anti-gravity flying in the air
[2215.68s -> 2218.00s]  and shooting hiders everywhere.
[2218.00s -> 2221.36s]  It's a super interesting video you must watch.
[2221.36s -> 2224.16s]  So lots of that, and also humanoid football
[2224.16s -> 2225.84s]  and capture the flag from DeepMind.
[2225.84s -> 2228.32s]  Lots of interesting behaviors emerging that we observed.
[2231.28s -> 2232.88s]  Here's my favorite one.
[2232.88s -> 2235.72s]  But these labels, so here, these
[2235.72s -> 2239.08s]  are labels that are provided by OpenAI, running and chasing,
[2239.08s -> 2241.60s]  fort building, and ramp views.
[2241.60s -> 2245.40s]  And these ones were that a human or humans,
[2245.40s -> 2249.08s]  when painstakingly, one by one, watch all these videos
[2249.08s -> 2251.52s]  and label them manually.
[2251.52s -> 2254.84s]  So our question is, is there a better way
[2254.84s -> 2257.48s]  to discover these emergent behaviors?
[2257.48s -> 2259.80s]  Perhaps some nice visualization can
[2259.80s -> 2264.96s]  help us explore this complex domain a little better.
[2264.96s -> 2267.80s]  So that's our goal.
[2267.80s -> 2270.24s]  So in this work, we're going to, again,
[2270.24s -> 2272.76s]  treat the agents like an observational study,
[2272.76s -> 2273.76s]  like a new species.
[2273.80s -> 2275.80s]  And we're going to do observational study.
[2275.80s -> 2277.60s]  And what that means is that we only
[2277.60s -> 2280.56s]  get to observe state and action pair, so where they are,
[2280.56s -> 2284.08s]  what are they doing, yeah, what are they doing.
[2284.08s -> 2287.20s]  And we're going to discover agent behavior
[2287.20s -> 2290.32s]  by basically kind of like a clustering the data.
[2290.32s -> 2292.60s]  That's all we're going to do.
[2292.60s -> 2293.80s]  And how do we do it?
[2293.80s -> 2295.56s]  Pretty simple.
[2295.56s -> 2298.12s]  A generative model, have you covered the Bayesian
[2298.12s -> 2299.76s]  generative model, graphical model?
[2299.76s -> 2301.94s]  No, gotcha, OK.
[2301.94s -> 2302.60s]  So think about.
[2303.56s -> 2304.84s]  Hi.
[2304.84s -> 2307.16s]  That was what you teach.
[2307.16s -> 2309.36s]  Yeah, so this is a graphical model.
[2309.36s -> 2314.28s]  Think about this as a fake or hypothetical data generation
[2314.28s -> 2315.12s]  process.
[2315.12s -> 2316.32s]  So how does this work?
[2316.32s -> 2319.24s]  Like I'm generating the data, I created this system.
[2319.24s -> 2323.24s]  I'm going to first generate a joint latent embedding space
[2323.24s -> 2326.20s]  that represents numbers, that represents all the behaviors
[2326.20s -> 2327.00s]  in the system.
[2327.00s -> 2328.72s]  And then I'm going to, for each agent,
[2328.72s -> 2331.48s]  I'm going to generate another embedding.
[2331.48s -> 2335.36s]  And each embedding, when it's conditioned with state,
[2335.36s -> 2337.20s]  it's going to generate policy.
[2337.20s -> 2338.96s]  It's going to decide what it's going to do,
[2338.96s -> 2342.38s]  what action is given the state and the embedding pair.
[2342.38s -> 2345.24s]  And then what that whole thing generates
[2345.24s -> 2348.24s]  is what you see, the state and action pair.
[2348.24s -> 2349.56s]  So how does this work?
[2349.56s -> 2351.88s]  And then given this, you build a model,
[2351.88s -> 2354.88s]  and you do inference to learn all these parameters.
[2354.88s -> 2356.68s]  Kind of same business as neural network,
[2356.68s -> 2360.08s]  but it just have a little more structure.
[2360.16s -> 2361.80s]  So this is completely made up, right?
[2361.80s -> 2366.16s]  This is like my idea of how these new species might work.
[2366.16s -> 2368.72s]  And our goal is to, we're going to try this and see
[2368.72s -> 2370.92s]  if anything useful comes up.
[2370.92s -> 2373.32s]  And the way you do this is, one of the ways you do this
[2373.32s -> 2375.88s]  is you optimize for variation and lower bound.
[2375.88s -> 2377.00s]  You need to know that.
[2377.00s -> 2379.28s]  It's very cool, actually, if one
[2379.28s -> 2382.44s]  gets into this exponential family business.
[2382.44s -> 2384.16s]  Very cool.
[2384.16s -> 2387.40s]  CS228.
[2387.40s -> 2389.60s]  So here's one of the results that we had.
[2389.64s -> 2392.20s]  It's a domain called MuJoCo.
[2392.20s -> 2395.00s]  Here we're going to pretend that we have two agents, one
[2395.00s -> 2397.84s]  controlling back leg and one controlling the front leg.
[2397.84s -> 2400.16s]  And on the right, we're showing that joint embedding
[2400.16s -> 2405.52s]  space, z omega and z alpha while video is running.
[2405.52s -> 2408.64s]  I'm going to try to put the video back.
[2411.92s -> 2413.24s]  So now I'm going to select.
[2413.24s -> 2416.72s]  This is a visualization that we built online.
[2416.72s -> 2418.00s]  You can go check it out.
[2418.00s -> 2421.44s]  You can select a little space in agent one space.
[2421.44s -> 2425.12s]  And you see it maps to pretty tight space in agent zero.
[2425.12s -> 2427.12s]  And it shows pretty decent running ability.
[2427.12s -> 2428.76s]  So that's cool.
[2428.76s -> 2432.48s]  And now I'm going to select somewhere else in agent one
[2432.48s -> 2435.56s]  that maps to dispersed area in agent zero.
[2435.56s -> 2438.80s]  It looks like it's not doing as well.
[2438.80s -> 2440.36s]  And this is just an insight that we
[2440.36s -> 2442.48s]  gain for this data only.
[2442.48s -> 2447.36s]  But I was quickly able to identify this tight mapping
[2447.36s -> 2451.08s]  business kind of represents the good running behavior
[2451.08s -> 2452.68s]  and bad running behaviors.
[2452.68s -> 2455.20s]  That's something that you can do pretty efficiently.
[2455.20s -> 2458.16s]  And now I'm going to show you something more interesting.
[2458.16s -> 2460.68s]  So of course, we have to do this because we have the data.
[2460.68s -> 2461.44s]  It's here.
[2461.44s -> 2462.96s]  It's so cool.
[2462.96s -> 2467.20s]  So we apply this framework in the when AI is hide and seek.
[2467.20s -> 2468.64s]  This has four agent.
[2468.64s -> 2470.12s]  It looks like a simple game, but it
[2470.12s -> 2473.64s]  has pretty complex structure, 100 dimensional observations,
[2473.64s -> 2475.60s]  five dimensional action space.
[2475.60s -> 2478.16s]  So in this work, remember that we
[2478.16s -> 2481.20s]  pretend that we don't know the labels given by OpenAI.
[2481.20s -> 2484.32s]  We just shuffle them in the mix.
[2484.32s -> 2486.48s]  But we can color them, our results,
[2486.48s -> 2488.36s]  with respect to their labels.
[2488.36s -> 2492.76s]  So again, this is the result of z omega and z alpha,
[2492.76s -> 2494.08s]  the individual agents.
[2494.08s -> 2496.44s]  But the coloring is something that we didn't know before.
[2496.44s -> 2499.36s]  We just did it after the fact.
[2499.36s -> 2503.56s]  You can see in the z omega, there's nice kind of pattern
[2503.56s -> 2507.40s]  that we can roughly separate what makes sense to humans
[2507.40s -> 2508.88s]  and what makes sense to us.
[2508.88s -> 2513.56s]  But remember, the green and gray, kind of everywhere,
[2513.56s -> 2514.52s]  they're mixed.
[2514.52s -> 2518.48s]  So in this particular run of OpenAI's hide and seek,
[2518.48s -> 2520.44s]  it seemed that those two representations
[2520.44s -> 2523.36s]  were kind of entangled.
[2523.36s -> 2525.44s]  The running and chasing, the blue dots,
[2525.44s -> 2528.32s]  it seems to be pretty separate and distinguishable
[2528.32s -> 2530.00s]  from all the other colors.
[2530.00s -> 2532.16s]  And that kind of makes sense because that's the basis
[2532.72s -> 2533.60s]  of playing this game.
[2533.60s -> 2535.44s]  So if you don't have that representation,
[2535.44s -> 2537.76s]  you have a big trouble.
[2537.76s -> 2542.96s]  But in case of like orange, which is fork building,
[2542.96s -> 2546.24s]  it's a lot more distinguishable in hiders.
[2546.24s -> 2548.08s]  And that makes sense because hiders
[2548.08s -> 2550.24s]  are the ones building the fort.
[2550.24s -> 2551.72s]  Then seekers don't build the fort.
[2551.72s -> 2554.40s]  So orange is a little more entangled in seekers.
[2554.40s -> 2558.64s]  Perhaps if seekers had built more separate fort building
[2558.64s -> 2560.76s]  representation, maybe they would have win this game.
[2564.36s -> 2567.92s]  So this work, can we learn something interesting,
[2567.92s -> 2571.96s]  emerging behaviors by just simply observing the system?
[2571.96s -> 2574.12s]  The answer seems to be yes, at least for the domains
[2574.12s -> 2575.20s]  that we tested.
[2575.20s -> 2578.00s]  A lot more complex domains should be tested,
[2578.00s -> 2581.32s]  but these are the ones we had.
[2581.32s -> 2583.44s]  But remember that these methods don't give you
[2583.44s -> 2584.76s]  names of these clusters.
[2584.76s -> 2588.36s]  So you would have to go and investigate and click through
[2588.40s -> 2590.76s]  and explore.
[2590.76s -> 2594.76s]  And if the cluster represents superhuman concept,
[2594.76s -> 2596.12s]  this is not going to help you.
[2596.12s -> 2598.00s]  And I'll talk a little more about the work
[2598.00s -> 2599.68s]  that we do try to help them.
[2599.68s -> 2600.80s]  But this is not for you.
[2600.80s -> 2603.48s]  This is not going to help you there.
[2603.48s -> 2607.12s]  And also, if you have access to the model and the reward
[2607.12s -> 2609.16s]  signal, you should use it.
[2609.16s -> 2611.20s]  Why dump it?
[2611.20s -> 2613.28s]  So next work, we do use it.
[2613.28s -> 2617.04s]  I'm going to talk about this work with Nico and Natasha
[2617.04s -> 2619.60s]  and Shai Yin.
[2619.60s -> 2622.24s]  So here, this time we're going to intervene.
[2622.24s -> 2623.80s]  We're going to be a little intrusive,
[2623.80s -> 2626.48s]  but hopefully we'll learn a little more.
[2626.48s -> 2629.48s]  So the problem is that we're going to build a new multi-agent
[2629.48s -> 2631.52s]  system, going to build it from scratch,
[2631.52s -> 2633.60s]  such that we can do control testing.
[2633.60s -> 2635.60s]  But at the same time, we shouldn't sacrifice
[2635.60s -> 2636.64s]  the performance.
[2636.64s -> 2639.20s]  So we're going to try to match the performance
[2639.20s -> 2640.80s]  of the overall system.
[2640.80s -> 2643.40s]  We do succeed.
[2643.40s -> 2646.68s]  I had this paper collaboration with folks at Stanford actually
[2646.68s -> 2651.04s]  here in 2020, where we proposed this pretty simple idea, which
[2651.04s -> 2653.32s]  is you have a neural network.
[2653.32s -> 2657.16s]  Why don't we embed concepts in the middle of the bottleneck,
[2657.16s -> 2659.32s]  where one neuron represents trees,
[2659.32s -> 2662.32s]  the other represents stripes, and just train the model
[2662.32s -> 2663.88s]  end to end.
[2663.88s -> 2665.40s]  And why are we doing this?
[2665.40s -> 2667.72s]  Well, because then at inference time,
[2667.72s -> 2669.92s]  you can actually intervene.
[2669.92s -> 2672.20s]  You can pretend, you know, predicting zebra,
[2672.20s -> 2673.88s]  I don't think trees should matter.
[2673.88s -> 2676.16s]  So I'm going to zero out this neuron and feed forward
[2676.16s -> 2677.56s]  and see what happens.
[2677.56s -> 2679.80s]  So it's particularly useful in the medical setting,
[2679.80s -> 2682.10s]  where there are some features that doctors don't want.
[2682.10s -> 2684.76s]  We can cancel out and test.
[2684.76s -> 2688.72s]  So this is the work to extend this to RL setting.
[2688.72s -> 2693.16s]  It's actually not as simple extension then as we thought.
[2693.16s -> 2694.92s]  It came out to be pretty complex.
[2694.92s -> 2697.24s]  But essentially, we're doing that.
[2697.24s -> 2699.88s]  And we're building each of the concept bottleneck
[2699.88s -> 2702.16s]  for each agent.
[2702.16s -> 2705.00s]  And at the end of the day, what you optimize is what you usually do.
[2705.00s -> 2707.12s]  A typical PPO, just think about this
[2707.12s -> 2711.64s]  as make the other system work, plus minimizing
[2711.64s -> 2714.88s]  the difference between the true concept and estimated concept.
[2714.88s -> 2717.64s]  That's all you do.
[2717.64s -> 2718.60s]  Why are we doing this?
[2718.60s -> 2719.60s]  You can intervene.
[2719.60s -> 2722.40s]  You can pretend now agent two, pretend
[2722.40s -> 2724.52s]  that you can't see agent one.
[2724.52s -> 2725.90s]  What happens now?
[2725.90s -> 2729.36s]  That's what we're doing here.
[2729.36s -> 2731.56s]  We're going to do this in two domains.
[2731.56s -> 2733.56s]  First domain, how many people looked
[2733.60s -> 2737.56s]  at the saw this cooking game before?
[2737.56s -> 2740.92s]  Yeah, it's a pretty commonly used cooking
[2740.92s -> 2743.88s]  domain in reinforcement learning, very simple.
[2743.88s -> 2746.28s]  We have two agents, yellow and blue.
[2746.28s -> 2748.08s]  And they're going to make soup.
[2748.08s -> 2749.56s]  They can bring three tomatoes.
[2749.56s -> 2750.80s]  They get a reward.
[2750.80s -> 2753.48s]  They wait for the tomato and bring the dishes,
[2753.48s -> 2754.96s]  a dish to the cooking pot.
[2754.96s -> 2755.80s]  They get a reward.
[2755.80s -> 2758.24s]  Finally, their goal is to deliver as many soups as
[2758.24s -> 2761.36s]  possible, given some time.
[2761.40s -> 2764.52s]  And here, concepts that we use are agent prediction,
[2764.52s -> 2768.16s]  orientation, agent has tomato, has dish, et cetera, et cetera,
[2768.16s -> 2771.28s]  something that's immediately available to you already.
[2771.28s -> 2773.60s]  And you can, of course, tweak the environment
[2773.60s -> 2774.92s]  to make it more fun.
[2774.92s -> 2778.20s]  So you can make it that they have to collaborate.
[2778.20s -> 2779.72s]  You can build a wall between them
[2779.72s -> 2781.18s]  so that they have to work together
[2781.18s -> 2783.36s]  in order to serve any tomato soup.
[2783.36s -> 2785.20s]  Or you can make them freely available.
[2785.20s -> 2788.44s]  You can work independently or together, whatever your choice.
[2791.52s -> 2793.92s]  First, just kind of send you to check,
[2793.92s -> 2798.84s]  was that you can detect the emerging behavior
[2798.84s -> 2801.32s]  of coordination versus non-coordination.
[2801.32s -> 2803.94s]  So when the impassable environment,
[2803.94s -> 2805.84s]  when we made up that environment,
[2805.84s -> 2808.96s]  and suppose the RL system that we trained worked,
[2808.96s -> 2811.20s]  they were able to deliver some soups.
[2811.20s -> 2813.56s]  Then you see that when we intervene, this graph,
[2813.56s -> 2814.40s]  let me explain.
[2814.40s -> 2817.80s]  This is a reward of an agent 1.
[2817.80s -> 2819.64s]  When there's a no intervention, so this
[2819.68s -> 2824.08s]  is a perfectly good world, and when there was an intervention.
[2824.08s -> 2827.48s]  This is average value of intervening on all concepts,
[2827.48s -> 2830.78s]  but I'm also going to show you each concept soon.
[2830.78s -> 2832.88s]  If you compare left and right, you
[2832.88s -> 2836.12s]  can tell that in the right, when we intervene,
[2836.12s -> 2839.80s]  reward deteriorated quite a lot for both of them.
[2839.80s -> 2842.88s]  And that's one way to see, ah, they are coordinating.
[2842.88s -> 2846.56s]  Because somehow intervening at this concept
[2846.58s -> 2850.52s]  impacted a lot of their performance.
[2850.52s -> 2853.24s]  But this is what was really interesting to me,
[2853.24s -> 2854.28s]  and I'm curious.
[2854.28s -> 2855.60s]  Anyone can guess.
[2855.60s -> 2860.08s]  So this is the same graph as the one you saw before,
[2860.08s -> 2864.00s]  but except I'm plotting for intervention for each concept.
[2864.00s -> 2867.12s]  So I'm intervening team position, team orientation,
[2867.12s -> 2869.98s]  team has tomato, et cetera, et cetera.
[2869.98s -> 2872.64s]  It turns out that they're using,
[2872.64s -> 2876.20s]  or rather, when we intervene on team orientation,
[2876.24s -> 2879.16s]  the degradation of performance was the biggest to the extent
[2879.16s -> 2880.68s]  that we believe that orientation
[2880.68s -> 2883.76s]  had to do with subcoordination.
[2883.76s -> 2886.44s]  Does anyone can guess why this might be?
[2892.24s -> 2893.96s]  It's not the position.
[2893.96s -> 2895.32s]  It's the orientation.
[2899.68s -> 2900.40s]  Yes?
[2900.40s -> 2902.36s]  Just a clarification question on orientation.
[2902.36s -> 2904.12s]  Is that like the direction that the team
[2904.12s -> 2904.56s]  made this particular?
[2904.56s -> 2905.20s]  Yes.
[2905.20s -> 2909.56s]  So it seems like orientation would let you predict
[2909.56s -> 2910.64s]  where the moving heads.
[2910.64s -> 2911.76s]  Yes, yes, that's right.
[2911.76s -> 2914.32s]  Yes, where were you when I was pulling
[2914.32s -> 2916.68s]  my hair over this question?
[2916.68s -> 2917.88s]  Yes, that's exactly right.
[2917.88s -> 2920.12s]  And initially, I was really puzzled.
[2920.12s -> 2920.88s]  Why not position?
[2920.88s -> 2922.60s]  Because I expected to be positioned.
[2922.60s -> 2924.08s]  But exactly, that's exactly right.
[2924.08s -> 2927.28s]  So the orientation is the first signal
[2927.28s -> 2930.40s]  that an Asian can get about the next move
[2930.40s -> 2931.80s]  over the other Asian.
[2931.80s -> 2934.20s]  Because they're facing the pot, they're going to the pot.
[2934.20s -> 2937.24s]  They're facing the tomato, they were going to get the tomato.
[2937.24s -> 2940.20s]  Really interesting intuition.
[2940.20s -> 2945.60s]  Obvious to some, but I needed this graph to work that out.
[2945.60s -> 2949.24s]  And of course, you can use this to identify lazy agents.
[2949.24s -> 2954.20s]  If you look at the rightmost yellow agent, our friend,
[2954.20s -> 2957.20s]  just chilling in the background.
[2957.20s -> 2959.44s]  And these lazy, if you train RL agents,
[2959.44s -> 2961.60s]  there's always some agents just hanging out.
[2961.60s -> 2963.56s]  They just don't do anything.
[2963.60s -> 2967.20s]  And you can easily identify this by using this graph.
[2967.20s -> 2971.28s]  If I intervene, it just doesn't impact any of their rewards.
[2971.28s -> 2972.24s]  That one's mean.
[2975.20s -> 2976.68s]  So the second domain, we're going
[2976.68s -> 2979.24s]  to look at a little more complex domain.
[2979.24s -> 2983.08s]  So this is studying inter-Asian social dynamics.
[2983.08s -> 2985.88s]  So in this domain, there is a little bit of tension.
[2985.88s -> 2987.40s]  This is called a cleanup.
[2987.40s -> 2989.20s]  We have four agents.
[2989.20s -> 2992.84s]  They only get rewards if they eat apples, just yellow things
[2992.84s -> 2995.04s]  or green things or apples.
[2995.04s -> 2998.68s]  But if you don't clean the river, then apple stops URL.
[2998.68s -> 3001.40s]  So somebody has to clean the river.
[3001.40s -> 3004.60s]  And you can see if you have four people trying
[3004.60s -> 3008.72s]  to collect apples, you can just wait until someone else
[3008.72s -> 3011.08s]  to clean the river and then collect the apples.
[3011.08s -> 3012.72s]  And in fact, that's sometimes what happens.
[3015.44s -> 3020.44s]  And concepts here, again, are pretty common things,
[3020.48s -> 3025.04s]  position orientation and pollution positions, et cetera.
[3026.56s -> 3030.92s]  So when we first plotted the same graph
[3030.92s -> 3035.92s]  as the previous domain, it tells a story.
[3036.28s -> 3041.28s]  So the story here is that when I intervene on agent one,
[3041.96s -> 3045.76s]  it seems to influence agent two quite a lot
[3045.76s -> 3050.00s]  if you look at these three different graph,
[3050.56s -> 3053.68s]  how reward was impacted when I intervene on agent one.
[3053.68s -> 3055.20s]  It's agent three and four are fine,
[3055.20s -> 3057.32s]  but it seems that only agent two is influenced.
[3057.32s -> 3060.68s]  Same with idle time, same with the inter-Asian distance.
[3060.68s -> 3063.36s]  So we were like, oh, maybe that's true.
[3063.36s -> 3064.56s]  But we keep wondering,
[3064.56s -> 3066.68s]  there's like a lot going on in this domain.
[3066.68s -> 3069.68s]  Like, how do we know this is the case?
[3069.68s -> 3073.28s]  So we decided to take another step.
[3073.28s -> 3077.36s]  So we're gonna do a little more work here,
[3077.36s -> 3078.48s]  but not a lot.
[3078.52s -> 3080.28s]  We're gonna build a graph
[3080.28s -> 3083.00s]  to discover inter-agent relationships.
[3083.00s -> 3085.56s]  This is the simplest, dumbest way to build a graph.
[3085.56s -> 3087.36s]  But again, I like simple things.
[3087.36s -> 3088.56s]  So how do you build a graph?
[3088.56s -> 3090.16s]  Well, suppose that you have,
[3090.16s -> 3091.72s]  you're building a graph between movies.
[3091.72s -> 3093.04s]  This is like not what we do,
[3093.04s -> 3096.28s]  but just to describe what we were trying to do.
[3096.28s -> 3099.48s]  We have each row, we're gonna build a matrix.
[3099.48s -> 3101.24s]  Each row is a movie
[3101.24s -> 3105.12s]  and each column consists of features of these movies.
[3105.12s -> 3108.12s]  So length, jungle of the movie, and so on.
[3108.12s -> 3110.48s]  And the simplest way to build a graph
[3110.48s -> 3112.08s]  is to do a regression.
[3112.08s -> 3116.20s]  So exclude i-th row,
[3116.20s -> 3118.60s]  and then we're gonna regress over everyone else.
[3118.60s -> 3120.72s]  And that gives me beta,
[3120.72s -> 3124.08s]  which is the kind of coefficients for each of these.
[3124.08s -> 3128.96s]  And that beta represents the strength of the edges.
[3128.96s -> 3130.80s]  So this movie is more related to this movie
[3130.80s -> 3131.84s]  and not the other movie.
[3131.84s -> 3133.24s]  And ta-da, you have a graph.
[3133.24s -> 3134.20s]  It's like dumbest way.
[3134.20s -> 3135.40s]  There's a lot of caveats to,
[3135.40s -> 3136.92s]  you shouldn't do this a lot of times,
[3136.92s -> 3139.36s]  but this is the simplest way to do it.
[3140.44s -> 3142.12s]  So we did the same thing here.
[3142.12s -> 3144.44s]  Instead, instead of movie,
[3144.44s -> 3148.48s]  we're going to use intervention on concept C
[3148.48s -> 3151.20s]  on agent N as our node.
[3151.20s -> 3153.80s]  And to build this matrix,
[3153.80s -> 3156.16s]  we're going to use intervention outcome,
[3156.16s -> 3159.76s]  which wouldn't have been available without our framework
[3159.76s -> 3163.52s]  for reward, resource collected, and many other things.
[3165.16s -> 3166.72s]  And when you build this graph,
[3166.72s -> 3167.56s]  at the end of the day,
[3167.56s -> 3170.08s]  you get betas that represent relationship
[3170.08s -> 3173.20s]  between these interventions, okay?
[3174.24s -> 3177.16s]  So I had a graph of that matrix
[3177.16s -> 3180.32s]  apparently I removed before I came over,
[3180.32s -> 3182.12s]  but imagine there was a matrix.
[3183.12s -> 3186.40s]  There is a nicely highlighted between agent one and four
[3186.40s -> 3187.84s]  and that only,
[3187.84s -> 3191.16s]  contradicting the original hypothesis that we had.
[3191.16s -> 3193.24s]  And this is the video of it.
[3193.24s -> 3195.64s]  So when we stared at that matrix,
[3195.64s -> 3199.28s]  it turns out that there's no high edge,
[3199.28s -> 3202.16s]  strong edges between agent one and two.
[3202.16s -> 3203.60s]  So we were like, that's weird,
[3203.60s -> 3206.36s]  but there is strong edges between agent one and four.
[3206.36s -> 3207.96s]  So we like dig deeper into it,
[3207.96s -> 3212.00s]  watched a lot of sessions to validate what's happening.
[3212.00s -> 3213.72s]  And it turns out that
[3213.72s -> 3215.68s]  the story was a lot more complicated.
[3215.68s -> 3219.20s]  The one's orientation was important for four,
[3219.20s -> 3220.88s]  but when that fails,
[3220.88s -> 3223.36s]  agent one and two kind of gets cornered in.
[3223.36s -> 3224.80s]  And you can see that in the graph,
[3224.84s -> 3228.32s]  agent four kind of get agent one and four,
[3228.32s -> 3230.64s]  sorry, one and two, blue and yellow agent
[3230.64s -> 3231.92s]  kind of gets in the corner together.
[3231.92s -> 3233.52s]  They kind of get stuck.
[3233.52s -> 3235.88s]  And this is simply just accidental
[3235.88s -> 3238.76s]  because of the way that we built this environment.
[3238.76s -> 3240.28s]  It just happened.
[3240.28s -> 3243.36s]  But the true, the raw statistics
[3243.36s -> 3244.72s]  wouldn't have told us this story
[3244.72s -> 3246.16s]  that this was completely accidental.
[3246.16s -> 3247.84s]  In fact, there was no correlation,
[3247.84s -> 3250.28s]  no coordination between agent one and two.
[3250.28s -> 3251.48s]  But only after the graph,
[3251.48s -> 3254.64s]  we realized this was the case.
[3254.64s -> 3256.68s]  Now this might be one-off case,
[3256.68s -> 3257.52s]  but you know what?
[3257.52s -> 3260.76s]  A lot of emerging behaviors that we want to detect,
[3260.76s -> 3262.64s]  a lot of them will be one-off case.
[3262.64s -> 3265.04s]  And we really want to get to the truth of that
[3265.04s -> 3267.80s]  rather than having some surface level statistics.
[3270.44s -> 3274.96s]  So can we build multi-agent system
[3274.96s -> 3277.28s]  that enables intervention and performs as well?
[3277.28s -> 3278.20s]  I'm sorry, yes.
[3278.20s -> 3281.08s]  There's a graph that shows the red line and blue line,
[3281.08s -> 3282.88s]  roughly aligned, that's good news.
[3282.92s -> 3284.44s]  We are performing as well.
[3285.56s -> 3288.44s]  But remember these concepts, you need to label them
[3288.44s -> 3290.64s]  for you to have some way of getting those concepts,
[3290.64s -> 3292.32s]  positions, and orientation.
[3292.32s -> 3294.16s]  That might be something that we would love
[3294.16s -> 3295.58s]  to extend in the future.
[3296.64s -> 3298.20s]  Before I go on, any questions?
[3303.96s -> 3304.80s]  You shy?
[3306.00s -> 3306.84s]  You shy?
[3310.28s -> 3311.12s]  Cool.
[3311.96s -> 3312.80s]  All right.
[3313.52s -> 3316.00s]  So I did tell you that we're not going to know
[3316.96s -> 3318.72s]  the solution to move 37.
[3318.72s -> 3319.72s]  I still don't, okay?
[3319.72s -> 3321.04s]  I still don't.
[3321.04s -> 3323.76s]  But I'll tell you a little bit of work
[3323.76s -> 3326.40s]  that I'm currently doing, I'm really excited about.
[3327.50s -> 3329.84s]  That we started thinking, you know what?
[3329.84s -> 3333.16s]  Will this understanding move 37 happen before,
[3333.16s -> 3334.52s]  within my lifetime?
[3334.52s -> 3335.80s]  And I was like, oh, maybe not,
[3335.80s -> 3337.52s]  but I kind of want it to happen.
[3337.52s -> 3340.40s]  So we start, this is all about research, right?
[3340.40s -> 3342.18s]  You started carving out a space
[3342.22s -> 3344.30s]  where things are a little bit solvable,
[3344.30s -> 3346.22s]  and you try to attack that problem.
[3346.22s -> 3348.74s]  So this is our attempt to do exactly that,
[3348.74s -> 3351.80s]  to get a little closer to our ultimate goal,
[3351.80s -> 3354.64s]  my ultimate goal of understanding that move 37.
[3356.34s -> 3358.70s]  So before that, how many people here know AlphaZero
[3358.70s -> 3359.62s]  from DeepMind?
[3359.62s -> 3360.78s]  Yes.
[3360.78s -> 3363.98s]  AlphaZero is a self-trained,
[3363.98s -> 3366.28s]  a self-trained chess playing machine
[3366.28s -> 3368.94s]  that beats, that has higher ELO rating
[3368.94s -> 3370.22s]  than any other humans,
[3370.38s -> 3372.26s]  and beats Stockfish, which is arguably
[3372.26s -> 3375.06s]  no existing human can beat Stockfish.
[3375.06s -> 3376.74s]  So in the previous paper,
[3376.74s -> 3381.74s]  we try to discover human chess concepts in this network.
[3382.14s -> 3386.36s]  So when does concept like material imbalance
[3386.36s -> 3389.32s]  appear in its network, which layer,
[3389.32s -> 3391.66s]  and when in the training time,
[3391.66s -> 3395.14s]  and which we call what, when, and where plots.
[3395.14s -> 3398.62s]  And we also compare the evolution of opening moves
[3398.62s -> 3400.26s]  between humans and AlphaZero.
[3400.26s -> 3402.42s]  These are the first couple of moves that you make
[3402.42s -> 3404.86s]  when you play chess, and as you can see,
[3404.86s -> 3406.66s]  there's a pretty huge difference.
[3406.66s -> 3409.90s]  Left is human, right is AlphaZero.
[3409.90s -> 3412.98s]  It turns out that AlphaZero can master,
[3412.98s -> 3414.48s]  or supposedly master,
[3414.48s -> 3417.58s]  a lot of variety different types of openings.
[3417.58s -> 3419.36s]  Openings can be very aggressive,
[3419.36s -> 3421.26s]  openings can be very boring,
[3421.26s -> 3423.34s]  could be very long-range,
[3423.34s -> 3426.46s]  targeting for long-range strategy or short-range,
[3426.46s -> 3427.42s]  very different.
[3427.42s -> 3429.14s]  So that begs the question,
[3429.14s -> 3432.44s]  what does AlphaZero know that humans don't know?
[3432.44s -> 3435.42s]  Don't you wanna learn what that might be?
[3436.42s -> 3438.02s]  So that's what we're doing right now.
[3438.02s -> 3442.12s]  We're actually almost, we're about to evaluate.
[3442.12s -> 3447.12s]  So the goal of this work is teach the world chess champion
[3447.18s -> 3451.12s]  a new chess, superhuman chess strategy.
[3451.12s -> 3454.02s]  And we just got yes from Magnus Carlsen,
[3454.02s -> 3456.54s]  who is the world chess champion.
[3456.54s -> 3458.10s]  He just lost the match, I know,
[3458.10s -> 3461.46s]  but he's still champion in my mind.
[3461.46s -> 3464.38s]  He's still champion in two categories, actually.
[3464.38s -> 3466.34s]  So the way that we are doing this is
[3466.34s -> 3469.36s]  we're going to discover new chess strategy
[3469.36s -> 3474.14s]  by explicitly forgetting existing chess strategy,
[3474.14s -> 3476.54s]  which we have a lot of data for.
[3476.54s -> 3478.86s]  And then we're going to learn a graph,
[3478.86s -> 3481.30s]  this time a little more complicated graph,
[3481.30s -> 3485.56s]  by using the existing relationships
[3485.56s -> 3487.40s]  between existing concepts
[3487.40s -> 3489.60s]  so that we can get a little bit more idea
[3489.60s -> 3492.08s]  of what the new concept might look like.
[3492.08s -> 3495.48s]  And Magnus Carlsen, so my favorite part about this work,
[3495.48s -> 3497.20s]  I talk about carving out.
[3497.20s -> 3499.08s]  My favorite part about this work is that
[3499.08s -> 3501.52s]  the evaluation is going to be pretty clear.
[3501.52s -> 3503.88s]  So it's not just like Magnus coming in and says,
[3503.88s -> 3505.04s]  oh, your work is kind of nice
[3505.04s -> 3506.76s]  and say nice things about our work.
[3506.76s -> 3509.88s]  No, Magnus actually has to solve some puzzles.
[3509.88s -> 3511.76s]  And we will be able to evaluate him
[3511.76s -> 3513.32s]  whether he did it or not.
[3513.32s -> 3515.40s]  So it's like a kind of success and fail.
[3515.40s -> 3516.48s]  But I'm extremely excited.
[3516.48s -> 3520.32s]  This kind of work I can only do because of Lisa,
[3520.32s -> 3522.56s]  who is a champion herself,
[3522.56s -> 3525.40s]  but also a PhD student at Oxford.
[3525.40s -> 3527.64s]  And she played against Magnus in the past
[3527.64s -> 3530.00s]  and many others chess players in the world.
[3530.00s -> 3532.50s]  And she's going to be the ultimate
[3532.50s -> 3536.40s]  pre-superhuman filtering to filter out these concepts
[3536.40s -> 3538.68s]  that will eventually get to Magnus.
[3539.56s -> 3540.80s]  So I'm super excited about this.
[3540.80s -> 3542.56s]  I have no results, but it's coming up.
[3542.56s -> 3543.84s]  I'm excited, yes?
[3546.00s -> 3547.72s]  How are the puzzles generated?
[3547.72s -> 3550.28s]  Because there's already so many puzzles out there.
[3550.28s -> 3553.20s]  So I'm assuming that there's probably something near
[3553.20s -> 3556.60s]  about your time and what are the puzzles about
[3556.60s -> 3557.84s]  and how are the generators?
[3557.84s -> 3559.20s]  Puzzles are actually pretty simple.
[3559.20s -> 3562.36s]  So the way that we generate concepts
[3562.36s -> 3565.52s]  are within the embedding space of alpha zero.
[3565.52s -> 3567.44s]  And given that, because alpha zero
[3567.44s -> 3569.76s]  has really weird architecture,
[3569.76s -> 3571.72s]  so every single latent rare in alpha zero
[3571.72s -> 3573.96s]  has the exact same position as a chess board.
[3573.96s -> 3575.84s]  That's just the way that they decide to do it.
[3575.84s -> 3578.48s]  So because of that, we can actually identify
[3578.48s -> 3580.68s]  or generate the board positions
[3580.68s -> 3582.92s]  that corresponds to that concept.
[3582.92s -> 3584.84s]  And because we have MCTS,
[3584.84s -> 3588.60s]  we can predict what move it's going to make
[3588.60s -> 3590.28s]  given that board position.
[3590.28s -> 3591.20s]  Because at inference time,
[3591.20s -> 3594.00s]  it's actually deterministic of a whole alpha zero thing.
[3594.00s -> 3596.56s]  So we have a lot of board positions
[3596.56s -> 3598.44s]  and that's all you need for puzzles.
[3598.44s -> 3599.68s]  You give a board position
[3599.68s -> 3601.60s]  and then ask Magnus to make a move.
[3601.60s -> 3602.96s]  We explain the concept
[3602.96s -> 3605.28s]  and then give Magnus more board positions
[3605.28s -> 3608.84s]  and see if we can apply that concept that he just learned.
[3609.72s -> 3611.80s]  So are you like trying to analyze the computer
[3611.80s -> 3614.76s]  because there's not really any kind of software
[3614.76s -> 3616.36s]  on these puzzles, right?
[3616.36s -> 3618.20s]  But it seems like you're kind of ameliorate
[3618.20s -> 3619.60s]  in the computer.
[3619.60s -> 3621.76s]  Is it kind of different about the board?
[3621.76s -> 3625.12s]  Yeah, so if I were to ask Stockfish
[3625.12s -> 3627.40s]  to solve those puzzles,
[3627.40s -> 3628.92s]  that would be a different question.
[3628.92s -> 3630.28s]  Because we are interested in
[3630.28s -> 3632.84s]  whether we can teach human, not Stockfish.
[3632.84s -> 3633.92s]  Stockfish might be able to do it.
[3633.92s -> 3637.00s]  That's actually an interesting thing that we could do
[3637.00s -> 3637.96s]  now that I think about it.
[3637.96s -> 3641.44s]  But our goal is to just teach one superhuman.
[3641.44s -> 3643.20s]  Like if I have, for example,
[3643.20s -> 3645.60s]  10,000 superhuman concepts
[3645.60s -> 3649.20s]  and only three of them are digestible by Magnus,
[3649.20s -> 3650.24s]  that's a win.
[3650.24s -> 3652.96s]  That would be a big win for this type of research.
[3656.40s -> 3657.40s]  Questions?
[3660.00s -> 3661.20s]  All right.
[3661.24s -> 3664.60s]  Yeah, so wrap up.
[3664.60s -> 3666.88s]  Small steps towards our hopes and dreams.
[3666.88s -> 3669.88s]  We talked about the gap between what machines know
[3669.88s -> 3672.24s]  versus what we think machines know.
[3672.24s -> 3675.04s]  Three ideas why that might be true.
[3675.04s -> 3677.68s]  The three different maybe angles we can try to attack
[3677.68s -> 3681.36s]  and answer those questions and bridge that gap.
[3681.36s -> 3685.20s]  We talked about studying aliens, these machines
[3685.20s -> 3687.40s]  in observation study or control study.
[3687.40s -> 3690.48s]  There are many other ways to study a species.
[3690.68s -> 3691.52s]  I'm not an expert,
[3691.52s -> 3693.84s]  but anthropology and other humanities studies
[3693.84s -> 3696.60s]  would know a lot there more about this.
[3696.60s -> 3699.12s]  And maybe, just maybe,
[3699.12s -> 3702.52s]  we can try to understand move 37 at some point,
[3702.52s -> 3704.32s]  hopefully within my lifetime,
[3704.32s -> 3708.60s]  through this chess project that I'm very excited about.
[3708.60s -> 3709.80s]  Thank you.
[3709.80s -> 3712.00s]  Okay, thank you very much.
[3712.00s -> 3713.00s]  Questions?
[3713.00s -> 3714.00s]  Thanks.
[3714.00s -> 3715.00s]  Yeah.
[3715.00s -> 3718.00s]  You talked about interpretability research
[3718.00s -> 3721.00s]  that costs NLP, vision, and RL.
[3721.00s -> 3722.20s]  Do you think there's much hope
[3722.20s -> 3724.60s]  for taking certain interpretability techniques
[3724.60s -> 3727.00s]  from one modality into other modalities?
[3727.00s -> 3730.00s]  And if so, what's that look like?
[3730.00s -> 3731.00s]  Hmm.
[3734.00s -> 3735.00s]  Okay.
[3735.00s -> 3736.00s]  Thank you very much.
[3736.00s -> 3737.00s]  Thank you.
[3737.00s -> 3738.00s]  Thank you.
[3738.00s -> 3739.00s]  Thank you.
[3739.20s -> 3741.20s]  So it depends on your goal.
[3741.20s -> 3744.20s]  I think, like think about fairness research,
[3744.20s -> 3748.20s]  which builds on strong mathematical foundation,
[3748.20s -> 3749.20s]  and that's like applicable
[3749.20s -> 3752.20s]  for any questions around fairness,
[3752.20s -> 3753.20s]  or hopefully applicable.
[3753.20s -> 3755.20s]  But then once you,
[3755.20s -> 3757.20s]  if your goal is to actually solve
[3757.20s -> 3760.20s]  a fairness issue at hand
[3760.20s -> 3762.20s]  for somebody, the real person in the world,
[3762.20s -> 3764.20s]  that's a completely different question.
[3764.20s -> 3765.20s]  You would have to customize it
[3765.20s -> 3767.20s]  for a particular application.
[3767.20s -> 3768.20s]  So there are two venues.
[3768.40s -> 3770.40s]  And I think similar is true in interpretability,
[3770.40s -> 3772.40s]  like the theory work that I talked about.
[3772.40s -> 3775.40s]  SHAP and IG are used across domains,
[3775.40s -> 3776.40s]  like vision, text.
[3776.40s -> 3778.40s]  So that theory paper would be applicable
[3778.40s -> 3780.40s]  across the domain.
[3780.40s -> 3781.40s]  Things like RL
[3781.40s -> 3784.40s]  and the way that we build that generative model,
[3784.40s -> 3786.40s]  you would need to test a little bit more
[3786.40s -> 3788.40s]  to make sure that this works in NLP.
[3788.40s -> 3790.40s]  I don't even know how to think
[3790.40s -> 3792.40s]  about agents in NLP yet,
[3792.40s -> 3794.40s]  so it would need a little bit of tweaking.
[3794.40s -> 3796.40s]  But both directions are fruitful.
[3798.40s -> 3800.40s]  I don't have a question.
[3800.40s -> 3802.40s]  So I saw the recent work
[3802.40s -> 3805.40s]  in which some amateur Go players
[3805.40s -> 3807.40s]  found a very tricky strategy
[3807.40s -> 3808.40s]  to trick up,
[3808.40s -> 3810.40s]  I think it was AlphaGo,
[3810.40s -> 3812.40s]  and that seemed like a concept
[3812.40s -> 3814.40s]  that humans know that machines don't
[3814.40s -> 3816.40s]  in that Venn diagram.
[3816.40s -> 3818.40s]  I just wanted to hear your thoughts about that.
[3818.40s -> 3820.40s]  Yeah, actually it's funny you mentioned that.
[3820.40s -> 3824.40s]  Lisa can beat AlphaZero pretty easily.
[3824.40s -> 3826.40s]  And it's a similar idea.
[3826.60s -> 3828.60s]  Because you kind of know
[3828.60s -> 3830.60s]  what are the most unseen
[3830.60s -> 3832.60s]  out-of-distribution moves are
[3832.60s -> 3834.60s]  and she can break AlphaZero pretty easily.
[3834.60s -> 3836.60s]  And Lisa guessed that
[3836.60s -> 3838.60s]  if he said Dole had known something more
[3838.60s -> 3839.60s]  about AI,
[3839.60s -> 3841.60s]  then maybe he would have tried to confuse AlphaGo.
[3841.60s -> 3843.60s]  But the truth is
[3843.60s -> 3845.60s]  it's a high-stake game.
[3845.60s -> 3847.60s]  Like he said Dole is
[3847.60s -> 3849.60s]  the famous star worldwide
[3849.60s -> 3851.60s]  so he wouldn't want to make a move
[3851.60s -> 3853.60s]  that would be seen as a complete mistake
[3853.60s -> 3855.60s]  like the one that makes
[3855.60s -> 3857.60s]  the one that Magnus made a couple of days ago
[3857.60s -> 3859.60s]  that got on the newsfeed everywhere
[3859.60s -> 3861.60s]  that he made this whole century-wide mistake
[3861.60s -> 3863.60s]  and that probably hurts.
[3866.60s -> 3868.60s]  Any other questions?
[3868.60s -> 3870.60s]  What about like the game question
[3870.60s -> 3872.60s]  about the AlphaGo game
[3872.60s -> 3875.60s]  or the TAS or AlphaZero for example?
[3875.60s -> 3877.60s]  I just like building machines
[3877.60s -> 3879.60s]  that play these games really well.
[3879.60s -> 3881.60s]  But I'm wondering
[3881.60s -> 3883.60s]  if you're curious about how
[3883.60s -> 3885.60s]  these work that I've presented are pretty new.
[3885.60s -> 3887.60s]  But there has been a bit of discussion
[3887.60s -> 3889.60s]  in the robotics
[3889.60s -> 3891.60s]  applying potentially just to robotics
[3891.60s -> 3893.60s]  and of course I can't talk about details
[3893.60s -> 3895.60s]  but
[3895.60s -> 3897.60s]  things that
[3897.60s -> 3899.60s]  reinforcement learning in the wild people
[3899.60s -> 3901.60s]  worry about are some of the surprises.
[3901.60s -> 3903.60s]  Right?
[3903.60s -> 3905.60s]  If you're wondering if
[3905.60s -> 3907.60s]  you're interested in learning about
[3907.60s -> 3909.60s]  the TAS or AlphaZero
[3909.60s -> 3911.60s]  or the TAS or AlphaZero
[3911.60s -> 3913.60s]  or the surprises
[3913.60s -> 3915.60s]  if you have a test for it
[3915.60s -> 3917.60s]  like if you have a unit test for it
[3917.60s -> 3919.60s]  you're never going to fail
[3919.60s -> 3921.60s]  because you're going to test before you deploy.
[3921.60s -> 3923.60s]  I think the biggest risk for any of these deployment systems
[3923.60s -> 3925.60s]  is the surprises
[3925.60s -> 3927.60s]  that you didn't expect.
[3927.60s -> 3929.60s]  So my work around the visualization
[3929.60s -> 3931.60s]  and others
[3931.60s -> 3933.60s]  aim to help you with that.
[3933.60s -> 3935.60s]  So we may not know names
[3935.60s -> 3937.60s]  of these surprises
[3937.60s -> 3939.60s]  but here's a tool that
[3939.60s -> 3941.60s]  gives you those surprises before someone else does
[3941.60s -> 3943.60s]  or someone else gets harmed.
[3949.60s -> 3951.60s]  Thanks so much for the talk.
[3951.60s -> 3953.60s]  This is kind of an open-ended question
[3953.60s -> 3955.60s]  but I was wondering, we're talking about
[3955.60s -> 3957.60s]  a lot of ways in which we try to
[3957.60s -> 3959.60s]  visualize or understand what's going on
[3959.60s -> 3961.60s]  in the representation inside the machine.
[3961.60s -> 3963.60s]  But I was wondering whether
[3963.60s -> 3965.60s]  we could turn it around
[3965.60s -> 3967.60s]  and try to teach machines to tell us
[3967.60s -> 3969.60s]  what's using our language
[3969.60s -> 3971.60s]  and what they're doing in their representations
[3971.60s -> 3973.60s]  because we don't represent machines of ours
[3973.60s -> 3975.60s]  and then get the machine to do the translation
[3975.60s -> 3977.60s]  for us instead of us going into the machine.
[3977.60s -> 3979.60s]  Yeah, great question.
[3979.60s -> 3981.60s]  So it's a really interesting question
[3981.60s -> 3983.60s]  because that's something that I
[3983.60s -> 3985.60s]  kind of
[3985.60s -> 3987.60s]  tried in my previous work
[3987.60s -> 3989.60s]  testing with the concept activation
[3989.60s -> 3991.60s]  vectors. So that was to
[3991.60s -> 3993.60s]  map human language into machine
[3993.60s -> 3995.60s]  space so that they can only speak our language
[3995.60s -> 3997.60s]  because I understand my language and just talk
[3997.60s -> 3999.60s]  to me in my language.
[3999.60s -> 4001.60s]  The challenge is that how would you do
[4001.60s -> 4003.60s]  that for something like alpha zero?
[4003.60s -> 4005.60s]  We don't have a vocabulary
[4005.60s -> 4007.60s]  for it, like move 37.
[4007.60s -> 4009.60s]  Then there's going to be a lot of missing
[4009.60s -> 4011.60s]  valuable knowledge
[4011.60s -> 4013.60s]  that we might not get from
[4013.60s -> 4015.60s]  the machine. So I think
[4015.60s -> 4017.60s]  the approach has to be both ways.
[4017.60s -> 4019.60s]  We should leverage as much as we can.
[4019.60s -> 4021.60s]  But acknowledging that even that
[4021.60s -> 4023.60s]  mapping, that trying to map
[4023.60s -> 4025.60s]  our language to machines is
[4025.60s -> 4027.60s]  not going to be perfect
[4027.60s -> 4029.60s]  because it's a kind of proxy for
[4029.60s -> 4031.60s]  what we think a penguin is.
[4031.60s -> 4033.60s]  There's a psychology research that says
[4033.60s -> 4035.60s]  everyone thinks very differently about what
[4035.60s -> 4037.60s]  penguin is. Like if I
[4037.60s -> 4039.60s]  picture a penguin,
[4039.60s -> 4041.60s]  everyone is thinking different penguin right now.
[4041.60s -> 4043.60s]  Australia has the cutest
[4043.60s -> 4045.60s]  penguin, the fairy penguin.
[4045.60s -> 4047.60s]  I'm thinking that, but I don't know how many people are thinking
[4047.60s -> 4049.60s]  that. So given that
[4049.60s -> 4051.60s]  we are so different, machines
[4051.60s -> 4053.60s]  are not thinking something else. So how do you bridge
[4053.60s -> 4055.60s]  that gap? Extend that to 100
[4055.60s -> 4057.60s]  concepts and composing those
[4057.60s -> 4059.60s]  concepts, it's going to go out a while very soon.
[4059.60s -> 4061.60s]  So there's pros and cons.
[4061.60s -> 4063.60s]  I'm into both of them. I think
[4063.60s -> 4065.60s]  some applications
[4065.60s -> 4067.60s]  exclusively just
[4067.60s -> 4069.60s]  using human concepts are still
[4069.60s -> 4071.60s]  very helpful. It gets you
[4071.60s -> 4073.60s]  halfway.
[4073.60s -> 4075.60s]  But my ambition is that
[4075.60s -> 4077.60s]  we shouldn't stop there. We should benefit from
[4077.60s -> 4079.60s]  them by having them
[4079.60s -> 4081.60s]  just new things that we didn't know
[4081.60s -> 4083.60s]  before.
[4083.60s -> 4085.60s]  Yeah.
[4085.60s -> 4087.60s]  So in the second thing you talked about with
[4087.60s -> 4089.60s]  Jerome, you said that like
[4089.60s -> 4091.60s]  where knowledge is located in the embedding space
[4091.60s -> 4093.60s]  isn't super poorly
[4093.60s -> 4095.60s]  changed that knowledge. Do you think
[4095.60s -> 4097.60s]  that has any implications for
[4097.60s -> 4099.60s]  the later stuff you talked about?
[4099.60s -> 4101.60s]  But like
[4101.60s -> 4103.60s]  trying to locate
[4103.60s -> 4105.60s]  specific strategies in the embedding space
[4105.60s -> 4107.60s]  might not be as helpful?
[4107.60s -> 4109.60s]  What are the alternatives?
[4109.60s -> 4111.60s]  I guess I don't know the alternatives
[4111.60s -> 4113.60s]  just because I feel like the Jerome thing
[4113.60s -> 4115.60s]  is like not stored.
[4115.60s -> 4117.60s]  That's possible. So like it's like some
[4117.60s -> 4119.60s]  transformed space of our embedding
[4119.60s -> 4121.60s]  space in alpha zero. Maybe it's a function
[4121.60s -> 4123.60s]  of applied to that embedding
[4123.60s -> 4125.60s]  space. So thinking about that
[4125.60s -> 4127.60s]  as a raw vector is
[4127.60s -> 4129.60s]  a dead end. Could be.
[4129.60s -> 4131.60s]  We'll see how this chess project
[4131.60s -> 4133.60s]  goes in a couple months.
[4133.60s -> 4135.60s]  I might rethink my strategy
[4135.60s -> 4137.60s]  but interesting thought.
[4137.60s -> 4139.60s]  Yeah.
[4139.60s -> 4141.60s]  So I'm a psychology major and I do realize that a lot of this stuff
[4141.60s -> 4143.60s]  that we're trying to hear
[4143.60s -> 4145.60s]  is how we figure out how
[4145.60s -> 4147.60s]  our brains work.
[4147.60s -> 4149.60s]  So do you think that this
[4149.60s -> 4151.60s]  would there be
[4151.60s -> 4153.60s]  stuff that's
[4153.60s -> 4155.60s]  applicable to neural networks?
[4155.60s -> 4157.60s]  And on the contrary, do you think
[4157.60s -> 4159.60s]  this interpretability of a study of neural networks
[4159.60s -> 4161.60s]  would help us understand stuff
[4161.60s -> 4163.60s]  for our own brain?
[4163.60s -> 4165.60s]  Jeff Hinton. He would
[4165.60s -> 4167.60s]  really like this. So I believe
[4167.60s -> 4169.60s]  you probably know about this story. I think
[4169.60s -> 4171.60s]  that's how it all started.
[4171.60s -> 4173.60s]  The whole neural network is to understand
[4173.60s -> 4175.60s]  human brain.
[4175.60s -> 4177.60s]  So
[4177.60s -> 4179.60s]  that's the answer to your question.
[4179.60s -> 4181.60s]  Interesting, however, in my view
[4181.60s -> 4183.60s]  there is some biases that we
[4183.60s -> 4185.60s]  have in
[4185.60s -> 4187.60s]  neuroscience because of the limitations
[4187.60s -> 4189.60s]  of tools like physical tools and
[4189.60s -> 4191.60s]  availability of humans that you can poke in.
[4191.60s -> 4193.60s]  I think that influences interpretability
[4193.60s -> 4195.60s]  research. And I'll try to give you an example of
[4195.60s -> 4197.60s]  what I mean. So in the
[4197.60s -> 4199.60s]  horizontal line and
[4199.60s -> 4201.60s]  vertical line neuron in cat brains, so they
[4201.60s -> 4203.60s]  put the prop in and figure out this
[4203.60s -> 4205.60s]  one neuron detects vertical lines and
[4205.60s -> 4207.60s]  you can validate it. It's really cool if you look at
[4207.60s -> 4209.60s]  the video. The video is still online.
[4209.60s -> 4211.60s]  Yeah, what is it?
[4211.60s -> 4213.60s]  Yes, yes, yes.
[4213.60s -> 4215.60s]  So why did they do that?
[4215.60s -> 4217.60s]  Well, because you had one cat
[4217.60s -> 4219.60s]  and a poor, poor cat.
[4219.60s -> 4221.60s]  And you had, we can only
[4221.60s -> 4223.60s]  probe a few neurons at a time,
[4223.60s -> 4225.60s]  right? So that
[4225.60s -> 4227.60s]  implies a lot, a few interpretability
[4227.60s -> 4229.60s]  research actually looked at,
[4229.60s -> 4231.60s]  are very focused on neuron-wise
[4231.60s -> 4233.60s]  representation, like this one neuron
[4233.60s -> 4235.60s]  must be very special. I actually think
[4235.60s -> 4237.60s]  that's not true. That was limited
[4237.60s -> 4239.60s]  by our ability, like physical ability
[4239.60s -> 4241.60s]  to plop organisms. But
[4241.60s -> 4243.60s]  in your network, you don't have to do that. You can apply
[4243.60s -> 4245.60s]  functions to embeddings. You can change the
[4245.60s -> 4247.60s]  whole embedding to something else, overwrite.
[4247.60s -> 4249.60s]  So that kind of is
[4249.60s -> 4251.60s]  actually a
[4251.60s -> 4253.60s]  obstacle in our thinking
[4253.60s -> 4255.60s]  rather than helping.
[4260.60s -> 4262.60s]  Okay, maybe we should call it there.
[4262.60s -> 4264.60s]  So for Thursday, we're not
[4264.60s -> 4266.60s]  having a lecture
[4266.60s -> 4268.60s]  on Thursday. There'll be
[4268.60s -> 4270.60s]  TAs and meetings here.
[4270.60s -> 4272.60s]  So if you have any last minute
[4272.60s -> 4274.60s]  panic from your project
[4274.60s -> 4276.60s]  or think we might have some
[4276.60s -> 4278.60s]  great insight to help you,
[4278.60s -> 4280.60s]  we probably won't actually.
[4280.60s -> 4282.60s]  Do come along
[4282.60s -> 4284.60s]  and you can chat to us
[4284.60s -> 4286.60s]  at the final project.
[4286.60s -> 4288.60s]  That means
[4288.60s -> 4290.60s]  that Phoenix got to give the final
[4290.60s -> 4292.60s]  lecture of CS224
[4292.60s -> 4294.60s]  in today.
[4294.60s -> 4296.60s]  Thank you.
