# Detected language: en (p=1.00)

[0.00s -> 13.20s]  Good evening people. How are you guys doing? All right. My name is Arshad Sharma. I'm
[13.20s -> 18.24s]  a PhD student at Stanford and I'm very, very excited to talk about post training
[18.24s -> 21.88s]  generally speaking for large language models and I hope you guys are ready to learn
[21.88s -> 26.16s]  some stuff because this has been one of the last few years in machine learning have
[26.16s -> 30.92s]  been very, very exciting with the advent of large language models, chat GPD and
[30.92s -> 35.56s]  everything to that extent and hopefully after today's lecture you will be more
[35.56s -> 39.44s]  comfortable understanding how we go from pre-trained models to models like
[39.44s -> 43.64s]  chat GPD and we'll take a whole journey through prompting instruction
[43.64s -> 54.88s]  fine-tuning and DPO in our lecture. So let's get started. All right. So
[54.92s -> 62.32s]  something that has been very fundamental to our entire field is this idea of
[62.32s -> 67.12s]  scaling laws and models are increasingly becoming larger and larger and they're
[67.12s -> 71.12s]  expanding more and more compute. So this is a graph of models starting all
[71.12s -> 75.18s]  the way back in 1950s to somewhere around these are still this is an
[75.18s -> 79.64s]  outdated graph so like this shows up to 10 to power 24 flops or floating
[79.64s -> 82.52s]  point operations that go into pre-training these models but the
[82.52s -> 87.28s]  number is well above 10 to power 26 now but you can see the graph and the way
[87.28s -> 91.92s]  it's trending and more and more compute requires more and more data
[91.92s -> 95.08s]  because you need to train on something meaningful and this is roughly the
[95.08s -> 98.54s]  trend on the amount of language tokens that are going into the language
[98.54s -> 103.92s]  models in pre-training and again this plot is outdated. Does anybody want
[103.92s -> 109.36s]  to guess like we're in 2024 2022 we were at 1.4 trillion tokens or words
[109.48s -> 113.08s]  roughly speaking in language model pre-training do it doesn't even only
[113.08s -> 117.00s]  guess like where we are in 2024
[120.16s -> 124.16s]  that's a pretty good guess yeah so we're close to 15 trillion tokens
[124.16s -> 129.56s]  recent llama 3 models were roughly trained on 15 trillion tokens so yeah
[129.56s -> 134.88s]  just just for a second appreciate that these are a lot of words this is not
[134.88s -> 139.00s]  yeah I don't I don't think anybody of us listens to like trillions of tokens
[139.00s -> 145.08s]  in our lifetime so this is where we are right now and I hope you guys were
[145.08s -> 151.24s]  here for the pre-training lectures cool so what do we do so like I mean
[151.24s -> 155.00s]  broadly speaking we are really just learning to predict text tokens or
[155.00s -> 159.68s]  language tokens but what do we learn in the process of pre-training why
[159.68s -> 163.64s]  is why are people spending so much money so much compute because these
[163.64s -> 167.72s]  compute and tokens take dollars to do and we're on the order of spending
[167.72s -> 172.20s]  hundreds of millions of dollars on these runs so why are we doing this and
[172.20s -> 176.20s]  this is basically a recall from whatever you've probably learned till
[176.20s -> 180.20s]  now but we're learning things like oh we're learning knowledge Stanford
[180.20s -> 184.24s]  University is located in Santa Clara California or wherever you want to like
[184.24s -> 188.48s]  say you're learning syntax you're learning semantics of the sentences
[188.48s -> 191.88s]  these are things that you would expect to learn when you're training on
[191.88s -> 195.84s]  language data broadly you're probably learning a lot about different
[195.84s -> 199.00s]  languages as well so like depending on your text data distribution you're
[199.00s -> 203.40s]  learning a lot of things but the models we interact with are very
[203.40s -> 206.96s]  intelligent so where is that coming from like I mean just simply learning
[206.96s -> 212.00s]  about very factual things and it's a very simple loss function we're
[212.00s -> 217.40s]  optimizing and where is that intelligence coming from and this perhaps is the
[217.40s -> 224.52s]  interesting bit recently like people have like started accumulating evidence
[224.52s -> 228.84s]  for that that when you optimize the next token prediction losses you're not
[228.84s -> 232.60s]  just learning about syntax you're not just learning knowledge but you're
[232.60s -> 236.96s]  starting to like form models of agents beliefs and actions as well so
[236.96s -> 240.96s]  how do we know this again a lot of this is speculative evidence but it's
[240.96s -> 244.00s]  baby to like form an understanding that the losses we're optimizing are not
[244.00s -> 247.28s]  just about the data fitting the data but you start learning something maybe
[247.28s -> 254.00s]  more meaningful as well for example like I mean in this specific case we
[254.00s -> 258.08s]  changed the last sentence and the prediction of the text of the next tech
[258.08s -> 262.96s]  that is predicted changes as well so here it starts with Pat watches a
[262.96s -> 266.00s]  demonstration of a bowling ball and leaf being dropped at the same time
[266.00s -> 270.24s]  Pat who is a physicist predicts that the bowling ball and the leaf land at
[270.24s -> 274.80s]  the same rate we all know gravity the way it works but when you late change
[274.80s -> 279.28s]  the last sentence to Pat who has never seen this demonstration before Pat
[279.28s -> 282.64s]  predicts that bowling ball will fall to the ground first maybe somebody who's
[282.64s -> 286.60s]  never seen this experiment before might intuitively believe that correct so
[286.60s -> 290.44s]  like I mean the language model was able to predict this so how do you predict
[290.44s -> 295.68s]  this you have to have some notion of understanding of how humans work to
[295.68s -> 299.16s]  even be able to predict this and that's maybe like something that is not
[299.16s -> 305.08s]  obvious when you're simply optimizing to predict the text similarly like I
[305.08s -> 307.48s]  mean these kind of examples are we're going to run through some examples to
[307.48s -> 310.12s]  like sort of communicate that when you're pre training these models you're
[310.12s -> 313.84s]  learning much more than just language tokens and so on you're also learning
[313.84s -> 317.88s]  about math like you're able to understand what a graph of a circle
[317.88s -> 322.44s]  means and what the center is and we're how to like understand equations
[322.44s -> 327.00s]  probably my favorite example something I use pretty much every day is you're
[327.00s -> 330.68s]  learning how to write code so I don't know how many of you have
[330.68s -> 334.88s]  interactive that copilot before but if you have like you probably know like if
[334.88s -> 338.08s]  you write down a few comments write on a function template it will
[338.08s -> 343.48s]  automatically complete code for you so again it's not perfect but it has to
[343.48s -> 346.88s]  have some deeper understanding of what your intent is for something like that
[346.88s -> 352.00s]  to emerge and similarly we have examples from medicine as well I don't
[352.00s -> 355.36s]  know about you guys but like whenever I have some issue I probably go to chat
[355.36s -> 358.64s]  GPD or Claude or something to that effect and ask them a diagnosis for
[358.64s -> 364.56s]  those things as well I don't recommend that please don't take medical advice
[364.60s -> 370.92s]  from me but yeah so broadly like the way we're seeing language models at this
[370.92s -> 373.44s]  point is that like they're sort of emerging as its general-purpose
[373.44s -> 377.92s]  multitask assistance and it's very strange right like I mean you started
[377.92s -> 380.48s]  off a text token prediction and we're reaching the stage where you can like
[380.48s -> 384.40s]  sort of rely to them on them to do many many different things so how are
[384.40s -> 387.12s]  we getting there and I'm sure you all are aware of like what these models
[387.12s -> 393.80s]  are so yeah so today's lecture is largely going to be about how do we go
[393.80s -> 397.96s]  from something Stanford University is located this very simple pre-training
[397.96s -> 402.00s]  task a very simple procedure well it's more complicated but in abstract terms
[402.00s -> 407.00s]  it's not very complicated do like something as powerful as chat GPD cool
[407.00s -> 412.60s]  so I recommend you guys stopping me asking me a lot of questions because
[412.60s -> 415.48s]  there's a there's a lot of fun examples and a lot of fun techniques so
[415.48s -> 420.36s]  like I want you guys to like learn everything about here so the overall plan
[420.36s -> 425.32s]  is we're going to talk about zero shot and few shot in context learning next
[425.32s -> 428.44s]  we're going to follow up with instruction fine-tuning and then we're
[428.44s -> 431.76s]  going to talk about optimizing for preferences and this is where roughly
[431.76s -> 435.68s]  things are right now in the industry and then we're going to talk about
[435.68s -> 442.44s]  what's next what the limitations are and how do we move from here go so
[442.44s -> 446.80s]  we're going to start up a zero shot and few shot in context learning
[447.68s -> 451.64s]  broadly we're going to take an example of GPT or the generative pre-trained
[451.64s -> 454.88s]  transformer and this is a whole series of models that started off in roughly
[454.88s -> 460.12s]  2018 and like up to 2020 they were building GPT GPT to GPT 3 so we're
[460.12s -> 465.00s]  going to start off with this example and yes so it's a decoder only model
[465.00s -> 469.96s]  that is trained on roughly 4.6 gigabytes of text and it has 12
[469.96s -> 472.92s]  layers of transformers there's and it's trained with the next token
[472.96s -> 478.68s]  prediction loss and the first model obviously was not extremely good but it
[478.68s -> 482.52s]  started showing that like hey like this technique for pre-training can be
[482.52s -> 485.68s]  very effective for general purpose tasks and we're going to see some
[485.68s -> 491.04s]  examples for example like I mean here it's able to do the task for
[491.04s -> 502.64s]  entertainment and yeah and GPD 1 itself was not very strong as a model so like
[502.64s -> 506.08s]  but they took the same recipe and like I mean tried to like increase the
[506.08s -> 510.52s]  model size so they went from 117 million parameters to about 1.5 billion
[510.52s -> 514.60s]  parameters and we're now scaling up the data alongside as well so we've
[514.60s -> 518.36s]  been from 4 gigabytes of data to approximately 40 gigabytes of data and
[518.40s -> 522.36s]  pre-training is a whole different like melting pot of techniques and there's a
[522.36s -> 525.52s]  lot that goes into it but like roughly for example here they filter
[525.52s -> 531.80s]  data by the number of upwards on their added data and yeah so this is
[531.80s -> 535.24s]  roughly where we are and I think one of the things that started emerging
[535.24s -> 540.52s]  with GPT 2 is zero shot learning and what do we mean by zero shot
[540.52s -> 546.20s]  learning conventionally in the field like when we pre-trained models there
[546.24s -> 550.88s]  was the idea that you take a few examples you update the model and then you are
[550.88s -> 555.32s]  able to adapt to a specific task but as you pre-trained on more and more data
[555.32s -> 558.40s]  and more and more tasks you sort of start seeing this phenomena where
[558.40s -> 562.00s]  they're able to do the task basically zero shot they're shown no
[562.00s -> 565.36s]  examples of how to do the task and you can start thinking about how you
[565.36s -> 570.00s]  can do it summarization you can follow some instructions you can do maybe a
[570.00s -> 573.36s]  little bit of math as well so this is where the idea of zero shot learning
[573.36s -> 582.56s]  started to emerge yeah so how do we do zero shot learning or task specific
[582.56s -> 585.80s]  learning from these pre-trained models really the idea is like we have
[585.80s -> 589.52s]  to be creative here we know that these are text prediction models if you
[589.52s -> 593.32s]  put in a text they will complete whatever follows so if we can sort of
[593.32s -> 597.12s]  course these models into completing the task we care about maybe it's
[597.12s -> 602.80s]  question answering we can so start getting them to solve tasks here so for
[602.80s -> 607.00s]  example if you want to ask questions about Tom Brady you sort of set it up
[607.00s -> 610.80s]  you saw to put information about Tom Brady and then you put a question that
[610.80s -> 613.96s]  you wanted to answer and then it will auto complete in some sense so
[613.96s -> 617.44s]  this is one early perspective on these models these are very advanced
[617.44s -> 623.52s]  autocomplete models and similarly if you want to figure out like which
[623.52s -> 626.76s]  answer is true or which is not something that it is very useful to
[626.76s -> 632.72s]  measure is log probabilities so for example we want to figure out what is
[632.72s -> 636.68s]  the word it referring to here in the sentence the cat couldn't fit into the
[636.68s -> 641.32s]  hat because it was too big what we can do is we can take the sentence
[641.32s -> 646.00s]  replace it with either the cat or either the hat and then you can
[646.00s -> 650.04s]  measure the probability of which more which one does the model think is
[650.04s -> 654.84s]  higher and you can sort of get the idea of what the reference here is so
[654.84s -> 658.40s]  none of this is like in the training data it's simply learning to
[658.40s -> 661.72s]  predict text but you can start seeing how like we can leverage these models
[661.72s -> 669.24s]  to do other tasks as well besides prediction so this is just more
[669.24s -> 674.28s]  evidence about like how GPT-2 no task specific fine-tuning no task specific
[674.28s -> 679.28s]  training it simply is learning to predict text and it establishes the
[679.28s -> 682.76s]  state-of-the-art on many many different tasks simply by scaling up the
[682.76s -> 690.36s]  model parameters and scaling up the amount of data it's trained on so this
[690.36s -> 696.00s]  is a fun example so if you want to do summarization for data or like you have
[696.00s -> 699.72s]  a news article that you want to summarize so how do you get a zero shot
[699.72s -> 704.20s]  model to do it this answer is you put the document into the context and
[704.20s -> 710.12s]  you simply put TLDR in front of it now like I mean if most of the data
[710.12s -> 714.64s]  on internet whenever you see TLDR you'll naturally summarize it so yeah
[714.64s -> 717.44s]  you can get zero shot performance and summarization here as well and again
[717.48s -> 721.84s]  this is not trained to do summarization in any specific way and it's still
[721.84s -> 730.16s]  doing really well simply because of its pre-training data so yeah I think
[730.16s -> 733.20s]  GPT-2 TLDR is somewhere there and like some of the very task
[733.20s -> 738.96s]  specific train models are like and I think you will see the trend but again
[738.96s -> 742.36s]  if you were Alec Radford or somebody like I mean you see like these cool
[742.36s -> 746.36s]  things emerging your next step would obviously be I'm gonna scale this up a
[746.40s -> 749.68s]  little more I'm gonna make an even bigger model I'm gonna train it on even
[749.68s -> 755.76s]  more data and we'll see how things go right so that's how we got GPT-3 we went
[755.76s -> 760.40s]  from 1.5 billion parameters to 175 billion parameters we are well over
[760.40s -> 764.64s]  like 40 gigabytes of data to 600 gigabytes of data of course they now
[764.64s -> 767.68s]  we're in like terabytes of data and text is a very compressed
[767.68s -> 773.52s]  representation so like terabytes of data is a lot and you know we talked
[773.56s -> 783.20s]  about zero-shot learning the cool thing that emerged in GPT-3 is like no you
[783.20s -> 786.80s]  typically put the passage if you have like interacted with reddit or something
[786.80s -> 790.40s]  like that typically somebody will write an entire post and then end with
[790.40s -> 794.80s]  TLDR here's the summary of the thing too long didn't read or if you have
[794.80s -> 802.92s]  used oh yeah there are situations where it also comes first but
[802.92s -> 807.08s]  one reason is that these are like decoder only models so like they are
[807.08s -> 810.20s]  often these are causal attention models so they typically need to see the
[810.20s -> 824.96s]  context before there's probably a lot of data where the TLDR comes first but
[824.96s -> 830.08s]  there's probably a lot of data where TLDR comes after as well cool so we saw
[830.16s -> 834.96s]  zero-shot learning emerging in GPT-2 few-shot learning maybe seems slightly
[834.96s -> 838.24s]  easier but like this is where things started getting really funny is that
[838.24s -> 840.76s]  like you're starting to beat state-of-the-art simply by just
[840.76s -> 845.96s]  putting examples in context so yeah what does few-shot learning mean here
[845.96s -> 852.60s]  what is what are we talking about as I mentioned like the typical idea here
[852.60s -> 856.12s]  is is that like you want to solve translation so you would put some
[856.16s -> 861.72s]  examples of translation into context and you know this is a correction task
[861.72s -> 865.84s]  here or maybe you want to be interested in translation and no gradient updates
[865.84s -> 870.96s]  no learning in any conventional sense whatsoever you put a few examples in
[870.96s -> 875.08s]  and that's it like I mean you know how to solve the task isn't that like
[875.08s -> 880.32s]  crazy like you're built you guys did the assignment on translation right so
[880.44s -> 888.16s]  but this is what the modern NLP looks like so yeah you put in some examples
[888.16s -> 892.04s]  and you have the entire system and this is where things are really interesting
[892.04s -> 895.40s]  is that all these tasks specific models that were created to like be
[895.40s -> 898.88s]  really really good at translation or really good at summarization you can
[898.88s -> 903.44s]  just put let's look at this graph so we start with a zero shot performance of
[903.44s -> 906.40s]  this in a similar fashion that I described earlier and you start
[906.40s -> 909.96s]  somewhere there you put one example in of translation from English to French
[910.12s -> 914.12s]  you get to somewhere like already at a fine-tuned level few examples in
[914.12s -> 919.08s]  you're already starting to like be close to the state-of-the-art models
[919.08s -> 923.08s]  Wait, but in that graph the state-of-the-art is really high, isn't that the fine-tuned state-of-the-art model?
[923.08s -> 927.88s]  fine-tuned word of the word plus plus here I think is like the word I'm referring to
[927.88s -> 931.64s]  fine-tuned state-of-the-art like where it's just trained exclusively on a lot of
[931.64s -> 937.80s]  translation data might be like slightly better yes and I think that's the
[937.80s -> 941.00s]  relevant comparison here is is the in context learning starts to emerge at
[941.00s -> 948.32s]  scale so and this is I think like the key point is that this some of this is
[948.32s -> 952.84s]  contested just to be very upfront but like there's this idea of emergence of
[952.84s -> 956.24s]  this property as you train on more compute and more scale there's more
[956.24s -> 959.84s]  recent research which suggests that if you plot the x-axis correctly it
[959.84s -> 963.48s]  feels less emergent but the general idea is as you increase the number of
[963.48s -> 967.76s]  parameters and increase the number of compute that is going into the models
[967.76s -> 971.40s]  the ability to just go from a few examples to really strong performance
[971.40s -> 981.44s]  is very compelling cool and yeah I think as I explained earlier the general
[981.44s -> 984.16s]  idea is that this is very different from the conventional idea of
[984.16s -> 987.80s]  fine-tuning that we typically go for instead of like iterating over examples
[987.80s -> 992.24s]  putting into context and doing gradient updates we are actually just going for
[992.24s -> 995.12s]  a few short prompting we're gonna put in few examples and that's gonna give
[995.12s -> 1007.20s]  us the system yes I mean the exact details roughly can depend on the prom
[1007.20s -> 1012.32s]  template that you use but typically you would just put examples so like sea
[1012.32s -> 1016.44s]  otter and put these examples and then whatever your task is you can just let
[1016.44s -> 1020.64s]  the model complete from there because it can infer the task based on the
[1020.64s -> 1032.00s]  examples you've given any other questions cool so yeah like I mean we
[1032.00s -> 1035.44s]  have gotten from zero shot prompting and like we've seen for seeing that few
[1035.44s -> 1038.92s]  shot prompting is becoming really competitive with good models but there's
[1038.92s -> 1041.52s]  still limitations to this like I mean you cannot solve every task that you
[1041.52s -> 1045.80s]  see here and particularly like things that involve like richer multi-step
[1045.80s -> 1049.44s]  reasoning is something that actually can be pretty challenging and just to be
[1049.44s -> 1053.92s]  fair human struggle at these tasks as well so things like addition and so on
[1053.92s -> 1058.56s]  like these are these are probably like still still hard to do like when
[1058.56s -> 1063.40s]  you keep increasing the number of digits but one thing that you have to
[1063.40s -> 1066.64s]  start being creative with I alluded to this earlier is that you can get
[1066.64s -> 1070.92s]  these models through the task if you're creative in how you prompt the model
[1070.92s -> 1077.48s]  and this is what we're gonna see next so this technique called chain of
[1077.48s -> 1081.24s]  thought prompting emerged here and the idea that we have explored thus far is
[1081.24s -> 1088.04s]  that we put in examples of the kind of tasks we want to do and we expect the
[1088.04s -> 1093.32s]  model to zero shot learn what the task is and go from there the idea is that
[1093.32s -> 1097.24s]  like instead of just showing what the task is you show them examples where
[1097.24s -> 1101.00s]  the reason through the task so they're not just learning to do the task but
[1101.00s -> 1104.36s]  also learning how the reasoning is working so in this example initially we
[1104.36s -> 1107.72s]  started with like we have to solve a simple math problem and we are just
[1107.72s -> 1111.88s]  shown exactly the answer directly instead of doing that and if you do
[1111.88s -> 1115.72s]  that directly you observe that the model gets the answer wrong instead
[1115.72s -> 1119.68s]  of that what if you show model how to reason about the task showed a chain
[1119.68s -> 1124.96s]  of thought and include that in the prompt as well and then you ask it a
[1124.96s -> 1129.36s]  new question the idea is that now the model is not just going to output an
[1129.36s -> 1133.36s]  answer it's going to reason about the task and it's going to do actually a
[1133.36s -> 1138.96s]  lot better and this has been shown to be very effective chain of thought is
[1138.96s -> 1143.76s]  also as you can see like I mean it's again something that improves a lot
[1143.76s -> 1150.24s]  with model scale it's not just yeah but what you can probably start
[1150.24s -> 1155.16s]  saying is like it's nearly better than supervised best models here so
[1155.16s -> 1159.60s]  power models roughly were about five forty forty billion parameters and
[1159.60s -> 1163.08s]  simply with this chain of thought kind of skill you're already like beating
[1163.08s -> 1173.40s]  state-of-the-art cool so yeah so I showed you examples of chain of thought
[1173.40s -> 1176.80s]  reasoning tub to this point where you go through a reasoning chain but you
[1176.80s -> 1180.12s]  can be even slightly smarter than that you might not even need to show them
[1180.12s -> 1183.56s]  any examples you just need to trick them into thinking about what to do
[1183.56s -> 1191.88s]  next so yeah the sim this idea emerged in this paper called very well let's
[1191.88s -> 1196.36s]  think step by step where instead of even showing an example you just start
[1196.36s -> 1202.36s]  your answer with let's think step by step and that's it like I mean the
[1202.36s -> 1206.36s]  model will start reasoning about the answer itself instead of just like
[1206.36s -> 1214.00s]  autocompleting to an answer and you get something like this so maybe you
[1214.00s -> 1217.32s]  don't even need to show any examples like you can probably induce the
[1217.32s -> 1222.72s]  reasoning behavior through zero shot behavior as well and again what the
[1222.72s -> 1226.80s]  final numbers look like is like compared to zero shot performance that we got
[1226.80s -> 1231.00s]  from essentially autocompleting this zero shot chain of thought
[1231.00s -> 1234.24s]  substantially improves the performance so you go from like seventeen point
[1234.24s -> 1237.68s]  seven to seventy eight point seven it's still worse than still putting
[1237.68s -> 1241.44s]  like examples of reasoning and multi shot few shot chain of thought as
[1241.44s -> 1245.12s]  well but you can see like how much it improves the performance simply by
[1245.12s -> 1249.52s]  asking you to let's think by the step by step and maybe this is like a lesson
[1249.52s -> 1253.40s]  that interacting with these models is like when you interact with these
[1253.40s -> 1257.92s]  models you might not get the exact desired behavior from these models up
[1257.92s -> 1263.12s]  front but often like these models are capable of doing the behavior that you
[1263.12s -> 1267.48s]  might want and often you have to think about how to induce that behavior
[1267.48s -> 1270.80s]  such that and the right way to think perhaps is like what is the
[1270.80s -> 1273.40s]  pre-training data what is them data on the internet it might have seen
[1273.40s -> 1278.16s]  which induces a similar behavior to the kind I want and you probably want to
[1278.16s -> 1281.96s]  like think about that and then induce these kinds of behaviors from
[1281.96s -> 1290.32s]  those models and yeah like I mean you know we've designed some of these
[1290.32s -> 1294.52s]  prompts you can also like get an LM to design these prompts as well there's
[1294.52s -> 1298.72s]  like recursive self-improving ideas here that happen and you can bump up
[1298.72s -> 1306.56s]  the performance a little bit more cool so what we have seen so far is that as
[1306.56s -> 1311.12s]  models get stronger and stronger you can start forcing them to do your task
[1311.12s -> 1315.12s]  zero shot or with few shot examples and you can trick them into
[1315.12s -> 1321.36s]  thinking what task you want them to solve but the downside is that there's
[1321.36s -> 1325.20s]  only so much you can fit into context this might not be very true
[1325.20s -> 1330.16s]  anymore what is like becoming increasingly larger context but it's
[1330.16s -> 1333.48s]  still somewhat unsatisfactory think you have to trick the model into doing your
[1333.48s -> 1339.36s]  task rather than like it just doing the task you wanted to do and potentially
[1339.36s -> 1342.68s]  like I mean going forward like you probably still want to fine-tune these
[1342.68s -> 1346.60s]  models for more and more complex tasks and that's where we're gonna go
[1346.60s -> 1352.72s]  forward in this next section we're gonna cover is instruction fine-tuning and
[1352.72s -> 1358.92s]  the general idea we have right now is that as we talked about pre-training is
[1358.92s -> 1363.56s]  not about assisting users it is about predicting the next token now you can't
[1363.56s -> 1367.88s]  trick it into assisting users and is following the instruction you wanted
[1367.88s -> 1371.92s]  to but in general that's not what it retrained it for and this is an
[1371.92s -> 1376.08s]  example of where if you ask GPT 3 pretty strong model to explain like
[1376.08s -> 1379.24s]  moon landing to a six-year-old in a few sentences and it will follow up
[1379.32s -> 1382.80s]  with more questions about what a six-year-old might want this is not
[1382.80s -> 1388.20s]  what you wanted the model to do right so it's a general term that people use
[1388.20s -> 1392.48s]  these days is that they are not aligned with user intent and the next
[1392.48s -> 1395.24s]  section that we're going to cover are going to talk about how to align it
[1395.24s -> 1397.56s]  with the user intent so that they don't have to trick the model into
[1397.56s -> 1404.00s]  whatever we wanted to do and this is a kind of like desired completion we
[1404.08s -> 1411.52s]  want at the end of instruction tuning and yeah how do we get from those
[1411.52s -> 1417.80s]  pre-trained models to models which can respond to user intent again I hope
[1417.80s -> 1421.52s]  this was covered somewhere in the class the general idea of pre-training and
[1421.52s -> 1425.48s]  fine-tuning but what you have probably seen thus far is that you
[1425.48s -> 1430.00s]  pre-trained on a lot of different language desk on data but then you
[1430.00s -> 1434.32s]  fine-tune on your specific task so you're taking the same decoder only
[1434.32s -> 1438.80s]  models and you're fine-tuning to a some task with very little amount of
[1438.80s -> 1442.48s]  data the thing that is going to be different now is not that we're no
[1442.48s -> 1445.24s]  longer fine-tuning on a little amount of data we're going to find you know
[1445.24s -> 1448.84s]  many many different tasks and we're going to just try to put them into a
[1448.84s -> 1455.88s]  single usable UX for users and this is where fine-tuning or instruction
[1456.08s -> 1464.88s]  fine-tuning comes in cool so again the recipe is not very very complicated here
[1464.88s -> 1468.60s]  we're going to collect a lot of examples of instruction and output
[1468.60s -> 1472.24s]  pairs and the instructions are going to range over several tasks different
[1472.24s -> 1474.92s]  forms there's going to be question answering they're going to be
[1474.92s -> 1478.40s]  summarization translation code reasoning and so on and we're going
[1478.40s -> 1484.20s]  to collect a lot of examples related to all those tasks and the idea is like
[1484.24s -> 1488.68s]  coming your train on instruction and output pairs exactly with them and then
[1488.68s -> 1492.96s]  we're going to evaluate on some unseen tasks as well so this is a
[1492.96s -> 1498.80s]  general paradigm of instruction fine-tuning and again it's the same
[1498.80s -> 1502.08s]  idea which we explored in pre-training is that data plus scale is really
[1502.08s -> 1506.92s]  important and these days like I mean you start off with like one task
[1506.92s -> 1510.56s]  you're now extending it over thousands of thousands and thousands of tasks with
[1510.56s -> 1514.00s]  like three million plus examples and this is generally like a broad range of
[1514.00s -> 1518.60s]  tasks that you might see in instruction fine-tuning data sets and yeah you
[1518.60s -> 1521.28s]  might even think of like why are we calling it fine-tuning anymore like
[1521.28s -> 1526.88s]  it's almost starting to look like pre-training but yeah we can these are
[1526.88s -> 1533.08s]  just terms so you can decide whatever is it you were comfortable with so yeah
[1533.08s -> 1537.40s]  we we get this like huge instruction data set we fine-tune our model the
[1537.40s -> 1543.12s]  next question is like how do we evaluate these data sets now I think
[1543.36s -> 1546.24s]  you guys will see another lecture on evaluation so I don't want to like dive
[1546.24s -> 1550.56s]  too deep into this but generally evaluation of these language models is
[1550.56s -> 1554.48s]  an extremely tricky topic there's a lot of biases that you need to deal
[1554.48s -> 1558.56s]  with and a lot of this will be covered but some more recent progress
[1558.56s -> 1561.64s]  on this is like we are starting to curate these like really large
[1561.64s -> 1566.88s]  benchmarks like MML you where the models are tested in a broad range of
[1566.88s -> 1571.44s]  diverse knowledge and this is just one example which is and these are the
[1571.48s -> 1576.12s]  topics that you will see and just to give some intuition of what the examples
[1576.12s -> 1580.32s]  in these evaluation look like under astronomy you might be asked what is
[1580.32s -> 1584.20s]  true for type 1a supernova or you might be asked some questions about
[1584.20s -> 1589.32s]  biology and there's a huge host of tasks for this and you can typically
[1589.32s -> 1592.96s]  like these are multi-choice questions and you can ask the model to answer
[1592.96s -> 1595.52s]  the question if their instruction fine-tuned already hopefully they can
[1595.52s -> 1599.48s]  like simply answer the question but you can also chain of thought prompt
[1599.48s -> 1604.44s]  these questions or future prompt these questions too and recently there's been
[1604.44s -> 1609.12s]  a huge amount of progress on this benchmark what people have observed is
[1609.12s -> 1611.80s]  like more and more pre-training on more and more data and larger models
[1611.80s -> 1616.88s]  is simply just like climbing up these the number on this so 90% is
[1616.88s -> 1620.92s]  often seen as a benchmark number that these model wanted to cross because
[1620.92s -> 1625.40s]  it's roughly like human level knowledge or understanding and recently
[1625.40s -> 1631.76s]  the gemini models perfectly crossed this number so yeah go ahead
[1631.76s -> 1638.52s]  isn't this like the entire sort of thing all over again like ImageNet at some point
[1638.52s -> 1644.32s]  you're like okay maybe my methods are too fine-tuned implicitly on the ImageNet
[1644.32s -> 1647.04s]  and isn't something like that happening here as well?
[1647.04s -> 1653.40s]  yes I think this is a tricky topic because a lot of the models often
[1653.40s -> 1657.36s]  there's this idea about whether your test sets are leaking into your training
[1657.36s -> 1661.44s]  data set and there's huge concerns about that it's a perfectly valid
[1661.44s -> 1665.16s]  question to ask how do we even evaluate this is why evaluation is
[1665.16s -> 1669.52s]  actually very tricky but one general thing to be careful about is like at
[1669.52s -> 1672.72s]  some point like it doesn't matter what your trained test is if the models are
[1672.72s -> 1676.92s]  generally useful if their models are doing useful stuff like does it matter
[1676.92s -> 1681.72s]  like if your tests if you're trained on everything you care about and it
[1681.72s -> 1688.84s]  does well on it like does it matter so yeah again we still need better ways to
[1688.84s -> 1692.96s]  evaluate the models and to understand what methods are doing and how they're
[1692.96s -> 1696.76s]  if they're improving the model or not but at some point like that those
[1696.76s -> 1704.96s]  boundaries start to like be less important cool so massive progress on
[1704.96s -> 1709.16s]  this benchmark starting with gpt2 and like we are in roughly at 90% which to
[1709.16s -> 1712.68s]  the point where these benchmarks are starting to become unclear if like
[1712.68s -> 1717.40s]  improvements on these are actually meaningful or not in fact like most of
[1717.40s -> 1721.04s]  the times when the models are wrong like you might often find that the
[1721.04s -> 1726.08s]  question itself was unclear ambiguous so all evaluation benchmarks have a
[1726.08s -> 1732.68s]  certain limited utility to them so yeah I'm gonna go over like another
[1732.68s -> 1738.20s]  evaluation example of how this recipe like changes things so t5 models were
[1738.24s -> 1742.68s]  instruction fine-tuned on a huge number of tasks and another trend to him or
[1742.68s -> 1746.40s]  which I think will be the team across this lecture is that as your models
[1746.40s -> 1749.56s]  become larger as they're trained on more data they become more and more
[1749.56s -> 1753.68s]  responsive to your task information as well so what you will observe here
[1753.68s -> 1757.52s]  is like as a number of parameters increase we have like t5 small front
[1757.52s -> 1761.96s]  t5 small and we go up to 11 billion parameters we have where we have
[1761.96s -> 1767.84s]  t5 X XL you'll see that the improvement actually improves like going from a
[1767.84s -> 1771.60s]  pre-trained to an instruction model the instruction model is all the more better
[1771.60s -> 1776.28s]  at following instructions so the difference is plus 6.1 and goes to plus
[1776.28s -> 1781.52s]  26.6 as the models become larger so this is another very encouraging trend
[1781.52s -> 1786.72s]  that you probably should train on a lot of data with a lot of compute and
[1786.72s -> 1796.04s]  you know pre training just keeps on giving so yeah you I hope you guys
[1796.04s -> 1798.44s]  get a chance to like play with a lot of these models I think you already
[1798.44s -> 1803.96s]  hopefully are but yeah before instruction fine-tuning something when
[1803.96s -> 1808.36s]  you're asked a question related disambiguation QA you get something
[1808.36s -> 1812.40s]  like this and it doesn't actually follow the let's think by step-by-step
[1812.40s -> 1817.32s]  instruction very clearly but after instruction fine-tuning it is able to
[1817.32s -> 1823.20s]  answer the question here and yeah like more recently people have been like
[1823.20s -> 1825.84s]  researching into like what the instruction tuning data sets should
[1825.84s -> 1829.12s]  look like there's a huge plethora of instruction tuning data sets now
[1829.12s -> 1832.16s]  available like this is just a representative diagram and there's a
[1832.16s -> 1837.64s]  view open source community developing around these as well some high-level
[1837.64s -> 1843.88s]  lessons that we have learned from this is one lesson that I think might
[1843.88s -> 1847.28s]  be interesting is that we can actually use really large strong models
[1847.28s -> 1850.52s]  to generate some of the instruction tuning data to train some of our
[1850.52s -> 1855.20s]  smaller models so take your favorite model right now gpt4 maybe or maybe
[1855.20s -> 1859.04s]  clod or whatever and you can get it to answer some questions and generate
[1859.04s -> 1864.04s]  instruction output pairs for training your open source or smaller model and
[1864.04s -> 1867.72s]  actually is a very successful recipe so instead of getting a human to
[1867.72s -> 1871.08s]  collect all the instruction output pairs or getting humans to generate the
[1871.08s -> 1874.88s]  answers you can get bigger models to generate the answers as well so that's
[1874.88s -> 1879.68s]  number one thing that like has recently emerged another thing that is being
[1879.68s -> 1883.32s]  emerged or is like being discussed is how much data do we need I talked
[1883.36s -> 1886.92s]  about millions of examples but like people have often found that if you have
[1886.92s -> 1890.88s]  really high quality example you can get away with thousand examples as well so
[1890.88s -> 1894.40s]  this is the paper less is more for alignment and this is still an active
[1894.40s -> 1897.84s]  area of research and how like data scaling and instruction tuning affects
[1897.84s -> 1903.40s]  the final model performance and yeah crowdsourcing these models can be
[1903.40s -> 1907.36s]  effective as well so they're in very cool benchmarks are emerging like open
[1907.36s -> 1913.24s]  assistant yeah a lot of activity in the field and hopefully like a lot
[1913.24s -> 1920.64s]  more progress as we go on yes question sort of in the spirit of this Lima paper
[1920.64s -> 1927.52s]  doesn't like go or like I don't know like math word problems have this
[1927.52s -> 1933.00s]  desired structure so they shouldn't be just like between like some English
[1933.00s -> 1937.20s]  stuff and then just be like okay this is the best way we can get at some
[1937.20s -> 1942.40s]  point right because like code code has the structure where there's like you're
[1942.40s -> 1949.00s]  going sort of step-by-step and you're thinking in some way taking down a high
[1949.00s -> 1952.40s]  concept in this model so you can consider the code have like a high value
[1952.40s -> 1958.36s]  tokens so maybe like just doing so I think there's again pre-training is a
[1958.36s -> 1963.16s]  whole dark art that I am not completely familiar with but code
[1963.16s -> 1967.00s]  actually ends up being really useful in pre-training mixtures and people do
[1967.00s -> 1973.64s]  like up way core data quite a lot similarly like a mean but it depends
[1973.64s -> 1976.92s]  upon what the users are going to use the models for right some people might
[1976.92s -> 1979.60s]  use it for code some people might use for reasoning but that's not the
[1979.60s -> 1983.08s]  only task we care about as you might see later on in the next step we'll
[1983.08s -> 1986.88s]  discuss this as well is that people often use these models for creative
[1986.88s -> 1990.68s]  tasks they wanted to write a story they wanted to generate a movie script
[1990.68s -> 1995.00s]  or so on and I don't know if like necessarily training on reasoning
[1995.00s -> 2001.92s]  only tasked would help with that so good like there exists like some data
[2001.92s -> 2009.80s]  distribution which is like high value for creative tasks yes like I mean it
[2009.80s -> 2015.60s]  seems like a lot of people write about stories and everything on the
[2015.60s -> 2019.88s]  internet all the time like which is not code and sometimes like there's this
[2019.88s -> 2022.64s]  idea of hallucinations as well in this like food but you can often think
[2022.72s -> 2029.20s]  like hey like creativity might be a byproduct of hallucinations as well so I
[2029.20s -> 2033.12s]  don't know what exact data would like lead to like more creative models but
[2033.12s -> 2036.40s]  generally like there's a lot of data or a lot of stories that are written on
[2036.40s -> 2041.20s]  the internet which allows the model to be creative yeah but I don't know if
[2041.20s -> 2045.24s]  I have a specific answer to the question cool so we discussed
[2045.24s -> 2050.08s]  instruction fine-tuning very simple and very straightforward there's like no
[2050.12s -> 2053.60s]  complicated algorithms here just collect a lot of data and then you can
[2053.60s -> 2058.16s]  start leveraging the performance at scale as well like as models become
[2058.16s -> 2063.24s]  better these models also become more easily specifiable and they become
[2063.24s -> 2066.80s]  more responsive to tasks as well we're going to discuss some limitations
[2066.80s -> 2069.56s]  and I think this is like really important to understand why we are going
[2069.56s -> 2076.60s]  to optimize for human preferences cool so we talked a bit about this like
[2076.60s -> 2081.40s]  instruction fine-tuning is necessarily contingent on humans labeling the data
[2081.40s -> 2087.92s]  now humans it's expensive to collect this data especially as the questions
[2087.92s -> 2092.00s]  become more and more complex you want to answer questions about what I'm
[2092.00s -> 2096.20s]  which may be at physics PhD level or things to that effect these things
[2096.20s -> 2101.52s]  become increasingly expensive to collect so yeah this is maybe like perhaps
[2101.52s -> 2104.24s]  obvious like collecting data pre-training does not require any
[2104.24s -> 2107.76s]  specific data you scrape data off the web but for instruction fine-tuning you
[2107.76s -> 2110.96s]  probably need to recruit some people to write down answer to your instructions
[2110.96s -> 2115.64s]  so this can give them very expensive very quickly but there's more limitations
[2115.64s -> 2119.76s]  to this as well and we we're just discussing this like they're open-ended
[2119.76s -> 2123.56s]  tasks related to creativity that don't really have like an exact correct
[2123.56s -> 2128.00s]  answer to begin with so how do you generate the right answer to this kind
[2128.00s -> 2135.96s]  of a question and yeah like language modeling inherently like penalizes all
[2135.96s -> 2141.00s]  token level mistakes equally this is what supervised fine-tuning does as
[2141.00s -> 2144.80s]  well but often like not all mistakes are the same so this is an
[2144.80s -> 2148.80s]  example where you're trying to do this prediction task avatar is a fantasy
[2148.80s -> 2153.96s]  TV show and perhaps you can see like I mean calling it an adventure TV show
[2153.96s -> 2159.32s]  is perhaps okay but calling it a musical may be like a much worse
[2159.32s -> 2166.48s]  mistake but both these mistakes are penalized equally and I think one
[2166.48s -> 2170.88s]  general aspect which is like more becoming increasingly relevant is that
[2170.88s -> 2174.28s]  the humans that you might ask might not generate the right or the highest
[2174.28s -> 2178.56s]  quality answer your models are becoming increasingly competitive and you want
[2178.56s -> 2183.08s]  in some sense you're going to be limited by how high quality the answer
[2183.80s -> 2188.56s]  humans can generate but often I find that the models are generating better
[2188.56s -> 2192.24s]  and better answers so do we really want to keep relying on humans to write
[2192.24s -> 2199.52s]  down the answers or do we want to like somehow go over that so these are
[2199.52s -> 2204.84s]  the three problems we have talked about with instruction fine-tuning and
[2204.84s -> 2211.00s]  we made a lot of progress with this but this is not how we got charged GPT and
[2211.00s -> 2215.32s]  one high-level problem here is that even though when even when we are
[2215.32s -> 2221.16s]  instruction fine-tuning there is still a huge mismatch between the end goal is
[2221.16s -> 2225.88s]  to optimize for human preferences generate an output that a human might
[2225.88s -> 2230.36s]  like and we're still doing prediction kind of tasks where we're predicting
[2230.36s -> 2233.76s]  the next token but now in a more curated data set so there's still a
[2233.76s -> 2238.64s]  bit of a mismatch going on here and it's not exactly what we want to do
[2238.68s -> 2241.96s]  hopefully like I mean I'm gonna take a second here to pause because this is
[2241.96s -> 2247.20s]  important to understand the next section and if there's any questions feel free
[2247.20s -> 2259.96s]  to ask that's a good question so I think this is still one of the more
[2259.96s -> 2263.52s]  important steps that you take before taking the next step but people are
[2263.52s -> 2267.24s]  trying to like remove the step altogether and jump directly to the
[2267.24s -> 2271.84s]  next step so there's work emerging on that but yeah this is still a very
[2271.84s -> 2280.40s]  important step before we do the next step good it's probably also present in
[2280.40s -> 2285.48s]  pre-training and so how do you avoid that versus by having a lot of
[2285.48s -> 2290.12s]  data yeah that's a great question there's two different there's one
[2290.12s -> 2294.68s]  difference one major difference on pre-training pre-training covers a lot
[2294.76s -> 2300.16s]  more text so just for context like I mean as we talked about it's pre-training is
[2300.16s -> 2304.96s]  roughly 15 trillion tokens whereas like supervised instruction fine-tuning might
[2304.96s -> 2308.08s]  be somewhere on the order of millions to billions of tokens so it's like few
[2308.08s -> 2312.36s]  orders of magnitude lower typically you'd only see one answer for a specific
[2312.36s -> 2316.36s]  instruction but during pre-training you'll see multiple texts and multiple
[2316.36s -> 2321.00s]  completions for a same kind of a prompt now that's good because when you
[2321.00s -> 2324.00s]  see multiple answers or completions during pre-training you sort of start
[2324.00s -> 2328.04s]  to weigh different answers you start to put probability mass on different kind
[2328.04s -> 2331.04s]  of answers or completions but an instruction fine-tuning might force
[2331.04s -> 2337.52s]  you to put and wait on only one answer does it okay but generally yeah
[2337.52s -> 2344.60s]  like I mean this is a problem in both the stages you're right anything else
[2344.64s -> 2353.92s]  cool so as this whole thing alludes to we're going to start to attempt to
[2353.92s -> 2357.64s]  satisfy human preferences directly we're no longer going to like try to like
[2357.64s -> 2361.48s]  get humans to generate some data and try to do some kind of a token level
[2361.48s -> 2365.00s]  prediction loss we're going to try to optimize for human preferences
[2365.00s -> 2371.40s]  directly and that is the general field of RLHF and that's the final step in
[2371.44s -> 2376.64s]  typically getting a model like chart GPT so we talked about how collecting
[2376.64s -> 2379.64s]  demonstration is expensive and there's still a broad mismatch between the LM
[2379.64s -> 2382.80s]  objective and human preferences and now we're going to try and optimize for
[2382.80s -> 2388.56s]  human preferences directly so what is optimizing for human preferences even
[2388.56s -> 2393.60s]  mean to like concretely establish that let's go through like a specific
[2393.60s -> 2398.92s]  example in mind which is summarization we want to train a model to be better
[2398.92s -> 2402.76s]  at summarization and we want to satisfy human preferences so let's
[2402.76s -> 2406.64s]  imagine that the human is able to prescribe a reward for a specific
[2406.64s -> 2409.68s]  summary let's just pretend there is a reward function you and I can assign
[2409.68s -> 2413.32s]  say like reward this is plus one this is minus one or something to that
[2413.32s -> 2425.20s]  effect okay so in this specific case we have this input X which which is
[2425.20s -> 2428.32s]  about an earthquake in San Francisco so this is a news article that we want to
[2428.32s -> 2434.84s]  summarize and let's pretend that we had get these rewards and we want to
[2434.84s -> 2440.28s]  optimize this so we get one summary Y1 which gives us an earthquake hit and so
[2440.28s -> 2444.40s]  on and we assign a reward of 8.0 and another summary which gives us a reward
[2444.40s -> 2449.12s]  of 1.2 generally speaking like the objective that we want to set up is
[2449.12s -> 2454.00s]  something of the following form where we want to take our language model P
[2454.00s -> 2460.36s]  theta which generates a completion Y given an input X and we want to maximize
[2460.36s -> 2465.44s]  the reward of our XY where X is the input and Y is the output summary in
[2465.44s -> 2472.52s]  this specific task and maybe like just to like really concretely point out
[2472.52s -> 2476.32s]  something here this is different from everything that we have done in one
[2476.32s -> 2482.64s]  very specific way we are sampling from the model itself in the bottom term if
[2482.64s -> 2487.04s]  you see like we're using Y from P theta everything we've seen so far the data
[2487.04s -> 2490.20s]  is sampled from some other source either during pre-training either in
[2490.20s -> 2493.76s]  supervised fine-tuning and we're maximizing the log likelihood of those
[2493.76s -> 2497.68s]  tokens but now we're explicitly sampling from a model and optimizing
[2497.68s -> 2505.32s]  potentially a non-differentiable objective cool so broadly the RLHF
[2505.32s -> 2509.56s]  pipeline looks something like this and first step is still instruction tuning
[2509.56s -> 2513.72s]  something we have seen up until now where we take our pre-trained model we
[2513.72s -> 2517.60s]  instruction tune on a large collection of tasks and we get something which
[2517.60s -> 2523.40s]  starts responding to our desired intent or not but there are two more steps
[2523.40s -> 2525.84s]  after this which are typically followed in creating something like
[2525.84s -> 2530.36s]  instruct GPT the first step is estimating some kind of a reward model
[2530.36s -> 2534.04s]  something which tells us given an instruction how much would a human like
[2534.04s -> 2538.20s]  this answer or how much would a human hate this answer so we looked at
[2538.20s -> 2541.00s]  something like this earlier but I didn't talk about how do we even get something
[2541.00s -> 2545.64s]  like that that's the second step and then we take this reward model and we
[2545.64s -> 2548.80s]  optimize it through the optimization that I suggested earlier so the
[2548.80s -> 2554.52s]  maximizing the expected reward under your language model and we're going to
[2554.52s -> 2559.16s]  go over a lot over in the second and third steps so the first question we
[2559.16s -> 2562.92s]  want to answer is how do we even get like a reward model what about what
[2562.92s -> 2568.04s]  humans are going to like like this is a very ill-defined problem generally
[2568.04s -> 2573.52s]  speaking so there's there's two problems here that we can address first
[2573.52s -> 2577.44s]  is a human in the loop is expensive so let's say like if I ask a model to
[2577.44s -> 2581.56s]  like generate an answer and then I get a human to labels with some kind of
[2581.56s -> 2585.36s]  a score I'm doing this over millions of completions that is not very
[2585.36s -> 2591.88s]  scalable I I don't want to sit around and label millions of examples so this
[2591.88s -> 2595.44s]  is very easy like we're in a machine learning class so what are we gonna
[2595.48s -> 2598.76s]  do what we're gonna do is we're gonna train something which predicts what a
[2598.76s -> 2604.36s]  human would like or what a human might not like and this is roughly this is
[2604.36s -> 2607.48s]  essentially a machine learning problem where we take these reward scores and
[2607.48s -> 2612.28s]  we try to train a reward model to predict given an input and output what
[2612.28s -> 2617.16s]  the reward scores would look like simple simple machine learning
[2617.16s -> 2625.32s]  regression style problem you might have seen this earlier cool now there's
[2625.32s -> 2639.72s]  a bigger problem here and that's a good question generally like what we do is
[2639.72s -> 2643.52s]  like we still typically need reward models where they need to be able to
[2643.52s -> 2646.72s]  understand the text really well so like bigger models and like they're
[2646.72s -> 2650.00s]  typically initialized from the language model that you trained pre-trained as
[2650.00s -> 2654.00s]  well so you typically start with the pre-trained language model and do some
[2654.00s -> 2664.36s]  kind of prediction that we'll talk about and they'll give you a score it
[2664.36s -> 2668.88s]  doesn't need to it can put the X and Y like it only sees X and Y as an input
[2668.88s -> 2673.00s]  so it doesn't need to typically see it separated it's just gonna predict a
[2673.00s -> 2677.28s]  score at the end okay yeah at the X and Y is more for notational
[2677.28s -> 2682.08s]  convenience here because for us X and Y are different X's a question user
[2682.08s -> 2690.48s]  asked and why is something the model generated cool now this is the bigger
[2690.48s -> 2695.12s]  problem here and human judgments are very noisy we've talked about we want
[2695.12s -> 2699.68s]  to assign a score to a completion this is something that's like extremely
[2699.68s -> 2703.48s]  non-trivial to do so if I give you a summary like this what score are you
[2703.48s -> 2707.96s]  going to assign on a scale of 10 if you ask me on different days I'll give
[2707.96s -> 2713.16s]  a different answer first of all but across humans itself this this number is
[2713.16s -> 2717.64s]  not calibrated in any meaningful way so you could assign number of four point
[2717.64s -> 2720.12s]  one six point six and different humans will just simply assign
[2720.12s -> 2724.20s]  different scores and there are ways to address this you can like calibrate
[2724.20s -> 2727.72s]  humans you can give them a specific rubric you can like talk to them but
[2727.72s -> 2731.76s]  it's a very complicated process and like still like there's a lot of room
[2731.76s -> 2735.36s]  for judgment which is not typically very nice for training a model like this
[2735.36s -> 2743.44s]  if your labels can vary a lot it's just hard to predict so the way this is
[2743.44s -> 2747.64s]  addressed is that instead of trying to predict the reward label directly you
[2747.64s -> 2750.56s]  actually want to set up a problem in slightly different way what is something
[2750.56s -> 2755.08s]  much easier for humans to do is that give them two answers or maybe many
[2755.08s -> 2759.36s]  answers and tell them ask them which one is better so this is where the
[2759.36s -> 2766.16s]  idea of asking humans to rank answers comes in so if I give you a whole news
[2766.16s -> 2769.32s]  article and ask you which summary is better you might be able to give me a
[2769.32s -> 2772.52s]  ranking that oh this second summary is the worst with the first one is
[2772.52s -> 2775.40s]  better and the third one is somewhere in the middle between those two so you
[2775.40s -> 2782.08s]  get like a ranking which gives you preference over summaries and hopefully
[2782.08s -> 2786.44s]  like I mean you can see like the idea that is important here is that even
[2786.52s -> 2790.12s]  when we have some kind of a consistent utility function even when I have it's
[2790.12s -> 2793.92s]  much easier to compare to something and know that which is better than this
[2793.92s -> 2798.28s]  rather than ascribing it an arbitrary number on a scale and that's where the
[2798.28s -> 2804.48s]  signal from something like this is a lot better now how do we get like we
[2804.48s -> 2807.48s]  talked about we need like we get this kind of a preference data and now we
[2807.48s -> 2812.96s]  need some kind of a reward score out of this and we shove in like our input
[2812.96s -> 2815.68s]  we shove in a summary as well and we still need to get a score out of this
[2815.88s -> 2818.96s]  but it's not clearly obvious to me like how do we take this data and convert it
[2818.96s -> 2826.72s]  into that kind of score in comes our pretty good friends named Bradley Terry
[2826.72s -> 2833.28s]  and essentially like there's a lot of study in like many in economics and
[2833.28s -> 2838.64s]  like psychology which basically tries to model how humans make decisions in
[2838.64s -> 2842.88s]  specific case like this Bradley Terry model essentially says that a
[2842.88s -> 2850.12s]  probability that a human chooses answer y1 over y2 is based on the difference
[2850.12s -> 2855.16s]  between the rewards that humans assign internally and then you take a sigmoid
[2855.16s -> 2860.60s]  around it so if you've looked at binary classification before the logic
[2860.60s -> 2865.40s]  is simply the difference between the reward of some y1 minus y2 or the
[2865.40s -> 2871.72s]  difference between the winning completion and the losing completion is
[2871.72s -> 2880.28s]  everybody with me till this point so the idea is that like if you have a data
[2880.28s -> 2885.52s]  set we're given y1 and y2 where y1 is a winning completion and we have a
[2885.52s -> 2890.28s]  winning completion yw and a losing completion yl the winning completion
[2890.28s -> 2899.84s]  should score higher than the losing completion sorry what like what is the
[2899.84s -> 2903.88s]  type of J like this number here that we're getting as the expectation is it
[2903.88s -> 2908.04s]  a log prop or what is it's an expected log prop so it will be a
[2908.04s -> 2913.48s]  scalar at the end sigmoid is so you're taking their let's see over one
[2913.48s -> 2920.08s]  model which gives a score r1 to like yw and r2 to yl you subtract that
[2920.08s -> 2922.96s]  number you get another number you put it into sigmoid and you get a
[2922.96s -> 2927.72s]  probability because sigmoid will convert a logic into probability and then you
[2927.72s -> 2931.96s]  take a logarithm of that and you take the expectation of everything and you
[2931.96s -> 2935.80s]  get this final number which tells you how good your reward model is doing on
[2935.80s -> 2945.56s]  the entire data set a good model of humans would score very low here so it
[2945.56s -> 2948.96s]  would generally assign a higher reward to the winning completion and
[2948.96s -> 2955.96s]  generally assign a lower reward to the losing completion cool the math is
[2955.96s -> 2964.24s]  just beginning so hold on to your seats cool so now let's see where we are
[2964.24s -> 2969.60s]  we have a pre-trained model P P T Y given X and we got this like fancy
[2969.60s -> 2973.72s]  reward model which tells us that hey how we have a model of humans and it
[2973.72s -> 2976.60s]  can tell us which instruction which answer they like and which is to
[2976.60s -> 2982.64s]  answer do not like now to do RLHF generally like I mean we've
[2982.64s -> 2986.56s]  discussed what this will look like we will copy our pre-trained model or
[2986.56s -> 2990.16s]  instruction tuned model and we'll optimize the parameters for those
[2990.16s -> 2994.88s]  models and I suggested that the objective that we want to optimize is
[2994.88s -> 3001.28s]  the expected reward when we sample completions from P theta and we're
[3001.28s -> 3004.12s]  going to optimize our learned reward model instead of like the true reward
[3004.12s -> 3008.56s]  model which humans would have typically assigned do you see any problem with
[3008.56s -> 3016.12s]  this is there something that's wrong here or like that might go wrong if we
[3016.12s -> 3031.68s]  do something along these lines go for it might collapse yes but generally at
[3031.68s -> 3034.80s]  least from my intuition like if you're ever doing something and you have
[3034.80s -> 3039.88s]  you're optimizing some learned metric I'd be very careful because typically
[3039.88s -> 3042.84s]  our loss functions are very clearly defined but here my reward model is
[3042.84s -> 3049.20s]  learned what when it's learned it means it will have errors yes so it's
[3049.20s -> 3051.80s]  going to be trained on some distribution it will generalize as
[3051.80s -> 3056.56s]  well but it will have errors and when you're optimizing against a learned
[3056.56s -> 3061.44s]  model it will tend to have the reward model so it might exploit the reward
[3061.44s -> 3066.16s]  model might erroneously assign a really high score to a really bad completion if
[3066.16s -> 3069.60s]  your policy learns or if your language model learns to do that it will
[3069.60s -> 3073.32s]  completely hack that and start generating those gibberish completions
[3073.32s -> 3079.28s]  so just as a general machine learning tip as well if you're optimizing a
[3079.28s -> 3082.00s]  learned metric be careful about what you're optimizing and make sure that
[3082.00s -> 3088.48s]  it's actually reliable and the way and this is obviously not desirable like
[3088.48s -> 3091.08s]  I mean if you start optimizing this objective you're going to convert your
[3091.08s -> 3094.88s]  gibberish language models very very quickly so typically what people do is
[3094.88s -> 3098.56s]  that you want to add some kind of a penalty that like avoids it drifting
[3098.56s -> 3103.56s]  too far from its initialization and why do we want to do that like if it
[3103.56s -> 3106.12s]  cannot drift from too far from its initialization we know the
[3106.12s -> 3109.96s]  initialization of the model is a decent language model and we know it
[3109.96s -> 3114.40s]  is not necessarily satisfying this reward model too much and we also know that
[3114.40s -> 3119.04s]  like the reward model is trained on a distribution of completions where the
[3119.08s -> 3124.28s]  initial model is so typically when we talk about training this reward model we
[3124.28s -> 3127.36s]  have trained on certain completions which are sampled from this initial
[3127.36s -> 3130.72s]  distribution so we know the reward model will be somewhat reliable in that
[3130.72s -> 3135.24s]  distribution so we're just going to simply add a penalty which tells us that
[3135.24s -> 3140.40s]  you should not drift too far away from the initial distribution and just to go
[3140.40s -> 3145.68s]  over this we want to maximize the objective where we have RM Phi which is
[3145.68s -> 3151.32s]  our learned reward model but we're going to add this term beta log ratio and the
[3151.32s -> 3155.04s]  ratio is our the model we're optimizing P theta and our initial
[3155.04s -> 3159.48s]  model PT and what this says is that if we assign a much higher
[3159.48s -> 3163.56s]  probability to certain completion as compared to our pre-trained model
[3163.56s -> 3168.68s]  you're going to add an increasingly large penalty to it and simply you're
[3168.68s -> 3172.52s]  paying a price for drifting too far from initial distribution if you guys
[3172.56s -> 3176.60s]  have taken like machine learning this the expectation of this quantity can is
[3176.60s -> 3181.76s]  exactly the callback library divergence or KL divergence between P theta
[3181.76s -> 3186.44s]  and P PT so you're penalizing drifting between two distributions go
[3186.44s -> 3197.56s]  for it. Shouldn't you also do this like add a penalty in the previous version where you had to fine-tune or is this only relevant for the RLHF?
[3198.04s -> 3209.08s]  That's a good question so I think people do add some kinds of regularization in fine-tuning it's not nearly not as critical when you're doing this with RL like the
[3209.08s -> 3213.32s]  incentive is to exploit this reward model as well as much as possible and
[3213.32s -> 3217.64s]  we'll see examples where like the learned reward predicts like it's doing
[3217.64s -> 3222.08s]  really well but the true reward models are completely garbage so it's much
[3222.08s -> 3236.68s]  more important in this optimization. Cool so now this course does not assume background on reinforcement learning so we're not going to go into reinforcement
[3236.68s -> 3239.12s]  learning but I just want to give a very high level intuition about how
[3239.12s -> 3243.84s]  this works and reinforcement learning is not typically just used for language
[3243.84s -> 3249.24s]  model it's been applied to several domains of interest game playing agents
[3249.24s -> 3256.68s]  robotics developing chip designs and so on and their interest between like RL
[3256.68s -> 3261.28s]  and model elements it's also like dates back to roughly 2016 as well but
[3261.28s -> 3264.92s]  like it's been really successful recently and especially with the
[3264.92s -> 3269.56s]  success of RLHF. The general idea is that we're going to use our model
[3269.56s -> 3273.36s]  that we're optimizing to generate several completions for an instruction
[3273.36s -> 3279.16s]  we're going to compute the reward under our learned reward model and then
[3279.16s -> 3283.00s]  we're going to simply try and like update our model to increase
[3283.00s -> 3286.48s]  the probability on the high reward completions so when we sample a model
[3286.48s -> 3290.00s]  we'll see completions of varying quality and we'll see some good
[3290.00s -> 3292.92s]  completions good summaries for our tasks some bad summaries for our task
[3292.92s -> 3297.88s]  and we'll try to update our log probabilities in a way such that the
[3297.88s -> 3301.64s]  reward for when you use the updated model you're typically in the high
[3301.64s -> 3312.00s]  reward region does a high level summary like make sense cool and RLHF is
[3312.00s -> 3316.16s]  incredibly successful I think this is a very good example of this is the same
[3316.16s -> 3323.76s]  summarization example and I think the key point here is that the performance
[3323.76s -> 3327.56s]  improves by increasing the model size for sure we have seen this in many
[3327.56s -> 3331.56s]  different example what you can actually see is that even very small models can
[3331.56s -> 3336.48s]  outperform human completions if you train it with RLHF and this is
[3336.48s -> 3339.24s]  exactly the result you see here the reference summaries are human
[3339.24s -> 3343.28s]  generated and when you evaluate I mean you ask humans which ones they
[3343.28s -> 3346.64s]  prefer they often prefer the model generated summary over the human
[3346.64s -> 3349.44s]  generated summary and there's something you only observe with RLHF
[3349.44s -> 3353.32s]  even at small scales and again the same scaling phenomena still holds here
[3353.32s -> 3357.60s]  bigger models do become more responsive but RLHF itself is very impactful
[3357.60s -> 3367.68s]  here cool the problem with RLHF is that it's just incredibly complex like I
[3367.68s -> 3371.96s]  gave you a very high level summary that's like doesn't that there's whole
[3371.96s -> 3377.40s]  courses on this for a reason so it just and this image is not for you to
[3377.40s -> 3383.64s]  understand it's just completely to intimidate you so you want to fit a
[3383.64s -> 3387.56s]  value function to something there's you have to sample the model a lot it can
[3387.56s -> 3391.00s]  be sensitive to a lot of hyper parameters so there's a lot that goes
[3391.00s -> 3397.32s]  on here and yeah if you started implementing an RLHF pipeline it can be
[3397.32s -> 3401.24s]  very hard and this is the reason why like a lot of RLHF was restricted to
[3401.24s -> 3405.84s]  very very like high compute high resource places and it was not very
[3405.84s -> 3409.48s]  accessible so what we're going to talk about and cover in this course is
[3409.48s -> 3412.20s]  something called direct preference optimization which is a much simpler
[3412.20s -> 3416.24s]  alternative to RLHF and hopefully like that's much more accessible but
[3416.24s -> 3420.64s]  please bear with me there will be a lot of math on here but the end goal of the
[3420.64s -> 3424.36s]  math is to make a come up with a very simple algorithm so hopefully like it's
[3424.36s -> 3434.12s]  and feel free to stop me and ask me questions in terms like GPT 4 versus
[3434.12s -> 3438.60s]  3 like how much is the number of parameters in the base model help with
[3438.60s -> 3442.40s]  like we need to reduce the number of parameters or like in order to write
[3442.40s -> 3446.80s]  using the number of like examples from humans or RLHFs that work well
[3446.80s -> 3451.08s]  yeah that's a really good question so generally speaking as the if you hold
[3451.08s -> 3455.08s]  the data set size constant and simply increase the model size it will
[3455.08s -> 3459.56s]  improve quite a lot but the nice thing is that you can reuse the data
[3459.56s -> 3463.76s]  and you can keep adding data yeah as you keep like scaling models up so
[3463.76s -> 3466.96s]  typically like nobody tries to like reduce the amount of data collection
[3466.96s -> 3474.88s]  yeah right you just keep increasing both the things out so we talked about RLHF
[3474.88s -> 3479.52s]  and the current pipeline is something like we train a reward model on the
[3479.52s -> 3482.96s]  comparison data that we have seen so far and we're going to optimize we're
[3482.96s -> 3485.44s]  going to start with our pre-trained or instruction tuned model and convert
[3485.44s -> 3490.56s]  it into an RLHF model using the reinforcement learning techniques now
[3490.56s -> 3494.20s]  they're really the key idea in direct preference optimization is what if we
[3494.24s -> 3498.44s]  could just simply write a reward model in terms of our language model itself
[3498.44s -> 3503.20s]  now to intuitively understand that like what is going on a language model is
[3503.20s -> 3507.88s]  assigning probabilities to whatever is the most plausible completion next but
[3507.88s -> 3511.20s]  those plausible completions might not be what we intended but you could
[3511.20s -> 3515.16s]  restrict the probability simply to the completions that a human might like
[3515.16s -> 3519.32s]  and then the log probabilities of your model might represent something which
[3519.32s -> 3522.24s]  the humans might like and not just some arbitrary completion on the
[3522.28s -> 3526.16s]  internet so there is a direct correspondence between the log
[3526.16s -> 3530.36s]  probability that a language model assigns and how much a human might like
[3530.36s -> 3534.96s]  the answer they can have like a direct correspondence in them and this
[3534.96s -> 3538.16s]  is not some arbitrary intuition that I'm trying to like come up with we will
[3538.16s -> 3543.60s]  derive this mathematically so the general idea of a direct preference
[3543.60s -> 3547.00s]  optimization is going to be we're going to write down reward model in
[3547.00s -> 3551.12s]  terms of our language model and now that we can write our reward model in
[3551.12s -> 3555.36s]  terms of our language model we can simply solve directly fit our reward
[3555.36s -> 3560.80s]  model to the preference data we have and we don't need to do the RL step at
[3560.80s -> 3564.08s]  all so we started off with some preference data and we simply fit our
[3564.08s -> 3568.96s]  reward model to it which directly optimizes the language parameters and
[3568.96s -> 3573.12s]  maybe at a high level why is this like even possible like we did this
[3573.12s -> 3576.28s]  like really cumbersome process with fitting a reward model and optimizing
[3576.28s -> 3581.00s]  it but in the whole process the only external information that was being
[3581.00s -> 3585.00s]  added to the system like was human labels labels on the preference data
[3585.00s -> 3589.00s]  when we optimize a learned reward model there's no new information being
[3589.00s -> 3592.96s]  added into the system so this is why something like this is even possible
[3592.96s -> 3598.16s]  for quite a few years this was not obvious but like as you will see like
[3598.16s -> 3601.64s]  some of these results like start to make sense so we're going to derive
[3601.64s -> 3606.28s]  direct preference optimization I'll be after I'll be here after the class as
[3606.28s -> 3613.80s]  well if you have questions but I hopefully like this is clear so yes we
[3613.80s -> 3617.32s]  discussed that we wanted to solve this expected reward problem where we want
[3617.32s -> 3621.36s]  to maximize the expected reward but we subtract this term which is the beta
[3621.36s -> 3625.60s]  log ratio which essentially penalizes the distance between where our current
[3625.60s -> 3628.92s]  model is and where we started off so we don't want to drift too far away
[3629.16s -> 3637.16s]  from our from where we started now it turns out that this specific problem
[3637.16s -> 3642.16s]  instead of doing like an iterative routine there's actually a closed form
[3642.16s -> 3648.96s]  solution to this problem so the closed form solution looks something like this
[3648.96s -> 3652.72s]  again if you have seen the Boltzmann distribution or something to that
[3652.72s -> 3656.72s]  effect before this is very basically the same idea but the idea is this if
[3656.72s -> 3660.84s]  we're going to take a pre-trained distribution PPT y given X and we're
[3660.84s -> 3666.92s]  going to rewrite the distribution by the expected reward so if we're if if a
[3666.92s -> 3669.28s]  completion has a very high reward it's going to have a higher
[3669.28s -> 3672.16s]  probability mass and if it has a lower reward it's going to have a
[3672.16s -> 3675.24s]  lower probability mass and it's determined by the expected reward and
[3675.24s -> 3679.52s]  beta is a hyper parameter which essentially governs like what is the
[3679.52s -> 3685.00s]  trade-off between the reward model and the constraint and as beta becomes
[3685.00s -> 3688.12s]  lower and lower you're going to start paying more and more attention to the
[3688.12s -> 3694.00s]  reward model so the probabilities look something like this and there is this
[3694.00s -> 3698.84s]  like really annoying term the ZX and the reason why it exists is that the
[3698.84s -> 3704.08s]  numerator by itself is not normalized it's not a probability distribution so to
[3704.08s -> 3708.00s]  construct like an actual probability distribution you have to normalize it
[3708.00s -> 3717.20s]  and ZX is simply just this normalization and that's exactly like
[3717.20s -> 3720.96s]  it's sum over all y's for a given instruction and that's exactly why this
[3720.96s -> 3724.08s]  is very pesky is like it's intractable if I take an instruction
[3724.08s -> 3727.12s]  and try to sum over every possible completion and not just like
[3727.12s -> 3731.76s]  syntactically correct ones every single possible we have 50,000 tokens
[3731.76s -> 3735.88s]  maybe even more and the completions can go arbitrary long so this space is
[3735.88s -> 3744.08s]  completely intractable this quantity is not easy to approximate even so the main
[3744.08s -> 3747.84s]  point here is that you if you're given a reward model you can actually there
[3747.84s -> 3750.60s]  does exist at least a closed form solution which tells us what the
[3750.60s -> 3753.80s]  optimal policy will look like or optimal language model will look like
[3753.80s -> 3757.88s]  but if you do a little bit of algebra just move some terms around take a
[3757.88s -> 3761.04s]  logarithm here or there I promise this is not very complicated you can
[3761.04s -> 3765.52s]  actually express the reward model in terms of the language model itself and
[3765.52s -> 3771.64s]  I think this term is reasonably intuitive as well what it says is that a
[3771.64s -> 3777.84s]  completion y hat has a high reward if the model my optimal policy assigns a
[3777.84s -> 3783.04s]  higher probability to it relative to my initialized model and this is scaled
[3783.04s -> 3788.76s]  by beta so the beta log ratio is what we're looking at here and the
[3788.76s -> 3792.04s]  partition function let's just ignore it for now but it's intractable but the
[3792.04s -> 3800.20s]  beta log ratio is the key part here is everyone following along awesome
[3800.20s -> 3807.12s]  okay so right now I'm talking about optimal policies but really like every
[3807.12s -> 3811.12s]  policy is probably optimal for some kind of a reward right like this is
[3811.12s -> 3814.64s]  mathematically true as well so the important bit here is that you can
[3814.64s -> 3819.20s]  actually express you take a current policy take your initialized model and
[3819.20s -> 3823.52s]  you can get some kind of a reward model out of it and this is the exact
[3823.52s -> 3827.88s]  identity which leads to this so reward model can be expressed in terms
[3827.88s -> 3833.24s]  of your language model barring the log partition term which we'll see what
[3833.24s -> 3835.80s]  happens to it
[3836.12s -> 3840.20s]  like why is it that we can swap because there is a thing that we're trying to
[3840.20s -> 3846.20s]  optimize and how to start yeah for now like we're not optimizing any reward
[3846.20s -> 3850.88s]  model all I'm saying is that if I take my current language model it is
[3850.88s -> 3856.80s]  it probably represents some kind of a reward model implicitly because of
[3856.80s -> 3860.44s]  this relationship because this holds for every piece star in every reward
[3860.44s -> 3864.84s]  model what I'm saying is that like there if I plug in my current language
[3864.84s -> 3870.44s]  model it also represents some kind of a reward model I'm not saying it's optimal
[3875.24s -> 3885.36s]  yes initially it's zero but like we can optimize the parameters okay yeah
[3885.36s -> 3892.88s]  but that's a good observation that is basically zero in the beginning any
[3892.88s -> 3911.16s]  other questions that's that's the next step yes but the key idea is that like
[3911.16s -> 3915.16s]  my log my language model the probabilities already implicitly define
[3915.16s -> 3918.88s]  a reward model I think that's really the main point here and this
[3918.88s -> 3925.72s]  mathematical relationship is exact cool now like I mean I'm obviously ignoring
[3925.72s -> 3930.52s]  like the elephant in the room here which is the partition function it's not
[3930.52s -> 3933.76s]  going to magically vanish away so like if this was just a beta log ratio
[3933.76s -> 3937.20s]  that would be really nice I can compute all these quantities I know how
[3937.20s -> 3940.56s]  to compute the log probability under my language model and how to compute the
[3940.56s -> 3944.32s]  log probability under my pre-trained model and I can compute the reward
[3944.32s -> 3948.56s]  score and I can optimize this but I don't know what to do about my log
[3948.56s -> 3955.72s]  partition function this is where something fun happens so recall what the
[3955.72s -> 3960.04s]  reward modeling objective was when we started off like we started off with
[3960.04s -> 3964.80s]  their friends Bradley Terry again and what we really wanted to optimize was
[3964.80s -> 3967.32s]  the reward difference between the winning completion and the losing
[3967.32s -> 3972.96s]  completion and really like I mean that's all we care about we don't care
[3972.96s -> 3976.56s]  about the exact reward itself what we care about is maximizing the
[3976.56s -> 3981.48s]  difference between the difference between winning and losing completion and
[3981.48s -> 3985.56s]  that's actually really key here because if you plug in the definition
[3985.56s -> 3991.32s]  of the RM theta there what you'll observe is that the partition
[3991.32s -> 3998.08s]  function actually just cancels out now why does it cancel out the input
[3998.08s -> 4002.16s]  is exactly the same the X is actually exactly the same in the difference so
[4002.16s -> 4005.72s]  the partition function ZX will just cancel out like it's the same in both
[4005.80s -> 4008.64s]  terms so what you get is that the reward difference between the winning and
[4008.64s -> 4012.08s]  losing completion is the differences between the beta log ratio for the
[4012.08s -> 4016.88s]  winning and losing completion you can plug in the terms you can work it out
[4016.88s -> 4022.76s]  it's fairly simple so the partition function which was our like which was
[4022.76s -> 4025.24s]  something we could not address we could not compute actually just simply
[4025.24s -> 4035.40s]  vanished away but it appears here in this equation
[4036.72s -> 4042.48s]  so we're going to take this equation the last line that you see and we're going
[4042.48s -> 4052.72s]  to plug in in place of RM Phi so and in this the first loss equation yes so
[4052.72s -> 4059.36s]  the first loss equation is the Bradley Terry loss model cool so this really is
[4059.36s -> 4063.04s]  it like I mean the key observation is we could express our reward model in
[4063.24s -> 4066.16s]  terms of language model and our problems with the partition function actually go
[4066.16s -> 4071.16s]  away because we were optimizing the Bradley Terry model and what do you
[4071.16s -> 4076.04s]  get is something like this is that we're going to express the loss
[4076.04s -> 4080.32s]  function directly in terms of our language bar model parameters theta and
[4080.32s -> 4086.20s]  we're going to be able to directly optimize on our data without doing any
[4086.20s -> 4090.68s]  RL steps or not and this is simply a binary classification problem so we're
[4090.68s -> 4094.24s]  really just trying to classify whether an answer is good or bad and that's
[4094.24s -> 4100.76s]  really what we're doing before I go on like people want to like absorb this
[4100.76s -> 4104.84s]  in like a mean feel they're okay with it
[4111.32s -> 4117.28s]  good question it's the same data set we started with in our LHF as well but
[4117.28s -> 4121.08s]  the way the process works is that you take a set of instruction and get the
[4121.08s -> 4124.72s]  model to generate some answers and then you get humans to label which answer
[4124.72s -> 4128.80s]  they prefer so they're model generated typically they can be human
[4128.80s -> 4131.64s]  generated as well but they're typically model generated and then you
[4131.64s -> 4136.56s]  get some preference labels all you need is a label saying which is a
[4136.56s -> 4145.56s]  better answer you must be losing some information because of lack of
[4145.56s -> 4152.24s]  information about like other you're canceling out your because of the lack
[4152.24s -> 4157.56s]  of any information about the partition function yeah you are bound to lose
[4157.56s -> 4162.20s]  information about like other possible completions which you would have taken
[4162.60s -> 4169.32s]  that's a really good question I don't think I'll be able to completely answer
[4169.32s -> 4173.12s]  this question in time but like partition function is almost kind of a
[4173.12s -> 4177.60s]  free variable so I think the problem here is that the reward model there
[4177.60s -> 4183.04s]  think of when you there's many reward models that satisfy this optimization so
[4183.04s -> 4186.72s]  there's a free variable here that you can actually completely remove and
[4186.72s -> 4190.04s]  that's what this optimization benefits from so think of it this way like if I
[4190.04s -> 4193.40s]  assign something a reward of plus one and assign something a reward of minus
[4193.40s -> 4196.48s]  one that's basically the same as saying as it's a reward of plus hundred
[4196.48s -> 4204.12s]  and ninety nine and it will give you the same loss right so that scale
[4204.12s -> 4208.40s]  doesn't shift invariant in a ways
[4210.00s -> 4215.80s]  not what you want though like okay like if you have if you're actually
[4215.80s -> 4220.72s]  getting a reward model right like hundred ninety nine is like much you
[4220.72s -> 4227.36s]  should pay much less attention to that as compared to like one zero what we're
[4227.36s -> 4231.08s]  assuming is our choice model here is like if a human prefers something over
[4231.08s -> 4235.32s]  the other like the probability is governed only by the difference between
[4235.32s -> 4240.12s]  the rewards so that's an assumption that every RLHF also makes and like
[4240.12s -> 4246.56s]  DPO also makes now is that assumption true not completely true but like it
[4246.56s -> 4254.68s]  holds to a fairly large degree but that's a good question yeah cool I'll
[4254.68s -> 4259.56s]  move on in rest of time and really like I mean the goal of this plot is to
[4259.56s -> 4264.68s]  like we actually get fairly performant models when we optimize things with DPO
[4264.68s -> 4268.44s]  V so in this plot I think the main thing that you should look at is PPO
[4268.44s -> 4271.64s]  which is the typical RLHF pipeline and we are evaluating the models for
[4271.64s -> 4276.80s]  summarization and we're comparing two human summaries and what we find is a
[4276.80s -> 4280.16s]  deeper and deeper sort of do similarly but you're really not losing much by
[4280.16s -> 4283.92s]  just doing the DPO procedure instead of RLHF and that's really compelling
[4283.92s -> 4287.04s]  because DPO is simply a classification loss instead of like a whole
[4287.04s -> 4293.72s]  reinforcement learning procedure so I'm going to quickly summarize what we have
[4293.72s -> 4299.00s]  seen thus far is that we want to optimize for human preferences so and
[4299.00s -> 4302.44s]  the way we do this is like instead of relying on uncalibrated scores we're
[4302.44s -> 4306.84s]  getting comparison data and feedback on that and we use this ranking data to
[4306.84s -> 4310.80s]  either do something like RLHF where we first fit a reward model and
[4310.80s -> 4315.92s]  optimize using reinforcement learning or we do something like direct preference
[4315.92s -> 4319.76s]  optimization we simply take the dataset and do a classification loss on
[4319.76s -> 4325.80s]  that and yeah like there's trade-offs in these algorithms like people when they
[4325.80s -> 4329.00s]  have a lot of computational budget they typically maybe go for our lecture
[4329.00s -> 4332.56s]  for some routine like that but if you're really looking to get the bang
[4332.56s -> 4335.96s]  for your buck like I mean you might want to go for DPO and if and that's
[4335.96s -> 4340.56s]  like probably going to work out of the box it's a still an active area of
[4340.56s -> 4343.08s]  research people are still trying to understand how to like best work
[4343.08s -> 4346.48s]  with these algorithms so like I'm not making any strong claims here but like
[4346.48s -> 4349.56s]  both of these algorithms are very effective DPO is just much simpler to
[4349.56s -> 4356.40s]  work with cool so yeah like I mean let's see like we went through all this
[4356.40s -> 4362.92s]  instruction tuning RLHF what do we get instruct GPT is the first model
[4362.92s -> 4366.76s]  which sort of followed this pipeline it defined this pipeline so we got
[4366.76s -> 4371.48s]  models which did 30,000 or so tasks remember when we were doing like only
[4371.48s -> 4375.00s]  one task and now we have scaled it up from thousand tasks to like 30,000
[4375.00s -> 4378.60s]  different tasks with many many different examples so that's like where we are
[4378.60s -> 4383.04s]  with instruct GPT and it follows this pipeline that we just described in this
[4383.04s -> 4386.32s]  case they're following a specific RLHF pipeline where we explicitly fit a
[4386.32s -> 4390.76s]  reward model and then do some kind of a reinforcement learning routine on top of
[4390.76s -> 4396.64s]  it and yeah like the task collected from labelers looks something like this
[4396.64s -> 4402.44s]  I leave it to imagination or you can look at the details but how we started
[4402.44s -> 4407.80s]  off with this model was something like completions we see from GPT 3 which you
[4407.80s -> 4411.32s]  know explained the moon landing to six year and like it is not really
[4411.32s -> 4414.12s]  following the instructions but instruct GPT will give you something which is
[4414.12s -> 4418.12s]  meaningful so it's inferring what a user wanted from the specific
[4418.12s -> 4423.44s]  instruction and it's converting to a realistic answer that a user might like
[4423.44s -> 4428.60s]  and yeah these are just more examples of what an instruct GPT like model
[4428.60s -> 4432.72s]  would do whereas your base model might not follow the instructions to your
[4432.72s -> 4440.40s]  desired intentions and yeah like we went from instruct GPT to charge GPT and it
[4440.40s -> 4445.40s]  was essentially this pipeline the key difference here is that it is still
[4445.40s -> 4449.56s]  doing the instruction tuning but it is more optimized for dialogue more
[4449.56s -> 4454.12s]  optimized for interacting with users so the core algorithmic techniques that
[4454.12s -> 4458.08s]  we discussed today are what give us charge GPT but you have to be really
[4458.08s -> 4461.60s]  careful about the kind of data you're training on and that's really the whole
[4461.60s -> 4468.76s]  game but this is the foundation for charge GPT and yeah it follows the same
[4468.76s -> 4473.56s]  pipeline as well and you might look at you might interact with charge GPT I'm
[4473.56s -> 4476.56s]  sure you all have interacted with it some form or not but like this is an
[4476.56s -> 4482.84s]  example of what a charge GPT interaction might look like you want
[4482.84s -> 4486.20s]  to make a Gen Z so like I mean you can you know the idea here is that
[4486.20s -> 4489.56s]  it's like very good at responding to instructions and intent this is not
[4489.56s -> 4494.52s]  something that we could like even few shot in very easily these are kind of
[4494.52s -> 4497.76s]  instructions are hard to come examples for but like this is probably
[4497.76s -> 4500.92s]  not something you trained on either but it's able to like infer the intent
[4500.92s -> 4505.20s]  and generalize very very nicely and that's something I find personally very
[4505.20s -> 4512.24s]  remarkable cool and there's been a lot of progress on the open source front
[4512.24s -> 4515.00s]  as well so like DPO is much simpler and much more efficient and
[4515.04s -> 4520.36s]  essentially all the open source models these days are using DPO so this is a
[4520.36s -> 4524.16s]  leaderboard that is maintained by hugging hugging face here so like I mean
[4524.16s -> 4528.12s]  nine out of ten models here are trained with DPO so that's been
[4528.12s -> 4531.24s]  something that's been enabled the open source community to instruction
[4531.24s -> 4535.64s]  tune their model betters as well and same is being used in many production
[4535.64s -> 4540.20s]  models now as well Mistral is using DPO, llama3 use DPO so these are very
[4540.20s -> 4544.20s]  very strong models which are nearly GPT4 level and they're also like
[4544.20s -> 4549.84s]  starting to use these algorithms as well and something that's very cool to
[4549.84s -> 4553.60s]  see is like like we went through all this like optimization and like I mean
[4553.60s -> 4556.72s]  math and stuff but what is really fundamentally changing in the behavior
[4556.72s -> 4561.20s]  and I think this is a really good example is that if you simply ask an
[4561.20s -> 4565.72s]  instruction for us and ask for an SFT output from an instruction tuned model
[4565.72s -> 4569.36s]  you'll get something like this but when you are let's have the model you
[4569.36s -> 4572.52s]  actually get a lot more details in your answer and they'll probably organize
[4572.52s -> 4576.32s]  the answers a little better and there's something that they are maybe humans
[4576.32s -> 4580.76s]  prefer that's why it's and a property that is emerging in these model but
[4580.76s -> 4584.72s]  it's something that's a very clear difference between simply instruction
[4584.72s -> 4593.52s]  tuned models and some models which are RLHFed so yeah we discuss like
[4593.52s -> 4598.48s]  this whole RLHF routine where we are directly modeling the preferences and
[4598.48s -> 4602.48s]  we are generalizing beyond label data and we also discuss RL can be very
[4602.48s -> 4607.04s]  tricky to correctly implement or DPO sort of implements this or like
[4607.04s -> 4611.28s]  avoid some of these issue and we briefly also touched upon the idea of
[4611.28s -> 4616.52s]  reward model and reward hacking and when you're optimizing for learned
[4616.52s -> 4622.28s]  reward models you will often see this example is that there's a way for it to
[4622.28s -> 4627.96s]  just simply crash into the object some keep repeating repetitively
[4627.96s -> 4631.28s]  crashing the board to get more and more points that wasn't the goal of
[4631.44s -> 4637.28s]  this game so this is a very common example that is shown for reward hacking
[4637.28s -> 4641.92s]  if you do not specify rewards well the models can like learn weird behaviors
[4641.92s -> 4645.64s]  which are not your desired intent and there's something a lot of people worry
[4645.64s -> 4649.56s]  about as well part of the reason is reinforcement learning is a very strong
[4649.56s -> 4654.12s]  optimization algorithm it's at the heart of AlphaGo AlphaZero which
[4654.12s -> 4657.72s]  like results in superhuman models so you have to be careful about how you
[4657.80s -> 4661.56s]  specify things and the other thing is like even optimizing for human
[4661.56s -> 4665.04s]  preferences is often not the right thing because humans are not do not
[4665.04s -> 4668.64s]  always like things which are in their best interest so something that emerges
[4668.64s -> 4672.96s]  is that they like authoritative and helpful answers but they often like
[4672.96s -> 4677.72s]  don't necessarily like truthful answers so one property that happens is
[4677.72s -> 4680.92s]  like is that they prefer authoritativeness more than
[4680.92s -> 4686.40s]  correctness which is maybe like not something nice please go ahead
[4686.40s -> 4690.80s]  maybe like changing TV so like now widely used by the public will maybe
[4690.80s -> 4694.56s]  change the like how people really really like make their words whose I at
[4694.56s -> 4697.68s]  least feel like now when I go to touch it I think it gives me five like
[4697.68s -> 4702.64s]  detailed paragraphs of information sometimes yeah but maybe in the
[4702.64s -> 4705.80s]  original reward function in the original ratings people actually burn
[4705.80s -> 4710.76s]  that and I burn less yeah that's a great point because like as these
[4710.76s -> 4713.76s]  models like integrate more and more into our system they're going to
[4713.76s -> 4718.16s]  collect more and more data and they will like pick up on things maybe
[4718.16s -> 4723.16s]  undesirable things as well as far as I understand chat GPD is really cutting
[4723.16s -> 4726.32s]  down on the verbosity which is like a huge issue that all of these models
[4726.32s -> 4730.28s]  are trying to cut down on and they are dealing with that part of the
[4730.28s -> 4732.96s]  reason why that emerges is that when you collect preference data at scale
[4732.96s -> 4737.28s]  people are not necessarily reading the answers the Turkers might just simply
[4737.28s -> 4740.92s]  choose the longer answer and that's a property that actually goes into these
[4740.92s -> 4744.32s]  models so but hopefully like these things will improve over time as they
[4744.32s -> 4748.08s]  get more feedback and yeah hallucinations is not a problem that is
[4748.08s -> 4751.00s]  going to go away with RL and we talked a bit about reward hacking as
[4751.00s -> 4757.76s]  well biases from things and so on but hopefully like I mean what I want to
[4757.76s -> 4762.52s]  conclude out is like we started with pre-trained models we we had these
[4762.52s -> 4765.92s]  things which could predict text and we got chat GPD and hopefully like it's a
[4765.92s -> 4770.64s]  little more clear how we go from something like that to charge GPD and
[4770.64s -> 4774.64s]  that's I'll end here
