# Detected language: en (p=1.00)

[0.00s -> 6.12s]  Great.
[6.12s -> 9.00s]  So I think let's get started because we have a lot to cover today.
[9.00s -> 11.04s]  So my name is Yan.
[11.04s -> 15.94s]  For those who don't know me, I'm a third year PhD student advised by Tatsu and Percy.
[15.94s -> 19.50s]  And today I'll be talking about benchmarking and evaluations.
[19.50s -> 24.00s]  So benchmarking and evaluations are honestly something that I think not enough people
[24.00s -> 26.56s]  look at in academia.
[26.56s -> 30.44s]  But if you really want to put something in production and you really care about, let's
[30.44s -> 34.50s]  say real world machine learning, evaluation is really key.
[34.50s -> 37.52s]  So let's talk about that.
[37.52s -> 44.28s]  So overview of what we'll talk about first is different reasons for measuring performance.
[44.28s -> 48.28s]  Then I'll talk about text classification and how you measure performance there.
[48.28s -> 51.38s]  Then text generations and how you measure performance there.
[51.38s -> 58.16s]  And then finally, how do you evaluate current large language models and some issues and
[58.16s -> 63.86s]  challenges with the ways that we actually perform evaluations.
[63.86s -> 69.60s]  So my mental model of how you actually develop a machine learning model is that first you
[69.60s -> 71.90s]  will be training your model.
[71.90s -> 77.22s]  So here measuring performance is really key because you need to have a loss that you
[77.22s -> 80.62s]  need to know basically how to optimize.
[80.62s -> 85.24s]  Then once you are optimizing your loss, the second step is basically development.
[85.24s -> 91.78s]  So usually this is hyper-parameter tuning or, for example, if you have early stopping
[91.78s -> 92.78s]  during your models.
[92.78s -> 96.30s]  Like if you see that your model is not performing that well, you might or that there's
[96.30s -> 100.42s]  like some overfitting happening, you might decide to stop or you might decide to like
[100.42s -> 103.14s]  change the learning rate during the training of your model.
[103.14s -> 105.88s]  So development is kind of the second step.
[105.88s -> 110.10s]  And here you need to measure performance because you need to know how to do actually models
[110.58s -> 115.78s]  sorry, hyper-parameter tuning and like changing hyper-parameters.
[115.78s -> 118.46s]  Then the third step is essentially model selection.
[118.46s -> 123.40s]  So if I have a task that I really care about, which model performs best for my task?
[123.40s -> 125.42s]  That might be a model that I have trained.
[125.42s -> 128.76s]  It might be a model that another group has trained.
[128.76s -> 132.26s]  And finally, at least in the real world, you would decide to deploy your model.
[132.26s -> 136.46s]  And here measuring performance is really key because you need to know whether your model
[136.46s -> 139.30s]  is good enough to put in production.
[139.30s -> 143.66s]  In the parallel universe that we live in, there's also the publishing.
[143.66s -> 150.66s]  So you basically need to test like evaluate your model on standard benchmarks.
[150.66s -> 154.94s]  And the reason why we do that is essentially for communicating to different groups the quality
[154.94s -> 156.58s]  of our model.
[156.58s -> 160.46s]  So at every step of this pipeline, you really need to measure performance.
[160.46s -> 162.06s]  And that's what we'll talk about today.
[162.06s -> 165.64s]  But what is key to understand is that at different steps, you need to measure performance
[165.64s -> 166.64s]  in different ways.
[166.64s -> 172.92s]  So there's really not a single ideal way of measuring performance.
[172.92s -> 177.36s]  So for example, on the left, when you train your model for evaluating performance, you
[177.36s -> 182.84s]  really need to have a way of measuring performance that is super fast, super cheap,
[182.84s -> 183.84s]  and differentiable.
[183.84s -> 186.92s]  Because usually, I mean, with neural networks, you basically back propagate to the loss.
[186.92s -> 188.70s]  So it needs to be differentiable.
[188.70s -> 194.32s]  And finally, you really cannot have a way for your model to optimize some shortcuts
[194.32s -> 198.34s]  to optimize the loss, even though it's not really what you want it to optimize.
[198.34s -> 203.46s]  And as you move more to the right, basically, you're allowed or like you will measure
[203.46s -> 205.88s]  performance less often.
[205.88s -> 213.28s]  So it's fine if it's more expensive, but you really like the risk that you really
[213.28s -> 217.84s]  need your evaluation metrics to be higher quality.
[217.84s -> 222.52s]  Because the issues if you put a model in production are higher.
[222.52s -> 226.92s]  So during the development stage, you need a way of measuring performance that is fast,
[226.92s -> 229.12s]  cheap, and also kind of avoiding shortcuts.
[229.12s -> 233.60s]  Because when you do hyper-antituning, you're essentially also optimizing over a certain
[233.60s -> 235.50s]  objective.
[235.50s -> 239.00s]  Model selection, it can be a little bit less fast and less cheap, but still you will
[239.00s -> 240.76s]  have to do it that many times.
[240.76s -> 244.90s]  And most importantly, when you deploy a model, you really want the way to evaluate performance
[244.90s -> 246.98s]  to be trustworthy.
[246.98s -> 250.64s]  Because once you put something in production, there's kind of no way to go back for
[250.64s -> 253.50s]  what happened during that time when it was in production.
[253.50s -> 255.92s]  You also want things to be very task specific.
[255.92s -> 260.86s]  So if I care about a certain task, when I put my model in production, you really need
[260.86s -> 262.48s]  to evaluate on that specific task.
[262.48s -> 264.16s]  I don't care about other tasks.
[264.16s -> 266.68s]  And finally, you need your metrics to be absolute.
[266.68s -> 271.04s]  So the reason why I'm highlighting that is that under three other steps, you really
[271.04s -> 273.36s]  just care about comparing between things.
[273.36s -> 278.74s]  Today is very different than if you want to kind of have a threshold which says if
[278.74s -> 282.66s]  I have less than 95% accuracy, I'm not putting my model in production.
[282.66s -> 284.90s]  Okay, and now let's talk about publishing.
[284.90s -> 288.62s]  This is a little bit different than honestly evaluation in the real world.
[288.62s -> 294.30s]  But when you basically do academic benchmarking and when you evaluate your models and academic
[294.30s -> 298.02s]  benchmarks, you want the benchmark to be reproducible and standardized.
[298.02s -> 302.86s]  And the reason why is basically because for the next five or six or ten years, everyone
[302.86s -> 307.32s]  will be evaluated on that one benchmark and you want papers in three years to be comparable
[307.32s -> 308.32s]  to yours.
[308.40s -> 311.04s]  So it's really important that your evaluations are reproducible.
[311.04s -> 315.36s]  Honestly, you don't really care about that in the real world.
[315.36s -> 321.08s]  You also want things to be easy to work with because researchers are usually a little
[321.08s -> 325.00s]  bit, they don't want to do additional work that they need to.
[325.00s -> 330.28s]  And also, they usually don't have that much resource, so it needs to be fast and cheap.
[330.28s -> 335.20s]  And finally, one thing which I really want to highlight is that for the academic benchmarks
[335.24s -> 340.16s]  that we usually have, it's fine if the metrics that we use are not perfect.
[340.16s -> 344.96s]  Because really what matters is that over ten years, the direction that your metrics is
[344.96s -> 352.20s]  showing you to go into, like basically how the field is moving, really, if the metric
[352.20s -> 355.72s]  is saying that it's slightly better, sorry, that it's better over ten years, that in
[355.72s -> 359.88s]  reality the field has made some progress.
[359.88s -> 366.72s]  So at a meta level, it's fine if you use crude metrics in academia.
[366.72s -> 373.88s]  And also, you kind of need to balance between difficulty and simplicity.
[373.88s -> 379.20s]  And what I mean by that is that if your benchmark is way too complicated, then basically
[379.20s -> 384.56s]  all methods will have essentially random performance, so no one will use your benchmark.
[384.56s -> 390.20s]  And if your benchmark is too simple, then the baseline will be so good that no one will
[390.20s -> 393.96s]  use your benchmark because no one can beat the baseline.
[393.96s -> 396.24s]  This is really something that is specific to academia.
[396.24s -> 398.80s]  In the real world, you're not going to be able to change the tasks that you're
[398.80s -> 403.00s]  performing based on how good your model is.
[403.00s -> 407.30s]  So that's why I kind of just want to highlight that because usually people talk about evaluations,
[407.30s -> 412.40s]  but there's really different ways of evaluating and different reasons why we evaluate.
[413.04s -> 414.20s]  Does that all make sense?
[414.20s -> 415.92s]  Also, feel free to ask questions.
[417.40s -> 418.98s]  Great, okay.
[418.98s -> 423.40s]  So benchmarks in academia, this is really the way we drive the field.
[423.40s -> 425.14s]  So this is the MMLU benchmark.
[425.14s -> 427.28s]  I think Archit briefly mentioned it,
[427.28s -> 429.66s]  but I'll talk about it later again.
[429.66s -> 432.68s]  So this is the most standard benchmark right now,
[432.68s -> 436.48s]  and you basically see that in the last four-ish years,
[436.48s -> 439.70s]  it has gone from 25% accuracy, which is essentially random
[439.74s -> 443.90s]  because it's multiple choice and there are four choices,
[443.90s -> 447.00s]  to around 90-ish percent accuracy.
[448.54s -> 452.54s]  So yeah, benchmarking is really what drives the progress of the field.
[452.54s -> 454.44s]  And again, you see what I mean here.
[454.44s -> 457.36s]  What I meant here is that it's not really the differences
[457.36s -> 460.44s]  between small points that matter, at least in academia.
[460.44s -> 462.34s]  You have to take a step back and you have to think,
[462.34s -> 465.86s]  what matters is how your models will perform over 10 years
[465.86s -> 468.10s]  and making sure that the model on the top right here
[468.10s -> 470.62s]  is better than the model on the bottom left,
[471.78s -> 474.54s]  even if the benchmark is not perfect.
[474.54s -> 477.14s]  And I think MMLU is a pretty good one in that sense.
[479.06s -> 481.42s]  Okay, so there are two main types,
[481.42s -> 485.94s]  at least classically, of tasks in NLP.
[485.94s -> 489.98s]  Closed-ended tasks, so I'll talk about it later,
[489.98s -> 492.62s]  but essentially, you can think about classification,
[492.62s -> 496.38s]  where you know exactly the label, the correct label
[496.38s -> 498.44s]  for the task that you're performing.
[498.44s -> 500.86s]  So here, this is the IMDb data set
[500.86s -> 504.14s]  where you're asked to say whether a sentence
[504.14s -> 506.36s]  has positive sentiment or negative sentiment.
[506.36s -> 508.42s]  So the text is, read the book, forget the movie.
[508.42s -> 511.62s]  So this is about a sentiment classification of the movie.
[511.62s -> 513.46s]  So here, it's basically negative.
[514.50s -> 516.30s]  And then there's open-ended evaluation.
[516.30s -> 517.58s]  So think about chatGPT,
[517.58s -> 519.24s]  like how do you evaluate something like that,
[519.24s -> 522.10s]  where really, there's no correct answer
[522.10s -> 525.74s]  and there are many possible correct answers
[525.76s -> 527.76s]  and they all have different qualities.
[529.12s -> 531.50s]  So we're gonna distinguish between those two.
[532.64s -> 534.56s]  So closed-ended evaluation.
[535.70s -> 537.98s]  So as I just said, closed-ended tasks,
[537.98s -> 541.34s]  there's a limited, it's basically defined as the task
[541.34s -> 544.70s]  where there's a limited number of potential answers,
[544.70s -> 547.42s]  think like less than 10, and often,
[547.42s -> 551.86s]  there's just one or maybe a few correct possible answers.
[552.58s -> 556.42s]  So this really is standard machine learning.
[556.42s -> 558.02s]  If you think about standard classification,
[558.02s -> 559.02s]  you can just do accuracy,
[559.02s -> 561.58s]  you can look at your precision, your recalls.
[561.58s -> 564.42s]  There's nothing special here about NLP.
[564.42s -> 567.18s]  That is not to say that it's simple,
[567.18s -> 569.94s]  it's just that there's nothing special about NLP here.
[571.12s -> 574.02s]  So some tasks, some closed-ended tasks,
[574.02s -> 576.70s]  I already told you about sentiment analysis.
[576.70s -> 579.22s]  So usually, this is a binary classification task
[579.22s -> 580.46s]  where you just have to say whether
[580.50s -> 583.74s]  the sentiment is positive or whether it's negative.
[583.74s -> 585.22s]  Another task is entailment.
[586.46s -> 588.70s]  Also, for sentiment analysis, the typical benchmarks,
[588.70s -> 590.90s]  I always put it next to the task,
[590.90s -> 593.36s]  is IMDB and SST from Stanford.
[593.36s -> 595.68s]  Entailment is SNLI, also from Stanford,
[595.68s -> 597.18s]  where basically you have some text.
[597.18s -> 601.18s]  So here, soccer game with multiple males playing,
[601.18s -> 602.54s]  and then you have a hypothesis,
[602.54s -> 603.82s]  some men are playing sport,
[603.82s -> 606.98s]  and you have to say whether the hypothesis is implied
[606.98s -> 608.98s]  or entailed by the text.
[608.98s -> 610.18s]  So here it is.
[611.14s -> 615.44s]  Other tasks, part of speech, typical benchmark,
[615.44s -> 618.38s]  pantry bank, and name entity recognition,
[618.38s -> 620.82s]  which is a kernel benchmark.
[622.04s -> 623.28s]  A few other tasks.
[623.28s -> 624.68s]  You don't need to know all of them,
[624.68s -> 627.72s]  but just to give you a brief overview.
[627.72s -> 629.78s]  Coreference resolution.
[629.78s -> 633.14s]  So this is actually a pretty challenging NLP task
[633.14s -> 634.86s]  where you have to say what pronoun,
[634.86s -> 637.22s]  which pronoun refers to what noun.
[637.22s -> 639.18s]  So you have here the sentence,
[639.18s -> 642.70s]  Mark told Pete many lies about himself,
[642.70s -> 644.78s]  which Pete included in his book,
[644.78s -> 646.38s]  he should have been more truthful.
[646.38s -> 649.96s]  And now you have to say what does he refer to,
[651.54s -> 653.50s]  whether he refers to Pete.
[653.50s -> 655.38s]  And then there's question answering,
[655.38s -> 657.08s]  where you basically have a long text,
[657.08s -> 658.34s]  and you ask a question,
[658.34s -> 660.62s]  and, well, sorry, the task asks a question,
[660.62s -> 661.90s]  and you're supposed to provide an answer
[661.90s -> 664.58s]  based on the text that you have before.
[664.58s -> 669.06s]  So those are some examples of close-ended tasks.
[670.02s -> 672.46s]  The key here is that the way we evaluate those
[672.46s -> 673.74s]  is just standard machine learning.
[673.74s -> 677.82s]  You can look at accuracy, precision, recall, F1 score.
[677.82s -> 680.22s]  Hopefully you all know about these type of metrics,
[680.22s -> 681.06s]  but if you don't,
[681.06s -> 685.22s]  you should look at Chris Peach's class.
[685.22s -> 687.34s]  I think it's CS224U.
[688.26s -> 689.40s]  But his lecture is online.
[689.40s -> 692.64s]  It's actually really good on different metrics.
[694.80s -> 696.78s]  So the ways that people evaluate
[696.78s -> 698.46s]  some of these benchmarks
[698.70s -> 702.46s]  is usually by looking at many of them concurrently.
[702.46s -> 704.22s]  So the most common, I would say,
[704.22s -> 707.26s]  like super or multitask benchmark is called superglue.
[708.10s -> 713.10s]  So here you see on the columns here
[713.10s -> 716.08s]  you have all the different tasks in superglue.
[716.08s -> 717.98s]  So I think there are eight or nine.
[718.86s -> 722.62s]  And then you really just look at the average performance
[722.62s -> 723.90s]  in each of these benchmarks,
[723.90s -> 725.38s]  and you get a ranking on that.
[725.38s -> 727.30s]  And it is kind of an attempt
[727.30s -> 730.50s]  to measure general language capabilities.
[730.50s -> 732.80s]  This is what people used to do, I would say,
[732.80s -> 736.62s]  until maybe two years ago.
[736.62s -> 739.58s]  I will tell you about what people do now
[739.58s -> 741.74s]  around the end of the lecture.
[741.74s -> 743.46s]  But yeah, superglue is definitely something
[743.46s -> 745.10s]  you should at least be aware of.
[745.90s -> 749.78s]  And the example of tasks that are in superglue,
[749.78s -> 753.74s]  one is BoolQ, which is simply you have some text,
[753.74s -> 755.82s]  you have some question, and you have to say
[755.82s -> 757.82s]  whether the answer is yes or whether it's no.
[757.82s -> 759.22s]  So that's very easy to evaluate.
[759.22s -> 762.14s]  You just look at accuracies or precision recall.
[762.14s -> 764.18s]  Entailment, we already talked about.
[765.06s -> 767.22s]  And then the other ones like coreference resolution,
[767.22s -> 769.74s]  which we also talked about, and meaning of words,
[769.74s -> 772.58s]  which is something where you have two sentences
[772.58s -> 774.54s]  with the same words, and you have to say
[774.54s -> 776.84s]  whether they actually mean the same thing
[776.84s -> 777.68s]  in this sentence.
[777.68s -> 779.14s]  For example, if you have bank,
[779.14s -> 782.46s]  it could mean bank like water and bank like money.
[782.50s -> 785.18s]  And you have to say whether in these two sentences
[785.18s -> 788.02s]  they refer to the same concept.
[788.02s -> 790.74s]  And there are some question-answering tasks too.
[791.90s -> 793.74s]  So this is about superglue.
[793.74s -> 794.98s]  Are there any questions?
[797.62s -> 798.66s]  No, cool.
[800.58s -> 803.70s]  So again, although I said many times
[803.70s -> 806.42s]  that this is essentially just classical machine learning,
[806.42s -> 809.12s]  I want to emphasize that it doesn't mean that it's simple
[809.12s -> 811.84s]  and you really have to think carefully about what you do
[812.12s -> 814.48s]  when you use those type of close-ended tasks.
[814.48s -> 816.22s]  In particular, you're gonna have to choose
[816.22s -> 818.56s]  whether you look at accuracies, precision, recall,
[818.56s -> 821.30s]  F1 score, ROC curves, AUC curves.
[821.30s -> 822.44s]  If you don't know these names,
[822.44s -> 825.90s]  you should really check out the sklearn documentation
[825.90s -> 829.66s]  or the lecture from Chris Peach that I linked above,
[829.66s -> 831.48s]  both of which are really good.
[831.48s -> 834.34s]  But depending on which metric you will choose,
[834.34s -> 837.68s]  you will decide on very different type of algorithms.
[837.68s -> 840.40s]  And the usual example is that if,
[841.24s -> 844.52s]  let's say you look at spam,
[844.52s -> 845.72s]  you want to do classification
[845.72s -> 848.54s]  of whether an email is spammed or not.
[848.54s -> 852.28s]  Most emails are not spammed, thankfully, at least I hope.
[853.20s -> 856.44s]  So let's say that 90% of emails were actually not spam
[856.44s -> 858.12s]  and only 10% of them are spam.
[858.12s -> 859.68s]  If you look at the accuracy,
[859.68s -> 861.80s]  then just a random classifier
[861.80s -> 865.56s]  that predicts the most likely label will get 90% accuracy.
[865.56s -> 867.60s]  And that seems, if you don't know really
[867.60s -> 870.08s]  about the dataset, like 90% accuracy seems good.
[870.60s -> 871.44s]  But in reality here,
[871.44s -> 873.60s]  it means that you're not classifying anything.
[873.60s -> 875.60s]  So that's why you want to look at precision, recall,
[875.60s -> 876.44s]  and F1 score.
[877.96s -> 880.32s]  Anyways, I will not talk too much about that
[880.32s -> 882.32s]  because again, this is not specific to NLP,
[882.32s -> 884.40s]  but it doesn't mean it's easy.
[884.40s -> 886.28s]  Another issue is that once you have
[886.28s -> 887.68s]  multiple different tasks,
[887.68s -> 890.32s]  there's a question of how do you aggregate these metrics?
[890.32s -> 892.48s]  So right before, I told you,
[892.48s -> 895.44s]  oh, you just take the average between all of these things.
[895.44s -> 898.52s]  This honestly is a really terrible thing to do,
[898.56s -> 900.36s]  but that's actually what people do.
[901.52s -> 904.20s]  But these columns actually mean very different things.
[904.20s -> 905.52s]  Some of them are accuracies,
[905.52s -> 909.44s]  others are F1 score, others are correlations,
[909.44s -> 910.48s]  and you just average everything.
[910.48s -> 912.36s]  I can't remember which benchmark,
[912.36s -> 913.92s]  but I remember a few years ago,
[913.92s -> 916.28s]  there was one benchmark where actually one of the columns
[916.28s -> 918.32s]  was you were better,
[918.32s -> 919.84s]  basically you had better performance
[919.84s -> 921.88s]  if the value was lower,
[921.88s -> 924.20s]  and you still took an average of these things
[924.20s -> 925.56s]  until someone realized that they're like,
[925.56s -> 928.08s]  maybe we should put a minus there.
[928.08s -> 930.80s]  So yeah, be careful,
[930.80s -> 934.72s]  and don't always think that what people do in academia
[934.72s -> 937.84s]  or that what people do is correct.
[938.88s -> 941.60s]  You should think a little bit about that.
[941.60s -> 942.80s]  Then there are some other questions
[942.80s -> 944.40s]  I want you to think about.
[944.40s -> 946.16s]  Where do those labels come from?
[946.16s -> 948.84s]  I said that is usually a real answer,
[948.84s -> 952.92s]  but how you actually get those labels is unclear,
[952.92s -> 956.72s]  so I will tell you about some issues in the next slide.
[957.40s -> 958.88s]  And also related to that,
[958.88s -> 960.44s]  there might be some spurious correlations,
[960.44s -> 962.68s]  and that's what we're gonna talk about right now.
[962.68s -> 967.68s]  So we already talked about SNLI, so entailment,
[968.28s -> 970.12s]  so here you have again your premise,
[970.12s -> 973.60s]  the economy could still be better,
[973.60s -> 976.44s]  and the hypothesis, the economy has never been better,
[976.44s -> 978.14s]  and you have to say whether the hypothesis
[978.14s -> 979.96s]  is implied by the premise.
[979.96s -> 983.84s]  And what this paper from 2019 found
[983.84s -> 986.08s]  is that actually all the different models
[986.12s -> 987.36s]  were performing really well,
[987.36s -> 991.80s]  but if you just classified based on the hypothesis,
[991.80s -> 993.32s]  you could also perform really well.
[993.32s -> 996.50s]  So even if you did not look at the premise,
[996.50s -> 997.90s]  which seems like something that you need
[997.90s -> 999.84s]  to take into account because it's part of the task,
[999.84s -> 1000.80s]  you could perform well.
[1000.80s -> 1003.48s]  And the reason why is because they realized
[1003.48s -> 1008.24s]  that when the humans actually wrote the hypotheses,
[1008.24s -> 1010.04s]  they were asked, write a hypothesis
[1010.04s -> 1012.80s]  which is not entailed by the premise,
[1012.80s -> 1015.60s]  and how humans usually do that is by adding a negation.
[1016.48s -> 1018.60s]  So if you only look at the hypothesis
[1018.60s -> 1019.58s]  and you see that there's a negation,
[1019.58s -> 1023.90s]  it's very likely that it's not entailed by the premise.
[1023.90s -> 1027.60s]  So again, even though this is standard machine learning,
[1027.60s -> 1030.80s]  be really careful about what metric you use
[1030.80s -> 1033.16s]  and where do the labels come from.
[1034.04s -> 1035.76s]  And don't do everything,
[1035.76s -> 1038.00s]  don't just use what people do
[1038.00s -> 1039.72s]  thinking that if there was an issue,
[1039.72s -> 1041.24s]  people would have realized.
[1041.88s -> 1046.56s]  So yeah, so that is previous correlations.
[1046.56s -> 1049.30s]  Any questions on close-ended tasks?
[1052.08s -> 1053.28s]  Cool.
[1053.28s -> 1054.88s]  Okay, open-ended evaluations.
[1054.88s -> 1056.06s]  I'm gonna mostly talk about that
[1056.06s -> 1058.78s]  because this is what is specific to NLP.
[1059.84s -> 1062.64s]  So open-ended evaluation or open-ended task
[1062.64s -> 1065.88s]  is essentially the opposite of the close-ended task
[1065.88s -> 1068.36s]  which is to say that there are many possible
[1068.36s -> 1071.56s]  correct answers and you cannot enumerate all of them.
[1072.68s -> 1075.52s]  So you really can't use standard machine learning metrics.
[1077.70s -> 1080.20s]  Even more than the fact that you cannot
[1080.20s -> 1081.96s]  enumerate all the possible answers,
[1081.96s -> 1086.20s]  usually there are different levels of correctness.
[1086.20s -> 1088.08s]  So if I ask you to write a book
[1088.08s -> 1090.10s]  or if I ask Chai GPT to write a book,
[1090.10s -> 1091.40s]  like it might be a decent book
[1091.40s -> 1093.40s]  but there might be a better book
[1093.40s -> 1094.40s]  that it could have written
[1094.40s -> 1095.88s]  or that another model could write.
[1095.88s -> 1097.20s]  So it's not just right and wrong,
[1097.88s -> 1099.08s]  it's a continuum.
[1100.56s -> 1101.88s]  Yeah, it's a continuum.
[1103.04s -> 1105.28s]  So standard examples for open-ended tasks.
[1105.28s -> 1107.62s]  The two most common ones are summarization.
[1108.60s -> 1111.96s]  So summarization, you have a long piece of text
[1111.96s -> 1113.58s]  and you just have to summarize it
[1113.58s -> 1115.20s]  in less than X characters.
[1116.16s -> 1121.16s]  Standard benchmark is the CNN and Daily Mail benchmarks.
[1121.68s -> 1125.28s]  So the way they actually collected that dataset
[1125.28s -> 1127.40s]  is that they took a lot of CNN articles
[1127.40s -> 1128.82s]  and you know at the top of CNN articles,
[1128.82s -> 1131.60s]  you have bullet points that kind of say
[1131.60s -> 1133.48s]  what are the most important things in the article.
[1133.48s -> 1136.56s]  So they use this as essentially the gold summary.
[1138.10s -> 1140.28s]  So this is the classical one for summarization.
[1140.28s -> 1142.80s]  For translation, you basically have sentences
[1142.80s -> 1143.92s]  in two different languages
[1143.92s -> 1146.56s]  and you have to translate from one to the other.
[1146.56s -> 1148.82s]  So those are the classical ones.
[1148.82s -> 1151.28s]  The way that people currently do it is,
[1151.28s -> 1153.84s]  I would say the most standard task right now
[1153.84s -> 1155.22s]  is instruction following.
[1156.10s -> 1159.66s]  Instruction following is kind of the mother of all tasks
[1159.66s -> 1164.54s]  in the sense that you can view any previous task
[1164.54s -> 1167.46s]  as just a chatbot or like some question
[1167.46s -> 1169.20s]  that you ask to basically chat GPT.
[1169.20s -> 1170.66s]  You can think classification,
[1170.66s -> 1172.44s]  I could just ask chat GPT to do that.
[1172.44s -> 1173.70s]  You can think summarization,
[1173.70s -> 1174.94s]  I could ask chat GPT to do that.
[1174.94s -> 1177.66s]  So essentially, you could just view chatbot
[1177.66s -> 1180.14s]  as the most general type of task
[1180.14s -> 1183.66s]  and you can ask it to perform any possible task
[1183.66s -> 1186.10s]  and it should just provide the answer for that task.
[1186.10s -> 1188.38s]  So this is what we call instruction following.
[1188.38s -> 1189.46s]  So as you might think,
[1190.58s -> 1193.30s]  evaluation is very hard in that domain
[1193.30s -> 1195.02s]  and that's what we'll talk about later
[1195.02s -> 1197.42s]  is how do you evaluate something like chat GPT.
[1198.96s -> 1200.98s]  Okay, so types of evaluation methods
[1200.98s -> 1203.36s]  for text generation or open-ended tasks.
[1204.70s -> 1207.14s]  The classical ones are content overlap metrics
[1207.14s -> 1208.78s]  which I'll talk about.
[1208.78s -> 1212.10s]  So that's really comparing just the words
[1212.10s -> 1213.66s]  between a reference answer,
[1213.66s -> 1215.30s]  a gold answer that humans wrote
[1215.30s -> 1218.92s]  and the actual generation that you got from your model.
[1218.92s -> 1221.10s]  Then there are model-based metrics
[1221.10s -> 1226.10s]  where you basically turn evaluation into machine learning.
[1226.54s -> 1230.46s]  So you train a model to basically become an evaluator
[1231.74s -> 1233.08s]  and then there's human evaluation
[1233.08s -> 1236.58s]  which is usually seen as the gold standard
[1236.58s -> 1238.04s]  for open-ended tasks.
[1238.04s -> 1242.12s]  Okay, so content overlap metrics.
[1242.12s -> 1242.96s]  So as I just said,
[1242.96s -> 1245.08s]  this is really just comparing word by word
[1246.00s -> 1249.56s]  or group of words between the generated sequence
[1249.56s -> 1250.76s]  and some reference.
[1252.04s -> 1254.30s]  So here I have the generated sequence
[1254.30s -> 1256.82s]  being the woman went to the hardware store
[1256.82s -> 1257.88s]  and the gold reference
[1257.88s -> 1260.68s]  which is the reference written by humans.
[1260.68s -> 1262.30s]  I actually don't even know what the task is
[1262.30s -> 1265.64s]  but the reference here is they walked to the grocery store
[1265.64s -> 1268.00s]  and then what you do is that you just compare
[1268.92s -> 1269.92s]  the two different sentences
[1269.92s -> 1271.96s]  by looking at the lexical similarity
[1271.96s -> 1273.20s]  between those two texts.
[1274.58s -> 1277.20s]  And this is super fast and efficient
[1277.20s -> 1278.40s]  and the way you usually do that
[1278.40s -> 1280.34s]  is by using n-gram overlap metrics.
[1280.34s -> 1282.36s]  So what I mean by this is that
[1282.36s -> 1284.76s]  the simplest possible thing is just to say
[1284.76s -> 1287.56s]  whether for every word in the generated sequence
[1287.56s -> 1290.32s]  whether it appears in the reference sequence
[1290.32s -> 1294.80s]  and if it does then you kind of increment your performance.
[1294.80s -> 1296.74s]  So n-grams is essentially the same thing
[1296.74s -> 1298.90s]  but instead of looking at a single word
[1298.90s -> 1301.26s]  you basically look at bigrams, trigrams
[1301.26s -> 1306.26s]  and kind of multiple words next to one another.
[1307.34s -> 1309.46s]  So the usual overlap metrics,
[1309.46s -> 1312.38s]  the most common ones are blue and the rouge.
[1312.38s -> 1314.58s]  Blue means blue and the rouge means red.
[1315.86s -> 1317.26s]  That's not what they stand for though
[1317.26s -> 1319.66s]  and I always forget what they stand for.
[1319.66s -> 1321.30s]  But basically blue what it is
[1321.30s -> 1324.30s]  is that it's a n-gram overlap metric
[1324.30s -> 1328.62s]  that tries to look at precision
[1328.62s -> 1332.50s]  while rouge is what looks at the recall.
[1332.50s -> 1335.22s]  So as I said before, as I alluded before
[1335.22s -> 1336.90s]  what is important even if you turn everything
[1336.90s -> 1339.22s]  into a kind of sentence classification
[1339.22s -> 1341.06s]  you have to think about whether you care
[1341.06s -> 1342.42s]  about precision or recall.
[1343.98s -> 1346.04s]  So those metrics are not ideal
[1346.04s -> 1348.26s]  but until I would say two years ago
[1348.26s -> 1349.74s]  they were the gold standard
[1349.74s -> 1352.52s]  for translation and summarization.
[1352.52s -> 1354.92s]  For translation people use blue
[1354.92s -> 1358.70s]  because you really want to,
[1358.70s -> 1359.54s]  yeah you really want to,
[1359.54s -> 1361.04s]  you basically look at the,
[1361.04s -> 1363.12s]  let's say I'm translating from French to English,
[1363.12s -> 1366.72s]  I want to look at the generated sequence in English
[1366.72s -> 1369.32s]  and the actual reference sequence in English
[1369.32s -> 1372.44s]  and I want to know whether every bigram
[1372.44s -> 1374.26s]  that I generated appears
[1374.26s -> 1376.48s]  or like how many of the bigrams that I generated
[1376.48s -> 1380.22s]  appears in the reference sequence.
[1381.22s -> 1383.46s]  There's one additional thing
[1383.46s -> 1385.06s]  which is that they don't only look at precision
[1385.06s -> 1386.54s]  because you could get a very high precision
[1386.54s -> 1388.94s]  by actually predicting something very small.
[1388.94s -> 1392.50s]  For example if you always predicted the word the
[1392.50s -> 1394.22s]  only generated the word the
[1394.22s -> 1396.90s]  you would most likely get very high precision
[1396.90s -> 1399.90s]  because the usually appears in every sentence
[1399.90s -> 1402.86s]  or like let's say a full stop.
[1403.76s -> 1406.62s]  So there's also like some length penalty.
[1406.62s -> 1408.16s]  And rouge is kind of the opposite
[1408.16s -> 1409.46s]  it just looks at recall.
[1410.62s -> 1415.06s]  So those are the common content overlap metrics.
[1415.06s -> 1419.12s]  And just to illustrate why those are not ideal,
[1420.86s -> 1421.78s]  well they have many issues
[1421.78s -> 1423.56s]  but one of them is that they don't really
[1423.56s -> 1427.30s]  take into account the semantic relatedness between words.
[1427.30s -> 1429.14s]  So imagine that Chris asks you
[1429.14s -> 1432.90s]  are you enjoying the CS224N lectures?
[1432.90s -> 1436.72s]  Of course the gold answer is heck yes.
[1437.88s -> 1439.14s]  So that's the reference answer.
[1439.14s -> 1441.80s]  So now let's say that the model just generates yes.
[1441.80s -> 1442.86s]  So here what you're gonna have
[1442.86s -> 1445.74s]  if I look at the blue score
[1445.74s -> 1449.84s]  I will have 67% essentially blue score
[1449.84s -> 1452.02s]  because two of the words that I generated
[1452.02s -> 1454.04s]  or two of the unigrams I generated
[1454.04s -> 1457.50s]  are in the reference, in the gold reference.
[1457.50s -> 1459.74s]  If I generate you know it
[1459.74s -> 1463.38s]  then I will only have a single token
[1463.38s -> 1465.22s]  in the generated sequence that appears
[1465.22s -> 1468.28s]  in the reference sequence which is the exclamation point
[1468.28s -> 1472.36s]  so I get a much lower blue score.
[1472.36s -> 1475.88s]  And if I just say yup then that doesn't appear at all
[1475.88s -> 1478.16s]  in the generated sorry in the reference sequence
[1478.16s -> 1481.64s]  so I get zero blue score which is a false negative
[1481.64s -> 1483.40s]  because really it literally means
[1483.40s -> 1485.54s]  the same thing as heck yes.
[1486.68s -> 1488.52s]  So hopefully you see that like
[1489.36s -> 1491.14s]  these metrics really have issues.
[1492.04s -> 1493.96s]  Also you can have false positives
[1493.96s -> 1496.36s]  for example if you say heck no
[1496.36s -> 1497.88s]  then most of the words are the same
[1498.44s -> 1500.48s]  so you get 67% blue score
[1501.40s -> 1503.84s]  but it really means something completely different.
[1506.16s -> 1507.00s]  Does that make sense?
[1507.00s -> 1507.84s]  Any questions?
[1509.96s -> 1510.96s]  Cool.
[1510.96s -> 1513.36s]  So very naturally now that you know
[1513.36s -> 1515.22s]  everything about word embeddings
[1515.22s -> 1518.84s]  what you might ask is oh why do we look at words
[1518.84s -> 1520.76s]  if what we could do is looking at like
[1520.76s -> 1523.92s]  learned repressations which really kind of
[1523.92s -> 1528.72s]  maintained the semantic similarity between words.
[1528.72s -> 1531.32s]  So this is exactly what people have done
[1532.60s -> 1537.76s]  around 2019 I think is that they took some
[1537.76s -> 1539.56s]  or even before actually 2016
[1539.56s -> 1541.92s]  they took some word embeddings
[1541.92s -> 1545.12s]  they associated every word in the
[1546.96s -> 1549.24s]  reference sequence to a word embedding
[1549.24s -> 1552.20s]  every word in the generated sequence
[1552.20s -> 1553.96s]  to the corresponding word embedding
[1553.96s -> 1556.72s]  and they basically started comparing the word embeddings.
[1556.72s -> 1558.72s]  So a very simple way of comparing word embeddings
[1558.72s -> 1560.24s]  is just you take the average
[1560.24s -> 1562.48s]  between the word embeddings in the reference sequence
[1562.48s -> 1564.00s]  and the average between the word embeddings
[1564.00s -> 1565.12s]  in the generated sequence
[1565.12s -> 1567.08s]  and you maybe look at cosine similarity.
[1567.08s -> 1568.86s]  I mean there are smarter ways of doing it
[1568.86s -> 1571.14s]  but honestly at this point it's not that important.
[1571.14s -> 1573.24s]  So you can think about averaging.
[1575.24s -> 1577.36s]  Another one as you know
[1578.54s -> 1581.12s]  at this point word embeddings
[1581.12s -> 1584.68s]  don't really take into account the contextual
[1584.68s -> 1588.80s]  or like the context of where the word basically appears.
[1588.80s -> 1591.60s]  So a better way of getting good repressations
[1591.60s -> 1595.18s]  for a word is by looking essentially at BERT.
[1595.18s -> 1597.52s]  So what you could do is you could take a BERT model
[1597.52s -> 1600.64s]  you could pass the generated sequence to it
[1600.64s -> 1603.04s]  you get some embeddings
[1603.04s -> 1605.28s]  and then you can take BERT again
[1605.28s -> 1607.76s]  the same BERT you pass the reference sequence to it
[1607.76s -> 1608.96s]  you get some other embeddings
[1608.96s -> 1610.72s]  and then you do again some comparison.
[1611.12s -> 1613.32s]  This BERT score, pretty famous paper
[1613.32s -> 1615.56s]  they do some smart comparison
[1615.56s -> 1617.88s]  but it's not that important to understand
[1617.88s -> 1620.02s]  what exactly they do.
[1620.02s -> 1623.52s]  What is important is that they take some smart averaging
[1623.52s -> 1624.84s]  between those words.
[1626.98s -> 1628.26s]  Cool, any questions?
[1631.62s -> 1632.46s]  Okay.
[1634.42s -> 1638.92s]  So that was the simplest type of learning methods
[1638.96s -> 1643.08s]  which is word matching.
[1643.08s -> 1647.00s]  Another slightly more complicated one is called BLURT
[1647.00s -> 1650.96s]  also pretty famous which is a mix between BLURT and BERT.
[1652.00s -> 1653.60s]  So the way that they did that
[1653.60s -> 1656.80s]  is that basically they took a pre-trained BERT
[1656.80s -> 1660.28s]  and then they do some continual pre-training
[1660.28s -> 1662.98s]  by trying to predict the BLURT score
[1662.98s -> 1664.20s]  and some other metrics
[1664.20s -> 1666.42s]  and then they fine-tune, that's the important part
[1666.42s -> 1668.72s]  is that they fine-tune their pre-trained model
[1669.36s -> 1671.48s]  to actually do the evaluation that they care about.
[1671.48s -> 1674.80s]  So let's say that I have a lot of different sequences
[1674.80s -> 1677.48s]  and I have some human annotations
[1677.48s -> 1679.16s]  of how I should be evaluating it
[1679.16s -> 1681.92s]  I could just treat that as a normal machine learning task
[1681.92s -> 1685.76s]  and I just fine-tune my BERT to do the evaluation.
[1686.88s -> 1687.94s]  So this is BLURT.
[1689.72s -> 1690.56s]  Any questions?
[1691.82s -> 1692.66s]  Yes.
[1693.66s -> 1696.40s]  If you pre-train on BLUE
[1696.40s -> 1697.88s]  wouldn't it cause the same problems
[1698.44s -> 1699.96s]  if your pre-training passes BLUE
[1699.96s -> 1702.50s]  then your BERT would learn the ability
[1702.50s -> 1706.04s]  to model languages semantically in the first place?
[1706.04s -> 1707.52s]  Yeah, that's a very good point.
[1707.52s -> 1709.88s]  So actually I also find it kind of surprising
[1709.88s -> 1711.36s]  so they did two things.
[1711.36s -> 1714.36s]  First they do the real pre-training of BERT
[1714.36s -> 1716.24s]  and then they do continual pre-training
[1716.24s -> 1718.14s]  for predicting BLUE.
[1718.14s -> 1719.76s]  And the reason why is because usually they say
[1719.76s -> 1722.40s]  we have a lot of sequences in our data set
[1722.40s -> 1723.24s]  that are unlabeled.
[1723.24s -> 1727.04s]  So we have some reference sequences
[1727.08s -> 1729.16s]  and some generated sequences
[1729.16s -> 1731.60s]  but we don't have the human annotation
[1731.60s -> 1733.40s]  of whether this is good or bad.
[1733.40s -> 1736.24s]  So we will treat that as an unsupervised learning
[1736.24s -> 1737.44s]  objective.
[1737.44s -> 1739.40s]  So what do you use for the unsupervised learning
[1739.40s -> 1740.32s]  objective?
[1740.32s -> 1741.40s]  Well you have to use something
[1741.40s -> 1742.72s]  and they basically use BLUE
[1742.72s -> 1746.44s]  and they also use actually BERT score.
[1746.44s -> 1748.34s]  So they use many different tasks
[1748.34s -> 1750.44s]  and they basically do multi-task learning.
[1751.88s -> 1752.72s]  Cool.
[1753.48s -> 1758.48s]  Okay, so one important issue with all these methods
[1759.00s -> 1761.68s]  is that really they can only be as good
[1761.68s -> 1763.16s]  as the references are.
[1763.16s -> 1766.84s]  And in reality the references are usually not that good.
[1766.84s -> 1771.84s]  So this is a paper that looks at summarization of news.
[1773.24s -> 1775.52s]  So basically as I said before,
[1775.52s -> 1778.60s]  most of the news summarization benchmarks
[1778.60s -> 1782.32s]  they usually take the reference summary
[1782.76s -> 1784.24s]  as being the bullet points that you find
[1784.24s -> 1785.88s]  at the top of an article.
[1786.78s -> 1788.28s]  And this is usually not that good.
[1788.28s -> 1792.88s]  So here what you see on the left,
[1792.88s -> 1795.52s]  this is what if you look at the correlation
[1795.52s -> 1800.52s]  between the x-axis being the human rate of,
[1801.04s -> 1803.88s]  or like the human evaluated performance of every model.
[1803.88s -> 1806.92s]  And then the y-axis you see the rouge L
[1806.92s -> 1808.68s]  which is just a variant of rouge.
[1809.68s -> 1813.48s]  And you look at whether basically these two are correlated.
[1813.48s -> 1815.00s]  And what you see is that they are essentially
[1815.00s -> 1817.64s]  not correlated which means that rouge L
[1817.64s -> 1822.44s]  on the standard references is really not correlated
[1822.44s -> 1824.72s]  to what humans would say is a good summary.
[1825.56s -> 1828.52s]  That is not to say that rouge is a bad score.
[1828.52s -> 1831.04s]  That is to say that actually the references are bad
[1831.04s -> 1833.52s]  because if you look at the exact same thing
[1833.52s -> 1837.48s]  but now you ask experts to write very good summaries
[1837.56s -> 1838.64s]  then you see that the correlation
[1838.64s -> 1840.40s]  actually increases by a decent amount.
[1840.40s -> 1843.66s]  Still not perfect, rouge is definitely not perfect
[1843.66s -> 1845.96s]  but at least it's much better.
[1845.96s -> 1847.92s]  So this is to say that the metric itself
[1847.92s -> 1850.24s]  is not always perfect but not only this
[1850.24s -> 1853.18s]  the references are usually actually not great.
[1855.16s -> 1856.62s]  Cool.
[1856.62s -> 1858.84s]  So that begs a very natural question
[1858.84s -> 1863.84s]  which is can we just dump and basically move away
[1863.94s -> 1865.70s]  from reference-based evaluation?
[1865.70s -> 1868.34s]  So as we just said, reference-based evaluations
[1868.34s -> 1870.62s]  are the ones that compare human-written references
[1870.62s -> 1874.90s]  to some model outputs using some different type of metrics
[1876.64s -> 1880.98s]  and those used to be the standard metrics
[1880.98s -> 1882.54s]  for evaluating or the standard benchmark
[1882.54s -> 1884.38s]  for evaluating NLP tasks.
[1884.38s -> 1887.48s]  I would say up to like two or three years ago.
[1887.48s -> 1892.32s]  Right now I think papers still have to always
[1892.32s -> 1895.02s]  show the blue scores like for example in translation.
[1895.78s -> 1899.44s]  Because reviewers want those but I don't think
[1899.44s -> 1902.62s]  like anyone in the real world actually uses them
[1902.62s -> 1904.16s]  but I might be wrong on that.
[1905.34s -> 1907.34s]  So yeah, so blue, rouge, bird score.
[1907.34s -> 1909.46s]  Oh and I was mostly talking about blue and rouge.
[1909.46s -> 1912.10s]  Bird score is actually still decently used
[1912.10s -> 1913.42s]  and actually pretty good.
[1914.38s -> 1916.34s]  Okay, so reference-free evaluation.
[1916.34s -> 1920.20s]  So reference-free evaluation is basically you have a model
[1920.20s -> 1922.34s]  and you ask it to give a score
[1922.34s -> 1924.80s]  but there are no human references.
[1924.80s -> 1927.68s]  So the way that this used to be done
[1927.68s -> 1930.96s]  is essentially by taking a model like bird again
[1930.96s -> 1934.48s]  but instead of comparing between a reference answer
[1934.48s -> 1937.36s]  and a generated answer, you could just ask it
[1937.36s -> 1939.92s]  to take the input and just predict the score.
[1939.92s -> 1941.44s]  That's like one simple way of doing it.
[1941.44s -> 1943.32s]  That used to really not work well.
[1943.32s -> 1946.28s]  And I say used to because until basically
[1946.28s -> 1948.26s]  chat GPT and GPT-4.
[1948.26s -> 1951.26s]  Now what people do and honestly that works super well
[1951.26s -> 1954.82s]  is that you just ask GPT-4 to do the same task
[1954.82s -> 1955.82s]  as you would ask a human.
[1955.82s -> 1958.42s]  So you give like a very long text
[1958.42s -> 1960.78s]  and then you give the generated summary
[1960.78s -> 1963.06s]  and you ask like how good is it essentially.
[1963.06s -> 1966.62s]  And that works surprisingly well.
[1966.62s -> 1969.76s]  So common benchmarks here are alpaca eval and empty bench.
[1969.76s -> 1971.02s]  There are many others now.
[1971.02s -> 1975.22s]  Honestly most people start using these type of techniques
[1975.22s -> 1977.66s]  but we'll be talking at least about alpaca eval.
[1979.74s -> 1980.74s]  Good.
[1980.90s -> 1983.02s]  So let's talk a little bit about human evaluation
[1983.02s -> 1986.34s]  before looping back to basically GPT-4.
[1987.80s -> 1990.60s]  So as we saw, the metrics until now,
[1990.60s -> 1992.62s]  they all have some shortcomings
[1993.74s -> 1995.66s]  and are definitely not as good as if you ask
[1995.66s -> 1997.70s]  directly human evaluation because they are based
[1997.70s -> 2000.22s]  on references.
[2001.52s -> 2004.82s]  So human evaluation is really the gold standard
[2004.82s -> 2009.46s]  for open-ended tasks.
[2009.46s -> 2013.90s]  And not only is it really the standard way
[2013.90s -> 2015.78s]  of doing evaluation or like the gold standard
[2015.78s -> 2017.82s]  for evaluation, it's also the gold standard
[2017.82s -> 2020.72s]  for developing new automatic evaluations.
[2020.72s -> 2024.52s]  So every time you develop a new automatic evaluations,
[2024.52s -> 2027.88s]  you will want to compare to what humans
[2027.88s -> 2030.16s]  would have basically predicted.
[2035.34s -> 2037.48s]  Okay so doing human evaluation,
[2037.48s -> 2039.18s]  at first it might seem very simple.
[2039.78s -> 2041.74s]  You basically ask humans to evaluate the quality
[2041.74s -> 2043.06s]  of some generated text.
[2043.06s -> 2044.72s]  Seems simple, right?
[2044.72s -> 2047.14s]  But actually it's super complicated
[2047.14s -> 2050.50s]  and it's a real challenge and it has many issues.
[2050.50s -> 2053.98s]  So first, oh sorry, I'll talk about that before.
[2053.98s -> 2055.80s]  Maybe one additional thing is that you should
[2055.80s -> 2058.42s]  not only ask the human, you usually ask it also
[2058.42s -> 2062.98s]  to ask them to evaluate across different axes.
[2062.98s -> 2064.66s]  For example, the fluency of the text
[2064.66s -> 2065.88s]  or the coherence of the text
[2065.92s -> 2068.56s]  or like common sense or like the style,
[2068.56s -> 2071.96s]  grammaticality, redundancy, and like different axes
[2071.96s -> 2073.32s]  that you might care about.
[2074.48s -> 2076.56s]  Another thing to note is that you should
[2076.56s -> 2080.56s]  absolutely never compare different human evaluations.
[2080.56s -> 2082.10s]  So if there's one paper that says,
[2082.10s -> 2085.14s]  oh, humans have evaluated the fluency of our text
[2085.14s -> 2087.70s]  to be, I don't know, four out of five,
[2087.70s -> 2090.40s]  and then another paper that says like three out of five,
[2090.40s -> 2091.96s]  like they use different humans,
[2091.96s -> 2095.08s]  different ways of like prompting the humans.
[2095.12s -> 2097.74s]  So it's absolutely not comparable.
[2098.72s -> 2102.20s]  Okay, so let's go back to some of the issues.
[2102.20s -> 2104.04s]  So as I said, human judgments are regarded
[2104.04s -> 2107.24s]  as the gold standard, but it definitely has issues.
[2107.24s -> 2108.98s]  First, it's super slow.
[2110.16s -> 2112.48s]  As you might expect, humans are definitely
[2112.48s -> 2115.36s]  not as fast as automatic metrics.
[2116.22s -> 2119.68s]  Second, at least in academia, it's still pretty expensive
[2119.68s -> 2124.64s]  to do because, I mean, when you pay well your workers,
[2125.12s -> 2128.00s]  it's pretty expensive to do well human evaluation.
[2129.04s -> 2131.52s]  Another part is inter-annotated disagreement.
[2131.52s -> 2134.52s]  So if I take two random people in this room
[2134.52s -> 2136.64s]  and I ask them to evaluate the quality
[2136.64s -> 2138.66s]  of a generated text, I can assure you
[2138.66s -> 2140.88s]  that you will really not agree.
[2140.88s -> 2144.54s]  So this is, especially if it's subjective, it's really bad.
[2144.54s -> 2147.16s]  But even if you talk for like one hour before
[2147.16s -> 2150.24s]  about like how you should be evaluating
[2151.24s -> 2155.40s]  generations, I can most likely guarantee you
[2155.40s -> 2159.28s]  that you will still disagree on many of the evaluations.
[2159.28s -> 2163.20s]  And to give you an example, when we were doing
[2163.20s -> 2166.20s]  Alpaca form last year, which is something
[2166.20s -> 2171.20s]  where we basically had to take some inputs
[2171.60s -> 2175.68s]  and then take two models, think ChatGPT, Alpaca,
[2175.68s -> 2178.96s]  and these type of models, and you just have
[2178.96s -> 2180.66s]  the two models predict an answer,
[2180.66s -> 2182.32s]  and then you ask the humans to say
[2182.32s -> 2184.28s]  which answer they prefer.
[2184.28s -> 2188.04s]  This is a very simple task, and this is what,
[2188.04s -> 2189.84s]  I will talk about it later, this is what
[2189.84s -> 2191.16s]  a lot of people basically use right now
[2191.16s -> 2194.80s]  for evaluating models like ChatGPT.
[2194.80s -> 2197.64s]  So an actual question is whether humans
[2197.64s -> 2200.16s]  are good at doing that, and what we saw is that,
[2200.16s -> 2202.44s]  so we were five researchers doing that,
[2202.44s -> 2205.20s]  and the five of us, we talked for like two or three
[2205.20s -> 2208.84s]  hours, we wrote extremely detailed rubrics
[2209.68s -> 2211.10s]  about how to do the evaluations, and still,
[2211.10s -> 2213.46s]  we only agreed 67% of the time.
[2213.46s -> 2218.16s]  So 50% is random, and if we just label things
[2218.16s -> 2221.60s]  independently, we only agree 67% of the time.
[2221.60s -> 2223.72s]  And we really tried to do our best.
[2223.72s -> 2225.32s]  We were working on this thing, so it's not
[2225.32s -> 2227.74s]  as if we were trying to do it quickly.
[2227.74s -> 2228.88s]  So really, people disagree.
[2228.88s -> 2231.56s]  Of course, if you then allow discussions
[2231.56s -> 2236.00s]  between the annotators, then agreement actually improves,
[2236.92s -> 2239.60s]  but then it becomes even slower and more expensive.
[2240.44s -> 2242.10s]  Intra-annotated disagreement.
[2242.10s -> 2243.98s]  This is something that is extremely annoying,
[2243.98s -> 2245.92s]  which is that if I ask a human,
[2245.92s -> 2248.68s]  if I ask myself right now to evaluate something,
[2248.68s -> 2250.96s]  or like in three hours, like after I have dinner,
[2250.96s -> 2253.24s]  or after I went to run, I will actually
[2253.24s -> 2255.24s]  give different annotations.
[2256.16s -> 2257.00s]  Yes?
[2257.00s -> 2260.06s]  How do you define samples you might need
[2260.06s -> 2261.98s]  to help by evaluating?
[2262.82s -> 2264.74s]  You mean for validating?
[2266.50s -> 2268.26s]  Yeah, so this is a very good question.
[2268.26s -> 2270.16s]  Honestly, there's no good answer.
[2270.16s -> 2272.58s]  The usual way that people do it is that
[2272.58s -> 2277.58s]  you look at some statistical metrics, basically,
[2278.58s -> 2280.10s]  where you're like, okay, I want to compare
[2280.10s -> 2281.42s]  between these two models.
[2281.42s -> 2284.02s]  I'm going to basically perform a t-test,
[2284.02s -> 2286.22s]  and I want to know that my p-value
[2286.22s -> 2288.00s]  is less than a certain amount.
[2288.00s -> 2289.50s]  What people usually do also when they have
[2289.50s -> 2292.50s]  human annotations, I unfortunately didn't
[2292.50s -> 2294.82s]  put a slide on that, but they have metrics
[2294.82s -> 2298.20s]  for computing the intra-annotated agreement,
[2298.20s -> 2299.74s]  and they try to achieve a certain
[2299.74s -> 2302.08s]  intra-annotated agreement, and if not,
[2302.08s -> 2304.40s]  they will essentially ask for more humans
[2304.40s -> 2305.62s]  or for relabelings.
[2309.66s -> 2312.02s]  It's not reproducible, and this is partly
[2312.02s -> 2314.74s]  because of the two things that we said before,
[2314.74s -> 2317.94s]  but also partly because, yeah,
[2318.14s -> 2320.34s]  mostly because of the two things before.
[2320.34s -> 2322.94s]  This is an interesting paper.
[2322.94s -> 2324.54s]  I think it's, I forgot which year.
[2324.54s -> 2327.42s]  I think it's from 2021, but I'm not sure,
[2327.42s -> 2328.58s]  where basically they say,
[2328.58s -> 2330.12s]  and I read from the abstract here,
[2330.12s -> 2332.66s]  just 5% of human evaluations are repeatable
[2332.66s -> 2334.94s]  in the sense that there are no prohibitive barriers
[2334.94s -> 2337.10s]  to repetition and sufficient information
[2337.10s -> 2340.62s]  about experimental design is publicly available
[2340.62s -> 2341.78s]  for rerunning them.
[2341.78s -> 2343.30s]  This is a paper that analyzed, I think,
[2343.30s -> 2347.30s]  128 different papers that were published
[2347.34s -> 2350.86s]  across five years, I think, between 2015 and 2020,
[2351.82s -> 2353.64s]  and they found that essentially only 5%
[2353.64s -> 2355.34s]  of those papers were reproducible.
[2357.06s -> 2359.32s]  So honestly, working with humans is hard.
[2359.32s -> 2361.42s]  That's definitely something to remember.
[2362.30s -> 2364.34s]  Another part is that humans only
[2365.62s -> 2368.38s]  basically evaluate precision and not recall.
[2368.38s -> 2369.58s]  So what I mean by that is that
[2369.58s -> 2372.90s]  if you show me what the model generated,
[2372.90s -> 2374.90s]  I can only evaluate that generation.
[2374.90s -> 2378.62s]  I can't evaluate all the other possible generations
[2378.62s -> 2380.10s]  that it could have generated,
[2381.00s -> 2382.70s]  because then you really have to sample a lot of things
[2382.70s -> 2385.66s]  and that will become way too slow and way too expensive.
[2386.54s -> 2390.20s]  And finally, usually the incentives are not aligned.
[2390.20s -> 2392.70s]  So what you want is for the humans
[2392.70s -> 2395.90s]  to basically do the best possible evaluations,
[2395.90s -> 2397.86s]  but what crowd workers usually want
[2397.86s -> 2400.34s]  is basically to maximize the amount of money
[2400.34s -> 2402.36s]  that they get paid per hour.
[2402.36s -> 2405.38s]  So to give you, again, a concrete example,
[2405.38s -> 2407.12s]  when we were doing our packet farm,
[2408.76s -> 2410.96s]  I think we were paying relatively well
[2410.96s -> 2413.08s]  in the sense that we were paying
[2413.08s -> 2416.68s]  1.5 times the minimum wage in California,
[2416.68s -> 2418.60s]  and then we divided basically,
[2418.60s -> 2420.72s]  we looked at how much time we would spend
[2420.72s -> 2424.48s]  to do the thing, basically to evaluate
[2424.48s -> 2426.88s]  a single example the best we could.
[2427.88s -> 2429.32s]  And then we divided by that time
[2429.32s -> 2432.54s]  to basically know how much we would pay for every example.
[2433.48s -> 2435.60s]  And what we realized is that
[2435.60s -> 2437.26s]  they ended up being paid, I think,
[2437.26s -> 2439.60s]  two or 2.5 times the minimum wage
[2439.60s -> 2440.68s]  because they were just doing things
[2440.68s -> 2443.32s]  like two, three times faster than us.
[2443.32s -> 2445.76s]  And I don't, I mean, we could be slow,
[2445.76s -> 2446.88s]  but I think what was happening
[2446.88s -> 2448.96s]  is that they were just trying to maximize
[2448.96s -> 2450.64s]  the dollars that they were getting per hour,
[2450.64s -> 2453.54s]  and as a result, they were finding shortcuts
[2453.54s -> 2455.84s]  for doing their evaluations.
[2455.84s -> 2457.08s]  And this is something that you really see
[2457.08s -> 2458.52s]  in a lot of papers.
[2458.52s -> 2460.08s]  For example, in our case,
[2460.08s -> 2462.72s]  you saw that humans really preferred longer answers.
[2462.72s -> 2466.28s]  And of course, if you give me two very long,
[2466.28s -> 2468.08s]  like two, sorry, generations,
[2468.08s -> 2470.24s]  and you ask me with minimal amount of work
[2470.24s -> 2471.60s]  to say which one is better,
[2471.60s -> 2472.60s]  like if I see a longer one,
[2472.60s -> 2474.14s]  I'm like, ah, probably there are more details,
[2474.14s -> 2476.04s]  probably it's better.
[2476.04s -> 2478.64s]  Anyways, it's not to say that everyone is like that,
[2478.64s -> 2480.62s]  but definitely, it's the incentives are not aligned,
[2480.62s -> 2482.48s]  so you have to be careful with this.
[2483.82s -> 2485.52s]  Other challenges.
[2485.56s -> 2488.64s]  First, you have to decide how to describe the task.
[2488.64s -> 2490.72s]  You really have to give very detailed rubrics
[2490.72s -> 2493.80s]  for how the humans have to evaluate the task.
[2493.80s -> 2494.64s]  Then there's the question of
[2494.64s -> 2496.72s]  how do you show the task to the humans?
[2496.72s -> 2498.60s]  For example, the order in which you give examples
[2498.60s -> 2500.52s]  are actually really important.
[2500.52s -> 2502.84s]  In our case, because we had two examples side by side,
[2502.84s -> 2504.28s]  they're actually, which one is on the left
[2504.28s -> 2505.40s]  and which one is on the right
[2505.40s -> 2506.78s]  is actually also very important.
[2506.78s -> 2509.28s]  So all these things really matter.
[2509.28s -> 2510.96s]  Of course, you can randomize these things away,
[2510.96s -> 2514.24s]  but it is, like, it adds challenges.
[2514.24s -> 2515.08s]  What metrics do you use?
[2515.52s -> 2517.52s]  I mean, this is not specific to humans.
[2517.52s -> 2520.96s]  Selecting the annotators, this is also very complicated.
[2520.96s -> 2522.40s]  You might think, okay, I have some money,
[2522.40s -> 2525.04s]  now I can go on Amazon, MTurk is,
[2525.04s -> 2527.84s]  and I can just ask them to evaluate
[2527.84s -> 2529.42s]  or to do some annotations.
[2529.42s -> 2532.60s]  But in reality, you want to have the good annotators.
[2532.60s -> 2535.80s]  So how it usually works in Amazon, in MTurk,
[2535.80s -> 2539.42s]  is that basically you say, oh, here's a task,
[2539.42s -> 2542.12s]  I want, like, 30 different people to do these annotations,
[2542.12s -> 2543.94s]  and then they start annotating,
[2543.94s -> 2547.26s]  and then if they don't achieve the level that you want,
[2547.26s -> 2549.90s]  you basically pay for what they annotated until then,
[2549.90s -> 2552.52s]  and you work with someone else afterwards.
[2552.52s -> 2554.24s]  So then there's a question of how do you decide
[2554.24s -> 2556.98s]  whether they achieved the performance that you want.
[2556.98s -> 2558.46s]  So you probably have to do, like,
[2558.46s -> 2559.90s]  some gold labeling before,
[2559.90s -> 2562.38s]  and then look at, like, some accuracies of how well,
[2562.38s -> 2564.58s]  and, like, some intra-annotator agreement with you
[2564.58s -> 2567.54s]  and with, like, the other researchers on your team.
[2567.54s -> 2569.90s]  So it is very complicated.
[2569.90s -> 2573.30s]  And not only this, you have to monitor that over time.
[2573.30s -> 2575.02s]  So there are many different ways you can monitor that
[2575.02s -> 2576.98s]  over time, looking again at the accuracy.
[2576.98s -> 2579.64s]  So maybe every, let's say a typical thing
[2579.64s -> 2582.14s]  is that every batch of example that you label,
[2582.14s -> 2586.16s]  you give a few examples that are actually ones
[2586.16s -> 2588.42s]  that you already know what the gold label is,
[2588.42s -> 2590.56s]  and you see how well they're performing on that.
[2590.56s -> 2592.06s]  Another way to look at is, like,
[2592.06s -> 2594.22s]  the time that they take to annotate.
[2595.58s -> 2596.42s]  Yeah.
[2597.62s -> 2599.66s]  Okay, so that was about humans.
[2599.66s -> 2602.86s]  So human evaluation is hard, but it is the gold standard.
[2603.90s -> 2606.30s]  Okay, now let's talk about reference-free evaluation
[2606.30s -> 2607.14s]  and chatbots.
[2607.14s -> 2610.14s]  So I already told you about it before very briefly.
[2610.14s -> 2612.94s]  How do you evaluate something like chatGPT?
[2612.94s -> 2614.26s]  This is extremely complicated,
[2614.26s -> 2616.90s]  because basically you could ask it any task you want,
[2617.98s -> 2621.30s]  and it can answer text that is arbitrarily long,
[2621.30s -> 2624.02s]  and that just makes evaluation extremely hard.
[2625.80s -> 2627.26s]  So as I suggested before,
[2627.26s -> 2628.42s]  the usual way that it's done
[2628.42s -> 2629.54s]  is that you take two models.
[2630.38s -> 2631.20s]  You put them side by side.
[2631.20s -> 2632.22s]  You ask the same question,
[2632.22s -> 2634.76s]  and you just ask either some humans or some model,
[2634.76s -> 2638.00s]  as we will see afterwards, which one is better.
[2639.14s -> 2642.68s]  So this is the most common benchmark right now,
[2642.68s -> 2644.50s]  I would say, for human evaluation.
[2644.50s -> 2646.26s]  It's called Chatbot Arena,
[2646.26s -> 2649.02s]  where basically anyone can go online
[2649.02s -> 2650.58s]  and just play for free
[2650.58s -> 2653.34s]  with some of the best models out there,
[2653.34s -> 2655.06s]  and all they ask you is to say
[2655.06s -> 2656.38s]  whether you prefer the one on the right
[2656.38s -> 2658.74s]  or whether you prefer the one on the left, essentially.
[2659.10s -> 2660.54s]  And then once they reach, I think,
[2660.54s -> 2664.98s]  a crazy amount of data, 200,000 human votes, for example,
[2664.98s -> 2666.88s]  they basically add it to a leaderboard.
[2666.88s -> 2668.84s]  And the way they add it to a leaderboard
[2668.84s -> 2670.62s]  is that they essentially,
[2670.62s -> 2672.42s]  I don't know if you know how chess works,
[2672.42s -> 2675.26s]  but they basically look at the ELO ratings.
[2675.26s -> 2679.26s]  So they basically put everything as if it was a tournament
[2679.26s -> 2680.30s]  such that not every model
[2680.30s -> 2683.00s]  has to play against every other model,
[2683.00s -> 2685.00s]  and then they get ELO scores.
[2685.00s -> 2690.00s]  Okay, so what's missing with this side-by-side human eval?
[2690.80s -> 2692.36s]  As I said, this is really the gold standard
[2692.36s -> 2694.90s]  for evaluation of chat LLMs,
[2694.90s -> 2696.80s]  but there are still some challenges.
[2696.80s -> 2700.44s]  First, it's basically random people online
[2700.44s -> 2701.92s]  that ask random questions,
[2701.92s -> 2704.40s]  and they provide their preferences.
[2705.36s -> 2707.40s]  So that may not be representative,
[2707.40s -> 2709.92s]  although arguably, when you have that many examples,
[2709.92s -> 2711.42s]  it becomes actually pretty representative
[2711.42s -> 2712.72s]  of what people would want.
[2713.56s -> 2717.22s]  So it's probably better than whatever we have,
[2718.32s -> 2719.88s]  but it is still not ideal.
[2719.88s -> 2723.16s]  And then really, the big issue is cost.
[2723.16s -> 2725.68s]  This takes a huge community effort
[2725.68s -> 2727.82s]  and a lot of people to work on that.
[2728.92s -> 2731.20s]  Also, it takes a lot of time
[2731.20s -> 2733.60s]  to get new models on the benchmark,
[2733.60s -> 2735.20s]  and only the notable models,
[2735.20s -> 2736.94s]  so things like the OpenAI models and the cloud
[2736.94s -> 2739.44s]  and the Google ones and the Facebook ones
[2739.44s -> 2740.88s]  are gonna be benchmarked.
[2740.88s -> 2742.48s]  You will never have for your random model
[2742.48s -> 2747.08s]  200,000 people who are willing to annotate it for free.
[2748.40s -> 2749.32s]  So this is an issue.
[2749.32s -> 2752.28s]  And again, as we talked about in the first slide,
[2752.28s -> 2753.56s]  even for those big companies,
[2753.56s -> 2754.60s]  they can definitely not do that
[2754.60s -> 2756.84s]  for development of their model.
[2756.84s -> 2758.68s]  This is something that comes at the end
[2758.68s -> 2759.88s]  for a model selection.
[2761.54s -> 2763.40s]  Okay, so how do we make it faster?
[2764.60s -> 2768.16s]  So one very natural solution
[2768.16s -> 2771.14s]  is basically to ask a large language model
[2771.14s -> 2772.36s]  to do the evaluation for you.
[2772.36s -> 2774.84s]  So imagine that I wanna compare GPT with MISROL.
[2774.84s -> 2779.84s]  I basically ask GPT-4 to evaluate which one is better.
[2779.88s -> 2781.46s]  This is surprisingly good,
[2781.46s -> 2783.36s]  and I will show you some results afterwards.
[2783.36s -> 2786.60s]  And some common versions are packer, eval, and emptybench,
[2786.60s -> 2788.40s]  probably the two most common ones.
[2790.08s -> 2791.32s]  So when we started doing that,
[2791.32s -> 2793.08s]  that's a problem I told you about.
[2793.08s -> 2794.88s]  We started that around last year,
[2795.38s -> 2800.38s]  and we found that using GPT-4 essentially for evaluation
[2800.76s -> 2803.16s]  is at least, if you look at the prices now,
[2803.16s -> 2806.08s]  it would be 100 times faster and 100 times cheaper
[2806.08s -> 2808.14s]  than if you use human evaluations.
[2809.08s -> 2811.60s]  But, and this is very surprising,
[2811.60s -> 2814.16s]  the agreement with humans is actually higher
[2814.16s -> 2816.96s]  than humans agree with themselves.
[2816.96s -> 2818.88s]  So what I mean by that is that if I ask,
[2818.88s -> 2819.84s]  so this is what we found.
[2819.84s -> 2821.68s]  If I ask four humans,
[2822.24s -> 2825.20s]  let's say I have a pool of four humans,
[2825.20s -> 2827.00s]  and I take out one human,
[2827.00s -> 2830.28s]  and I look at the agreement between that human's preferences
[2830.28s -> 2833.00s]  and the mode of the preferences of the three others,
[2833.00s -> 2835.44s]  and I do that in a leave one out fashion,
[2835.44s -> 2837.50s]  and I look at this agreement,
[2837.50s -> 2840.86s]  this will be lower than if I ask for the model
[2840.86s -> 2842.46s]  to predict essentially the preference
[2842.46s -> 2844.76s]  of the mode of the humans.
[2844.76s -> 2849.28s]  So in some ways, models are more highly correlated
[2849.28s -> 2850.88s]  with humans than humans themselves,
[2850.88s -> 2851.72s]  which is very surprising,
[2851.72s -> 2853.16s]  and I will tell you about it in two seconds
[2853.16s -> 2854.58s]  a little bit more.
[2854.58s -> 2856.42s]  When we did that, we actually used that
[2856.42s -> 2860.64s]  for collecting human preferences for our LHF,
[2860.64s -> 2863.56s]  so that's what we call our AIF,
[2863.56s -> 2866.90s]  as I think Archit told you about these things last week.
[2869.82s -> 2871.56s]  So going back to this issue,
[2871.56s -> 2874.48s]  or this surprising result that actually models
[2874.48s -> 2875.88s]  are more highly correlated with humans
[2875.88s -> 2877.04s]  than humans themselves,
[2877.04s -> 2880.12s]  the reason why this is is because humans actually
[2880.16s -> 2882.64s]  have high internet data disagreement
[2882.64s -> 2884.44s]  and have high variance, essentially.
[2885.56s -> 2888.64s]  Models, they will always be very consistent,
[2888.64s -> 2889.82s]  or maybe not perfectly,
[2889.82s -> 2891.64s]  like there's still some stochasticity,
[2891.64s -> 2895.20s]  but essentially they will always predict the same label,
[2895.20s -> 2896.60s]  so they have very little variance.
[2896.60s -> 2898.12s]  So here what you see on this plot
[2898.12s -> 2900.52s]  is on the X axis,
[2900.52s -> 2902.36s]  we estimated the variance,
[2902.36s -> 2903.96s]  and you see that the human has a variance
[2903.96s -> 2906.80s]  of like around 31 or 33,
[2906.80s -> 2908.44s]  while if you look at the red point,
[2908.44s -> 2911.52s]  this is basically if you just add GPT-4 to do evaluations.
[2911.52s -> 2914.20s]  So even though the bias is still pretty high,
[2914.20s -> 2917.12s]  so bias by definition for humans is zero,
[2917.12s -> 2920.04s]  for GPT-4 it is like around 32%,
[2920.04s -> 2924.02s]  and the variance is much lower than humans.
[2924.02s -> 2926.32s]  So this is why you can see that actually
[2926.32s -> 2928.04s]  sometimes agreement is higher,
[2928.04s -> 2930.16s]  but that's really because there's no variance,
[2930.16s -> 2932.72s]  or very little variance in LLMs.
[2934.80s -> 2936.24s]  Yeah, does that make sense?
[2939.04s -> 2939.88s]  Yeah.
[2939.88s -> 2940.72s]  It means that the internal covariance
[2940.72s -> 2942.08s]  of LLMs is higher than that human.
[2942.08s -> 2942.92s]  Sorry?
[2942.92s -> 2943.76s]  It means that the internal covariance
[2943.76s -> 2945.28s]  of LLMs is higher than that human.
[2945.28s -> 2946.12s]  Exactly.
[2946.12s -> 2947.92s]  So, which is actually a good sign,
[2947.92s -> 2951.88s]  because that makes it much easier for research.
[2951.88s -> 2954.08s]  The bad sign is that the bias is still high.
[2956.60s -> 2958.80s]  Okay, so things to be careful with,
[2958.80s -> 2959.64s]  when you work,
[2959.64s -> 2962.72s]  I mean this is both with humans and with LLMs.
[2963.56s -> 2965.08s]  There will be some spurious correlations.
[2965.08s -> 2966.80s]  So we already talked about spurious correlations,
[2966.80s -> 2969.16s]  but you will see a lot of those.
[2969.16s -> 2971.92s]  One very common example is length.
[2971.92s -> 2974.08s]  So if you just, as I told you before,
[2974.08s -> 2977.36s]  if you ask crowd workers which examples they preferred,
[2977.36s -> 2979.76s]  they are highly biased towards longer outputs.
[2979.76s -> 2981.34s]  So here, the blue is humans.
[2981.34s -> 2985.28s]  It's around, I think, 70% preferences for longer outputs.
[2985.28s -> 2988.32s]  And models are around the same bias.
[2989.50s -> 2991.40s]  And another example is preference for lists.
[2991.40s -> 2994.30s]  So usually if you see lists in an output,
[2994.30s -> 2996.26s]  models prefer these examples,
[2996.26s -> 2999.42s]  and model and humans prefer these examples.
[2999.42s -> 3002.78s]  Another bias or spurious correlation is a position.
[3002.78s -> 3004.92s]  I told you which one do you put on the left,
[3004.92s -> 3005.90s]  which one do you put on the right,
[3005.90s -> 3008.70s]  when you ask humans to label.
[3008.70s -> 3010.22s]  There's the same thing with models.
[3010.22s -> 3011.92s]  But this is usually pretty easy to control for.
[3011.92s -> 3013.34s]  You just randomize both.
[3013.34s -> 3016.10s]  Another issue is GPT for self bias.
[3016.10s -> 3018.54s]  So very naturally, you might wonder,
[3018.54s -> 3021.58s]  if I ask GPT to evaluate itself,
[3021.58s -> 3023.42s]  like it will probably bias,
[3023.42s -> 3025.76s]  and it will prefer itself than other models.
[3025.76s -> 3026.98s]  And this is true.
[3027.82s -> 3029.30s]  But less than what you might think.
[3029.30s -> 3030.94s]  I will tell you about it later.
[3030.94s -> 3033.02s]  Okay, so alpaca eval.
[3033.02s -> 3033.86s]  Should I?
[3035.14s -> 3036.78s]  Wait, until what time do I have?
[3040.22s -> 3041.74s]  Oh, thanks.
[3041.74s -> 3042.58s]  Great.
[3043.48s -> 3044.70s]  Okay, alpaca eval.
[3044.70s -> 3046.90s]  So alpaca eval is the benchmark that we developed
[3046.90s -> 3048.82s]  when we were working on alpaca.
[3050.08s -> 3052.40s]  So as I told you before,
[3052.40s -> 3053.60s]  one thing which is very important
[3053.60s -> 3055.66s]  is what you use for development.
[3056.60s -> 3058.24s]  So basically, for hyperparameter tuning.
[3058.24s -> 3061.26s]  So what we did is that we basically did not trust
[3061.26s -> 3063.52s]  many of the benchmarks out there at this point
[3063.52s -> 3064.62s]  for instruction following,
[3064.62s -> 3068.08s]  so we just developed a very small benchmark for ourselves,
[3068.08s -> 3070.38s]  and this is what we were doing for hyperparameter tuning.
[3070.38s -> 3072.48s]  And then it kind of became its own thing.
[3073.64s -> 3075.84s]  So alpaca eval, in a few numbers,
[3075.84s -> 3078.36s]  it has very high correlation with chatbot arena.
[3078.36s -> 3080.78s]  So the ranking, if you look at the correlation
[3080.78s -> 3084.28s]  between the ranking in chatbot arena and in alpaca eval,
[3084.28s -> 3086.14s]  it's 98%, so very high,
[3086.14s -> 3090.08s]  and it takes around three minutes and $10 to evaluate.
[3091.26s -> 3093.26s]  And the way it works, I think I already mentioned it,
[3093.26s -> 3095.18s]  but basically you take an instruction,
[3095.18s -> 3098.34s]  you generate an output from one model,
[3098.34s -> 3101.66s]  and then from another model that you're comparing it to,
[3101.66s -> 3104.38s]  and you ask GPT-4 to basically give the probability
[3104.38s -> 3107.68s]  that it prefers the model that you're evaluating
[3107.68s -> 3109.98s]  versus the baseline that you're comparing to.
[3111.26s -> 3112.94s]  And then you do some re-weighting,
[3112.94s -> 3114.70s]  and the reason why you do some re-weighting
[3114.70s -> 3117.30s]  is because these models, as I said,
[3117.30s -> 3119.78s]  are very biased towards longer outputs,
[3119.78s -> 3124.28s]  so you wanna re-weight such that if it's a longer output,
[3124.28s -> 3128.12s]  you give it a slightly less high preference,
[3128.12s -> 3130.62s]  and then you average across the entire data set
[3130.62s -> 3132.42s]  and you get a read rate.
[3132.42s -> 3133.68s]  So that's how it works.
[3135.56s -> 3136.40s]  Any questions?
[3138.84s -> 3139.68s]  Cool.
[3140.82s -> 3142.18s]  So system-level correlation,
[3142.18s -> 3144.26s]  so here what you see on the x-axis
[3144.26s -> 3145.62s]  is basically alpaca eval,
[3145.62s -> 3147.02s]  and it's a slight transformation of it,
[3147.02s -> 3149.58s]  but essentially alpaca eval scores,
[3149.58s -> 3151.86s]  and on the y-axis is this chatbot arena,
[3151.86s -> 3153.04s]  which is the gold standard,
[3153.04s -> 3155.96s]  and you see that things are relatively highly correlated,
[3155.96s -> 3159.98s]  and on the lower plots, you see basically the correlation
[3159.98s -> 3162.10s]  between different benchmark and chatbot arena,
[3162.10s -> 3163.90s]  and you see like MTBench and alpaca eval,
[3163.90s -> 3166.66s]  which are the two ones that use LLMs for evaluations,
[3166.66s -> 3169.50s]  are relatively highly correlated with chatbot arena,
[3169.50s -> 3172.34s]  and MMLU, which is the automated one
[3172.34s -> 3175.10s]  that doesn't use an LLM is also very highly correlated.
[3177.60s -> 3179.58s]  So I told you very briefly about the fact
[3179.58s -> 3181.38s]  that we had to do some re-weighting,
[3181.38s -> 3183.02s]  so I'm not gonna tell you how we do it,
[3183.02s -> 3184.92s]  but I wanna tell you why we do it.
[3186.06s -> 3188.46s]  One of the issues that we realized
[3189.78s -> 3193.06s]  a little bit too late is that if you take
[3193.06s -> 3196.26s]  something like GPT-4 and you just ask it,
[3196.26s -> 3198.50s]  you prompt it to be much more detailed,
[3198.50s -> 3201.30s]  to basically provide much more detailed answers,
[3201.30s -> 3204.30s]  its win rate, so its performance on your benchmark,
[3204.30s -> 3209.00s]  goes from 50% to 64.3, so that's this one, 64.3.
[3210.38s -> 3212.18s]  If you ask it to be more concise,
[3212.18s -> 3215.22s]  it decreases to 22.9, and that really doesn't fit
[3215.22s -> 3217.94s]  our mental model of what benchmarks should be doing.
[3217.94s -> 3221.02s]  If I just tweak a little bit the prompt,
[3221.02s -> 3224.34s]  I don't want my model to change completely its ranking.
[3225.60s -> 3227.62s]  So that's why we have to do some re-weighting,
[3227.62s -> 3229.66s]  and you see that after the re-weighting,
[3229.66s -> 3234.62s]  you basically have that the performance
[3234.62s -> 3237.14s]  after you ask the model to be more verbose
[3237.14s -> 3239.34s]  is very close to the performance
[3239.34s -> 3242.00s]  without any prompt tuning.
[3246.90s -> 3250.10s]  So I told you very briefly before about self-bias.
[3250.10s -> 3251.98s]  I do wanna say that I'm pretty surprised
[3251.98s -> 3255.50s]  about this result, but actually, self-bias exists,
[3255.50s -> 3259.10s]  but it's not as high as you might think.
[3259.10s -> 3262.26s]  So here you see on the x-axis, the ranking,
[3263.36s -> 3266.06s]  or like the different models that you're evaluating,
[3266.06s -> 3268.46s]  and on the, sorry, that's on the rows,
[3268.46s -> 3271.54s]  and on the columns, you see who is evaluating,
[3271.54s -> 3273.34s]  which model are you using for evaluation.
[3273.34s -> 3275.70s]  And you actually see that regardless of the model
[3275.70s -> 3279.82s]  that you evaluate with, the ranking will be the same.
[3279.82s -> 3283.78s]  So even though it's true that if I look at
[3284.76s -> 3287.98s]  miss role, evaluate by miss role,
[3287.98s -> 3290.98s]  it gives itself a much higher accuracy.
[3290.98s -> 3295.02s]  It still prefers Claude and GPT-4.
[3295.02s -> 3297.02s]  So it's not as bad as what you may think.
[3297.02s -> 3298.22s]  It's still bad, though.
[3300.58s -> 3301.42s]  Cool.
[3302.34s -> 3304.54s]  Okay, so that leads me to talking
[3304.54s -> 3306.82s]  about current evaluation of LLMs.
[3306.82s -> 3308.94s]  So I would say there are three main ways
[3308.94s -> 3311.12s]  that people currently evaluate LLMs.
[3311.12s -> 3313.38s]  The first one is perplexity,
[3313.90s -> 3315.26s]  which is essentially just looking at training losses
[3315.26s -> 3316.50s]  or validation losses.
[3317.56s -> 3320.02s]  The second one is basically averaging everything,
[3321.22s -> 3323.30s]  which is actually surprisingly more common
[3323.30s -> 3324.48s]  than what you may think.
[3324.48s -> 3327.30s]  And the third one is this arena-like,
[3327.30s -> 3330.18s]  or where you basically have comparisons between models
[3330.18s -> 3334.10s]  and either use humans or use models to do the evaluation.
[3334.10s -> 3336.78s]  And usually how it works is that pre-trained model,
[3336.78s -> 3339.32s]  let's say the new, like when LLM4 comes out
[3339.32s -> 3340.94s]  or like when GPT-5 comes out,
[3340.94s -> 3343.34s]  they basically mostly show perplexity
[3344.18s -> 3345.06s]  and average over everything.
[3345.06s -> 3347.74s]  And the fine-tuned models, they usually tend to show
[3347.74s -> 3349.02s]  average over everything
[3349.02s -> 3353.38s]  and performance under arena-like models.
[3353.38s -> 3358.06s]  And the reason why is because models that are fine-tuned,
[3358.06s -> 3360.70s]  usually the log likelihood that they predict
[3360.70s -> 3365.70s]  is not calibrated for your dataset.
[3367.82s -> 3370.58s]  So what do I mean by everything?
[3370.58s -> 3373.78s]  I would say the two most common benchmarks
[3373.78s -> 3376.86s]  that basically look at everything are Helm
[3376.86s -> 3379.62s]  and Hugging Face Open-LML Leaderboard.
[3379.62s -> 3381.68s]  It's really just a collection of a lot
[3381.68s -> 3384.50s]  of different automatically-evaluated benchmarks,
[3384.50s -> 3386.64s]  and you evaluate across all of them.
[3387.74s -> 3390.90s]  So what are some of the common benchmarks that we use?
[3391.74s -> 3395.54s]  One is, yeah, measuring like math performance.
[3395.54s -> 3398.16s]  So GSM-8K, that's a pretty common one.
[3398.16s -> 3399.86s]  That's basically grade school math.
[3399.86s -> 3403.14s]  MMLU is multiple choice question answering
[3403.14s -> 3405.22s]  on like math, science, history.
[3405.22s -> 3407.86s]  Legal bench is on the legal aspect.
[3407.86s -> 3409.06s]  Then you have MedQA.
[3409.06s -> 3410.58s]  I believe this is for Helm.
[3410.58s -> 3414.70s]  MedQA is medical licensing exams.
[3414.70s -> 3416.78s]  So you basically ask many, many different questions
[3416.78s -> 3419.40s]  that you can automatically evaluate,
[3420.46s -> 3422.34s]  and you hope that by taking averages,
[3423.20s -> 3425.54s]  it will say like how well your model performs.
[3425.54s -> 3427.30s]  So that's kind of like the newer version
[3427.30s -> 3428.74s]  of SuperGLUE, I would say.
[3429.94s -> 3431.68s]  One dataset that I wanna highlight,
[3431.68s -> 3433.38s]  which is probably, or one benchmark
[3433.38s -> 3434.86s]  which is probably the most widely used
[3434.86s -> 3437.46s]  and the one that people believe the most is MMLU,
[3437.46s -> 3440.54s]  so massively multitasked language understanding.
[3440.54s -> 3444.90s]  So this is, I think maybe Archit mentioned it last week,
[3444.90s -> 3449.90s]  but this is basically multiple choice questions
[3450.18s -> 3452.34s]  on 57 different tasks.
[3452.34s -> 3454.34s]  So you have tasks like formal logic,
[3454.34s -> 3458.30s]  conceptual physics, econometrics, and these type of tasks.
[3458.30s -> 3459.46s]  So here's an example.
[3460.42s -> 3463.82s]  What is true for a type Ia supernova?
[3463.82s -> 3465.86s]  This type occurs in binary system.
[3465.86s -> 3467.76s]  This type occurs in young galaxies,
[3467.76s -> 3469.50s]  and you basically have to say which answer.
[3469.50s -> 3470.42s]  So that seems very simple.
[3470.42s -> 3471.78s]  I mean, the task is not simple,
[3471.78s -> 3473.54s]  but the way you evaluate seems simple.
[3473.54s -> 3475.14s]  And then like high school biology.
[3475.14s -> 3477.58s]  In a population of giraffes and environmental,
[3477.58s -> 3478.94s]  nah, nah, nah, and then you,
[3478.94s -> 3481.94s]  this is an example of directional selection.
[3481.94s -> 3484.68s]  So that seems simple, but actually,
[3484.68s -> 3487.38s]  it's also more complicated than what you might think.
[3489.22s -> 3491.30s]  And I think I will tell you,
[3492.74s -> 3494.90s]  okay, I will tell you about it later.
[3494.90s -> 3496.94s]  But that's one of the most common,
[3496.94s -> 3498.66s]  probably the most common benchmark
[3498.66s -> 3500.42s]  and what people actually look at.
[3500.42s -> 3503.30s]  For example, when Mark Zuckerberg
[3503.30s -> 3505.78s]  said that LabMath 3 was out,
[3505.78s -> 3508.62s]  he, yeah, he talked about MMLU scores,
[3508.62s -> 3510.94s]  which I find kind of crazy.
[3510.94s -> 3515.50s]  But yeah, other capabilities that people look at, coding.
[3515.50s -> 3516.82s]  Coding is a very common one
[3516.82s -> 3521.02s]  that people evaluate on for two different reasons.
[3521.02s -> 3524.86s]  One, because coding is usually,
[3524.86s -> 3526.00s]  if you perform well on code,
[3526.00s -> 3526.84s]  you usually actually,
[3526.84s -> 3529.02s]  these models perform well on reasoning,
[3529.02s -> 3530.92s]  which is actually pretty cool.
[3530.92s -> 3532.46s]  So that's like highly correlated
[3532.46s -> 3534.28s]  with things that people care about.
[3534.28s -> 3536.06s]  Two, I mean, a lot of us are coders,
[3536.06s -> 3541.06s]  so we like to have better models for helping us coding.
[3541.22s -> 3542.38s]  And three, the other point is that
[3542.38s -> 3544.50s]  it's actually pretty easy to evaluate
[3544.50s -> 3545.90s]  because you can write test cases.
[3545.94s -> 3547.10s]  So you basically ask the model
[3547.10s -> 3551.26s]  to generate very long code or functions to do something,
[3551.26s -> 3552.94s]  and then you just run the test
[3552.94s -> 3555.68s]  and you see whether it succeeds or not.
[3555.68s -> 3556.52s]  Yes?
[3556.52s -> 3558.22s]  Sorry, going back to the previous set of variations,
[3558.22s -> 3561.98s]  some of them was short-form answers, apparently.
[3561.98s -> 3565.58s]  How would you validate short-form QA type of thing?
[3565.58s -> 3566.58s]  Multiple choice makes sense,
[3566.58s -> 3568.22s]  but if it's short-form QA,
[3568.22s -> 3570.78s]  how would you say something is correct
[3570.78s -> 3571.98s]  as an automatic metric?
[3571.98s -> 3576.98s]  Yeah, I actually don't know.
[3584.22s -> 3585.66s]  I actually don't know.
[3585.66s -> 3587.02s]  Yeah, I should check.
[3587.02s -> 3587.86s]  Sorry.
[3588.74s -> 3590.30s]  I don't know specifically for this one,
[3590.30s -> 3595.30s]  but Hotpot QA and Gear QA are other QA data sets,
[3595.86s -> 3598.54s]  and they look at F1 for the true and false,
[3598.54s -> 3600.86s]  and then they also have an exact match,
[3600.94s -> 3601.78s]  which is pretty punitive,
[3601.78s -> 3603.74s]  because if you said President Reagan,
[3603.74s -> 3606.22s]  and the answer is President Ronald Reagan,
[3606.22s -> 3607.82s]  they're like, she'll deem you,
[3607.82s -> 3610.22s]  but anyway, so they use an exact match on that.
[3612.54s -> 3613.38s]  Yeah.
[3614.74s -> 3616.38s]  Cool, thanks.
[3616.38s -> 3618.26s]  Okay, so MLU coding.
[3618.26s -> 3620.58s]  Another one that people started looking at are agents.
[3620.58s -> 3622.90s]  I think Shekhar is gonna give a lecture on it,
[3622.90s -> 3624.30s]  so I'm not gonna talk too much about it,
[3624.30s -> 3626.62s]  but one cool thing that LMs can do right now
[3626.62s -> 3629.82s]  is basically call APIs and then take actions
[3629.82s -> 3631.30s]  in the real world, essentially,
[3631.30s -> 3633.86s]  or take control of your computer.
[3633.86s -> 3636.86s]  You should not give it control to your computer.
[3636.86s -> 3638.82s]  So an actual question is,
[3638.82s -> 3640.50s]  how do you evaluate these type of things?
[3640.50s -> 3642.10s]  This is a real challenge,
[3643.30s -> 3646.70s]  because the biggest challenge is that,
[3646.70s -> 3648.62s]  for example, if I really wanted to evaluate
[3648.62s -> 3649.70s]  how good it is at coding,
[3649.70s -> 3651.98s]  or how good it is at doing things in my terminal,
[3651.98s -> 3653.50s]  I need to give it access to my terminal,
[3653.50s -> 3656.94s]  and I really don't wanna give my NLM access to my terminal,
[3657.78s -> 3660.34s]  so you really need to sandbox environments.
[3660.34s -> 3661.86s]  For the specific cases of terminal,
[3661.86s -> 3663.52s]  I mean, it's pretty easy to sandbox,
[3663.52s -> 3665.54s]  but once you wanna do evaluation
[3665.54s -> 3668.58s]  of a model that pinks people on Slack,
[3668.58s -> 3670.00s]  or writes things in your emails,
[3670.00s -> 3673.98s]  then you have to write an entire sandbox environment
[3673.98s -> 3675.70s]  for all the applications that you want
[3675.70s -> 3677.70s]  your LMs to have access to.
[3677.70s -> 3680.10s]  So this is actually really complicated,
[3680.10s -> 3682.02s]  and something that people really have to deal with
[3682.02s -> 3683.62s]  in kind of the real world.
[3684.62s -> 3686.50s]  Or at least will have to,
[3686.50s -> 3689.62s]  because right now it's still not in production.
[3689.62s -> 3692.94s]  Okay, the last part is, or the penultimate one,
[3692.94s -> 3694.22s]  perplexities.
[3694.22s -> 3697.66s]  So one thing which is very surprising,
[3697.66s -> 3699.28s]  or at least the first time you see it,
[3699.28s -> 3701.02s]  is that really the performance that you have
[3701.02s -> 3704.68s]  on pre-training is extremely highly correlated
[3704.68s -> 3707.78s]  with basically performance on any downstream task,
[3707.78s -> 3709.74s]  at least for the current types of LMs.
[3709.74s -> 3710.58s]  So what I mean by this
[3710.58s -> 3713.14s]  is that if you just look at your training performance,
[3713.62s -> 3714.78s]  just predicting the next word,
[3714.78s -> 3716.62s]  it's extremely highly correlated.
[3716.62s -> 3719.58s]  So this is the x-axis, which is essentially perplexities,
[3719.58s -> 3721.98s]  and the y-axis, which is just the average
[3721.98s -> 3723.62s]  over many different tasks.
[3723.62s -> 3726.14s]  What you will see is that tasks that perform well
[3726.14s -> 3731.14s]  on perplexities will actually have high average scores.
[3732.70s -> 3736.98s]  And as a result, a lot of people actually end up,
[3736.98s -> 3739.26s]  when they develop, just looking at perplexities,
[3739.26s -> 3740.40s]  and they just trust it enough
[3740.44s -> 3743.40s]  that they don't need to do the downstream evaluations.
[3743.40s -> 3745.12s]  I would not recommend doing it,
[3745.12s -> 3747.32s]  but if you have to have something quick and dirty,
[3747.32s -> 3749.44s]  it usually works pretty well.
[3749.44s -> 3750.92s]  One thing to be careful with, though,
[3750.92s -> 3753.36s]  is that the perplexities are not gonna be comparable
[3753.36s -> 3755.16s]  across different data sets.
[3755.16s -> 3756.52s]  So you really have to be careful
[3756.52s -> 3758.64s]  with what perplexities you're looking at.
[3758.64s -> 3761.56s]  And two, it will depend on the tokenizer.
[3761.56s -> 3766.56s]  So if you have like LMA3 and you compare it to Gemini,
[3766.60s -> 3770.24s]  even on the same data set,
[3770.24s -> 3773.52s]  it's gonna give different scores, and it's not comparable.
[3773.52s -> 3774.36s]  Yes?
[3774.36s -> 3775.18s]  Why is that?
[3775.18s -> 3776.02s]  It's a little better.
[3776.02s -> 3779.44s]  Oh, the easy answer, I mean, it's not the only answer,
[3779.44s -> 3781.96s]  but the easy answer is that if the vocabulary changes,
[3781.96s -> 3784.62s]  the size of the vocabulary changes,
[3784.62s -> 3786.76s]  then clearly the type of,
[3786.76s -> 3788.04s]  I mean, everything is not on,
[3788.04s -> 3789.84s]  like the upper bound is different.
[3789.84s -> 3793.20s]  What's the sequence like normalized?
[3793.20s -> 3794.72s]  What's the sequence like normalized?
[3794.72s -> 3796.52s]  Yeah, but I'm not talking about that.
[3797.36s -> 3798.28s]  I'm talking about the fact that,
[3798.28s -> 3799.10s]  I mean, just think about it
[3799.10s -> 3801.30s]  if you have a vocabulary size of one.
[3801.30s -> 3802.14s]  Oh, okay.
[3802.14s -> 3804.32s]  Then I have to always predict the same thing.
[3804.32s -> 3809.12s]  So like, basically your entropy depends,
[3809.12s -> 3810.52s]  your entropy is upper bounded by log
[3810.52s -> 3812.78s]  of like the cardinality of your vocabulary size.
[3812.78s -> 3814.38s]  So you're gonna depend on that.
[3815.92s -> 3816.76s]  Cool.
[3818.60s -> 3820.16s]  And the last one is arena-like.
[3820.16s -> 3821.16s]  As I already told you,
[3821.16s -> 3823.54s]  basically you compare different models,
[3823.54s -> 3825.08s]  you make them fight essentially against each other,
[3825.08s -> 3827.44s]  and you have ELO ratings at the end.
[3827.44s -> 3829.84s]  So that's really, a more general way of saying it
[3829.84s -> 3831.84s]  is I really just let the users decide.
[3832.88s -> 3834.48s]  And that works also pretty well.
[3835.54s -> 3840.54s]  Okay, issues and challenges with current evaluations.
[3840.64s -> 3842.44s]  First, consistency issues.
[3843.52s -> 3845.12s]  If you look at question answering,
[3845.12s -> 3847.18s]  sorry, multiple choice questions,
[3848.12s -> 3849.28s]  if you just change,
[3849.28s -> 3851.60s]  so you see on the top left and top right,
[3851.60s -> 3855.56s]  if you just change A, B, C, D to like random symbols,
[3855.56s -> 3857.28s]  the generations that you will give
[3857.28s -> 3858.42s]  are actually gonna be different.
[3858.42s -> 3859.88s]  And then the rankings
[3859.88s -> 3862.48s]  between different models will be different.
[3862.48s -> 3864.00s]  So even things that are very simple,
[3864.00s -> 3865.72s]  like multiple choice,
[3865.72s -> 3869.02s]  like selecting out of four choices,
[3869.02s -> 3871.24s]  will be very dependent on exactly
[3871.24s -> 3874.60s]  like how you format these choices.
[3874.60s -> 3876.84s]  And one real example,
[3876.84s -> 3879.36s]  that's what I was alluding to before, is MMLU.
[3879.76s -> 3882.26s]  MMLU seems really simple to evaluate.
[3882.26s -> 3886.92s]  You just ask it to say which one of the four
[3886.92s -> 3888.04s]  the model prefers.
[3889.12s -> 3891.28s]  But actually, for a very long time,
[3891.28s -> 3893.76s]  I think for nearly one year,
[3893.76s -> 3897.14s]  there were three main implementation of MMLU,
[3897.14s -> 3899.16s]  and people were comparing between those three,
[3899.16s -> 3902.72s]  having no idea that those two gave different scores.
[3902.72s -> 3905.12s]  And the two main differences were,
[3905.12s -> 3906.72s]  one, people use different prompts,
[3906.72s -> 3908.56s]  so that clearly will give different answers,
[3908.56s -> 3912.62s]  but two, they were using different ways of sampling
[3912.62s -> 3916.24s]  to get the actual most likely prediction.
[3916.24s -> 3917.46s]  So one of them, for example,
[3917.46s -> 3921.04s]  was saying I have the four choices,
[3921.04s -> 3922.90s]  now to get my most likely,
[3922.90s -> 3924.94s]  let's say that the correct answer is D,
[3925.88s -> 3930.62s]  I will just look at the most likely answers out of A, B, C, D.
[3930.62s -> 3934.16s]  Even though like Zygote was another answer
[3934.16s -> 3936.46s]  that has a higher likelihood,
[3936.46s -> 3937.40s]  I will not look at it
[3937.44s -> 3939.94s]  because I will basically do constraint decoding.
[3941.28s -> 3942.80s]  And if I do constraint decoding here,
[3942.80s -> 3945.50s]  I will say that the correct answer is D.
[3945.50s -> 3949.04s]  But if I actually just look at the most likely token,
[3949.04s -> 3950.54s]  I will not get the correct answer.
[3950.54s -> 3953.32s]  So those were two different implementation,
[3953.32s -> 3955.08s]  and a third different implementation,
[3955.08s -> 3957.04s]  which seems really different,
[3957.04s -> 3960.28s]  is that instead of generating the correct token,
[3960.28s -> 3962.72s]  which is basically the letter A, B, C, D,
[3962.72s -> 3966.44s]  you can look at, after this question,
[3966.44s -> 3970.78s]  what is the likelihood that the model would generate this?
[3970.78s -> 3973.44s]  So you would look at the log likelihood,
[3973.44s -> 3974.64s]  or like the perplexity, essentially,
[3974.64s -> 3975.96s]  of predicting this thing,
[3975.96s -> 3977.44s]  log likelihood of predicting that.
[3977.44s -> 3979.60s]  And that gives very different answers.
[3979.60s -> 3981.40s]  So if you look at the top right,
[3981.40s -> 3983.60s]  you see that LAMAS 65B,
[3983.60s -> 3986.70s]  MMLU on Helm was 63.7,
[3986.70s -> 3989.16s]  and the original MMLU was 63.6,
[3989.16s -> 3991.00s]  but on Harness,
[3991.00s -> 3994.50s]  which is the thing that actually Hugging Face uses,
[3994.50s -> 3995.48s]  is 48.8.
[3995.52s -> 3997.32s]  So that's like a huge difference.
[3997.32s -> 3998.16s]  Yeah?
[3998.16s -> 3999.46s]  What is Helm, Harness, and original,
[3999.46s -> 4001.88s]  it matches to these three things there?
[4001.88s -> 4004.36s]  Yeah, I can't remember which one does what,
[4004.36s -> 4006.44s]  but each of them does something different.
[4006.44s -> 4008.00s]  Actually, now it's not true anymore,
[4008.00s -> 4011.04s]  so the middle column changed what they are doing,
[4011.04s -> 4013.64s]  so they start matching the other two ones.
[4013.64s -> 4015.28s]  But at that time, they weren't.
[4016.28s -> 4017.72s]  I'm not sure which one.
[4017.72s -> 4022.72s]  My guess would be that they did the last one,
[4023.08s -> 4024.44s]  but I'm not entirely sure.
[4025.48s -> 4028.84s]  Okay, questions?
[4031.28s -> 4032.60s]  Cool.
[4032.60s -> 4034.18s]  Another issue, contamination.
[4035.18s -> 4037.56s]  So here you have Harness here.
[4037.56s -> 4039.80s]  If you don't follow him on Twitter, you should.
[4040.96s -> 4043.80s]  And he basically said that he was looking at
[4043.80s -> 4045.68s]  like code benchmarks,
[4045.68s -> 4049.92s]  and he was saying that pre-2021,
[4049.92s -> 4050.88s]  I can't remember which one,
[4050.88s -> 4053.36s]  or GPT-4, was getting 10 out of 10
[4053.36s -> 4054.90s]  on questions on Codeforce,
[4055.38s -> 4057.66s]  but after 2021, or like more recent problems,
[4057.66s -> 4059.10s]  it was getting zero out of 10,
[4059.10s -> 4062.38s]  which seems very, very strange.
[4062.38s -> 4063.90s]  So that really strongly points to the fact
[4063.90s -> 4064.82s]  that it was contaminated,
[4064.82s -> 4067.48s]  and the model was probably pre-trained,
[4067.48s -> 4069.78s]  or like that dataset, or the Codeforce dataset,
[4069.78s -> 4071.58s]  was probably in the pre-training dataset.
[4071.58s -> 4072.70s]  And of course, if you,
[4072.70s -> 4075.86s]  I mean, essentially you do training on your test set,
[4075.86s -> 4078.46s]  then you're gonna perform really well.
[4078.46s -> 4081.74s]  And Suzanne said also to follow,
[4081.74s -> 4085.62s]  also said something similar for PHY 1.5,
[4086.48s -> 4088.14s]  which is a model from Microsoft.
[4089.26s -> 4092.74s]  So what is challenging here is that with closed models,
[4092.74s -> 4094.60s]  I mean, there are two things actually that are challenging.
[4094.60s -> 4097.30s]  One is that those are pre-trained on so much data
[4097.30s -> 4098.86s]  that even if we had access to the data,
[4098.86s -> 4100.46s]  it would be hard to actually know what they,
[4100.46s -> 4102.78s]  like if they were pre-trained on your test set.
[4102.78s -> 4105.14s]  But two, those are all closed source models,
[4105.14s -> 4107.76s]  so you really don't even have access to a dataset,
[4107.76s -> 4110.26s]  so you have no idea if they were pre-trained on that data.
[4111.96s -> 4115.28s]  Overfitting issues, that's also relatively related,
[4115.28s -> 4117.18s]  but could be slightly different.
[4117.18s -> 4120.28s]  So here you see how much time it took
[4120.28s -> 4124.52s]  for standard datasets to achieve, quote,
[4124.52s -> 4127.58s]  like, square quotes, human level performance.
[4127.58s -> 4129.84s]  And what you see is that on the recent ones,
[4129.84s -> 4131.82s]  where you really have this pre-training,
[4131.82s -> 4133.62s]  in less than like six months,
[4133.62s -> 4136.98s]  you perform at human level performance.
[4136.98s -> 4140.30s]  We don't really know if it's because of the contamination,
[4140.30s -> 4142.14s]  or if it's simply that like a lot of people
[4142.14s -> 4144.90s]  are basically developing and trying to do
[4144.90s -> 4147.26s]  hyper-parameter tuning on these test sets.
[4147.26s -> 4149.06s]  We don't know why, but it's clearly an issue
[4149.06s -> 4150.32s]  with overfitting.
[4152.14s -> 4153.40s]  So how do you alleviate that?
[4153.40s -> 4155.78s]  One, you can have private test sets.
[4155.78s -> 4158.54s]  So there's a paper from I think two weeks ago
[4158.54s -> 4161.88s]  that presented GSM 1K,
[4161.88s -> 4163.82s]  which is the same thing as the GSM 8K
[4163.82s -> 4166.78s]  that we saw before, which is the math dataset,
[4166.78s -> 4170.22s]  but tries to basically regenerate
[4171.10s -> 4173.58s]  or resample this dataset, or recollect this dataset,
[4173.58s -> 4176.78s]  and then they look at how well different models
[4177.80s -> 4180.18s]  perform on both the GSM 1K and the GSM 8K.
[4180.18s -> 4182.38s]  And what you see is that at least the open-source models,
[4182.38s -> 4185.26s]  they perform much worse on a new dataset
[4185.26s -> 4188.54s]  than on the one that people are able to tune on.
[4189.62s -> 4192.94s]  This is not true though for like cloud and GPT-4.
[4193.86s -> 4197.48s]  Another one is Dynabench, or just dynamic test sets.
[4197.48s -> 4199.90s]  So ideally, every X number of days,
[4200.44s -> 4201.86s]  you would basically have new instructions
[4201.86s -> 4203.42s]  or new inputs to the models,
[4203.42s -> 4205.82s]  and your dataset would basically be dynamic.
[4205.82s -> 4208.42s]  That's essentially also what Chatbot Arena does.
[4208.42s -> 4210.50s]  So that definitely helps.
[4212.46s -> 4214.74s]  Another way of alleviating contaminators
[4214.74s -> 4217.22s]  is that you may try to estimate
[4217.22s -> 4219.58s]  or like to look at whether the models
[4219.58s -> 4221.74s]  were actually trained on your test set.
[4221.74s -> 4223.50s]  So one very simple way of doing it,
[4223.50s -> 4226.78s]  which actually works I think relatively well,
[4226.78s -> 4230.18s]  is just looking at the probability of different answers,
[4230.18s -> 4232.66s]  and you will see that if your model
[4232.66s -> 4234.58s]  is really sure about a certain answer,
[4234.58s -> 4237.18s]  then probably it was trained on that answer.
[4237.18s -> 4240.64s]  Another one, which is also really cool,
[4240.64s -> 4244.24s]  is looking at the order of your test set.
[4244.24s -> 4248.30s]  So if your model was trained or pre-trained
[4248.30s -> 4250.52s]  on the test set, then most likely,
[4250.52s -> 4253.18s]  it thinks that example two comes after example one.
[4253.18s -> 4255.54s]  So if you switch example one and example two,
[4255.54s -> 4258.02s]  and you see drops in log likelihoods,
[4258.02s -> 4259.58s]  then most likely the model was actually
[4259.58s -> 4261.08s]  pre-trained on that dataset.
[4266.06s -> 4267.26s]  Any questions here?
[4270.06s -> 4273.58s]  Okay, so another issue is that,
[4273.58s -> 4277.34s]  I mean really there's a monoculture of NLP benchmarking.
[4277.34s -> 4278.62s]  What I mean by this is mostly
[4278.62s -> 4280.78s]  the fact that we all just look at English.
[4281.70s -> 4285.30s]  And this is a paper from 2021, or 2022 I think,
[4285.94s -> 4286.82s]  but they look at ACL 2021,
[4286.82s -> 4291.82s]  which is probably the most common conference in NLP,
[4294.42s -> 4297.26s]  and they look at the best papers, so the oral papers,
[4297.26s -> 4300.58s]  and they saw that out of the 461 papers,
[4300.58s -> 4302.82s]  70% of them only look at English,
[4302.82s -> 4304.82s]  and 40% of them only look at accuracy,
[4304.82s -> 4306.42s]  so essentially just performance.
[4307.38s -> 4309.96s]  So there are very few papers that look at multi-linguality
[4309.96s -> 4313.92s]  and even efficiency on interpretability or fairness.
[4314.88s -> 4318.80s]  And there's a similar paper that analyzes
[4318.80s -> 4321.08s]  another conference in 2008,
[4321.08s -> 4322.36s]  and it was essentially the same findings.
[4322.36s -> 4325.06s]  So unfortunately it doesn't seem to improve over time.
[4327.56s -> 4330.46s]  The thing is, there are actually a lot of benchmarks
[4330.46s -> 4332.40s]  for multi-linguality.
[4332.40s -> 4336.64s]  I just hired a few here, Mega, Global, Bench, Extreme.
[4337.48s -> 4341.40s]  Those have at least 30, 40 languages,
[4341.40s -> 4343.88s]  and many, many different tasks.
[4344.72s -> 4346.52s]  So it's not that we don't have the benchmarks.
[4346.52s -> 4350.16s]  It's that there's no incentives, unfortunately,
[4350.16s -> 4351.72s]  in academia to actually train,
[4351.72s -> 4354.80s]  or sorry, to evaluate on those benchmarks.
[4355.70s -> 4359.36s]  So if you have the chance, use those benchmarks.
[4361.36s -> 4364.00s]  Another issue is that really we reduce
[4364.00s -> 4365.40s]  everything to a single metric.
[4365.40s -> 4368.20s]  So I already told you before, the way we aggregate metrics,
[4368.20s -> 4369.94s]  this is usually kind of broken
[4369.94s -> 4372.78s]  in some of these super benchmarks.
[4372.78s -> 4374.62s]  But also we only look at performance.
[4374.62s -> 4376.58s]  And in the real world, we really care
[4376.58s -> 4378.36s]  about computational efficiency too.
[4378.36s -> 4379.68s]  We also care about biases,
[4379.68s -> 4381.62s]  and we care about many other aspects,
[4381.62s -> 4385.00s]  and most of these benchmarks don't consider those.
[4385.00s -> 4387.10s]  Another part is that we usually average
[4387.10s -> 4388.74s]  across every example.
[4388.74s -> 4390.58s]  We just say that every example has the same value,
[4390.58s -> 4392.42s]  essentially the same weight.
[4392.42s -> 4395.14s]  So this is definitely unfair for minoritized groups.
[4395.14s -> 4398.30s]  But more than this, I think if,
[4398.30s -> 4400.94s]  for example, if you think about agents
[4400.98s -> 4404.26s]  where maybe one example will be how well it performs
[4404.26s -> 4409.26s]  on writing code that will actually be put in production
[4409.34s -> 4414.50s]  versus just answering your daily question
[4414.50s -> 4417.82s]  about where to buy the best burger,
[4419.42s -> 4421.50s]  the value that you will get out of these examples
[4421.50s -> 4422.34s]  are very different.
[4422.34s -> 4424.26s]  And right now, when we evaluate stuff,
[4424.26s -> 4425.30s]  we don't actually consider that.
[4425.30s -> 4427.16s]  So that's, I think, a real issue.
[4428.12s -> 4430.70s]  And also, basically, we don't take into account
[4431.34s -> 4432.86s]  that different people have different preferences.
[4433.82s -> 4435.76s]  So a few shout-outs.
[4435.76s -> 4438.08s]  One, considering computational efficiency.
[4438.08s -> 4440.26s]  So MLPerf has a great benchmark.
[4440.26s -> 4442.26s]  When basically, instead of trying to maximize
[4442.26s -> 4445.26s]  the performance on a certain benchmark,
[4445.26s -> 4448.26s]  they say, I want to achieve that performance
[4448.26s -> 4449.82s]  in the least amount of time.
[4450.86s -> 4453.90s]  So now you basically consider both accuracies
[4453.90s -> 4457.24s]  and speed, either for training or for inference.
[4458.24s -> 4462.24s]  For biases, DiscreamEval is a good data set from Entropic
[4464.28s -> 4466.84s]  where basically they have some templates.
[4468.16s -> 4469.92s]  So they try to ask questions like
[4470.84s -> 4474.10s]  knowing whether someone should keep their insurance or not,
[4474.10s -> 4476.20s]  and they have templates where they change
[4477.28s -> 4478.96s]  like the race or like the gender
[4478.96s -> 4481.56s]  in the template of the person,
[4481.56s -> 4483.88s]  and they see how the decisions
[4483.88s -> 4485.48s]  made by the model would change.
[4486.48s -> 4490.80s]  And, I mean, unfortunately but unsurprisingly,
[4490.80s -> 4492.56s]  you will see that some groups
[4492.56s -> 4495.00s]  are much more discriminated than others.
[4498.52s -> 4500.04s]  Other biases in our evaluation.
[4500.04s -> 4503.52s]  So I already told you slightly about multilingual issues,
[4503.52s -> 4505.68s]  but honestly, this issue about English
[4505.68s -> 4507.80s]  is much more prevalent than you would think.
[4507.80s -> 4510.00s]  For example, Bleu and Rusco,
[4510.00s -> 4513.28s]  they really assume that you basically have access to words.
[4513.44s -> 4515.70s]  You know how to tokenize and how to get words.
[4515.70s -> 4519.04s]  So I used to work with Thai and Vietnamese.
[4519.04s -> 4521.08s]  With Vietnamese, you have spaces in between words,
[4521.08s -> 4523.80s]  and Thai, you have no spaces between words.
[4523.80s -> 4526.52s]  You have no idea how to run like Bleu, Rusco.
[4526.52s -> 4529.24s]  Really, it's much more than just the data.
[4529.24s -> 4532.64s]  All our algorithms are really focused on English
[4532.64s -> 4534.92s]  or at least Western languages.
[4534.92s -> 4538.28s]  Biased LLM-based evaluations.
[4538.28s -> 4540.14s]  So one thing that I told you about
[4540.14s -> 4540.98s]  is that it's really cool
[4540.98s -> 4542.68s]  because now you can use essentially GPT-4
[4543.12s -> 4544.88s]  for doing labeling,
[4546.30s -> 4547.92s]  but that also means that,
[4547.92s -> 4549.92s]  given that GPT-4 is very consistent,
[4549.92s -> 4551.60s]  if it has some biases,
[4551.60s -> 4555.20s]  then most of essentially the NLP community
[4555.20s -> 4558.78s]  will have these biases scaled up essentially.
[4560.14s -> 4563.54s]  So one benchmark which tries to look at
[4563.54s -> 4567.50s]  whose opinions LLMs reflect by default.
[4567.50s -> 4570.12s]  This is actually pretty cool work that looks at
[4570.12s -> 4573.72s]  the output distribution of LLMs
[4573.72s -> 4575.36s]  on public opinion surveys.
[4575.36s -> 4578.68s]  So just trying to understand whether LLMs
[4580.68s -> 4583.36s]  reflect opinions from which groups.
[4583.36s -> 4585.84s]  And they find that at least after,
[4585.84s -> 4587.64s]  when you only do pre-training,
[4587.64s -> 4589.96s]  the models actually relatively well,
[4592.20s -> 4595.00s]  they are not too optimized to a single group.
[4595.00s -> 4596.60s]  But after, so this is in red,
[4596.60s -> 4598.22s]  but after fine-tuning,
[4598.22s -> 4600.12s]  you basically see that the models really
[4600.12s -> 4602.62s]  start being optimized for certain preferences,
[4602.62s -> 4603.46s]  which is unsurprising
[4603.46s -> 4605.88s]  because that's how we actually train the model.
[4605.88s -> 4609.30s]  And typically, these models actually
[4611.86s -> 4615.98s]  mostly show preferences from,
[4616.86s -> 4619.66s]  or actually the answer as if they were from,
[4619.66s -> 4622.62s]  I mean, white and Southeast Asian.
[4622.62s -> 4623.66s]  So I think the Southeast Asian
[4623.66s -> 4624.58s]  is actually pretty interesting.
[4624.58s -> 4626.50s]  I think it's probably because a lot of these models
[4626.50s -> 4629.98s]  were the human data that was used
[4629.98s -> 4632.58s]  for supervised fine-tuning and for LHF
[4632.58s -> 4636.66s]  was actually labeled by people in Southeast Asia,
[4636.66s -> 4638.48s]  which would explain why these models
[4638.48s -> 4640.82s]  have these type of views.
[4641.96s -> 4644.14s]  And usually also highly educated.
[4646.06s -> 4649.50s]  Okay, so this is the main challenge,
[4649.50s -> 4652.08s]  the challenges of all challenges.
[4652.08s -> 4654.74s]  We saw that there are many challenges in evaluation,
[4654.74s -> 4656.68s]  at least in academic benchmarking,
[4656.68s -> 4658.42s]  but the biggest one is that really,
[4658.42s -> 4661.52s]  there's no incentives for us to move to anything else.
[4662.86s -> 4666.58s]  And this is actually a pretty interesting paper
[4666.58s -> 4669.50s]  that looks at machine translation between,
[4669.50s -> 4671.94s]  all the papers, or many papers
[4673.50s -> 4676.18s]  from 2019 to 2020 in machine translation,
[4676.18s -> 4678.24s]  and they found that 82% of papers,
[4678.24s -> 4680.54s]  they only evaluated BLEU scores.
[4680.54s -> 4683.38s]  And as we said, BLEU scores have many, many issues.
[4683.38s -> 4688.14s]  And if you see, we know that there are many better metrics,
[4689.60s -> 4691.10s]  but still people are not incentivized
[4691.10s -> 4692.18s]  to look at anything else.
[4692.18s -> 4694.38s]  And actually, reviewers will usually ask you
[4694.38s -> 4697.40s]  to show performance on BLEU scores.
[4697.40s -> 4699.06s]  So it's not even that you're incentivized
[4699.06s -> 4699.94s]  not to look at something else,
[4699.94s -> 4701.52s]  you're also incentivized to continue.
[4701.52s -> 4702.36s]  And it kind of makes sense
[4702.36s -> 4704.02s]  because you wanna be able to compare to methods
[4704.02s -> 4705.24s]  from two, three years ago,
[4705.24s -> 4709.90s]  but it also means that it's hard for the academic field
[4709.90s -> 4711.56s]  to change to other benchmarks.
[4712.40s -> 4714.80s]  But this is really specific to academia.
[4714.80s -> 4716.68s]  In reality, if you know that your metric is bad,
[4716.68s -> 4717.50s]  just switch.
[4719.64s -> 4722.88s]  Okay, evaluation takeaways.
[4722.88s -> 4727.10s]  So first, I mentioned that there were different types
[4727.10s -> 4729.76s]  of evaluation and different desired properties
[4729.76s -> 4731.72s]  for different types of evaluation.
[4731.72s -> 4734.42s]  Then I talked about close-ended tasks
[4734.42s -> 4735.96s]  and how you evaluate those,
[4735.96s -> 4738.50s]  the fact that it's basically standard machine learning,
[4738.50s -> 4740.48s]  but that you have to think carefully
[4740.48s -> 4741.72s]  even though it's standard machine learning
[4741.72s -> 4743.48s]  of how you evaluate them.
[4743.48s -> 4746.04s]  Then there are open-ended tasks
[4746.04s -> 4747.88s]  where you look at content overlap metrics,
[4747.88s -> 4752.88s]  typically, so things like BLEU and Rouge and BERT score.
[4754.12s -> 4756.36s]  And then you have chatbot evaluations,
[4756.36s -> 4758.00s]  which is extremely difficult,
[4759.08s -> 4763.84s]  but people have started using
[4763.84s -> 4765.64s]  essentially LLM-based evaluations.
[4767.12s -> 4768.58s]  And then we talked about challenges,
[4768.58s -> 4769.84s]  one of them being consistency,
[4770.04s -> 4771.52s]  the other one contamination,
[4771.52s -> 4773.30s]  and the third one biases.
[4774.76s -> 4776.08s]  In reality, honestly,
[4776.08s -> 4779.02s]  the best evaluation is just check your outputs.
[4779.90s -> 4782.80s]  So I think too many people,
[4782.80s -> 4784.44s]  they just believe numbers.
[4784.44s -> 4788.20s]  In reality, never just believe numbers.
[4788.20s -> 4791.04s]  I remember when we did initially Alpaca,
[4791.04s -> 4793.24s]  we kind of believed our Alpaca eval,
[4793.24s -> 4795.36s]  but once we started playing with it,
[4795.36s -> 4796.20s]  that's when we were like,
[4796.20s -> 4797.14s]  okay, this thing is actually,
[4797.14s -> 4797.98s]  I mean, at that time, good.
[4797.98s -> 4799.48s]  Now it would be a pretty bad model.
[4800.06s -> 4800.90s]  But at that time, we're like,
[4800.90s -> 4802.60s]  okay, this thing is actually pretty good.
[4802.60s -> 4804.48s]  We should do something about it,
[4804.48s -> 4806.68s]  even though on maybe standard academic benchmarks,
[4806.68s -> 4807.64s]  it was pretty bad.
[4809.04s -> 4810.84s]  So yeah, don't rely on numbers.
[4810.84s -> 4814.04s]  And I'm happy to, what time is it,
[4814.04s -> 4818.30s]  to take any other questions that you may have.
[4819.40s -> 4820.24s]  Yes.
[4820.24s -> 4823.24s]  Question about, so there's this whole issue of bias,
[4823.24s -> 4825.24s]  which we're really trying to deal with,
[4825.24s -> 4827.40s]  but we're sweeping under the rug here.
[4827.40s -> 4830.68s]  So if we have a problem in which we're dealing
[4830.68s -> 4832.38s]  with a very specialized domain,
[4833.32s -> 4838.32s]  and yes, we try and go and run reference re-evals
[4839.08s -> 4841.16s]  using like, let's say GPT-4,
[4843.60s -> 4847.94s]  like, is it considered bad practice
[4847.94s -> 4851.80s]  to be checking a subset of these GPT-4 evals,
[4851.80s -> 4855.20s]  ranking them as ourselves, and then like,
[4855.20s -> 4860.20s]  and then using ourself, like inserting ourself
[4863.04s -> 4866.08s]  and our bias into this process
[4866.08s -> 4869.22s]  by actually looking at many, many, many data points.
[4870.92s -> 4872.66s]  So just to make sure I understand your question,
[4872.66s -> 4876.16s]  you're saying that if we try to look at ourselves
[4876.16s -> 4877.68s]  at the answers, we might be incorporating
[4877.68s -> 4878.72s]  some biases there.
[4878.72s -> 4881.48s]  Yes, but we should look at the answers
[4881.48s -> 4884.20s]  to make sure that GPT-4 isn't being biased
[4884.24s -> 4885.80s]  when it looks at the answers.
[4885.80s -> 4888.16s]  There's this tension here, and I don't know what the,
[4888.16s -> 4890.52s]  because in a controlled scientific experiment,
[4890.52s -> 4893.24s]  you would blind yourself to looking at these answers.
[4893.24s -> 4895.08s]  How do you deal with this?
[4895.08s -> 4896.28s]  Yeah, that's a good question.
[4896.28s -> 4898.48s]  I actually don't quite know, but one thing,
[4900.00s -> 4902.32s]  I actually feel less concerned about biases
[4902.32s -> 4904.04s]  of a single person.
[4904.04s -> 4906.48s]  My issue with the GPT-4 biases is that
[4906.48s -> 4908.48s]  it's the same across every model.
[4908.48s -> 4910.52s]  So things really scale up and kind of,
[4911.92s -> 4913.60s]  it's really, it becomes a monoculture.
[4914.12s -> 4917.80s]  And I think that's less, that's much worse
[4917.80s -> 4919.96s]  than if everyone incorporates a little bit of the biases
[4919.96s -> 4921.40s]  that they have in their direction.
[4921.40s -> 4924.14s]  I'm not saying that that's the best answer,
[4924.14s -> 4925.52s]  but I think it's slightly better
[4925.52s -> 4928.20s]  than just going with whatever they have, yeah.
[4928.20s -> 4929.32s]  How does one, and following up on that,
[4929.32s -> 4931.28s]  how do we avoid a situation if we're like,
[4931.28s -> 4933.64s]  one is trying to solve a problem with a model,
[4935.00s -> 4940.00s]  and one evaluates it with GPT chat, GPT-4,
[4941.00s -> 4944.88s]  and then one starts to look at it and say,
[4944.88s -> 4946.76s]  okay, this is good and stuff.
[4946.76s -> 4949.68s]  And then one goes, okay, this is great.
[4949.68s -> 4952.16s]  And everyone else in the world and GPT-4
[4952.16s -> 4954.00s]  thinks it's a terrible, terrible model,
[4954.00s -> 4956.72s]  and it's just someone being, and just some academic
[4956.72s -> 4960.32s]  being pressuring themselves into publishing something
[4960.32s -> 4962.28s]  that doesn't actually work.
[4962.28s -> 4963.68s]  How do you, how does the field
[4963.68s -> 4966.22s]  structurally avoid situations like that?
[4967.22s -> 4970.26s]  Well, I think that's one reason
[4970.26s -> 4971.66s]  why they want standardized benchmarks,
[4971.66s -> 4973.54s]  and why every reviewer actually wants
[4973.54s -> 4975.84s]  standardized benchmarks, because at least,
[4975.84s -> 4977.58s]  even though everyone knows that they're wrong,
[4977.58s -> 4980.22s]  they understand how they are wrong.
[4980.22s -> 4981.98s]  So I think that's like one perspective.
[4981.98s -> 4983.60s]  Another thing which is not,
[4983.60s -> 4984.86s]  doesn't completely answer your question,
[4984.86s -> 4989.48s]  but I think could be a potential solution,
[4990.64s -> 4993.34s]  is that how I view GPT-4 is just something
[4993.34s -> 4994.66s]  that is really good at performing
[4994.66s -> 4996.46s]  what I want it to perform.
[4996.46s -> 4999.74s]  Right now, the thing is, I'm not very specific
[4999.74s -> 5001.46s]  about what I want it to perform.
[5001.46s -> 5003.82s]  And as a result, it will basically come in
[5003.82s -> 5005.50s]  with its own biases that come from
[5005.50s -> 5008.46s]  its pre-training data or fine-tuning data.
[5008.46s -> 5010.38s]  A potentially better way of doing it
[5010.38s -> 5012.90s]  is that I could write exactly what I wanted.
[5012.90s -> 5015.98s]  So right now, when we do the prompting to GPT-4,
[5015.98s -> 5017.98s]  I basically ask a question, simple question,
[5017.98s -> 5021.58s]  like how good is the summary out of five.
[5021.58s -> 5023.66s]  But a much better way would probably be
[5023.70s -> 5025.66s]  writing a very detailed rubric of everything
[5025.66s -> 5028.94s]  that has to be in this answer for it to be a good answer.
[5028.94s -> 5029.78s]  And if you think about it,
[5029.78s -> 5032.46s]  this is exactly what like professors do
[5032.46s -> 5034.10s]  when they evaluate for class.
[5034.10s -> 5038.46s]  Like they basically say, okay, Yan is a okay TA,
[5038.46s -> 5041.12s]  but I cannot trust him like blindly.
[5041.12s -> 5042.94s]  So what I will do is that I will write
[5042.94s -> 5044.42s]  a very detailed rubric,
[5044.42s -> 5046.70s]  and I trust that he can apply that rubric.
[5046.70s -> 5047.54s]  And I think that's also how
[5047.54s -> 5049.40s]  we should be thinking about GPT-4.
[5049.40s -> 5051.26s]  This is not how we currently do it.
[5053.66s -> 5056.26s]  Any other questions?
