# Detected language: en (p=1.00)

[0.00s -> 11.88s]  So today, I'm delighted to introduce our first invited speaker, is Tao Akhila.
[11.88s -> 19.98s]  Tao has also been, as well as being invited, and I'll tell his background, he's also
[19.98s -> 24.46s]  in the Symbolic Systems program, has been an adjunct professor and has been involved
[24.46s -> 26.76s]  with some students in that role as well.
[26.76s -> 31.36s]  But in his invited role, he's originally from the Netherlands, where he even learnt
[31.36s -> 34.92s]  some logic, among other things, back in the old days.
[34.92s -> 42.96s]  But in more recent times, he's been a prominent deep learning researcher for a number of years.
[42.96s -> 50.20s]  He worked at Facebook, now Meta, in the FAIR unit, and was involved in various ideas,
[50.20s -> 54.24s]  including retrieval augmented generation.
[54.24s -> 57.88s]  After that, he then spent some time at Hugging Face.
[57.88s -> 62.48s]  He's become interested in looking at multimodal models, which is what he's going to be
[62.48s -> 64.84s]  talking about today.
[64.84s -> 68.16s]  And welcome Tao, it's great to have you.
[68.16s -> 73.16s]  Thank you very much.
[73.16s -> 76.72s]  All right.
[76.72s -> 77.72s]  That works, right?
[77.72s -> 78.72s]  Yes.
[78.72s -> 80.48s]  Yeah, thanks everyone for coming.
[80.48s -> 84.00s]  I understand that you get points for being here, so you're not really here for me.
[84.76s -> 87.20s]  But thanks for coming anyway.
[87.20s -> 89.80s]  So I'm going to talk about multimodal deep learning.
[89.80s -> 94.00s]  It's going to have an NLP focus, of course, as for this course, but it's also because
[94.00s -> 98.94s]  otherwise I would really be talking for many more hours than I have time for here.
[98.94s -> 103.56s]  So I'll try to really keep it focused on the things that I think will be most useful
[103.56s -> 104.82s]  for you to learn.
[104.82s -> 109.52s]  And so the first thing you should understand is that this whole concept of multimodality
[109.52s -> 112.40s]  is kind of ill-defined, actually.
[112.40s -> 117.04s]  So if you go to the dictionary you'll see that it means having or involving several
[117.04s -> 121.44s]  modes or modalities or maxima.
[121.44s -> 125.96s]  And so what mode here really means is, so it could be mode in the very generic sense
[125.96s -> 131.78s]  or it could be a very precise sense of the mode of a statistical distribution.
[131.78s -> 136.08s]  And so depending on the paper you're reading, in some cases people really mean the statistical
[136.08s -> 140.78s]  sense, in other cases people really mean this sort of very vague concept of a modality
[140.78s -> 143.90s]  where it really means the type of information that you're getting.
[143.90s -> 149.70s]  So an example of modality in that case is an image or speech signal or audio in general
[149.70s -> 153.24s]  or even olfaction, so smell or things like that.
[153.24s -> 159.74s]  So in this lecture we're just going to focus mostly on text because this is an NLP
[159.74s -> 164.30s]  course and we're going to focus on images mostly as the other modality to keep it
[164.30s -> 165.30s]  simple.
[165.30s -> 168.50s]  All right, so why does it matter?
[168.50s -> 171.38s]  Why do we care about multimodality?
[171.38s -> 175.54s]  And so there are a couple of really good reasons in general for this.
[175.54s -> 178.38s]  The first one is about faithfulness.
[178.38s -> 183.02s]  So if you look at how we humans understand the world, how we make sense of what happens
[183.02s -> 186.78s]  in the world, that is very multimodal, right?
[186.78s -> 192.74s]  So we perceive the world not just using vision or just audio, but we synthesize information
[192.74s -> 196.08s]  across all of these different modalities and that's how we understand the world and
[196.08s -> 197.92s]  each other.
[197.92s -> 200.80s]  There's also a very practical argument for doing it.
[200.80s -> 203.06s]  It's because the internet is multimodal, right?
[203.06s -> 207.98s]  So if you go to, I don't know, like Facebook or something like that, it rarely happens
[207.98s -> 209.86s]  that it's just text or just an image.
[209.86s -> 213.22s]  It's usually a combination of multiple modalities.
[213.22s -> 218.84s]  And then the final good reason that we're just starting to hit now, if you're really
[218.84s -> 222.56s]  following where the field is going, we're kind of running out of text data for these
[222.56s -> 224.10s]  large language models.
[224.10s -> 229.68s]  So one interesting way to keep scaling on the data side is to make use of all of these
[229.68s -> 231.06s]  other modalities, right?
[231.06s -> 235.12s]  So if you can have your language model also watch all of the videos of cats in the
[235.12s -> 239.26s]  world, it's going to understand the concept of cat much better.
[239.26s -> 240.80s]  And that's what we want to have in these models.
[240.80s -> 246.74s]  We want them to understand the world in the same way that humans understand it.
[246.74s -> 251.48s]  So right now, multimodality is really one of the main frontiers of this new foundation
[251.48s -> 255.82s]  model drive that we're all in right now.
[255.82s -> 257.94s]  There's a thing called the McGurk effect.
[257.94s -> 260.10s]  Let's see if it loads up.
[260.10s -> 267.10s]  So what we'll see when this loads is this guy over here, and we'll have the same audio
[267.10s -> 269.06s]  effect being played.
[269.06s -> 275.30s]  So the audio is exactly the same, and this man is going to say something like, bah, bah,
[275.30s -> 276.30s]  bah.
[276.30s -> 280.32s]  And so you're hearing a B there, I think, if you look at my mouth, because that's what
[280.32s -> 281.50s]  I said.
[281.50s -> 286.94s]  But if you then change the video to where he says, fah, fah, fah, with exactly the
[286.94s -> 290.74s]  same audio, you're going to hear the other version.
[290.74s -> 294.42s]  So unfortunately, I can't really like swap in the different audio here, so you have
[294.42s -> 296.00s]  to trust me for it.
[296.00s -> 302.34s]  We might suddenly start hearing a guy saying, fah, fah, fah, and then, all right.
[302.36s -> 305.72s]  So multimodal applications.
[305.72s -> 311.92s]  So when we have multiple modalities, we can do all kinds of interesting things.
[311.92s -> 316.84s]  And as I said, most of the use cases we have on the internet, they're all multimodal.
[316.84s -> 321.44s]  And there are some really kind of obvious things we would be interested in if we have
[321.44s -> 325.68s]  information from these different data sources, from different modalities.
[325.68s -> 328.08s]  So obviously, we might want to do retrieval.
[328.08s -> 333.30s]  So maybe given a bit of text, we want to find the right image, or maybe given some image,
[333.30s -> 336.38s]  we want to find the right text for it so we can match them up.
[336.38s -> 338.70s]  Obviously, we can also do this in a generative setting.
[338.70s -> 341.16s]  So then we have image captioning, which you've probably heard of.
[341.16s -> 343.70s]  We can do text-to-image generation.
[343.70s -> 346.62s]  So that's image synthesis, and so stable diffusion.
[346.62s -> 349.50s]  Everybody in the audience here has probably seen that.
[349.50s -> 353.26s]  Then we can do visual question answering, where we have an image and text, and then
[353.26s -> 355.02s]  we need to generate some new text.
[355.02s -> 358.02s]  We have multimodal classification, where we have image and text and we need to have
[358.02s -> 361.74s]  a label, for example, whether something is hate speech or not.
[361.74s -> 366.70s]  And then in general, we want to be able to have a richer understanding of information,
[366.70s -> 370.98s]  which means that we combine images and text and then use it for downstream applications
[370.98s -> 374.50s]  that require better understanding or better generation.
[374.50s -> 377.66s]  So this field really is super hot right now.
[377.66s -> 382.26s]  So there's this nice paper title, I predict that this paper is going to do really well
[382.50s -> 385.98s]  in terms of citations, just because it has such a citeable title.
[385.98s -> 389.06s]  I think a lot of people are not actually going to read it.
[389.06s -> 392.62s]  And so, I mean, I've been in this field for quite a while now, and people have been
[392.62s -> 394.10s]  saying this for a really long time.
[394.10s -> 395.22s]  I think Chris would agree.
[395.22s -> 399.62s]  So for decades, people have been saying that multimodal is the next big thing.
[399.62s -> 401.30s]  But now it's really true, I think.
[402.82s -> 406.62s]  All right, so the outline for what we're going to be talking about.
[406.62s -> 409.66s]  So first, I'm going to tell you a little bit about early models.
[409.70s -> 413.26s]  Then we're going to do a bit of a deep dive on some of the specifics.
[413.26s -> 416.62s]  Then we're going to go over a particular type of fusion,
[416.62s -> 418.74s]  contrastive models or late fusion.
[418.74s -> 421.06s]  Then we're going to go through a little bit of the history
[421.06s -> 423.98s]  of multimodal foundation models.
[423.98s -> 426.06s]  Then we're going to talk a little bit about evaluation,
[426.06s -> 427.62s]  a little bit about other modalities,
[427.62s -> 429.82s]  and then I'll make some predictions for the future
[429.82s -> 432.02s]  and hopefully maybe give you some cool research ideas
[432.02s -> 433.86s]  or things to talk or think about.
[435.10s -> 438.30s]  All right, so obviously, like there's a lot of work
[438.30s -> 439.98s]  that happened before deep learning,
[439.98s -> 443.58s]  but I think if you want to start from the deep learning revolution
[443.58s -> 446.58s]  and what was happening in images and text,
[446.58s -> 452.74s]  then a good starting point is, for example, Wasabi or Devise
[452.74s -> 456.06s]  or Richard Zoker, who you've probably heard of,
[456.06s -> 458.34s]  has done some really cool early work in this
[458.34s -> 460.94s]  that really pioneered a lot of these ideas.
[460.94s -> 465.06s]  And the basic gist of this is that we have a vision model.
[465.06s -> 467.86s]  On the one hand, we have a language model.
[468.78s -> 471.62s]  The first lecture of this course, I think, was about word embeddings.
[471.62s -> 474.22s]  So that's just your basic word embedding model.
[474.22s -> 476.34s]  And now we need to figure out how to align them
[476.34s -> 477.94s]  in the same multimodal space.
[478.58s -> 481.66s]  So the way you do that is you get some sort of similarity metric,
[481.66s -> 483.74s]  a score function or like a kernel function
[483.74s -> 485.98s]  if you're thinking about this from a support vector machine
[485.98s -> 487.42s]  literature perspective.
[487.42s -> 491.62s]  And now you need to figure out in a max margin or margin loss
[493.18s -> 496.34s]  how you want to align these two points in your embedding space.
[496.38s -> 498.86s]  So things that are similar, you want to bring them closer together,
[498.86s -> 501.78s]  things that are not, you want to bring them further apart.
[501.78s -> 504.98s]  And if you do that in this multimodal embedding space,
[504.98s -> 508.82s]  that means that you can do interesting cross-modal transfer
[508.82s -> 512.90s]  where you can take the word embedding for something like auto or like horse
[512.90s -> 516.74s]  and then you can find close images in the embedding space to that thing
[516.74s -> 519.22s]  and now you've solved the retrieval problem.
[519.98s -> 522.30s]  So this is a really nice early application
[522.30s -> 525.34s]  and I think a lot of the stuff that I'm going to talk about
[525.38s -> 529.06s]  in the early slides, you're going to see this thing come over and over again.
[529.06s -> 532.06s]  You're going to see it get kind of reinvented with fancier models
[532.06s -> 534.02s]  but it's basically all the same stuff.
[535.74s -> 539.14s]  So you can do cross-modal transfer where you have images and text
[539.14s -> 541.18s]  but you can also combine them together
[541.18s -> 543.42s]  so that you get a multimodal word embedding.
[544.26s -> 548.06s]  And so this just gives you a more accurate representation
[548.06s -> 550.66s]  of how humans understand word meaning
[550.66s -> 554.26s]  because when we think about the word moon or cat or something,
[554.26s -> 558.38s]  we can go to Wikipedia and read that a cat is a small carnivorous mammal
[558.38s -> 560.34s]  that people like to keep as pets
[560.34s -> 562.26s]  or we can just go and look at pictures of cats
[562.26s -> 564.90s]  and now we understand what a cat is, right?
[564.90s -> 566.78s]  And I would argue actually that for a lot of people,
[566.78s -> 570.02s]  the picture of the cat is much closer to the meaning of the concept of cat.
[571.30s -> 575.30s]  So some early work where people were trying to do this
[576.50s -> 580.06s]  is from Bruni et al where they did multimodal distributional semantics
[580.06s -> 584.22s]  using this very elegant approach called bag of visual words.
[584.22s -> 586.94s]  So just like who has heard of bag of visual words?
[588.74s -> 589.94s]  Very few people, okay.
[589.94s -> 593.22s]  So it's surprisingly simple and so I kind of like it.
[593.22s -> 594.54s]  It's nicely elegant.
[594.54s -> 597.22s]  So you take a picture of a moon in this case.
[597.22s -> 598.78s]  I think you can see it in the back too, right?
[598.78s -> 603.82s]  So we use an algorithm like SIFT to find interesting key points.
[603.82s -> 605.46s]  So it's sort of where the difference
[605.46s -> 608.10s]  between the pixels and the pixels next to it,
[608.10s -> 610.02s]  where that difference is big.
[610.02s -> 613.18s]  Those are sort of the spots you want to be looking at.
[613.22s -> 617.10s]  And for each of these key points, you get feature descriptors.
[617.10s -> 620.58s]  So relatively small vectors like 32 dimensional events
[620.58s -> 623.18s]  are kind of on the implementation of this.
[623.18s -> 625.58s]  And what you can do now with these feature descriptors
[625.58s -> 627.98s]  is you can cluster them using K-means
[627.98s -> 631.50s]  and then you assign every one of these points
[631.50s -> 633.46s]  so you can count how often they occur, right?
[633.46s -> 635.70s]  So in this picture of the moon, we have like,
[635.70s -> 636.98s]  actually the count is, oh yeah.
[636.98s -> 638.70s]  So there are three like red dots, right?
[638.70s -> 641.62s]  So that's why the red dot one is three.
[641.62s -> 646.10s]  So what that gives you is an idea of the visual words,
[646.10s -> 648.38s]  very similar to the original bag of words model
[648.38s -> 650.58s]  that you hopefully have heard about
[650.58s -> 652.50s]  maybe in the first lecture.
[652.50s -> 656.14s]  So that's the visual equivalent of the textual thing.
[656.14s -> 658.90s]  And so if you do this and you then concatenate
[658.90s -> 660.94s]  or you apply SVD to fuse the information,
[660.94s -> 663.30s]  what you get is a word embedding
[663.30s -> 666.66s]  that is much more representative of human meaning.
[666.66s -> 669.14s]  So, you know, as reflected in the data sets
[669.18s -> 672.70s]  that people used to care about at the time.
[672.70s -> 675.50s]  So after that, there were a couple of people,
[675.50s -> 677.62s]  me included, who tried to take these ideas
[677.62s -> 679.38s]  and then really apply deep learning to them.
[679.38s -> 682.46s]  So some of the very early versions of this
[682.46s -> 684.82s]  use convolutional neural networks
[684.82s -> 686.70s]  and then you can transfer the features
[686.70s -> 690.46s]  from your ConvNet and you take your word embeddings,
[690.46s -> 692.54s]  which you've seen in the first lecture,
[692.54s -> 694.70s]  and then you can concatenate them.
[694.70s -> 696.70s]  Now you have a multimodal word vector
[696.70s -> 698.62s]  or you can do something slightly fancier.
[698.62s -> 701.02s]  So you've seen the skip gram model.
[701.02s -> 703.82s]  You can also try to do skip gram predictions
[703.82s -> 705.42s]  onto image features, right?
[705.42s -> 708.10s]  So when you see a word like cat in some context,
[708.10s -> 710.78s]  like the cute little cat sat on the mat,
[710.78s -> 711.66s]  then when you see cat,
[711.66s -> 714.34s]  you also want to predict cat pictures.
[714.34s -> 716.50s]  So super easy ideas, but it turned out
[716.50s -> 719.54s]  that this gives you much richer word representations.
[719.54s -> 720.78s]  So that's kind of cool,
[720.78s -> 722.98s]  but obviously words are very limited.
[722.98s -> 726.18s]  What we really care about is not words, but sentences.
[726.18s -> 728.42s]  So then people started really looking
[728.42s -> 730.46s]  into sentence representations
[730.46s -> 732.42s]  and how can we figure out
[732.42s -> 734.82s]  how to get compositional understanding
[734.82s -> 736.42s]  in the sentence representations
[736.42s -> 739.02s]  and how do we align that with images?
[740.10s -> 742.54s]  So the loss here is very similar
[742.54s -> 744.46s]  to what we saw with words and pictures,
[744.46s -> 747.38s]  but now we just have a sentence encoder, right?
[747.38s -> 749.54s]  And so there are some really cool early papers
[749.54s -> 751.70s]  from Andrej Karpathy and Richard Soker
[751.70s -> 753.42s]  also had some work here.
[754.26s -> 757.82s]  And then, you know, so the basic idea is just
[757.82s -> 759.54s]  that instead of having these word embeddings,
[759.54s -> 762.18s]  we now have an LSCM in these papers
[762.18s -> 764.30s]  or some other kind of recurrent neural network
[764.30s -> 767.42s]  or in the case of this one, recursive neural network.
[767.42s -> 771.26s]  And then we try to align the features together.
[771.26s -> 773.38s]  And so these three or four papers
[773.38s -> 774.42s]  are actually very important.
[774.42s -> 776.38s]  This one by me is less important,
[776.38s -> 777.74s]  but it's still kind of interesting
[777.74s -> 780.26s]  because we showed here
[780.26s -> 782.22s]  that grounded sentence representation.
[782.22s -> 784.62s]  So if you actually just use this part here
[784.62s -> 787.54s]  as a sentence encoder for NLP tasks,
[787.54s -> 789.94s]  the ability to just predict pictures from it
[789.94s -> 792.86s]  already gives you a really good sentence representation.
[792.86s -> 795.64s]  Right, so just by predicting pictures,
[795.64s -> 797.82s]  you can sort of imagine what things look like
[797.82s -> 799.90s]  and that gives you a really good meaning representation
[799.90s -> 801.54s]  which you can then transfer to,
[801.54s -> 804.46s]  I don't know, sentiment classification or something else.
[806.70s -> 810.82s]  And then of course, once we have sentence encoders
[810.86s -> 813.14s]  or then we also have decoders.
[813.14s -> 816.54s]  And so when the sequence to sequence architecture came out,
[816.54s -> 819.26s]  which you've probably also heard about in this course,
[819.26s -> 822.26s]  what you can do instead of having a text encoder
[822.26s -> 823.54s]  for like your source language
[823.54s -> 824.86s]  if you're doing machine translation
[824.86s -> 829.86s]  is you can plug in a ConfNet instead of an LSTM encoder
[829.90s -> 831.76s]  and now you can generate captions.
[831.76s -> 833.94s]  So that's exactly what people did.
[833.94s -> 835.80s]  We used to have all of these fancy diagrams
[835.80s -> 838.14s]  in our papers then where we explained the LSTM
[838.14s -> 839.26s]  and how that works.
[839.26s -> 842.38s]  Probably people don't learn that anymore these days, they do?
[842.38s -> 844.46s]  Yeah, very good.
[844.46s -> 847.84s]  They might make a comeback, I think, at some point.
[847.84s -> 850.04s]  Transformers are gonna go away, we'll see.
[852.04s -> 854.78s]  And so one of the things that people figured out
[854.78s -> 856.68s]  in machine translation very early on
[856.68s -> 859.38s]  is that you can do alignment of words
[859.38s -> 861.84s]  between your source language and your target language.
[861.84s -> 864.10s]  And you can do the same thing actually with images.
[864.10s -> 866.06s]  So if you want to align a word
[867.06s -> 870.26s]  in your generated sequence with something in your picture,
[870.26s -> 873.54s]  then you can use the same approach for that.
[873.54s -> 875.78s]  And that approach, of course, is called attention.
[875.78s -> 878.38s]  So you've learned a lot about attention
[878.38s -> 879.66s]  probably in this course.
[879.66s -> 882.50s]  And so yeah, that was one of the building blocks
[882.50s -> 883.58s]  of these systems as well
[883.58s -> 885.82s]  where you can do very interesting things
[885.82s -> 888.54s]  and really see that when it has to generate
[888.54s -> 890.22s]  stop for the stop sign
[890.22s -> 892.82s]  that it's really actually looking at the stop sign.
[892.82s -> 895.86s]  So there's a really cool alignment going on there
[896.50s -> 897.34s]  in these models.
[898.34s -> 900.58s]  And so the final kind of early model
[900.58s -> 904.26s]  we should talk about a little bit is GANs.
[904.26s -> 905.70s]  Who here has heard of GANs?
[906.72s -> 909.30s]  Okay, that's a lot more than back official words.
[909.30s -> 911.22s]  I guess that makes sense.
[911.22s -> 914.26s]  And so yeah, the basic idea of a GAN
[914.26s -> 916.62s]  is really that you have this generator and discriminator
[916.62s -> 918.66s]  and you want to have the generator
[918.66s -> 920.46s]  generate images that the discriminator
[920.46s -> 923.22s]  cannot distinguish from,
[923.22s -> 925.78s]  so it cannot distinguish fake and real images.
[926.54s -> 928.30s]  And if you do that, you can actually condition that
[928.30s -> 930.10s]  on the piece of text
[930.10s -> 932.54s]  and then you can generate images
[932.54s -> 935.06s]  using some text prompt, right?
[935.06s -> 938.30s]  So that's what kind of the first versions
[938.30s -> 940.58s]  of stable diffusion were doing things like this
[940.58s -> 943.86s]  and it's all a natural progression to that model.
[945.02s -> 947.24s]  So those were the early models.
[948.38s -> 951.48s]  Maybe, do people have any burning questions about this
[951.48s -> 952.98s]  or does this all make sense?
[952.98s -> 956.38s]  All right.
[956.38s -> 958.98s]  So let's do a bit of a deeper dive then
[958.98s -> 962.10s]  on in particular on features and fusion.
[962.10s -> 964.10s]  So those are really the kind of core building blocks
[964.10s -> 966.08s]  for all of this multimodal stuff.
[966.08s -> 968.82s]  But before we go there, maybe very briefly,
[968.82s -> 971.68s]  like if all of this multimodal stuff is cool
[971.68s -> 975.62s]  and sort of useful and doesn't look that difficult,
[975.62s -> 978.58s]  like why aren't we all doing multimodal things?
[978.58s -> 981.98s]  So why do we focus on specific modalities?
[982.02s -> 983.46s]  And I think there are a couple of problems
[983.46s -> 985.62s]  just to be aware of.
[985.62s -> 988.18s]  So one is modalities can sometimes dominate,
[988.18s -> 990.64s]  especially text is much more dominant
[990.64s -> 993.22s]  than vision or audio in many use cases, right?
[993.22s -> 995.40s]  So you can already just have a model
[995.40s -> 997.02s]  that picks up on the text signal
[997.02s -> 999.34s]  and basically learns to ignore the image completely,
[999.34s -> 1001.32s]  which actually happened embarrassingly
[1001.32s -> 1002.94s]  for visual question answering.
[1002.94s -> 1003.78s]  We'll get to that.
[1003.78s -> 1005.66s]  So visual question answering, you could do that
[1005.66s -> 1007.70s]  without actually looking at the picture.
[1008.98s -> 1011.84s]  The additional modalities can add a lot of noise.
[1012.56s -> 1014.96s]  So it makes your machine learning problem more difficult.
[1014.96s -> 1016.66s]  You don't always have full coverage, right?
[1016.66s -> 1018.44s]  So as I said, if you look at Facebook posts,
[1018.44s -> 1020.32s]  sometimes you have text, sometimes you have pictures,
[1020.32s -> 1021.32s]  sometimes you have both,
[1021.32s -> 1023.44s]  but you don't have a guarantee that you always have both.
[1023.44s -> 1025.78s]  So how do you deal with that?
[1025.78s -> 1027.50s]  In many cases, we just really weren't ready.
[1027.50s -> 1030.08s]  It was too complicated to implement stuff.
[1030.08s -> 1031.24s]  And also just in general,
[1031.24s -> 1033.48s]  like how to design your model really
[1033.48s -> 1036.68s]  to combine all the information
[1036.68s -> 1038.08s]  is actually quite complicated.
[1038.08s -> 1043.08s]  So in order to maybe drive that point home a little bit,
[1044.84s -> 1045.94s]  so featurizing text,
[1045.94s -> 1048.74s]  I guess we all know how to do that by now,
[1048.74s -> 1050.80s]  especially sort of in the age of transformers
[1050.80s -> 1052.12s]  and before in LSCM story,
[1052.12s -> 1054.94s]  we just have like you have your batch by your sequence.
[1054.94s -> 1058.20s]  So batch size by sequence length by embedding size, right?
[1058.20s -> 1061.32s]  So it's always like a 3D tensor.
[1061.32s -> 1063.92s]  And that's how you encode your textual information
[1063.92s -> 1066.52s]  when you pump it through your neural net.
[1066.52s -> 1068.74s]  And so with images, it's slightly trickier
[1068.74s -> 1071.80s]  because you can just kind of look at the patches,
[1071.80s -> 1073.40s]  but then if you do convolutions,
[1073.40s -> 1075.40s]  you're kind of like shifting over the image
[1075.40s -> 1077.84s]  and then you're aggregating, right?
[1077.84s -> 1078.88s]  And in many cases,
[1078.88s -> 1081.16s]  you don't really want to be this uniform.
[1081.16s -> 1083.12s]  You want to have something that actually looks
[1083.12s -> 1085.14s]  at the things in the picture, right?
[1085.14s -> 1087.00s]  So this is called region features
[1087.00s -> 1088.72s]  where you would use an object detector
[1088.72s -> 1091.20s]  as a first step for processing your image.
[1091.20s -> 1093.04s]  And then you would have a component backbone
[1093.04s -> 1096.56s]  that encodes the features for that particular sub image,
[1096.56s -> 1098.52s]  like this guy's like skateboard or something.
[1098.52s -> 1101.10s]  It has its own like vector representation, right?
[1102.18s -> 1104.08s]  And then in terms of dense features,
[1104.08s -> 1105.72s]  we now also have vision transformer.
[1105.72s -> 1108.00s]  So we'll just very quickly go over that
[1108.00s -> 1109.80s]  to make sure we're on the same page.
[1109.80s -> 1112.00s]  So there are all these models like YOLO is a really good
[1112.00s -> 1114.30s]  one if you haven't heard of that yet.
[1114.30s -> 1117.84s]  So we're at YOLO v7 now, I think, or eight, I don't know.
[1117.84s -> 1121.64s]  So there's a new one coming out every other year
[1121.64s -> 1122.84s]  or something.
[1123.48s -> 1126.28s]  But the basic idea is that we get these bounding boxes
[1126.28s -> 1129.00s]  for things in the images or actually segmentations
[1129.00s -> 1131.04s]  with the bounding boxes is what people tend to use
[1131.04s -> 1132.54s]  and they have labels, right?
[1132.54s -> 1134.84s]  So this is labeled like backpack or something.
[1134.84s -> 1137.58s]  And so you can do this as a pre-processing step
[1137.58s -> 1140.94s]  on your image to get a much richer representation
[1140.94s -> 1142.30s]  of what is really in that image,
[1142.30s -> 1144.04s]  which you can then pump into your system
[1144.04s -> 1145.28s]  as we'll see later.
[1145.28s -> 1147.96s]  And so then how you encode the information
[1147.96s -> 1149.68s]  that is in these little bounding boxes
[1149.68s -> 1152.50s]  or actually in the image itself in general,
[1153.02s -> 1154.70s]  we just use a standard comp net for that.
[1154.70s -> 1158.72s]  And so this probably feels like super obvious now,
[1158.72s -> 1162.10s]  but in 2014, when people were starting to discover this,
[1162.10s -> 1165.18s]  it was really very surprising that you could just use
[1165.18s -> 1167.14s]  off the shelf comp net features
[1167.14s -> 1170.22s]  to really replace the entire computer vision pipeline.
[1170.22s -> 1172.66s]  So people used to do all of this very fancy,
[1172.66s -> 1174.74s]  sophisticated stuff and people, you know,
[1174.74s -> 1176.66s]  spent decades on trying to refine this.
[1176.66s -> 1179.50s]  And then it was all thrown away and replaced by a comp net
[1179.50s -> 1182.00s]  that does all of that stuff for free.
[1182.36s -> 1183.72s]  And so the cool thing you get there
[1183.72s -> 1186.88s]  is that you can transfer very easily across different tasks.
[1186.88s -> 1189.10s]  So you can have a very generic comp net
[1189.10s -> 1192.44s]  and then use it to all kinds of very specialized things
[1192.44s -> 1195.96s]  like spotting buildings in Paris, for example,
[1195.96s -> 1197.82s]  or flowers or other stuff.
[1198.92s -> 1201.92s]  And then of course, in the age of transformers,
[1201.92s -> 1204.32s]  how far in, we're already quite a while in,
[1204.32s -> 1207.40s]  this is only the first transformer actually in the slide deck.
[1207.40s -> 1210.84s]  So, you know, we're making good progress.
[1210.84s -> 1213.88s]  So vision transformers are what we would use these days
[1213.88s -> 1217.40s]  to encode the images where you have these flattened patches
[1217.40s -> 1220.60s]  and then you would do kind of the standard
[1220.60s -> 1222.96s]  birth architecture, maybe as you would know it
[1222.96s -> 1225.34s]  from this course, and then you do classification, right?
[1225.34s -> 1227.28s]  So this is all like a standard transformer,
[1227.28s -> 1229.36s]  everything standard, except now your input here
[1229.36s -> 1232.92s]  is not words or tokens, it's patches of an image.
[1232.92s -> 1234.36s]  And then you classify that.
[1235.76s -> 1238.06s]  All right, so then we have a bunch of features
[1238.06s -> 1240.20s]  and now how do we combine the information, right?
[1240.44s -> 1243.08s]  So let's say we have two vectors, U and V.
[1243.08s -> 1245.20s]  So, you know, it sounds easy, right,
[1245.20s -> 1247.58s]  to how we could combine them.
[1247.58s -> 1249.12s]  It turns out that there are actually
[1249.12s -> 1250.60s]  very many ways to combine them.
[1250.60s -> 1252.48s]  So I don't think it's really useful
[1252.48s -> 1255.32s]  to go over all the different ways here,
[1255.32s -> 1256.82s]  but you can do very simple things, right?
[1256.82s -> 1259.36s]  So obviously like inner product or similarity
[1259.36s -> 1260.78s]  is what you would use if you want to do
[1260.78s -> 1263.00s]  cross modal things, so if you want to embed things
[1263.00s -> 1264.94s]  in the same vector space,
[1264.94s -> 1268.56s]  but you can do sort of fancier projections on top
[1268.60s -> 1271.72s]  or different combinations that are kind of linear,
[1271.72s -> 1273.58s]  or you can do multiplicative things
[1273.58s -> 1276.36s]  where you multiply the components element wise,
[1276.36s -> 1278.72s]  or you do some sort of gating over the different features
[1278.72s -> 1280.12s]  you can do attention,
[1280.12s -> 1282.38s]  you can do fancier bilinear things,
[1282.38s -> 1285.28s]  you can do very fancy compact bilinear things.
[1285.28s -> 1287.34s]  So there's really a wealth of literature
[1287.34s -> 1288.78s]  kind of on all the different ways
[1288.78s -> 1290.60s]  you can combine two vectors.
[1290.60s -> 1293.80s]  And so this is called multimodal fusion
[1293.80s -> 1296.08s]  and most of the literature on multimodality
[1296.08s -> 1297.92s]  is essentially about this question,
[1297.96s -> 1299.78s]  what is the best way to do fusion?
[1299.78s -> 1300.62s]  And that's it.
[1302.24s -> 1304.28s]  So I think within that discussion
[1304.28s -> 1305.66s]  is maybe useful to distinguish
[1305.66s -> 1307.86s]  between different levels of fusion.
[1307.86s -> 1309.72s]  So you can do it very early
[1309.72s -> 1311.26s]  where basically you make sure
[1311.26s -> 1312.48s]  you have the different features
[1312.48s -> 1314.32s]  and then you just kind of,
[1314.32s -> 1316.56s]  in the sort of modern sense of attention,
[1316.56s -> 1318.80s]  you would attend to everything in all the features
[1318.80s -> 1320.00s]  from the beginning,
[1320.00s -> 1323.26s]  you can first treat them separately and then combine them,
[1323.26s -> 1325.34s]  or you can treat them as completely separate
[1325.34s -> 1328.10s]  and then you only combine the final scores, right?
[1328.10s -> 1331.50s]  And so that's kind of what we would call early fusion
[1331.50s -> 1334.54s]  and then sort of my invention for calling the middle part
[1334.54s -> 1335.78s]  would be sort of middle fusion
[1335.78s -> 1337.66s]  and then you have late fusion
[1337.66s -> 1340.58s]  where you really just combine the scores or the logits
[1340.58s -> 1342.66s]  but you don't really have any interaction
[1342.66s -> 1345.52s]  between the information from the different modalities.
[1346.88s -> 1350.54s]  So you could do really fun stuff with multimodal fusion.
[1350.54s -> 1354.14s]  So this is a paper I really like, Film,
[1354.14s -> 1357.46s]  where you have this sort of very special
[1359.10s -> 1361.10s]  feature map, this sort of F here,
[1361.10s -> 1364.86s]  and it gets modulated by a multiplicative factor.
[1364.86s -> 1368.16s]  So this gamma and an additive sort of bias vector,
[1368.16s -> 1370.58s]  this beta, and you have a different one
[1370.58s -> 1373.50s]  for every layer of a ResNet that is conditioned
[1373.50s -> 1376.30s]  on some encoding of the thing you're after.
[1376.30s -> 1377.14s]  So in this case,
[1377.14s -> 1378.66s]  are there more cubes than yellow things?
[1378.66s -> 1381.38s]  So we have some vector representation for that
[1381.38s -> 1383.04s]  and we use that vector representation
[1383.04s -> 1385.22s]  to modulate the ResNet blocks
[1385.22s -> 1387.48s]  at every layer of the ConvNet.
[1388.48s -> 1390.88s]  So you can really do very fun things
[1390.88s -> 1392.80s]  where you're sort of modulating one network
[1392.80s -> 1395.88s]  with the other one and really try to have them learn
[1395.88s -> 1397.44s]  as much as possible from that.
[1398.52s -> 1402.22s]  All right, so let's talk about late fusion then.
[1402.22s -> 1404.74s]  So late fusion is what we would now call
[1404.74s -> 1407.64s]  contrastive models but the basic idea
[1407.64s -> 1409.60s]  is that we have this similarity score.
[1409.60s -> 1410.96s]  So we have the two kind of,
[1410.96s -> 1413.32s]  we process the modalities completely independently
[1413.32s -> 1416.32s]  and then at the very end we do some combination.
[1416.32s -> 1420.48s]  And the most famous instance of that these days is CLIP.
[1420.48s -> 1421.68s]  So who's heard of CLIP?
[1423.80s -> 1427.84s]  Okay, so CLIP from OpenAI.
[1427.84s -> 1431.28s]  So it's, again, exactly the same contrastive loss
[1431.28s -> 1434.20s]  that we've seen in all these early approaches.
[1434.20s -> 1438.32s]  It does kind of negative sampling but then in batch.
[1438.32s -> 1439.48s]  So you just have a batch,
[1439.52s -> 1441.40s]  you have two things that are aligned, right?
[1441.40s -> 1444.48s]  So like this, the first piece of text and the first image
[1444.48s -> 1446.96s]  they are aligned so this is the right answer.
[1446.96s -> 1449.28s]  And I just wanna make sure that I rank this thing
[1449.28s -> 1452.06s]  higher than all the alternatives, right?
[1452.06s -> 1453.76s]  And I wanna make sure I rank this thing
[1453.76s -> 1455.38s]  higher than all the alternatives.
[1455.38s -> 1458.28s]  So it's a very, very simple idea.
[1458.28s -> 1460.92s]  Really, really nothing special about this architecture
[1460.92s -> 1462.48s]  that was sort of invented here
[1462.48s -> 1465.84s]  but what made this thing so cool was first of all
[1465.84s -> 1468.34s]  it was transformers and it was transformers all the way
[1468.34s -> 1470.32s]  so your text encoder would be a transformer
[1470.32s -> 1473.62s]  and your image encoder would be a VIT image encoder
[1473.62s -> 1475.86s]  so also a transformer.
[1475.86s -> 1479.38s]  And it was trained on lots and lots of web data.
[1479.38s -> 1481.58s]  So Alec Radford is really a genius
[1481.58s -> 1484.42s]  at creating very high quality data sets
[1484.42s -> 1487.70s]  and he created I think 300 million image text pairs
[1487.70s -> 1490.50s]  for this data set, trained a bigger model on it
[1490.50s -> 1492.74s]  than people used to do
[1492.74s -> 1495.62s]  and then we got this amazing model out of it.
[1496.14s -> 1500.22s]  And so moving away from the words there
[1500.22s -> 1503.30s]  to the sort of text that you would see on the internet.
[1503.30s -> 1505.88s]  So the caption for an image on the web
[1505.88s -> 1507.70s]  is not gonna say dog or cat,
[1507.70s -> 1510.14s]  it's gonna say a photo of a cat doing something,
[1510.14s -> 1510.96s]  something, right?
[1510.96s -> 1514.84s]  So that means that you can do kind of zero shot
[1514.84s -> 1517.82s]  label predictions where you have a photo of
[1517.82s -> 1521.62s]  and then you need to figure out what the right label
[1521.62s -> 1524.64s]  is for a given image using this kind of prompt, right?
[1524.68s -> 1527.20s]  So the thing you probably all know about
[1527.20s -> 1528.58s]  prompting large language models
[1528.58s -> 1530.92s]  and so you can prompt vision and language models
[1530.92s -> 1532.64s]  in very much the same way
[1532.64s -> 1534.74s]  and do zero shot generalization.
[1535.88s -> 1538.16s]  So if you want to read a really good paper,
[1538.16s -> 1539.84s]  I would recommend that you read this paper.
[1539.84s -> 1541.36s]  This is really one that's gonna teach you
[1541.36s -> 1542.84s]  how to write really good papers.
[1542.84s -> 1545.94s]  It's thorough and it's really worth a very close read
[1545.94s -> 1548.24s]  I think if you're interested in this field.
[1548.24s -> 1551.24s]  And so I think when it came out,
[1551.24s -> 1553.32s]  actually on ImageNet itself,
[1553.56s -> 1556.82s]  it didn't really outperform ResNet, right?
[1556.82s -> 1557.66s]  So you might think,
[1557.66s -> 1559.96s]  oh yeah, it actually is not all that special.
[1559.96s -> 1561.36s]  But what really made it special
[1561.36s -> 1563.14s]  was that it generalized much better
[1563.14s -> 1564.70s]  to these other data sets, right?
[1564.70s -> 1568.10s]  So this ResNet thing here is pretty terrible
[1568.10s -> 1571.48s]  at some of these kind of adversarial versions of ImageNet
[1571.48s -> 1573.16s]  and CLIP is super robust to that.
[1573.16s -> 1576.16s]  So it's just a way better image encoder in general.
[1577.64s -> 1580.00s]  So very quickly after CLIP,
[1580.00s -> 1583.88s]  there was this paper from Google using a line
[1583.88s -> 1587.56s]  which was basically exactly the same idea.
[1587.56s -> 1589.88s]  The field is not really that creative at all.
[1589.88s -> 1590.88s]  It's like the same idea
[1590.88s -> 1592.88s]  but then you just keep throwing more data
[1592.88s -> 1595.68s]  and more compute at it and that often works much better.
[1595.68s -> 1598.16s]  So that's what they found here too.
[1598.16s -> 1601.16s]  1.8 billion image taxpayers instead of 300 million
[1601.16s -> 1603.18s]  gets you a better model.
[1603.18s -> 1604.02s]  Surprise.
[1605.56s -> 1607.40s]  But so it's still very cool.
[1607.40s -> 1609.48s]  And what is really cool, I think,
[1609.88s -> 1611.48s]  is that there's this organization called Lion
[1611.48s -> 1616.48s]  where they've started this open source collective
[1616.76s -> 1619.88s]  to create really high quality data sets.
[1619.88s -> 1624.48s]  And so the Lion, the initial data set was,
[1624.48s -> 1627.60s]  how many examples in the initial Lion?
[1627.60s -> 1628.92s]  400 million.
[1628.92s -> 1631.24s]  He knows, I know that he knows.
[1631.24s -> 1634.96s]  And so now there's a much bigger version of Lion
[1634.96s -> 1638.12s]  that's even multilingual and it has five billion examples.
[1638.12s -> 1641.64s]  So stable diffusion was trained on sort of the image,
[1641.64s -> 1644.20s]  the English subset of this thing.
[1644.20s -> 1646.24s]  And that's one of the reasons that it's so awesome
[1646.24s -> 1649.22s]  is because it's just seeing a ton of data
[1649.22s -> 1651.40s]  and that really makes your system a lot better.
[1651.40s -> 1654.06s]  So if you're looking for like the ultimate data set
[1654.06s -> 1657.28s]  to play around with your own ideas,
[1657.28s -> 1658.88s]  if you have enough compute, obviously,
[1658.88s -> 1661.40s]  then you should really look at this data set.
[1662.56s -> 1666.52s]  All right, any questions about up until this point?
[1668.12s -> 1671.84s]  No, all right.
[1672.88s -> 1676.04s]  So then we'll move on from late fusion
[1676.04s -> 1679.64s]  to kind of middle fusion, early fusion.
[1679.64s -> 1681.68s]  And this really is kind of the core
[1681.68s -> 1684.52s]  of what I think a lot of people in the field right now,
[1684.52s -> 1686.12s]  or if you're interested in getting in this field,
[1686.12s -> 1687.86s]  or if you're going to go into industry
[1687.86s -> 1689.78s]  and you're going to be using this stuff,
[1689.78s -> 1692.32s]  like this is what you should really understand.
[1692.32s -> 1696.48s]  And again, like the ideas sort of stack onto each other.
[1696.50s -> 1698.92s]  So I've kind of sequenced the slides
[1698.92s -> 1701.32s]  to give you an idea sort of how the scientists
[1701.32s -> 1703.52s]  kind of came up with the next step.
[1703.52s -> 1705.12s]  And you can really see the architecture
[1705.12s -> 1707.22s]  just get slightly more and more advanced,
[1707.22s -> 1709.04s]  but basically a lot of it is just more data
[1709.04s -> 1710.70s]  and more compute again.
[1711.64s -> 1715.80s]  So who knows how BERT works?
[1717.08s -> 1721.64s]  Everybody should raise their head in this.
[1721.64s -> 1726.64s]  So yeah, so BERT is kind of so canonical.
[1726.76s -> 1728.80s]  I think everybody kind of gets how BERT works, right?
[1728.80s -> 1731.88s]  So I don't think we need a real refresher,
[1731.88s -> 1734.00s]  but I think you can think.
[1734.00s -> 1736.52s]  And so the reason I have to slide is because
[1736.52s -> 1739.86s]  I want you to think about if you have a BERT model
[1739.86s -> 1741.78s]  and you have a bunch of images,
[1741.78s -> 1743.66s]  how are you going to turn that BERT model
[1743.66s -> 1745.12s]  into something multimodal?
[1746.44s -> 1749.96s]  So there are a bunch of like obvious things you could do
[1749.96s -> 1751.92s]  given the kind of features I told you about
[1751.92s -> 1753.30s]  in the sort of fusion process.
[1753.30s -> 1755.48s]  So, you know, how are you going to do that?
[1756.62s -> 1759.52s]  Does anybody want to like say something?
[1764.26s -> 1765.92s]  Like if you're doing classification,
[1765.92s -> 1768.64s]  you can take the C-lessed opening from BERT
[1768.64s -> 1771.44s]  and then just concatenate it to whatever encoder,
[1771.44s -> 1773.84s]  like maybe an ANN or whatever you're training
[1773.84s -> 1777.60s]  on the data, concatenate it and then train it.
[1778.46s -> 1781.16s]  So you can take the ConfNet features
[1781.16s -> 1783.84s]  and a classifier token from BERT, concatenate them
[1783.84s -> 1787.44s]  and then classify for like a cat or something like that
[1787.44s -> 1790.10s]  or whatever the thing is you're interested in, yeah.
[1790.10s -> 1791.96s]  Yeah, so that's one thing that you could also
[1791.96s -> 1794.60s]  like take the ConfNet features and like give them
[1794.60s -> 1798.12s]  to the BERT model in lots of different ways, right?
[1798.12s -> 1799.56s]  We can use the region features.
[1799.56s -> 1804.12s]  So I think a lot of people when BERT came out
[1804.12s -> 1806.72s]  who were working in vision and language processing
[1806.76s -> 1808.04s]  were thinking exactly about, okay,
[1808.04s -> 1810.16s]  so do we do like middle fusion, late fusion?
[1810.16s -> 1811.18s]  Do we do early fusion?
[1811.18s -> 1813.44s]  How do we do the fusion?
[1813.44s -> 1816.68s]  And so there were a lot of papers all coming out
[1816.68s -> 1818.16s]  basically at around the same time
[1818.16s -> 1821.12s]  where people were doing versions of this.
[1821.12s -> 1823.06s]  So BERT was really kind of the innovation
[1823.06s -> 1825.14s]  and then everybody sort of just plugged it
[1825.14s -> 1825.98s]  into their own thing
[1825.98s -> 1828.36s]  because of plugging phase transformers and things like that.
[1828.36s -> 1833.32s]  So the first thing is visual BERT.
[1833.32s -> 1834.78s]  This was one of the very early ones
[1834.82s -> 1836.30s]  where you have this image
[1836.30s -> 1839.64s]  and people would do object detection on this.
[1839.64s -> 1841.78s]  So you get like a hat and a racket and a shirt
[1841.78s -> 1842.86s]  and things like that.
[1842.86s -> 1845.28s]  So you can just really take these features
[1845.28s -> 1849.16s]  and then plug them into your transformer model
[1849.16s -> 1852.78s]  and then you try to like recover the features.
[1852.78s -> 1854.50s]  And so this really is probably
[1854.50s -> 1856.50s]  like the simplest way to do it, right?
[1857.58s -> 1860.98s]  And so this is what we call a single stream architecture
[1860.98s -> 1863.38s]  where you have all of these kind of concatenating
[1863.60s -> 1865.02s]  the original input features
[1865.02s -> 1867.98s]  and then putting them through the same transformer.
[1867.98s -> 1868.98s]  What you can also do,
[1868.98s -> 1872.34s]  and that's something that this model called VILBERT did
[1872.34s -> 1874.66s]  is where you have two different streams.
[1874.66s -> 1878.26s]  So you essentially have these two parallel transformers
[1878.26s -> 1881.66s]  but at every layer you kind of give them
[1881.66s -> 1883.14s]  cross-attention, right?
[1883.14s -> 1884.98s]  So, or coattention as they call it.
[1884.98s -> 1885.82s]  That is basically like,
[1885.82s -> 1888.06s]  so you just make sure you have an attention map
[1888.06s -> 1888.90s]  that spans both
[1888.90s -> 1891.06s]  and then you just do your full normal
[1891.06s -> 1893.14s]  transformer layer again.
[1894.02s -> 1897.18s]  So this you can train just like your regular BERT, right?
[1897.18s -> 1902.18s]  So you have your mass language model here
[1902.18s -> 1904.50s]  and here you do sort of some equivalent of that.
[1904.50s -> 1907.34s]  And then you also have your next sentence prediction
[1907.34s -> 1910.98s]  which you probably remember from your BERT lecture.
[1910.98s -> 1912.58s]  But instead here we're saying, okay
[1912.58s -> 1915.64s]  is this image aligned with this piece of text or not?
[1916.98s -> 1918.26s]  There's also LexMERT.
[1918.26s -> 1920.02s]  I mean, I could go on forever.
[1920.02s -> 1922.06s]  There are like a hundred papers that came out
[1922.10s -> 1923.66s]  that did this all at the same time.
[1923.66s -> 1926.98s]  So LexMERT had a different cross-modal output encoder,
[1926.98s -> 1928.78s]  a bunch of different ways
[1928.78s -> 1930.90s]  of encoding the positional information, right?
[1930.90s -> 1931.74s]  So you could say, okay
[1931.74s -> 1933.32s]  I just have a bunch of bounding boxes
[1933.32s -> 1934.16s]  that are featurized
[1934.16s -> 1936.62s]  but I don't care about where they are in the image.
[1936.62s -> 1940.62s]  So it's just kind of like just a bag of bounding boxes
[1940.62s -> 1941.98s]  or you could say, I found it here.
[1941.98s -> 1943.90s]  Like this is the particular like top left
[1943.90s -> 1945.58s]  and bottom right coordinate.
[1945.58s -> 1948.18s]  And that's what you featurize into your network.
[1949.18s -> 1952.94s]  You can also do something even dumber.
[1952.94s -> 1955.14s]  And I can say that because this is my paper.
[1956.50s -> 1958.86s]  Where you just take the image itself,
[1958.86s -> 1960.86s]  you put it through a ResNet
[1960.86s -> 1962.94s]  and then you do a little bit of pooling
[1962.94s -> 1964.66s]  on the final feature maps
[1964.66s -> 1967.30s]  and you just give those feature maps to BERT.
[1968.50s -> 1970.14s]  And so you then need to distinguish
[1970.14s -> 1972.66s]  between like your text segment embeddings, right?
[1972.66s -> 1975.76s]  And your vision segment embeddings.
[1976.52s -> 1978.96s]  But so this actually works surprisingly well.
[1978.96s -> 1982.20s]  You don't have to do any additional training.
[1982.20s -> 1983.88s]  You can just take BERT out of the box.
[1983.88s -> 1985.04s]  Initially you freeze it.
[1985.04s -> 1987.68s]  You learn to project into BERT token space.
[1987.68s -> 1989.48s]  Then you unfreeze your ResNet.
[1989.48s -> 1991.64s]  And then finally you unfreeze your BERT.
[1991.64s -> 1993.92s]  And now you have a very good multimodal classifier
[1993.92s -> 1995.38s]  on the problem you care about.
[1995.38s -> 1997.48s]  So a lot of these other papers
[1997.48s -> 1999.82s]  they're doing what they call multimodal pre-training
[1999.82s -> 2002.28s]  where first you have a BERT model and a ResNet.
[2002.28s -> 2004.48s]  So they're kind of unimodally pre-trained
[2004.52s -> 2005.90s]  and then you cobble them together.
[2005.90s -> 2008.00s]  And then you have a multimodal
[2008.00s -> 2009.96s]  sort of intermediary pre-training step
[2009.96s -> 2012.48s]  before you fine tune it on the problem you care about.
[2012.48s -> 2013.78s]  And what we showed here
[2013.78s -> 2015.68s]  is that you don't really need that actually
[2015.68s -> 2016.52s]  in many cases.
[2016.52s -> 2018.42s]  So it's a very strong baseline.
[2019.76s -> 2022.90s]  You can also go to the pixel level completely.
[2022.90s -> 2025.48s]  So that's what they did in this other paper
[2025.48s -> 2027.20s]  called Pixel BERT where they,
[2027.20s -> 2030.08s]  it's basically exactly MMBT.
[2030.08s -> 2032.44s]  So the previous supervised one
[2032.44s -> 2035.24s]  but here they do do the multimodal pre-training step
[2035.24s -> 2037.84s]  and show that I think for BQA it helps a little bit.
[2038.84s -> 2042.08s]  So there are many of these BERTs
[2042.08s -> 2044.20s]  doing sort of visual things.
[2044.20s -> 2046.68s]  People really tried everything.
[2046.68s -> 2048.20s]  Here's another one called Uniter
[2048.20s -> 2050.96s]  where they added a bunch of different losses.
[2050.96s -> 2054.04s]  We can really talk about this for a very long time.
[2054.04s -> 2055.04s]  We're not gonna do that.
[2055.04s -> 2056.80s]  I'm just gonna kind of talk you through
[2056.80s -> 2058.56s]  some of the more interesting ones.
[2058.56s -> 2061.30s]  So this one I think is quite interesting built
[2061.30s -> 2063.42s]  because here this is really the first instance
[2063.42s -> 2067.06s]  where we are completely gone from ConfNet features.
[2067.06s -> 2069.98s]  So we don't do any pre-processing on the image,
[2069.98s -> 2071.10s]  no region features,
[2071.10s -> 2072.86s]  no backbone that it featurizes
[2072.86s -> 2075.60s]  the parts of the image we care about.
[2075.60s -> 2077.30s]  We just have these patches of the image.
[2077.30s -> 2080.32s]  So really in the grid we flatten those patches.
[2080.32s -> 2083.30s]  We just pump them into the transformer straight away.
[2083.30s -> 2084.90s]  So this really is like sort of BERT
[2084.90s -> 2087.02s]  and VIT together in one model
[2087.02s -> 2088.94s]  and this worked really very well.
[2089.86s -> 2092.30s]  So that's been the trend.
[2092.30s -> 2095.02s]  So here's a nice very long list
[2095.02s -> 2097.46s]  of all of these different models and what they do.
[2097.46s -> 2100.30s]  And so really the distinctions are just in
[2100.30s -> 2102.10s]  what is the text encoder that you use?
[2102.10s -> 2105.94s]  So do you use BERT or something fancier or better, Roberta?
[2106.94s -> 2108.38s]  What is your vision encoder?
[2108.38s -> 2111.08s]  So in many cases you have these region features.
[2111.08s -> 2113.54s]  So you would do an RCNN style thing
[2113.54s -> 2115.86s]  or you could just do a ResNet or a VIT.
[2115.86s -> 2117.60s]  You have different kinds of fusion.
[2117.60s -> 2120.52s]  So either single or dual stream as we talked about, right?
[2120.52s -> 2123.02s]  So Visual BERT or VIL BERT.
[2123.02s -> 2124.28s]  Different pre-training tasks.
[2124.28s -> 2128.32s]  So mass language modeling, image text matching.
[2128.32s -> 2131.76s]  There's a bunch of like funkier ones you can do.
[2131.76s -> 2135.36s]  So and then finally you can do multimodal pre-training
[2135.36s -> 2136.82s]  on all of these different data sets
[2136.82s -> 2138.28s]  that have aligned data.
[2139.44s -> 2141.88s]  So you are probably wondering like, okay,
[2141.88s -> 2144.50s]  so what is really the interesting difference
[2144.50s -> 2146.52s]  between a lot of these?
[2147.16s -> 2149.54s]  So I have another recommended paper
[2149.54s -> 2151.12s]  that if you're interested in this space
[2151.12s -> 2152.24s]  you should really take a look at.
[2152.24s -> 2154.56s]  It's also a really well done paper
[2154.56s -> 2159.32s]  where they unmask multimodal pre-training.
[2159.32s -> 2161.38s]  So basically they say,
[2161.38s -> 2164.34s]  if you take all of these little model inventions
[2164.34s -> 2166.32s]  and you train these different models
[2166.32s -> 2169.52s]  on exactly the same data in exactly the same way,
[2169.52s -> 2171.96s]  it turns out that they're all basically the same.
[2172.96s -> 2176.52s]  So that's a lot of kind of wasted effort
[2176.52s -> 2177.48s]  on the part of the field
[2177.48s -> 2179.60s]  because everybody is saying, well, my model is better
[2179.60s -> 2181.08s]  but it's actually just because you trained it
[2181.08s -> 2182.64s]  on different data.
[2182.64s -> 2186.04s]  There's no real sort of model innovation
[2186.04s -> 2188.42s]  going on in a lot of these things.
[2188.42s -> 2189.92s]  So I don't mean to sound discouraging
[2189.92s -> 2193.88s]  or anything like that, but I think that's why this paper
[2193.88s -> 2195.40s]  is really nice and really important
[2195.40s -> 2198.88s]  is because it just shows us what really matters.
[2199.88s -> 2202.92s]  So this is also work that I did myself
[2202.92s -> 2206.28s]  called Flava with my team
[2206.28s -> 2210.00s]  where we wanted to take these ideas really to the limit.
[2210.00s -> 2212.92s]  So a lot of the things that you've seen now,
[2212.92s -> 2214.56s]  so the visual birds and the VIL birds
[2214.56s -> 2215.40s]  and the things like that,
[2215.40s -> 2217.08s]  they're all about multimodal questions.
[2217.08s -> 2219.44s]  So how can we do visual question answering
[2219.44s -> 2220.42s]  or something like that
[2220.42s -> 2222.30s]  where we just have these two modalities,
[2222.30s -> 2223.48s]  we only care about problems
[2223.48s -> 2225.76s]  that always involve these two modalities.
[2225.76s -> 2227.12s]  And where we want to go,
[2227.16s -> 2229.40s]  and this is kind of the basic premise
[2229.40s -> 2231.74s]  I think of foundation models in general,
[2231.74s -> 2234.28s]  is that we have one model to rule them all.
[2234.28s -> 2236.12s]  So this one model can consume data
[2236.12s -> 2237.76s]  from all of these different modalities
[2237.76s -> 2239.22s]  and it can synthesize
[2239.22s -> 2240.76s]  across all of these different modalities
[2240.76s -> 2243.44s]  and then do useful things with that information.
[2244.32s -> 2247.32s]  So with Flava, that's exactly what we tried to build.
[2247.32s -> 2249.60s]  So we wanted to have one foundation model
[2249.60s -> 2251.06s]  that is good at vision and language
[2251.06s -> 2254.04s]  and computer vision and natural language processing
[2254.04s -> 2255.24s]  is jointly pre-trained
[2255.24s -> 2256.88s]  on all of these different data sources.
[2257.48s -> 2259.32s]  So it's also trained on just CC News,
[2259.32s -> 2261.96s]  Common Crawl and BookCorpus.
[2261.96s -> 2263.36s]  So it's very good at the sort of things
[2263.36s -> 2265.20s]  you would expect bird to be good at.
[2265.20s -> 2267.38s]  It's trained on ImageNet for image data.
[2267.38s -> 2269.40s]  So it's good at the things that you would expect
[2269.40s -> 2271.60s]  as the kind of basic image model to be good at.
[2271.60s -> 2273.88s]  And then you have this PMD dataset
[2273.88s -> 2278.88s]  that we created out of publicly available image text pairs
[2278.88s -> 2279.96s]  that we also train it on.
[2279.96s -> 2281.96s]  So this PMD dataset is really just,
[2281.96s -> 2284.32s]  if you take all the datasets that were ever created
[2284.36s -> 2286.84s]  that have image text pairs that are publicly available.
[2286.84s -> 2290.20s]  So unfortunately, the clip data and the Google aligned data
[2290.20s -> 2292.36s]  and all of these datasets, they haven't been open source.
[2292.36s -> 2295.04s]  So this is before Lion.
[2295.04s -> 2297.84s]  So now there's a good alternative to this.
[2297.84s -> 2299.54s]  But so this PMD dataset,
[2301.04s -> 2302.98s]  if you combine all of these image text pairs,
[2302.98s -> 2304.42s]  you get 70 million of them.
[2304.42s -> 2306.32s]  So that's still pretty decent size.
[2306.32s -> 2308.76s]  And then you can take all of this data
[2308.76s -> 2310.52s]  basically to solve all of these problems
[2310.52s -> 2312.60s]  that we know we care about in these different fields.
[2312.60s -> 2314.36s]  So you can do multimodal reasoning,
[2314.36s -> 2315.74s]  you can do language understanding,
[2315.74s -> 2317.52s]  you can do visual recognition,
[2317.52s -> 2319.40s]  all with exactly the same model.
[2319.40s -> 2321.24s]  And that's a very powerful idea.
[2321.24s -> 2324.48s]  I think if you work at a company like Facebook,
[2324.48s -> 2325.80s]  you don't want to have different models
[2325.80s -> 2327.36s]  for all kinds of different things.
[2327.36s -> 2328.56s]  You want to have one model
[2328.56s -> 2330.36s]  that you can really use for everything
[2330.36s -> 2333.22s]  that's gonna really make your life a lot easier.
[2334.12s -> 2337.80s]  So the exact architecture here is that on the one hand,
[2337.80s -> 2340.46s]  we have this image encoder where we take the image,
[2340.46s -> 2341.78s]  we encode it as patches,
[2341.78s -> 2344.62s]  and we just do what we call mass image modeling,
[2344.62s -> 2346.36s]  but it's basically mass language modeling
[2346.36s -> 2349.08s]  and then just on the image tokens.
[2350.26s -> 2351.80s]  And then on the other side,
[2351.80s -> 2356.52s]  we have the mass language modeling on the language.
[2356.52s -> 2358.50s]  So your regular sort of bird thing.
[2358.50s -> 2360.34s]  And then we have a multimodal part
[2360.34s -> 2363.02s]  where all of this information gets combined.
[2363.02s -> 2367.18s]  So we have a mass multimodal modeling loss term
[2367.18s -> 2369.02s]  where you can also do image text matching.
[2369.06s -> 2371.58s]  So this is like your bird next sentence prediction thing.
[2371.58s -> 2373.74s]  And then we also have a global contrastive loss,
[2373.74s -> 2375.50s]  which is exactly like a clip.
[2375.50s -> 2377.14s]  So if you do all of this stuff,
[2377.14s -> 2380.46s]  it's just all transformers all the way down.
[2380.46s -> 2382.42s]  It's sort of a very elegant way, I think,
[2382.42s -> 2384.98s]  to combine a lot of this information.
[2384.98s -> 2385.94s]  And when you do that,
[2385.94s -> 2387.62s]  you get something that can really do
[2387.62s -> 2389.24s]  a lot of things very well.
[2389.24s -> 2391.56s]  So we're not gonna talk about that table
[2391.56s -> 2393.10s]  is just way too many numbers,
[2393.10s -> 2394.84s]  but so just trust me,
[2394.84s -> 2398.02s]  we were pretty thorough generating the table here.
[2399.54s -> 2401.62s]  So over 35 different tasks,
[2401.62s -> 2404.46s]  if you compare FLAVA to all kinds of different ablations
[2404.46s -> 2406.10s]  in terms of clip models,
[2406.10s -> 2408.04s]  then this is just a much better way
[2408.04s -> 2410.26s]  to get to this information.
[2410.26s -> 2411.86s]  So I think this is a nice example
[2411.86s -> 2415.06s]  of where we're probably gonna go with the field
[2415.06s -> 2416.48s]  in the near future.
[2417.38s -> 2419.30s]  So the other trend that we see
[2419.30s -> 2420.94s]  very obviously in the field right now
[2420.94s -> 2423.66s]  is that everybody cares about generative models.
[2423.66s -> 2427.74s]  So language models and image generative models,
[2427.78s -> 2430.04s]  there's just a trend where we want to be generative,
[2430.04s -> 2430.88s]  we want to move away
[2430.88s -> 2433.62s]  from this contrastive discriminative stuff
[2433.62s -> 2436.30s]  to the more interesting, more richer representations
[2436.30s -> 2441.22s]  maybe that you get out of generating sequences or images.
[2441.22s -> 2444.22s]  So this SIM-VLM paper was one of the first ones
[2444.22s -> 2446.18s]  where they really had this separate decoder
[2446.18s -> 2449.50s]  that was trying to generate or kind of complete captions,
[2449.50s -> 2453.66s]  which they showed gives you a lot richer representations.
[2453.66s -> 2455.82s]  I think this is actually the current state of the art now,
[2455.86s -> 2456.94s]  it's called Koka.
[2457.98s -> 2459.86s]  So a lot of these models,
[2459.86s -> 2462.26s]  they all again look very similar,
[2462.26s -> 2463.54s]  but in this case now we're starting
[2463.54s -> 2465.30s]  to really see these text decoders.
[2465.30s -> 2467.04s]  So initially with clip,
[2467.04s -> 2469.22s]  I think that's also what they were trying to go for,
[2469.22s -> 2470.88s]  like OpenAI being a company
[2470.88s -> 2472.86s]  that really likes generative models,
[2472.86s -> 2474.30s]  but they couldn't really get it to work.
[2474.30s -> 2476.58s]  And I think so it took as a while as a field
[2476.58s -> 2479.32s]  to really figure out how to do this the right way.
[2480.90s -> 2483.68s]  And so right now we're really kind of
[2483.68s -> 2485.46s]  in the age of language models, right?
[2486.26s -> 2488.98s]  So one of the interesting things you can do
[2488.98s -> 2492.06s]  with language models is just keep them frozen
[2492.06s -> 2495.14s]  and then learn how to project into the language models.
[2495.14s -> 2498.30s]  So the MMBT architecture I talked about
[2498.30s -> 2499.74s]  where we had this BERT model,
[2499.74s -> 2500.94s]  we kind of kept it frozen
[2500.94s -> 2504.90s]  and we learned to project into the BERT token space.
[2504.90s -> 2506.42s]  You can do exactly the same thing,
[2506.42s -> 2510.30s]  but then with a much fancier model or something like T5,
[2510.30s -> 2512.14s]  even where you just have an encoder decoder
[2512.14s -> 2514.74s]  or some kind of generative part of this,
[2514.74s -> 2516.90s]  you keep that thing frozen
[2516.90s -> 2519.50s]  and then you learn to project into the token space
[2519.50s -> 2521.62s]  of that frozen language model
[2521.62s -> 2524.94s]  and then you can do lots of fun stuff it turns out.
[2524.94s -> 2526.58s]  So what they show in this paper
[2526.58s -> 2528.88s]  is that you then get few shot learners.
[2528.88s -> 2531.14s]  So all of the things you see with GPT-3
[2531.14s -> 2532.82s]  where you can just give it some kind of
[2532.82s -> 2534.06s]  in context examples
[2534.06s -> 2538.52s]  and it's gonna figure out binding kind of on the fly.
[2538.52s -> 2541.34s]  So it says like, this is a DEX and this is a BLICKED.
[2541.34s -> 2542.80s]  So what is this?
[2542.84s -> 2545.48s]  And then it gives you the answer that it's a DEX.
[2545.48s -> 2547.04s]  So it really learns in context
[2547.04s -> 2549.48s]  how you decide the feature mappings,
[2549.48s -> 2552.52s]  which is really kind of solving the grounding problem
[2552.52s -> 2556.10s]  that a lot of this multimodal stuff started with.
[2556.10s -> 2557.26s]  So I think that's very cool.
[2557.26s -> 2562.00s]  And then probably one of the coolest papers right now
[2562.00s -> 2564.08s]  or models right now that you might've heard of
[2564.08s -> 2567.52s]  if you follow the field is Flamingo out of DeepMind
[2567.52s -> 2570.26s]  where they take a Chinchilla language model.
[2571.14s -> 2573.68s]  And so this is really an optimal language model.
[2573.68s -> 2576.26s]  And now you have this vision encoder
[2576.26s -> 2579.46s]  that encodes multiple different images
[2579.46s -> 2581.46s]  that you can then do reasoning over
[2581.46s -> 2583.20s]  and then kind of auto-complete.
[2583.20s -> 2586.86s]  So what this gets you is just a much more powerful model
[2586.86s -> 2589.42s]  because you can do your generative
[2589.42s -> 2590.78s]  over lots of different images.
[2590.78s -> 2593.46s]  So it's really like stepwise, you can see it, right?
[2593.46s -> 2595.58s]  We started off with very simple transformers
[2595.58s -> 2597.02s]  and now we're actually at something
[2597.02s -> 2599.32s]  that is starting to get pretty complicated
[2599.32s -> 2601.20s]  because we have these building blocks
[2601.20s -> 2603.40s]  like a receiver resampler
[2603.40s -> 2605.96s]  where we have a bunch of different images
[2605.96s -> 2607.00s]  that we featurize
[2607.00s -> 2609.36s]  and now we need to compress the information
[2609.36s -> 2610.90s]  because sometimes we have three images,
[2610.90s -> 2612.28s]  sometimes we have five images.
[2612.28s -> 2614.12s]  So we wanna make sure that we can compress it
[2614.12s -> 2616.16s]  so that it's always ready for consumption
[2616.16s -> 2620.48s]  by the next layer of the language model.
[2620.48s -> 2622.12s]  And then, so this paper again
[2622.12s -> 2623.48s]  is a really good paper to read
[2623.48s -> 2625.92s]  because they actually, so this is not me,
[2625.92s -> 2626.88s]  this is not my code.
[2626.88s -> 2628.20s]  This comes from the actual paper.
[2628.20s -> 2630.60s]  So they just have the diagram together with the code
[2630.60s -> 2632.94s]  so that you can really understand what it's doing,
[2632.94s -> 2635.62s]  which I think is really great.
[2636.48s -> 2640.90s]  And so once you have your receiver resampling step,
[2640.90s -> 2644.12s]  what you then do is you do gated cross-attention.
[2644.12s -> 2646.26s]  This is how you implement it.
[2646.26s -> 2648.50s]  And so this gated cross-attention,
[2648.50s -> 2652.28s]  you do that before your frozen language model layer.
[2652.28s -> 2655.54s]  So you really just have a frozen Chinchilla language model
[2655.54s -> 2657.88s]  and you learn to kind of modulate the information
[2658.40s -> 2660.08s]  that goes into that language model.
[2660.08s -> 2662.32s]  You propagate the gradients all the way back.
[2662.32s -> 2664.12s]  You just don't update the language model.
[2664.12s -> 2665.76s]  So you're really kind of trying to figure out
[2665.76s -> 2668.12s]  like how am I gonna design my signal
[2668.12s -> 2671.08s]  so that my language model can do the most with it?
[2671.08s -> 2672.98s]  How am I gonna combine the information?
[2672.98s -> 2675.48s]  So you'll notice that now we do it before the layer.
[2675.48s -> 2676.56s]  In a lot of this other stuff,
[2676.56s -> 2678.24s]  you would do the attention after the layer,
[2678.24s -> 2679.76s]  but here you do it before.
[2681.44s -> 2684.70s]  So Karpathy, I think more than 10 years ago
[2684.70s -> 2687.00s]  had this image with Barack Obama
[2687.00s -> 2689.08s]  kind of setting his foot here on the scale
[2689.08s -> 2692.94s]  to make somebody think they're a lot heavier
[2692.94s -> 2694.52s]  than they really are.
[2694.52s -> 2696.46s]  So this is obviously funny to us,
[2697.48s -> 2699.40s]  but not to an AI system, I think,
[2699.40s -> 2702.20s]  unless it really understands the scene.
[2702.20s -> 2704.76s]  And so that's why Karpathy at the time said
[2704.76s -> 2706.76s]  this would be a really good visual Turing test.
[2706.76s -> 2708.78s]  Like if a system can figure this out,
[2708.78s -> 2711.06s]  then it's actually really smart.
[2711.06s -> 2713.06s]  And so obviously it's been a bit of a challenge
[2713.06s -> 2714.52s]  for everybody working in the field then
[2714.52s -> 2716.56s]  to get something that actually works on this.
[2717.00s -> 2720.10s]  So Flamingo, as it turns out, kind of gets the joke.
[2721.54s -> 2724.72s]  But yeah, so it's a bit unclear if it really gets the joke
[2724.72s -> 2726.24s]  because if you read this conversation,
[2726.24s -> 2727.76s]  it's sort of kind of getting steered
[2727.76s -> 2728.84s]  in the right direction, right?
[2728.84s -> 2731.28s]  But at least we're making progress,
[2731.28s -> 2732.48s]  let's put it that way.
[2733.60s -> 2736.16s]  And then, so in Flamingo,
[2736.16s -> 2737.64s]  you still have a lot of moving parts,
[2737.64s -> 2740.04s]  but you can really take this almost to the full extreme
[2740.04s -> 2742.16s]  where you try to freeze almost everything
[2742.16s -> 2744.56s]  and you just want to learn this kind of mapping
[2744.60s -> 2747.36s]  between your image encoder and your language model
[2747.36s -> 2748.48s]  or your image encoder
[2748.48s -> 2750.72s]  and your encoder-decoder architecture.
[2750.72s -> 2752.92s]  And all you really do is just the projection
[2752.92s -> 2754.44s]  between the two, right?
[2754.44s -> 2757.06s]  So there's this nice model called Blip2,
[2757.06s -> 2759.20s]  where they experiment with like OPT
[2759.20s -> 2760.92s]  for the language model and FlanT5
[2760.92s -> 2763.14s]  for the encoder-decoder architecture.
[2763.14s -> 2765.04s]  And this just gives you amazing results.
[2765.04s -> 2769.08s]  It gives you really complex captions and things like that
[2769.08s -> 2772.36s]  without any real direct supervision on the captions itself,
[2772.36s -> 2773.90s]  which is pretty impressive, I think.
[2774.10s -> 2775.66s]  So that just shows you the power
[2775.66s -> 2777.66s]  of language models in general.
[2779.58s -> 2781.46s]  So here I had some examples.
[2781.46s -> 2783.38s]  So it can really do different things
[2783.38s -> 2785.16s]  from captioning to reasoning
[2785.16s -> 2790.02s]  to visual question answering to location detection.
[2790.02s -> 2792.80s]  So you can have a long conversation with this system.
[2792.80s -> 2794.46s]  This really is kind of the future
[2794.46s -> 2795.34s]  where we're going, right?
[2795.34s -> 2796.68s]  Where we're gonna have a chat GPT,
[2796.68s -> 2799.68s]  but it's also gonna be able to see the world in a way.
[2801.22s -> 2803.50s]  And so I think an interesting thing,
[2804.04s -> 2806.14s]  so you've probably heard of like chain of thought prompting
[2806.14s -> 2808.14s]  and things like that, where you ask the language model,
[2808.14s -> 2810.58s]  like let's think step-by-step
[2810.58s -> 2814.14s]  and you can tell a vision and language model
[2814.14s -> 2818.42s]  generate a rationale for why something might be the case.
[2818.42s -> 2821.26s]  So you generate a potential explanation
[2821.26s -> 2823.22s]  for what your answer might be.
[2823.22s -> 2825.94s]  And then after that, you ask it to answer your question.
[2825.94s -> 2827.68s]  And it turns out that if you do that
[2827.68s -> 2829.78s]  sort of multimodal chain of thought prompting,
[2829.78s -> 2832.02s]  then the system gets much better.
[2832.02s -> 2835.60s]  And so this was like the new state of the art on science,
[2835.60s -> 2837.82s]  QA or a benchmark like that,
[2837.82s -> 2840.96s]  just because it learns to unpack the information.
[2840.96s -> 2843.62s]  And so I think we're really as a field
[2843.62s -> 2846.82s]  just starting to figure out what the potential is of this.
[2846.82s -> 2849.14s]  And I think this paper is where they also show
[2849.14s -> 2851.18s]  that multimodal chain of thought prompting
[2851.18s -> 2853.54s]  really gets you pretty amazing results.
[2853.54s -> 2857.08s]  And they show very nice results on Raven matrices
[2857.08s -> 2861.02s]  and like very complicated kind of IQ tests sort of things
[2861.34s -> 2863.36s]  that humans are supposed to be really good at,
[2863.36s -> 2864.90s]  but you have to be a pretty smart human
[2864.90s -> 2866.34s]  to really be good at this.
[2866.34s -> 2868.70s]  And this system just nails it.
[2868.70s -> 2871.70s]  So we're making super fast progress.
[2871.70s -> 2874.64s]  And we started off from a very simple bird model
[2874.64s -> 2876.38s]  that was able to look at some pictures
[2876.38s -> 2877.46s]  and now we're getting to these
[2877.46s -> 2880.54s]  very sophisticated foundation models.
[2880.54s -> 2882.46s]  So that was my short history
[2882.46s -> 2884.70s]  of multimodal foundation models.
[2886.36s -> 2888.74s]  So how much time do I have left?
[2888.78s -> 2891.34s]  So after five, 15 to 25 minutes.
[2891.34s -> 2892.34s]  All right, okay.
[2892.34s -> 2893.18s]  Plenty of time.
[2895.62s -> 2897.12s]  Yeah, please, questions.
[2900.46s -> 2902.98s]  Can we do much pre-processing of images
[2902.98s -> 2904.06s]  for these models anymore?
[2904.06s -> 2905.42s]  So I noticed one of the images
[2905.42s -> 2908.82s]  that just looked like they were boxes,
[2908.82s -> 2910.70s]  like square images passed through,
[2910.70s -> 2913.78s]  kind of no sense of shape.
[2913.78s -> 2914.90s]  Yeah, yeah.
[2914.90s -> 2918.38s]  So I think the history of computer vision
[2918.90s -> 2920.10s]  has been very similar to the history
[2920.10s -> 2921.40s]  of natural language processing,
[2921.40s -> 2923.26s]  where we thought we needed all of this structure
[2923.26s -> 2924.78s]  and all of these different things.
[2924.78s -> 2926.74s]  And it turns out you can just throw it all away
[2926.74s -> 2929.70s]  and just have a big transformer over the patches.
[2931.06s -> 2932.10s]  Sorry, yes.
[2933.10s -> 2935.26s]  It's CS 231 in one minute.
[2935.26s -> 2936.42s]  Take your time.
[2936.42s -> 2941.74s]  You mentioned a couple of times like model two
[2941.74s -> 2943.86s]  or was that me?
[2943.86s -> 2945.22s]  Yeah, yeah, sorry.
[2945.22s -> 2946.82s]  I should have explained that better maybe.
[2946.82s -> 2951.34s]  So it just means that we are not updating the weights.
[2951.34s -> 2955.94s]  So like if we go to this here, I think is a nice example.
[2955.94s -> 2959.58s]  So we have frozen self-attention.
[2959.58s -> 2962.30s]  So that just means that when we do a forward pass,
[2962.30s -> 2964.62s]  we go all the way to whatever we want to predict.
[2964.62s -> 2967.24s]  We get some gradients, we take them all the way down,
[2967.24s -> 2970.82s]  but we only update the non-frozen layers, right?
[2970.82s -> 2972.96s]  So here the gradients actually do get updated,
[2972.96s -> 2974.54s]  but these just never change.
[2974.54s -> 2976.32s]  And so the reason you wanna do that
[2976.76s -> 2979.04s]  is because otherwise you're gonna drift way too far.
[2979.04s -> 2981.68s]  And so then you're gonna kind of destroy
[2981.68s -> 2984.04s]  all of the cool stuff your language model has learned
[2984.04s -> 2986.76s]  because you're just gonna focus on the small data set
[2986.76s -> 2988.22s]  that you're training it on.
[2988.22s -> 2990.86s]  So you wanna preserve the abilities of the language model,
[2990.86s -> 2992.40s]  but you want it to become good
[2992.40s -> 2993.96s]  at the thing you care about.
[2999.00s -> 2999.84s]  Other questions?
[3001.80s -> 3003.24s]  In terms of all that model fusion,
[3003.24s -> 3005.48s]  is there a benefit to doing like the earlier middle fusion
[3005.52s -> 3009.00s]  as opposed to only doing the late fusion, I think?
[3009.00s -> 3011.88s]  Yeah, so I mean, we're gonna talk about evaluation next,
[3011.88s -> 3014.00s]  but so it really depends on the tasks
[3014.00s -> 3015.44s]  that you care about.
[3015.44s -> 3019.24s]  And so I would say the earlier is always the better
[3019.24s -> 3020.84s]  if you can afford it.
[3020.84s -> 3023.96s]  And so like CLIP is very efficient to train,
[3023.96s -> 3025.82s]  it's very late fusion, right, at the very end.
[3025.82s -> 3027.04s]  So there's no interaction
[3027.04s -> 3028.72s]  between the different modalities.
[3029.76s -> 3031.64s]  And so that's really good
[3031.64s -> 3033.28s]  if you want to be very efficient.
[3033.28s -> 3035.10s]  And if you wanna be like for training,
[3035.68s -> 3037.54s]  I think it's much nicer, right?
[3037.54s -> 3039.70s]  But if you want to have a richer understanding
[3039.70s -> 3041.58s]  of the multimodal signal,
[3041.58s -> 3043.38s]  then you want to do earlier fusion.
[3044.86s -> 3047.02s]  So it's, yeah, it's always a trade-off.
[3050.46s -> 3055.10s]  It seems that images are just a lot more data than text.
[3055.10s -> 3058.14s]  So how much more difficult are these to train?
[3058.14s -> 3062.46s]  And how much bigger does like the image processing
[3062.46s -> 3066.10s]  have to be compared to the language model?
[3066.10s -> 3070.66s]  Yeah, so images are more complex in a way,
[3070.66s -> 3074.58s]  but they're also kind of higher bandwidth representations,
[3074.58s -> 3077.30s]  right, so there's a lot of kind of like just pixels
[3077.30s -> 3079.78s]  that our brains just abstract away, right?
[3079.78s -> 3081.62s]  It's really about the scene that you're seeing
[3081.62s -> 3084.06s]  and like you're not really thinking too much
[3084.06s -> 3086.66s]  about the pixels themselves.
[3086.66s -> 3090.26s]  So like Jan LeCun likes to say that language
[3090.26s -> 3093.06s]  is just a kind of low bandwidth,
[3093.06s -> 3095.74s]  a proxy for a language of thought,
[3095.74s -> 3098.06s]  which is much richer and much higher bandwidth
[3098.06s -> 3101.36s]  and like he thinks probably visual, I'm not so sure.
[3102.26s -> 3106.78s]  But so, yeah, I don't think that there's necessarily
[3106.78s -> 3108.78s]  a difference between kind of the scaling laws
[3108.78s -> 3110.42s]  that you see in these systems.
[3112.10s -> 3113.86s]  Or at least we still have to figure that out.
[3113.86s -> 3116.46s]  We'll kind of talk about that towards the end as well.
[3116.46s -> 3117.30s]  Yeah.
[3119.30s -> 3123.50s]  Do these models also have certain social and culture bias,
[3123.50s -> 3125.74s]  just like the natural language model?
[3125.74s -> 3127.98s]  Oh yeah, they have terrible biases, yeah.
[3129.42s -> 3132.54s]  So yeah, some people are actually working on this
[3132.54s -> 3133.70s]  who are in this very room,
[3133.70s -> 3136.36s]  but so these models can be very racist
[3136.36s -> 3137.64s]  also in what they generate
[3137.64s -> 3140.18s]  or the kind of predictions they make.
[3140.18s -> 3144.18s]  So if you have an Asian basketball player
[3144.18s -> 3146.06s]  standing sort of like this with a basketball
[3146.66s -> 3147.50s]  very obviously there,
[3147.50s -> 3149.50s]  then the model will think that he's playing ping pong
[3149.50s -> 3150.50s]  because he's Asian.
[3151.58s -> 3152.42s]  I'm not joking.
[3155.62s -> 3159.30s]  So these models, yeah, just like all neural networks,
[3159.30s -> 3160.50s]  this is really a big problem
[3160.50s -> 3162.58s]  and one of the most interesting problems
[3162.58s -> 3164.74s]  that you should be working on if you're a student
[3164.74s -> 3166.42s]  and you want to make a difference is
[3166.42s -> 3168.98s]  how do we get these systems to be much better
[3168.98s -> 3170.34s]  at these sorts of things?
[3170.34s -> 3171.18s]  Yeah.
[3173.30s -> 3175.38s]  So in one of the examples you showed
[3175.38s -> 3178.26s]  that the model interprets from the content of an image.
[3178.26s -> 3181.02s]  So when we want to understand the content of a video,
[3181.02s -> 3183.62s]  so what actual challenges you might see
[3183.62s -> 3184.78s]  along this path
[3184.78s -> 3188.84s]  and what improvements we can make for this goal?
[3188.84s -> 3192.58s]  Yeah, so you're asking about the attention mess
[3192.58s -> 3193.54s]  sort of, right?
[3193.54s -> 3196.66s]  Yeah, so you can use the same idea for videos
[3196.66s -> 3198.10s]  and you just look at the video
[3198.10s -> 3200.22s]  and so these systems are so good
[3200.98s -> 3201.94s]  and now the object detectors are so good
[3201.94s -> 3205.06s]  you can really track objects kind of real time
[3205.06s -> 3207.30s]  as they go through your video
[3207.30s -> 3209.26s]  and so you can try to check how that aligns
[3209.26s -> 3211.96s]  with your attention mask in your model.
[3211.96s -> 3216.96s]  So a lot of videos I think are sort of interesting
[3217.14s -> 3219.14s]  but they're also not really interesting
[3219.14s -> 3222.22s]  because you can very often just sub-sample images
[3222.22s -> 3223.26s]  and solve the images
[3223.26s -> 3225.86s]  rather than having to deal with the complex video.
[3226.86s -> 3227.86s]  Good job.
[3229.22s -> 3230.70s]  All right, maybe one more question
[3230.70s -> 3232.62s]  and then we'll go do some evaluations.
[3233.62s -> 3236.38s]  So these multimodal models,
[3236.38s -> 3237.58s]  when you only provide,
[3237.58s -> 3239.42s]  let's say you only provided single-source media
[3239.42s -> 3241.70s]  so it's only text or vision,
[3241.70s -> 3243.34s]  how does it perform in that case?
[3243.34s -> 3246.90s]  Because it's obviously more geared for multimodal cases.
[3246.90s -> 3249.46s]  Yeah, so I mean, that's one of the giant shortcomings
[3249.46s -> 3250.54s]  of a lot of these models
[3250.54s -> 3253.66s]  is that they're really just built for multimodal stuff
[3253.66s -> 3256.98s]  and so what if I don't have an image, right?
[3256.98s -> 3260.50s]  And so, I mean, that's why we did Flava
[3260.50s -> 3261.70s]  because we want to have one model
[3261.70s -> 3263.86s]  that can do all of that stuff
[3263.86s -> 3266.06s]  and that's why in MMBT,
[3266.06s -> 3268.72s]  so the supervised multimodal bi-transformer,
[3268.72s -> 3270.46s]  we actually have an analysis of like
[3270.46s -> 3272.74s]  how robust is this model to missing images
[3272.74s -> 3274.72s]  or missing text.
[3274.72s -> 3277.14s]  But so I think a lot of folks
[3277.14s -> 3279.36s]  working on these early visual bird models
[3279.36s -> 3282.66s]  that were kind of myopically focused on VQA,
[3282.66s -> 3283.86s]  which is actually a great segue
[3283.86s -> 3286.26s]  to what I want to talk about next.
[3286.26s -> 3289.50s]  So it really depends on the task
[3289.50s -> 3291.34s]  that you care about, as I said, right?
[3291.34s -> 3294.46s]  And so I think if I'm going to tell you
[3294.46s -> 3295.46s]  about multimodality,
[3295.46s -> 3297.38s]  I also have to tell you how you're going to check
[3297.38s -> 3298.50s]  that the multimodal system
[3298.50s -> 3300.74s]  is actually good at multimodal things.
[3300.74s -> 3304.70s]  And so that's the topic of evaluation,
[3304.70s -> 3307.06s]  which actually is a super important topic.
[3307.06s -> 3308.02s]  And a lot of people,
[3308.02s -> 3310.64s]  they want to be cool and build big models,
[3310.68s -> 3312.34s]  but I think it should be way cooler
[3312.34s -> 3314.68s]  to do proper evaluation of these models,
[3314.68s -> 3316.30s]  especially if you're in academia
[3316.30s -> 3318.80s]  because you only have limited GPUs anyway, right?
[3318.80s -> 3321.14s]  So what can you do?
[3322.20s -> 3324.12s]  Sorry, I don't want to rub it in.
[3328.12s -> 3329.36s]  So how do you check?
[3329.36s -> 3333.40s]  Well, there's this amazing project.
[3333.40s -> 3335.00s]  So like ImageNet really changed
[3335.00s -> 3336.84s]  like the history of deep learning, I think,
[3336.84s -> 3338.80s]  and this other data set, COCO,
[3338.92s -> 3340.84s]  I think also really changed,
[3340.84s -> 3342.12s]  especially vision and language,
[3342.12s -> 3345.60s]  but also I think vision in general,
[3345.60s -> 3350.06s]  where they have just a bunch of main sort
[3350.06s -> 3351.04s]  of multimodal tasks.
[3351.04s -> 3353.56s]  So these images are very richly annotated
[3353.56s -> 3354.80s]  with all kinds of different things.
[3354.80s -> 3357.12s]  So like the segmentation of the objects,
[3357.12s -> 3358.56s]  the bounding boxes,
[3358.56s -> 3360.24s]  the labels of the bounding boxes,
[3360.24s -> 3364.04s]  they come at like a sort of a different pixel granularities
[3364.04s -> 3365.86s]  is a huge data set.
[3365.86s -> 3368.12s]  It's very fine-grained annotated
[3368.32s -> 3370.04s]  in terms of like the categories that it has,
[3370.04s -> 3374.32s]  and then you have five captions for each of these images.
[3374.32s -> 3377.12s]  And so this really was the first data set
[3377.12s -> 3378.96s]  that unlocked a lot of sort of vision
[3378.96s -> 3380.44s]  and language processing at scale,
[3380.44s -> 3383.04s]  because you had your picture and you had your caption,
[3383.04s -> 3384.44s]  and now you need to figure out,
[3384.44s -> 3386.96s]  okay, how do I give the right caption for this image?
[3386.96s -> 3388.60s]  So does image captioning work?
[3388.60s -> 3391.08s]  Can I retrieve, given some piece of text,
[3391.08s -> 3394.84s]  the right image or the image for the piece of text?
[3394.84s -> 3397.72s]  So there's a bunch of very impactful data sets
[3398.28s -> 3400.52s]  that do this stuff that we already talked about, LION,
[3400.52s -> 3402.80s]  but COCO really is the main one still,
[3402.80s -> 3404.56s]  I think that a lot of people kind of use
[3404.56s -> 3409.22s]  as the canonical instance of this data set category.
[3409.22s -> 3412.24s]  And then the other thing that people really care about
[3412.24s -> 3413.84s]  in vision and language processing
[3413.84s -> 3415.52s]  is visual question answering.
[3416.36s -> 3420.84s]  And so there really are a bunch of academic groups
[3420.84s -> 3423.78s]  who are or have been so focused on this task
[3423.78s -> 3425.80s]  that they didn't really care about anything else.
[3425.80s -> 3427.56s]  And that's why you see a lot of models
[3428.28s -> 3430.32s]  that are really optimized just for multimodal
[3430.32s -> 3431.88s]  and nothing else.
[3431.88s -> 3433.24s]  And you can see that kind of reflected
[3433.24s -> 3436.76s]  in the citation counts as of last night, 3 a.m.,
[3437.94s -> 3441.36s]  where, so VQA just has way more citations
[3441.36s -> 3444.84s]  than image captioning data sets, right?
[3444.84s -> 3447.52s]  So what you do here is you just have an image
[3447.52s -> 3450.16s]  and then people ask very simple questions,
[3450.16s -> 3453.16s]  so annotators, they ask these simple questions,
[3453.16s -> 3454.44s]  they give the answers,
[3454.44s -> 3457.48s]  and now we want to be able to answer these questions
[3458.36s -> 3459.20s]  with machines.
[3459.20s -> 3460.04s]  And as I alluded to earlier,
[3460.04s -> 3462.40s]  one of the kind of embarrassing backstories
[3462.40s -> 3464.52s]  of this data set was that the initial version
[3464.52s -> 3466.52s]  of the data set was actually found
[3466.52s -> 3470.68s]  to have images not really matter at all.
[3470.68s -> 3472.72s]  So you could just look at the question,
[3472.72s -> 3474.24s]  then it could have something like
[3474.24s -> 3476.98s]  how many slices of pizza are there?
[3476.98s -> 3479.04s]  And so, well, not in that particular case,
[3479.04s -> 3481.28s]  but in almost all of the data set,
[3481.28s -> 3482.94s]  the right answer for how much
[3482.94s -> 3484.56s]  or how many question was two.
[3485.44s -> 3487.78s]  So if you just predicted two to every how much
[3487.78s -> 3488.62s]  or how many question,
[3488.62s -> 3492.20s]  you got like 70% accuracy on the counting category.
[3492.20s -> 3496.64s]  So careful data set or evaluation benchmark design
[3496.64s -> 3498.04s]  is also really a skill,
[3498.04s -> 3500.04s]  and you really need to think about what you're doing.
[3500.04s -> 3502.20s]  You can't just like set some data aside
[3502.20s -> 3503.40s]  and evaluate it as long as you have
[3503.40s -> 3505.76s]  to really think about what you're doing.
[3505.76s -> 3507.96s]  And so there's GQA by Chris, actually,
[3507.96s -> 3510.40s]  which is also just, I think,
[3510.40s -> 3512.56s]  a better designed version of this data set, maybe.
[3512.56s -> 3514.98s]  So you might want to use that these days.
[3516.32s -> 3520.72s]  There are also kind of very targeted data sets
[3520.72s -> 3522.96s]  that really try to measure one particular thing.
[3522.96s -> 3524.36s]  And I think one of the things
[3524.36s -> 3527.12s]  we really want to get at with these models
[3527.12s -> 3529.36s]  is what we would call compositionality, right?
[3529.36s -> 3531.70s]  So we want to be able to really take the parts
[3531.70s -> 3533.64s]  and reason about the whole
[3533.64s -> 3535.08s]  and understand the relationships
[3535.08s -> 3536.32s]  between the different concepts.
[3536.32s -> 3539.36s]  So clever was a very clever data set
[3539.36s -> 3543.06s]  that was designed really to measure the compositionality,
[3543.06s -> 3545.20s]  both on the language side and on the vision side.
[3545.20s -> 3546.86s]  So you have to understand the relationships
[3546.86s -> 3549.92s]  between all of these different objects in the images.
[3549.92s -> 3551.82s]  So that's been a pretty impactful data set,
[3551.82s -> 3553.96s]  I think, for really forcing people
[3553.96s -> 3556.48s]  to think about compositionality.
[3556.48s -> 3561.32s]  But a lot of these data sets really had big problems.
[3561.32s -> 3565.24s]  So one of the problem is they were too easy.
[3565.24s -> 3567.12s]  So VQA is sort of like plateauing out.
[3567.12s -> 3568.80s]  We can talk about that a little bit too.
[3568.80s -> 3570.04s]  Wasn't really realistic.
[3570.04s -> 3571.96s]  So you could solve VQA
[3571.96s -> 3574.88s]  and that's probably gonna make some people's lives better.
[3574.88s -> 3576.64s]  You're all like trying to process the memes.
[3576.64s -> 3577.64s]  I can see everybody.
[3577.64s -> 3580.64s]  Okay, let's get to the memes first then.
[3580.64s -> 3584.98s]  So obviously, so these memes are not actually
[3584.98s -> 3586.64s]  in the data set.
[3586.64s -> 3589.12s]  So I could put some really hateful memes
[3589.12s -> 3591.08s]  about sort of Hitler or something,
[3591.08s -> 3592.08s]  which are in the data set,
[3592.08s -> 3594.10s]  but that would be less fun.
[3594.10s -> 3596.70s]  So these are meme meme examples
[3596.70s -> 3601.70s]  to kind of demonstrate how the data set was constructed.
[3602.22s -> 3604.94s]  And so one of the problems we had, as I said,
[3604.94s -> 3607.30s]  like VQA, the V didn't really matter.
[3607.30s -> 3609.04s]  What we want to have is a data set
[3609.04s -> 3611.74s]  if we care about multimodality specifically.
[3611.74s -> 3614.18s]  It's like, how do we get a data set
[3614.18s -> 3615.88s]  that you can only get right
[3615.88s -> 3617.58s]  if you are good at multimodal reasoning
[3617.58s -> 3620.06s]  and otherwise you're just gonna screw it up.
[3620.06s -> 3621.62s]  And so this is what we came up with
[3621.62s -> 3623.90s]  is if you have a meme like this one,
[3623.90s -> 3625.22s]  love the way you smell today.
[3625.22s -> 3626.38s]  I mean, that's not very nice
[3626.90s -> 3628.58s]  if you send this to your friends, right?
[3631.02s -> 3633.98s]  But so it turns out that if you just swap out
[3633.98s -> 3636.70s]  the background, now it's a very nice thing to say.
[3637.70s -> 3640.10s]  And like this one is, I don't know,
[3640.10s -> 3641.50s]  maybe a bit weird if you like this,
[3641.50s -> 3644.84s]  but there's nothing wrong with it, right?
[3646.62s -> 3648.18s]  And so it's the same for this one here.
[3648.18s -> 3650.30s]  Like look how many people love you with the tumbleweed.
[3650.30s -> 3651.14s]  That's really sad.
[3651.14s -> 3654.58s]  And if you change just one word suddenly
[3654.62s -> 3656.82s]  it's like a really nice thing to say, right?
[3658.14s -> 3660.50s]  So if you want to solve this,
[3660.50s -> 3664.26s]  if you want to classify this correctly for the meanness,
[3664.26s -> 3667.30s]  then you have to really understand multimodal reasoning.
[3667.30s -> 3669.08s]  You have to understand the relationship
[3669.08s -> 3670.62s]  between the image and the text
[3670.62s -> 3672.46s]  in order to get to the right label, right?
[3672.46s -> 3675.40s]  And so it was really constructed by design to do that.
[3676.50s -> 3679.38s]  And so how we did it exactly
[3679.38s -> 3682.90s]  is we used some really highly trained annotators.
[3682.94s -> 3684.98s]  And then one of the big problems
[3684.98s -> 3686.28s]  with a lot of these data sets
[3686.28s -> 3688.82s]  is that nobody really knows
[3688.82s -> 3691.30s]  who owns the meme, for example, right?
[3691.30s -> 3692.46s]  So if somebody makes this meme
[3692.46s -> 3694.14s]  now they technically own a copyright.
[3694.14s -> 3696.30s]  And so when I made this data set
[3696.30s -> 3698.18s]  I was working at the Facebook
[3698.18s -> 3700.86s]  and they were very afraid of copyright things.
[3700.86s -> 3703.06s]  So what we actually had to do is
[3703.06s -> 3705.26s]  we had to pay people to make new memes.
[3708.70s -> 3709.66s]  So not from scratch.
[3709.66s -> 3711.94s]  So we could show them kind of the actual examples
[3711.98s -> 3714.98s]  and then they had to try to find images
[3714.98s -> 3717.40s]  that were kind of corresponding
[3717.40s -> 3718.90s]  to the original source image
[3718.90s -> 3720.54s]  and try to recreate the meme
[3720.54s -> 3723.56s]  but now with an image that we could buy from Getty.
[3724.46s -> 3727.46s]  And so we gave a lot of money to Getty
[3727.46s -> 3731.06s]  so that we could then release the data set to the public
[3731.06s -> 3733.18s]  so that people could do actually research on this
[3733.18s -> 3735.26s]  and understand for their multimodal models
[3735.26s -> 3737.10s]  whether they're good or not.
[3737.10s -> 3738.58s]  And so we really tried to make it
[3738.58s -> 3743.58s]  so that we had these benign confounders.
[3743.58s -> 3746.42s]  Sorry, it's a startup world with co-founders.
[3747.62s -> 3750.06s]  So the confounder here is obviously
[3750.06s -> 3751.46s]  that you have your original meme
[3751.46s -> 3753.70s]  and then you have your confounder
[3753.70s -> 3755.38s]  where you swap out one of the modalities
[3755.38s -> 3756.58s]  and here you have the other one, right?
[3756.58s -> 3759.98s]  So we had our annotators do that as well.
[3759.98s -> 3764.08s]  And so this led to a really nice data set, I think,
[3764.08s -> 3766.14s]  because it showed some of the intuitions
[3766.14s -> 3768.38s]  that I think a lot of people in the field had
[3769.22s -> 3772.74s]  which is that multimodal pre-training doesn't really work.
[3772.74s -> 3773.58s]  Is that an alarm?
[3775.06s -> 3778.10s]  So multimodal pre-training doesn't really work.
[3778.10s -> 3780.78s]  And so all of this stuff that people have been doing
[3780.78s -> 3782.94s]  with all their fancy visual bird models
[3782.94s -> 3783.98s]  actually turned out maybe
[3783.98s -> 3786.62s]  to not really be that useful anyway.
[3786.62s -> 3789.48s]  And so maybe it got you like one point extra, right?
[3789.48s -> 3792.18s]  From visual bird to like a different visual bird
[3792.18s -> 3793.88s]  like less than a point
[3793.88s -> 3796.78s]  just by doing that multimodal pre-training.
[3797.62s -> 3801.14s]  So that means we still have to figure this stuff out, right?
[3801.14s -> 3803.18s]  This data set is far from solved
[3803.18s -> 3805.22s]  and we still have a long way to go
[3805.22s -> 3806.80s]  despite all of these fancy models
[3806.80s -> 3809.90s]  and a new paper coming out every week
[3809.90s -> 3811.14s]  that does something new.
[3811.14s -> 3813.16s]  Like we're not there yet.
[3813.16s -> 3816.30s]  And I think that's encouraging, especially for you
[3816.30s -> 3818.70s]  when you can go out and solve it.
[3819.84s -> 3822.10s]  So what we did with this data set
[3822.10s -> 3823.38s]  is we organized a competition.
[3823.38s -> 3825.80s]  We had 100K in prize money
[3825.84s -> 3828.04s]  to try to see what people could come up with.
[3829.00s -> 3832.60s]  And so there was a lot of nice work coming out of that
[3832.60s -> 3835.20s]  and we really kind of managed to crank the numbers up
[3835.20s -> 3836.36s]  by quite a lot.
[3837.60s -> 3840.32s]  But the solutions were slightly disappointing.
[3840.32s -> 3842.22s]  So I don't know if you've ever used Kaggle
[3842.22s -> 3844.00s]  but if you want to really win on Kaggle
[3844.00s -> 3845.88s]  you just have to ensemble the hell out
[3845.88s -> 3847.00s]  of all of the different models
[3847.00s -> 3848.48s]  that are current state of the art
[3848.48s -> 3850.24s]  and then you're very likely to win, right?
[3850.24s -> 3852.88s]  And so that's what happened here
[3853.88s -> 3857.16s]  where there wasn't really the fundamental breakthrough
[3857.16s -> 3858.48s]  we had maybe been hoping for.
[3858.48s -> 3861.46s]  So that still needs to be built, I think.
[3863.08s -> 3864.32s]  So this other data set
[3864.32s -> 3866.16s]  I just want to kind of briefly talk about.
[3866.16s -> 3868.06s]  So the theme sort of of this section is like
[3868.06s -> 3871.64s]  if you make a data set, think about it very carefully
[3871.64s -> 3873.60s]  because you can really be very creative with this
[3873.60s -> 3876.48s]  and really measure the things you're trying to get at.
[3876.48s -> 3879.72s]  So this data set, Winterground,
[3879.72s -> 3880.90s]  we were trying to figure out,
[3881.10s -> 3882.94s]  how good is CLIP actually?
[3882.94s -> 3884.30s]  So it looks really amazing
[3884.30s -> 3887.28s]  and it's way better than things that were previously there
[3887.28s -> 3890.34s]  but does it understand compositional relationships
[3890.34s -> 3892.34s]  in the same way that humans would understand it
[3892.34s -> 3893.72s]  or is it sort of just fitting
[3893.72s -> 3895.50s]  onto the data distribution
[3895.50s -> 3898.44s]  and it can be very good at the head of the distribution
[3898.44s -> 3900.42s]  but it's terrible at the tail.
[3900.42s -> 3903.54s]  And you can probably already guess where this is going.
[3903.54s -> 3906.24s]  But so just to give you an illustration
[3906.24s -> 3907.62s]  of what is in this data set
[3907.66s -> 3911.06s]  you would have some plants surrounding a light bulb
[3911.06s -> 3913.98s]  or you would have a light bulb surrounding some plants.
[3913.98s -> 3917.82s]  So notice that the words here are exactly the same words
[3917.82s -> 3919.58s]  but in a different order, right?
[3919.58s -> 3923.48s]  So and so the visual depiction of these words
[3923.48s -> 3924.94s]  is very, very different.
[3924.94s -> 3927.14s]  So if your model, your contrastive model
[3927.14s -> 3930.92s]  is actually good at understanding the visual semantic
[3930.92s -> 3934.74s]  or the visual linguistic compositionality
[3934.74s -> 3938.26s]  of these examples,
[3938.26s -> 3939.54s]  then you can get it right.
[3939.54s -> 3941.74s]  But again, if it's actually just overfitting
[3941.74s -> 3943.46s]  on the data distribution that is seen
[3943.46s -> 3947.22s]  and it just kind of is biased toward what it sees often,
[3947.22s -> 3948.86s]  then it doesn't really get it, right?
[3948.86s -> 3951.62s]  And so one paper that we use
[3951.62s -> 3953.90s]  as a source of inspiration for this work
[3953.90s -> 3956.42s]  is this paper here,
[3956.42s -> 3959.48s]  order word matters pre-training for little.
[3959.48s -> 3961.38s]  So we actually found that the order of words
[3961.38s -> 3963.06s]  doesn't even matter that much
[3963.06s -> 3965.94s]  for general pre-training very often,
[3965.94s -> 3967.56s]  which is also kind of a scary thing, right?
[3967.56s -> 3969.22s]  So this is deep learning for NLP.
[3969.22s -> 3971.42s]  We think that language is really important
[3971.42s -> 3973.94s]  but these models can reason about language
[3973.94s -> 3975.70s]  even if you shuffle all the words.
[3976.86s -> 3979.52s]  And so that's probably not what we want to have.
[3979.52s -> 3982.54s]  And so that doesn't tell you something
[3982.54s -> 3984.98s]  about how great we are as researchers.
[3984.98s -> 3986.70s]  It tells you something about how terrible
[3986.70s -> 3989.12s]  our evaluation benchmarks are, right?
[3989.12s -> 3990.72s]  And that's what we need to fix.
[3991.60s -> 3993.40s]  So what we did with this data set,
[3993.40s -> 3994.74s]  here are some other nice examples,
[3994.74s -> 3996.12s]  like there's a mug in some grass
[3996.12s -> 3997.60s]  or there's some grass in a mug.
[3997.60s -> 3999.54s]  These are very different pictures, right?
[3999.54s -> 4001.60s]  And so for us, these are trivial.
[4001.60s -> 4004.12s]  So what's the difference between a truck fire
[4004.12s -> 4005.10s]  and a fire truck?
[4006.56s -> 4008.56s]  They're pretty important, I think,
[4008.56s -> 4010.64s]  also to get that distinction right.
[4011.72s -> 4014.04s]  So guess what?
[4015.08s -> 4017.16s]  State-of-the-art models often perform
[4017.16s -> 4018.52s]  below random chance.
[4020.72s -> 4025.08s]  So as I said, we still have a lot of work to do,
[4025.08s -> 4026.68s]  which is good.
[4026.68s -> 4029.00s]  And so when this paper came out,
[4029.00s -> 4031.92s]  I think the reaction was really nice.
[4031.92s -> 4034.40s]  And so when DALI-2 came out,
[4035.96s -> 4037.68s]  so you probably heard of DALI-2, right?
[4037.68s -> 4039.48s]  So it's sort of like stable diffusion
[4039.48s -> 4041.30s]  but then before stable diffusion.
[4042.20s -> 4044.70s]  And so this was really the first model
[4044.70s -> 4047.26s]  that really showed just how impressive
[4047.26s -> 4049.40s]  these generative models can be
[4049.88s -> 4051.30s]  when they're creating images.
[4051.30s -> 4054.20s]  So this is, there's a mug in some grass.
[4054.20s -> 4056.20s]  You do have to kind of cheat a little bit
[4056.20s -> 4059.02s]  because you have to add digital art here.
[4059.02s -> 4062.18s]  If you don't add that, then it breaks down completely.
[4063.56s -> 4065.18s]  So it's sort of prompt hacking, I think,
[4065.18s -> 4066.80s]  or sort of tuning on the test set.
[4066.80s -> 4068.46s]  But okay, you know.
[4069.72s -> 4071.16s]  So this is pretty good, right?
[4071.16s -> 4074.80s]  So it definitely is better than I think a lot of people
[4074.80s -> 4077.20s]  would have expected even a couple of years ago.
[4078.04s -> 4082.12s]  But it's not perfect because people on the internet
[4082.12s -> 4084.82s]  like to take more pictures of spoons than forks.
[4086.76s -> 4090.16s]  So if you say there are fewer spoons than forks
[4090.16s -> 4092.54s]  or there are fewer forks than spoons,
[4092.54s -> 4094.32s]  it just really likes spoons more.
[4094.32s -> 4099.32s]  You know, and so maybe it's like the matrix or something.
[4100.60s -> 4105.06s]  I don't know, but spoons are just nicer.
[4105.26s -> 4108.22s]  So again, what you can see here is that these models
[4108.22s -> 4110.34s]  really are just reflections of the data
[4110.34s -> 4112.62s]  that they're trained on, right?
[4112.62s -> 4115.34s]  And yeah, so models are getting better,
[4115.34s -> 4117.22s]  but if you've looked at stable diffusion,
[4117.22s -> 4119.90s]  like it still can't count fingers and things like that.
[4119.90s -> 4123.58s]  So again, there's still a lot of cool work to be done.
[4124.62s -> 4126.40s]  Any questions on evaluation?
[4132.90s -> 4134.42s]  No? Okay.
[4134.50s -> 4137.98s]  So let's talk about other modalities then.
[4137.98s -> 4140.10s]  So we've really just been focused on images
[4140.10s -> 4141.18s]  and images are great.
[4141.18s -> 4145.10s]  There are lots of images on the internet
[4145.10s -> 4147.18s]  and so that makes it sort of an obvious thing
[4147.18s -> 4148.48s]  to focus on.
[4148.48s -> 4150.86s]  It's also, I think if you look at our brain,
[4150.86s -> 4153.14s]  like vision is a very dominant modality, right?
[4153.14s -> 4156.78s]  So how we understand the world is very vision driven,
[4156.78s -> 4158.90s]  but it doesn't have to be the case.
[4158.90s -> 4160.96s]  So there's all these other interesting problems
[4160.96s -> 4162.98s]  that involve different modalities.
[4163.26s -> 4166.50s]  So the most obvious one is just speech or audio, right?
[4166.50s -> 4169.16s]  So after seeing comes hearing
[4169.16s -> 4172.82s]  and really we could do another lecture just like this,
[4172.82s -> 4174.24s]  just on speech and audio
[4174.24s -> 4176.70s]  and there's lots of interesting stuff to talk about.
[4176.70s -> 4178.18s]  Obviously we don't have time,
[4178.18s -> 4181.06s]  but I'll give you another nice example
[4181.06s -> 4185.06s]  of how amazing Alec Redford is at creating data sets.
[4185.06s -> 4186.78s]  So there's this whisper model
[4186.78s -> 4189.66s]  that came out of OpenAI not too long ago,
[4189.66s -> 4192.10s]  which was trained on 680,000 hours
[4192.10s -> 4195.50s]  of multilingual multitask speech data.
[4195.50s -> 4197.46s]  So speech with transcriptions
[4197.46s -> 4201.34s]  and they trained this very fancy thing on there,
[4201.34s -> 4203.00s]  which actually is not very fancy at all.
[4203.00s -> 4204.44s]  It's just the log-mail spectrogram.
[4204.44s -> 4206.66s]  So how you represent the audio signal
[4206.66s -> 4208.68s]  and then you feed that into a big transformer.
[4208.68s -> 4211.44s]  So this is sort of your encoder self-attention here,
[4211.44s -> 4212.28s]  right?
[4212.28s -> 4213.12s]  And then you have your decoder
[4213.12s -> 4214.90s]  where you have your cross-attention
[4214.90s -> 4217.34s]  and then you just generate the sequence.
[4217.34s -> 4220.94s]  So this is encoder decoder basic transformer model,
[4220.94s -> 4223.34s]  but your input is convolutions,
[4223.34s -> 4224.66s]  one-dimensional convolutions
[4224.66s -> 4226.30s]  over the log-mail spectrogram.
[4227.26s -> 4228.54s]  And so there's lots of papers
[4228.54s -> 4230.22s]  that do very similar things.
[4230.22s -> 4232.50s]  There's models like Wave2Vec
[4232.50s -> 4234.80s]  that tried to turn the wave signal into vectors,
[4234.80s -> 4237.86s]  or you can discretize it in lots of different ways.
[4237.86s -> 4240.10s]  So there's a wealth of literature.
[4240.10s -> 4242.78s]  Then I think one of the funny observations actually
[4242.78s -> 4245.86s]  is that you can just reduce audio to vision anyway,
[4245.86s -> 4246.70s]  right?
[4246.70s -> 4248.44s]  So that's what you could sort of argue
[4248.44s -> 4249.98s]  this log-mail spectrogram does,
[4250.94s -> 4252.62s]  so not to toot my own horn,
[4252.62s -> 4255.46s]  but in 27 I did this paper where we showed
[4255.46s -> 4258.50s]  that you can just take a real audio sample,
[4258.50s -> 4263.26s]  turn it into a kind of a spectrogram,
[4263.26s -> 4264.32s]  really just a spectrogram.
[4264.32s -> 4268.06s]  So what does the spectrum of the audio file look like?
[4268.06s -> 4270.06s]  Feed that to a regular ConvNet,
[4270.06s -> 4271.54s]  like an AlexNet even,
[4271.54s -> 4273.70s]  and then that gives you amazing auditory features.
[4273.70s -> 4275.58s]  So now you can use this to distinguish
[4275.58s -> 4277.98s]  between violins or guitars and things like that.
[4277.98s -> 4281.58s]  So maybe you can just reduce all of this to vision.
[4281.58s -> 4283.52s]  So one question maybe you could ask is,
[4283.52s -> 4286.10s]  can we also reduce language to vision
[4286.10s -> 4287.34s]  or vision to language?
[4288.60s -> 4291.14s]  So that's sort of what people are thinking about.
[4292.62s -> 4293.98s]  So we talked about video.
[4293.98s -> 4295.36s]  There was a question about video.
[4295.36s -> 4297.90s]  So a lot of these ideas also extend
[4297.90s -> 4299.38s]  pretty directly to video,
[4299.38s -> 4301.26s]  but now you just have more data.
[4301.26s -> 4303.22s]  So like Flamingo already had a bunch
[4303.22s -> 4304.30s]  of different images in it.
[4304.30s -> 4306.94s]  You can do Flamingo over videos.
[4306.94s -> 4310.18s]  Probably a lot of the images are pretty useless
[4310.18s -> 4312.56s]  for what you're trying to do with this video model.
[4312.56s -> 4313.94s]  So they're too similar.
[4313.94s -> 4316.14s]  It doesn't really add all that much information.
[4316.14s -> 4318.34s]  So you want to subsample the frames
[4318.34s -> 4320.24s]  so that you get the most useful information
[4320.24s -> 4322.08s]  out of your video.
[4322.08s -> 4323.70s]  And so there's a bunch of approaches
[4323.70s -> 4325.58s]  that kind of take the key frames
[4325.58s -> 4328.42s]  and then you just do a standard joint vision
[4328.42s -> 4331.66s]  and language transformer encoder thing on top of that.
[4331.66s -> 4334.14s]  So this is kind of becoming hopefully by now
[4334.14s -> 4335.56s]  a very familiar recipe.
[4336.44s -> 4337.68s]  And so there's this,
[4337.68s -> 4340.92s]  so Merlot is a nice architecture that does this.
[4340.92s -> 4343.44s]  And then they came up with Merlot Reserve,
[4343.44s -> 4344.74s]  kind of a silly name,
[4344.74s -> 4348.04s]  where they also added audio to this model.
[4348.04s -> 4350.20s]  So this is now a tri-modal model.
[4350.20s -> 4353.44s]  And so we're going towards this foundation model
[4353.44s -> 4356.36s]  that can consume all of these different modalities
[4356.36s -> 4357.20s]  all in one go.
[4357.20s -> 4359.80s]  And that's really like a clear trend in the field.
[4361.86s -> 4364.08s]  Another very interesting direction,
[4364.08s -> 4366.76s]  I think where in the field we were very excited
[4366.76s -> 4367.70s]  about this for a while,
[4367.70s -> 4371.00s]  but I think it's sort of gone now
[4371.00s -> 4373.20s]  because it's too difficult to create
[4373.20s -> 4375.20s]  lots of high quality data in this setting.
[4375.20s -> 4376.88s]  But what you can do is you can have
[4376.88s -> 4378.98s]  simulated environments.
[4378.98s -> 4381.68s]  So this is a paper from DeepMind from 2017
[4381.68s -> 4383.84s]  where they had this agent walk around in a maze
[4383.84s -> 4386.20s]  and then it could have natural language instructions.
[4386.20s -> 4388.68s]  It could also generalize to like decks and blics
[4388.68s -> 4391.28s]  and different sort of groundings and assignments
[4391.28s -> 4393.86s]  that you could do in that environment.
[4394.50s -> 4395.62s]  So this is a super interesting direction,
[4395.62s -> 4396.62s]  I think in the long term,
[4396.62s -> 4398.70s]  because this is how humans learn language, right?
[4398.70s -> 4400.06s]  Like we walk around in the world,
[4400.06s -> 4401.58s]  we interact with our environments,
[4401.58s -> 4404.42s]  we have all of these different perceptual observations,
[4404.42s -> 4406.26s]  we synthesize them in our brain,
[4406.26s -> 4407.78s]  we manipulate objects,
[4407.78s -> 4409.26s]  we change our own viewpoint,
[4409.26s -> 4412.26s]  and that's how we learn everything we know about the world
[4412.26s -> 4415.46s]  and so our language is very intricately connected
[4415.46s -> 4418.36s]  to that world and how we observe it.
[4418.36s -> 4421.26s]  So I think that that might make a comeback
[4421.26s -> 4422.76s]  at some point in the future.
[4423.60s -> 4425.66s]  You can also do other stuff,
[4425.66s -> 4428.64s]  so especially with this kind of conditioning on text
[4428.64s -> 4430.68s]  that we're seeing a lot of, right?
[4430.68s -> 4433.16s]  So, you know, DALI2 and Sable Diffusion
[4433.16s -> 4434.44s]  and all of these different things
[4434.44s -> 4437.92s]  and the original GAN we talked about at the beginning.
[4437.92s -> 4439.06s]  You can do the same thing,
[4439.06s -> 4442.28s]  but now you're generating 3D point clouds, right?
[4442.28s -> 4446.04s]  So this is a 3D Corgi using a Corgi.
[4446.04s -> 4448.08s]  And so this prompt can probably become
[4448.08s -> 4449.54s]  much more complex over time
[4449.54s -> 4451.96s]  and you can do like sort of AutoCAD design
[4452.00s -> 4453.76s]  and just say like give me a house
[4453.76s -> 4456.64s]  and it's just going to design the whole house for you.
[4456.64s -> 4459.12s]  So you can just like tweak the prompt
[4459.12s -> 4459.96s]  and things like that,
[4459.96s -> 4461.44s]  like that's all coming
[4461.44s -> 4463.64s]  or even already here in many cases.
[4464.68s -> 4468.00s]  So the final modality I just briefly wanted to talk about
[4468.00s -> 4470.28s]  is olfactory embeddings.
[4473.72s -> 4477.12s]  And so olfaction means smell, if you didn't know.
[4478.12s -> 4479.74s]  And so it turns out,
[4479.74s -> 4484.68s]  so my PhD thesis was about grounding semantics
[4484.68s -> 4487.06s]  in a different perceptual modality.
[4487.06s -> 4489.28s]  So a lot of my work started in vision
[4489.28s -> 4490.12s]  and then it's like, okay,
[4490.12s -> 4492.36s]  now audio is sort of the obvious next one, right?
[4492.36s -> 4494.14s]  So you can learn the meaning of violin
[4494.14s -> 4496.78s]  and then maybe you can learn that violin,
[4496.78s -> 4498.70s]  like what a violin looks like and what it is
[4498.70s -> 4499.60s]  and what it sounds like
[4499.60s -> 4501.82s]  and that's going to give you a richer representation.
[4501.82s -> 4503.66s]  But for a lot of these words,
[4503.66s -> 4506.40s]  what's actually very primitive to their meaning
[4506.40s -> 4507.62s]  is what they smell like
[4507.62s -> 4509.30s]  because in our brains,
[4509.30s -> 4510.90s]  that's really one of the core areas
[4510.90s -> 4513.46s]  and one of the oldest areas in your brain.
[4513.46s -> 4515.66s]  So what you can try to do
[4515.66s -> 4519.34s]  if you want to complete all of your perceptual modalities
[4519.34s -> 4521.46s]  is you can try to build olfactory embedding.
[4521.46s -> 4524.58s]  So it was kind of a joke paper I did,
[4524.58s -> 4528.04s]  but the funny thing is it actually worked.
[4529.30s -> 4532.66s]  So there's a catalog,
[4532.66s -> 4536.86s]  this Sigma Aldrich Fine Flavors and Fragrances Catalog
[4536.86s -> 4540.18s]  where you can look up words like melon and pineapple
[4540.18s -> 4541.18s]  and then it's going to give you
[4541.18s -> 4543.14s]  all of the chemical compounds
[4543.14s -> 4545.22s]  that produce the smell or taste.
[4546.18s -> 4547.90s]  And so if you do that,
[4547.90s -> 4549.74s]  then you can count the occurrences
[4549.74s -> 4551.46s]  and then you can sort of do SVD
[4551.46s -> 4554.04s]  or something like that only to get it
[4554.04s -> 4556.38s]  to be a bit more of a real embedding model.
[4556.38s -> 4559.66s]  So now you get smell embeddings, smell vectors
[4559.66s -> 4562.92s]  and then you can compute similarity judgments
[4562.92s -> 4564.42s]  between these smells.
[4564.42s -> 4567.10s]  So turns out apple smells like pear
[4567.10s -> 4571.16s]  and the chocolate and cocoa and sweet and coffee
[4571.16s -> 4572.26s]  are sort of related.
[4572.26s -> 4574.62s]  So you get these clusters of different smells
[4574.62s -> 4577.26s]  just based off of their chemical compounds.
[4577.26s -> 4580.06s]  So this bag of chemical compounds model
[4580.06s -> 4582.42s]  gives you a very rich representation.
[4582.42s -> 4585.06s]  And so if you look at all of the words
[4585.06s -> 4587.86s]  that are concrete enough to have smell,
[4587.86s -> 4590.42s]  so like if you have a word like democracy in there
[4590.42s -> 4593.22s]  that doesn't really smell like anything.
[4593.22s -> 4596.46s]  So you ignore democracy.
[4597.94s -> 4601.06s]  You just focus on the things that smell
[4601.06s -> 4603.18s]  or that could smell, I guess.
[4603.18s -> 4605.90s]  And then so the really interesting thing to me
[4605.90s -> 4610.26s]  is that this is much more correlated
[4610.26s -> 4611.94s]  with human similarity judgments
[4611.94s -> 4615.06s]  than the linguistic vectors we had at the time.
[4615.06s -> 4617.66s]  So for a word like apple,
[4617.66s -> 4619.14s]  you can just get a word vector
[4619.14s -> 4621.50s]  like you've learned in your first lecture.
[4621.50s -> 4624.82s]  And so you can do skip gram and things like that.
[4624.82s -> 4627.38s]  But that thing is not going to be as correlated
[4627.38s -> 4629.30s]  with human similarity judgments
[4629.30s -> 4632.34s]  as this bag of chemical compounds model.
[4632.34s -> 4634.46s]  So that's pretty interesting.
[4634.46s -> 4635.90s]  So even something like smell
[4635.90s -> 4638.46s]  where maybe we think this doesn't really matter.
[4638.46s -> 4639.78s]  If you really want to understand
[4639.78s -> 4641.70s]  how humans understand language,
[4641.70s -> 4643.66s]  then maybe you want to include this
[4643.66s -> 4645.30s]  in your foundation model too.
[4647.82s -> 4650.02s]  But I would start with the other modalities.
[4651.02s -> 4651.86s]  All right.
[4653.86s -> 4655.82s]  Okay, yeah, sorry.
[4655.82s -> 4657.02s]  So where to next?
[4657.02s -> 4659.62s]  I'll just, I think I've already said most of this actually.
[4659.62s -> 4662.42s]  So one foundation model is going to rule them all.
[4663.38s -> 4665.94s]  And so, I mean, there will be many of these,
[4665.94s -> 4667.22s]  but a lot of them are going to have
[4667.22s -> 4668.82s]  very similar traits, I think.
[4669.74s -> 4671.86s]  We're going to be looking at scaling laws
[4671.86s -> 4674.22s]  and trying to understand really what is the relationship
[4674.22s -> 4675.58s]  between the different modalities,
[4675.58s -> 4678.86s]  which one do we want more of, that sort of stuff.
[4678.86s -> 4680.38s]  We're going to have retrieval augmentation.
[4680.38s -> 4682.02s]  This thing is going to be really huge.
[4682.02s -> 4684.60s]  If you've heard of rag, or if you haven't,
[4684.60s -> 4686.06s]  you should look it up.
[4686.06s -> 4687.74s]  So all of these parts of these models
[4687.74s -> 4689.46s]  can also be multimodal.
[4689.46s -> 4692.14s]  We need way better evaluation and better measurements.
[4692.14s -> 4694.06s]  We already talked about that too.
[4694.06s -> 4694.90s]  And that's all I had.
[4694.90s -> 4695.74s]  Thank you.
[4695.74s -> 4696.56s]  Thank you.
[4696.56s -> 4697.40s]  Thank you.
[4697.40s -> 4698.22s]  Thank you.
