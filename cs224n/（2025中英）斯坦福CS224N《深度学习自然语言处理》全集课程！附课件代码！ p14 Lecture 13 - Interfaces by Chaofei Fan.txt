# Detected language: en (p=1.00)

[0.00s -> 8.88s]  Thanks everyone for coming.
[8.88s -> 13.72s]  It's a really busy time in the quarter, everyone's busy with homework project and midterms.
[13.72s -> 20.92s]  Yeah, today I'm going to tell you something I'm really passionate about, is the speech
[20.92s -> 23.26s]  pre-computer interface.
[23.26s -> 28.26s]  My research, so before that I just saw some self-introductions.
[28.26s -> 33.78s]  So I'm Chao Fei, I'm from the Stanford NPTEL lab.
[33.78s -> 40.02s]  Our lab is trying to build speech pre-computer interfaces to help people restore communication
[40.02s -> 43.62s]  or restore movements.
[43.62s -> 50.18s]  So today I think I'm just going to really tell you guys how cool this pre-computer
[50.18s -> 57.46s]  interface is, given that we have so many recent developments in the AI and machine learning.
[57.46s -> 61.70s]  And I hope you guys can enjoy this talk.
[61.70s -> 71.84s]  All right, so let me first start with a video to give you some motivations that why we
[71.84s -> 74.30s]  want to build a pre-computer interface.
[80.78s -> 88.26s]  Yeah, I think what this story tell is that we saw this teenager Howard, who's 21 at the
[88.26s -> 94.62s]  time of this video, was shot and he lost all his dreams because of a severe stroke
[94.62s -> 100.54s]  that also made him in this kind of a locking state where he can't move.
[100.54s -> 107.70s]  And he talked about he used to going out and play football, making friends, and just
[107.70s -> 109.46s]  let his emotions out.
[109.46s -> 114.54s]  I think all this is a loss to him and I think the most important thing is that he couldn't
[114.54s -> 119.22s]  really speak to express himself, to let all the emotions out.
[119.22s -> 124.50s]  So Howard is just like one of those individuals who suffer from this kind of neurological
[124.50s -> 131.60s]  disease disorder such as brain stem stroke or ALS that can cause severe speech and
[131.60s -> 134.70s]  motor impairment and even complete loss of speech.
[138.10s -> 141.42s]  Those individuals, I think the life is really challenging for them, right?
[141.42s -> 142.42s]  Just think about it.
[142.42s -> 144.34s]  You cannot speak, you cannot move.
[144.34s -> 148.98s]  You still have a fully functioned brain, but everything is lost.
[148.98s -> 153.26s]  All your dreams could be shattered.
[153.26s -> 159.00s]  So I think for people like Howard, as you just saw in this video, the way they can
[159.00s -> 164.30s]  still communicate with the outside world, with their loved one, is through this assistive
[164.30s -> 168.22s]  communication devices such as the one we just saw in the video, which is this kind
[168.22s -> 172.58s]  of letter board that has letters organized physically.
[172.58s -> 177.62s]  And for people like Howard, which may still have some residual movement, they can use
[177.62s -> 184.58s]  their gaze to tell his friend where he's looking at and then his friend can use the
[184.58s -> 187.18s]  gaze to tell what letter he's trying to say.
[187.18s -> 191.50s]  Just imagine how slow this process is if you want to just say a sentence.
[191.70s -> 198.06s]  It might take you a few minutes to express simple things like how are you, I'm feeling
[198.06s -> 200.42s]  not comfortable today.
[200.42s -> 207.10s]  An alternative here is you can also use an eye tracking device so that people can
[207.10s -> 212.78s]  use eye trackings to type on the computer on virtual keyboard.
[212.78s -> 218.02s]  But just think about it, if you have to look at the computer screen all the times,
[218.02s -> 220.26s]  all day, it's really tiring for them.
[220.30s -> 225.90s]  And also these people are not like us, they probably even if they still have a residual
[225.90s -> 231.46s]  eye movement, it's very hard for them to move their eyes so it's very tiring as well.
[236.14s -> 241.78s]  Maybe something different here is that maybe some of you guys have already seen this recently
[241.78s -> 246.10s]  as some videos published by a company called Neuralink.
[246.10s -> 249.54s]  For example, here's one video here, let me see if we can play them.
[250.70s -> 251.90s]  Hopefully I can play them.
[254.30s -> 260.10s]  All right, so I think here is that this company Neuralink is developing this kind
[260.10s -> 268.74s]  of like a tiny implantable device that can be actually placed inside your skull
[268.74s -> 273.14s]  and then rate the brain signals.
[273.14s -> 278.98s]  The hope here is that because for people like Howard, their brain is still fully functioning,
[278.98s -> 285.46s]  so we kind of hope here is that maybe we can using this kind of like a direct interface
[285.46s -> 291.06s]  with their brain so that they can still use their intact brain to control a computer or
[291.06s -> 294.58s]  even robots to help them to live a normal life.
[294.58s -> 302.74s]  And here is a quote from their participants, Nolan, and he's pretty excited about being
[302.74s -> 309.94s]  able to using this very state of art BCI to be able to connect with their families and
[309.94s -> 312.66s]  then be able to support himself.
[312.66s -> 316.82s]  I think this kind of like BCI, what I'm trying to say here is that for people like
[316.82s -> 324.10s]  Howard or like a lot of people who has lost their control of their body and language,
[324.10s -> 326.14s]  I think the BCI can bring hope to them.
[326.14s -> 332.42s]  So that's what I'm trying to motivate here today is that we're trying to use BCI to
[332.42s -> 333.78s]  really help these people.
[333.78s -> 338.74s]  But I think before going into the details about how this works, I just want to first
[338.74s -> 344.02s]  go through a brief history of brain-computer interface just to help you guys understand
[344.02s -> 348.34s]  how this thing works, like why we can put such tiny devices into the brain and
[348.34s -> 351.58s]  then suddenly can interpret what the brain is doing.
[351.58s -> 353.38s]  There's a lot of interesting stories here.
[353.38s -> 358.22s]  So let me start with a brief history of BCI.
[358.22s -> 367.18s]  So first, back to the 19th century here, a British scientist called Richard Caton started
[367.18s -> 370.50s]  to do some experiments on animals.
[370.50s -> 376.58s]  And one of the things he found is that you can actually measure the brain, like measure
[376.58s -> 379.54s]  the electricity from the brain.
[379.54s -> 385.98s]  Moreover, if you ask the animal, not ask the animal, but let the animals do some tasks,
[385.98s -> 391.98s]  actually moving their heads, then you can see that the electricity changes somehow.
[391.98s -> 396.98s]  So I think this is the very first early experiments that scientists do to show that
[396.98s -> 400.58s]  okay, actually you can decode some signals from the brain, but we still don't know
[400.58s -> 407.34s]  how exactly what those, exactly what those electric signals means here.
[407.34s -> 418.06s]  So fast forward to 1924, a German scientist called Hans Berger invented this device called,
[418.06s -> 423.14s]  yeah, I always forget how to read that word, but anyway, so it's short for EEG.
[423.14s -> 425.98s]  So it's basically, it's kind of like on the right, you can see, it's kind of like
[425.98s -> 432.10s]  this electrode that you can place on the outside of your, basically on your scalp and
[432.10s -> 436.52s]  then measure this kind of like a wave, like signals.
[436.52s -> 445.18s]  So what this scientist Hans Berger found is that, so first, he's the first scientist
[445.18s -> 450.70s]  to find that you can actually measure this kind of like wave, like signals from like
[450.70s -> 453.68s]  just this kind of like electrodes placed on the head.
[453.68s -> 459.60s]  And then he found that this kind of signals has a very different frequency depending
[459.60s -> 466.24s]  on how the user, how the patient is, like the state of patient, for example, if the
[466.28s -> 470.24s]  patient is in this kind of very calm state, then it will have generated this kind of slow
[470.24s -> 476.20s]  alpha wave around like 10 to maybe 20 hertz, I forgot the exact range, but if the patient
[476.20s -> 480.48s]  has opened their eyes and then doing some like cognitive demanding tasks, then you'll
[480.48s -> 482.50s]  see really sharp beta waves.
[482.50s -> 486.82s]  So he's the first scientist to discover that you can actually, you know, using this kind
[486.82s -> 492.40s]  of like electrodes to measure the brain, the electricity, the electric signal of the brain.
[492.40s -> 500.14s]  And also there's a funny story here is that Hans Berger used to be a soldier and then one
[500.14s -> 503.64s]  day he was training on a horse and then he fell from the horse and had suffered from
[503.64s -> 504.64s]  a concussion.
[504.64s -> 509.78s]  He also has like a twin sister, not a twin, but he has a, yes, he has a twin sister
[509.78s -> 516.60s]  and then the story is that at the same day her sister felt like there is something
[516.60s -> 520.72s]  weird and then he's just starting to worry about his brother.
[520.72s -> 525.56s]  So her sister, his sister sent a telegraph to his father and telling his brother that
[525.56s -> 527.84s]  is his brother okay?
[527.84s -> 532.16s]  So this really intrigued Hans Berger that, you know, maybe there's something called
[532.16s -> 536.86s]  a telepathy that can connect to people through this kind of like, you know, whatever
[536.86s -> 537.94s]  the brain waves.
[537.94s -> 544.20s]  So that's his motivations to start to study psychology and neuroscience and you find this
[544.20s -> 549.92s]  EEG, which we are still using today to diagnose like things like epilepsy.
[549.92s -> 554.52s]  Okay.
[554.52s -> 561.02s]  And then people starting to using this kind of EEG devices to perform like to use it
[561.02s -> 566.92s]  to maybe things we can somehow detect this kind of wave like things from the brain
[566.92s -> 570.68s]  and then we can also control the like the frequency of the wave.
[570.68s -> 576.88s]  So someone started like a musician here starting to use this EEG devices to perform
[576.88s -> 577.88s]  music, right?
[578.08s -> 582.60s]  Anyway, so I guess you guys already got the idea that someone is controlling like trying
[582.60s -> 585.48s]  to perform some music with his brain wave.
[585.48s -> 591.00s]  I think this is a really cool experiment that's done in I think in the 1950s and
[591.00s -> 597.04s]  you can already see that people starting to get the idea that you can actually bypass
[597.04s -> 598.64s]  like your body.
[598.64s -> 602.00s]  You can actually use your brain, directly connect your brain to some like external
[602.00s -> 604.16s]  device and controlling that device.
[604.20s -> 609.68s]  So I think the idea here is that what if we can also leverage the same idea but to help
[609.68s -> 615.12s]  people with like hover, you know, you can help them to maybe control robotic arm.
[615.12s -> 622.08s]  But like the problem with this kind of like a EEG or external measuring device is that
[622.08s -> 624.22s]  the signal you get is very weak.
[624.22s -> 630.48s]  Just imagine that your brain is generating, you know, we probably know that like the
[630.48s -> 632.24s]  brain has a lot of neurons, right?
[632.24s -> 637.36s]  Neurons actually generating a lot of like signals and if you just put some electrodes
[637.36s -> 643.56s]  on the scalp and then what you are actually measuring is like the average neuron firing
[643.56s -> 648.76s]  of like maybe millions of neurons, which is if you think about like analogy is that
[648.76s -> 655.40s]  if you are trying to see, hear what people are saying in the room next to us, but we
[655.40s -> 660.52s]  can only try to figure out like what they are saying in this room, what we are hearing
[660.52s -> 662.28s]  is kind of the mumbling of a lot of things.
[662.28s -> 666.28s]  We can probably just tell maybe they are in this kind of like a happy mood or maybe
[666.28s -> 670.40s]  they have reached a conclusion but not exactly what they are trying to say.
[670.40s -> 676.84s]  So the limitation here is that this kind of EEG device can only give us very low precision
[676.84s -> 678.62s]  or like low resolution signal.
[678.62s -> 680.54s]  We want to get better signal.
[680.54s -> 681.80s]  So how?
[681.80s -> 688.20s]  I think the answer is trying to go inside the brain and then putting this kind of electrodes
[688.20s -> 696.44s]  next to a neuron and then trying to directly measure the neural activities of these neurons.
[696.44s -> 705.32s]  And for the purpose of this talk, we're mostly going to focus on the neurons in this region
[705.32s -> 706.92s]  of the brain called motor cortex.
[706.92s -> 712.16s]  So this brain, as some of you may already know, that brain has different regions that's
[712.16s -> 713.82s]  doing different tasks.
[713.82s -> 717.46s]  So in the center of the brain is called motor cortex.
[717.46s -> 721.32s]  It's basically controlling all your muscles, all your body muscles.
[721.32s -> 726.16s]  So the hope here that if we can understand the neuron, the coding of like the information
[726.16s -> 731.42s]  of the neuron that's encoded here, then perhaps we can decode this information and
[731.42s -> 738.06s]  then use this information to help people like Howard to be able to control a external
[738.06s -> 744.20s]  arm or be able to speak again.
[744.20s -> 750.86s]  So here's some very basic neuroscience.
[750.86s -> 754.42s]  So we know that there's this kind of cell called neurons, right?
[754.42s -> 760.86s]  So each one of this thing is called neuron, and this is the body of a neuron called soma,
[760.86s -> 762.52s]  and then this is the axon.
[762.52s -> 764.44s]  So this is another neuron.
[764.44s -> 769.30s]  So neuron connects through this tiny thing called synapse.
[769.30s -> 773.60s]  So if neuron has want to transfer some information to another neuron, just like in artificial
[773.60s -> 779.08s]  neural network, you have some neurons and then you want to send information to the
[779.08s -> 784.16s]  next layer, you can basically, this neuron will generate some action potential, which
[784.16s -> 790.88s]  is just some like electricity here to signal that another neuron that there is some information
[790.88s -> 792.32s]  there.
[792.32s -> 799.08s]  So if you put tiny electrodes, say, on the axon of this neuron here and then measure
[799.08s -> 803.60s]  the membrane potential, what you will get is something like this.
[803.60s -> 808.24s]  So you will have like on the x-axis is the time, and on the y-axis is the measure
[808.24s -> 811.30s]  the electric potential.
[811.30s -> 818.56s]  And then you will see this kind of like a very sharp spikes, and then if you zoom into
[818.56s -> 824.20s]  the spikes, you will see this kind of like a typical firing signature of the neuron,
[824.20s -> 829.62s]  which is like the electric, like if the voltage suddenly goes up and then goes down.
[829.62s -> 834.44s]  So basically, what you can measure at the neuron is that it's kind of very sharp spikes.
[834.44s -> 838.28s]  That's what you will get by putting electrodes next to a neuron.
[838.28s -> 845.12s]  OK, so how do we figure out what kind of information is encoded in this what we call
[845.12s -> 848.60s]  a spike train?
[848.60s -> 852.14s]  We can perform some behavior tasks here.
[852.14s -> 858.88s]  So for example, here, suppose I was still listening to a single neuron, and this neuron
[858.88s -> 861.98s]  is, let's say, we are using a monkey for this experiment, right?
[861.98s -> 866.16s]  So we are instructing, training the monkey to do two things.
[866.16s -> 871.28s]  One thing is basically we are trying to instruct the monkey to move his hands either to
[871.28s -> 879.10s]  the left or to the right, and then we measure the firing of the spikes of that single neuron,
[879.10s -> 883.78s]  and then see, trying to get what kind of information is encoded by that neuron.
[883.78s -> 890.36s]  So what you see here is that each row here is basically a spike train of that neuron,
[890.36s -> 891.36s]  as you just saw.
[891.36s -> 896.40s]  Here, each vertical line is just a spike of that neuron.
[896.40s -> 897.40s]  Each row is a trial.
[897.52s -> 907.44s]  A trial means that the monkey is just trying to move his hand in one direction.
[907.44s -> 914.80s]  Then the vertical, if you see here that because you can see here that the neuron seems to
[914.80s -> 918.58s]  fire slightly different across trials.
[918.58s -> 922.92s]  So I think that's the one fundamental properties of a neuron is that it's very noisy.
[922.92s -> 925.30s]  It's not like an artificial neural network.
[925.30s -> 931.54s]  If you put something in, you always get something out, whereas in a real neural network, things
[931.54s -> 932.54s]  are really noisy.
[932.54s -> 936.28s]  So sometimes they fire a little bit faster, but sometimes they fire a little bit slower
[936.28s -> 939.90s]  under the same experimental conditions.
[939.90s -> 947.76s]  So here, what we are trying to measure is that what kind of information is this
[947.76s -> 954.36s]  neural encoding when the monkey has moved his limb to the left or limb to the right,
[954.36s -> 961.60s]  and then we can also split this information encoding into two phases.
[961.60s -> 964.82s]  One is a preparation and the other is execution.
[964.82s -> 974.20s]  So for execution, execution means that the monkey is actually moving his arms, whereas
[974.20s -> 979.48s]  the preparation means that the monkey is preparing to move by holding his arm fixed.
[979.48s -> 985.94s]  So he will actually move his arm at this go time here.
[985.94s -> 992.66s]  So what we can see here is that it seems like this neuron likes to fire a lot during
[992.66s -> 997.00s]  the execution when the monkey's hand is moving to the right.
[997.00s -> 1004.38s]  And it also fires a little bit more when the monkey is preparing to move to the left.
[1004.38s -> 1011.22s]  So this means that maybe the neuron is encoding some movement direction here.
[1011.22s -> 1016.62s]  So basically, if you can repeat this experiment for many different neurons and for a lot of
[1016.62s -> 1022.78s]  different directions, eventually what scientists find is that the neurons, like if you fit,
[1022.78s -> 1028.10s]  like say for a single neuron, if you fit the firing rates of that neuron, basically
[1028.10s -> 1033.94s]  how many spikes it's generating every second to different movement directions, you can
[1033.98s -> 1037.38s]  fit this kind of like a cosine tuning curve to it.
[1037.38s -> 1041.82s]  So what this tuning curve means that on the y-axis is the firing rate and on the horizontal
[1041.82s -> 1044.12s]  axis is the movement direction.
[1044.12s -> 1051.18s]  So this neuron prefers to fire the most when the movement is, say, 180 degrees to some
[1051.18s -> 1056.26s]  reference, and then the firing gradually goes down.
[1056.26s -> 1060.70s]  So that's like one of the first things scientists find, like how the single neuron
[1060.70s -> 1065.34s]  is encoding like movement information.
[1065.34s -> 1068.54s]  And now if you measure multiple neurons, like you will find that each neuron could
[1068.54s -> 1070.38s]  encode very different information.
[1070.38s -> 1074.46s]  For example, this green neuron here, its tuning curve is slightly shifted to the right
[1074.46s -> 1076.60s]  and then the magnitude is shifted down.
[1076.60s -> 1081.70s]  So its preferred direction is around maybe 250.
[1081.70s -> 1088.82s]  Now with two neurons, you can actually decode out more, actually decode out what the intended
[1088.82s -> 1090.58s]  movement direction is.
[1090.58s -> 1094.70s]  So for example, with a single neuron, suppose right now I measure the firing rate is around
[1094.70s -> 1100.82s]  like 30 spikes per second, then there could be two movement direction with 120 and then
[1100.82s -> 1101.82s]  240.
[1101.82s -> 1106.34s]  However, with second neurons here, you can see that we can basically eliminate
[1106.34s -> 1112.22s]  the, like suppose we measure the second neuron fires around five spikes per second, then
[1112.22s -> 1118.18s]  we can exactly pinpoint that it's actually the movement direction is 120 instead of
[1118.18s -> 1120.82s]  the other one.
[1120.82s -> 1132.22s]  However, we know that neurons has some noises, so we actually cannot really exactly tell
[1132.22s -> 1135.18s]  the movement direction by using two neurons here.
[1135.18s -> 1140.92s]  So for example, in the third part here, right, due to the noise, the actual, suppose
[1140.92s -> 1149.36s]  the ground shoots like the actual fire rate is those gray lines, but due to the noise,
[1149.36s -> 1153.14s]  the firing rate is slightly shifted to those dashed lines.
[1153.14s -> 1157.76s]  And you can see that originally if we can decode the movement direction is 120, but
[1157.76s -> 1163.04s]  in this case, the possibility becomes like there's four possibilities we cannot exactly,
[1163.04s -> 1165.80s]  we cannot uniquely define.
[1166.80s -> 1171.88s]  However, you can see that maybe it's still more likely that the direction that monkey
[1171.88s -> 1178.80s]  tries to move is like around 120 rather than the one to that which is around 50
[1178.80s -> 1182.36s]  and then the other one is greater than 240, right?
[1182.36s -> 1185.96s]  So how do we deal with this kind of like a noisy neuron?
[1185.96s -> 1191.12s]  Like how can we still uniquely more accurate, like how can we still accurately decide
[1191.12s -> 1198.36s]  decoding this intended movements from this kind of like a multiple neuron recordings?
[1198.36s -> 1202.60s]  I think we can basically use machine learning here, right?
[1202.60s -> 1207.12s]  So we can treat this as a kind of like a classification problem.
[1207.12s -> 1212.36s]  So here we're plotting each dot here is basically a firing the combinations of two
[1212.36s -> 1220.48s]  neurons and the color here basically represents the intended movement direction.
[1220.48s -> 1225.16s]  And now if you like somehow train a machine learning classifier here, now you can basically
[1225.16s -> 1230.40s]  see we can draw some like decision boundaries where I say on the right side where those
[1230.40s -> 1236.36s]  if like on new measurement that we get that the firing rate is somehow drops falls
[1236.36s -> 1241.28s]  onto this here, then we probably know that it's going to be the monkey is trying
[1241.28s -> 1245.40s]  to move to the left direction, right?
[1245.40s -> 1252.80s]  So okay, so I guess here like we know that, okay, we can do this kind of single neuron
[1252.80s -> 1257.52s]  measurement, we can measure firing rates of multiple neurons and then by training
[1257.52s -> 1263.80s]  a machine learning models, we can use this machine learning models with the neural
[1263.80s -> 1267.84s]  data to infer like what's the likely movement directions.
[1267.84s -> 1273.40s]  So this is how we are going to build up to actually build a brain computer interface.
[1273.40s -> 1274.40s]  Questions?
[1274.40s -> 1279.68s]  Yes, for all of these, you've mentioned a neuron one as a very specific number, how do you
[1279.68s -> 1283.04s]  pinpoint which neurons can submit?
[1283.04s -> 1287.08s]  So here, neuron one is basically like we are taking, so we are here making an assumption
[1287.08s -> 1294.60s]  that each tiny electrode here, you see, is measuring like exactly one neuron and
[1294.60s -> 1299.36s]  then that electrode will be fixed, always be fixed, always measuring the firing rates
[1299.36s -> 1303.12s]  of that neurons, yeah.
[1303.16s -> 1308.08s]  But in the real case, it's not always the case because like you think about it, the
[1308.08s -> 1312.80s]  brain is this kind of like a soft structure, so if you put electrodes there, it could
[1312.80s -> 1315.00s]  move a little bit and measure different neurons.
[1315.00s -> 1321.16s]  So that's one of the challenging problems of BCI, is that how you can deal with this
[1321.16s -> 1324.80s]  kind of neural change, recording change, yeah.
[1324.80s -> 1327.16s]  All right, let's go back to here.
[1327.16s -> 1332.52s]  So now we can basically know that we can put some electrodes into the brain, into
[1332.56s -> 1337.96s]  a motor cortex, measure some signals, and then we know how the neuron encodes those
[1337.96s -> 1343.16s]  signals and then we can also build a machine learning decoder to decode those signals.
[1343.16s -> 1348.88s]  So basically have some like methods to be able to build a brain-computer interface
[1348.88s -> 1357.24s]  so that we can interpret what a still functioning, fully functioning brain trying to do.
[1357.24s -> 1367.72s]  One more thing is that, so how we can record these signals, yeah, this is a very complicated
[1367.72s -> 1370.72s]  figure, but don't worry about all the details.
[1370.72s -> 1375.24s]  What I'm trying to show here that basically there's a lot of different technologies
[1375.24s -> 1380.82s]  that you can use to record brain signals, but when you think about these technologies,
[1380.82s -> 1385.20s]  you can think about it as in this kind of two-dimensional way.
[1385.20s -> 1391.66s]  So on the y-axis, just think about it as a spatial resolution.
[1391.66s -> 1400.84s]  So the higher up you go on the y-axis, that means that you can basically measure,
[1400.84s -> 1410.56s]  you can basically measure like very, like the basic shows like what's the region,
[1410.56s -> 1413.32s]  like what's the size of the region of the brain that you can measure.
[1413.32s -> 1417.72s]  So if you go really high up there, then that means that you can only measure, say, a very
[1417.72s -> 1424.92s]  large, like the average brain activity is of a very large brain area, whereas if you
[1424.92s -> 1429.04s]  go down the y-axis, that means that you can actually measure two very fine-grained
[1429.04s -> 1432.36s]  scales like such as a single neurons.
[1432.36s -> 1438.88s]  Whereas the horizontal axis here means like the temporal resolution, that means that
[1439.08s -> 1444.28s]  for technologies such as this kind of like a single neuron recordings, you can basically
[1444.28s -> 1450.60s]  measure like exactly at each time point, for example, like one millisecond what's
[1450.60s -> 1454.52s]  the electric potential is for that single neuron.
[1454.52s -> 1459.92s]  Whereas for technology, recording technology such as MRI, which basically measures like
[1459.92s -> 1465.12s]  the blood flow in a small brain region, you can only measure on average like around
[1465.32s -> 1470.20s]  say 0.5 seconds or one second what's the blood flow changes in that small brain area.
[1470.20s -> 1474.64s]  So that's really like average of a lot of information because we know that the neuron
[1474.64s -> 1477.36s]  fires at this really fast speed, right?
[1477.36s -> 1482.88s]  The firing, the electric potential change of neuron is usually around order of one
[1482.88s -> 1483.72s]  millisecond, right?
[1483.72s -> 1488.96s]  If you can only measure things around like say one second, you are basically like
[1488.96s -> 1492.36s]  averaging smoothing out a lot of information.
[1492.36s -> 1500.20s]  So ideally, we want to have something both have like a high spatial resolution and also
[1500.20s -> 1504.48s]  temporal resolution here.
[1504.48s -> 1511.84s]  So what we will use in most of this right now in a lot of like clinical trial in our
[1511.84s -> 1516.04s]  lab is this kind of like a motor electrode arrays.
[1516.16s -> 1525.16s]  So each electrode here is like a tiny needle that can measure maybe a signal of a few neurons
[1525.16s -> 1530.44s]  and then you put these needles into a small tiny like a square on the size of like a
[1530.44s -> 1535.36s]  fingernail and then you can measure maybe on order of hundreds of neurons.
[1535.36s -> 1542.00s]  All right, so now we can have these devices to measure neurons.
[1542.08s -> 1548.08s]  So let's go into more like examples of how we do this here.
[1553.68s -> 1558.68s]  So here's a, let's take an example here.
[1558.68s -> 1565.60s]  Suppose someone has have like say spinal cord injury and then lost the connection to his
[1565.60s -> 1569.48s]  body, so his body is still fully functioning.
[1569.52s -> 1573.64s]  The question here is that whether we can still, what kind of information we can still decode
[1573.64s -> 1580.36s]  from his motor cortex such that we can decode those informations and then use those informations
[1580.36s -> 1585.96s]  to either control his arm or like his own arm or like artificial arm.
[1585.96s -> 1593.04s]  What we are going to do is trying to put this kind of tiny electrodes, microelectrode
[1593.04s -> 1604.60s]  arrays into his motor cortex, really penetrating into the surface of his motor cortex.
[1604.60s -> 1610.60s]  And each electrode here, as you see here, is this kind of tiny needle and those query
[1610.60s -> 1613.80s]  triangle is the size of a neuron.
[1613.80s -> 1619.84s]  So each electrode maybe is measuring the firing potential of multiple, local field potential
[1619.84s -> 1626.56s]  of multiple neurons around it.
[1626.56s -> 1632.60s]  And we can pass all this information in real time to a computer through this kind
[1632.60s -> 1644.12s]  of a wire right now.
[1644.12s -> 1649.00s]  And then this is what we get on the computer here is that, for example, here each block
[1649.04s -> 1653.80s]  is basically the measurement of that one electrode.
[1653.80s -> 1660.96s]  And if we do some behavior experiments, as we just showed now, we can probably figure
[1660.96s -> 1663.24s]  out the tuning curve for each electrode.
[1663.24s -> 1672.40s]  For example, this one is probably its preferred direction is to the left.
[1672.40s -> 1680.44s]  So we can repeat the experiments, behavior experiments for other channels here and probably
[1680.44s -> 1685.72s]  train an ML decoder to figure out what each channel is encoding, the preferred direction
[1685.72s -> 1686.80s]  for each channel.
[1686.80s -> 1694.32s]  So once we have the decoder trained, and then at test time, we can basically ask our
[1694.32s -> 1700.12s]  participants who has the thing implying his brain to trying to imagine to move his
[1700.12s -> 1702.04s]  hand to some directions.
[1702.04s -> 1706.16s]  And then the decoder we're trying to figure out the direction he's trying to move.
[1706.16s -> 1708.40s]  So that's the basic idea here.
[1708.40s -> 1713.16s]  Let me go to a demo here.
[1713.16s -> 1719.74s]  So this is one of the research coming out of our lab in 2017.
[1719.74s -> 1727.76s]  So here you see a participant is typing on virtual keyboard with her mind.
[1727.76s -> 1736.20s]  And that on the bottom shows the typing speed measured as the correct characters per minute.
[1736.20s -> 1746.68s]  So it peaks around 40 and then on average, maybe it's around like 20.
[1746.68s -> 1756.96s]  I think this is really amazing.
[1756.96s -> 1763.44s]  Think about four people like Howard who used to have to using this kind of a letterboard
[1763.44s -> 1765.16s]  to communicate.
[1765.16s -> 1773.20s]  Now with this green computer interface, he can fully communicate by himself through
[1773.20s -> 1774.88s]  like say a computer.
[1774.88s -> 1781.44s]  So that's a huge improvement over a letterboard.
[1781.44s -> 1785.44s]  He still opens his eyes and she still opens her eyes.
[1785.44s -> 1788.00s]  So is there anything with the eye tracking?
[1788.00s -> 1789.00s]  Not for this experiment.
[1789.00s -> 1795.00s]  So even if she closed the eye, it would still work?
[1795.00s -> 1798.36s]  Yeah, it would still work, but she won't have the visual feedback.
[1798.36s -> 1800.36s]  She won't know where she's typing.
[1800.36s -> 1809.44s]  How about if she came up with a character in the mind without looking at the keyboard?
[1809.44s -> 1810.52s]  Okay.
[1810.52s -> 1812.52s]  That's something I'm going to show next.
[1813.52s -> 1820.52s]  How do you know whether it's the person who mistyped or whether it's the machine that's
[1820.52s -> 1824.52s]  not capturing the correct character?
[1824.52s -> 1828.52s]  Like I'm confused by what you mean by character.
[1828.52s -> 1829.52s]  Okay.
[1829.52s -> 1830.52s]  Oh, here.
[1830.52s -> 1832.52s]  So yeah, that's a good question.
[1832.52s -> 1834.52s]  So let me clarify here.
[1834.52s -> 1840.52s]  I think the task here is, maybe it's not readable, but I think the task here is basically
[1841.52s -> 1844.52s]  she is copying a sentence.
[1844.52s -> 1849.52s]  So we know the ground truth and we can measure the error rate.
[1849.52s -> 1851.52s]  And yeah.
[1851.52s -> 1857.52s]  I was going to ask how do you capture the clicking motion or the selection motion?
[1857.52s -> 1862.52s]  Is that easy to distinguish or is there a certain way of knowing that the user is
[1862.52s -> 1868.52s]  pressing down or does she visualize like a mouse or actually clicking?
[1868.52s -> 1869.52s]  That's really a good question.
[1869.52s -> 1875.52s]  So as I just mentioned, right, so we can decode movements and we can also decode like say
[1875.52s -> 1877.52s]  different gestures.
[1877.52s -> 1882.52s]  Like say you can use this kind of gestures or like move her elbows.
[1882.52s -> 1888.52s]  So you can just imagine different motor movements and then we can basically map, decode those
[1888.52s -> 1894.52s]  motor movements and map that to say a click signal or different signals.
[1895.52s -> 1902.52s]  So let's say if the person look at a keyboard and then remember the keyboard in her memory
[1902.52s -> 1904.52s]  and then she close the eye.
[1904.52s -> 1907.52s]  And then that's the same word.
[1909.52s -> 1913.52s]  I think that's really even like, that's even harder for me to do, right?
[1913.52s -> 1917.52s]  I can't even, like can you remember the keyboard and then just control like say
[1917.52s -> 1918.52s]  a mouse and.
[1918.52s -> 1923.52s]  I use the keyboard every day so I definitely remember the word nation in my mind
[1923.52s -> 1925.52s]  and then just close my eye.
[1925.52s -> 1928.52s]  This is like a virtual keyboard so it's not like a physical keyboard.
[1928.52s -> 1930.52s]  We can use your muscle memories.
[1930.52s -> 1936.52s]  Yeah, so maybe one thing I have to clarify here is that the mental image for her is
[1936.52s -> 1940.52s]  to say controlling like a, what's the word?
[1940.52s -> 1943.52s]  I mean it's just like controlling like a mouse, right?
[1943.52s -> 1949.52s]  So she's not actually doing the touch typing but she is actually moving like say a mouse.
[1956.52s -> 1962.52s]  All right, so this is basically just a showcase that building up on all the knowledge
[1962.52s -> 1970.52s]  we have learned about the brain so we can basically decode some attempted movements
[1970.52s -> 1978.52s]  from people like this, I forgot her name but I think it's T6, a codename T6
[1978.52s -> 1983.52s]  that we can really help these kind of people to regain communication
[1983.52s -> 1985.52s]  through this kind of BCI.
[1985.52s -> 1991.52s]  And I think, as I mentioned earlier, you can also use BCI to control robotic arms.
[1991.52s -> 1999.52s]  So for example in this case, this is the participants in Caltech.
[2000.52s -> 2002.52s]  There you go.
[2005.52s -> 2010.52s]  He's using his mind to control this robotic sound which grabs him a drink.
[2021.52s -> 2022.52s]  Cheers.
[2023.52s -> 2025.52s]  All right.
[2025.52s -> 2026.52s]  Yay.
[2029.52s -> 2031.52s]  Did you finish that thing off?
[2031.52s -> 2032.52s]  That's good.
[2038.52s -> 2040.52s]  And that's that science experiment.
[2046.52s -> 2047.52s]  All right.
[2048.52s -> 2049.52s]  Yay.
[2049.52s -> 2050.52s]  Well done.
[2050.52s -> 2051.52s]  Yay.
[2052.52s -> 2053.52s]  All right.
[2053.52s -> 2059.52s]  So and also you can do things like restore walking abilities.
[2060.52s -> 2066.52s]  I think as someone just mentioned right now, just now is that maybe there's like
[2066.52s -> 2070.52s]  we can try to restore different modalities of communication.
[2070.52s -> 2075.52s]  For example, just now we're just using the movements and by restoring movements
[2075.52s -> 2076.52s]  we can control computer.
[2076.52s -> 2081.52s]  But like how about we directly restore the abilities to do handwriting, right?
[2081.52s -> 2086.52s]  Because handwriting is very natural ways to communicate.
[2086.52s -> 2093.52s]  So Frank Witt, research scientist from our lab in 2021 published a paper
[2093.52s -> 2097.52s]  to show that we can actually do this kind of handwriting BCI.
[2097.52s -> 2102.52s]  And he showed that it's actually really, really fast compared to the previous one.
[2102.52s -> 2107.52s]  Okay, so now we have seen that there's different ways to restore communication.
[2110.52s -> 2114.52s]  Here's just like a measurement of different ways of communicating, right?
[2114.52s -> 2121.52s]  You can see on the very left is say sip and puff interface is very slow.
[2121.52s -> 2125.52s]  Basically that means it's for someone who can not really move
[2125.52s -> 2128.52s]  but can still do some like breathing.
[2128.52s -> 2132.52s]  They can do the sip and puff to say yes and no to communicate.
[2132.52s -> 2136.52s]  That's really slow, maybe around five words per minute.
[2136.52s -> 2141.52s]  For a normal person, I'm really surprised that on average a normal person can write
[2141.52s -> 2145.52s]  maybe like 13 or 14 words per minute.
[2145.52s -> 2148.52s]  That's really slow but maybe that's just the average speed.
[2148.52s -> 2151.52s]  And on the very far right side is the natural communication
[2151.52s -> 2156.52s]  which can reach up to 150 or 160 words per minute.
[2157.52s -> 2159.52s]  Just put everything into context here.
[2159.52s -> 2162.52s]  So the 2D cursor I just showed you guys right now
[2162.52s -> 2166.52s]  can do eight words per minute.
[2166.52s -> 2170.52s]  And the handwriting can do around 18 words per minute.
[2170.52s -> 2177.52s]  So compared to the say letter board or like this kind of eye tracking,
[2177.52s -> 2183.52s]  we are really make a lot of like advancement here.
[2183.52s -> 2188.52s]  But still it's far, way far from like the natural conversation speed.
[2188.52s -> 2192.52s]  So the next question basically is, okay, how can we get there?
[2192.52s -> 2198.52s]  Can we actually restore speech with brain computer interface?
[2203.52s -> 2209.52s]  Together I think there's a huge barrier here.
[2209.52s -> 2212.52s]  First is that the language processing in the brain
[2212.52s -> 2214.52s]  is a really complicated process.
[2214.52s -> 2218.52s]  So for example, here is showing all the brain areas
[2218.52s -> 2220.52s]  that's involved in the language.
[2220.52s -> 2222.52s]  And we still don't know exactly how this happens
[2222.52s -> 2227.52s]  but this is just our best guess at how language is processed in the brain.
[2227.52s -> 2231.52s]  On the very right you see that there's a lot of brain regions
[2231.52s -> 2234.52s]  that's involved with knowledge and reasoning.
[2234.52s -> 2241.52s]  In the center is maybe areas that's involved with semantics and syntax.
[2241.52s -> 2245.52s]  And on the very left is about the perception of speech
[2245.52s -> 2248.52s]  and then the production of speech.
[2248.52s -> 2250.52s]  Language is really complex.
[2250.52s -> 2256.52s]  So maybe the hope here is that we can start with motor cortex,
[2256.52s -> 2258.52s]  with motor planning of language.
[2258.52s -> 2260.52s]  Because I've just shown you guys that we already know
[2260.52s -> 2263.52s]  how the motor cortex can encode movements.
[2263.52s -> 2267.52s]  And we can also know that in order to produce language
[2267.52s -> 2270.52s]  we need to speak and then maybe we can put some electrodes
[2270.52s -> 2272.52s]  into this part of the motor cortex
[2272.52s -> 2275.52s]  that actually controls our orofacial muscles
[2275.52s -> 2277.52s]  and then try to decode some information there
[2277.52s -> 2279.52s]  and see if we can actually restore speech.
[2285.52s -> 2287.52s]  Actually being able to restore speech
[2287.52s -> 2289.52s]  is I think it's more complicated
[2289.52s -> 2292.52s]  compared to the restoring movements.
[2292.52s -> 2294.52s]  So what I'm trying to say here is that
[2294.52s -> 2296.52s]  the production of speech is really
[2296.52s -> 2299.52s]  a lot of complicated movements and it's really rapid.
[2299.52s -> 2301.52s]  It's just more than just moving your hands
[2301.52s -> 2304.52s]  to certain directions.
[2304.52s -> 2306.52s]  So restoring speech is much harder
[2306.52s -> 2313.52s]  than just decoding out movements of each articulator.
[2313.52s -> 2316.52s]  So instead of trying to decode the movements
[2316.52s -> 2318.52s]  of each articulator, because it's very hard, right?
[2318.52s -> 2325.52s]  So also for people who have lost speech,
[2325.52s -> 2329.52s]  basically it's very hard to actually measure
[2329.52s -> 2333.52s]  their speech articulator movements.
[2333.52s -> 2336.52s]  Instead maybe we can try to decode out
[2336.52s -> 2338.52s]  this kind of discrete phonemes
[2338.52s -> 2340.52s]  instead of this kind of continuous speech
[2340.52s -> 2342.52s]  articulator movements because we know that
[2342.52s -> 2345.52s]  all the languages can be decomposed
[2345.52s -> 2348.52s]  into this kind of basic phonetic units.
[2348.52s -> 2350.52s]  For example, for English we know that
[2350.52s -> 2352.52s]  there's different vowels and different consonants.
[2353.52s -> 2355.52s]  They are correlated with how you place
[2355.52s -> 2357.52s]  your talents in your mouth and how you
[2357.52s -> 2359.52s]  place your different speech articulators.
[2359.52s -> 2362.52s]  So here we are trying to, instead of decoding
[2362.52s -> 2364.52s]  the actual articulator movements,
[2364.52s -> 2366.52s]  we are trying to decode this kind of
[2366.52s -> 2369.52s]  discrete phonemic tokens.
[2369.52s -> 2374.52s]  And then there's a previous work showing that
[2374.52s -> 2376.52s]  if you put some electrodes on the motor cortex
[2376.52s -> 2380.52s]  and then you can actually tell the differences
[2380.52s -> 2382.52s]  by measuring the differences of different
[2382.52s -> 2385.52s]  phonemes by measuring the electric activities
[2385.52s -> 2387.52s]  in the motor cortex.
[2387.52s -> 2389.52s]  So there's a hope of being able to
[2389.52s -> 2392.52s]  restore speech by just putting electrodes
[2392.52s -> 2394.52s]  in the motor cortex here.
[2396.52s -> 2401.52s]  And indeed, so in 2021, researchers from UCSF
[2401.52s -> 2404.52s]  actually demonstrated it's actually feasible
[2404.52s -> 2406.52s]  to build this kind of like small vocabulary
[2406.52s -> 2410.52s]  speech PCA with this ECoG recording technology.
[2410.52s -> 2413.52s]  The difference between ECoG and then
[2413.52s -> 2415.52s]  the microelectrode array I just showed you guys
[2415.52s -> 2419.52s]  is that whereas the microelectrode array
[2419.52s -> 2421.52s]  actually penetrates into the cortex,
[2421.52s -> 2423.52s]  but the ECoG stays on cortex.
[2423.52s -> 2425.52s]  So it doesn't actually record single neuron firing.
[2425.52s -> 2428.52s]  It still records some average neural activities
[2428.52s -> 2429.52s]  over a small region.
[2429.52s -> 2431.52s]  So compared to microelectrode arrays,
[2431.52s -> 2435.52s]  they will have a slightly lower resolution.
[2435.52s -> 2438.52s]  So that's why their prototype is
[2438.52s -> 2440.52s]  around this kind of small vocabulary PCA,
[2440.52s -> 2443.52s]  which can only decode 50 words
[2443.52s -> 2446.52s]  at around maybe 75% accuracy.
[2446.52s -> 2448.52s]  But this is still very exciting work
[2448.52s -> 2451.52s]  that showcase that you can actually achieve,
[2451.52s -> 2454.52s]  maybe achieve this kind of speech decoding
[2454.52s -> 2457.52s]  by putting some electrodes into the motor cortex.
[2459.52s -> 2462.52s]  All right, so I'll just right now go into the
[2462.52s -> 2464.52s]  research that's done in our lab,
[2464.52s -> 2466.52s]  which is to build a high-performance speech
[2466.52s -> 2468.52s]  neurocortisies.
[2469.52s -> 2475.52s]  So in 2022, so we recruited a participant
[2475.52s -> 2479.52s]  codename T12, who has ALS.
[2479.52s -> 2485.52s]  So T12, she used to be a very active person.
[2485.52s -> 2488.52s]  She likes to ride horse, likes to jog.
[2488.52s -> 2492.52s]  But because of ALS, a couple years ago
[2492.52s -> 2495.52s]  she basically couldn't do all those things
[2495.52s -> 2497.52s]  that she used to enjoy.
[2497.52s -> 2501.52s]  And unlike most ALS patients,
[2501.52s -> 2505.52s]  her symptoms start with orofacial movements first.
[2505.52s -> 2509.52s]  So she still can move her hands a little bit,
[2509.52s -> 2512.52s]  but she cannot really speak intelligibly.
[2514.52s -> 2518.52s]  So we decided to put four microelectrode arrays
[2518.52s -> 2521.52s]  into her brain, two arrays into her motor cortex
[2521.52s -> 2525.52s]  and then two arrays into the part of the boar cortex area,
[2525.52s -> 2529.52s]  which is supposed to be involved with language planning.
[2529.52s -> 2532.52s]  So the hope here is that we want to both decode
[2532.52s -> 2537.52s]  the execution of speech, the production of speech,
[2537.52s -> 2541.52s]  which is how you control your speech allocators
[2541.52s -> 2544.52s]  and also maybe decode some high-level planning
[2544.52s -> 2546.52s]  about the speech.
[2546.52s -> 2548.52s]  So that's why we want to put arrays into
[2548.52s -> 2550.52s]  two different brain regions here.
[2551.52s -> 2555.52s]  So the first thing we do after we put arrays in her brain
[2555.52s -> 2558.52s]  is that we did some behavior test
[2558.52s -> 2560.52s]  to see what kind of information we can decode
[2560.52s -> 2562.52s]  from those arrays.
[2562.52s -> 2566.52s]  So here's the first result we got
[2566.52s -> 2572.52s]  is that we are trying to classify different tasks here.
[2572.52s -> 2575.52s]  The first plot is that we are trying to use
[2575.52s -> 2580.52s]  these four arrays to classify the orofacial movements.
[2580.52s -> 2583.52s]  So this dashed line is the cue
[2583.52s -> 2587.52s]  that she's actually executing those orofacial movements
[2587.52s -> 2589.52s]  and then before this dashed line
[2589.52s -> 2593.52s]  she is trying to prefer to do those orofacial movements.
[2593.52s -> 2597.52s]  So you can see that these two right lines here
[2597.52s -> 2603.52s]  show that you can classify, you can predict those movements
[2603.52s -> 2606.52s]  much better above chance using these two arrays
[2606.52s -> 2608.52s]  in the motor cortex, whereas these two arrays
[2608.52s -> 2611.52s]  in the broadcast area, you can basically,
[2611.52s -> 2614.52s]  you can't really predict too much above chance,
[2614.52s -> 2617.52s]  especially during the execution of those movements.
[2617.52s -> 2621.52s]  And for single phonemes, which we instruct
[2621.52s -> 2624.52s]  our participants to speak single English phonemes,
[2624.52s -> 2628.52s]  you can also predict those things
[2628.52s -> 2631.52s]  much higher above chance using these two arrays
[2631.52s -> 2634.52s]  from the motor cortex and also for the words,
[2634.52s -> 2637.52s]  for single words.
[2637.52s -> 2640.52s]  So what these results tell us is that
[2640.52s -> 2643.52s]  for those two arrays we have put into T12's mind,
[2643.52s -> 2645.52s]  these two arrays in the motor cortex
[2645.52s -> 2648.52s]  contains a lot of information about the phonemes
[2648.52s -> 2652.52s]  being articulated and also the words being articulated.
[2652.52s -> 2654.52s]  But those two arrays in the broadcast area,
[2654.52s -> 2657.52s]  which is supposed to help us to figure out
[2657.52s -> 2659.52s]  the planning of the speech production
[2659.52s -> 2661.52s]  doesn't contain too much information.
[2661.52s -> 2663.52s]  So that's really intriguing to us
[2663.52s -> 2666.52s]  and we're still trying to figure out why that's true.
[2666.52s -> 2670.52s]  So for the rest of the talk here,
[2670.52s -> 2672.52s]  we're mostly using only these two arrays
[2672.52s -> 2674.52s]  in the motor cortex here.
[2675.52s -> 2679.52s]  So now we know that there's phonetic information
[2679.52s -> 2682.52s]  being encoded in those two arrays.
[2682.52s -> 2685.52s]  What we're going to do next is actually trying to build
[2685.52s -> 2690.52s]  a real-time brain-to-text BCI here.
[2690.52s -> 2692.52s]  So what we're going to do is,
[2692.52s -> 2695.52s]  let me just show you a video demo first
[2695.52s -> 2702.52s]  to get a sense of what the BCI is trying to build.
[2702.52s -> 2705.52s]  So here, this is our participants.
[2705.52s -> 2708.52s]  So she is connected to our decoding machine
[2708.52s -> 2710.52s]  through this cable here,
[2710.52s -> 2712.52s]  which transmits her neural signals in real-time
[2712.52s -> 2714.52s]  to a decoding machine.
[2714.52s -> 2716.52s]  On this screen you can see that there's a sentence there
[2716.52s -> 2719.52s]  that we instructed her to copy,
[2719.52s -> 2721.52s]  basically to read out.
[2721.52s -> 2726.52s]  And once this red square turns green,
[2726.52s -> 2728.52s]  she will try to speak.
[2728.52s -> 2731.52s]  And then what you will see below here
[2731.52s -> 2733.52s]  is what the machine has decoded.
[2752.52s -> 2754.52s]  That will be good.
[2762.52s -> 2764.52s]  I did well in school.
[2774.52s -> 2777.52s]  I don't see much pollution turning on.
[2778.52s -> 2782.52s]  All right, so that's almost perfect decoding results
[2782.52s -> 2783.52s]  from her.
[2783.52s -> 2784.52s]  And you can tell from the video
[2784.52s -> 2786.52s]  that although she can vocalize,
[2786.52s -> 2788.52s]  but it's not really intelligible
[2788.52s -> 2793.52s]  because of her limited artificial muscle movements.
[2793.52s -> 2796.52s]  But we can still decode out from her brain signal
[2796.52s -> 2798.52s]  what she is trying to say.
[2798.52s -> 2799.52s]  And this video is,
[2799.52s -> 2801.52s]  so the task I just showed you is
[2801.52s -> 2803.52s]  she's trying to copy a sentence.
[2803.52s -> 2806.52s]  This is trying to answer a question here.
[2807.52s -> 2808.52s]  Okay.
[2823.52s -> 2826.52s]  And we also tried different modalities,
[2826.52s -> 2831.52s]  which is because when she tries to attempt to articulate,
[2831.52s -> 2833.52s]  it's actually very tiring for her
[2833.52s -> 2836.52s]  to actually articulate those sounds.
[2836.52s -> 2838.52s]  So what we try here is that only instructed her
[2838.52s -> 2841.52s]  to move her mouth or move her articulators,
[2841.52s -> 2843.52s]  but not vocalize.
[2843.52s -> 2845.52s]  So what we call this is silent speech.
[2845.52s -> 2847.52s]  And we can still decode pretty well
[2847.52s -> 2850.52s]  using this kind of silent speech modality.
[2866.52s -> 2884.52s]  All right, okay.
[2884.52s -> 2886.52s]  Okay, so let's just move on more technical details
[2886.52s -> 2891.52s]  about how we build this speech preset here.
[2891.52s -> 2894.52s]  So as I just mentioned, right,
[2894.52s -> 2895.52s]  so the first thing we need to do
[2895.52s -> 2897.52s]  is to try to build a decoder.
[2897.52s -> 2899.52s]  And before building that decoder,
[2899.52s -> 2901.52s]  we need to do some data collection.
[2901.52s -> 2903.52s]  So here's our research scientist Frank
[2903.52s -> 2905.52s]  sitting next to T12
[2905.52s -> 2908.52s]  and asking her to read that sentence on the screen.
[2908.52s -> 2911.52s]  And then we'll record the neural activities
[2911.52s -> 2913.52s]  of her saying that sentence.
[2913.52s -> 2915.52s]  So we'll have this kind of paired data,
[2915.52s -> 2916.52s]  collected paired data,
[2916.52s -> 2918.52s]  where the input is the neural activity
[2918.52s -> 2920.52s]  and the output is the targeted sentence
[2920.52s -> 2922.52s]  we want to decode.
[2922.52s -> 2927.52s]  So we basically go to where T12 lived
[2927.52s -> 2930.52s]  and then we'll do some data collection session there
[2930.52s -> 2934.52s]  and then testing the decoder.
[2934.52s -> 2937.52s]  The way we collect data is that
[2937.52s -> 2941.52s]  because we only have very limited time
[2941.52s -> 2946.52s]  and we cannot ask T12 to speak a lot of sentences.
[2946.52s -> 2950.52s]  So we'll divide our data collection
[2950.52s -> 2951.52s]  into this kind of block structure
[2951.52s -> 2955.52s]  where we instructed her to speak 40 sentences every block
[2955.52s -> 2956.52s]  and then she take a break
[2956.52s -> 2958.52s]  and then we collect another block.
[2958.52s -> 2961.52s]  So the data collection will last about 100 minutes
[2961.52s -> 2964.52s]  for every research session.
[2964.52s -> 2967.52s]  And then we'll train a decoder.
[2967.52s -> 2969.52s]  Maybe that takes like 10 to 20 minutes.
[2969.52s -> 2971.52s]  It's really quick.
[2971.52s -> 2972.52s]  After training a decoder,
[2972.52s -> 2975.52s]  we'll start actually evaluating the performance of decoder
[2975.52s -> 2978.52s]  by asking our participants to speak some new sentences
[2978.52s -> 2980.52s]  and then see how well we can decode
[2980.52s -> 2983.52s]  on those new set of sentences.
[2983.52s -> 2987.52s]  So in total, we did the experiment sessions
[2987.52s -> 2989.52s]  over maybe three months of time
[2989.52s -> 2993.52s]  and then we collect about like 10,000 sentences
[2993.52s -> 2999.52s]  from this like a switchboard telephone conversation corpus,
[2999.52s -> 3001.52s]  which we really want to emphasize
[3001.52s -> 3005.52s]  that we want to decode this kind of conversational English.
[3005.52s -> 3007.52s]  Once we have the data,
[3007.52s -> 3010.52s]  then we can try to see how we can design a decoder
[3010.52s -> 3012.52s]  that can best solve this task.
[3012.52s -> 3015.52s]  So let's first define the problem here.
[3015.52s -> 3018.52s]  So we have some neural features inputs, right?
[3018.52s -> 3020.52s]  So let's say we have some neural features,
[3020.52s -> 3022.52s]  which is a time series.
[3022.52s -> 3025.52s]  You can think about as maybe similar to audio
[3025.52s -> 3034.52s]  that at each time point we'll get some feature vectors.
[3034.52s -> 3037.52s]  The output of this decoder is a set of words, right?
[3037.52s -> 3040.52s]  So we know that she's trying to speak some sentence,
[3040.52s -> 3042.52s]  so we are trying to decode the words
[3042.52s -> 3047.52s]  from this input neural features here.
[3047.52s -> 3049.52s]  As I mentioned earlier,
[3049.52s -> 3052.52s]  so instead of directly decoding words
[3052.52s -> 3054.52s]  from the input sentences,
[3054.52s -> 3057.52s]  maybe we want to have this intermediate target
[3057.52s -> 3059.52s]  of phonemes to decode.
[3059.52s -> 3061.52s]  The reason is that first we know
[3061.52s -> 3065.52s]  that there's only 40 phonemes in English,
[3065.52s -> 3067.52s]  so that's a much smaller set compared to the number of words.
[3067.52s -> 3069.52s]  So if you want to train a decoder
[3069.52s -> 3071.52s]  that can actually decode words,
[3071.52s -> 3073.52s]  then you need to have much more data
[3073.52s -> 3075.52s]  to cover all the possible words.
[3075.52s -> 3076.52s]  Whereas for phonemes,
[3076.52s -> 3078.52s]  you probably need way less data
[3078.52s -> 3080.52s]  to cover all the 40 phonemes here.
[3080.52s -> 3083.52s]  So instead of directly decoding the words,
[3083.52s -> 3086.52s]  we decided to decode an intermediate representation
[3086.52s -> 3090.52s]  of phonemes from the neural input features.
[3090.52s -> 3093.52s]  Okay, so basically there's two decoders we want to design.
[3093.52s -> 3095.52s]  One is the first is a neural two-phoneme decoder,
[3095.52s -> 3097.52s]  and the second is a phoneme two-word decoder.
[3097.52s -> 3099.52s]  So that's the two decoders
[3099.52s -> 3103.52s]  that we'll have to design this task.
[3103.52s -> 3107.52s]  Let's focus on the neural two-phoneme decoder first.
[3107.52s -> 3111.52s]  So basically I think at this point of class,
[3111.52s -> 3113.52s]  we probably know that we can treat this problem
[3113.52s -> 3115.52s]  as a sequence-to-sequence problem, right?
[3115.52s -> 3117.52s]  So the input is some feature sequence,
[3117.52s -> 3120.52s]  the output is a token sequence.
[3120.52s -> 3123.52s]  And the four-sequence, two-sequence problem,
[3123.52s -> 3125.52s]  we probably know that we can use
[3125.52s -> 3128.52s]  some encoder and decoder models
[3128.52s -> 3130.52s]  to solve this problem, right?
[3130.52s -> 3133.52s]  However, for encoder and decoder model,
[3133.52s -> 3136.52s]  it's actually more powerful than we actually need
[3136.52s -> 3139.52s]  because encoder and decoder model
[3139.52s -> 3141.52s]  allows this kind of arbitrary alignments
[3141.52s -> 3143.52s]  between inputs and outputs.
[3143.52s -> 3145.52s]  That's really helpful for tasks
[3145.52s -> 3146.52s]  such as machine translation,
[3146.52s -> 3150.52s]  whereas some languages have different order
[3150.52s -> 3154.52s]  than other languages.
[3154.52s -> 3159.52s]  But here, we know that the alignment
[3159.52s -> 3161.52s]  is actually more monotonic
[3161.52s -> 3163.52s]  compared to the same machine translation
[3163.52s -> 3165.52s]  where the alignment is arbitrary.
[3165.52s -> 3168.52s]  And monotonic means that you know that
[3168.52s -> 3171.52s]  probably, say for example,
[3171.52s -> 3173.52s]  the first two neural features
[3173.52s -> 3176.52s]  corresponds to the first phonemes in output sentence
[3176.52s -> 3178.52s]  rather than the last phoneme in the output sentence, right?
[3178.52s -> 3182.52s]  So this is kind of the monotonic alignment.
[3182.52s -> 3187.52s]  So to solve these problems of monotonic alignments,
[3187.52s -> 3190.52s]  we can actually borrow idea that people have developed
[3190.52s -> 3193.52s]  for machine learning tasks
[3193.52s -> 3196.52s]  such as handwriting recognition and speech recognition,
[3196.52s -> 3199.52s]  where the task is also trying to decode
[3199.52s -> 3203.52s]  a letter sequence or phoneme sequence,
[3203.52s -> 3210.52s]  also letter sequences from some speech features.
[3210.52s -> 3212.52s]  And the technique we are going to use
[3212.52s -> 3214.52s]  is called connectionist temporal classification.
[3214.52s -> 3217.52s]  So for people who have taken CS224S,
[3217.52s -> 3220.52s]  you probably already know what this means,
[3220.52s -> 3222.52s]  but I'm going to do a little bit of introduction
[3222.52s -> 3224.52s]  about this thing.
[3224.52s -> 3226.52s]  I guess I don't have too much time,
[3226.52s -> 3228.52s]  but I'll just quickly go right here.
[3228.52s -> 3232.52s]  So what CDC, the connectionist classification, do
[3232.52s -> 3235.52s]  is that giving some input sequence, right,
[3235.52s -> 3238.52s]  the goal is that we want to decode some output sequence,
[3238.52s -> 3241.52s]  but we don't know the exact alignment between them,
[3241.52s -> 3245.52s]  and usually the input output has some lens mismatch.
[3245.52s -> 3249.52s]  For example, in the case of, say, speech recognition,
[3249.52s -> 3253.52s]  where the input could have a lens
[3253.52s -> 3257.52s]  of several thousands of frames, right?
[3257.52s -> 3260.52s]  Where each frame corresponds to a very fine level,
[3260.52s -> 3263.52s]  like fine high temporal resolution feature
[3263.52s -> 3266.52s]  that's recorded at, say, 20 milliseconds,
[3266.52s -> 3269.52s]  whereas the output only has a few tokens.
[3269.52s -> 3272.52s]  So that's like a huge lens mismatch here.
[3272.52s -> 3276.52s]  What we can do is we can still use,
[3276.52s -> 3279.52s]  like, say, RNN transformer model
[3279.52s -> 3284.52s]  to predict what the output token is at each time step.
[3284.52s -> 3287.52s]  And then we somehow have to figure out a way
[3287.52s -> 3291.52s]  to fill in between the output tokens,
[3291.52s -> 3294.52s]  some, like, spacers so that the output token
[3294.52s -> 3296.52s]  can also have the same lens as the input,
[3296.52s -> 3298.52s]  output sequence can have the same lens
[3298.52s -> 3299.52s]  as the input sequence.
[3299.52s -> 3301.52s]  So what the CDC loss does
[3301.52s -> 3307.52s]  is introduce this additional blank token as output.
[3307.52s -> 3310.52s]  With this blank token, what you can actually do is,
[3310.52s -> 3314.52s]  for example, here is an example output
[3314.52s -> 3316.52s]  of the CDC classifier.
[3316.52s -> 3320.52s]  It's trying to produce this kind of sequence here, right?
[3320.52s -> 3324.52s]  What you can do is first you merge repeated tokens,
[3324.52s -> 3327.52s]  and then you're taking out that blank tokens.
[3327.52s -> 3329.52s]  So what you get is a much shorter sequence
[3329.52s -> 3331.52s]  that corresponds to output.
[3331.52s -> 3335.52s]  So what CDC loss does is it allows you
[3335.52s -> 3337.52s]  to do a sequence-to-sequence problem
[3337.52s -> 3340.52s]  that has a different input and output lens
[3340.52s -> 3346.52s]  and also has this kind of monotonic alignment property.
[3346.52s -> 3349.52s]  Let me just skip through this detail here,
[3349.52s -> 3353.52s]  how you can train a CDC loss here.
[3353.52s -> 3358.52s]  Just skip through this thing.
[3358.52s -> 3362.52s]  So now let's suppose that we have this CDC loss
[3362.52s -> 3366.52s]  that can actually be used to train our model, right?
[3366.52s -> 3369.52s]  The next problem is what kind of decoder
[3369.52s -> 3372.52s]  we want to use for this task.
[3372.52s -> 3373.52s]  What kind of neural network decoder
[3373.52s -> 3375.52s]  to use for this task?
[3375.52s -> 3376.52s]  I think at this point of class,
[3376.52s -> 3377.52s]  I think most of you guys are convinced
[3377.52s -> 3379.52s]  that transformer is really powerful, right?
[3379.52s -> 3382.52s]  There's no reason for me to say more about it.
[3382.52s -> 3384.52s]  In this case, we don't want to use transformer.
[3384.52s -> 3387.52s]  The reason is that we don't have a large data set,
[3387.52s -> 3388.52s]  as I mentioned previously.
[3388.52s -> 3390.52s]  We only have 10,000 sentences, right?
[3390.52s -> 3392.52s]  And also, transformer is really good
[3392.52s -> 3394.52s]  at dealing with long-range dependencies.
[3394.52s -> 3397.52s]  But here, for the speech production,
[3397.52s -> 3401.52s]  there's no really required for long-range dependency.
[3401.52s -> 3404.52s]  So let's just go back to the very simple RNN.
[3404.52s -> 3407.52s]  We know that RNN works for small data sets,
[3407.52s -> 3413.52s]  and it can deal with short-range dependency pretty well.
[3413.52s -> 3415.52s]  And another nice thing about RNN
[3415.52s -> 3417.52s]  is that it's very efficient to run in real time.
[3417.52s -> 3420.52s]  You can do, like, put a very complicated RNN,
[3420.52s -> 3423.52s]  even run very efficiently on your mobile phone.
[3424.52s -> 3431.52s]  One, like, the most popular RNN we have learned
[3431.52s -> 3433.52s]  is the LSTM, right?
[3433.52s -> 3436.52s]  It uses this kind of, like, memory state here.
[3436.52s -> 3437.52s]  Where's my cursor?
[3437.52s -> 3439.52s]  It uses this memory state
[3439.52s -> 3442.52s]  to try to store some long-term,
[3442.52s -> 3444.52s]  like, long-range information,
[3444.52s -> 3448.52s]  and then use this different input and forget gate,
[3448.52s -> 3449.52s]  input and output gates,
[3449.52s -> 3451.52s]  to control how you can read and write
[3451.52s -> 3452.52s]  into that memory state, right?
[3453.52s -> 3457.52s]  But LSTM is also very complicated.
[3457.52s -> 3460.52s]  There's a variant of LSTM called GRU,
[3460.52s -> 3462.52s]  Gated Recurring Unit.
[3462.52s -> 3464.52s]  It tries, like, I think the idea here
[3464.52s -> 3466.52s]  is that it's trying to combine this memory state
[3466.52s -> 3469.52s]  and the hidden state into just one hidden state.
[3469.52s -> 3473.52s]  By doing that, you can also reduce some gates.
[3473.52s -> 3477.52s]  So GRU is basically a more simpler version of LSTM
[3477.52s -> 3480.52s]  that works really well when you have a small data set.
[3480.52s -> 3485.52s]  So here we use GRU instead of LSTM for our task.
[3486.52s -> 3488.52s]  So now we know, like, how we can decode phonemes,
[3488.52s -> 3490.52s]  and now we have neural network models
[3490.52s -> 3492.52s]  to decode phonemes, right?
[3492.52s -> 3493.52s]  We know how to train the model.
[3493.52s -> 3495.52s]  So at inference time,
[3495.52s -> 3497.52s]  by inference time I mean that at testing time,
[3497.52s -> 3501.52s]  you can pass in some neural activities into our decoders,
[3501.52s -> 3502.52s]  and then you will decode out
[3502.52s -> 3504.52s]  some, like, phoneme probabilities, right?
[3504.52s -> 3507.52s]  So there's maybe at the first timestamp
[3507.52s -> 3509.52s]  the highest probability is high.
[3510.52s -> 3512.52s]  The problem here is how do I figure out
[3512.52s -> 3515.52s]  the most likely output sequences
[3515.52s -> 3517.52s]  given this phoneme probability, right?
[3517.52s -> 3519.52s]  So basically the task is to find
[3519.52s -> 3521.52s]  the most likely output sequences here.
[3524.52s -> 3526.52s]  I think for this problem,
[3526.52s -> 3529.52s]  I think since we have already did something similar
[3529.52s -> 3530.52s]  in the assignment three,
[3530.52s -> 3532.52s]  which is that we can use beam search
[3532.52s -> 3534.52s]  to figure out the most likely sequence here.
[3535.52s -> 3537.52s]  However, there's one caveat with the beam search
[3537.52s -> 3540.52s]  when you're applying it to the CTC loss,
[3540.52s -> 3543.52s]  but I'm not going to expand it too much here.
[3545.52s -> 3547.52s]  Yeah, so let's just skip over that.
[3550.52s -> 3552.52s]  Now suppose that we can use the beam search
[3552.52s -> 3555.52s]  to find the most likely phoneme sequences.
[3555.52s -> 3558.52s]  How do we convert that phoneme sequences into words, right?
[3558.52s -> 3559.52s]  So that's because, like,
[3559.52s -> 3561.52s]  we eventually want to decode sentences,
[3561.52s -> 3564.52s]  but not just, like, a sequence of phonemes.
[3565.52s -> 3568.52s]  So one thing you can modify the beam search is that
[3568.52s -> 3570.52s]  you can, if you have, like, an English dictionary
[3570.52s -> 3573.52s]  where you can map each word into its pronunciations,
[3573.52s -> 3575.52s]  then while doing the beam search,
[3575.52s -> 3576.52s]  we're going to basically see that
[3576.52s -> 3578.52s]  if you decode some phoneme sequences
[3578.52s -> 3579.52s]  that corresponds to words
[3579.52s -> 3581.52s]  and can basically replace that phoneme sequence
[3581.52s -> 3582.52s]  with that word, right?
[3585.52s -> 3586.52s]  However, you can actually do better
[3586.52s -> 3589.52s]  by using a language model.
[3589.52s -> 3591.52s]  For example, here,
[3591.52s -> 3598.52s]  that's the decoding equation.
[3598.52s -> 3601.52s]  What we want to do here is that
[3601.52s -> 3603.52s]  here the x is the input,
[3603.52s -> 3606.52s]  the y is the decoded word sequences.
[3606.52s -> 3610.52s]  And then, because not all word sequences
[3610.52s -> 3612.52s]  have the same likelihood, right?
[3612.52s -> 3614.52s]  So some word sequences, say,
[3614.52s -> 3616.52s]  suppose I decode a sentence, say,
[3616.52s -> 3617.52s]  called I can spoke.
[3617.52s -> 3620.52s]  That doesn't seem like syntactically correct.
[3620.52s -> 3622.52s]  So we can maybe use the language model
[3622.52s -> 3625.52s]  to evaluate the probabilities of each decoded hypothesis
[3625.52s -> 3628.52s]  and then using that as some sort of weight
[3628.52s -> 3632.52s]  on the final decoding probabilities.
[3632.52s -> 3634.52s]  So we're adding this extra term here
[3634.52s -> 3637.52s]  called the probabilities of a sentence here.
[3637.52s -> 3639.52s]  Actually, you can just decompose that
[3639.52s -> 3643.52s]  into the probabilities of each token
[3643.52s -> 3645.52s]  given its previous tokens.
[3645.52s -> 3646.52s]  And you can basically measure these things
[3646.52s -> 3648.52s]  using any language model, right?
[3650.52s -> 3652.52s]  Another thing we want to add here is
[3652.52s -> 3654.52s]  another term is a word insertion bonus.
[3654.52s -> 3657.52s]  So one problem about this language model,
[3657.52s -> 3660.52s]  this probability of sentence is that
[3660.52s -> 3663.52s]  actually longer sentences will have
[3663.52s -> 3665.52s]  smaller probabilities than shorter sentences.
[3665.52s -> 3667.52s]  That's just like the properties
[3667.52s -> 3672.52s]  of how you decompose this probability here.
[3672.52s -> 3675.52s]  So we want to actually balance the length
[3675.52s -> 3677.52s]  of the decoded sequence here
[3677.52s -> 3679.52s]  by adding some word insertion bonus.
[3679.52s -> 3684.52s]  So eventually trying to optimize this equation here
[3684.52s -> 3689.52s]  using both the probabilities generated by the Rn decoder
[3689.52s -> 3692.52s]  and then using some sort of language model,
[3692.52s -> 3695.52s]  like weights and then word insertion bonus.
[3695.52s -> 3698.52s]  And also some weights here you can optimize.
[3698.52s -> 3701.52s]  Okay, just trying to put everything together.
[3701.52s -> 3704.52s]  So suppose that you have a neural feature inputs here,
[3704.52s -> 3706.52s]  which is you get the neural features
[3706.52s -> 3708.52s]  every 20 milliseconds.
[3708.52s -> 3710.52s]  You pass that through GRU,
[3710.52s -> 3712.52s]  and now you've got some foreign probabilities, right?
[3712.52s -> 3714.52s]  This all happens in real time,
[3714.52s -> 3716.52s]  so everything has to happen under,
[3716.52s -> 3718.52s]  like all the computation needs to be done
[3718.52s -> 3720.52s]  within 20 milliseconds.
[3720.52s -> 3721.52s]  You do a really quick beam search,
[3721.52s -> 3722.52s]  and then you find, okay,
[3722.52s -> 3727.52s]  maybe this volume corresponds to the word I,
[3727.52s -> 3729.52s]  or the word I.
[3729.52s -> 3731.52s]  And then here we want to use the n-gram language model
[3731.52s -> 3733.52s]  instead of like a more powerful
[3733.52s -> 3734.52s]  transformer language model.
[3734.52s -> 3737.52s]  The reason is that we want to really do
[3737.52s -> 3741.52s]  a lot of evaluation really quickly under 20 milliseconds.
[3741.52s -> 3745.52s]  So suppose that you have like, say, 100 hypothesis, right?
[3745.52s -> 3746.52s]  And then you want to all evaluate
[3746.52s -> 3748.52s]  the possibility of them.
[3748.52s -> 3750.52s]  If you use a transformer language model,
[3750.52s -> 3753.52s]  such as GPT-3, which is really powerful,
[3753.52s -> 3756.52s]  but you cannot really do really fast inferences
[3756.52s -> 3758.52s]  under 20 milliseconds, right?
[3758.52s -> 3759.52s]  So whereas the n-gram language model,
[3759.52s -> 3761.52s]  you can just load everything into memory
[3761.52s -> 3763.52s]  and all the evaluation is just a memory lookup,
[3763.52s -> 3766.52s]  so it's really quick.
[3766.52s -> 3768.52s]  After that, you can get some probabilities out,
[3768.52s -> 3771.52s]  and then you just keep, say, the top K hypothesis
[3771.52s -> 3774.52s]  for the next step of a beam search here.
[3774.52s -> 3777.52s]  So that's how we use the n-gram language model
[3777.52s -> 3781.52s]  in the real-time decoding.
[3781.52s -> 3783.52s]  After that, we'll use the transformer language model
[3783.52s -> 3788.52s]  to re-rank all the hypothesis generated
[3788.52s -> 3790.52s]  by the n-gram language model.
[3790.52s -> 3792.52s]  So this actually happens when you actually decode it
[3792.52s -> 3793.52s]  out the entire sentence.
[3793.52s -> 3797.52s]  Say, I can keep the most likely 100 sentences,
[3797.52s -> 3799.52s]  and then I can, at this time,
[3799.52s -> 3801.52s]  I can use the transformer language model,
[3801.52s -> 3804.52s]  which can quickly evaluate the probabilities,
[3804.52s -> 3806.52s]  say, only for 100 hypothesis
[3806.52s -> 3809.52s]  under the time of maybe half a second, right?
[3809.52s -> 3812.52s]  And then I can get a better probability measurement
[3812.52s -> 3815.52s]  of all the sentences here.
[3815.52s -> 3817.52s]  Yeah, so putting everything together,
[3817.52s -> 3819.52s]  this is how the entire system works
[3819.52s -> 3821.52s]  when I show you the previous video,
[3822.52s -> 3826.52s]  is that we can, right now, using this complicated,
[3826.52s -> 3829.52s]  not compact, multi-stage machine learning model
[3829.52s -> 3832.52s]  to accurately decode what the person is trying to say
[3832.52s -> 3837.52s]  and build this high-performance neural speech PCI.
[3839.52s -> 3840.52s]  All right, well, almost time here,
[3840.52s -> 3843.52s]  so just skip the evaluation part.
[3843.52s -> 3845.52s]  Evaluation is how we measure the performance.
[3845.52s -> 3847.52s]  It's basically measuring the word error rate.
[3848.52s -> 3853.52s]  It will also have all the data open as a competition,
[3853.52s -> 3854.52s]  so if you guys are trying,
[3854.52s -> 3856.52s]  really want to curious about this thing,
[3856.52s -> 3858.52s]  you can try to play around with it.
[3862.52s -> 3864.52s]  I think the most exciting thing about doing this research
[3864.52s -> 3868.52s]  is that you actually see how your research
[3868.52s -> 3870.52s]  can impact people here.
[3870.52s -> 3872.52s]  So this is a quote from our participant, T12,
[3872.52s -> 3874.52s]  and then this is how she reacts
[3874.52s -> 3877.52s]  when this thing first worked for her.
[3877.52s -> 3880.52s]  It's really exciting that she can speak
[3880.52s -> 3883.52s]  after so many years of silence.
[3885.52s -> 3887.52s]  Okay, so in the last five minutes,
[3887.52s -> 3888.52s]  maybe I can just go a little bit
[3888.52s -> 3892.52s]  into what I think is the future of BCIs here.
[3892.52s -> 3893.52s]  So I think what I've just shown you guys
[3893.52s -> 3896.52s]  is that using BCIs, we can help people
[3896.52s -> 3901.52s]  to either restore movements or restore communication.
[3902.52s -> 3905.52s]  One exciting direction, I think,
[3905.52s -> 3909.52s]  is this kind of multimodal BCI.
[3909.52s -> 3913.52s]  Here is a work published by a group at UCSF
[3913.52s -> 3915.52s]  is that they are trying to decode
[3915.52s -> 3920.52s]  not only the phonemes, but also speech,
[3920.52s -> 3922.52s]  I mean, the actual speech,
[3922.52s -> 3925.52s]  and then also some article gestures
[3925.52s -> 3929.52s]  so that you can actually move 3D avatars here.
[3930.52s -> 3932.52s]  And also, as I just mentioned,
[3932.52s -> 3939.52s]  is that the final goal of the speech BCI
[3939.52s -> 3941.52s]  is that you want to actually deploy it
[3941.52s -> 3944.52s]  for people to be able to use it every day
[3944.52s -> 3946.52s]  just as we use our phones.
[3948.52s -> 3951.52s]  So here's a more recent development of speech BCI
[3951.52s -> 3954.52s]  by our collaborators at UC Davis.
[3954.52s -> 3956.52s]  What they do is they actually put four arrays
[3956.52s -> 3958.52s]  into the motor cortex,
[3958.52s -> 3961.52s]  and actually have better signals than we do.
[3961.52s -> 3963.52s]  So what I can actually show you is that,
[3963.52s -> 3966.52s]  so for here, just for reference,
[3966.52s -> 3968.52s]  I just forgot to mention that
[3968.52s -> 3970.52s]  the final performance of our system
[3970.52s -> 3975.52s]  is that we can get maybe around 25% word error.
[3975.52s -> 3978.52s]  It means that for every 100 words
[3978.52s -> 3982.52s]  the participant says 25 of them maybe is wrong.
[3982.52s -> 3986.52s]  So for this latest work at the UC Davis,
[3986.52s -> 3989.52s]  they show that you can actually get close to zero
[3989.52s -> 3992.52s]  word error rate in a few sessions
[3992.52s -> 3995.52s]  by training the system more and more continuously.
[3995.52s -> 3998.52s]  So it's actually being very close
[3998.52s -> 4001.52s]  to being actually a usable system right now.
[4001.52s -> 4003.52s]  And here's a video of their participants
[4003.52s -> 4006.52s]  using the system to speak.
[4006.52s -> 4008.52s]  It's very accurate.
[4010.52s -> 4012.52s]  He's actually using the system
[4013.52s -> 4016.52s]  every day right now for communicating
[4016.52s -> 4018.52s]  with his family and for work.
[4025.52s -> 4028.52s]  That really cannot be understand how important that is.
[4030.52s -> 4033.52s]  All right, so I think the most exciting direction
[4033.52s -> 4035.52s]  I think happened, at least personally I think
[4035.52s -> 4037.52s]  is happening in our lab is that
[4038.52s -> 4041.52s]  we're trying to, maybe trying to restore
[4041.52s -> 4043.52s]  more effortless and natural communication
[4043.52s -> 4045.52s]  by decoding this kind of inner speech.
[4045.52s -> 4049.52s]  So previously all the speech I just show you,
[4049.52s -> 4051.52s]  I think the maximum speed we can do is
[4051.52s -> 4054.52s]  maybe decode at 60 to 70 words per minute,
[4054.52s -> 4057.52s]  but that's still far slower compared to natural conversation
[4057.52s -> 4060.52s]  which happens at 150 words per minute.
[4060.52s -> 4062.52s]  So one of the reason is that
[4062.52s -> 4064.52s]  for all these participants,
[4064.52s -> 4067.52s]  if we ask them to try to attempt to speak
[4067.52s -> 4070.52s]  because they have been lost speech so many years,
[4070.52s -> 4073.52s]  it's very hard for them to speak at a normal rate.
[4073.52s -> 4076.52s]  However, we know that a lot of people
[4076.52s -> 4078.52s]  have this kind of inner speech, right?
[4078.52s -> 4081.52s]  We're kind of like talking to ourself in our mind.
[4081.52s -> 4083.52s]  I think the research question here is
[4083.52s -> 4086.52s]  whether we can decode this sort of inner speech.
[4087.52s -> 4090.52s]  So it is like some preliminary work
[4090.52s -> 4092.52s]  from one collaborators in our lab
[4092.52s -> 4094.52s]  show that you can actually do so.
[4094.52s -> 4096.52s]  So for example here,
[4096.52s -> 4098.52s]  the results show that
[4098.52s -> 4100.52s]  if you decode attempted speech,
[4100.52s -> 4102.52s]  which is why I just show you,
[4102.52s -> 4105.52s]  that you can do maybe for a small set of words,
[4105.52s -> 4109.52s]  you can do say at the 90% accuracy,
[4109.52s -> 4111.52s]  but if you ask the participants
[4111.52s -> 4114.52s]  to imagining moving her mouth
[4114.52s -> 4117.52s]  or imagining like a voice in her head,
[4117.52s -> 4119.52s]  you can do pretty well, right?
[4119.52s -> 4122.52s]  So it's not as good as the attempted speech,
[4122.52s -> 4125.52s]  but still much better than, much higher than chance.
[4125.52s -> 4127.52s]  So I think this showing that it's possible
[4127.52s -> 4129.52s]  in the future that we can decode
[4129.52s -> 4131.52s]  this sort of like an inner speech
[4131.52s -> 4134.52s]  to fully restore natural communication
[4134.52s -> 4137.52s]  to people like Hallward and K-12.
[4137.52s -> 4140.52s]  But I think there's a more controversial issue
[4140.52s -> 4142.52s]  regarding this kind of inner speech
[4142.52s -> 4145.52s]  because what if you can decode something
[4145.52s -> 4148.52s]  that's your private thoughts or private memories
[4148.52s -> 4150.52s]  that you don't want to express, right?
[4150.52s -> 4152.52s]  That's a very difficult question here.
[4152.52s -> 4155.52s]  And also, as I just mentioned,
[4155.52s -> 4157.52s]  not everyone has inner speech.
[4157.52s -> 4158.52s]  And also someone maybe like,
[4158.52s -> 4161.52s]  when you think about this kind of like speech, right?
[4161.52s -> 4163.52s]  Speech is just a external,
[4163.52s -> 4166.52s]  one external representation of your internal thoughts.
[4166.52s -> 4168.52s]  So it's like a linear representation
[4168.52s -> 4169.52s]  that you want to put out
[4169.52s -> 4172.52s]  through this kind of like a medium of speech,
[4172.52s -> 4174.52s]  whereas your inner thoughts could be more complex
[4174.52s -> 4176.52s]  and more multi-dimensional.
[4176.52s -> 4178.52s]  So it's very hard to decide
[4178.52s -> 4180.52s]  where you want to put a race
[4180.52s -> 4182.52s]  and where you want to decode all those inner thoughts.
[4182.52s -> 4184.52s]  But I think that's also a very exciting opportunity
[4184.52s -> 4190.52s]  for us to learn more about the speech processing in the brain.
[4190.52s -> 4192.52s]  As I just mentioned,
[4192.52s -> 4194.52s]  if you want to decode this kind of inner speech,
[4194.52s -> 4196.52s]  then you also come into this
[4196.52s -> 4199.52s]  facing a lot of new ethical questions.
[4199.52s -> 4202.52s]  That's really, I think, thought-provoking.
[4202.52s -> 4206.52s]  For example, suppose should we allow BCIs
[4206.52s -> 4209.52s]  to decode our inner memories, right?
[4209.52s -> 4212.52s]  What if we decode something you don't want to say?
[4213.52s -> 4215.52s]  How can we deal with that?
[4215.52s -> 4218.52s]  On the other hand, what if we can actually use these things
[4218.52s -> 4221.52s]  to help people who have lost their memories
[4221.52s -> 4224.52s]  due to like an Alzheimer's disease, right?
[4224.52s -> 4228.52s]  Or we can read out some like subconscious fear
[4228.52s -> 4232.52s]  that can help people to do their psychotherapies.
[4232.52s -> 4235.52s]  How should we decide whether we want to allow
[4235.52s -> 4237.52s]  this kind of inner thought decoding or not,
[4237.52s -> 4240.52s]  or memory decoding or not?
[4240.52s -> 4244.52s]  And also, I think a more deeper question is
[4244.52s -> 4249.52s]  what if one day we could do this kind of cognitive enhancement with BCI,
[4249.52s -> 4252.52s]  such as what if you can move a robotic arm
[4252.52s -> 4254.52s]  much faster than your real arm?
[4254.52s -> 4256.52s]  Is that allowed?
[4256.52s -> 4258.52s]  Or can you actually purchase a memory
[4258.52s -> 4262.52s]  so that you can skip the CS224M class?
[4262.52s -> 4265.52s]  Yeah, I think that's really a hard question to answer,
[4265.52s -> 4268.52s]  but I'd just like to throw this question out
[4268.52s -> 4271.52s]  so that it's not only a BCI problem,
[4271.52s -> 4274.52s]  but we're also facing this problem right now, right?
[4274.52s -> 4277.52s]  There's a lot of ways you can do enhancement yourself.
[4277.52s -> 4280.52s]  So I guess what I'm trying to say here is that
[4280.52s -> 4285.52s]  BCI will raise a lot of new ethical questions.
[4285.52s -> 4289.52s]  So this is taking this quote from this textbook here.
[4289.52s -> 4291.52s]  What it's trying to say is that
[4291.52s -> 4294.52s]  I think this question is
[4294.52s -> 4296.52s]  we're not really looking for an answer here,
[4296.52s -> 4298.52s]  but I think the point here is that
[4298.52s -> 4302.52s]  maybe we just want to keep this in discussion
[4302.52s -> 4306.52s]  with scientists, with engineers, with policymakers,
[4306.52s -> 4310.52s]  and just to make sure that everything is well.
[4310.52s -> 4314.52s]  We can use BCI to help people that really need them
[4314.52s -> 4316.52s]  and also be aware that there could be
[4316.52s -> 4319.52s]  some of our potential issues here.
[4322.52s -> 4324.52s]  Just to give a summary here.
[4325.52s -> 4327.52s]  I hope I can convince you guys that BCI
[4327.52s -> 4330.52s]  has really cool new research directions.
[4330.52s -> 4333.52s]  It's at the intersection of AI, machine learning,
[4333.52s -> 4336.52s]  neuroscience, and neural engineering.
[4336.52s -> 4339.52s]  We'll soon have this kind of systems
[4339.52s -> 4341.52s]  that can really help people
[4341.52s -> 4343.52s]  to be able to communicate again.
[4343.52s -> 4345.52s]  And also there's really cool opportunities
[4345.52s -> 4350.52s]  to understand how the brain processes languages.
[4350.52s -> 4352.52s]  I think the most important thing is
[4352.52s -> 4356.52s]  that we are bringing hope to people like Howard and T12.
[4356.52s -> 4358.52s]  Thank you everyone.
[4358.52s -> 4361.52s]  Special thanks to the people in my lab here.
[4361.52s -> 4363.52s]  Thank you.
