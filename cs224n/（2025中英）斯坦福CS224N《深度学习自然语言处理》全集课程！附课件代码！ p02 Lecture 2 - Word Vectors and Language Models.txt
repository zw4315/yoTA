# Detected language: en (p=1.00)

[0.00s -> 8.76s]  Okay, I should try and get started.
[8.76s -> 15.60s]  Okay, so what we're going to do today is we're going to try and do everything else
[15.60s -> 21.60s]  that you need to know about word vectors and start to learn a teeny bit about neural
[21.60s -> 22.60s]  nets.
[22.60s -> 26.40s]  And then we'll kind of get much further into sort of doing more with the math of
[26.40s -> 27.88s]  neural nets next week.
[27.88s -> 31.00s]  So this is the general plan.
[31.00s -> 37.50s]  So I'm going to sort of finish up from where I was last time with optimization basics.
[37.50s -> 42.08s]  Then look a little bit more about word2vec and word vectors.
[42.08s -> 45.28s]  And then some of the variants of word2vec.
[45.28s -> 49.96s]  And then I'm going to briefly consider alternatives, sort of like what can you get from just
[49.96s -> 52.80s]  counting words in different ways.
[52.80s -> 58.52s]  Then we're going to go on and talk a little bit about the evaluation of word vectors.
[58.52s -> 64.40s]  The topic of word sensors that already came up a couple of times last time when people
[64.40s -> 66.20s]  were asking questions.
[66.20s -> 72.00s]  And then towards the end, start to introduce the idea of classification, doing neural
[72.00s -> 77.98s]  classification, and what neural networks are about, which is something that we'll then
[77.98s -> 82.24s]  expand on more in the second week.
[82.24s -> 86.72s]  Before I get into that, just notes on course organization.
[86.72s -> 94.80s]  So remember the first assignment is already out and it's due before class next Tuesday.
[94.80s -> 102.36s]  So then our Python review session is going to be taught this Friday, 3.30 to 4.20.
[102.36s -> 103.64s]  It's not going to be taught here.
[103.64s -> 107.62s]  It's going to be taught in Gates BO1, the Gates basement.
[107.62s -> 112.20s]  And encourage everyone again to come to office hours and help sessions.
[112.20s -> 114.00s]  They've already started.
[114.00s -> 116.24s]  They're listed on the website.
[116.24s -> 123.28s]  We're having these sort of office hour help sessions in classrooms with multiple TAs.
[123.28s -> 127.20s]  So just turn up if you're on campus and you can be helped.
[127.20s -> 131.52s]  And if you are on campus, we'd like you to just turn up, though we do also have a
[131.52s -> 138.24s]  Zoom option for Stanford Online students.
[138.24s -> 142.56s]  Actually I have office hours, which I have not yet opened, but I will open some time
[142.56s -> 143.56s]  tonight.
[143.56s -> 146.68s]  They're going to be on Monday afternoons.
[146.68s -> 150.88s]  Now obviously given the number of people, not everyone can make it into my office
[150.88s -> 151.88s]  hours.
[151.88s -> 153.62s]  And I'm going to do these by appointment.
[153.62s -> 157.56s]  So they're by 15 minute appointments on Calendly.
[157.56s -> 161.80s]  But you know, I'm very happy to talk to some people.
[161.80s -> 167.36s]  And you know, I put this little note at the end saying don't hog the slots.
[167.36s -> 171.48s]  Some people think it would be a really good idea if they really work out how to sign
[171.48s -> 175.56s]  up every week for an office hour session with me.
[175.56s -> 178.30s]  And that's sort of a little bit antisocial.
[178.30s -> 181.92s]  So think about that.
[181.92s -> 182.92s]  Okay.
[182.92s -> 189.80s]  So at the end of last time, I did a sort of bad job of trying to write on slides
[189.80s -> 194.60s]  of working out the derivatives of Word2Vec.
[194.60s -> 199.32s]  And hopefully you can read it much more clearly in the version that appears on the website
[199.32s -> 202.82s]  where I do it at home more carefully.
[202.82s -> 210.12s]  So that was saying that we had this Mox function and our job was to work out its derivatives,
[210.12s -> 215.58s]  which would tell us which direction to go to walk downhill.
[215.58s -> 218.20s]  And so I didn't really quite finish the loop here.
[218.20s -> 223.22s]  So you know, we have some cost function that we want to minimize.
[223.22s -> 230.48s]  And then we work out the gradients of that function to work out which direction is downhill.
[230.48s -> 238.12s]  And then the simplest algorithm is then that we work out the direction downhill.
[238.12s -> 242.38s]  We walk a little bit in that direction and then we repeat.
[242.38s -> 244.92s]  We work out the gradient at this point.
[244.92s -> 250.34s]  We walk downhill a little bit and we keep on going and we'll get to the minimum.
[250.34s -> 254.70s]  And with a sort of a one-dimensional function like this, it's sort of very simple.
[254.70s -> 256.56s]  We're just walking downhill.
[256.56s -> 261.18s]  But when we have a function of many, many dimensions, when we calculate the gradient
[261.18s -> 266.44s]  at different points, we might be starting to walk in different directions.
[266.44s -> 270.22s]  And so that's why we need to do calculus and have gradients.
[270.22s -> 275.78s]  And so this gives us the basic algorithm of gradient descent.
[275.80s -> 280.14s]  And so under the gradient descent algorithm,
[280.14s -> 286.74s]  what we're doing is that we've got a loss function j.
[286.74s -> 289.14s]  We're working out its gradient.
[289.14s -> 294.14s]  And then we're taking a little bit of, a little multiplier of the gradient so
[294.14s -> 297.26s]  that alpha is our step size or learning rate.
[297.26s -> 299.90s]  And that's normally alpha's a very small number,
[299.90s -> 303.06s]  something like 10 to the minus 3 or 10 to the minus 4, or
[303.06s -> 304.94s]  maybe even 10 to the minus 5.
[304.98s -> 308.34s]  So we're taking a really little bit of the gradient and
[308.34s -> 314.46s]  then we're subtracting it from our parameters to get new parameters.
[314.46s -> 317.78s]  And as we do that, we will walk downhill.
[317.78s -> 321.58s]  And the reason why we want to have a small learning rate is we don't want to
[321.58s -> 322.62s]  walk too fast.
[322.62s -> 326.00s]  So if from here we worked out the gradient and said,
[326.00s -> 329.46s]  it's in this direction and we just kept on walking.
[329.46s -> 332.30s]  We sort of might end up way over here.
[332.30s -> 334.06s]  Or if we had a really big step size,
[334.06s -> 337.54s]  we might even end up at a worse point than we started with.
[337.54s -> 341.06s]  So we want to take little steps to walk downhill.
[341.06s -> 344.86s]  And so that's the very basic gradient descent algorithm.
[344.86s -> 350.18s]  Now, the very basic gradient descent algorithm we never use.
[350.18s -> 353.54s]  What we actually use is the next thing up,
[353.54s -> 356.74s]  which is called stochastic gradient descent.
[356.74s -> 361.26s]  So the problem is, for the basic gradient descent algorithm,
[361.26s -> 366.42s]  we've worked out for an entire set of data
[367.86s -> 371.10s]  what the objective function is and
[371.10s -> 375.86s]  what the slope at the point of evaluation is.
[375.86s -> 383.38s]  And in general, we've got a lot of data in which we're computing models.
[383.38s -> 389.18s]  So simply trying to calculate our objective function over all of our
[389.18s -> 391.42s]  data for our model, the training data for
[391.42s -> 395.58s]  the model would take us a very, very long time.
[395.58s -> 399.38s]  And so that's very expensive to compute.
[399.38s -> 403.10s]  And so we'd wait a very long time before we make even a single step
[403.10s -> 405.14s]  of gradient update.
[405.14s -> 408.50s]  So for neural nets, what you're always doing is using this variant
[408.50s -> 410.98s]  that's called stochastic gradient descent.
[410.98s -> 413.30s]  And so for stochastic gradient descent,
[413.30s -> 417.62s]  what that means is we pick a very small subset of our data.
[417.62s -> 421.46s]  Like maybe we pick 16 or 32 data items.
[421.46s -> 424.82s]  And we pretend that's all of our data.
[424.82s -> 428.90s]  And we evaluate the function j based on that small subset and
[428.90s -> 432.10s]  work out the gradient based on that small subset.
[432.10s -> 435.14s]  So it's a noisy, inaccurate estimate of the gradient.
[435.14s -> 441.06s]  And we use that to be the direction in which we walk.
[441.06s -> 445.22s]  So that's normally referred to also as having mini-batches or
[445.22s -> 446.86s]  mini-batch gradient descent.
[448.58s -> 453.02s]  And in theory, working out the gradient
[453.02s -> 458.16s]  based on this small subset is an approximation.
[458.16s -> 461.18s]  But one of the interesting things in the way things
[461.18s -> 465.38s]  have emerged in neural network land is it turns out that neural networks
[465.38s -> 470.10s]  actually often work better when you throw some noise into the system.
[470.10s -> 473.54s]  That having this noise in the system gives you jiggle and
[473.54s -> 475.14s]  moves things around.
[475.14s -> 481.06s]  And so actually stochastic gradient descent not only is way, way, way faster,
[481.06s -> 485.38s]  but actually works better as a system for optimization of neural networks.
[487.82s -> 492.86s]  Okay, so if you remember from last time for word2vec,
[492.86s -> 499.42s]  the idea was we started by just saying each word has a random vector
[499.42s -> 504.66s]  representing it, so we will literally sort of just get random small numbers and
[504.66s -> 507.54s]  fill up the vectors with those random small numbers.
[507.54s -> 511.86s]  And there's an important point there, which is you do have to initialize
[511.86s -> 514.42s]  your vectors with random small numbers.
[514.42s -> 518.94s]  If you just leave all the vectors as zero, then nothing works.
[518.94s -> 524.10s]  And that's because if everything starts off the same,
[524.10s -> 528.54s]  you get these sort of false symmetries, which means that you can't learn.
[528.54s -> 533.88s]  So you always do want to be initializing your vectors with random numbers.
[533.88s -> 537.28s]  And then we're gonna go through each position in the corpus.
[537.28s -> 539.72s]  Using our estimates, we're gonna try and
[539.72s -> 545.08s]  predict the probability of words in the context as we talked about last time.
[545.08s -> 550.24s]  Then we, so that gives us an objective function from which we can then look at
[550.24s -> 555.20s]  our errors, look at our gradient, and update the vectors so
[555.20s -> 558.60s]  that they learn to predict surrounding words better.
[558.60s -> 564.00s]  And so the incredible thing is that we can do no more than that,
[564.00s -> 569.20s]  and we end up learning word vectors, which actually capture quite a lot
[569.20s -> 574.52s]  of the semantics, the meaning and relationships between different words.
[574.52s -> 580.84s]  So when this was first discovered for
[580.84s -> 584.76s]  these algorithms, I mean it really feels like magic that you can just sort of
[585.12s -> 590.76s]  do this math, simple math over a lot of text, and
[590.76s -> 593.60s]  actually learn about the meanings of words.
[593.60s -> 596.60s]  That it's sort of just sort of surprising that something so
[596.60s -> 597.80s]  simple could work.
[597.80s -> 601.60s]  But as time has gone on, this same recipe has then been for
[601.60s -> 607.96s]  all kinds of learning about the behavior of language from neural networks.
[609.40s -> 613.76s]  So let's just go through a sense of how that is.
[613.76s -> 618.68s]  But before we do that, so let me just mention.
[618.68s -> 621.40s]  So for our word2vec algorithm,
[621.40s -> 625.40s]  the only parameters of the model are these word vectors.
[625.40s -> 629.16s]  They're the outside word vectors and the center word vectors,
[629.16s -> 633.56s]  which we actually treat as disjoint as I mentioned last time.
[633.56s -> 638.32s]  And when we do the computations, we're considering the dot product between
[638.32s -> 644.88s]  the various possible outside words with our center word.
[644.88s -> 648.36s]  And we're using those to get a probability distribution
[648.36s -> 653.32s]  over how likely the model thinks that different outside words were.
[653.32s -> 657.84s]  And then we're comparing that to the actual outside word in the context.
[657.84s -> 660.32s]  And that gives us our source of error.
[660.32s -> 665.28s]  So as such, this is what's referred to in NLP as a bag of words model.
[665.28s -> 668.88s]  That it doesn't actually know about the structure of sentences and
[668.88s -> 671.68s]  or even what's to the left and what's to the right.
[671.68s -> 676.52s]  It's predicting exactly the same probabilities at each position to the left
[676.52s -> 677.44s]  or right.
[677.44s -> 681.56s]  But it's wanting to know about what kind of words appear in the context
[681.56s -> 682.64s]  of the center word.
[684.68s -> 692.84s]  So I just wanted to stop this for a minute and let's see, not that one.
[695.28s -> 704.84s]  So let's give you some kind of a sense that this really does work.
[704.84s -> 713.16s]  So this is a little Jupyter notebook that I've got for this, okay.
[713.16s -> 717.32s]  And so this is using, and here I'm using a package gensim,
[717.32s -> 722.08s]  which we don't continue to use after that really.
[722.28s -> 727.44s]  But it's sort of one package to let you load and play with word vectors.
[727.44s -> 733.52s]  And the word vectors I'm gonna use here are glove word vectors.
[733.52s -> 737.12s]  And actually I'm gonna talk, glove was a model we built at Stanford.
[737.12s -> 740.72s]  And I'm gonna actually talk about it a little bit later.
[740.72s -> 745.84s]  So strictly speaking, these aren't exactly word vectors,
[745.84s -> 748.44s]  but they behave in exactly the same way.
[749.40s -> 753.40s]  And so, okay, so now it's loaded up my word vectors,
[753.40s -> 756.04s]  cuz the word vectors are a big data file.
[756.04s -> 760.92s]  And so, as we've discussed, for a word, right,
[760.92s -> 764.72s]  that the representation of any word, here's bread,
[764.72s -> 768.36s]  is just a vector of real numbers, right?
[768.36s -> 773.56s]  So I'm using a hundred dimensional word vectors to keep things
[773.56s -> 776.12s]  quicker for my class demo.
[776.12s -> 778.08s]  So this is the word bread.
[778.08s -> 782.88s]  And then I can say, well, what's the representation for croissant?
[785.52s -> 788.48s]  And this is croissant.
[788.48s -> 791.48s]  And we can sort of get a visual sense of,
[791.48s -> 793.52s]  they're at least a little bit similar, right?
[793.52s -> 796.48s]  So the first components are both negative.
[796.48s -> 798.72s]  The second components are both positive.
[798.72s -> 802.08s]  The third components are both negative and large.
[802.08s -> 805.08s]  The fourth components are both positive, right?
[805.44s -> 810.52s]  They seem like they're kind of similar vectors, so that seems kind of hopeful,
[810.52s -> 812.92s]  cuz that means that it knows that bread and
[812.92s -> 816.04s]  croissant are a bit similar to each other.
[818.04s -> 822.52s]  This package has a nice simple function where, rather than doing that by hand,
[822.52s -> 827.48s]  you can just ask it about all the word vectors and say which ones are most
[827.48s -> 834.96s]  similar, so I can ask it what words in its vocabulary are most similar to USA.
[835.84s -> 838.96s]  And in this model, everything's been lower cased, I should mention.
[838.96s -> 842.32s]  And so if I do that, it has Canada, America,
[842.32s -> 845.72s]  U.S.A, then United States, Australia.
[845.72s -> 850.20s]  Well, they seem a fairly reasonable list of most similar words, though you might
[850.20s -> 855.32s]  think it's a little strange that Canada wins out over the USA with dots over it.
[857.96s -> 861.04s]  Similarly, I can ask it what's most similar to banana, and
[861.08s -> 865.52s]  I get coconut, mango, bananas, potato, pineapple, fruit, etc.
[865.52s -> 870.00s]  Again, pretty sensible, a little bit of a bias to more tropical fruits.
[870.00s -> 874.60s]  Or I can go to croissant and ask what's most similar to croissant.
[874.60s -> 877.12s]  The most similar things to croissant isn't bread, but
[877.12s -> 880.24s]  it's things like brioche, baguette, focaccia,
[880.24s -> 883.28s]  which sort of basically makes sense, though here's pudding here.
[885.44s -> 890.32s]  And I got, wait, I'd already done, sorry, yeah, I remember what this is.
[890.52s -> 895.60s]  So with this most similar, you've got a positive word vector, and
[895.60s -> 900.32s]  you're saying, what other words are most similar in position to that?
[900.32s -> 904.04s]  There's something else you can do, which you can say is, this is,
[904.04s -> 907.12s]  let me take the negative of that word vector and
[907.12s -> 910.48s]  say what's most similar to the negative of it.
[910.48s -> 914.88s]  And you could possibly think that might be useful to find antonyms or
[914.88s -> 916.36s]  something like that.
[916.36s -> 918.42s]  I mean, the truth is it isn't.
[918.42s -> 922.26s]  If you ask for the things that are most similar to the negative of
[922.26s -> 926.98s]  the banana vector, and in most other vectors it's the same,
[926.98s -> 930.98s]  you get out these weirdo things that you're not really sure if they're words
[930.98s -> 935.14s]  at all, or maybe they are in some other language, or some of them are names,
[935.14s -> 939.62s]  right, like Shunichi's a Japanese name, but not very useful stuff.
[939.62s -> 944.70s]  They don't really feel like a negative of banana.
[944.70s -> 950.98s]  But it turns out that from there, we get this powerful ability
[950.98s -> 955.90s]  that was observed for word2vec, which is that we could isolate
[955.90s -> 962.90s]  semantic components and then put them together in interesting ways.
[962.90s -> 967.74s]  So looking at this picture, what we could do is start with a positive
[967.74s -> 971.90s]  vector for king, from the origin up to king.
[971.90s -> 978.26s]  Then we could use the negation to say, subtract out the vector for man.
[978.26s -> 983.82s]  And then we could have another positive vector of add on the vector for woman.
[983.82s -> 988.46s]  And then we can ask the model is, if you're over here in the space,
[988.46s -> 992.54s]  what is the nearest word to you over there?
[992.54s -> 996.70s]  And so that's what this next thing does, right?
[996.70s -> 1001.30s]  It sort of says, positive vector for king, negative for man,
[1001.30s -> 1005.10s]  also positive for queen, where does that get you to?
[1005.10s -> 1008.82s]  And that gets you to queen, yay.
[1008.82s -> 1013.74s]  And so this was the most celebrated property that was discovered
[1013.74s -> 1018.62s]  with these word vectors, that they weren't only good for meaning similarity,
[1018.62s -> 1024.98s]  but that they were good for doing these kind of meaning components.
[1024.98s -> 1029.58s]  And these got referred to as analogies, because you can think of them as
[1029.58s -> 1032.02s]  a is to b is, c is to what?
[1032.02s -> 1035.58s]  So it's sort of like, woman is to king, no, sorry.
[1036.86s -> 1043.10s]  Man is to king, or king is to man is, I'm saying this the wrong way around.
[1043.10s -> 1047.78s]  Man is to king as woman is to what in the analogies.
[1047.78s -> 1052.10s]  And so here I've defined a little function that is now saying
[1052.10s -> 1056.78s]  this little function just automates that and will compute analogies.
[1057.78s -> 1062.10s]  And so now I can ask it in just this analogy format.
[1062.10s -> 1064.82s]  Man is to king as woman is to queen.
[1064.82s -> 1069.62s]  And that one was sort of the canonical example.
[1069.62s -> 1073.98s]  But you can actually sort of have fun with this.
[1073.98s -> 1079.54s]  And I mean, this is pretty old fashioned stuff.
[1079.54s -> 1084.46s]  I feel like I'm maybe like now at this point an old guy
[1084.50s -> 1088.38s]  talking about how much fun we used to have sitting around the radio,
[1088.38s -> 1093.98s]  listening to radio plays, because basically no one uses this stuff anymore.
[1093.98s -> 1097.90s]  And there are much, much better and fancier things like chat GPT.
[1097.90s -> 1105.26s]  But back in the day when I was younger, it was really stunning already
[1105.26s -> 1110.62s]  just how this very simple model built on very simple data
[1110.62s -> 1117.42s]  could just have quite good semantic understanding and do quite good analogies.
[1117.42s -> 1122.18s]  So you can actually play with this quite a bit and have a bit of fun.
[1122.18s -> 1126.94s]  So you can do something like analogy, Australia,
[1126.94s -> 1131.70s]  comma, beer, France.
[1131.70s -> 1133.98s]  Okay, what do people think the answer will be?
[1133.98s -> 1135.82s]  Close.
[1135.82s -> 1136.58s]  Close?
[1136.58s -> 1138.10s]  The answer gives us champagne.
[1138.10s -> 1140.10s]  But that seems a pretty good answer.
[1140.90s -> 1143.46s]  I could then put in Russia.
[1143.46s -> 1144.90s]  What do people think?
[1144.90s -> 1146.10s]  Vodka.
[1146.10s -> 1149.10s]  Yeah, it'll see, it can get that vodka.
[1149.10s -> 1153.90s]  This is actually works kind of interestingly.
[1153.90s -> 1155.90s]  I could do a different one.
[1155.90s -> 1158.38s]  I can do the test something different.
[1158.38s -> 1165.46s]  I can do something like pencil is to sketching
[1165.46s -> 1173.30s]  as camera is to photographing.
[1173.30s -> 1175.66s]  Yeah, that works quite well.
[1175.66s -> 1180.54s]  So we built this model in 2014.
[1180.54s -> 1184.18s]  So it's a little bit out of date in politics.
[1184.18s -> 1190.02s]  So we can't do the last decade of politics, which is maybe unfortunate.
[1190.02s -> 1193.54s]  But we could try out older politics questions.
[1193.66s -> 1200.66s]  So we could try Obama is to Clinton.
[1203.34s -> 1210.26s]  As Reagan is to, if you remember your US world history class.
[1210.26s -> 1212.70s]  Any guesses what it's gonna say?
[1214.38s -> 1216.22s]  There's a Bush one.
[1216.22s -> 1218.38s]  Any other ideas?
[1218.38s -> 1221.26s]  Some people have different opinions of Bill Clinton.
[1221.26s -> 1226.78s]  Any, what it answers is Nixon,
[1226.78s -> 1228.82s]  which I think is actually kind of fair.
[1233.22s -> 1239.22s]  You could also get it to do some just sort of language syntactic facts.
[1239.22s -> 1247.14s]  So you can do something like tallest to tallest as long,
[1247.14s -> 1252.90s]  oops, as long is to, this one's easy.
[1255.46s -> 1260.10s]  Yeah, so with this simple method of learning,
[1260.10s -> 1262.98s]  with this simple bag of words model,
[1262.98s -> 1268.26s]  it's enough to learn a lot about the semantics of words.
[1268.26s -> 1272.82s]  And stuff that's beyond conventional semantics, right?
[1272.86s -> 1277.06s]  Like our examples with Australia's to beer, Russia's to vodka.
[1277.06s -> 1281.94s]  I mean that's sort of cultural world knowledge which goes a little bit beyond
[1281.94s -> 1285.18s]  what people normally think of as sort of word meaning semantics, but
[1285.18s -> 1286.14s]  it's also in there.
[1286.14s -> 1287.10s]  Yes?
[1287.10s -> 1291.50s]  If you perhaps track the distance from let's say like man and king,
[1291.50s -> 1294.14s]  does that also capture a concept of relationship between two words?
[1294.14s -> 1297.66s]  Like would that give you back like ruler or something like that?
[1297.66s -> 1298.98s]  Where taking the distance,
[1299.02s -> 1301.14s]  Like a difference between two vectors does capture some concept.
[1302.50s -> 1306.34s]  Like the distance between man.
[1306.34s -> 1311.66s]  So man compared to king should be a ruler concept.
[1311.66s -> 1312.82s]  But isn't that what I'm using?
[1312.82s -> 1319.50s]  Cuz then I'm taking that, I'm taking the distance between man and
[1319.50s -> 1324.90s]  king is what I'm adding on to woman to get the queen, right?
[1326.66s -> 1327.22s]  Right, yeah.
[1327.22s -> 1330.90s]  So I can, depending on how, if you think of these words,
[1330.90s -> 1335.66s]  depending on which thing you think of as the analogy, you can think of it.
[1335.66s -> 1340.62s]  You've both got a vector,
[1340.62s -> 1345.46s]  a difference vector between words that gives you a gender analogy and
[1345.46s -> 1347.78s]  one that gives you a ruler analogy.
[1347.78s -> 1348.78s]  Yeah, absolutely.
[1350.46s -> 1351.46s]  Any other questions?
[1353.14s -> 1354.14s]  Yeah?
[1354.58s -> 1360.94s]  In the watch algorithm, we get two vectors for each one, a U and a V.
[1360.94s -> 1363.26s]  But here you only have one vector.
[1363.26s -> 1367.06s]  So how do you go from two to one?
[1367.06s -> 1368.18s]  Good question.
[1368.18s -> 1372.30s]  I mean, the commonest way in practice was you just average the two of them.
[1372.30s -> 1378.98s]  And really you sort of find out that they end up very close.
[1378.98s -> 1382.10s]  You know, because if you think of it,
[1382.10s -> 1384.90s]  since you're going along every position of the text,
[1384.90s -> 1389.90s]  you're both gonna be the case where if the text is sort of the octopus
[1389.90s -> 1394.42s]  has legs, you're gonna have octopus in the center with legs in the context.
[1394.42s -> 1395.90s]  Then a couple of time steps later,
[1395.90s -> 1399.26s]  it's gonna be legs in the center with octopus in the context.
[1399.26s -> 1403.58s]  So although they vary a bit for all the regions of the neural nets vary,
[1403.58s -> 1407.30s]  basically they end up very similar and people normally just average them.
[1407.30s -> 1408.02s]  Yeah?
[1408.02s -> 1409.86s]  Can you mess with the process?
[1409.90s -> 1415.54s]  So use this, the answer of one to then be placed into the analogy
[1415.54s -> 1418.98s]  function of the another and see how far away you can go before it starts to break down.
[1421.54s -> 1423.66s]  I think you can.
[1423.66s -> 1426.62s]  So you're, wait, you're wanting to- How far away,
[1426.62s -> 1430.46s]  how distant are the relations between two words can you do before it starts
[1430.46s -> 1436.82s]  providing incorrect relationship between words?
[1436.82s -> 1439.82s]  Are you wanting to sort of make two steps from somewhere?
[1440.54s -> 1443.86s]  Yeah, like two steps or three steps or like how far,
[1443.86s -> 1450.54s]  how many steps can you go away from king before it starts failing?
[1450.54s -> 1451.82s]  So it doesn't always work.
[1451.82s -> 1453.94s]  I mean, there are certainly examples that will fail.
[1455.38s -> 1459.74s]  I'm sort of shy to try that now cuz I don't have a predefined function that
[1459.74s -> 1462.14s]  did it, and that might take me too long.
[1462.14s -> 1465.94s]  But you could play with it at home and see how it works for you.
[1465.98s -> 1466.98s]  Okay.
[1466.98s -> 1469.98s]  I'm curious, as a quantification,
[1469.98s -> 1475.06s]  why is it that we use two separate sets of vectors for word2vec?
[1475.06s -> 1478.30s]  Is it just to get more parameters, or is there-
[1478.30s -> 1479.30s]  I'll get back to that.
[1480.42s -> 1481.86s]  Maybe I should go on at this point.
[1483.50s -> 1487.30s]  Let me move on and kind of just get through some more details
[1487.30s -> 1489.06s]  of the word2vec algorithm.
[1490.30s -> 1494.42s]  So just a technical point on this class so
[1494.42s -> 1498.14s]  you don't make any big mistakes and waste your weekend.
[1498.14s -> 1503.06s]  I mean, for most instances of 224n, we've actually had people
[1503.06s -> 1506.86s]  implement from scratch word2vec as assignment two.
[1506.86s -> 1512.82s]  But for this quarter, doing it in spring quarter, as you probably know,
[1512.82s -> 1517.30s]  spring quarter is actually a little shorter than the other two quarters.
[1517.30s -> 1521.10s]  We decided to skip having people implement word2vec.
[1521.10s -> 1523.90s]  So don't look at the old assignment two that says implement
[1523.90s -> 1527.06s]  word2vec or else you'll be misspending your time.
[1527.06s -> 1530.66s]  Wait for the new assignment two to come out.
[1530.66s -> 1535.66s]  But despite that, let me just sort of say a little bit more
[1535.66s -> 1537.38s]  about some of the details.
[1537.38s -> 1537.78s]  Right, yeah.
[1537.78s -> 1540.38s]  So why two vectors?
[1540.38s -> 1545.62s]  So the two vectors is it just makes the math a little bit easy.
[1545.62s -> 1551.42s]  So if you think about the math, right, if you have the same
[1551.46s -> 1555.34s]  vectors for the center word and for the outside words.
[1555.34s -> 1559.62s]  Well, for whatever the center word is, let's say it's octopus,
[1559.62s -> 1564.54s]  that when you're going through the trying out every possible
[1564.54s -> 1569.94s]  context word for the normalization, at some point you'll hit octopus again.
[1569.94s -> 1573.38s]  And so at that point, you'll have a quadratic term, right?
[1573.38s -> 1576.26s]  You'll have the x squared of the octopus vector.
[1576.26s -> 1580.54s]  And that kind of messes up, I mean, you're clever people.
[1580.54s -> 1584.86s]  You could work out the math of it, but it makes the math more of a mess, right?
[1584.86s -> 1589.10s]  Cuz every other term it's something different, and it's just like ax.
[1589.10s -> 1592.22s]  And then at one position you've gone x squared.
[1592.22s -> 1594.70s]  So it just makes the math messier.
[1594.70s -> 1599.82s]  And so they kept it really simple by just having them be disjoint vectors.
[1599.82s -> 1602.06s]  But it doesn't make it better.
[1602.06s -> 1607.42s]  I mean, it actually turns out it works a fraction better if you do it right.
[1607.58s -> 1612.78s]  But in practice, people have usually just estimated them separately and
[1612.78s -> 1614.86s]  then average them at the end.
[1614.86s -> 1618.74s]  There, if you actually look at the paper, here's Mikolov et al,
[1618.74s -> 1622.46s]  you can find it, 2013 paper.
[1622.46s -> 1626.98s]  There's actually sort of a family of methods that they describe.
[1626.98s -> 1632.22s]  So they describe two methods, one of which was that you have
[1632.22s -> 1636.02s]  an inside word that's predicting the words around it.
[1636.06s -> 1640.14s]  And then the other one tried to predict the center word from all the words in
[1640.14s -> 1644.34s]  the context, which was called continuous bag of words in their paper.
[1644.34s -> 1648.58s]  The one that I've described as skip gram, which is simpler and
[1648.58s -> 1650.66s]  works just great.
[1650.66s -> 1655.38s]  But then the other part of it is for
[1655.38s -> 1659.90s]  working out what loss function to be used for training.
[1659.94s -> 1666.14s]  And what I've presented so far is naive softmax,
[1666.14s -> 1670.58s]  where we just consider every possible choice of a context word and
[1670.58s -> 1672.02s]  just run all the math.
[1673.02s -> 1675.10s]  You know, that's totally doable.
[1675.10s -> 1677.26s]  And with our modern super fast computers,
[1677.26s -> 1679.34s]  it's not even that unreasonable to do.
[1679.34s -> 1681.70s]  We do things like this all the time.
[1681.70s -> 1685.02s]  But at least at the time that they wrote their paper,
[1685.02s -> 1687.06s]  this seemed kind of expensive.
[1687.06s -> 1691.38s]  And they considered other alternatives like a hierarchical softmax,
[1691.38s -> 1693.66s]  which I'm not gonna explain right now.
[1693.66s -> 1696.82s]  But I do just wanna explain negative sampling.
[1698.26s -> 1702.50s]  Okay, so this is just to sort of see a bit of a different way of doing things.
[1702.50s -> 1705.42s]  So for what we did last time,
[1705.42s -> 1709.50s]  we had this sort of straightforward softmax equation.
[1709.50s -> 1715.46s]  And so in the denominator, you're summing over every word in the vocabulary.
[1715.46s -> 1718.98s]  And so if you might have 400,000 words in your vocabulary,
[1718.98s -> 1723.94s]  a lot of words in human languages, you know, that's kind of a big sum.
[1723.94s -> 1726.66s]  Especially when for each element of the sum,
[1726.66s -> 1730.58s]  you're taking a dot product between 100 dimensional or
[1730.58s -> 1733.82s]  300 dimensional vectors and then exponentiating it, right?
[1733.82s -> 1737.42s]  A lot of math going on somewhere in there.
[1737.42s -> 1741.70s]  So maybe we could short circuit that.
[1741.70s -> 1745.86s]  And so the idea of the negative sampling was to say, well,
[1745.86s -> 1750.30s]  rather than evaluating it for every single possible word,
[1750.30s -> 1756.34s]  maybe we could just sort of train some simpler logistic regressions
[1756.34s -> 1762.06s]  where they're gonna say, you should like the true word that's in the context.
[1762.06s -> 1767.34s]  And if we randomly pick a few other words, you shouldn't like them very much.
[1767.34s -> 1769.90s]  And that's skip gram negative sampling.
[1769.94s -> 1773.82s]  So that's what this looks like as an equation.
[1773.82s -> 1779.30s]  So we've got our center word and our actual context word.
[1779.30s -> 1786.58s]  And we're saying, well, let's work out the term for the actual center word.
[1786.58s -> 1792.22s]  We'd like this to be high probability.
[1792.22s -> 1796.62s]  So since we're minimizing, we're gonna negate that and have it go down.
[1796.62s -> 1799.10s]  And then we're gonna sample some other words.
[1799.10s -> 1801.26s]  And we'd like this to be the opposite.
[1801.26s -> 1805.78s]  But the other thing that we've changed here is now we're not using
[1805.78s -> 1807.74s]  the softmax anymore.
[1807.74s -> 1810.90s]  We're using this sigma, which stands for
[1810.90s -> 1815.06s]  the logistic function, which is often called the sigmoid.
[1815.06s -> 1817.22s]  Sigmoid just means S-shaped.
[1817.22s -> 1821.38s]  But you can actually have an infinity of S-shaped functions.
[1821.38s -> 1824.74s]  And the one that we actually use is the logistic function.
[1824.74s -> 1828.38s]  So the logistic function has this form and
[1828.38s -> 1835.94s]  maps from any real number to a probability between 0 and 1.
[1835.94s -> 1840.54s]  So what we're wanting to say at that point is for
[1840.54s -> 1845.98s]  the real outside word, we're hoping that this dot product is large.
[1845.98s -> 1848.46s]  So its probability is near 1.
[1848.46s -> 1853.10s]  And so that will then sort of help with the minimization.
[1853.10s -> 1859.10s]  And for the other words, we'd like their probability to be small.
[1859.10s -> 1862.78s]  So we'd like them to appear sort of over here.
[1864.14s -> 1866.18s]  And that's what this is calculating.
[1866.18s -> 1870.62s]  But as written, it's sort of sticking the minus sign on the inside there,
[1870.62s -> 1873.74s]  which works because this is symmetric, right?
[1873.74s -> 1878.46s]  So you're wanting to be over here, which means that if you negate it,
[1878.46s -> 1880.86s]  you'll be on this side, which will be large.
[1884.10s -> 1888.90s]  Okay, and so then the final bit of this, which is the asterisk,
[1888.90s -> 1893.70s]  is so we're going to pick a few words, it might only be 5 or
[1893.70s -> 1896.86s]  10 that are our negative samples.
[1896.86s -> 1903.42s]  But for picking those words, what works well is not just to sort of
[1903.42s -> 1909.34s]  pick sort of randomly, uniformly from all the 400,000 words in our vocab.
[1909.38s -> 1913.66s]  What you basically want to do is sort of be paying attention to how common
[1913.66s -> 1917.82s]  the words are, so something like that is a really common word.
[1917.82s -> 1921.10s]  And so we refer to that as the unigram distribution.
[1921.10s -> 1924.70s]  That means you're sort of just taking individual words independently,
[1924.70s -> 1926.14s]  how common they are.
[1926.14s -> 1929.98s]  So about 10% of the time, you'd be choosing that.
[1929.98s -> 1934.30s]  But so that's sort of roughly what you want to do for sampling.
[1934.30s -> 1936.86s]  But people have found that you can actually do,
[1937.06s -> 1941.18s]  But people have found that you can actually do even a bit better than that.
[1941.18s -> 1946.26s]  So the standard thing that they presented for word2vec is you're taking
[1946.26s -> 1952.54s]  the unigram probability of the word and raising it to the power three-quarters.
[1952.54s -> 1954.18s]  What does that end up doing?
[1956.74s -> 1958.74s]  Question for the audience.
[1958.74s -> 1962.90s]  If I take probabilities and raise them to the three-quarters.
[1963.90s -> 1969.02s]  Some less frequent words just become sampling more.
[1969.02s -> 1970.18s]  Correct.
[1970.18s -> 1970.42s]  Yeah.
[1970.42s -> 1976.54s]  So raising it to the three-quarters means that you're sort of somewhat
[1976.54s -> 1981.34s]  upping the probability of the less frequent words.
[1981.34s -> 1987.06s]  So you're sort of in between, you know, between having every word uniform
[1987.06s -> 1991.10s]  and exactly using their relative frequencies in the text.
[1991.10s -> 1994.38s]  You're sort of moving a little bit in the direction of uniform.
[1994.38s -> 1998.78s]  And so you get better results by going somewhat in the distance
[1998.78s -> 2000.90s]  of sampling more uniformly.
[2000.90s -> 2003.58s]  But you don't want to go all the way there, which
[2003.58s -> 2008.30s]  should correspond to, I guess, putting a 0 in there rather than
[2008.30s -> 2011.18s]  the three-quarters.
[2011.18s -> 2013.38s]  OK.
[2013.38s -> 2014.94s]  Yeah.
[2014.94s -> 2016.58s]  OK.
[2016.58s -> 2020.54s]  Let's see, I had an aside here, but time rushes along.
[2020.54s -> 2022.14s]  So let's not bother with this side.
[2022.14s -> 2024.18s]  It's not that important.
[2024.18s -> 2024.90s]  OK.
[2024.90s -> 2028.66s]  So that's the word-to-vec algorithm that we've
[2028.66s -> 2034.66s]  seen all of in its different forms.
[2034.66s -> 2039.54s]  A reasonable wonder that you could have at this point
[2039.54s -> 2045.42s]  is, you know, this seems a kind of a weird way of doing
[2045.42s -> 2047.06s]  what we're wanting to do, right?
[2047.06s -> 2052.62s]  The idea is, look, we have this text, we have words,
[2052.62s -> 2055.86s]  and we have words in the context of words.
[2055.86s -> 2058.30s]  It sort of seems like an obvious thing to do
[2058.30s -> 2061.74s]  would be to say, well, let's just count some statistics.
[2061.74s -> 2063.78s]  We have words, and there are other words
[2063.78s -> 2065.14s]  that occur in their context.
[2065.14s -> 2069.06s]  So let's just see how often the word swim occurs
[2069.06s -> 2072.14s]  next to octopus and how often the word fish
[2072.14s -> 2073.86s]  occurs next to octopus.
[2073.90s -> 2078.30s]  Let's get some counts and see how often words occur
[2078.30s -> 2080.18s]  in the context of other words.
[2080.18s -> 2083.94s]  And maybe we could use that to calculate
[2083.94s -> 2086.98s]  some form of word vectors.
[2086.98s -> 2090.30s]  And so that's something that people have already also
[2090.30s -> 2091.34s]  considered.
[2091.34s -> 2095.22s]  So if we use the same kind of idea of a context window,
[2095.22s -> 2098.74s]  we could just make a matrix of how often words occur
[2098.74s -> 2100.66s]  in the context of other words.
[2100.66s -> 2103.02s]  And so, you know, here's a baby example.
[2103.02s -> 2105.46s]  My corpus is, I like deep learning.
[2105.46s -> 2106.62s]  I like NLP.
[2106.62s -> 2108.90s]  I enjoy flying.
[2108.90s -> 2110.98s]  And my context window I'm using
[2110.98s -> 2113.58s]  is just one word to the left and the right.
[2113.58s -> 2119.26s]  And then I can make this kind of co-occurrence count matrix
[2119.26s -> 2122.10s]  where I'm putting in the counts of different words
[2122.10s -> 2123.58s]  in every context.
[2123.58s -> 2127.18s]  And, you know, because my corpus is so small,
[2127.18s -> 2130.42s]  everything in the matrix is a 0 or 1,
[2130.42s -> 2132.54s]  except for right here where I've got the 2's.
[2132.54s -> 2134.70s]  Because I have I like twice, right?
[2134.70s -> 2137.78s]  But in principle, I've got a matrix of counts
[2137.78s -> 2140.94s]  for all the different counts here.
[2140.94s -> 2145.78s]  So maybe, you know, this gives me a word vector, right?
[2145.78s -> 2149.14s]  You know, here's a word vector for deep.
[2149.14s -> 2151.06s]  It's this long vector here.
[2151.06s -> 2153.86s]  And, you know, I could just say that is my word vector.
[2153.86s -> 2156.90s]  And indeed, sometimes people have done that.
[2156.90s -> 2160.30s]  But they're kind of ungainly word vectors.
[2160.30s -> 2164.86s]  Because if we have 400,000 words in our vocabulary,
[2164.86s -> 2169.04s]  the size of this matrix is 400,000 by 400,000,
[2169.04s -> 2172.54s]  which is a lot worse than our word vectors.
[2172.54s -> 2175.48s]  Because if we're making them only 100 dimensional,
[2175.48s -> 2180.22s]  we've only got 400,000 by 100, which is still a big number.
[2180.22s -> 2184.08s]  But it's a lot smaller than 400,000 times 400,000.
[2184.08s -> 2185.74s]  So that's inconvenient.
[2185.74s -> 2189.04s]  So when people have started with these kind
[2189.12s -> 2192.56s]  of co-occurrence matrix, the general thing
[2192.56s -> 2195.44s]  that people have done is to say, well,
[2195.44s -> 2200.16s]  somehow we want to reduce the dimensionality of that matrix
[2200.16s -> 2205.00s]  so that we have a smaller matrix to deal with.
[2205.00s -> 2208.48s]  And so then how can we reduce the dimensionality
[2208.48s -> 2209.60s]  of the matrix?
[2209.60s -> 2213.24s]  And at this point, if you remember your linear algebra
[2213.24s -> 2214.84s]  and stuff like that, you should
[2214.84s -> 2217.36s]  be thinking of things like PCA.
[2217.36s -> 2220.08s]  And in particular, if you want it to work for any matrix
[2220.08s -> 2224.40s]  of any shape, there's the singular value decomposition.
[2224.40s -> 2227.92s]  So there's a classic singular value decomposition
[2227.92s -> 2229.60s]  for any matrix.
[2229.60s -> 2234.40s]  You can rewrite it as a product of three matrices, a U
[2234.40s -> 2237.72s]  and a V, which are both orthonormal,
[2237.72s -> 2246.92s]  which means that you get these independent vectors.
[2246.96s -> 2250.04s]  They're orthogonal to each other.
[2250.04s -> 2254.20s]  And then in the middle, we have the singular vectors,
[2254.20s -> 2256.00s]  which are ordered in size.
[2256.00s -> 2258.40s]  It's the most important singular vector.
[2258.40s -> 2260.60s]  And these are sort of weighting terms
[2260.60s -> 2265.12s]  on the different number of the different dimensions.
[2265.12s -> 2269.56s]  And so this is sort of the full SVD decomposition.
[2269.56s -> 2272.86s]  But part of it is irrelevant, because if I've
[2272.86s -> 2275.92s]  got this picture, nothing is happening
[2275.92s -> 2279.12s]  on the part that's sort of shown in yellow there.
[2279.12s -> 2283.22s]  But if you want, at the moment, this
[2283.22s -> 2287.48s]  is just a full decomposition.
[2287.48s -> 2290.40s]  But if we wanted to have sort of smaller, low-dimensional
[2290.40s -> 2294.12s]  vectors, well, the next trick we pull is we say,
[2294.12s -> 2297.60s]  well, we know where the smallest singular vectors are.
[2297.60s -> 2300.16s]  So we could just set them to 0.
[2300.16s -> 2303.82s]  And if we did that, then more of this goes away.
[2303.86s -> 2308.14s]  And we end up with two-dimensional representations
[2308.14s -> 2310.00s]  of our words.
[2310.00s -> 2312.34s]  And so that gives us another way
[2312.34s -> 2318.22s]  of forming low-dimensional word representations.
[2318.22s -> 2320.46s]  And this has actually been explored
[2320.46s -> 2323.54s]  before modern neural word vectors
[2323.54s -> 2327.86s]  using algorithms such as latent semantic analysis.
[2327.86s -> 2331.42s]  And it has sort of half worked,
[2331.42s -> 2334.70s]  but it never worked very well.
[2334.70s -> 2337.54s]  But some people, especially in psychology,
[2337.54s -> 2340.16s]  had kept on working on it.
[2340.16s -> 2343.84s]  And among other people, in the early 2000s,
[2343.84s -> 2346.86s]  there was this grad student, Doug Roady,
[2346.86s -> 2348.70s]  who kept on working on it.
[2348.70s -> 2352.14s]  And he came up with an algorithm
[2352.14s -> 2354.42s]  that he called coals.
[2354.42s -> 2359.28s]  And he'd known, as other people before him had known,
[2359.32s -> 2362.64s]  that just sort of doing an SVD on raw counts
[2362.64s -> 2366.48s]  didn't seem to give you word vectors that worked very well.
[2366.48s -> 2369.44s]  But he had some ideas to do better than that.
[2369.44s -> 2374.04s]  So one thing that helps a lot is if you log the frequencies.
[2374.04s -> 2377.80s]  So you can put log frequencies in the cells.
[2377.80s -> 2382.22s]  But then he sort of used some other ideas, some of which
[2382.22s -> 2385.12s]  were also picked up in Word2Vec, one of which
[2385.12s -> 2388.44s]  is ramping the windows so that you count closer words
[2388.44s -> 2391.48s]  more than further away words.
[2391.48s -> 2395.04s]  He used Pearson correlations instead of counts, et cetera.
[2395.04s -> 2399.04s]  But he ended up coming up with a low dimensional version
[2399.04s -> 2402.70s]  of word vectors that are sort of ultimately still based
[2402.70s -> 2404.88s]  on an SVD.
[2404.88s -> 2407.48s]  And he got out these word vectors.
[2407.48s -> 2412.04s]  And interestingly, no one really noticed at the time.
[2412.04s -> 2414.92s]  But Doug Roady, in his dissertation,
[2414.92s -> 2418.00s]  effectively discovered this same property
[2418.04s -> 2420.64s]  of having linear semantic components.
[2420.64s -> 2422.72s]  So look, here we go.
[2422.72s -> 2426.02s]  So this is actually a picture from his dissertation.
[2426.02s -> 2429.40s]  And look here, we've got this meaning component, which
[2429.40s -> 2431.18s]  is doer of an event.
[2431.18s -> 2433.16s]  And he's essentially shown with the way
[2433.16s -> 2437.68s]  he's processed his word vectors that the doer of an event
[2437.68s -> 2439.96s]  is a linear meaning component that you
[2439.96s -> 2444.68s]  can use to move between a verb and the doer of the verb.
[2444.68s -> 2447.60s]  Kind of cool, but he didn't become famous
[2447.60s -> 2449.50s]  because no one was paying attention
[2449.50s -> 2452.00s]  to what he had come up with.
[2452.00s -> 2456.82s]  So once Word2Vec became popular,
[2456.82s -> 2461.16s]  that was something that I was kind of interested in.
[2461.16s -> 2466.86s]  And so working together with a postdoc, Jeffrey Pennington,
[2466.86s -> 2471.04s]  we thought that there was interest
[2471.04s -> 2474.16s]  in this sort of space of doing things
[2474.16s -> 2476.56s]  with matrices of counts.
[2476.56s -> 2479.44s]  And how do you then get them to work well
[2479.44s -> 2482.80s]  as word vectors in the same way that Word2Vec worked well
[2482.80s -> 2484.12s]  as word vectors?
[2484.12s -> 2488.48s]  And so that's what led into the GloVe algorithm that was
[2488.48s -> 2490.48s]  what I was actually showing you.
[2490.48s -> 2495.48s]  And so what we wanted was to say, look,
[2495.48s -> 2500.80s]  we want a model in which linear components, sort
[2500.80s -> 2504.16s]  of adding or subtracting a vector and a vector space,
[2504.20s -> 2507.36s]  correspond to a meaning difference.
[2507.36s -> 2510.24s]  How can we do that?
[2510.24s -> 2515.44s]  And Jeffrey did good thinking and math
[2515.44s -> 2518.40s]  and thought about that for a bit.
[2518.40s -> 2521.62s]  And his solution was to say, well,
[2521.62s -> 2527.12s]  if we think that ratios of co-occurrence probabilities
[2527.12s -> 2529.84s]  can encode meaning components, so if we
[2529.84s -> 2533.12s]  can make a ratio of co-occurrence probabilities
[2533.12s -> 2535.76s]  into something linear in the vector space,
[2535.76s -> 2541.00s]  we'll get the kind of result that Word2Vec or Doug Rode got.
[2541.00s -> 2542.80s]  So what does that mean?
[2542.80s -> 2546.04s]  Well, so if you start thinking of words occurring
[2546.04s -> 2548.44s]  in the context of ice, you might
[2548.44s -> 2550.44s]  think that sort of solid and water
[2550.44s -> 2552.56s]  are likely to occur near ice.
[2552.56s -> 2556.32s]  And gas or a random word like random
[2556.32s -> 2559.52s]  aren't likely to occur near ice.
[2559.52s -> 2563.72s]  And similarly, for steam, you'd expect
[2563.72s -> 2567.92s]  that gas and water are likely to occur near steam,
[2567.92s -> 2570.72s]  but probably not solid or random.
[2570.72s -> 2573.60s]  And well, if you're just looking at one of these,
[2573.60s -> 2575.56s]  you don't really get meaning components
[2575.56s -> 2578.28s]  because you get something that's large here
[2578.28s -> 2579.92s]  or large here.
[2579.92s -> 2583.40s]  But if you then look at the ratio of two
[2583.40s -> 2586.02s]  of these co-occurrence probabilities,
[2586.02s -> 2589.66s]  then what you get out is that for solid,
[2589.66s -> 2593.70s]  it's going to be large, and small is going to be.
[2593.70s -> 2595.90s]  And for gas, it's going to be small.
[2595.90s -> 2599.58s]  And so you're getting direction in the space, which
[2599.58s -> 2603.98s]  will correspond to the solid-liquid-gas dimension
[2603.98s -> 2607.30s]  of physics, whereas for the other words,
[2607.30s -> 2609.06s]  it will be about one.
[2609.06s -> 2612.70s]  This is just the wave your hands.
[2612.70s -> 2614.98s]  This was the conception of the idea
[2615.02s -> 2617.62s]  that if you actually do the counts, this actually works out.
[2617.62s -> 2622.42s]  So using real data, this is what you get for co-occurrence.
[2622.42s -> 2626.66s]  And indeed, you kind of get these sort of factors of 10
[2626.66s -> 2629.26s]  in both of these directions of these two.
[2629.26s -> 2633.66s]  And the numbers that are over there are approximately one.
[2633.66s -> 2636.98s]  So Jeffrey's idea was, well, we're
[2636.98s -> 2641.22s]  going to start with a co-occurrence count matrix,
[2641.22s -> 2647.14s]  and we want to make this turn into a linear component.
[2647.14s -> 2649.06s]  And well, how do you do that?
[2649.06s -> 2652.24s]  Well, first of all, it sort of makes sense immediately
[2652.24s -> 2654.10s]  that you should be putting a log in, right?
[2654.10s -> 2656.94s]  Because once you put a log in, this ratio
[2656.94s -> 2660.86s]  will be being turned into something that's subtracted.
[2660.86s -> 2663.50s]  And so simply all you have to do
[2663.50s -> 2669.22s]  is have a log-by-linear model where the dot product of two
[2669.30s -> 2673.06s]  word vectors models this conditional probability.
[2673.06s -> 2676.50s]  And then the difference between two vectors
[2676.50s -> 2678.54s]  will be corresponding to this log
[2678.54s -> 2682.78s]  of the ratio of their co-occurrence probabilities.
[2682.78s -> 2686.44s]  So that was basically the glove model.
[2686.44s -> 2693.46s]  So you're wanting to model this dot product such
[2693.46s -> 2697.34s]  that it's being close to the log of the co-occurrence
[2697.38s -> 2698.50s]  probability.
[2698.50s -> 2700.74s]  But you sort of do a little bit of extra work
[2700.74s -> 2705.14s]  to have some bias terms and some frequency thresholds,
[2705.14s -> 2707.42s]  which aren't very important.
[2707.42s -> 2709.02s]  So I'm going to skip past them.
[2709.02s -> 2711.90s]  But I think that basic intuition
[2711.90s -> 2715.06s]  as to what's the important thing to get linear meaning
[2715.06s -> 2717.74s]  components is a good one to know about.
[2720.58s -> 2721.38s]  OK.
[2721.38s -> 2724.26s]  Is everyone good to there?
[2724.26s -> 2725.90s]  Cool.
[2725.90s -> 2726.78s]  Yes.
[2726.78s -> 2730.14s]  Oh, I noticed the original x matrix you showed
[2730.14s -> 2732.02s]  was like 3 by 5 or something.
[2732.02s -> 2733.58s]  Shouldn't it be square?
[2733.58s -> 2734.46s]  So yeah.
[2734.46s -> 2737.26s]  I mean, if you're doing, sorry, yeah.
[2737.26s -> 2739.38s]  Maybe should have just shown you a square one.
[2739.38s -> 2741.82s]  If you were just doing vocabulary to vocabulary,
[2741.82s -> 2743.98s]  yes, it should be square.
[2743.98s -> 2745.54s]  But there was a bit in the slides
[2745.54s -> 2747.46s]  that I didn't mention that there was another way you
[2747.46s -> 2750.42s]  could do it where you did it words versus documents,
[2750.42s -> 2752.22s]  and then it would be non-square.
[2752.22s -> 2753.50s]  But yeah, you're right.
[2753.50s -> 2759.06s]  So we can just consider the square case.
[2759.06s -> 2761.82s]  OK.
[2761.82s -> 2765.60s]  So hey, I showed you that demo of the GloVe vectors.
[2765.60s -> 2768.82s]  And they worked great, didn't they?
[2768.82s -> 2771.26s]  So these are good vectors.
[2771.26s -> 2774.10s]  But in general in NLP, we'd like
[2774.10s -> 2777.50s]  to have things that we can evaluate and know
[2777.50s -> 2779.94s]  whether things are really good.
[2779.94s -> 2783.54s]  And so everywhere through the course,
[2783.54s -> 2785.86s]  we're going to want to evaluate things and work out
[2785.86s -> 2790.14s]  how good they are, and what's better, and what's worse.
[2790.14s -> 2793.54s]  And so one of the fundamental notions of evaluation
[2793.54s -> 2795.18s]  that will come up again and again
[2795.18s -> 2798.30s]  is intrinsic and extrinsic evaluations.
[2798.30s -> 2801.38s]  So an intrinsic evaluation is where
[2801.38s -> 2806.52s]  you are doing a very specific internal subtask,
[2806.52s -> 2809.24s]  and you just try and score whether it's good or bad.
[2809.24s -> 2812.08s]  So normally, intrinsic evaluations
[2812.08s -> 2814.72s]  are fast to compute, help you understand
[2814.72s -> 2816.60s]  the component you're building.
[2816.60s -> 2820.76s]  But they are sort of distant from your downstream task,
[2820.76s -> 2822.92s]  and improving the numbers internally
[2822.92s -> 2825.36s]  may or may not help you.
[2825.36s -> 2830.44s]  And that's the contrast with an extrinsic evaluation, which
[2830.44s -> 2832.96s]  is that you've got some real task.
[2832.96s -> 2835.80s]  You want to do question answering or document
[2835.84s -> 2839.44s]  summarization or machine translation,
[2839.44s -> 2842.12s]  and you want to know whether some clever bit
[2842.12s -> 2846.76s]  of internal modeling will help you on that task.
[2846.76s -> 2849.78s]  So then you have to sort of run an entire system
[2849.78s -> 2853.24s]  and work out downstream accuracies
[2853.24s -> 2855.20s]  and find out whether it actually helps you
[2855.20s -> 2856.84s]  at the end of the day.
[2856.84s -> 2859.80s]  But that often means it's kind of indirect,
[2859.80s -> 2863.32s]  so hard to see exactly what's happening in your task.
[2863.36s -> 2866.88s]  So for something like word vectors,
[2866.88s -> 2870.52s]  if we just sort of measure, are they modeling word similarity
[2870.52s -> 2874.44s]  well, that's an intrinsic evaluation.
[2874.44s -> 2879.46s]  But we'd probably like to know whether they model word
[2879.46s -> 2882.76s]  similarity well for some downstream task, which
[2882.76s -> 2885.36s]  might be doing web search.
[2885.36s -> 2890.36s]  We'd like, when you say cell phone or mobile phone,
[2890.36s -> 2892.96s]  that it comes out about the same.
[2892.96s -> 2894.72s]  So that would then be web search,
[2894.72s -> 2898.48s]  might be our extrinsic evaluation.
[2898.48s -> 2905.40s]  So for word vectors, two intrinsic evaluations
[2905.40s -> 2907.40s]  are the ones we've already seen.
[2907.40s -> 2911.88s]  So there's the word vector analogies.
[2911.88s -> 2912.86s]  I cheated.
[2912.86s -> 2914.64s]  When I showed you the glove demo,
[2914.64s -> 2916.80s]  I only showed you ones that work.
[2916.80s -> 2918.48s]  But if you play for it yourself,
[2918.48s -> 2921.00s]  you can find some that don't work.
[2921.04s -> 2927.52s]  So what we can do is sort of have a set of word analogies
[2927.52s -> 2929.88s]  and find out which ones work.
[2929.88s -> 2932.04s]  Now, in general, glove does work.
[2932.04s -> 2936.28s]  Here's a set of word vectors showing you
[2936.28s -> 2938.72s]  the sort of male-female distinction.
[2938.72s -> 2940.80s]  That's kind of good and linear.
[2940.80s -> 2944.04s]  But in general, for different ones, it's going to work
[2944.04s -> 2945.08s]  and it's not working.
[2945.08s -> 2947.08s]  You're going to be able to score what percentage
[2947.08s -> 2949.50s]  of the time it works.
[2949.54s -> 2952.66s]  Or we can do word similarity.
[2952.66s -> 2955.74s]  How we do word similarity is we actually
[2955.74s -> 2958.26s]  use human judgments of similarity.
[2958.26s -> 2963.64s]  So psychologists ask undergrads, and they say,
[2963.64s -> 2966.02s]  here is the word plane and car.
[2966.02s -> 2969.50s]  How similar are they on a scale of 1 to 10,
[2969.50s -> 2970.94s]  or 0 to 10 maybe?
[2970.94s -> 2972.78s]  Actually, I think it's 0 to 10 here.
[2972.78s -> 2978.02s]  On a scale of 0 to 10, and the person says, mm, 7.
[2978.02s -> 2980.86s]  And then they ask another person.
[2980.86s -> 2984.58s]  And they average what the undergrads say,
[2984.58s -> 2986.26s]  and they come out with these numbers.
[2986.26s -> 2988.86s]  So tiger, tiger gets 10.
[2988.86s -> 2993.26s]  Book and paper got an average of 7.46.
[2993.26s -> 2996.60s]  Plane and car got 5.77.
[2996.60s -> 2999.30s]  Stock and phone got 1.62.
[2999.30s -> 3002.38s]  And stock and Jaguar got 0.92.
[3002.38s -> 3004.46s]  Noisy process, but you roughly get
[3004.46s -> 3007.16s]  to see how similar people think words are.
[3007.20s -> 3011.40s]  And so then we ask our models to also score
[3011.40s -> 3013.88s]  how similar they think words are.
[3013.88s -> 3020.36s]  And then we get models of how well the scores are correlated
[3020.36s -> 3025.80s]  between human judgments and our model's judgments.
[3025.80s -> 3028.52s]  And so here are sort of a big table of numbers
[3028.52s -> 3031.24s]  that we don't need to go through all of.
[3031.24s -> 3036.10s]  But it sort of shows that a plane SVD works terribly.
[3036.10s -> 3038.82s]  Simply doing an SVD over log counts
[3038.82s -> 3041.70s]  already starts to work reasonably.
[3041.70s -> 3045.50s]  And then here's the two word-to-beck algorithms,
[3045.50s -> 3047.94s]  SIBO and skip-gram.
[3047.94s -> 3050.42s]  And here are numbers from our glob vectors.
[3050.42s -> 3052.42s]  And so you get these kind of scores
[3052.42s -> 3054.94s]  that you can then score different models as
[3054.94s -> 3057.30s]  to how good they are.
[3057.30s -> 3060.82s]  And well, then, you can also, oh, sorry, yeah.
[3060.82s -> 3062.34s]  That's the only thing I have there.
[3062.94s -> 3067.06s]  What can you do for downstream evaluation?
[3067.06s -> 3070.34s]  Well, then you want to pick some downstream task.
[3070.34s -> 3072.94s]  And so a simple downstream task that's
[3072.94s -> 3076.26s]  been used a lot in NLP is what's
[3076.26s -> 3078.94s]  called named entity recognition.
[3078.94s -> 3082.54s]  And so that's recognizing names of things
[3082.54s -> 3084.46s]  and what type they are.
[3084.46s -> 3087.98s]  So if the sentence is Chris Manning lives in Palo Alto,
[3087.98s -> 3090.38s]  you want to say Chris and Manning.
[3090.38s -> 3092.32s]  That's the name of a person.
[3092.32s -> 3096.44s]  And Palo and Alto, that's the name of a place.
[3096.44s -> 3098.54s]  So that can be the task.
[3098.54s -> 3100.36s]  And well, that's the kind of task
[3100.36s -> 3103.84s]  which you might think word vectors would help you with.
[3103.84s -> 3106.38s]  And it's indeed the case, right?
[3106.38s -> 3108.80s]  So what's labeled as discrete was
[3108.80s -> 3114.24s]  a baseline symbolic probabilistic named entity
[3114.24s -> 3115.84s]  recognition task.
[3115.84s -> 3118.36s]  And by putting word vectors into it,
[3118.36s -> 3120.72s]  you can make the numbers go up.
[3120.76s -> 3123.44s]  So these numbers for GloVe are higher
[3123.44s -> 3125.56s]  than the ones on the first line.
[3125.56s -> 3129.00s]  And so I'm getting substantial improvements
[3129.00s -> 3131.56s]  from adding word vectors to my system.
[3131.56s -> 3132.06s]  Yay.
[3136.32s -> 3139.20s]  OK.
[3139.20s -> 3141.52s]  I'll pile ahead into the next thing.
[3141.52s -> 3143.36s]  This next one, I think, is interesting
[3143.36s -> 3145.20s]  and we should spend a minute on.
[3145.20s -> 3147.96s]  And it came up in your questions last time.
[3148.08s -> 3151.24s]  Words have lots of meanings.
[3151.24s -> 3156.54s]  Most words have a whole bunch of meanings.
[3156.54s -> 3159.32s]  Words that don't have a lot of different meanings
[3159.32s -> 3162.72s]  are only some very specialized scientific words.
[3162.72s -> 3163.28s]  OK.
[3163.28s -> 3166.28s]  So my example of a word with multiple meanings
[3166.28s -> 3169.40s]  is probably not the first one you think of all the time.
[3169.40s -> 3171.76s]  The most famous example of a word with a lot of meanings
[3171.76s -> 3174.40s]  is bank, which already came up last time.
[3174.40s -> 3176.52s]  And I used star, which is another one.
[3176.52s -> 3179.20s]  Here's a word that you probably don't use that often,
[3179.20s -> 3181.32s]  but it still has lots of meanings.
[3181.32s -> 3183.36s]  So the word pike, what are some things
[3183.36s -> 3186.68s]  that the word pike can mean?
[3186.68s -> 3187.28s]  A fish.
[3187.28s -> 3188.68s]  Yes, it's a kind of fish.
[3188.68s -> 3190.20s]  OK, we've got one.
[3190.20s -> 3191.72s]  What else could a pike be?
[3191.72s -> 3192.36s]  Yeah.
[3192.36s -> 3193.32s]  A spear.
[3193.32s -> 3195.88s]  A spear, yeah, for the Dungeons and Dragons crowd.
[3195.88s -> 3198.00s]  Yeah, there's a long arm, right?
[3198.00s -> 3199.32s]  Yep, that's another one.
[3199.32s -> 3200.88s]  Yeah.
[3200.88s -> 3201.88s]  A road, right.
[3201.88s -> 3206.04s]  Yes, so pike is used as a shorthand, well, a shorthand
[3206.56s -> 3207.24s]  for a turnpike.
[3207.24s -> 3209.80s]  Why it's called a turnpike was, yeah, originally you
[3209.80s -> 3213.76s]  had the speery-looking thing at the start of it
[3213.76s -> 3214.92s]  as sort of count people.
[3214.92s -> 3218.16s]  OK, we've got three other meanings for pike, yeah?
[3218.16s -> 3219.96s]  Is it also a crab?
[3219.96s -> 3221.56s]  Like a fraternity?
[3221.56s -> 3224.40s]  The name of a fraternity?
[3224.40s -> 3225.24s]  I'll believe you.
[3225.24s -> 3226.72s]  I can't say I know that one.
[3229.72s -> 3232.24s]  Other pikes?
[3232.28s -> 3234.80s]  I'll be sharp as like a needle.
[3234.80s -> 3237.24s]  I'll be sharp.
[3237.24s -> 3237.92s]  Maybe.
[3237.92s -> 3240.36s]  I mean, I think it's really the sort of pike as the weapon.
[3243.48s -> 3246.56s]  Any others scratch your head?
[3246.56s -> 3248.32s]  One that I think a lot of you
[3248.32s -> 3254.84s]  will have seen in diving and swimming, you can do a pike.
[3254.84s -> 3258.44s]  Olympics, if you see Olympic diving, there are pikes.
[3258.44s -> 3259.40s]  Anyone seen those?
[3262.28s -> 3265.08s]  Trust me, there's a pike.
[3265.08s -> 3270.20s]  OK, and so we've sort of been doing the noun uses,
[3270.20s -> 3273.80s]  but you can also use pike as a verb, right?
[3273.80s -> 3276.88s]  You know, like, once you've got your medieval weapon,
[3276.88s -> 3279.32s]  you can pike somebody.
[3279.32s -> 3283.56s]  And that's a usage of pike.
[3283.56s -> 3284.76s]  And you can do other ones.
[3284.76s -> 3286.08s]  So here we go.
[3286.08s -> 3291.56s]  Here's ones I got from a dictionary.
[3291.56s -> 3293.12s]  We got most of those.
[3293.12s -> 3294.88s]  There are sort of weirder usages, right?
[3294.88s -> 3296.44s]  Like coming down the pike, that's
[3296.44s -> 3298.24s]  kind of a metaphorical use that
[3298.24s -> 3302.28s]  comes from the road sense, but it sort of
[3302.28s -> 3305.08s]  ends up meaning the future.
[3305.08s -> 3306.60s]  Yeah.
[3306.60s -> 3310.88s]  In Australia, we also use pike to mean sort of chicken
[3310.88s -> 3313.20s]  out of doing something.
[3313.20s -> 3315.64s]  But I don't think that usage is really used in the US.
[3315.64s -> 3317.48s]  Anyway, words have lots of meanings.
[3317.48s -> 3319.60s]  So how can you deal with that?
[3319.60s -> 3321.20s]  Well, one way you could deal with it
[3321.24s -> 3327.00s]  is to say, OK, words have several meanings.
[3327.00s -> 3331.16s]  And so we're just going to say words have several meanings.
[3331.16s -> 3335.04s]  And then we're going to take instances of words and text.
[3335.04s -> 3337.04s]  We're going to cluster them based
[3337.04s -> 3338.76s]  on their similarity of occurrence
[3338.76s -> 3344.76s]  to decide which sense of the word to regard each token as.
[3344.76s -> 3349.64s]  And then we're going to learn word vectors for those token
[3349.64s -> 3352.24s]  clusters, which are our senses.
[3352.24s -> 3353.88s]  And you can do that.
[3353.88s -> 3358.88s]  We did that in 2012 before Word2Vec came out.
[3358.88s -> 3361.40s]  So you see here we have bank one.
[3361.40s -> 3365.92s]  And somewhere over here we have bank two.
[3365.92s -> 3371.08s]  And here we have jaguar one, jaguar two, jaguar three,
[3371.08s -> 3372.72s]  jaguar four.
[3372.72s -> 3375.84s]  And this really works out great, right?
[3375.84s -> 3382.28s]  So jaguar one picks out the sense of the kind of car,
[3382.28s -> 3385.00s]  and it's close to luxury and convertible.
[3385.00s -> 3389.84s]  Jaguar two comes right close to software and Microsoft.
[3389.84s -> 3392.32s]  And this one's a bit of a historical one.
[3392.32s -> 3397.84s]  But that's when most of you were five or whatever.
[3397.84s -> 3403.48s]  You might remember Apple used to use large cats for versions
[3403.48s -> 3406.32s]  of Mac OS, right?
[3406.32s -> 3410.48s]  So sort of Mac OS 10.3 or something like that.
[3410.48s -> 3413.24s]  A long time ago it was called Jaguar, right?
[3413.24s -> 3416.52s]  So it's software close to Microsoft.
[3416.52s -> 3422.92s]  Jaguar three, OK, string, keyboard, solo, musical,
[3422.92s -> 3427.52s]  drum, bass, that's because there's a Jaguar keyboard.
[3427.52s -> 3432.36s]  And then finally, what we think of as the basic sense,
[3432.36s -> 3436.64s]  but turns out turns up rather less in text corporate.
[3436.64s -> 3440.00s]  Normally, Jaguar next to Hunter is the animal, right?
[3440.00s -> 3443.52s]  So it's done a good job of learning the different senses.
[3443.52s -> 3448.80s]  But that's not what's actually usually done these days.
[3448.80s -> 3451.40s]  And instead, what's usually done
[3451.40s -> 3455.24s]  is you do only have one vector for Jaguar.
[3455.24s -> 3459.32s]  And when you do that, or pike here,
[3459.32s -> 3466.56s]  the one vector you learn is a weighted average
[3466.56s -> 3471.00s]  of the vectors that you would have learned for the senses.
[3471.00s -> 3474.44s]  It's often referred to as a superposition
[3474.44s -> 3477.56s]  because somehow neural net math people
[3477.56s -> 3480.00s]  like to use physics terms.
[3480.00s -> 3483.44s]  And so they call it a superposition.
[3483.44s -> 3485.04s]  But it's a weighted average.
[3485.04s -> 3487.44s]  So you're taking the relative frequency
[3487.52s -> 3490.56s]  of the different senses and multiplying the vectors
[3490.56s -> 3493.04s]  you would have learned if you'd had sense vectors.
[3493.04s -> 3498.96s]  And that's what you get as the representation as a whole.
[3498.96s -> 3504.24s]  And I can make a sort of a linguistic argument
[3504.24s -> 3507.16s]  as to why you might want to do that, which
[3507.16s -> 3512.26s]  is although this model of words have senses
[3512.26s -> 3516.64s]  is very longstanding and common, I
[3516.64s -> 3519.28s]  mean, it's essentially the way dictionaries are built, right?
[3519.28s -> 3520.84s]  You look up a word in the dictionary,
[3520.84s -> 3523.96s]  and it says sense one, sense two, sense three.
[3523.96s -> 3526.92s]  And you get them for things like bank or jaguars
[3526.92s -> 3528.34s]  we're talking about.
[3528.34s -> 3532.40s]  I mean, it's sort of really a broken model, right?
[3532.40s -> 3537.40s]  That word meanings have a lot of nuance.
[3537.40s -> 3540.32s]  They're used in a lot of different contexts.
[3540.32s -> 3544.36s]  There are extreme examples like bank, wherever it was,
[3544.36s -> 3547.72s]  where we have finance bank and bank of a riverbank
[3547.72s -> 3551.96s]  over here, where it seems like the senses are this far apart.
[3551.96s -> 3555.40s]  But most words have sort of different meanings,
[3555.40s -> 3557.56s]  but they're not actually that far apart.
[3557.56s -> 3560.04s]  And trying to cut them into senses
[3560.04s -> 3562.56s]  seems actually very artificial.
[3562.56s -> 3567.32s]  And if you look up five different dictionaries,
[3567.32s -> 3570.40s]  and you say, how many senses does this word have,
[3570.40s -> 3573.64s]  pretty much everyone will give you a different answer.
[3573.72s -> 3578.44s]  The kind of situation you have is a word like field.
[3578.44s -> 3583.66s]  Well, a field can be used for a place where you grow a crop.
[3583.66s -> 3586.62s]  It can be used for sort of natural things,
[3586.62s -> 3590.72s]  like a rock field or an ice field.
[3590.72s -> 3594.12s]  It can be used for a sporting field.
[3594.12s -> 3596.64s]  There's the mathematical sense of field.
[3596.64s -> 3598.32s]  Now, all of these things sort of
[3598.32s -> 3600.12s]  have something to do with each other.
[3600.12s -> 3601.72s]  I mean, the math one's further away,
[3601.84s -> 3605.80s]  the physical ones are sort of flat spaces.
[3605.80s -> 3610.48s]  But the sense of it being a sporting field
[3610.48s -> 3612.92s]  is really kind of different from the sense of it
[3612.92s -> 3615.52s]  being an ice field.
[3615.52s -> 3618.64s]  Is the ice field and the rock field different?
[3618.64s -> 3620.60s]  Or am I just modifying them?
[3620.60s -> 3622.32s]  Are they different senses?
[3622.32s -> 3628.40s]  So really, you sort of have a kind of what a math person
[3628.40s -> 3631.16s]  say is sort of like some probability density
[3631.16s -> 3633.36s]  distribution over things that can be
[3633.36s -> 3635.16s]  meant by the meaning of a word.
[3635.16s -> 3638.58s]  So it sort of maybe makes sense to more use this model
[3638.58s -> 3640.20s]  where you're just actually saying,
[3640.20s -> 3644.16s]  we have a vector that's an average of all the contexts.
[3644.16s -> 3646.84s]  And we'll see more of that when we get to contextual word
[3646.84s -> 3649.60s]  vectors later on.
[3649.60s -> 3653.12s]  But one more surprising result on this
[3653.12s -> 3661.16s]  is since you have the vector for pike overall
[3661.16s -> 3668.00s]  being the sum of these different sense vectors,
[3668.00s -> 3671.00s]  standard math would tell you that if you just
[3671.00s -> 3672.84s]  have the single vector, there's
[3672.84s -> 3677.32s]  no way that you can recover the individual sense vectors.
[3677.32s -> 3685.76s]  But higher math tells you that actually these vector spaces
[3685.76s -> 3689.52s]  are so high dimensional and sparse
[3689.52s -> 3693.36s]  that you can use ideas from sparse coding theory
[3693.36s -> 3696.80s]  to reconstruct the sense vectors out
[3696.80s -> 3698.44s]  of the whole vector.
[3698.44s -> 3702.04s]  And if you actually want to understand this,
[3702.04s -> 3704.48s]  some of the people in statistics, David Donahoe,
[3704.48s -> 3706.68s]  I think is one of them, teach courses
[3706.68s -> 3709.20s]  on sparse coding theory.
[3709.20s -> 3711.96s]  But I'm not going to try and teach that.
[3711.96s -> 3716.24s]  But here's an example from this paper, the Sanjeev Arora
[3716.24s -> 3719.58s]  et al, where one of the et als is Ting Yumar, who's
[3719.58s -> 3722.80s]  now a faculty in computer science here,
[3722.80s -> 3726.84s]  where they are starting off with the word vector
[3726.84s -> 3731.80s]  and using sparse coding to divide out sense vectors
[3731.80s -> 3733.76s]  from one word vector.
[3733.76s -> 3735.16s]  And they work pretty well, right?
[3735.16s -> 3739.24s]  So here's one sense of tie, which is piece of clothing,
[3739.24s -> 3743.92s]  another sense of tie, which is ties in the game.
[3743.92s -> 3746.72s]  This one is sort of similar to that one, I'll admit.
[3746.72s -> 3749.88s]  But this sense of tie here is then a tie
[3749.88s -> 3753.72s]  as sort of you put on your electrical cables.
[3753.72s -> 3756.32s]  Then you have the musical sense of tie, right?
[3756.32s -> 3758.24s]  At least four out of five, they've
[3758.24s -> 3760.76s]  done a pretty good job of getting senses out
[3760.76s -> 3764.76s]  of this single word vector by sparse coding.
[3764.76s -> 3766.92s]  So sparse coding must be cool if you want
[3766.92s -> 3770.28s]  to go off and learn more about it.
[3770.28s -> 3770.80s]  OK.
[3774.20s -> 3775.16s]  OK.
[3775.16s -> 3778.08s]  So that's everything I was going to say about words,
[3778.08s -> 3780.84s]  vectors, and word senses.
[3780.84s -> 3782.56s]  Is everyone good to there?
[3782.56s -> 3783.48s]  Any questions?
[3786.56s -> 3789.48s]  I'll rush ahead for the last two pieces.
[3789.48s -> 3793.60s]  OK, so I just wanted to start to introduce in the last 15
[3793.60s -> 3801.72s]  minutes the ideas of how we can build neural classifiers
[3801.72s -> 3806.56s]  and how we start to build in general neural networks.
[3806.56s -> 3809.04s]  I mean, in a sense, we've already
[3809.04s -> 3812.92s]  built a very simple neural classifier,
[3812.92s -> 3816.08s]  because our word2vec model is predicting
[3816.08s -> 3818.40s]  what words are likely to occur
[3818.40s -> 3821.04s]  in the context of another word.
[3821.04s -> 3822.76s]  And you can think of that as a classifier.
[3822.76s -> 3824.48s]  But let's look at a simple classifier,
[3824.48s -> 3828.16s]  like our named entity recognizer that I mentioned before.
[3828.16s -> 3830.04s]  So for the named entity recognizer,
[3830.04s -> 3832.80s]  we want to label words with their class.
[3832.80s -> 3836.72s]  So we want to say these two words are a person,
[3836.72s -> 3839.84s]  but the same words, Paris and Hilton,
[3839.84s -> 3843.64s]  are then locations in this second sentence.
[3843.64s -> 3848.56s]  So words can be ambiguous as to what their class is.
[3848.56s -> 3851.64s]  And the other state is that they're not
[3851.64s -> 3853.32s]  a named entity at all.
[3853.32s -> 3856.52s]  They're just a word that is some other word.
[3856.52s -> 3860.32s]  And this is something that's used in lots of places
[3860.32s -> 3863.08s]  as a bit of understanding.
[3863.08s -> 3866.00s]  So if you've seen any of those web pages
[3866.00s -> 3869.76s]  where they've tagged company names with a stock ticker
[3870.72s -> 3873.88s]  or there's links on a Wikipedia page
[3873.88s -> 3876.52s]  to a Wikipedia page or something like that,
[3876.52s -> 3879.84s]  you've got named entities where commonly
[3879.84s -> 3882.24s]  after finding the named entities,
[3882.24s -> 3885.44s]  you're doing this second stage of entity linking
[3885.44s -> 3887.72s]  where you're then linking the named entity
[3887.72s -> 3891.44s]  to some canonical form of it, like a Wikipedia page.
[3891.44s -> 3894.24s]  But we're not gonna talk about the second part of it
[3894.24s -> 3895.74s]  for the rest of the day.
[3896.66s -> 3901.66s]  And so we could say that building with our word vectors,
[3903.62s -> 3907.58s]  we've got this simple task where what we're gonna do
[3907.58s -> 3910.02s]  is we're gonna look at a word in context
[3910.02s -> 3913.06s]  because sometimes Paris is a name of a person,
[3913.06s -> 3915.10s]  sometimes it's a location.
[3915.10s -> 3917.74s]  And so we're gonna want to look at this word
[3917.74s -> 3920.54s]  in its context and say, aha,
[3920.54s -> 3924.10s]  this is a name of a location in this instance.
[3924.10s -> 3927.14s]  And so the way that we're gonna do it
[3927.14s -> 3931.88s]  is we're going to form a window classifier.
[3931.88s -> 3933.38s]  So we're gonna take a word
[3933.38s -> 3936.54s]  with a couple of words of context on each side
[3936.54s -> 3939.62s]  and for the words in our context window,
[3939.62s -> 3941.34s]  we're gonna use our word vectors
[3941.34s -> 3943.62s]  because we wanna show they're useful for something.
[3943.62s -> 3946.76s]  And then we wanna feed this into something
[3946.76s -> 3950.90s]  that is a classifier and our classifier,
[3950.90s -> 3954.02s]  it's actually gonna be a really simple loca-classifier.
[3954.82s -> 3958.66s]  We're only here gonna do location or not a location.
[3958.66s -> 3961.08s]  So this one here, we're wanting to say
[3961.08s -> 3966.08s]  for this window here, yes, it's a location.
[3966.90s -> 3969.54s]  And whereas if it had been,
[3969.54s -> 3973.46s]  I, what, I love Paris Hilton greatly,
[3973.46s -> 3976.38s]  then we'd be saying no because Paris,
[3976.38s -> 3979.10s]  the word in the middle of the context
[3979.10s -> 3981.66s]  then isn't a location.
[3981.66s -> 3984.58s]  So that's sort of the idea of a classification
[3984.58s -> 3986.94s]  or classifier, we're making,
[3986.94s -> 3991.94s]  assigning some set of classes to things, right?
[3992.30s -> 3995.30s]  So in general for classifiers,
[3995.30s -> 3997.18s]  we do supervised learning,
[3997.18s -> 4000.30s]  which means we have some labeled examples,
[4000.30s -> 4002.02s]  our training data set.
[4002.02s -> 4005.74s]  So we have input items xi and for each one,
[4005.74s -> 4008.20s]  we've got a class yi.
[4008.24s -> 4012.02s]  So I had for my example, training examples,
[4012.02s -> 4015.90s]  ones like I love Paris Hilton greatly,
[4015.90s -> 4020.16s]  that was negative, not a location,
[4020.16s -> 4023.06s]  and I visit Paris every spring,
[4023.06s -> 4025.08s]  that's positive, that is a location
[4025.08s -> 4027.70s]  where I'm actually classifying the middle word.
[4027.70s -> 4031.08s]  Okay, so inputs, labels, and in general,
[4031.08s -> 4034.12s]  we've got labels are a set of classes.
[4034.12s -> 4037.94s]  So my set here is simply location, not a location,
[4038.54s -> 4041.06s]  but I could get fancier and I could say,
[4041.06s -> 4045.68s]  I've got five classes, I've got location, person, name,
[4047.58s -> 4051.74s]  whatever other ones there are, company name, drug name,
[4051.74s -> 4053.88s]  right, I could be assigning a bunch of,
[4053.88s -> 4057.62s]  or other, not a name, a bunch of different classes,
[4057.62s -> 4060.14s]  but I'm gonna be doing it with only two
[4060.14s -> 4062.14s]  because I'm using this example
[4062.14s -> 4064.78s]  on next Tuesday's lecture as well
[4064.78s -> 4067.42s]  and I'm wanting to keep it simple.
[4067.94s -> 4070.14s]  So that's what we're going to do.
[4070.14s -> 4074.32s]  And so what we're gonna be using in our class
[4074.32s -> 4076.94s]  is neural classifiers.
[4076.94s -> 4081.94s]  And so I just wanted to sort of go through quickly
[4082.70s -> 4085.54s]  just the sort of food for thought as we go into it.
[4085.54s -> 4090.54s]  So for a typical stats machine learning classifier,
[4091.44s -> 4094.66s]  you can build classifiers like logistic regression
[4094.66s -> 4097.80s]  or softmax classifiers or other ones
[4097.80s -> 4101.02s]  like support vector machines or naive Bayes
[4101.02s -> 4104.34s]  or whatever else you might've seen.
[4104.34s -> 4107.28s]  The vast majority of these classifiers
[4107.28s -> 4109.26s]  are linear classifiers,
[4109.26s -> 4112.20s]  meaning that they have a linear decision boundary.
[4112.20s -> 4114.78s]  And when we're learning these classifiers,
[4114.78s -> 4117.70s]  we're learning parameters here, W,
[4117.70s -> 4120.14s]  but our inputs are fixed,
[4120.14s -> 4122.90s]  that our inputs are represented by symbols
[4122.90s -> 4125.66s]  like or quantities.
[4125.66s -> 4127.06s]  So we have fixed inputs,
[4127.06s -> 4129.98s]  we learn parameters as weights
[4129.98s -> 4133.30s]  that are used to multiply the inputs
[4133.30s -> 4136.26s]  and then we use a linear decision boundary.
[4136.26s -> 4139.46s]  So when we have our neural classifier,
[4139.46s -> 4142.74s]  we're kind of getting some more power.
[4142.74s -> 4144.06s]  So first of all,
[4144.06s -> 4147.50s]  we're not only learning weights W for our classifier,
[4147.50s -> 4150.42s]  we're also learning distributed representations
[4150.42s -> 4151.94s]  for our words.
[4151.94s -> 4156.14s]  So our words can sort of re-represent,
[4156.14s -> 4159.98s]  our word vectors re-represent the actual words as symbols
[4159.98s -> 4162.98s]  and can move them around in the space.
[4162.98s -> 4166.48s]  So that in terms of the original space,
[4166.48s -> 4168.70s]  we've got a non-linear classifier
[4168.70s -> 4172.62s]  that can represent much more complex functions,
[4172.62s -> 4176.38s]  but we will then sort of use the word vectors
[4176.38s -> 4181.38s]  to re-represent those words to do a final classification.
[4181.54s -> 4184.64s]  So at the end of our deep network,
[4184.64s -> 4186.16s]  which we're about to build,
[4186.16s -> 4188.46s]  we will have a linear classifier
[4188.46s -> 4191.46s]  in terms of our re-represented vectors,
[4191.46s -> 4193.92s]  but not in terms of our original space.
[4193.92s -> 4196.74s]  Let me try and be concrete about that.
[4196.74s -> 4198.90s]  Okay, so here's what I'm gonna use
[4198.90s -> 4201.68s]  and we'll use again next Tuesday
[4201.68s -> 4205.58s]  as my little neural network.
[4205.58s -> 4208.74s]  And so I start with some words,
[4208.74s -> 4211.18s]  museums in Paris are amazing.
[4211.82s -> 4215.82s]  I first of all come up with the word embedding of those
[4215.82s -> 4217.42s]  using my word vectors.
[4217.42s -> 4221.62s]  So now I've got this sort of high dimensional vector,
[4221.62s -> 4224.66s]  which is just a concatenation of five word vectors.
[4224.66s -> 4226.78s]  So if I have a hundred dimensional word vectors,
[4226.78s -> 4228.66s]  this is 500 dimensional.
[4228.66s -> 4232.58s]  And then I'm gonna put it through a neural network layer,
[4232.58s -> 4236.90s]  which is simply multiplying that vector by a matrix
[4236.90s -> 4240.16s]  and adding on a bias vector.
[4240.16s -> 4243.60s]  And then I'm gonna put it through some non-linearity,
[4243.60s -> 4245.00s]  which might be, for example,
[4245.00s -> 4247.64s]  a logistic function that we've already seen.
[4247.64s -> 4250.84s]  So that'll give me a new representation.
[4250.84s -> 4255.84s]  And in particular, if the W is say eight by 500,
[4258.04s -> 4260.68s]  I'll be reducing it to a much,
[4260.68s -> 4262.60s]  oh, what a, yeah, eight by 500,
[4262.60s -> 4266.48s]  I'll be reducing it to a much smaller vector, right?
[4266.48s -> 4268.88s]  So then I can do after that,
[4268.88s -> 4272.72s]  I can multiply my hidden representation,
[4272.72s -> 4275.92s]  the middle of my neural network by another vector.
[4275.92s -> 4277.80s]  And that will give me a score.
[4277.80s -> 4281.00s]  And I'm gonna put the score into the logistic function
[4281.00s -> 4283.50s]  that we saw earlier to say, what's the probability,
[4283.50s -> 4285.20s]  this is the location.
[4285.20s -> 4287.90s]  So at this point,
[4287.90s -> 4292.44s]  my classifier is going to be a linear classifier
[4292.44s -> 4295.08s]  in terms of this internal representation
[4295.08s -> 4296.84s]  that's used right at the end,
[4296.88s -> 4299.56s]  but it's going to be a non-linear classifier
[4299.56s -> 4301.60s]  in terms of my word vectors.
[4306.16s -> 4307.00s]  Okay.
[4309.84s -> 4311.40s]  Great.
[4311.40s -> 4313.68s]  Here's one other thing.
[4313.68s -> 4318.68s]  This is just sort of a note for learner head,
[4318.72s -> 4319.96s]  since you wanna know this
[4319.96s -> 4322.36s]  when we start doing the next assignments.
[4322.36s -> 4325.04s]  I mean, up until now I've presented everything
[4325.04s -> 4330.04s]  as doing log likelihood and negative log likelihood
[4330.58s -> 4332.96s]  for building our models.
[4332.96s -> 4335.90s]  Very soon now, assignment two,
[4335.90s -> 4339.96s]  we're gonna be starting to do things with PyTorch.
[4339.96s -> 4344.96s]  And when you start working out your losses with PyTorch,
[4346.28s -> 4350.56s]  what you're gonna be wanting to use is cross-entropy loss.
[4350.56s -> 4354.86s]  And so let me quickly say what cross-entropy loss is.
[4355.70s -> 4358.38s]  So cross-entropy is from information theory.
[4358.38s -> 4362.58s]  So if you have a true probability distribution P
[4362.58s -> 4366.32s]  and you're computing a probability distribution Q,
[4366.32s -> 4369.98s]  your cross-entropy loss is like this.
[4369.98s -> 4374.98s]  So it's the log of your model probability,
[4376.78s -> 4378.42s]  the expectation of that
[4378.42s -> 4381.94s]  under your true probability distribution.
[4381.94s -> 4384.00s]  But there's sort of a special case,
[4384.00s -> 4389.00s]  whereas if you have ground truth or gold or target data
[4389.56s -> 4393.02s]  where things are labeled one, zero,
[4393.02s -> 4398.02s]  so like for examples of I love Paris when warm, right?
[4401.36s -> 4404.36s]  I'm just labeling it one for location,
[4404.36s -> 4406.20s]  probability one, it's the location,
[4406.20s -> 4409.36s]  probability zero, it's not a location.
[4409.36s -> 4412.06s]  So if you're just labeling the right class
[4412.06s -> 4415.86s]  as probability one, then in this summation,
[4415.86s -> 4419.30s]  every other term goes to zero.
[4419.30s -> 4421.40s]  And the only thing you're left with
[4421.40s -> 4426.40s]  is what log probability is my model
[4427.62s -> 4430.34s]  giving to the right class?
[4430.34s -> 4433.98s]  And so that then is your log likelihood,
[4433.98s -> 4438.46s]  which we can use for the negative log likelihood.
[4439.38s -> 4442.70s]  A little bit of a complication here.
[4442.70s -> 4445.94s]  Just remember that you wanna use cross-entropy loss
[4445.94s -> 4448.42s]  in PyTorch when building the model.
[4448.42s -> 4451.76s]  Okay, before we end today,
[4451.76s -> 4456.34s]  here is my obligatory one picture of human neurons.
[4456.34s -> 4457.18s]  Don't miss it,
[4457.18s -> 4460.20s]  because I'm not gonna show any more of these.
[4460.20s -> 4462.98s]  Okay, these are human neurons, right?
[4462.98s -> 4467.98s]  Human neurons were the inspiration for neurons
[4468.46s -> 4469.82s]  in neural networks, right?
[4469.82s -> 4473.98s]  So human neurons have a single output,
[4473.98s -> 4476.94s]  which comes down this axon.
[4476.94s -> 4481.94s]  And then when you have these outputs,
[4483.66s -> 4488.02s]  they then feed into other neurons.
[4488.02s -> 4490.22s]  I guess I don't really have an example here,
[4490.22s -> 4492.72s]  but in general, one output can feed
[4492.72s -> 4494.32s]  into multiple different neurons.
[4494.32s -> 4496.62s]  You can see the different things hanging into it.
[4496.62s -> 4499.90s]  So you should have the output connecting to the input
[4499.90s -> 4502.82s]  and sort of where you make this connection, right?
[4502.82s -> 4505.78s]  That's the synapses that people talk about.
[4505.78s -> 4510.50s]  And so one neuron will normally have many, many inputs
[4510.50s -> 4513.22s]  where it picks things up from other neurons
[4513.22s -> 4517.04s]  and they all go into the nucleus of the cell
[4517.04s -> 4521.34s]  and the nucleus combines together all those inputs.
[4521.34s -> 4523.02s]  And kind of what happens is
[4523.02s -> 4526.42s]  if there's enough positive activation
[4527.10s -> 4529.82s]  from all of these inputs,
[4529.82s -> 4533.62s]  it then sends signals down as output.
[4533.62s -> 4538.62s]  Now, strictly how neurons work is that they send spikes.
[4539.06s -> 4541.86s]  So the level of activations in neuron
[4541.86s -> 4544.50s]  is its rate of spiking.
[4544.50s -> 4546.22s]  But that immediately got turned
[4546.22s -> 4550.44s]  in artificial neural networks into just a real value
[4550.44s -> 4553.76s]  as to what is this level of activation.
[4553.76s -> 4555.18s]  And so it does this.
[4555.18s -> 4558.54s]  So this was kind of the genuine inspiration
[4558.54s -> 4561.02s]  of all of our neural networks, right?
[4561.02s -> 4563.54s]  So a binary logistic regression
[4563.54s -> 4566.70s]  is kind of a bit similar to a neuron, right?
[4566.70s -> 4569.34s]  It has multiple inputs.
[4569.34s -> 4574.34s]  You're working out your total level of excitation,
[4574.42s -> 4577.50s]  where in particular, you can have inputs
[4577.50s -> 4580.50s]  that are both exciting positive inputs
[4580.50s -> 4584.38s]  and inputs that are negative,
[4584.64s -> 4586.46s]  which are then inhibitory inputs.
[4586.46s -> 4589.70s]  You combine them all together and you get an output
[4589.70s -> 4593.38s]  that's your level of excitation.
[4593.38s -> 4595.30s]  And you're then sort of converting that
[4595.30s -> 4597.34s]  through some non-linearity.
[4597.34s -> 4599.64s]  And so this was proposed
[4599.64s -> 4602.86s]  as a very simple model of human neurons.
[4602.86s -> 4606.26s]  Now, human neurons are way more complex than this.
[4606.26s -> 4608.98s]  And some people like neuroscientists
[4608.98s -> 4611.74s]  think we maybe should be doing a better model
[4611.74s -> 4614.18s]  of actual human neurons.
[4614.90s -> 4617.38s]  But in terms of what's being done
[4617.38s -> 4620.78s]  in the current neural networks eat the world revolution,
[4620.78s -> 4622.66s]  everyone's forgotten about that.
[4622.66s -> 4626.86s]  And it's just sticking with this very, very simple model
[4626.86s -> 4630.66s]  which conveniently turns into linear algebra
[4630.66s -> 4633.06s]  in a very simple way.
[4633.06s -> 4638.06s]  So this gives us sort of like a single neuron,
[4638.14s -> 4639.82s]  but then precise, right?
[4639.82s -> 4643.26s]  So this is which this single neuron,
[4643.30s -> 4644.98s]  if you use the logistic function
[4644.98s -> 4647.54s]  as identical to logistic regression,
[4647.54s -> 4649.70s]  which you've probably seen in some stats class
[4649.70s -> 4650.74s]  or something.
[4650.74s -> 4653.70s]  But the difference is that for neural networks,
[4653.70s -> 4657.00s]  we don't just have one logistic regression.
[4657.00s -> 4662.00s]  We have a bunch of logistic regressions at once.
[4662.38s -> 4665.06s]  And well, that'd be tricky if we had to define
[4665.06s -> 4669.06s]  what each of these logistic regressions was calculating.
[4669.06s -> 4670.62s]  But what we don't,
[4670.62s -> 4673.60s]  what we do is we just feed them
[4673.60s -> 4676.84s]  into another logistic regression.
[4676.84s -> 4681.00s]  And so we have some eventual output
[4681.00s -> 4684.86s]  that we want to be something like we want it to say,
[4684.86s -> 4687.22s]  this is or isn't a location.
[4687.22s -> 4691.80s]  But then what will happen is by our machine learning,
[4691.80s -> 4694.70s]  these intermediate logistic regressions
[4694.70s -> 4698.34s]  will figure out all by themselves something useful to do.
[4698.34s -> 4699.48s]  That's the magic.
[4700.46s -> 4705.46s]  Right, so that you get this sort of self-learning property
[4705.50s -> 4708.42s]  where the model has a lot of parameters
[4708.42s -> 4711.96s]  and internally will work out useful things to do.
[4711.96s -> 4714.28s]  So in general, we can get more magic
[4714.28s -> 4717.12s]  by having more layers in the neural network
[4717.12s -> 4720.64s]  and that we will build up functions.
[4720.64s -> 4723.88s]  So effectively, these intermediate layers
[4723.88s -> 4728.68s]  let us learn a model that re-represents the input data
[4728.68s -> 4732.20s]  in ways that will make it easier to classify
[4732.20s -> 4734.04s]  or easier to interpret
[4734.04s -> 4738.08s]  and do things with downstream in our neural network.
[4740.26s -> 4743.18s]  And it's time, so I should stop there.
[4743.18s -> 4744.02s]  Thank you.
