# Detected language: en (p=1.00)

[0.00s -> 11.86s]  Okay, welcome everyone to week four we're into now.
[11.86s -> 19.16s]  So for today, what I want to do is first of all, well, a couple more bits on machine
[19.16s -> 24.12s]  translation, especially just talking a little bit about evaluating machine translation,
[24.12s -> 27.16s]  that I want to spend a while on attention.
[27.16s -> 33.92s]  So attention is a very fundamental concept of neural networks,
[33.92s -> 37.72s]  which was originally developed in the context of machine translation.
[37.72s -> 43.28s]  There's also then a very central concept when we're talking about transformers,
[43.28s -> 46.60s]  which we then start talking about on Thursday.
[46.60s -> 51.28s]  Okay. So getting straight into it.
[51.28s -> 56.68s]  So this is the picture that we saw towards the end of last time.
[56.68s -> 60.76s]  This is how we were baking machine translation system,
[60.76s -> 64.76s]  where we are using a multi-layer LSTM,
[64.76s -> 68.44s]  where we're feeding in a source sentence,
[68.44s -> 76.02s]  and then we're flipping to then turning the model into a decoder with different parameters,
[76.02s -> 81.56s]  which would generate one word at a time to generate the translated sentence.
[81.72s -> 86.28s]  So, here I've got a German sentence and it's
[86.28s -> 90.60s]  produced an English translation that looks a pretty good one.
[90.60s -> 95.40s]  But we're going to want to have a way of deciding,
[95.40s -> 99.72s]  well, are we producing good translations or not?
[99.72s -> 104.24s]  So we need some way to evaluate machine translation.
[104.24s -> 110.88s]  Now, this is a complex area because if you start poking around in the literature,
[111.24s -> 115.44s]  people have proposed literally hundreds of
[115.44s -> 119.44s]  different measures that could be used to evaluate machine translation systems.
[119.44s -> 122.68s]  I'm guilty of writing a couple of papers on it myself,
[122.68s -> 125.16s]  so I'm contributed to the problem.
[125.16s -> 132.18s]  But by far the most commonly common measure that you see to this day was
[132.18s -> 137.54s]  essentially the first measure proposed to automatically evaluate machine translation,
[137.54s -> 139.60s]  which was the blue measure,
[139.72s -> 145.70s]  which was proposed to understand for bilingual evaluation under study,
[145.70s -> 149.28s]  though it went along with the fact that it was proposed by IBM.
[149.28s -> 151.28s]  Probably not a coincidence.
[151.28s -> 154.34s]  So, until this point,
[154.34s -> 159.80s]  the only way that people would really use for evaluating translations was getting
[159.80s -> 164.40s]  human beings to look at them and say how good of a translation this is.
[164.56s -> 170.56s]  And that's still a gold standard measure that is widely used for
[170.56s -> 175.84s]  evaluating translations because many of the automatic measures have various kinds
[175.84s -> 181.80s]  of biases and problems that make human evaluation useful.
[182.80s -> 186.36s]  But on the other hand, a lot of the time,
[186.36s -> 190.40s]  we'd like to iterate quickly on evaluations.
[190.40s -> 194.48s]  We'd like to use evaluations and training loops and things like that.
[194.48s -> 198.48s]  And the IBM people with the blue paper suggests, well,
[198.48s -> 204.88s]  maybe we can come up with a halfway decent automatic method of doing translations.
[204.88s -> 209.56s]  And the idea of what they proposed was this, that we're going to have one or
[209.56s -> 213.48s]  more reference translations for a piece of text.
[213.48s -> 216.40s]  So, these are human written translations.
[216.40s -> 222.84s]  And then we can score any automatic translation mainly on
[222.84s -> 229.00s]  how often they have overlapping one, two, three, and four grams.
[230.00s -> 231.60s]  The number four isn't special.
[231.60s -> 233.60s]  You could have only gone up to three or five, but
[233.60s -> 236.32s]  four was seen as a reasonable length.
[236.32s -> 241.28s]  Overlapping n-grams with one of the reference translations.
[241.28s -> 243.56s]  And the more overlap you have, the better.
[243.56s -> 250.16s]  And we, we, this discussion of this evaluation in the assignment,
[250.16s -> 251.92s]  so you can think about it a bit more, and
[251.92s -> 256.00s]  I won't go actually through all the formulas right now.
[256.00s -> 258.12s]  But you know, that's most of it.
[258.12s -> 262.00s]  And so here's a picture of how that looks.
[262.00s -> 266.80s]  So the original idea was, what we should do is, you know,
[266.80s -> 272.20s]  have several reference translations, and then we'd get a machine translation.
[272.20s -> 275.68s]  And then we'd look at this machine translation and
[275.68s -> 279.80s]  try and find pieces of it in the reference translation.
[279.80s -> 283.24s]  So we can certainly find the unigram, the.
[283.24s -> 289.64s]  We can't find American at all, but we can find International Airport and
[289.64s -> 292.52s]  It's in the second reference translation.
[292.52s -> 295.52s]  So we're gonna get a four gram match for that.
[295.52s -> 298.12s]  We can find the again, that's easy.
[298.12s -> 301.80s]  Office All receives one's call Self the Sand Arab.
[301.80s -> 303.80s]  Not a very good translation this, right?
[303.80s -> 309.52s]  So that all misses, but then you start to find other pieces that do overlap.
[309.52s -> 312.12s]  And you use those to work out a score.
[312.12s -> 317.60s]  The original idea was you should always have multiple reference translations so
[317.60s -> 322.48s]  that you can sample the space of possible translations and
[322.48s -> 324.28s]  have reasonable coverage.
[324.28s -> 327.88s]  In practice for what's been done more recently, it's not so
[327.88s -> 331.92s]  uncommon that people do this with only one reference translation.
[331.92s -> 336.36s]  And the argument then is still on a kind of a probabilistic basis.
[336.36s -> 338.40s]  The more often you have a good translation,
[338.40s -> 342.68s]  the more often you'll get matches, and therefore your score will be better.
[344.24s -> 350.12s]  Yeah, so why, you know,
[350.12s -> 355.76s]  why did people come up with this and why is it still imperfect?
[355.76s -> 360.64s]  Well, the problem with translation is that there isn't one right answer.
[360.64s -> 364.56s]  It's not like the kind of classification things you see in machine learning,
[364.56s -> 368.80s]  where you show people a picture and the right answer is to say this,
[368.80s -> 374.24s]  the class of this object is whatever, a labradoodle, right?
[374.24s -> 376.36s]  Dog breeds or something, right?
[376.36s -> 380.60s]  That for any sentence, there are many different ways to translate it.
[380.60s -> 383.12s]  And, you know, translators can sit around and
[383.16s -> 386.32s]  argue that, oh, this phrasing's a little bit nicer than this phrasing,
[386.32s -> 387.36s]  blah, blah, blah, blah.
[387.36s -> 391.68s]  But to a first approximation, you would translate a sentence in lots of ways.
[391.68s -> 396.28s]  And those different ways of translation can involve different word orders.
[396.28s -> 400.80s]  So you can't really sort of check the words off as you come down the sentence.
[400.80s -> 405.72s]  And that's what motivated this idea of sort of matching n-grams anywhere.
[405.72s -> 411.26s]  So you can get reasonable credit for having the right matches.
[411.26s -> 416.66s]  But, you know, nevertheless, it's a pretty crude version of it, right?
[417.74s -> 420.54s]  You know, you can still get a poor blue score for
[420.54s -> 423.94s]  a good translation just because the words you chose didn't
[423.94s -> 426.62s]  happen to match a reference translation.
[426.62s -> 429.06s]  And also, you can get points for
[429.06s -> 432.58s]  things without really having a good translation at all, right?
[432.58s -> 436.86s]  If you just have words that match, even if they're having completely
[436.86s -> 440.98s]  the wrong role in the sentence, you will get some points.
[440.98s -> 444.46s]  But it's harder to get n-gram matches for
[444.46s -> 448.90s]  larger n unless you're using words the right way.
[448.90s -> 452.62s]  There's one other trick in the blue measure that there's a penalty for
[452.62s -> 455.22s]  two short system translations.
[455.22s -> 458.70s]  Cuz otherwise, you could leave out everything difficult and
[458.70s -> 461.62s]  only translate the easy part of the sentence.
[461.62s -> 465.66s]  And then for the bits you have translated, you could then be getting
[465.66s -> 469.02s]  a high score for the precision of those pieces.
[469.98s -> 474.58s]  Okay, so we'll use, when you're developing your MT systems for
[474.58s -> 477.78s]  assignment three, we'll use them with blue.
[479.34s -> 483.66s]  So now we have an evaluation measure.
[483.66s -> 493.06s]  We can start looking at how well the systems do on a blue score.
[493.06s -> 497.02s]  And blue scores are theoretically between 0 and 100, but
[497.02s -> 500.90s]  you're never gonna get to 100 because of the variations of how you can translate
[500.90s -> 502.38s]  things.
[502.38s -> 508.98s]  And so typically, if you can start to get to the 20s, the translations,
[508.98s -> 513.10s]  you can sort of understand what the source document was about.
[513.10s -> 515.54s]  Once you get into the 30s and 40s,
[515.54s -> 517.78s]  the translations are getting much, much better.
[517.86s -> 524.14s]  Yeah, so statistical phrase-based translation was pioneered by IBM in
[524.14s -> 532.10s]  the late 90s actually, and was sort of redeveloped in the 2000s decade.
[532.10s -> 536.86s]  And it was what Google launched as Google Translate in the 2000s decade.
[536.86s -> 541.06s]  And it continued to be worked on for sort of the following decade.
[541.06s -> 547.38s]  But there was basically a strong sense that progress in translation
[548.18s -> 552.50s]  during statistical phrase-based systems had basically stalled.
[552.50s -> 557.18s]  That it got a little bit better each year as people could build traditional
[557.18s -> 561.22s]  N-gram language models with more data every year and things like that.
[561.22s -> 565.34s]  But the numbers were barely going upwards.
[565.34s -> 571.90s]  So in the years from about 2005 to 15 or
[571.90s -> 576.66s]  maybe 14, the dominant idea in the machine translation community
[577.22s -> 581.14s]  was the way we were gonna get better machine translation
[581.14s -> 584.18s]  is doing syntax-based machine translation.
[584.18s -> 588.66s]  If we actually knew the structure of sentences and we'd pass them up,
[588.66s -> 592.02s]  then we'd know what the role of words was in sentences, and
[592.02s -> 595.02s]  then we'd be able to translate much better.
[595.02s -> 599.78s]  And this was particularly invoked by looking at languages where translation
[599.78s -> 601.10s]  worked terribly.
[601.10s -> 606.02s]  So in those days, translation worked sort of okay for
[606.06s -> 611.22s]  languages like French to English or Spanish to English,
[611.22s -> 614.98s]  which are kind of sort of similar European languages.
[614.98s -> 621.34s]  But the results worked way worse for Chinese to English or German to English.
[621.34s -> 625.62s]  And even though English is a Germanic language, German has a very different
[625.62s -> 631.26s]  word order to English with commonly verbs at the end of a clause and
[631.26s -> 633.06s]  different elements being fronted.
[633.06s -> 640.58s]  And so people tried to work on grammar-based,
[640.58s -> 643.90s]  syntax-based methods of statistical machine translation.
[643.90s -> 648.22s]  And I was one of those who worked on those in the late 2000s decade.
[648.22s -> 651.98s]  But the truth is it sort of didn't really work, right?
[651.98s -> 657.74s]  If the rate of progress in syntax-based
[657.74s -> 661.78s]  machine translation had slightly more slope
[661.78s -> 665.22s]  than phrase-based machine translation over these years,
[665.22s -> 669.22s]  the amount of slope wasn't very much.
[669.22s -> 673.70s]  So things were completely then thrown on the head when neural machine
[673.70s -> 675.66s]  translation got invented.
[675.66s -> 680.82s]  Cuz as I explained, the first attempts were in 2014.
[680.82s -> 686.42s]  The first cases in which it was evaluated in bake-off evaluations was 2015.
[686.42s -> 692.42s]  And so in 2015, it wasn't as good as the best other machine translation methods.
[692.42s -> 694.94s]  But by 2016, it was.
[694.94s -> 699.30s]  And it was just on this much, much steeper slope of getting way, way better.
[699.30s -> 704.34s]  And this graph only goes up to 2019, but it's continued to go up.
[704.34s -> 709.14s]  And so it's not that uncommon these days that you see blue numbers in
[709.14s -> 713.30s]  the 50s and 60s for neural machine translation systems.
[714.30s -> 717.34s]  So that's a good news story.
[717.34s -> 723.74s]  So after this, I wanna go on and sort of introduce this idea of attention,
[723.74s -> 729.74s]  which is now a very fundamental important idea in neural systems.
[729.74s -> 734.02s]  It's also interesting because it's actually something
[734.02s -> 737.14s]  novel that was invented kinda recently.
[737.14s -> 742.74s]  So for everything that we've done in neural networks up until now,
[743.22s -> 747.38s]  really it had all been invented before the turn of the millennium, right?
[747.38s -> 754.46s]  So basic feed forward neural networks, recurrent neural networks, LSTMs,
[754.46s -> 758.82s]  other things that we haven't talked about like convolutional neural networks.
[758.82s -> 761.78s]  They were all invented last millennium.
[761.78s -> 767.54s]  It was really a waiting game at that point until there was sufficient data and
[767.54s -> 772.34s]  computational power for them really to show how good they were.
[772.34s -> 778.42s]  But attention was something that actually got invented in 2014
[778.42s -> 781.78s]  in the origins of neural machine translation.
[781.78s -> 784.94s]  And it proved to be a very transformative idea for
[784.94s -> 787.02s]  making neural networks more powerful.
[789.10s -> 794.26s]  So the idea of what motivated attention was looking at exactly this kind of
[794.26s -> 796.22s]  machine translation problem.
[796.22s -> 799.90s]  So we're running our LSTM over the source sentence and
[799.90s -> 804.30s]  then we were using this hidden state as the previous hidden state
[804.30s -> 810.70s]  that we're feeding into the generator LSTM for the target sentence.
[810.70s -> 816.14s]  And what that means is everything useful about this sentence
[816.14s -> 818.94s]  has to be stuffed into that one vector.
[818.94s -> 822.90s]  And well, that's maybe not so hard if you've got a four word sentence.
[822.90s -> 826.90s]  But maybe you've got a 40 word sentence out here and
[826.90s -> 832.14s]  it seems to be kind of implausible that it'd be a good idea to be trying to
[832.14s -> 836.10s]  fit everything about that sentence into this one hidden state.
[836.10s -> 838.30s]  And well, obviously there are crude solutions to this.
[838.30s -> 840.06s]  You make the hidden states bigger and
[840.06s -> 842.30s]  then you've got more representational space.
[842.30s -> 847.86s]  You use a multi-layer LSTM, you've got more representational space.
[847.86s -> 852.38s]  But it still seems a very questionable thing to do.
[852.38s -> 855.90s]  And it's certainly not like what a human being does, right?
[855.90s -> 860.62s]  Like if a human being is translating a sentence, they read the sentence and
[860.62s -> 862.58s]  they've got some idea of its meaning.
[862.58s -> 865.70s]  But as they start to translate, they look back at the earlier parts of
[865.70s -> 869.40s]  the sentence and make use of that in their translation.
[869.40s -> 874.26s]  And so that doesn't seem like it's a very plausible model.
[874.26s -> 879.34s]  So the idea should be that our neural network should be able to attend
[879.38s -> 882.26s]  to different things in the source so
[882.26s -> 887.70s]  that they can get information as needed looking back in the sentence.
[887.70s -> 890.42s]  And so this is the idea of attention.
[890.42s -> 894.86s]  So on each step of the decoder, we're going to insert
[894.86s -> 899.22s]  direct connections to the encoder so we can look at particular words in
[899.22s -> 900.14s]  the sentence.
[901.74s -> 905.74s]  So I've got a bunch of diagram sentences that go through what we do.
[905.74s -> 910.30s]  And then after that, I'll present the equations that go along with this.
[910.30s -> 914.50s]  Okay, so once we're starting to translate,
[914.50s -> 919.22s]  we've got a hidden state at the start of our generator.
[919.22s -> 924.62s]  And then we're going to use this hidden state as our key
[924.62s -> 929.74s]  to look back into the encoder to try and find useful stuff.
[929.78s -> 935.98s]  So we're going to compare, in a way I'll make precise later,
[935.98s -> 944.70s]  the hidden state with the hidden state at every position in the source sentence.
[944.70s -> 951.38s]  And based on our comparisons, we're going to work out an attention score.
[951.38s -> 956.82s]  Where should we be looking at in the source sentence while generating
[956.82s -> 959.58s]  here the first word of the translation?
[960.46s -> 965.62s]  And so based on these attention scores, we'll stick them into a softmax,
[965.62s -> 970.70s]  as we commonly do, and we'll then get a probability distribution or
[970.70s -> 974.34s]  a weighting over the different positions in the sentence.
[974.34s -> 980.06s]  And then we will use this weighting to compute a representation
[980.06s -> 985.50s]  based on the encoder, which is then going to be
[986.26s -> 989.66s]  average of the encoder states.
[989.66s -> 993.26s]  So in this particular case, it'll be nearly entirely
[993.26s -> 998.10s]  the representation above the first word, eel, which means he in French.
[999.22s -> 1005.10s]  So then we'll take that attention output and
[1005.10s -> 1011.06s]  we'll combine it with the hidden state of our decoder.
[1011.10s -> 1016.42s]  And we'll use both of them together to generate an output vector,
[1016.42s -> 1019.38s]  which we stick through our softmax and
[1019.38s -> 1024.46s]  generate a word as the first word of the translation, y1.
[1024.46s -> 1027.26s]  And so then at that point, we just repeat this over.
[1030.26s -> 1033.02s]  So we then go on to generating the second word.
[1034.34s -> 1037.06s]  Well, we copy down the first word generator,
[1037.06s -> 1039.30s]  start to generate the second word.
[1039.30s -> 1042.74s]  We work out attention at every position.
[1042.74s -> 1048.54s]  It gives us, I'm sorry, there's a little note there,
[1048.54s -> 1052.14s]  which is a little fine point, which maybe I won't deal with.
[1052.14s -> 1057.58s]  But it points out sometimes you also do things like stick the previous
[1057.58s -> 1062.30s]  time steps attention output into the next step as an extra input.
[1062.30s -> 1066.18s]  And we actually do that in, it should say assignment three there,
[1066.18s -> 1067.86s]  that's buggy.
[1067.86s -> 1071.74s]  So there are other ways to use things, but I'll sort of gloss over that.
[1071.74s -> 1077.50s]  So we generate another word and we sort of repeat over.
[1077.50s -> 1082.78s]  And at each time step, we're looking at different words in the source, and
[1082.78s -> 1085.34s]  they will help us to translate the sentence.
[1086.74s -> 1087.26s]  Yeah?
[1087.26s -> 1092.82s]  So quick question, why does the start word point to the attention score?
[1092.82s -> 1093.62s]  Why is it not?
[1098.62s -> 1100.34s]  Wait, wait, say it again.
[1100.34s -> 1102.74s]  Why is the green part pointing to the-
[1102.74s -> 1104.90s]  Because the green, okay.
[1104.90s -> 1109.82s]  So the green vector, the hidden vector of the decoder,
[1109.82s -> 1114.26s]  is going to be used together with the hidden states.
[1114.26s -> 1120.26s]  The hidden vectors of the encoder one at a time to calculate the attention scores.
[1120.26s -> 1125.62s]  So the attention score at a position is going to be a function of
[1125.62s -> 1130.06s]  the hidden state of the decoder at that position and
[1130.06s -> 1132.74s]  the current hidden state of the decoder.
[1135.46s -> 1138.46s]  And I'll explain exactly how in a moment.
[1139.78s -> 1141.06s]  Any other questions?
[1141.06s -> 1149.70s]  Okay, well, so here it is in math.
[1149.70s -> 1155.54s]  Okay, so we have encoder hidden states which we're gonna call h.
[1157.86s -> 1162.18s]  So we have decoder hidden states which we're gonna call s, so
[1162.18s -> 1163.46s]  they're something different.
[1164.62s -> 1169.30s]  And we go into at each point being some particular time step t.
[1169.34s -> 1171.38s]  So we'll be dealing with s t.
[1172.94s -> 1178.98s]  So to calculate the attention scores for
[1178.98s -> 1184.62s]  generating the word for time step t,
[1184.62s -> 1189.22s]  we're going to calculate an attention score for
[1189.22s -> 1192.38s]  each position in the encoder.
[1192.38s -> 1196.38s]  Okay, I'll discuss alternatives for this in a moment.
[1196.42s -> 1201.82s]  But the very easiest way to calculate an attention score which is shown here
[1201.82s -> 1207.14s]  is to take a dot product between the hidden state
[1207.14s -> 1212.50s]  of the encoder and the current hidden state of the decoder.
[1212.50s -> 1215.14s]  And so that's what we're showing here.
[1215.14s -> 1219.58s]  So that will give us some dot product score which is just any number at all.
[1220.58s -> 1226.38s]  Then the next thing we do is we stick those ET scores into our softmax
[1226.38s -> 1232.02s]  distribution and then that gives us our probability distribution
[1232.02s -> 1237.62s]  as to how much weight to put on each position in the encoder.
[1237.62s -> 1242.14s]  And so then we are calculating the weighted average of the encoder hidden
[1242.14s -> 1247.34s]  states which we're just doing with the obvious equation
[1247.34s -> 1251.82s]  that we're taking the weighted sum of the hidden states of the encoder
[1251.82s -> 1253.98s]  based on the attention weights.
[1253.98s -> 1259.86s]  And then what we want to do is concatenate our
[1259.86s -> 1265.62s]  attention output and the hidden state of the decoder.
[1265.62s -> 1269.14s]  And we're going to, which is giving us then a double length vector and
[1269.14s -> 1275.50s]  then we're going to feed that into producing the next word from the decoder.
[1275.50s -> 1280.50s]  So typically that means we're multiplying that vector by another matrix.
[1280.50s -> 1286.14s]  And then putting it through a softmax to get a probability
[1286.14s -> 1292.42s]  distribution over words to output and choosing the highest probability word.
[1295.50s -> 1299.70s]  Okay, that makes sense, I hope.
[1299.70s -> 1303.38s]  Yeah, okay, so attention is great.
[1303.38s -> 1309.42s]  So inventing this idea was completely transformative.
[1309.42s -> 1314.34s]  So the very first modern neural machine translation system was done at Google
[1314.34s -> 1321.30s]  in 2014 and they used a pure but very large,
[1321.30s -> 1326.58s]  very deep LSTM.
[1326.58s -> 1331.02s]  So it was an eight layer deep LSTM with a very large hidden state for
[1331.02s -> 1335.74s]  the time, and they were able to get good results.
[1335.74s -> 1342.86s]  But very shortly thereafter, people at the University of Montreal,
[1342.86s -> 1347.50s]  Dima Badenow, Keihin Cho, and Yoshua Bengio did a second
[1347.50s -> 1353.02s]  version of machine translation using attention.
[1353.02s -> 1357.26s]  And with a much more modest compute budget of the kind that you can afford in
[1357.26s -> 1362.22s]  universities, they were able to get better results because attention was their
[1362.22s -> 1364.42s]  secret thing.
[1364.42s -> 1370.06s]  So attention significantly improved NMT performance and essentially every
[1370.06s -> 1376.54s]  neural machine translation system since has used attention like we've just seen.
[1376.54s -> 1379.70s]  It's more human-like as I was indicating,
[1379.70s -> 1381.42s]  because it's sort of what a human would do.
[1381.42s -> 1384.74s]  You'd look back in the sentence to see what you need to translate.
[1385.18s -> 1387.30s]  And it solves this bottleneck problem.
[1387.30s -> 1391.90s]  You now no longer have to stuff all the information about the source sentence
[1391.90s -> 1394.06s]  into one hidden state.
[1394.06s -> 1398.38s]  You can have the whole of your representational space from your entire
[1398.38s -> 1401.82s]  encoding and use it as you need it.
[1401.82s -> 1404.38s]  It also helps with the vanishing gradient problem.
[1404.38s -> 1408.78s]  This is connected to what I was saying last time when talking about residual
[1408.78s -> 1413.30s]  connections, that a way out of the vanishing gradient problem is to direct
[1413.34s -> 1417.94s]  connect things and this provides shortcut connections to all of the hidden
[1417.94s -> 1419.22s]  states of the encoder.
[1420.58s -> 1426.78s]  And another nice thing that attention does is it gives you some interpretability.
[1426.78s -> 1430.50s]  So by looking at where the model is attending,
[1430.50s -> 1435.26s]  you can basically see what it's translating at different time steps.
[1435.26s -> 1438.94s]  And so that can be really useful.
[1438.94s -> 1443.86s]  And so it's kind of like we can see what we're translating
[1443.86s -> 1447.66s]  where without explicitly having trained a system that does that.
[1447.66s -> 1452.22s]  So for my little toy sentence here, if he hit me with a pie,
[1452.22s -> 1457.58s]  at the first position, it was looking at the first word,
[1457.58s -> 1460.74s]  il, he, which it translates.
[1460.74s -> 1464.98s]  Then there's, in French, there's this sort of verb entartes,
[1464.98s -> 1466.38s]  a sort of pie somebody.
[1466.42s -> 1469.70s]  I guess in English as well, you can use pie as a verb, right?
[1469.70s -> 1477.42s]  So the a is a sort of perfect past auxiliary.
[1477.42s -> 1481.98s]  So it's sort of like he has me pied, is what the French words are one
[1481.98s -> 1483.02s]  at a time.
[1483.02s -> 1486.66s]  And so the hit is already looking at the pied,
[1486.66s -> 1490.50s]  then the me is attending to the me, which means me.
[1490.50s -> 1494.62s]  And then all with the pie is attending still to entartes,
[1494.62s -> 1498.22s]  which is basically the right kind of alignment that you want for
[1498.22s -> 1499.78s]  words of a sentence.
[1499.78s -> 1502.18s]  So that's pretty cool too.
[1504.30s -> 1510.10s]  Okay, so I presented up until this point just this,
[1512.90s -> 1515.66s]  said, oh, we could do a dot product.
[1515.66s -> 1518.58s]  But in general, there's more to it than that.
[1518.58s -> 1523.46s]  So what we have is we have some values h1 to hn.
[1523.46s -> 1525.86s]  And we have a query vector.
[1525.86s -> 1530.74s]  And we wanna work out how to do attention based on these things.
[1530.74s -> 1536.38s]  So attention always involves computing some attention scores and
[1536.38s -> 1540.06s]  taking the softmax to get an attention distribution and
[1540.06s -> 1542.86s]  then getting an attention output.
[1542.86s -> 1548.26s]  But the part where there's variation is how do you compute these attention scores?
[1548.26s -> 1551.34s]  And a number of different ways have been done for that.
[1551.34s -> 1554.42s]  And I just wanna go through that a little bit.
[1556.42s -> 1561.94s]  So the simplest way that I just presented is this dot product attention.
[1561.94s -> 1565.30s]  We just take the hidden states and dot product the whole of them.
[1566.50s -> 1573.02s]  And that sort of works, but it doesn't actually work great.
[1573.02s -> 1578.54s]  And I sort of discussed this a bit when talking about LSTMs last time, right?
[1578.54s -> 1585.42s]  That the hidden state of an LSTM is its complete memory, right?
[1585.42s -> 1589.50s]  So it has to variously store lots of things in that memory.
[1589.50s -> 1595.26s]  It's gotta be storing information that'll help it output the right word.
[1595.26s -> 1598.46s]  It has to be storing information about the future,
[1598.46s -> 1604.22s]  about other things that you'll want to say given the kind of sentence context,
[1604.22s -> 1607.02s]  grammar and previous words you've said, right?
[1607.02s -> 1609.42s]  You've sort of got all kinds of memory.
[1609.42s -> 1615.90s]  And so it sort of makes sense that some of it would be useful for
[1615.90s -> 1620.54s]  linking up for looking back, and some of it would be less useful.
[1620.54s -> 1624.18s]  You sort of want to find the parts that are related to what you want to say
[1624.18s -> 1629.10s]  immediately, not all the parts that do all of the rest of the future.
[1630.10s -> 1639.90s]  So that suggested maybe you could do a more general form of attention.
[1639.90s -> 1647.30s]  And so Tung Luong and me in 2015 suggested maybe we could introduce
[1647.30s -> 1652.10s]  what we called bilinear attention, which I still think is a better name.
[1652.10s -> 1656.94s]  But the rest of the world came to call multiplicative attention,
[1656.94s -> 1663.66s]  where what we're doing is between these two vectors, we're sticking a matrix.
[1663.66s -> 1667.10s]  And so we're then learning the parameters of this matrix,
[1667.10s -> 1670.58s]  just like everything else in our neural network.
[1670.58s -> 1675.50s]  And so effectively, this matrix can learn
[1675.50s -> 1681.66s]  which parts of the generator hidden state
[1681.66s -> 1687.82s]  you should be looking to find where in the hidden states of the encoder.
[1687.82s -> 1691.94s]  In particular, it no longer requires that things have to match up dimension by
[1691.94s -> 1693.10s]  dimension.
[1693.10s -> 1697.34s]  It could be the case that the encoder's storing information about word
[1697.34s -> 1702.50s]  meaning here, and the decoder's storing
[1702.50s -> 1704.98s]  information about word meaning here.
[1704.98s -> 1709.10s]  And by learning appropriate parameters in this matrix,
[1709.14s -> 1714.58s]  we can sort of match those together and work out the right place to pay attention.
[1714.58s -> 1721.34s]  So that seemed kind of a cool approach to us.
[1721.34s -> 1721.86s]  Yeah?
[1721.86s -> 1725.66s]  Why don't you all say that you can even build a neural network that's
[1725.66s -> 1729.30s]  going to make HIs and input and outputs?
[1729.30s -> 1730.34s]  You can do that.
[1730.34s -> 1732.98s]  I was going to get to that on the next slide.
[1732.98s -> 1736.30s]  Actually, that's in a way sort of going backwards.
[1736.30s -> 1738.54s]  But I will get to it on the next slide.
[1738.54s -> 1747.90s]  But before I do that, I will show you these other versions.
[1747.90s -> 1755.62s]  So the one thing you might wonder about doing it this way is,
[1755.62s -> 1760.50s]  you know, there's a lot of parameters that you have to learn in the matrix W.
[1760.50s -> 1763.78s]  You know, there aren't that many in my example because there are only 36.
[1763.78s -> 1767.66s]  But that's because my hidden states are only of length 6, right?
[1767.78s -> 1771.38s]  And if your hidden states are of length 1,000 say,
[1771.38s -> 1775.98s]  then you've got a million parameters in that W matrix.
[1775.98s -> 1779.94s]  And that seems like it might be kind of problematic.
[1779.94s -> 1784.46s]  And so the way to get beyond that,
[1784.46s -> 1788.18s]  which was fairly quickly suggested thereafter is, well,
[1788.18s -> 1791.70s]  maybe rather than having that whole big matrix in the middle,
[1791.70s -> 1796.98s]  instead what we could do is format as a low rank matrix.
[1796.98s -> 1801.98s]  And the easy way to make a low rank matrix is you take two skinny matrices
[1801.98s -> 1807.26s]  like this where this is the rank of the pieces and multiply them together,
[1807.26s -> 1811.50s]  which would give us the big matrix that I showed on the last slide.
[1811.50s -> 1816.58s]  And so this gives you a low parameter version of
[1816.58s -> 1821.18s]  the bilinear attention matrix from the last slide.
[1821.18s -> 1826.94s]  But at that point, if you just do a teeny bit of linear algebra,
[1826.94s -> 1831.38s]  this computation is exactly the same as saying, well,
[1831.38s -> 1836.58s]  what I'm gonna do is I'm gonna take each of these two vectors and
[1836.58s -> 1842.82s]  project them to a lower dimensional space using this low rank transformation matrix.
[1842.82s -> 1848.46s]  And then I'm gonna take the dot product in this low dimensional space.
[1848.46s -> 1854.42s]  And on Thursday when you get to transformers,
[1855.34s -> 1859.58s]  what you will see that transformers do is this.
[1859.58s -> 1863.22s]  That they're taking the big vector and
[1863.22s -> 1866.54s]  they're projecting it to a low dimensional space and
[1866.54s -> 1871.42s]  then taking dot product attention in that low dimensional space.
[1872.90s -> 1875.26s]  Okay, back to the question.
[1876.38s -> 1878.22s]  Yeah, you're totally right.
[1878.22s -> 1885.86s]  And at this point, I'm going sort of in an ahistorical manner.
[1885.86s -> 1891.10s]  Because yeah, actually the first form of attention that was proposed in
[1891.10s -> 1896.58s]  the Baden-Aradal paper was, hey,
[1896.58s -> 1901.74s]  let's just stick a little neural net there to calculate attention scores.
[1901.74s -> 1906.54s]  So we take the s and the h,
[1906.54s -> 1911.62s]  we multiply them both by a matrix, add them, put them through a tan h,
[1911.62s -> 1915.42s]  multiply that by a vector, and we get a number.
[1915.42s -> 1920.74s]  This looks just like kind of computations we've used everywhere else in an LSTM.
[1920.74s -> 1925.02s]  So there's a little neural net that's calculating the attention scores, and
[1925.02s -> 1929.50s]  then they go into a softmax as usual.
[1929.50s -> 1932.10s]  In most of the literature, this is called additive attention,
[1932.10s -> 1934.30s]  which also seems to be a really weird name.
[1934.30s -> 1937.70s]  I mean, I think kind of saying you've got a little neural net
[1937.70s -> 1940.02s]  makes more sense for that one.
[1942.02s -> 1947.06s]  But anyway, so this is what they proposed and used.
[1947.06s -> 1954.58s]  And at this point, it's a little bit complex, to be honest.
[1954.58s -> 1962.54s]  I mean, so when we wrote our paper the next year,
[1962.54s -> 1967.46s]  we had found that the bilinear attention worked better for us.
[1967.46s -> 1972.34s]  But there was subsequent work, especially this massive exploration of neural
[1972.34s -> 1977.06s]  machine translation architectures that argued that actually,
[1978.18s -> 1984.38s]  with the right kinds of good hyperparameter optimization,
[1984.38s -> 1989.94s]  that actually this is the best kind, this is better than the bilinear attention.
[1989.94s -> 1993.34s]  But this is a lot more complex and
[1993.34s -> 1998.22s]  a lot slower than doing what you're doing in the upper part of the chart.
[1998.22s -> 2000.78s]  So regardless of whether it's better or not,
[2000.78s -> 2003.98s]  in practice what's completely one is doing this.
[2003.98s -> 2006.22s]  And this is what transformers use and
[2006.22s -> 2009.62s]  just about all other neural nets that are used these days.
[2011.66s -> 2015.30s]  Okay, questions on attention will be found in assignment three.
[2016.02s -> 2021.86s]  Yeah, so I won't say much more about this now,
[2021.86s -> 2025.58s]  and we'll see more of it just next lecture.
[2025.58s -> 2028.94s]  But attention is a very general technique, right?
[2028.94s -> 2033.06s]  It was a great way to improve machine translation, and
[2033.06s -> 2035.34s]  that was how it was first invented.
[2035.34s -> 2039.18s]  But for all kinds of neural architectures, for
[2039.18s -> 2043.78s]  all kinds of purposes, you can stick attention into them.
[2043.78s -> 2047.78s]  And the general finding was that always improved results.
[2047.78s -> 2053.94s]  So in general, anywhere where you have a vector of values, a vector query,
[2053.94s -> 2059.46s]  and you can use attention to then sort of get a weighted average of the values
[2059.46s -> 2064.42s]  which finds relevant information that you can use to improve your performance.
[2066.74s -> 2071.94s]  And so maybe I won't try and even give examples of that now, but
[2071.94s -> 2077.02s]  you'll sort of see another example of attention immediately when we do things
[2077.02s -> 2084.02s]  on Thursday where we then sort of start doing self attention inside transformers.
[2084.02s -> 2084.98s]  Yes?
[2084.98s -> 2090.14s]  In the bilinear case, did you also try non-linearity?
[2090.14s -> 2091.26s]  No, we did not.
[2091.26s -> 2105.82s]  I mean, it didn't seem especially necessary, I don't know.
[2105.82s -> 2107.58s]  But no, we did not.
[2107.58s -> 2114.46s]  Okay, well, this is the end of the part with attention.
[2114.46s -> 2115.82s]  Are there any other questions?
[2117.90s -> 2118.42s]  Yes?
[2119.34s -> 2124.02s]  For the RNN attention stuff, is there a need for
[2124.02s -> 2127.22s]  position information or is that not required to solve the-
[2127.22s -> 2128.78s]  Oh, you said need for-
[2128.78s -> 2130.02s]  Positional information?
[2132.42s -> 2140.82s]  So there was none and it seemed like it wasn't very required.
[2140.82s -> 2147.50s]  I mean, you could, yeah, I mean, you could make some info,
[2147.50s -> 2153.98s]  you could make some argument that maybe position information might have been useful.
[2153.98s -> 2158.78s]  But there's also a good argument that it wasn't necessary and
[2158.78s -> 2163.82s]  the sort of recent everywhere usage of positional
[2163.82s -> 2168.90s]  information only becomes necessary when you get to a transformer.
[2168.90s -> 2173.66s]  And the reason for that is going back to the pictures,
[2174.10s -> 2182.70s]  for these encoder states, they're being calculated with respect to
[2182.70s -> 2187.58s]  the previous encoder state, right, because it's a recurrent neural network.
[2187.58s -> 2192.62s]  And therefore, the representation here knows something about the path, so
[2192.62s -> 2195.54s]  it kind of knows what position it's in, basically.
[2195.54s -> 2200.06s]  And so that's giving a lot of that information.
[2200.06s -> 2204.30s]  Or another way to think about it is, this final representation will give
[2204.30s -> 2207.98s]  a certain overall sense of the semantics of the sentence.
[2207.98s -> 2212.10s]  And so to the extent that you're looking backwards, the more sort of
[2212.10s -> 2217.58s]  associative matching of similar semantic content that's needed seems sufficient.
[2217.58s -> 2220.26s]  And you don't really need additional positional information.
[2223.78s -> 2226.38s]  Okay, I will go on.
[2226.42s -> 2233.18s]  Okay, so that's the neural network's content for today.
[2233.18s -> 2239.98s]  And so for the remaining 39 minutes, I want to talk final projects,
[2239.98s -> 2243.94s]  but also a bit about data experiments and things like that.
[2243.94s -> 2246.38s]  Okay, so this is a reminder on the class.
[2246.38s -> 2251.62s]  So we've got the four assignments, which are 48%.
[2251.62s -> 2254.98s]  And then the big other part of what you need to do
[2254.98s -> 2259.26s]  is the final project, which is 49%,
[2259.26s -> 2262.30s]  almost completing things out except for the participation.
[2264.66s -> 2270.82s]  And let me just give one note back to collaboration, the honor code.
[2270.82s -> 2276.02s]  I mean, for final projects, it's quite usual that people use
[2276.02s -> 2279.06s]  all sorts of stuff that were written by other people.
[2279.06s -> 2280.50s]  That's completely fine.
[2280.50s -> 2284.46s]  We don't expect you to implement everything from scratch.
[2284.98s -> 2287.14s]  But you must document what you're using.
[2287.14s -> 2291.46s]  You know, give references or URLs if you're using other people's code
[2291.46s -> 2293.06s]  rather than writing your own.
[2293.06s -> 2295.86s]  We do not want to know what code you wrote yourself and
[2295.86s -> 2299.18s]  what things you downloaded from Pi Pi.
[2299.18s -> 2303.90s]  And in particular, in thinking about final projects,
[2303.90s -> 2309.34s]  the question of interest for us is what value add did you provide, right?
[2309.34s -> 2313.58s]  So you haven't done something great if you've downloaded a really good neural
[2313.70s -> 2317.74s]  network and run it on some data, and it produces really good results.
[2317.74s -> 2320.18s]  That's not much value add.
[2320.18s -> 2324.58s]  So if you want to have value add in that context, you at least want to be
[2324.58s -> 2330.62s]  doing something interesting about understanding why it works so well.
[2330.62s -> 2333.50s]  What kind of examples it doesn't work well on,
[2333.50s -> 2336.50s]  doing some thorough experimental analysis.
[2336.50s -> 2344.62s]  Yeah, a couple of other points there.
[2344.62s -> 2348.90s]  Okay, so for the final project for
[2348.90s -> 2353.50s]  this class, there's a binary choice.
[2353.50s -> 2356.78s]  You can either do our default final project,
[2356.78s -> 2359.10s]  which I'll talk about more a bit later.
[2359.10s -> 2362.34s]  Or you can come up with your own final project and
[2362.34s -> 2363.94s]  I'll talk about that a bit too.
[2364.90s -> 2369.94s]  So we allow team sizes of one to three.
[2369.94s -> 2376.06s]  The complicated thing that comes up, actually sorry,
[2376.06s -> 2378.10s]  I should say the other point first.
[2378.10s -> 2384.02s]  Yeah, so if you do, we generally encourage people to form teams.
[2384.02s -> 2386.94s]  It means that you can do something more interesting,
[2386.94s -> 2390.82s]  it's more motivational, you can make friends, whatever.
[2390.82s -> 2393.10s]  So teams are good.
[2393.14s -> 2397.86s]  On expectations for teams, our expectation for
[2397.86s -> 2403.22s]  teams is that a bigger team should be able to do proportionally more work.
[2403.22s -> 2405.90s]  Now, and so when we're grading things,
[2405.90s -> 2411.26s]  we expect to see more work from larger teams.
[2411.26s -> 2415.82s]  Now, how this works out is kind of, I will admit,
[2415.82s -> 2421.02s]  a little bit complicated cuz there's sort of a quality issue
[2421.06s -> 2425.22s]  that's separate from the amount of work.
[2425.22s -> 2429.90s]  So the reality is that it's just always the case
[2429.90s -> 2435.06s]  that several of the very best projects are one person efforts because
[2435.06s -> 2439.94s]  they're just somebody who has a good idea and knows what they want to do and
[2439.94s -> 2442.90s]  does it by themselves and it is great.
[2442.90s -> 2446.90s]  But they're also great multi-person projects as well.
[2446.90s -> 2452.62s]  But the point I'm meaning is, well, it kind of doesn't work if you're
[2452.62s -> 2457.78s]  a one-person project and you try and attempt a huge amount of stuff and
[2457.78s -> 2460.38s]  you can only get one-third of the way through it.
[2460.38s -> 2464.54s]  That's not a good recipe for doing well in the final project.
[2464.54s -> 2469.46s]  For any project, you really need to sort of be completing something and
[2469.46s -> 2470.86s]  showing something.
[2470.86s -> 2474.94s]  But nevertheless, if you're a one person and
[2474.98s -> 2479.78s]  you can show something kind of interesting, even if our reaction is,
[2479.78s -> 2484.70s]  well, this would have been much better if they'd shown it was better than
[2484.70s -> 2488.42s]  this other kind of model, or it would have been really nice if they'd
[2488.42s -> 2491.06s]  run ablations to work things out.
[2491.06s -> 2494.02s]  Well, if you're one person, we'll give you a buy and say,
[2494.02s -> 2496.62s]  but it was only one person.
[2496.62s -> 2501.10s]  Whereas if you're a three person team and it seems like you obviously
[2501.10s -> 2503.30s]  should have compared it to some other models and
[2503.30s -> 2506.82s]  you obviously could have run it on some other data sets,
[2506.82s -> 2510.90s]  then we'll feel like, well, as a three person team, they obviously should have
[2510.90s -> 2515.54s]  done that and therefore we should give them a less good score.
[2515.54s -> 2518.94s]  And that's how that worked out.
[2518.94s -> 2525.26s]  The complication comes with other things people are doing at the same time.
[2525.26s -> 2531.14s]  We allow people to do final projects that are shared with multiple classes.
[2532.10s -> 2535.50s]  But our expectation is, again, that you'll do more work.
[2535.50s -> 2540.14s]  So if there are two of you who are using one project for both this class and
[2540.14s -> 2545.50s]  CS 231N say, then it's sort of like it's a four person project and
[2545.50s -> 2548.14s]  you should be doing a lot of work for it.
[2549.58s -> 2553.22s]  There are other cases, sometimes people have RA ships or
[2553.22s -> 2557.02s]  their PhD rotation students or other things.
[2557.02s -> 2559.86s]  If you're doing it for other things, we'd like you to tell us and
[2559.86s -> 2563.62s]  we expect you to be doing more work for it.
[2565.82s -> 2570.06s]  Okay, I'm very happy to talk to people about final projects and
[2570.06s -> 2572.50s]  have been talking to people about final projects.
[2572.50s -> 2574.50s]  But unfortunately, there's only one of me, so
[2574.50s -> 2578.50s]  I definitely can't talk to 500 people about final projects.
[2578.50s -> 2583.74s]  So I do also encourage you to talk to all of the TAs about final projects.
[2583.74s -> 2588.02s]  So on the office hours page under all of the TAs,
[2588.02s -> 2591.94s]  there's some information about things that they know about.
[2591.94s -> 2595.06s]  So if you know what your project is about, you could at least try and
[2595.06s -> 2599.98s]  find one of the most useful TAs or just find a TA with a friendly face.
[2599.98s -> 2605.30s]  Whatever mechanism you use, talk to TAs about final projects.
[2606.74s -> 2609.30s]  Yeah, so default final projects.
[2609.30s -> 2616.86s]  So what it's gonna be is, so Bert was a famous early transformer.
[2616.90s -> 2619.30s]  And we're going to be sort of building and
[2619.30s -> 2623.82s]  experimenting with a minimal Bert implementation.
[2623.82s -> 2630.70s]  So if you do this, there's part of an implementation of Bert.
[2630.70s -> 2632.70s]  And you're meant to finish it off.
[2632.70s -> 2634.62s]  And you're meant to fine tune it and
[2634.62s -> 2639.50s]  get some data results for doing sentiment analysis.
[2639.50s -> 2644.82s]  And then basically, we want even the default final project to be
[2644.82s -> 2648.34s]  an open-ended project where people can do different things.
[2648.34s -> 2653.02s]  And so then there's lots of other ideas or you can come up with your own
[2653.02s -> 2657.70s]  of ways you could extend this system and make it better.
[2657.70s -> 2660.70s]  Which might be with paraphrasing, contrastive learning,
[2660.70s -> 2663.22s]  low rank adaptation, something.
[2663.22s -> 2666.70s]  And you can do something and that is your final project.
[2668.94s -> 2672.14s]  So why choose the final project?
[2672.14s -> 2678.22s]  So if you haven't had much experience with research, you don't have any
[2678.22s -> 2682.82s]  real idea of what you want to do for a final project, or
[2682.82s -> 2686.14s]  you'd like something with clear guidance and a goal.
[2686.14s -> 2689.46s]  And a leaderboard, because we provide a leaderboard for
[2689.46s -> 2693.94s]  people doing the default final project of how good your performance is
[2693.94s -> 2698.54s]  on the tasks we provide, then you can do the final project.
[2698.54s -> 2701.02s]  And I mean, honestly, I think for many people,
[2701.02s -> 2705.14s]  the best option is to do the final project.
[2705.14s -> 2708.50s]  For sort of past performance, typically about half the students do
[2708.50s -> 2713.90s]  the final project, including some people who start off thinking,
[2713.90s -> 2715.78s]  I'll do a custom final project.
[2715.78s -> 2717.86s]  Then after a couple of weeks,
[2717.86s -> 2722.10s]  they decide, this makes no sense what I was suggesting, it's not working at all.
[2722.10s -> 2725.30s]  I'm just gonna abandon and flip to the default final project.
[2725.82s -> 2730.54s]  Okay, but we also allow custom final projects, and
[2730.54s -> 2733.22s]  there are good reasons to do custom final projects.
[2733.22s -> 2736.62s]  So if you have some topic or
[2736.62s -> 2741.14s]  research idea that you're excited about, maybe you're already even
[2741.14s -> 2746.94s]  working on it, or you want to try something different on your own, or
[2746.94s -> 2750.58s]  you'd just like to have more of the experience of trying to come up with
[2750.58s -> 2753.94s]  a research goal, finding the necessary data and tools.
[2753.98s -> 2757.98s]  And starting from scratch, which is actually very educational,
[2757.98s -> 2765.70s]  if considerably harder, well then the custom final project is fine for you.
[2765.70s -> 2769.94s]  Restriction on topics, I think we'd already sort of signaled this on Ed.
[2771.30s -> 2776.98s]  We insist for CS224N final projects that they have
[2776.98s -> 2782.26s]  to substantively involve both human language and neural networks.
[2782.46s -> 2785.38s]  Cuz this is the NLP class.
[2785.38s -> 2791.22s]  So we'd like people to know and learn something about human language.
[2791.22s -> 2795.22s]  I'm totally aware of the fact that you can use these same models for
[2795.22s -> 2802.30s]  bioinformatic sequences or music or radar, whatever.
[2802.30s -> 2807.26s]  But we'd like you to do something with human language for this class.
[2807.26s -> 2811.30s]  That doesn't mean it has to be only about human language, so
[2811.38s -> 2818.34s]  people have done things like visual language models or music and language.
[2818.34s -> 2825.18s]  So it can have a combination of modalities, but it has to substantively,
[2825.18s -> 2828.54s]  not completely trivially involve human language.
[2828.54s -> 2830.94s]  If you've got any questions about that, ask.
[2830.94s -> 2833.94s]  And it also has to substantively involve neural networks.
[2833.94s -> 2836.98s]  So again, it doesn't have to be wholly about neural networks.
[2836.98s -> 2841.50s]  If you've got some ideas thinking, I think I could show using kernel
[2841.50s -> 2845.82s]  machines that they work just as well as having multilayer neural networks or
[2845.82s -> 2848.94s]  something like that, that's of course fine to do as well.
[2851.02s -> 2856.58s]  Gamesmanship, the default final project is more guided,
[2856.58s -> 2860.06s]  but it's not meant to be a complete slacker's ride.
[2860.06s -> 2863.38s]  We're hoping that people do the same amount of work for
[2863.38s -> 2865.42s]  either kind of project.
[2865.42s -> 2870.14s]  But on the other hand, it does kind of give you sort of a clearer focus and
[2870.14s -> 2875.54s]  course of things to do, but it's still an open-ended project.
[2876.66s -> 2882.22s]  So for both default final projects and custom final projects,
[2882.22s -> 2887.38s]  there are great projects and there are not so great projects.
[2887.38s -> 2891.86s]  If anything, there's a bit more variance in the custom final projects.
[2891.86s -> 2897.62s]  So the path of success is not to try and do something for
[2897.62s -> 2902.42s]  a custom final project that just looks really weak compared to
[2902.42s -> 2904.66s]  people's default final projects.
[2905.86s -> 2910.42s]  Okay, you can get good grades either way.
[2910.42s -> 2914.78s]  We give best project awards to both kinds of projects.
[2914.78s -> 2917.70s]  So yeah, it's really not that there's some secret one you have to pick.
[2918.22s -> 2928.14s]  Computing, yeah, so to be honest with the confessions right at the beginning,
[2928.14s -> 2931.46s]  we're actually in a less good position for
[2931.46s -> 2935.14s]  computing than we've been in recent years.
[2935.14s -> 2937.46s]  And it's all open AI's fault.
[2937.46s -> 2939.78s]  No, that part isn't.
[2939.78s -> 2945.10s]  But up until and including last year,
[2945.10s -> 2949.90s]  we were actually had invariably managed to get very
[2949.90s -> 2955.62s]  generous cloud computing giveaways from one or other cloud computing provider,
[2955.62s -> 2962.02s]  which really provided a lot of computing support.
[2962.02s -> 2965.94s]  But there's the great GPU shortage on at the moment,
[2965.94s -> 2968.82s]  due to the great success of large language models.
[2968.82s -> 2973.30s]  And it turns out that cloud compute providers just aren't being as generous
[2973.46s -> 2974.70s]  as they used to be.
[2974.70s -> 2979.58s]  And gee, I guess the AWS rep was pointing out that my
[2979.58s -> 2984.98s]  course was their single largest grant of free GPUs last year.
[2984.98s -> 2987.14s]  So it's getting harder to do.
[2987.14s -> 2994.98s]  So really people will have to patch things together more in many cases.
[2994.98s -> 3000.38s]  And so we'll be relying on the ingenuity of students to be able to
[3000.38s -> 3003.06s]  find free and cheap stuff.
[3003.82s -> 3008.26s]  So Google is giving $50 credit per person on GCP,
[3008.26s -> 3013.22s]  which can be used for assignments three, four, and the final project.
[3013.22s -> 3017.94s]  On all the clouds, if you haven't used a cloud with an account before,
[3017.94s -> 3023.66s]  you can usually get some free starter credits, which can be a useful thing.
[3024.78s -> 3028.30s]  There are the sort of Jupyter notebooks in the cloud.
[3028.30s -> 3035.50s]  So the most used one is Google Colab, which allows limited GPU use.
[3035.50s -> 3040.14s]  It often tends to get tighter later in the quarter.
[3040.14s -> 3045.18s]  So you might find it a good investment to not have a couple of lattes and
[3045.18s -> 3048.30s]  pay ten bucks a month to get Colab Pro,
[3048.30s -> 3052.02s]  which gives you much better access to GPUs.
[3052.02s -> 3055.26s]  But there are alternatives to that which you might also want to look at.
[3055.30s -> 3061.50s]  So AWS provides a Jupyter notebook environment, say to make a studio lab.
[3061.50s -> 3067.86s]  And also owned by Google, Kaggle separately provides Kaggle notebooks,
[3067.86s -> 3074.06s]  which actually commonly give you better GPU access than Google Colab provides,
[3074.06s -> 3078.14s]  even though they're otherwise not as nice.
[3078.14s -> 3083.86s]  Kaggle notebooks are sort of just bare bones Jupyter notebooks,
[3083.86s -> 3087.78s]  whereas Colab has some fancier UI stuff grafted on it.
[3089.30s -> 3094.02s]  So other possibilities, Modal is a low priced GPU provider and
[3094.02s -> 3100.26s]  allows a certain amount of free GPU usage a month, so that could be handy.
[3100.26s -> 3105.94s]  There are other lower cost GPU providers like Fast AI, which could be of relevance.
[3105.94s -> 3110.02s]  And then the other thing that I'll say more about in a minute is,
[3110.02s -> 3113.74s]  the way things have changed with large language models,
[3114.66s -> 3116.78s]  there are lots of projects that you might want to do,
[3116.78s -> 3120.74s]  where you're not actually building models at all yourself.
[3120.74s -> 3126.14s]  But you're wanting to do experiments on large language models, or
[3126.14s -> 3131.26s]  you're wanting to do in context learning with large language models, or
[3131.26s -> 3133.22s]  other things of that sort.
[3133.22s -> 3138.82s]  And then what you want is to have access to large language models.
[3138.82s -> 3142.82s]  And in particular, you probably want to have API access, so
[3142.86s -> 3144.10s]  you can automate things.
[3144.10s -> 3148.90s]  So another thing that we have been able to get is through the generosity of
[3148.90s -> 3154.34s]  Together AI, that Together AI is providing $50 of API
[3154.34s -> 3159.98s]  access to large language models, which can actually be a lot.
[3159.98s -> 3163.98s]  How much of a lot it is depends on how big a model you're using.
[3163.98s -> 3168.30s]  So something you should think about is how big a model do you really need to use
[3168.30s -> 3170.02s]  to show something.
[3170.02s -> 3174.98s]  Because if you can run a 7 billion parameter language model on together,
[3174.98s -> 3179.26s]  you can put a huge number of tokens through it for 50 bucks.
[3179.26s -> 3184.06s]  Whereas if you want to run a much bigger model, then the number of
[3184.06s -> 3187.82s]  tokens you can get through it goes down by orders of magnitude.
[3189.26s -> 3192.90s]  So that's good, and I mentioned some other ones.
[3192.90s -> 3197.86s]  So we've already put a whole bunch of documents up on Ed that talk about
[3197.86s -> 3202.18s]  these different GPU options, so do look at those.
[3204.18s -> 3206.70s]  Okay, jumping ahead.
[3206.70s -> 3210.78s]  So the first thing you have to do is a project proposal.
[3210.78s -> 3212.42s]  So it's one per team.
[3212.42s -> 3216.10s]  So I guess the first step is to work out who your team is.
[3216.10s -> 3218.98s]  And so for the project proposal,
[3218.98s -> 3223.82s]  part of it is actually giving us the details of your project.
[3223.82s -> 3227.10s]  But there's another major part of it,
[3227.10s -> 3232.54s]  which is writing a review of a key research paper for your topic.
[3232.54s -> 3236.74s]  So for the default final project, we provide some suggestions so
[3236.74s -> 3239.42s]  you can find something else if you've got another idea for
[3239.42s -> 3241.22s]  how to extend the project.
[3241.22s -> 3244.10s]  For your custom project, you're finding your own.
[3244.10s -> 3249.06s]  But what we want you to do is get some practice at looking at a research paper,
[3249.06s -> 3253.78s]  understanding what it's doing, understanding what's convincing,
[3253.78s -> 3256.78s]  what it didn't consider, what it failed to do.
[3256.78s -> 3262.22s]  And so we want you to write a two-page summary of a research paper.
[3262.22s -> 3268.50s]  And the goal is for you to be thinking critically about this research paper of
[3268.50s -> 3273.42s]  what did it do that was exciting versus what did it claim was exciting but
[3273.42s -> 3277.02s]  was really obvious, or perhaps even wrong, etc.
[3278.38s -> 3281.70s]  Okay, and then, right, so then after, so
[3282.62s -> 3288.42s]  after that, we want you to say what you're planning to do.
[3288.42s -> 3292.30s]  That may be very straightforward for a default final project, but
[3292.30s -> 3298.02s]  it's really important for a custom final project.
[3298.02s -> 3305.14s]  And in particular, tell us about the literature you're going to use,
[3305.14s -> 3308.30s]  if any, and the kind of models you're going to explore.
[3308.34s -> 3313.82s]  But it turns out that when we're unhappy with custom final projects,
[3313.82s -> 3319.38s]  the two commonest complaints about what you tell us about custom final projects
[3319.38s -> 3324.22s]  is you don't make clear what data you're going to use because
[3324.22s -> 3327.78s]  we're sort of worried already if you haven't worked out by the project
[3327.78s -> 3332.02s]  proposal deadline what data you can use for your final project.
[3332.02s -> 3336.78s]  And if you don't tell us how you're going to evaluate your system.
[3336.82s -> 3340.98s]  We want to know how you're going to measure whether you're getting any success.
[3340.98s -> 3346.62s]  As a new thing this year, we'd like you to include
[3346.62s -> 3351.62s]  an ethical considerations paragraph outlining potential ethical challenges of
[3351.62s -> 3357.50s]  your work if it were deployed in the real world and how that might be mitigated.
[3357.50s -> 3362.02s]  This is something that now a lot of conferences are requiring and
[3362.02s -> 3363.98s]  a lot of grants are requiring.
[3364.02s -> 3367.22s]  So I want to give you a little bit of practice on that by writing a paragraph
[3367.22s -> 3367.74s]  of that.
[3368.78s -> 3374.70s]  How much there is to talk about varies somewhat on what you're trying to do and
[3374.70s -> 3377.42s]  whether it has a lot of ethical problems or
[3377.42s -> 3380.78s]  whether it's a fairly straightforward question answering system.
[3380.78s -> 3385.42s]  But in all cases you might think about what are the possible
[3385.42s -> 3387.74s]  ethical considerations of this piece of work.
[3388.98s -> 3391.70s]  Okay, the whole thing is maximum four pages.
[3392.70s -> 3399.34s]  Okay, so for the research paper summary, yeah.
[3399.34s -> 3401.62s]  Do think critically, right?
[3401.62s -> 3409.54s]  I mean, the worst summaries are essentially
[3409.54s -> 3414.98s]  people that just paraphrase what's in the abstract and introduction of the paper.
[3414.98s -> 3418.90s]  And we want you to think a bit harder about this.
[3418.94s -> 3422.42s]  What were the novel contributions of the paper?
[3423.86s -> 3427.18s]  Is it something that you could use for different kinds of problems and
[3427.18s -> 3428.02s]  different ways?
[3428.02s -> 3431.66s]  Or was it really exploiting a trick of one data set?
[3431.66s -> 3436.10s]  Are there things that it seemed like they missed or
[3436.10s -> 3441.02s]  could have done differently or you weren't convinced were done properly?
[3441.02s -> 3445.90s]  Is it similar or distinctive to other papers that are dealing with the same
[3445.94s -> 3446.86s]  topic?
[3446.86s -> 3450.70s]  Does it suggest perhaps something that you could try that extends beyond
[3450.70s -> 3451.18s]  the paper?
[3453.38s -> 3457.58s]  Okay, and for grading these final project proposals,
[3457.58s -> 3461.82s]  most of the points are on that paper review.
[3461.82s -> 3464.66s]  And so do pay attention to it.
[3464.66s -> 3468.42s]  There are some points on the project plan.
[3468.42s -> 3473.90s]  But really, we're wanting to mainly give you formative feedback
[3473.90s -> 3479.06s]  on the project plan and comments as to how we think it's realistic or unrealistic.
[3479.06s -> 3484.90s]  But nevertheless, we're expecting you to sort of have an idea,
[3484.90s -> 3487.70s]  have thought through how you can investigate it,
[3487.70s -> 3492.98s]  thought through how you can evaluate it, data sets, baselines, things like that.
[3492.98s -> 3494.86s]  Yeah, I should emphasize this.
[3494.86s -> 3497.06s]  Do you have an appropriate baseline?
[3497.06s -> 3501.14s]  So if anything that you're doing,
[3501.14s -> 3503.78s]  you should have something you can compare it against.
[3503.78s -> 3507.86s]  So sometimes it's a previous system that did exactly the same thing.
[3507.86s -> 3511.22s]  But if you're doing something more novel and interesting,
[3511.22s -> 3515.22s]  you should be thinking of some seat of the pants,
[3515.22s -> 3518.90s]  obvious way to do things and proving that you can do it better.
[3518.90s -> 3521.82s]  And what that is depends a lot on what your project is.
[3521.82s -> 3527.94s]  But if you're building some complex neural net that's going to be
[3527.94s -> 3532.82s]  used to work out textual similarity between two pieces of text.
[3532.90s -> 3538.10s]  A simple way of working out textual similarity between two pieces of text is to
[3538.10s -> 3542.58s]  look up the word vectors for every word in the text and average them together and
[3542.58s -> 3547.54s]  work out the dot product between those average vectors.
[3547.54s -> 3551.62s]  And unless your complex neural network is significantly better than that,
[3551.62s -> 3554.26s]  it doesn't seem like it's a very good system.
[3554.26s -> 3557.14s]  So you should always attempt to have some baselines.
[3557.54s -> 3562.30s]  After the project proposal,
[3562.30s -> 3566.30s]  we also have a project milestone stuck in the middle to make sure
[3566.30s -> 3568.90s]  everybody has making some progress.
[3568.90s -> 3573.66s]  This is just to help make sure people do get through things and
[3573.66s -> 3577.38s]  keep working on it, so we'll have good final projects.
[3577.38s -> 3582.50s]  For most final projects, I'll say more about this in a minute,
[3583.14s -> 3587.78s]  the crucial thing we expect for the milestone is that you know,
[3587.78s -> 3591.30s]  you've kind of got set up and you can run something.
[3591.30s -> 3594.66s]  It might just be your baseline of looking up the word vectors, but
[3594.66s -> 3597.58s]  means you've kind of got the data and the framework and
[3597.58s -> 3600.66s]  something that you can run and produce a number from it.
[3602.22s -> 3604.98s]  And then there's the final project.
[3606.34s -> 3610.10s]  We have people submit their code for the final projects, but
[3610.10s -> 3616.62s]  the final projects are evaluated almost entirely,
[3616.62s -> 3622.62s]  unless there's some major worries or concerns based on your project report.
[3622.62s -> 3626.02s]  So make sure you put time into the project report,
[3626.02s -> 3630.46s]  which is essentially a research paper like a conference paper.
[3630.46s -> 3633.34s]  And they can be up to eight pages.
[3633.34s -> 3638.54s]  And it varies on what you're doing, but this is the kind of picture typically
[3638.58s -> 3640.62s]  of what a paper will look like.
[3640.62s -> 3642.90s]  It'll have an abstract and introduction.
[3642.90s -> 3645.74s]  It'll talk about other related work.
[3645.74s -> 3649.30s]  It'll present the model you're using, the data you're using, and
[3649.30s -> 3651.58s]  your experiments and their results.
[3651.58s -> 3656.10s]  And have some insightful comments in its analysis and conclusion at the end.
[3657.82s -> 3661.98s]  Okay, finding research topics for
[3661.98s -> 3666.54s]  custom projects, all kinds of things you can do.
[3666.90s -> 3671.26s]  Basic philosophy of science, you're normally either starting off with,
[3671.26s -> 3674.30s]  here's some problem I wanna make some progress on.
[3674.30s -> 3680.30s]  Or here's this cool idea for theoretical technique or change in something.
[3680.30s -> 3683.42s]  And I wanna show it's better than other ways of doing it, and
[3683.42s -> 3686.06s]  you're working from that.
[3686.06s -> 3690.42s]  We allow different kinds of projects.
[3690.42s -> 3694.82s]  One common type of project is you've got some task of interest and
[3694.82s -> 3699.30s]  you're gonna try and solve it or make progress on it somehow.
[3699.30s -> 3705.70s]  That you wanna get information out of state department documents.
[3705.70s -> 3710.38s]  And you're gonna see how well you can do it with neural NLP.
[3710.38s -> 3714.02s]  A second kind is you've got some ideas of doing something different with
[3714.02s -> 3715.46s]  neural networks.
[3715.46s -> 3718.58s]  And then you're gonna see how well it works.
[3718.58s -> 3722.22s]  Or maybe, given there are large language models these days,
[3722.26s -> 3725.06s]  you're gonna see how using large language models,
[3725.06s -> 3729.18s]  you can do something interesting by in context learning or
[3729.18s -> 3732.46s]  building a larger language model program.
[3732.46s -> 3740.74s]  So nearly all 224N projects are in those first three types.
[3740.74s -> 3746.14s]  Where at the end of the day, you've got some kind of system and
[3746.14s -> 3749.54s]  you've got some kind of data and you're gonna evaluate it.
[3749.62s -> 3753.34s]  But that's not a 100% requirement.
[3753.34s -> 3757.78s]  There are different kinds of projects you can do and a few people do.
[3757.78s -> 3761.78s]  So you can do an analysis interpretability project.
[3761.78s -> 3764.90s]  So you could be interested in something like,
[3764.90s -> 3769.34s]  how could these transformer models possibly
[3769.34s -> 3775.50s]  understand what I say to them and give the right answers to my statements.
[3775.50s -> 3779.14s]  Let me try and look inside the neural networks and
[3779.14s -> 3781.10s]  see what they're computing how.
[3781.10s -> 3786.18s]  Recently there's been a lot of work on this topic often under titles like
[3786.18s -> 3790.90s]  mechanistic interpretability, circuit training and things like that.
[3790.90s -> 3794.90s]  So you can do some kind of analysis or interpretability project.
[3794.90s -> 3801.02s]  Or you could even just do it looking at the behavior of models of some task.
[3801.02s -> 3808.06s]  So you could take some linguistic task like metaphor interpretation and
[3808.06s -> 3812.22s]  see which neural networks can interpret them correctly and which can't or
[3812.22s -> 3816.26s]  which kinds of ones they can interpret correctly or not and do things like that.
[3817.70s -> 3822.06s]  Another kind is a theoretical project.
[3822.06s -> 3826.98s]  Occasionally people have done things looking
[3826.98s -> 3832.02s]  at the behavior of well,
[3832.02s -> 3836.78s]  that's a good example somewhere that's in the math.
[3836.78s -> 3841.18s]  So an example was actually done a few years ago and turned into
[3841.18s -> 3847.06s]  a conference paper was looking at in the estimation of word vectors,
[3847.06s -> 3851.98s]  the stability of the word vectors that were
[3851.98s -> 3857.30s]  computed by different algorithms Word2Vec versus GloVe,
[3857.30s -> 3862.42s]  and deriving results with proofs about
[3862.42s -> 3868.70s]  the stability of the vectors that were calculated.
[3868.70s -> 3871.82s]  So that's allowed, but we don't see many of those.
[3871.82s -> 3877.58s]  Here very quickly, just sort of random things.
[3877.58s -> 3882.06s]  So a lot of past projects you can find on the 224N web page.
[3882.06s -> 3885.94s]  You can just find different past year reports and
[3885.94s -> 3890.34s]  you can look at them to get ideas as you wish.
[3890.34s -> 3896.10s]  So deep poetry was a gated LSTM where the idea was as well.
[3896.10s -> 3900.22s]  So a language model that generates successive words.
[3900.22s -> 3906.50s]  They had extra stuff in it to make it rhyme in a poetry like pattern.
[3906.50s -> 3908.46s]  That was kind of fun.
[3908.46s -> 3914.78s]  You can do a re-implementation of a paper that has been done previously.
[3914.78s -> 3918.26s]  This is actually kind of an old one, but I remember it well.
[3918.26s -> 3922.98s]  So back in the days before transformers deep-minded these kind
[3922.98s -> 3926.50s]  of interesting papers on neural Turing machines and
[3926.50s -> 3931.38s]  differentiable neural computers, but
[3931.38s -> 3934.26s]  they didn't release implementations of them.
[3934.62s -> 3938.50s]  And so Carol set about writing her own implementation of
[3938.50s -> 3944.42s]  a differentiable neural computer, which in a way was a little bit crazy.
[3944.42s -> 3948.38s]  And a few days before the deadline, she still hadn't gone at working.
[3948.38s -> 3950.94s]  So it could have been a complete disaster, but
[3950.94s -> 3953.98s]  she did get it working before the deadline and
[3953.98s -> 3957.46s]  got it to run producing some interesting results.
[3957.46s -> 3958.70s]  So that was kind of cool.
[3958.70s -> 3962.26s]  So if it's something interesting, it doesn't have to be original.
[3962.26s -> 3965.58s]  It can be sort of re-implementing something interesting.
[3967.06s -> 3973.74s]  Okay, sometimes papers do get published later as interesting ones.
[3973.74s -> 3976.94s]  This was a paper that was sort of, again from the early days,
[3976.94s -> 3982.14s]  it was sort of fairly simple, but it was a novel thing that gave progress.
[3982.14s -> 3986.06s]  So the way we've sort of presented these RNNs,
[3986.06s -> 3988.90s]  you have sort of word vectors at the bottom, and
[3988.90s -> 3991.90s]  then you kind of compute the softmax at the top.
[3991.90s -> 3997.42s]  But if you think about the sort of multiplying by the output matrix and
[3997.42s -> 3999.46s]  then putting that into the softmax,
[3999.46s -> 4003.10s]  that output matrix is also like a set of word vectors.
[4003.10s -> 4008.14s]  Because you have a column for each word, and then you get a score for
[4008.14s -> 4012.58s]  each output word, and then you're putting a softmax over that.
[4012.58s -> 4017.02s]  And so their idea was, well, maybe you could sort of share those two
[4018.54s -> 4021.66s]  sets of vectors, and you'd be able to get improvements from that.
[4021.66s -> 4022.34s]  And you could.
[4024.54s -> 4027.54s]  Okay, maybe I won't talk about that one.
[4027.54s -> 4030.30s]  Sometimes people have worked on quantized models.
[4030.30s -> 4034.90s]  That's sort of more of a general neural network technique.
[4034.90s -> 4037.82s]  But providing you show you can do useful things with it,
[4037.82s -> 4041.42s]  like have good language modeling results, even with quantized vectors,
[4041.42s -> 4043.74s]  we'll count that as using language.
[4043.74s -> 4048.82s]  So in recent times, these two, last two are from 2024,
[4048.82s -> 4053.54s]  a lot of the time people are doing projects with pre-trained
[4053.54s -> 4059.98s]  large language models, which we will be talking about in the next three models,
[4059.98s -> 4064.82s]  three lectures, and then doing things with them.
[4064.82s -> 4069.26s]  And so you can do lightweight parameter efficient fine tuning methods.
[4069.26s -> 4072.82s]  You can do in context learning methods and things like this.
[4072.94s -> 4079.50s]  And I suspect that probably quite a few of you will do projects of this kind.
[4079.50s -> 4081.66s]  So here's an example.
[4081.66s -> 4088.46s]  So lots of work has been done on producing code language models.
[4090.46s -> 4098.90s]  And so these people decided to improve the generation of Fortran.
[4098.90s -> 4100.50s]  Maybe they're physicists, I don't know.
[4100.54s -> 4106.46s]  And so they were able to show that they could
[4106.46s -> 4115.54s]  use parameter efficient fine tuning to improve code llama for producing Fortran.
[4115.54s -> 4117.78s]  Now where was the natural language?
[4117.78s -> 4120.94s]  Code has natural language comments in it.
[4120.94s -> 4126.70s]  And the comments can be useful for explaining what you want the code to do.
[4126.78s -> 4133.74s]  And so it was effectively doing translation from human language,
[4133.74s -> 4138.18s]  explanation of what the code was meant to do into pieces of code.
[4140.62s -> 4148.98s]  Here was another one which was doing AI fashion driven cataloging,
[4148.98s -> 4152.18s]  transforming images into textual descriptions.
[4152.18s -> 4155.82s]  Which again was starting off with the existing visual language model and
[4155.98s -> 4157.14s]  looking at how to fine tune it.
[4158.94s -> 4161.98s]  Okay, other places to look for stuff.
[4163.18s -> 4167.42s]  So you can get kind of lots of ideas of areas and
[4167.42s -> 4169.86s]  things people do by looking at past papers.
[4169.86s -> 4172.82s]  They're also welcome to have your own original ideas,
[4172.82s -> 4176.26s]  thinking about anything you know or work on in the world.
[4176.26s -> 4182.10s]  So for NLP papers, there's a site called the ACL Anthology that's good for them.
[4182.70s -> 4186.58s]  There are lots of papers on language that also appear in machine learning
[4186.58s -> 4191.14s]  conferences, so you can look at the NeurIPS or ICLR proceedings.
[4191.14s -> 4194.34s]  You can look at past 224N projects.
[4194.34s -> 4199.62s]  And then the archive preprint servers got tons of papers on everything,
[4199.62s -> 4202.82s]  including NLP, and you can look there.
[4202.82s -> 4205.62s]  But I do actually think it's some of the funnest,
[4205.62s -> 4209.26s]  best projects are actually people that find their own problem,
[4209.26s -> 4212.50s]  which is an interesting problem in their world.
[4212.50s -> 4217.10s]  You know, if there's anything about a cool website that has text on it, and
[4217.10s -> 4220.30s]  you think you could kind of get information out of automatically by
[4220.30s -> 4223.70s]  using a language model or something, there's probably something interesting and
[4223.70s -> 4225.94s]  different you can do there.
[4225.94s -> 4229.78s]  Another place to look is that there are various leaderboards for
[4229.78s -> 4232.34s]  the state of the art on different problems.
[4232.34s -> 4234.82s]  And you can start looking through leaderboards for
[4234.82s -> 4237.26s]  stuff and see what you find there.
[4239.42s -> 4243.90s]  But on the other hand, the disadvantage of looking at things like leaderboards and
[4243.90s -> 4247.62s]  past conferences is you sort of tend to be
[4247.62s -> 4251.02s]  trying to do a bit better on a problem someone else has done.
[4251.02s -> 4255.18s]  And that's part of why really often in research,
[4255.18s -> 4258.74s]  it's a clever thing to think of something different.
[4258.74s -> 4262.02s]  Perhaps not too far from things that other people have done, but
[4262.02s -> 4266.74s]  somehow different so you'll be able to do something a bit more original and
[4266.74s -> 4268.06s]  different for what you're doing.
[4270.26s -> 4275.46s]  Yeah, I do just want to go through this a bit quickly.
[4276.50s -> 4280.90s]  That, you know, for
[4280.90s -> 4285.54s]  sort of decade that I've been doing natural language processing with
[4285.54s -> 4291.22s]  deep learning, there's sort of been a sea change in what's possible.
[4291.22s -> 4295.86s]  So in the early days of the deep learning revival,
[4296.46s -> 4300.34s]  most of the work in people's papers were trying to find
[4300.34s -> 4303.06s]  better deep learning architectures.
[4303.06s -> 4307.06s]  So that would be, here is some question answering system.
[4307.06s -> 4310.78s]  I've got an idea of how I could add attention in some new place, or
[4310.78s -> 4317.50s]  I could add a new layer into the neural network, and the numbers will go up.
[4317.50s -> 4322.30s]  And there were lots of papers like that, and it was a lot of fun.
[4322.30s -> 4327.90s]  And that's what a lot of good CS224N projects did too.
[4327.90s -> 4331.38s]  And people were often able to build systems from scratch
[4331.38s -> 4334.50s]  that were close to the state of the art.
[4334.50s -> 4337.34s]  But in the last five years,
[4337.34s -> 4343.62s]  your chances of doing this have become pretty slim, frankly.
[4343.62s -> 4347.10s]  If you've really got a good idea and it's something different and
[4347.10s -> 4351.22s]  original, by all means, but it's kind of hard.
[4351.26s -> 4357.82s]  So most work these days, even for people who are professional researchers,
[4357.82s -> 4366.06s]  that they're making use of existing large pre-trained models in some way.
[4366.06s -> 4367.90s]  And then once you're doing that,
[4367.90s -> 4371.50s]  that actually sort of fixes a lot of your architectural choices.
[4371.50s -> 4376.26s]  Because your large pre-trained neural network has a certain architecture, and
[4376.26s -> 4377.98s]  you kind of have to live with that.
[4378.14s -> 4382.38s]  You might be able to do interesting things by adapting it with something like
[4382.38s -> 4385.26s]  low rank adaptation around the side or something.
[4385.26s -> 4389.14s]  But nevertheless, there's sort of constraints on what you want to do.
[4389.14s -> 4395.10s]  So for just about any practical project, like you've got some data set and
[4395.10s -> 4399.30s]  you want to understand it and get facts out of it or something like that.
[4399.30s -> 4403.14s]  Essentially, the only sensible choice is to say,
[4403.18s -> 4407.18s]  I am going to use Hugging Face Transformers,
[4407.18s -> 4410.22s]  which we have a tutorial on coming up ahead.
[4410.22s -> 4415.54s]  And I will load some pre-trained model and I will be running it over the text.
[4415.54s -> 4418.66s]  And then I'll be working out some other stuff I can do on a top and
[4418.66s -> 4419.78s]  around that.
[4419.78s -> 4424.62s]  So building your own architecture is really only a sensible choice.
[4424.62s -> 4427.32s]  If you can do something in the small,
[4427.32s -> 4431.82s]  which is more a sort of exploring architecture's project.
[4431.82s -> 4435.10s]  If you've kind of got an idea of, hey, I've got an idea for
[4435.10s -> 4439.38s]  a different non-linearity that I think will work better than using Arello.
[4439.38s -> 4443.74s]  Let me investigate kind of thing, because then you can do small experiments.
[4445.50s -> 4450.90s]  Yeah, maybe I won't read out all of this list, but there are lists of
[4450.90s -> 4456.30s]  sort of some of the ideas of what's more interesting now.
[4456.30s -> 4461.74s]  But do be cognizant of the world we're in and
[4461.74s -> 4463.42s]  terms of scale.
[4463.42s -> 4468.10s]  I mean, one of the problems we now have is that people
[4468.10s -> 4473.26s]  have seen the latest paper that was being pushed by DeepMind or
[4473.26s -> 4479.34s]  whoever doing some cool graph structured reasoning search to do things.
[4479.34s -> 4483.62s]  And they turn up and say, I wanna do this for my project.
[4483.62s -> 4488.70s]  But a lot of the time, if you read further into the paper,
[4488.74s -> 4493.34s]  you'll find that they were doing it on 32A100s for a month.
[4493.34s -> 4496.86s]  And that's not the scale of compute that you're gonna
[4496.86s -> 4501.02s]  have available to you in almost all circumstances.
[4501.02s -> 4502.90s]  Maybe they're one or two industry students.
[4502.90s -> 4505.62s]  For the industry students, then you can do that.
[4505.62s -> 4506.70s]  If so, go for it.
[4506.70s -> 4510.54s]  But for the vast majority of people, not likely.
[4510.54s -> 4516.30s]  So you do have to do something that is practical.
[4516.30s -> 4521.46s]  But that practicality is true for a vast majority of the people in the world.
[4521.46s -> 4526.70s]  And if you look around in blogs and so on, you find lots of people
[4526.70s -> 4531.18s]  doing stuff in lightweight ways and describing how to do that.
[4531.18s -> 4535.34s]  And that's why methods like parameter efficient fine tuning are really popular
[4535.34s -> 4538.74s]  because you can do them in lightweight ways.
[4538.74s -> 4543.34s]  The question related to that, and I'll end on this,
[4543.34s -> 4550.50s]  is I just wanted to sort of mention again.
[4550.50s -> 4555.08s]  If you want to, you're welcome to use GPT-4, or Gemini Pro, or
[4555.08s -> 4560.30s]  Claude Opus, or any of these models in your project.
[4560.30s -> 4564.46s]  But it has to be then API usage.
[4564.46s -> 4568.38s]  You can't possibly train your own big models.
[4568.38s -> 4571.86s]  I mean, even for the models that are available open source and
[4571.86s -> 4578.90s]  like those, for big models you can't even load them into the kind of GPUs you have.
[4578.90s -> 4583.42s]  So probably you can load a llama 7b model, but
[4583.42s -> 4588.22s]  you can't just load into your GPU a llama 70b model.
[4588.22s -> 4593.58s]  So you have to be realistic on that size.
[4593.58s -> 4598.18s]  But there's actually now lots of interesting things you can do with API
[4598.18s -> 4602.74s]  access, doing things like in context learning and prompting and exploring that.
[4602.74s -> 4607.82s]  Or building larger language model programs around these language
[4607.82s -> 4611.74s]  model components, and you're certainly encouraged to do that.
[4611.74s -> 4615.70s]  Lots of other things you can do, such as analysis projects,
[4615.70s -> 4619.74s]  which look at are these models sexist and racist still?
[4619.74s -> 4623.38s]  Or do they have good understanding of analogies?
[4623.42s -> 4629.02s]  Or can they interpret love letters or whatever is your topic of interest?
[4629.02s -> 4633.98s]  Lots of things you can do, and that's totally allowed.
[4633.98s -> 4638.74s]  But again, remember that we'll be trying to evaluate this on
[4638.74s -> 4641.34s]  what interesting stuff you did.
[4641.34s -> 4646.02s]  So your project shouldn't be, I ran this stuff through GPT-4 and
[4646.02s -> 4649.26s]  it produced great summaries of the documents, I am done.
[4649.30s -> 4656.34s]  The question is, what did you do in addition to that to have an interesting research project?
[4656.34s -> 4657.50s]  Okay, I'll stop there.
[4657.50s -> 4657.98s]  Thanks a lot.
