# Detected language: en (p=1.00)

[0.00s -> 11.08s]  And so today, I kind of just want to cover the fundamentals of PyTorch, really just
[11.08s -> 15.92s]  kind of see what are the similarities between PyTorch and NumPy and Python, which you
[15.92s -> 21.36s]  guys are used to at this point, and see how we can build up a lot of the building
[21.36s -> 25.60s]  blocks that we'll need in order to define more complex models.
[25.60s -> 29.80s]  So specifically, we're going to talk today about tensors, what are tensor objects,
[29.80s -> 35.68s]  how do we manipulate them, what is AutoGrad, how PyTorch helps us compute
[35.68s -> 39.92s]  different gradients, and finally, how we actually do optimization and how we
[39.92s -> 43.36s]  write the training loop for our neural networks. And if we have time at
[43.36s -> 47.40s]  the end, then we'll try and go through a bit of a demo to kind of put
[47.40s -> 52.40s]  everything together and see how everything comes together when you want
[52.44s -> 59.72s]  to solve an actual NLP task. All right, so let's get started. So if you go to the
[59.72s -> 64.88s]  course website, there is a notebook, and you can just make a copy of this Colab
[64.88s -> 71.56s]  notebook and then just run the cells as we go. And so to start, today we're
[71.56s -> 75.80s]  talking about PyTorch, like I said. It's a deep learning framework that really
[75.80s -> 80.16s]  does two main things. One is it makes it very easy to author and manipulate
[80.20s -> 84.84s]  tensors and make use of your GPU so that you can actually leverage a lot of
[84.84s -> 90.12s]  that capability. And two is it makes the process of authoring neural networks
[90.12s -> 94.12s]  much simpler. You can now use different building blocks like linear
[94.12s -> 98.76s]  layers and different loss functions and compose them in different ways in
[98.76s -> 102.56s]  order to author the types of models that you need for your specific use
[102.56s -> 107.52s]  cases. And so PyTorch is one of the two main frameworks along with
[107.56s -> 112.48s]  TensorFlow in this class will focus on PyTorch, but they're quite similar. And so
[112.48s -> 116.80s]  we'll start by importing torch and we'll import the neural network
[116.80s -> 122.56s]  module, which is torch.nn. And for this first part of the tutorial, I want to
[122.56s -> 127.52s]  talk a bit about tensors. One thing that you guys are all familiar with now
[127.52s -> 134.40s]  is NumPy arrays. And so pretty much you can think about tensors as the
[134.44s -> 138.96s]  equivalent in PyTorch to NumPy arrays. They're essentially multi-dimensional
[138.96s -> 144.08s]  arrays that you can manipulate in different ways and you'll essentially
[144.08s -> 148.48s]  use them to represent your data to be able to actually manipulate it and
[148.48s -> 152.92s]  perform all the different matrix operations that underlie your neural
[152.92s -> 159.70s]  network. And so in this case, for example, if we're thinking of an image, one way
[159.70s -> 164.98s]  you can think about it in terms of a tensor is it's a 256 by 256 tensor,
[164.98s -> 171.22s]  where it has a width of 256 pixels and a height of 256 pixels. And for
[171.22s -> 174.98s]  instance, if we have a batch of images and those images contain three
[174.98s -> 179.70s]  channels like red, green, and blue, then we might have a four-dimensional tensor,
[179.70s -> 184.18s]  which is the batch size by the number of channels by the width and the
[184.18s -> 187.26s]  height. And so everything we're going to see today is all going to be
[187.26s -> 191.66s]  represented as tensors, which you can just think of as multi-dimensional
[191.66s -> 196.30s]  arrays. And so to kind of get some intuition about this, we're going to
[196.30s -> 200.74s]  spend a little bit of time going through essentially lists of lists and
[200.74s -> 205.46s]  how we can convert them into tensors and how we can manipulate them with
[205.46s -> 210.66s]  different operations. So to start off with, we just have a simple list of
[210.66s -> 216.22s]  lists that you're all familiar with. In this case, it's a two by three list. And
[216.22s -> 222.94s]  now we want to create a tensor. And so here the way we'll create this tensor
[222.94s -> 228.54s]  is by doing torch dot tensor and then essentially writing the same syntax
[228.54s -> 232.30s]  that we had before. Just write out the list of lists that represents that
[232.30s -> 239.66s]  particular tensor. And so in this case, we get back a tensor object, which is
[239.70s -> 244.50s]  the same shape and contains the same data. And so now the second thing with
[244.50s -> 248.98s]  the tensor is that it contains a data type. So there's different data types. For
[248.98s -> 252.90s]  instance, there are different varying level of precision floating point
[252.90s -> 256.10s]  numbers that you can use. You can have integers, you can have different
[256.10s -> 260.34s]  data types that actually populate your tensor. And so by default, I believe
[260.34s -> 266.42s]  this will be float 32, but you can explicitly specify which data type your
[266.42s -> 271.58s]  tensor is by passing in the D type argument. And so we see here now, even
[271.58s -> 275.10s]  though we wrote in a bunch of integers, they have a decimal point, which
[275.10s -> 279.86s]  indicates that they're floating point numbers. And so same thing here, we
[279.86s -> 287.66s]  could create another tensor, in this case with data type float 32. And in
[287.66s -> 292.94s]  this third example, you see that we create another tensor. We don't actually
[293.18s -> 298.62s]  specify the data type, but PyTorch essentially implicitly takes the data
[298.62s -> 302.30s]  type to be floating point since we actually passed in a floating point
[302.30s -> 308.34s]  number into this tensor. So pretty much at a high level, tensors are like
[308.34s -> 311.98s]  multi-dimensional arrays, we can specify the data type for them, we can
[311.98s -> 317.02s]  populate them just like NumPy arrays. Okay, so now great, we know how to
[317.02s -> 321.14s]  create tensors, we know that ultimately everything that we work with, all the
[321.14s -> 325.26s]  data we have is going to be expressed as tensors. Now the question is, what are
[325.26s -> 328.94s]  the functions that we have to manipulate them? And so we have some
[328.94s -> 334.14s]  basic utilities that can help us instantiate tensors easily, specifically
[334.14s -> 341.94s]  torch.zeros and torch.ones. These are two ways to create tensors of a
[341.94s -> 346.22s]  particular shape, in this case tensors of all zeros or tensors of all ones.
[346.26s -> 350.86s]  And you'll see that this will be very helpful when you do your homeworks,
[350.86s -> 355.74s]  typically you'll want to just need to just create a bunch of zero matrix and
[355.74s -> 359.34s]  it'll be very easy to just specify the shape here without having to write
[359.34s -> 363.78s]  everything out super explicitly. And then you can update that tensor as
[363.78s -> 369.94s]  needed. Another thing you can do is, just like we have ranges in Python, so
[369.94s -> 375.14s]  if you want to loop over a bunch of numbers, you can specify a range. You can
[375.14s -> 381.90s]  also use torch.arange to be able to actually instantiate a tensor with a
[381.90s -> 387.08s]  particular range. In this case we just looped over the numbers 1 through 10, you
[387.08s -> 391.90s]  could reshape this and make it 1 through 5 and then 6 through 10. That's
[391.90s -> 398.02s]  another way to be able to instantiate tensors. And finally, something to note is
[398.02s -> 404.70s]  that when we apply particular operations, such as just simple Python operations
[404.74s -> 409.58s]  like addition or multiplication, by default they're going to be element-wise,
[409.58s -> 414.66s]  so they'll apply to all the elements in our tensor. So in this case we took
[414.66s -> 419.78s]  our tensor, I think this one was probably from earlier above, and we
[419.78s -> 425.30s]  added 2 everywhere. Here we've multiplied everything by 2. But pretty
[425.30s -> 429.38s]  much the PyTorch semantics for broadcasting work pretty much the same
[429.46s -> 436.90s]  as the NumPy semantics. So if you pretty much have different matrix operations
[436.90s -> 440.74s]  where you need to batch across a particular dimension, PyTorch will be
[440.74s -> 444.86s]  smart about it and it will actually make sure that you broadcast over the
[444.86s -> 448.30s]  appropriate dimensions, although of course you have to make sure that the shapes
[448.30s -> 453.34s]  are compatible based on the actual broadcasting rules. So we'll get to
[453.34s -> 458.38s]  that in a little bit when we look at reshaping and how different
[458.86s -> 464.42s]  operations have those semantics. In this case we have to define the, I guess I'm
[464.42s -> 469.30s]  not personally aware of how you would define kind of a jagged tensor that has
[469.30s -> 475.06s]  unequal dimensions, but typically we don't want to do that because it
[475.06s -> 478.82s]  makes our computation a lot more complex and so in cases where we have,
[478.82s -> 482.86s]  you know, for instance we have different sentences that we turn into
[482.86s -> 487.90s]  tokens, we might have different linked sentences in our training set. We'll
[487.94s -> 491.78s]  actually pad all the dimensions to be the same because ultimately we want to
[491.78s -> 495.74s]  do everything with matrix operations and so in order to do that we need to have
[495.74s -> 501.22s]  a matrix of a fixed shape. But yeah, that's a good point. I'm not sure
[501.22s -> 504.78s]  if there is a way to do that, but typically we just get around this by
[504.78s -> 510.38s]  padding. Okay, so now we know how to define tensors. We can do some
[510.38s -> 515.90s]  interesting things with them. So here we've created two tensors. One of them is
[515.94s -> 521.62s]  a three by two tensor, the other one is a two by four tensor, and I think the
[521.62s -> 525.46s]  answer is written up here, but what do we expect is the shape when we multiply
[525.46s -> 532.62s]  these two tensors? So we have a three by two tensor and a two by four tensor.
[532.62s -> 541.34s]  Yeah, three by four. And so more generally we can use matmul in order
[541.38s -> 546.58s]  to do matrix multiplication. It also implements batched matrix multiplication
[546.58s -> 552.18s]  and so I won't go over the entire review of broadcasting semantics, but the
[552.18s -> 557.62s]  main gist is that the dimensions of two tensors are compatible if you can
[557.62s -> 563.42s]  left pad the tensors with ones so that the dimensions that line up either
[563.42s -> 567.30s]  A have the same number in that dimension or B one of them is a dummy
[567.38s -> 572.14s]  dimension, one of them has a one, and in that case in those dummy dimensions
[572.14s -> 576.10s]  PyTorch will actually make sure to copy over the tensor as many times as
[576.10s -> 580.42s]  needed so that you can then actually perform the operation and that's useful
[580.42s -> 584.62s]  when you want to do things like batch dot products or batch matrix
[584.62s -> 591.14s]  multiplications and I guess the final point here is there's also a
[591.14s -> 594.86s]  shorthand notation that you can use so instead of kind of having to type out
[594.90s -> 599.46s]  matmul every time you can just use the at operator similar to numpy.
[599.46s -> 604.46s]  Effectively that's kind of where we get into how batching works so for
[604.46s -> 612.74s]  example if you had let's say two tensors that have some batch
[612.74s -> 619.46s]  dimension and then one of them is m by 1 and the other one is 1 by n and
[619.46s -> 624.98s]  if you do a bit batched matrix multiply to those two tensors now what you
[624.98s -> 628.66s]  effectively do is you preserve the batch dimension and then you're doing a
[628.66s -> 632.90s]  matrix multiplication between an m by 1 tensor and a 1 by n so you get
[632.90s -> 639.14s]  something that's the batch dimension by m by n so effectively they're kind of
[639.14s -> 642.90s]  more I think the full semantics are written out on the PyTorch website for
[642.90s -> 646.50s]  how the matrix multiplication works but you're right you don't just have these
[646.50s -> 649.74s]  cases where you have two two-dimensional tensors you can have
[649.74s -> 654.22s]  arbitrary number of dimensions and as long as the dimensions match up based on
[654.22s -> 658.42s]  those semantics I was saying then you can multiply it alternatively you can do
[658.42s -> 661.70s]  what I do which is just multiply it anyways and then if it throws an error
[661.70s -> 666.50s]  print out the shapes and kind of work from there that tends to be faster in
[666.50s -> 674.78s]  my opinion a lot of ways but yeah that's a good point all right so yeah
[674.78s -> 679.30s]  let's keep going through some of the other different functionalities here so
[679.30s -> 683.90s]  we can define another tensor and kind of one of the key things that we always
[683.90s -> 689.48s]  want to look at is the shape so in this case we just have a 1d tensor of
[689.48s -> 695.66s]  length 3 so the torch dot size just gives us three in general this is kind
[695.66s -> 698.62s]  of one of the key debugging steps and something that I'll try and
[698.62s -> 702.90s]  emphasize a lot throughout this session which is printing the shapes of
[702.94s -> 706.82s]  all of your tensors is probably your best resource when it comes to debugging
[706.82s -> 710.90s]  it's kind of one of the hardest things to intuit exactly what's going
[710.90s -> 715.38s]  on once you start stacking a lot of different operations together so
[715.38s -> 718.74s]  printing out the shapes that each point and seeing do they match what
[718.74s -> 724.34s]  you expect is something important and it's better to rely on that than just
[724.34s -> 728.30s]  on the error message that PyTorch gives you because under the hood PyTorch
[728.42s -> 733.50s]  might implement certain optimizations and actually reshape the underlying tensor
[733.50s -> 737.02s]  you have so you may not see the numbers you expect so it's always great
[737.02s -> 745.46s]  to print out the shape and so yeah let's so again we can always print
[745.46s -> 751.50s]  out the shape and we can have a more complex in this case a three
[751.50s -> 756.58s]  dimensional tensor which is three by two by four and we can print out the
[756.62s -> 762.18s]  shape and we can see all the dimensions here and so now you're like okay great
[762.18s -> 766.38s]  we have tensors we can look at their shapes but what do we actually do with
[766.38s -> 770.26s]  them and so now let's get into kind of what are the operations that we can
[770.26s -> 776.78s]  apply to these tensors and so one of them is it's very easy to reshape
[776.78s -> 784.98s]  tensors so in this case we're creating this 15 dimensional tensor that's the
[784.98s -> 791.82s]  numbers 1 to 15 and now we're reshaping it so now it's 5 by 3 tensor here and
[791.82s -> 797.26s]  so you might wonder well like what's what's the point of that and it's
[797.26s -> 801.74s]  because a lot of times when we are doing machine learning we actually
[801.74s -> 805.02s]  want to learn in batches and so we might take our data and we might
[805.02s -> 809.42s]  reshape it so now that instead of kind of being a long flat and list of
[809.42s -> 813.70s]  things we actually have a set of batches or in in some cases we have a
[813.78s -> 819.58s]  set of batches of a set of sentences or sequences of a particular length and
[819.58s -> 823.82s]  each of the elements in that sequence has an embedding of a particular
[823.82s -> 828.14s]  dimension and so based on the types of operations that you're trying to do
[828.14s -> 833.14s]  you'll sometimes need to reshape those tensors and sometimes you'll want
[833.14s -> 837.86s]  to particularly sometimes transpose dimensions if you want to for
[837.86s -> 844.62s]  instance reorganize your data so that's another operation to keep in mind I
[844.62s -> 850.98s]  believe the difference is view will view will create a view of the
[850.98s -> 854.66s]  underlying tensor and so I think the underlying tensor will still have this
[854.66s -> 863.90s]  same shape reshape will actually modify the tensor all right and then
[863.94s -> 869.10s]  finally like I said at the beginning your intuition about PyTorch tensors can
[869.10s -> 874.78s]  simply be there kind of a nice easy way to work with NumPy arrays but they
[874.78s -> 879.82s]  have all these great properties like now we can essentially use them with
[879.82s -> 885.50s]  GPUs and it's very optimized and we can also compute gradients quickly and
[885.50s -> 889.50s]  to kind of just emphasize this point if you have some NumPy code and you
[889.50s -> 893.34s]  have a bunch of NumPy arrays you can directly convert them into PyTorch
[893.34s -> 899.10s]  tensors by simply casting them and you can also take those tensors and
[899.10s -> 902.78s]  convert them back to NumPy arrays
[903.46s -> 908.54s]  all right and so one of the things you might be asking is why do we care
[908.54s -> 913.70s]  about tensors what makes them good and one of the great things about them
[913.70s -> 918.62s]  is that they support vectorized operations very easily essentially we
[918.62s -> 922.70s]  can parallelize a lot of different computations and do them for instance
[922.70s -> 927.18s]  across a batch of data all at once and one of those operations you might want
[927.18s -> 934.66s]  to do for instance is a sum so you can take in this case a tensor which is
[934.66s -> 942.98s]  shaped five by seven and it looks like that's not working you can take a
[942.98s -> 946.90s]  tensor that's shaped five by seven and now you can compute different
[946.90s -> 951.66s]  operations on it that essentially collapse the dimensionality so the first
[951.74s -> 956.14s]  one is sum and so you can take it and you can sum across both the rows as
[956.14s -> 960.06s]  well as the columns and so one way I like to think about this to kind of
[960.06s -> 965.26s]  keep them straight is that the dimension that you specify in the sum
[965.26s -> 970.46s]  is the dimension you're collapsing so in this case if you take the data and sum
[970.46s -> 974.98s]  over dimension zero because you know the shape of the underlying tensor is
[974.98s -> 980.46s]  five by seven you've collapsed the zeroth dimension so you should be left
[980.46s -> 984.86s]  with something that's just shaped seven and if you see the actual tensor you
[984.86s -> 990.62s]  got 75 80 85 90 you get this tensor which is shaped seven
[990.62s -> 993.66s]  alternatively you can think about whether or not you're kind of summing
[993.66s -> 998.54s]  across the rows or summing across the columns but it's not just some it
[998.54s -> 1002.46s]  applies to other operations as well you can compute standard deviations you
[1002.46s -> 1006.66s]  can normalize your data you can do other operations which essentially batch
[1006.66s -> 1012.78s]  across the entire set of data and not only do these apply over one dimension
[1012.78s -> 1017.14s]  but here you can see that if you don't specify any dimensions then by default
[1017.14s -> 1021.42s]  the operation actually applies to the entire tensor so here we end up just
[1021.42s -> 1025.10s]  taking the sum of the entire thing so if you think about it the zeroth
[1025.10s -> 1028.46s]  dimension is the number of rows there are five rows and there are seven
[1028.46s -> 1036.12s]  columns so if we sum out the rows then we're actually summing across the
[1036.12s -> 1041.36s]  columns and so now we only have seven values but I like to think about more
[1041.36s -> 1044.60s]  just in terms of the dimensions to keep it straight rather than rows or columns
[1044.60s -> 1048.52s]  because it can get confusing if you're summing out dimension zero then
[1048.52s -> 1052.20s]  effectively you've taken something which has some shape that's dimension
[1052.20s -> 1057.36s]  zero by dimension one to just whatever is the dimension one shape and then
[1057.36s -> 1060.98s]  from there you can kind of figure out okay which way did I actually sum to
[1060.98s -> 1066.06s]  check if you were right. NumPy implements a lot of this vectorization and
[1066.06s -> 1071.06s]  I believe in the homework that you have right now I think part of your job
[1071.06s -> 1075.06s]  is to vectorize a lot of these things so the big advantage with PyTorch
[1075.06s -> 1079.62s]  is that essentially it's optimized to be able to take advantage of your GPU
[1079.62s -> 1084.02s]  when we actually start building out neural networks that are bigger that
[1084.02s -> 1087.26s]  involve more computation we're going to be doing a lot of these matrix
[1087.30s -> 1091.78s]  multiplication operations that it's going to be a lot better for our processor if
[1091.78s -> 1096.06s]  we can make use of the GPU and so that's where PyTorch really comes in
[1096.06s -> 1101.94s]  handy in addition to also defining a lot of those neural network modules as
[1101.94s -> 1106.02s]  we'll see later for you so that now you don't need to worry about for
[1106.02s -> 1111.10s]  instance implementing a basic linear layer and back propagation from scratch
[1111.10s -> 1115.30s]  and also your optimizer all of those things will be built in and you can
[1115.34s -> 1119.98s]  just call the respective API's to make use of them whereas in Python and
[1119.98s -> 1131.14s]  NumPy you might have to do a lot of that coding yourself yeah alright so we'll
[1131.14s -> 1139.26s]  keep going so this is a quiz except I think it tells you the answer so it's
[1139.26s -> 1144.90s]  not much of a quiz but pretty much you know what would you do if now I told
[1144.90s -> 1149.62s]  you instead of you know summing over this tensor I want you to compute the
[1149.62s -> 1153.66s]  average and so there's there's two different ways you could compute the
[1153.66s -> 1157.54s]  average you could compute the average across the rows or across the
[1157.54s -> 1163.66s]  columns and so essentially now we kind of get back to this question of
[1163.66s -> 1167.52s]  well which dimension am I actually going to reduce over and so here if we
[1167.52s -> 1172.02s]  want to preserve the rows then we need to actually sum over the second
[1172.02s -> 1178.70s]  dimension they're really the first 0th and first so the first dimension is
[1178.70s -> 1183.50s]  what we had to sum over because we want to preserve the zeroth dimension
[1183.50s -> 1188.90s]  and so that's why for row average you see the dim equals 1 and for column
[1188.90s -> 1194.82s]  average same reasoning is why you see the dim equals 0 and so if we run
[1194.82s -> 1200.50s]  this code we'll see kind of what are the shapes that we expect if we're
[1200.50s -> 1204.86s]  taking the average over rows then an object that's two by three should just
[1204.86s -> 1210.42s]  become an object that's two it's just a one-dimensional almost vector you can
[1210.42s -> 1215.68s]  think of and if we are averaging across the columns there's three columns
[1215.68s -> 1219.94s]  so now our average should have three values and so now we're left with a
[1219.94s -> 1226.58s]  three to a one-dimensional tensor of length three so yeah does that kind of
[1226.58s -> 1229.50s]  make sense I guess is this general intuition about how we deal with
[1229.74s -> 1234.34s]  shapes and how some of these operations manipulate shapes so now we'll get into
[1234.34s -> 1241.50s]  indexing this can get a little bit tricky but I think you'll find that the
[1241.50s -> 1248.26s]  semantics are very similar to numpy so one of the things that you can do in
[1248.26s -> 1251.70s]  numpy is that you can take these numpy arrays and you can slice across them
[1251.70s -> 1257.18s]  in many different ways you can create copies of them and you can index
[1257.22s -> 1261.78s]  across particular dimensions to select out different elements different rows or
[1261.78s -> 1267.14s]  different columns and so in this case let's take this example tensor which is
[1267.14s -> 1273.94s]  three by two by two and first thing you always want to do when you have a
[1273.94s -> 1277.78s]  new tensor print out its shape understand what you're working with and
[1277.98s -> 1287.54s]  so I guess I may have shown this already but what will X bracket zero print out
[1287.54s -> 1294.30s]  what happens if we index into just the first element what's the shape of this
[1295.50s -> 1300.26s]  yeah two by two right because if you think about it our tensor is really
[1300.26s -> 1304.68s]  just a list of three things each of those things happens to also be a two by
[1304.68s -> 1309.92s]  two tensor so we get a two by two object in this case the first thing one
[1309.92s -> 1316.24s]  two three four and so just like numpy if you provide a colon in a particular
[1316.24s -> 1322.24s]  dimension it means essentially copy over that dimension so if we do X
[1322.24s -> 1325.72s]  bracket zero implicitly we're essentially putting a colon for all
[1325.72s -> 1330.92s]  the other dimensions so it's essentially saying grab the first thing along the
[1330.92s -> 1334.28s]  zeroth dimension and then grab everything along the other two
[1334.28s -> 1342.12s]  dimensions if we now take just the zeroth along the element along the
[1342.12s -> 1349.52s]  first dimension what are we going to get well ultimately we're going to get
[1349.52s -> 1355.76s]  now if you look the kind of first dimension where these three things the
[1355.76s -> 1360.44s]  second dimension is now each of these two rows within those things so like
[1360.44s -> 1365.08s]  one two and three four five six and seven eight nine ten and eleven twelve
[1365.08s -> 1370.44s]  so if we index into the second dimension or the first dimension and
[1370.44s -> 1375.40s]  get the zeroth element then we're going to end up with one two five six
[1375.40s -> 1382.20s]  and nine ten and even if that's a little bit tricky you can kind of go
[1382.20s -> 1387.32s]  back to the trick I mentioned before where we're slicing across the first
[1387.32s -> 1391.40s]  dimension so if we look at the shape of our tensor it's three by two by two
[1391.40s -> 1396.40s]  if we collapse the first dimension that two in the middle we're left with
[1396.40s -> 1402.04s]  something that's three by two so it might seem a little bit trivial kind
[1402.04s -> 1404.96s]  of going through this in a lot of detail but I think it's important
[1404.96s -> 1409.28s]  because it can get tricky when your tensor shapes get more complicated how
[1409.28s -> 1413.52s]  to actually reason about this and so I won't go through every example here
[1413.56s -> 1418.64s]  since a lot of them kind of reinforce the same thing but I'll just highlight a few
[1418.64s -> 1426.68s]  things just like numpy you can choose to get a range of elements in this case
[1426.68s -> 1434.12s]  we're taking this new tensor which is one two one through fifteen rearranged
[1434.12s -> 1439.92s]  as a five by three tensor and if we take the zero through third row
[1439.92s -> 1445.84s]  exclusive will get the first three rows and we can do the same thing but now
[1445.84s -> 1452.80s]  with slicing across multiple dimensions and I think the final point I want to
[1452.80s -> 1459.12s]  talk about here is list indexing list indexing is also present in numpy and
[1459.12s -> 1463.84s]  it's a very clever shorthand for being able to essentially select out
[1463.88s -> 1470.40s]  multiple elements at once so in this case what you can do is if you want to
[1470.40s -> 1476.64s]  get the zero the second and fourth element of our matrix you can just
[1476.64s -> 1481.64s]  instead of indexing with a particular number or set of numbers index with a
[1481.64s -> 1489.08s]  list of indices so in this case if we go up to our tensor if we take out
[1489.08s -> 1495.64s]  the zeroth the second and the fourth we should see those three rows and that's
[1495.64s -> 1498.28s]  what we end up getting
[1500.28s -> 1505.24s]  yeah again these are kind of a lot of examples to just reiterate the same
[1505.24s -> 1509.32s]  point which is that you can slice across your data in multiple ways and
[1509.32s -> 1514.68s]  at different points are going to need to do that so being familiar with the
[1514.68s -> 1518.52s]  shapes that you understand what's the underlying output that you expect is
[1518.52s -> 1523.40s]  important in this case for instance we're slicing across the first and the
[1523.40s -> 1530.04s]  second dimension and we're keeping the first the zeroth and so we're going to
[1530.04s -> 1534.00s]  end up getting essentially kind of the top left element of each of those
[1534.00s -> 1539.64s]  three things in our tensor if we scroll all the way up here we'll get
[1539.64s -> 1544.56s]  this one we'll get this five and we'll get this nine because we go across
[1544.56s -> 1548.44s]  all of the zeroth dimension and then across the first and the second we only
[1548.44s -> 1554.56s]  take the first the zeroth element in both of those positions and so that's
[1554.56s -> 1564.08s]  why we get one five nine and also of course you can you know apply all
[1564.08s -> 1568.44s]  of the colons to get back the original tensor
[1568.44s -> 1576.56s]  okay and then I think the last thing when it comes to indexing is
[1576.56s -> 1581.64s]  conversions so typically when we're writing code with neural networks
[1581.64s -> 1586.84s]  ultimately we're going to you know process some data through a network and
[1586.84s -> 1590.72s]  we're going to get a loss and that loss needs to be a scalar and then
[1590.72s -> 1594.48s]  we're going to compute gradients with respect to that loss so one thing
[1594.52s -> 1598.60s]  to keep in mind is that sometimes you might have an operation and it fails
[1598.60s -> 1602.48s]  because it was actually expecting a scalar value rather than a tensor and
[1602.48s -> 1607.76s]  so you can extract out the scalar from this one by one tensor by just
[1607.76s -> 1613.56s]  calling dot item so in this case you know if you have a tensor which is
[1613.56s -> 1617.72s]  just literally one then you can actually get the Python scalar that
[1617.72s -> 1621.76s]  corresponds to it by calling dot item so now we can get into the more
[1621.76s -> 1627.12s]  interesting stuff one of the really cool things with PyTorch is AutoGrad and
[1627.12s -> 1633.04s]  what AutoGrad is is PyTorch essentially provides an automatic
[1633.04s -> 1639.00s]  differentiation package where when you define your neural network you're
[1639.00s -> 1645.56s]  essentially defining many nodes that compute some function and in the
[1645.56s -> 1648.88s]  forward pass you're kind of running your data through those nodes but what
[1648.92s -> 1653.16s]  PyTorch is doing on the back end is that each of those points it's going to
[1653.16s -> 1657.60s]  actually store the gradients and accumulate them so that every time you
[1657.60s -> 1662.28s]  do your backwards pass you apply the chain rule to be able to calculate all
[1662.28s -> 1667.72s]  these different gradients and PyTorch caches those gradients and then you
[1667.72s -> 1671.16s]  will have access to all of those gradients to be able to actually then
[1671.16s -> 1676.90s]  run your favorite optimizer and optimize you know with SGD or with
[1676.94s -> 1682.34s]  Adam or whichever optimizer you choose and so that's kind of one of the great
[1682.34s -> 1686.10s]  features you don't have to worry about actually writing the code that computes
[1686.10s -> 1690.46s]  all these gradients and actually caches all of them properly applies the
[1690.46s -> 1694.42s]  chain rule does all these steps you can abstract all of that away with just
[1694.42s -> 1700.02s]  one call to dot backward and so in this case we'll run through a little
[1700.02s -> 1703.98s]  bit of an example where we'll see the gradients getting computed
[1703.98s -> 1713.38s]  automatically so in this case we're going to initialize a tensor and
[1713.38s -> 1718.10s]  requires grad is true by default it just means that by default for a given
[1718.10s -> 1725.44s]  tensor Python PyTorch will store the gradient associated with it and you
[1725.44s -> 1730.66s]  might wonder well you know why why why do we have this you know when we
[1730.66s -> 1735.06s]  always want to store the gradient and the answer is it trained time you need
[1735.06s -> 1738.50s]  the gradients in order to actually train your network but at inference
[1738.50s -> 1742.02s]  time you'd actually want to disable your gradients and you can actually do
[1742.02s -> 1745.74s]  that because it's a lot of extra computation that's not needed since
[1745.74s -> 1751.94s]  you're not making any updates to your network anymore and so let's create
[1751.94s -> 1757.18s]  this right now we don't have any gradients being computed because we
[1757.22s -> 1764.18s]  haven't actually called backwards to actually compute some quantity with
[1764.18s -> 1769.02s]  respect to this particular tensor we haven't actually computed those
[1769.02s -> 1773.86s]  gradients yet so right now the dot grad feature which will actually store the
[1773.86s -> 1778.86s]  gradient associated with that tensor is none and so now let's just define a
[1778.86s -> 1784.38s]  really simple function we have X we're going to define the function Y equals
[1784.42s -> 1791.70s]  3x squared and so now we're going to call Y dot backward and so now what
[1791.70s -> 1796.62s]  happens is when we actually print out X dot grad what we should expect to see
[1796.62s -> 1804.00s]  is number 12 and the reason is that our function Y is 3x squared if we
[1804.00s -> 1809.82s]  compute the gradient of that function we're going to get 6x and our actual
[1809.86s -> 1817.22s]  value was 2 so the actual gradient is going to be 12 and we see that when we
[1817.22s -> 1824.02s]  print out X dot grad that's what we get and now we'll just run it again let's
[1824.02s -> 1829.34s]  set Z equal to 3x squared we call Z dot backwards and we print out X dot
[1829.34s -> 1837.06s]  grad again and now we see that I may not run this in the right order okay
[1837.06s -> 1844.06s]  so here in the second one that I re-ran we see that it says 24 and so you
[1844.06s -> 1847.42s]  might be wondering well I just did the same thing twice shouldn't I see 12
[1847.42s -> 1852.42s]  again and the answer is that by default PyTorch will accumulate the
[1852.42s -> 1857.30s]  gradients so it won't actually rewrite the gradient each time you
[1857.30s -> 1861.82s]  compute it it will sum it and the reason is because when you actually have
[1861.82s -> 1866.04s]  back propagation for your network you want to accumulate the gradients you
[1866.04s -> 1869.40s]  know across all of your examples and then actually apply your update you
[1869.40s -> 1873.96s]  don't want to overwrite the gradient but this also means that every time you
[1873.96s -> 1877.72s]  have a training iteration for your network you need to zero out the
[1877.72s -> 1881.88s]  gradient because you don't want the previous gradients from the last epoch
[1881.88s -> 1886.28s]  where you iterated through all of your training data to mess with the
[1886.28s -> 1890.44s]  current update that you're doing so that's kind of one thing to note
[1890.48s -> 1896.40s]  which is that that's essentially why we will see when we actually write the
[1896.40s -> 1900.24s]  training loop you have to run zero grad in order to zero out the gradient
[1900.24s -> 1905.92s]  yes so I accidentally ran the cells in the wrong order maybe to make it
[1905.92s -> 1913.92s]  more clear let me put this one first so this is actually what it
[1913.92s -> 1917.80s]  should look like which is that we ran it once and I ran this cell first and
[1917.84s -> 1924.80s]  it has 12 and then we ran it a second time and we get 24 yes so if you have
[1924.80s -> 1930.00s]  all of your tensors defined then when you actually call dot backwards if it's a
[1930.00s -> 1933.64s]  function of multiple variables it's going to compute all of those
[1933.64s -> 1938.16s]  partials all those gradients yeah so what's happening here is that the way
[1938.16s -> 1944.68s]  pytorch works is that it's storing the accumulate accumulated gradient at X
[1944.68s -> 1949.64s]  and so we've essentially made two different backwards passes we've called
[1949.64s -> 1954.92s]  it once on this function Y and we've which is a function of X and we've
[1954.92s -> 1959.12s]  called it once on Z which is also a function of X and so you're right we
[1959.12s -> 1962.12s]  can't actually disambiguate which came from what we just see the
[1962.12s -> 1967.16s]  accumulated gradient but typically that's actually exactly what we want
[1967.16s -> 1971.92s]  because what we want is to be able to run our network and accumulate the
[1971.92s -> 1976.28s]  gradient across all of the training examples that define our loss and then
[1976.28s -> 1980.48s]  perform our optimizer step so yeah even with respect to one thing it doesn't
[1980.48s -> 1984.00s]  matter because in practice each of those things is really a different
[1984.00s -> 1987.96s]  example in our set of training examples and so we're not interested in
[1987.96s -> 1991.08s]  you know the gradient from one example we're actually interested in the overall
[1991.08s -> 1996.96s]  gradient so going back to this example what's happening here is that in
[1997.00s -> 2003.04s]  the backwards pass what it's doing is you can imagine there's the X tensor and
[2003.04s -> 2006.60s]  then there's the dot grad attribute which is another separate tensor it's
[2006.60s -> 2011.64s]  going to be the same shape as X and what that is storing is it's storing
[2011.64s -> 2016.24s]  the accumulated gradient from every single time that you've called dot
[2016.24s -> 2022.32s]  backward on a quantity that essentially has some dependency on X that will have
[2022.36s -> 2026.80s]  a nonzero gradient and so the first time we call it the gradient will be 12
[2026.80s -> 2033.32s]  because 6x 6 times 2 12 the second time we do it with Z it's also still 12
[2033.32s -> 2037.44s]  but the point is that dot grad doesn't actually overwrite the gradient
[2037.44s -> 2041.36s]  each time you call dot backwards it simply adds them it accumulates them
[2041.36s -> 2045.92s]  and kind of the intuition there is that ultimately you're going to want to
[2045.92s -> 2051.44s]  compute the gradient with respect to the loss and that loss is going to be
[2051.44s -> 2055.24s]  made up of many different examples and so you need to accumulate the gradient
[2055.24s -> 2059.00s]  from all of those in order to make a single update and then of course
[2059.00s -> 2062.52s]  you'll have to zero that out because every time you make one pass
[2062.52s -> 2066.48s]  through all of your data you don't want that next batch of data to also
[2066.48s -> 2069.44s]  be double counting the previous batches update you want to keep those
[2069.44s -> 2075.80s]  separate and so we'll see that in a second yeah
[2076.64s -> 2083.76s]  all right so now we're going to move on to one of the final pieces of the puzzle
[2083.76s -> 2089.12s]  which is neural networks how do we actually use them in PyTorch and once
[2089.12s -> 2093.44s]  we have that and we have our optimization we'll finally be able to
[2093.44s -> 2096.28s]  figure out how do we actually train a neural network what does that look like
[2096.28s -> 2103.36s]  and why it's so clean and efficient when you do it in PyTorch so the first
[2103.44s -> 2107.88s]  thing that you want to do is we're going to be defining neural networks in terms
[2107.88s -> 2113.80s]  of existing building blocks in terms of existing API's which will implement for
[2113.80s -> 2118.12s]  instance linear layers or different activation functions that we need so
[2118.12s -> 2122.16s]  we're going to import torch dot NN because that is the neural network
[2122.16s -> 2126.24s]  package that we're going to make use of and so let's start with the linear
[2126.24s -> 2131.36s]  layer the way the linear layer works in PyTorch is it takes in two
[2131.40s -> 2136.32s]  arguments it takes in the input dimension and then the output dimension
[2136.32s -> 2143.56s]  and so pretty much what it does is it takes in some input which has some
[2143.56s -> 2149.68s]  arbitrary amount of dimensions and then finally the input dimension and it will
[2149.68s -> 2153.88s]  essentially output it to that same set of dimensions except the output
[2153.88s -> 2160.16s]  dimension in the very last place and you can think of the linear layer as
[2160.20s -> 2167.60s]  essentially just performing a simple AX plus B by default it's going to it's
[2167.60s -> 2171.28s]  going to apply a bias but you can also disable that if you don't want a bias
[2171.28s -> 2185.60s]  term and so let's look at a small example so so here we have our input
[2186.04s -> 2192.84s]  and we're going to create a linear layer in this case as an input size of four
[2192.84s -> 2200.16s]  and output size of two and all we're going to do is once we define it by
[2200.16s -> 2204.48s]  instantiating with NN dot linear whatever the name of our layer is in
[2204.48s -> 2208.80s]  this case we called it linear we just essentially apply it with parentheses as
[2208.80s -> 2214.40s]  if it were a function to whatever input and that actually does the actual
[2214.40s -> 2223.64s]  forward pass through this linear layer to get our output and so you can see
[2223.64s -> 2227.76s]  that the original shape was two by three by four then we pass it through this
[2227.76s -> 2231.44s]  linear layer which has an output dimension of size two and so ultimately
[2231.44s -> 2236.64s]  our output is two by three by two which is good that's what we expect
[2236.64s -> 2242.02s]  that's not shape error but you know something common that you'll see is you
[2242.02s -> 2250.14s]  know maybe you decide to you get a little confused and maybe you do let's
[2250.14s -> 2256.98s]  say two by two you match the wrong dimension and so here we're going to
[2256.98s -> 2261.74s]  get a shape error and you see that the error message isn't as helpful because
[2261.74s -> 2264.34s]  it's actually changed the shape of what we were working with we said this
[2264.34s -> 2268.58s]  was two by three by four under the hood PyTorch has changed this to a six by
[2268.58s -> 2274.70s]  four but if we you know in this case it's obvious because we instantiated it
[2274.70s -> 2279.50s]  with the shape but if we didn't have the shape then one simple thing we could
[2279.50s -> 2282.98s]  do is actually just print out the shape and we'd see okay this last
[2282.98s -> 2287.42s]  dimension is size four so I actually need to change my input dimension in my
[2287.42s -> 2297.94s]  linear layer to be size four and you'll also notice on this output we have
[2297.94s -> 2302.42s]  this grad function and so that's because we're actually computing and storing the
[2302.42s -> 2306.70s]  gradients here for our tensor
[2311.86s -> 2316.62s]  yeah so typically we think of the first dimension as the batch dimension
[2316.62s -> 2320.22s]  so in this case it said and this you can think of as if you had a batch of
[2320.22s -> 2323.82s]  images it would be the number of images if you had a training corpus of
[2323.82s -> 2330.98s]  text it would be essentially the number of sentences or sequences pretty much
[2330.98s -> 2334.34s]  that is usually considered the batch dimension the star in the case of
[2334.34s -> 2338.58s]  there can be an arbitrary number of dimensions so for instance if we had
[2338.58s -> 2344.10s]  images this could be a four-dimensional tensor object it could be the batch
[2344.10s -> 2348.54s]  size by the number of channels by the height by the width but in general
[2348.54s -> 2352.94s]  there's no fixed number of dimensions your input tensor can be any number of
[2352.98s -> 2357.30s]  dimensions the key is just that that last dimension needs to match up with the
[2357.30s -> 2363.88s]  input dimension of your linear layer the two is the output size so essentially
[2363.88s -> 2369.06s]  we're saying that we're going to map this last dimension which is
[2369.06s -> 2373.50s]  four-dimensional to now two-dimensional so in general you know you can think of
[2373.50s -> 2377.34s]  this is if we're stacking the neural network this is kind of the input
[2377.34s -> 2384.82s]  dimension size and this would be like the hidden dimension size and so one
[2384.82s -> 2387.64s]  thing we can do is we can actually print out the parameters and we can
[2387.64s -> 2391.56s]  actually see what are the values of our linear layer or in general for any
[2391.56s -> 2396.10s]  layer that we define in our neural network what are the actual parameters
[2396.10s -> 2402.74s]  and in this case we see that there's two sets of parameters because we have
[2402.78s -> 2410.50s]  a bias as well as the actual the actual linear layer itself and so both of them
[2410.50s -> 2417.74s]  store the gradients and in this case you know these are these are what the
[2417.74s -> 2421.30s]  current values of these parameters are and they'll change as we train the
[2421.30s -> 2431.98s]  network okay so now let's go through some of the other module layers so in
[2432.02s -> 2436.34s]  general nn.linear is one of the layers you have access to you have a
[2436.34s -> 2439.98s]  couple of other different layers that are pretty common you have 2d
[2439.98s -> 2443.94s]  convolutions you have transpose convolutions you have batch norm
[2443.94s -> 2447.58s]  layers when you need to do normalization in your network you can
[2447.58s -> 2451.06s]  do up sampling you can do max pooling you can do lots of different
[2451.06s -> 2454.62s]  operators but the main key here is that all of them are built-in building
[2454.62s -> 2460.46s]  blocks that you can just call just like we did with nn.linear and so
[2460.46s -> 2464.94s]  let's just go I guess I'm running out of time but let's just try and go
[2464.94s -> 2468.58s]  through these last few layers and then I'll wrap up by kind of showing an
[2468.58s -> 2473.36s]  example that puts it all together so in this case we can define an
[2473.36s -> 2477.50s]  activation function which is typical with our networks we need to introduce
[2477.50s -> 2481.98s]  nonlinearities in this case we use the sigmoid function and so now we can
[2481.98s -> 2485.66s]  define our our network is this very simple thing which had one linear layer
[2485.70s -> 2492.38s]  and then an activation and in general when we compose these layers together
[2492.38s -> 2497.82s]  we don't need to actually write every single line by line applying the next
[2497.82s -> 2501.86s]  layer we can actually stack all of them together in this case we can use
[2501.86s -> 2506.06s]  nn.sequential and list all the layers so here we have our linear
[2506.06s -> 2511.86s]  layer followed by our sigmoid and then now we're just essentially passing
[2511.90s -> 2515.98s]  the input through this whole set of layers all at once so we take our input
[2515.98s -> 2523.06s]  we call block on the input and we get the output and so let's just kind of
[2523.06s -> 2525.94s]  see putting it all together what does it look like to define a network and
[2525.94s -> 2529.98s]  what does it look like when we train one so here we're going to actually
[2529.98s -> 2534.42s]  define a multi-layer perceptron and the way it works is to define a neural
[2534.42s -> 2539.26s]  network you extend the nn.module class the key here is there's really
[2539.34s -> 2543.02s]  two main things you have to define when you create your own network one is the
[2543.02s -> 2546.94s]  initialization so in the init function you actually initialize all the
[2546.94s -> 2551.82s]  parameters you need in this case we initialize an input size hidden size
[2551.82s -> 2556.70s]  and we actually define the model itself in this case it's a simple model
[2556.70s -> 2562.30s]  which consists of a linear layer followed by an activation followed by
[2562.30s -> 2566.62s]  another linear layer followed by a final activation and the second
[2566.62s -> 2570.22s]  function we have to define is the forward which actually does the forward
[2570.22s -> 2575.10s]  pass of the network and so here our forward function takes in our input x
[2575.10s -> 2579.90s]  in general it could take in some arbitrary amount of inputs into this
[2579.90s -> 2584.34s]  function but essentially it needs to figure out how are you actually
[2584.34s -> 2587.74s]  computing the output and in this case it's very simple we just pass it
[2587.74s -> 2595.34s]  into the network that we just defined and return the output and again you
[2595.34s -> 2598.98s]  could do this more explicitly by kind of doing what we did earlier where we
[2598.98s -> 2602.62s]  could actually write out all of the layers individually instead of wrapping
[2602.62s -> 2609.70s]  them into one object and then doing a line-by-line operation for each one of
[2609.70s -> 2616.14s]  these layers and so finally if we define our class it's very simple to
[2616.14s -> 2621.22s]  use it we can now just instantiate some input instantiate our model by
[2621.22s -> 2625.22s]  calling multi-layer perceptron with our parameters and then just pass it
[2625.22s -> 2632.54s]  through our model so that's great but this is all just the forward pass how
[2632.54s -> 2636.22s]  do we actually train the network how do we actually make it better and so
[2636.22s -> 2641.54s]  this is the final step which is we have optimization built in to PyTorch so
[2641.54s -> 2644.70s]  we have this backward function which goes and computes all these gradients
[2644.70s -> 2648.38s]  in the backward pass and now the only step left is to actually update the
[2648.38s -> 2653.22s]  parameters using those gradients and so here we'll import the torch.optim
[2653.26s -> 2659.26s]  package which contains all the optimizers that you need essentially
[2659.26s -> 2663.34s]  this part is just creating some random data so that we can actually
[2663.34s -> 2668.74s]  decide how to fit our data but this is really the key here which is we'll
[2668.74s -> 2675.18s]  instantiate our model that we define we'll define the atom optimizer and
[2675.18s -> 2679.06s]  we'll define it with a particular learning rate we'll define a loss
[2679.06s -> 2682.70s]  function which is again another built-in module in this case we're using the
[2682.70s -> 2688.18s]  cross-entropy loss and finally to calculate our predictions all we do is
[2688.18s -> 2692.78s]  simply is just call model on our actual input and to calculate our loss
[2692.78s -> 2697.18s]  we just call our loss function on our predictions and our true labels
[2697.18s -> 2703.26s]  and we extract the scalar here and now when we put it all together this is
[2703.26s -> 2707.14s]  what the training loop looks like we have some number of epochs that we want
[2707.14s -> 2711.46s]  to train our network for each of these epochs the first thing we do is
[2711.46s -> 2715.02s]  we take our optimizer and we zero out the gradient and the reason we do that
[2715.02s -> 2719.34s]  is because like many of you noted we actually are accumulating the gradient
[2719.34s -> 2723.06s]  we're not resetting it every time we call dot backward so we zero out the
[2723.06s -> 2728.98s]  gradient we get our model predictions by doing a forward pass we then compute
[2728.98s -> 2734.98s]  the loss between the predictions and the true values finally we call loss dot
[2734.98s -> 2738.90s]  backward this is what actually computes all the gradients in the backward
[2738.90s -> 2745.26s]  pass from our loss and the final step is we call dot step on our optimizer in
[2745.26s -> 2749.66s]  this case we're using Adam and this will take a step on our loss function
[2749.66s -> 2754.74s]  and so if we run this code we end up seeing that we're able to start with
[2754.74s -> 2758.94s]  some training loss which is relatively high and in 10 epochs were able to
[2758.94s -> 2764.58s]  essentially completely fit our data and if we print out our model
[2764.58s -> 2768.02s]  parameters and we printed them out from the start as well we'd see that they've
[2768.02s -> 2773.42s]  changed as we've actually done this optimization and so I'll kind of wrap it
[2773.42s -> 2778.22s]  up here but I think the key takeaway is that a lot of the things that you're
[2778.22s -> 2781.90s]  doing at the beginning of this class are really about understanding the basics
[2781.90s -> 2785.86s]  of how neural networks work how you actually implement them how you
[2785.86s -> 2789.70s]  implement the backward pass the great thing about pytorch is that once you
[2789.70s -> 2792.74s]  get to the very next assignment you'll see that now that you have a good
[2792.74s -> 2796.42s]  underlying understanding of those things you can abstract a lot of the
[2796.46s -> 2800.78s]  complexity of how do you do backprop how do you store all these gradients how do
[2800.78s -> 2805.18s]  you compute them how do you actually run the optimizer and let pytorch handle
[2805.18s -> 2808.66s]  all that for you and you can use all of these building blocks all these
[2808.66s -> 2813.22s]  different neural network layers to now define your own networks that you can
[2813.22s -> 2816.74s]  use to solve whatever problems you need
