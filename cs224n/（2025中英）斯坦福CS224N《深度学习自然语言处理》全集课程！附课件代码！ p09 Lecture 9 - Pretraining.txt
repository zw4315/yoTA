# Detected language: en (p=1.00)

[0.00s -> 13.28s]  Hello! Welcome to CS224n! Today we'll be talking about pre-training, which is another
[13.28s -> 19.88s]  exciting topic on the road to modern natural language processing.
[19.88s -> 29.52s]  Okay. How is everyone doing? Thumbs up, some side, some down. Wow. No response bias
[29.52s -> 34.20s]  there. All thumbs up. Oh, side. Nice. I like that honesty. That's good.
[34.20s -> 43.00s]  Okay. So we're now, what is this, week five? Yes, it's week five. And we have a couple,
[43.00s -> 48.76s]  so this lecture, the Transformers lecture, and then to a lesser extent Thursday's lecture
[48.76s -> 55.40s]  on natural language generation will be sort of the sum of lectures for the assignments
[55.40s -> 63.60s]  you have to do. So assignment five is coming out on Thursday, and the topics covered in
[63.60s -> 67.96s]  this lecture and the self-attentioned Transformers and again a little bit of natural language
[67.96s -> 72.28s]  generation will be tested in assignment five, and then the rest of the course will
[72.28s -> 77.50s]  go through some really fascinating topics in sort of modern natural language processing
[77.50s -> 83.00s]  that should be useful for your final projects and future jobs and interviews and intellectual
[83.08s -> 91.64s]  curiosity. But I think that today's lecture is significantly less technical and detailed
[91.64s -> 97.44s]  than last Thursday's on self-attention and Transformers, but should give you an idea of
[97.44s -> 104.68s]  the sort of world of pre-training and sort of how it helps define natural language
[104.68s -> 110.32s]  processing today. So a reminder about assignment five, your project proposals also are due on
[110.44s -> 116.92s]  Tuesday, next Tuesday. Please do get those in, try to get them in on time so that we can
[116.92s -> 123.44s]  give you prompt feedback about your project proposals. And yeah, so let's jump into it.
[123.44s -> 134.28s]  Okay, so what we're going to start with today is a bit of a technical detail on
[134.28s -> 139.48s]  word structure and sort of how we model the input sequence of words that we get.
[140.24s -> 147.64s]  When we were teaching Word2vec and sort of all the methods that we've talked about so far,
[147.64s -> 152.52s]  we assumed a finite vocabulary, right? So you had a vocabulary V that you define via whatever,
[152.52s -> 157.00s]  you've looked at some data, you've decided what the words are in that data. And so,
[157.68s -> 165.12s]  you have some words like hat and learn, and you have this embedding, it's in red,
[165.12s -> 168.92s]  because you've learned it properly. Actually, let's replace hat and learn with pizza and
[168.92s -> 176.04s]  tasty. Those are better. And so that's all well and good. You see these words in your model,
[176.04s -> 183.32s]  and you have an embedding that's been learned on your data to sort of know what to do when
[183.32s -> 188.16s]  you see those words. But when you see some sort of variations, maybe you see like tasty,
[188.16s -> 195.40s]  and maybe a typo like learn, or maybe novel items where it's like a word that you as a
[195.48s -> 201.84s]  human can understand as sort of this combination. This is called derivational morphology of this
[201.84s -> 208.72s]  word transformer that you know, and if I, which means take this noun and give me back a verb
[208.72s -> 215.32s]  that means to make more like that noun. To transformerify NLP might mean to make NLP more
[215.32s -> 221.68s]  like using transformers and such. And for each of these, this maybe didn't show up in your
[221.72s -> 227.08s]  training corpus. And language is always doing this. People are always coming up with new words,
[227.08s -> 232.72s]  and there's new domains, and there's the young people are always making new words. It's great.
[232.72s -> 236.72s]  And so it's a problem for your model though, right? Because you've defined this finite
[236.72s -> 242.64s]  vocabulary, and there's sort of no mapping in that vocabulary for each of these things. Even
[242.64s -> 247.12s]  though their meanings should be relatively well-defined based on the data you've seen so
[247.12s -> 252.80s]  far, it's just that the sort of string of characters that define them aren't quite what you've seen.
[252.80s -> 258.40s]  And so what do you do? Well, maybe you map them to this sort of universal unknown tokens.
[258.40s -> 262.40s]  This is UNK, right? So it's like, I see something I don't know what, I've never seen
[262.40s -> 267.60s]  it before. I'm gonna say it's always represented by the same token UNK. And so that's been done
[267.60s -> 272.80s]  in the past. And that's sort of bad, right? Because it's totally like losing tons of
[272.80s -> 276.56s]  information. But, you know, you need to map it to something.
[278.56s -> 284.40s]  And so this is like a clear problem. Especially, I mean, in English it's a problem. In many of
[284.40s -> 291.04s]  the rules languages, it's a substantially larger problem, right? So, you know, English has
[291.60s -> 295.92s]  relatively simple word structure. There's a couple of conjugations for each verb,
[295.92s -> 303.92s]  like, you know, eat, eats, eaten, ate. But in a language with much more complex morphology,
[303.92s -> 311.28s]  or word structure, you'll have a considerably more complex sort of set of things that you
[311.28s -> 316.64s]  can see in the world. So here is a a conjugation table for a Swahili verb,
[317.44s -> 322.96s]  and it has over 300 conjugations. And if I defined a vocabulary to be every unique
[322.96s -> 328.72s]  string of characters maps to its own word, then every one of the 300 conjugations would get an
[328.72s -> 334.64s]  independent vector under my model. Which makes no sense, because the 300 conjugations
[334.64s -> 340.40s]  obviously have a lot in common, and differ by sort of meaningful extents. So you don't
[340.40s -> 346.24s]  want to do this. I'd have to have a huge vocabulary if I wanted all conjugations to show up.
[346.24s -> 351.92s]  And that's a mistake for efficiency reasons and for learning reasons. Any questions so far?
[353.92s -> 365.36s]  Cool. Okay, and so what we end up doing is we'll look at sub-word structure,
[365.36s -> 369.04s]  sub-word modeling. So what we're going to do is we're going to say, I'm not going to even try
[369.04s -> 376.08s]  to define what the set of all words is. I'm going to define my vocabulary to include
[376.64s -> 377.68s]  parts of words.
[386.24s -> 391.20s]  So I'm going to split words into sequences of known sub-words. And so there's a simple
[391.20s -> 396.40s]  sort of algorithm for this, where you start with all characters. So if I only had a
[396.40s -> 404.16s]  vocabulary of all characters, and maybe like an end-of-word symbol for a finite data set,
[404.16s -> 408.80s]  then no matter what word I saw in the future, as long as I had seen all possible characters,
[408.80s -> 412.00s]  I could take the word and say, I don't know what this word is. I'm going to split it into
[412.00s -> 416.24s]  all of its individual characters. So you won't have this unk problem. You can sort of represent
[416.24s -> 422.24s]  any word. And then you're going to find common adjacent characters and say, okay, A and B co-occur
[422.24s -> 426.24s]  next to each other quite a bit, so I'm going to add a new word to my vocabulary.
[426.96s -> 431.36s]  Now it's all characters plus this new word AB, which is a sub-word.
[432.32s -> 437.12s]  And likewise, so now I'm going to replace the character pair with a new sub-word and repeat
[437.12s -> 441.52s]  until you add a lot, a lot, a lot of vocabulary items through this process of what things tend
[441.52s -> 447.36s]  to co-occur next to each other. And so what you'll end up with is a vocabulary
[447.36s -> 452.16s]  of very commonly co-occurring sort of substrings by which you can build up words.
[452.96s -> 456.16s]  And this was originally developed for machine translation, but then has been used
[456.96s -> 460.96s]  considerably in pretty much all modern language models.
[460.96s -> 466.00s]  So now we have a hat and learn, hat and learn. So in our sub-word vocabulary,
[466.00s -> 470.72s]  hat and learn showed up enough that they're their own individual words. So that's
[470.72s -> 476.48s]  sort of good, right? So simple common words show up as a word in your vocabulary,
[476.48s -> 481.04s]  just like you'd like them to, but now tasty maybe gets split into T-A-A.
[481.04s -> 484.88s]  And then maybe, you know, in some cases, this hash hash means like,
[484.88s -> 492.00s]  don't add a space next, right? So T-A-A and then A-A-A and then S-T-Y, right?
[492.00s -> 496.24s]  So I've actually taken one sort of thing that seems like a word and in my vocabulary,
[496.24s -> 502.96s]  it's now split into three sub-word tokens. So when I pass this to my transformer or to
[502.96s -> 506.88s]  my recurrent neural network, right, the recurrent neural network would take T-A-A
[507.52s -> 515.04s]  as just a single element, do the RNN update, and then take A-A-A, do the RNN update, and then S-T-Y.
[515.04s -> 520.80s]  So it could learn to process constructions like this, and maybe I can even add more
[520.80s -> 524.80s]  A-A-As in the middle, right, and have it do something similar, instead of just seeing
[524.80s -> 534.00s]  the entire word tasty and not knowing what it means. Is that? That's feedback, yeah.
[536.88s -> 544.40s]  How loud is that feedback? Are we good? Okay, I think we're fixed. Great.
[546.40s -> 551.12s]  And so same with transformer, if I maybe transformer is its own word, and then if I,
[551.12s -> 555.68s]  and so you can see that you have sort of three learned embeddings instead of one sort
[555.68s -> 561.28s]  of useless UNK embedding. This is just wildly useful and is used pretty much everywhere.
[561.28s -> 567.20s]  Variants of this algorithm are used pretty much everywhere in modern NLP. Questions? Yes.
[568.56s -> 572.40s]  If we have three embeddings for tasty, do we just add them together?
[573.20s -> 576.40s]  The question is, if we have three embeddings for tasty, do we just add them together?
[578.00s -> 582.40s]  If we want to represent, so when we're actually processing the sequence,
[582.40s -> 591.04s]  I'd see something like, I learned about the T-A-A-A-A-S-T-Y, so it'd actually be totally
[591.04s -> 596.00s]  separate tokens, but if I wanted to then say, what's my representation of this thing?
[597.36s -> 602.08s]  It depends on what you want to do. Sometimes you average the contextual representations of
[602.08s -> 608.40s]  the three, or look at the last one maybe. At that point it's unclear what to do, but
[608.40s -> 618.96s]  everything sort of works okay. How do you know where to split based on the algorithm that I
[619.84s -> 625.76s]  specified earlier for learning the vocabulary? You've learned this vocabulary by just combining
[625.76s -> 631.20s]  commonly co-occurring adjacent strings of letters. A-B co-occurred a lot, so now I've
[631.20s -> 636.40s]  got a new word that's A-B. Then when I'm actually walking through and tokenizing,
[636.40s -> 640.08s]  I try to split as little as possible, so I split words into the maximal
[640.88s -> 644.00s]  sort of sub-word that takes up the most characters. There are algorithms for this.
[645.04s -> 650.32s]  Yeah, so I'm like, okay, if I want to split this up, there's many ways I could split it
[650.32s -> 654.32s]  up, and you try to find some approximate, what the best way to split it into the
[654.32s -> 664.48s]  fewest words is. The question is, do people use punctuation in the character set?
[664.48s -> 670.40s]  Do people do it? Yes, absolutely. Sort of from this point on,
[672.00s -> 678.32s]  just assume that what text is given to these models is as unprocessed as possible. You try
[678.32s -> 683.44s]  to make it sort of clean-looking text, where you've removed HTML tags maybe if it's from the
[683.44s -> 689.92s]  internet or whatever, but then beyond that you process it as little as possible so that it
[689.92s -> 695.92s]  reflects as well as possible what people might actually be using this for. So maybe earlier
[695.92s -> 700.64s]  in the course when we were looking at Word2vec, maybe we might have thought about, oh, we don't
[700.64s -> 706.64s]  want Word2vec vectors of punctuation or something like that. Now everything is just
[707.20s -> 712.00s]  as close as possible to what the text you'd get with people trying to use your system would be.
[712.00s -> 717.20s]  So yes, in practice punctuation and like dot dot dot might be its own word, you know,
[717.68s -> 723.12s]  and maybe a sequence of hyphens because people make big bars across tables.
[727.76s -> 735.84s]  How does it impact one word now? It could be multiple embeddings versus a single embedding.
[735.84s -> 743.36s]  Does the system treat those any differently? The question is, does the system treat any
[743.36s -> 748.24s]  differently words that are really themselves a whole word versus words that are sort of pieces?
[748.24s -> 755.36s]  No, the system has no idea. They're all just indices into your embedding vocabulary matrix,
[756.24s -> 757.92s]  so they're all treated equally.
[761.28s -> 765.04s]  What about really long words that are, I guess, relatively common? Because if you're
[765.04s -> 768.56s]  building up some character all the way up, what happens then?
[769.20s -> 772.72s]  Yeah, the question is, what happens to very long words if you're building up from
[773.44s -> 779.28s]  character pairs and portions of characters? In practice the statistics speak really
[779.92s -> 784.64s]  well for themselves, so if a long word is very common it will end up in the vocabulary,
[784.64s -> 790.56s]  and if it's not very common it won't. There are algorithms that aren't this that do slightly
[790.56s -> 796.80s]  better in various ways, but the intuition that you sort of figure out what the common
[796.80s -> 801.92s]  co-occurring substrings are, sort of independent of length almost, is the right intuition to have.
[802.72s -> 806.00s]  You can actually just look at the learned vocabularies of a lot of these models,
[806.56s -> 812.16s]  and you see some long words just because if they showed up a lot.
[816.88s -> 822.96s]  I'm curious, how does it weigh the frequency? So let's say there's like if-y,
[822.96s -> 829.92s]  or in your next slide it was like if-by at the very last one. So if could be really common,
[830.00s -> 835.12s]  how does it weigh the frequency of a subword versus the length of it? It tries to spread it
[835.12s -> 839.84s]  up into the smallest number, but what if it split it up into three, but one of them was super
[839.84s -> 847.60s]  common? The question is, if transformer is a subword in my vocabulary, and if is a subword,
[847.60s -> 854.48s]  and y is a subword, and if-y as a three-letter tuple is also a subword, how does it choose
[854.48s -> 861.36s]  to take it? Maybe it's not very common as opposed to splitting it into more subwords.
[862.88s -> 867.12s]  It's just a choice. We choose to try to take the smallest number of subwords, because that tends
[867.12s -> 873.04s]  to be more of the bottleneck as opposed to having a bunch of very common, very short subwords.
[874.72s -> 879.28s]  Sequence length is a big problem in transformers, and this seems to be sort of what works.
[879.28s -> 883.84s]  Although trying to split things into multiple options of a sequence and running the transformer
[883.84s -> 886.88s]  on all of them is a thing that people have done to see which one will work better.
[887.68s -> 892.08s]  But yeah, having fewer, bigger subwords tends to be the best sort of idea. I'm going to start
[892.08s -> 899.36s]  moving on, though. Feel free to ask me more questions about this afterward. Okay, so let's
[899.36s -> 904.80s]  talk about pre-training from the context of the course so far. At the very beginning of the
[904.80s -> 909.04s]  course, we gave you this quote, which was, you know, you shall know a word by the company
[909.04s -> 914.08s]  it keeps. This was the sort of thesis of the distributional hypothesis, that the meaning
[914.08s -> 919.36s]  of the word is defined by, or at least reflected by, what words it tends to co-occur
[919.36s -> 926.64s]  around, and we implemented this via Word2vec. The same person who made that quote had a
[926.64s -> 934.48s]  separate quote, actually earlier, that continues this notion of meaning as defined by context,
[934.48s -> 939.76s]  which has something along the lines of, well, you know, since the word shows up in context when
[939.76s -> 945.12s]  we actually use it, when we speak to each other, the meaning of the word should be defined in the
[945.12s -> 950.56s]  context that it actually shows up in. And so, you know, the complete meaning of a word is always
[950.56s -> 955.76s]  contextual, and no study of meaning apart from a complete context can be taken seriously.
[955.84s -> 961.68s]  So, right, the big difference here is, like, at Word2vec training time, if I have
[962.64s -> 969.68s]  the word record, R-E-C-O-R-D, when I'm training Word2vec, I get one vector, or two, but,
[970.32s -> 979.12s]  you know, one vector meaning record, the string, and it has to learn by what context
[979.12s -> 986.56s]  it shows up in, that sometimes, you know, it can mean I record, i.e. the verb, or record, i.e.
[986.56s -> 991.68s]  the noun, right, but I only have one vector to represent it. And so when I use the Word2vec
[991.68s -> 998.32s]  embedding of record, it sort of has this mixture meaning of both of its sort of senses,
[998.32s -> 1003.36s]  right? It doesn't get to specialize and say, oh, this part means record, and this part
[1003.36s -> 1009.52s]  means record. And so Word2vec is going to just sort of fail, and so I can build better
[1009.52s -> 1014.24s]  representations of language through these contextual representations that are going to
[1014.24s -> 1018.32s]  take things like recurrent neural networks or transformers that we used before to build up
[1018.32s -> 1027.52s]  sort of contextual meaning. So what we had before were pre-trained word embeddings, and
[1027.52s -> 1034.72s]  then we had sort of a big box on top of it like a transformer or an LSTM that was not pre-trained,
[1034.72s -> 1040.96s]  right? So you learn via context your word embeddings here, and then you have a task
[1040.96s -> 1046.72s]  like sentiment analysis or machine translation or parsing or whatever, and you initialize
[1046.72s -> 1051.52s]  all the parameters of this randomly, and then you train to predict your label.
[1051.52s -> 1059.36s]  And the big difference in today's work is that we're going to try to pre-train all the parameters.
[1059.36s -> 1065.36s]  So I have my big transformer, and instead of just pre-training my word embeddings with Word2vec,
[1065.36s -> 1068.16s]  I'm going to train all of the parameters of the network,
[1070.72s -> 1077.44s]  trying to teach it much more about language that I could use in my downstream tasks.
[1077.44s -> 1085.52s]  So now the labeled data that I have for, say, machine translation might need to be smaller.
[1085.52s -> 1089.92s]  I might not need as much of it because I've already trained much more of the network
[1089.92s -> 1093.36s]  than I otherwise would have if I had just gotten sort of Word2vec embeddings.
[1095.68s -> 1101.60s]  Okay, so here I've pre-trained this entire sort of structure, the word embeddings,
[1101.60s -> 1106.96s]  the transformer on top. Everything's been trained via methods that we'll talk about today,
[1106.96s -> 1111.44s]  and so what does this give you? I mean, it gives you very strong representations of language.
[1111.44s -> 1118.64s]  So the meaning of record and record will be different in the sort of contextual representations
[1118.64s -> 1123.84s]  that know where in the sequence it is and what words are co-occurring with it in this specific
[1123.84s -> 1128.72s]  input than Word2vec, which only has one representation for record independent of
[1128.72s -> 1134.80s]  where it shows up. It'll also be used as strong parameter initializations for NLP models.
[1134.80s -> 1139.84s]  So in all of your homework so far, you've worked with building out a natural language
[1139.84s -> 1143.52s]  processing system sort of from scratch, right? Like, how do I initialize this weight matrix?
[1143.52s -> 1148.96s]  And we always say, oh, you know, small, normally distributed noise, like little values
[1150.80s -> 1156.08s]  close to zero. And here we're going to say, well, just like we were going to use the
[1156.08s -> 1160.72s]  Word2vec embeddings and those sort of encoded structure, I'm going to start maybe my machine
[1160.80s -> 1166.40s]  translation system from a parameter initialization that's given to me via pre-training.
[1167.68s -> 1171.92s]  And then also it's going to give us probability distributions of our language that we can use to
[1171.92s -> 1177.04s]  generate and otherwise, and we'll talk about this. Okay, so whole models are going to be
[1177.04s -> 1183.84s]  pre-trained. So all of pre-training is effectively going to be centered around this idea of
[1183.84s -> 1188.72s]  reconstructing the input. So you have an input, it's a sequence of text that some human
[1188.72s -> 1196.64s]  has generated. And the sort of hypothesis is that by masking out part of it and tasking
[1196.64s -> 1202.40s]  a neural network with reconstructing the original input, that neural network has to learn a lot
[1202.40s -> 1207.92s]  about language, about the world, in order to do a good job of reconstructing the input, right?
[1207.92s -> 1213.76s]  So this is now a supervised learning problem, just like machine translation, right? I've taken
[1213.76s -> 1218.56s]  this sentence that just existed, Stanford University is located in, say, Palo Alto,
[1218.56s -> 1226.80s]  California, or Stanford, California, I guess. And I have, by removing this part of the sentence,
[1228.08s -> 1233.60s]  made a label for myself, right? The input is this sort of broken masked sentence,
[1233.60s -> 1241.92s]  and the label is Stanford, or Palo Alto. So if I give this example to a network and ask it
[1241.92s -> 1246.72s]  to predict the center thing, as it's doing its gradient step on this input, it's going to
[1246.72s -> 1252.08s]  encode information about the co-occurrence between this context, Stanford University is located in,
[1252.08s -> 1257.12s]  and Palo Alto. So by tasking it with this, it might learn, say, where Stanford is.
[1258.16s -> 1262.88s]  What else might it learn? Well, it can learn things about, maybe, syntax. So I put blank
[1262.88s -> 1268.08s]  fork down on the table. Here, there's only a certain set of words that could go here.
[1268.08s -> 1271.76s]  I put the fork down on the table. I put a fork down on the table.
[1271.76s -> 1277.84s]  These are syntactic constraints, right? So the context shows me what kinds of words
[1277.84s -> 1284.16s]  can appear in what kinds of contexts. The woman walked across the street checking
[1284.16s -> 1290.00s]  for traffic over blank shoulder. Any ideas on what could go here? Her, right?
[1290.00s -> 1296.56s]  So this sort of co-reference between this entity, who is being discussed in the world,
[1296.72s -> 1302.00s]  this woman and her shoulder. Now, when I discuss this sort of linguistic concept,
[1302.00s -> 1305.60s]  the word her here is a co-referent to woman, right? It's referring to the same
[1305.60s -> 1309.60s]  entity in the discourse. And so the network might be able to learn things about
[1311.04s -> 1318.40s]  what entities are doing what where. It can learn things about semantics.
[1318.40s -> 1322.64s]  So if I went to the ocean to see the fish, turtles, seals, and blank,
[1322.64s -> 1327.04s]  then the word that's in the blank should be sort of a member of the class that I'm thinking
[1327.04s -> 1332.08s]  of as a person writing this sentence of stuff that I see when I go to the ocean and see these
[1332.08s -> 1336.48s]  other things as well, right? So in order to do this prediction task, maybe I'll learn about
[1337.28s -> 1344.40s]  the semantics of aquatic creatures. Okay, so what else could I learn?
[1344.96s -> 1348.88s]  Overall, the value I got from the two hours watching it was the sum total of the popcorn
[1348.88s -> 1354.16s]  and drink. The movie was blank. What kind of task could I be learning from doing this
[1354.16s -> 1360.40s]  sort of prediction problem? Sentiment, exactly. So this is just a naturalistic
[1360.40s -> 1367.84s]  sort of text that I naturally wrote myself. But by saying, oh, the movie was bad,
[1368.56s -> 1373.84s]  I'm learning about sort of the latent sentiment of the person who wrote this,
[1373.84s -> 1375.60s]  what they were feeling about the movie at the time.
[1376.08s -> 1380.32s]  So maybe if I see a new review later on, I can just paste in the review,
[1380.32s -> 1385.52s]  say the movie was blank, and if the model generates bad or good,
[1386.08s -> 1389.52s]  that could be implicitly solving the task of sentiment analysis.
[1392.88s -> 1395.76s]  So here's another one. Iroh went to the kitchen to make some tea.
[1395.76s -> 1400.08s]  Standing next to Iroh, Zuko pondered his destiny. Zuko left the blank.
[1400.32s -> 1405.92s]  Okay, so in this scenario, we've got a world implicitly that's been designed by the person
[1405.92s -> 1411.52s]  who is creating this text. I've got physical locations in the discourse, like the kitchen,
[1412.40s -> 1416.72s]  and I've got Zuko. We've got Iroh in the kitchen. Zuko's next to Iroh.
[1417.92s -> 1423.92s]  So Zuko must be in the kitchen. So what could Zuko leave but the kitchen?
[1424.00s -> 1430.48s]  So what could Zuko leave but the kitchen? And so in terms of latent notions of embodiment and
[1430.48s -> 1434.88s]  physical location, the way that people talk about people being next to something and then
[1434.88s -> 1440.64s]  leaving something could tell you stuff about sort of, yeah, a little bit about how the
[1440.64s -> 1448.00s]  world works even. So here's the sequence. I was thinking about the sequence that goes one,
[1448.00s -> 1456.24s]  one, two, three, five, eight, 13, 21, blank. And this is a pretty tough one, right?
[1457.84s -> 1462.24s]  This is the Fibonacci sequence, right? If you had a model by looking at a bunch of numbers
[1462.24s -> 1467.52s]  from the Fibonacci sequence, learn to in general predict the next one. It's a question
[1467.52s -> 1473.44s]  you should be thinking about throughout the lecture. Okay, any questions on these sort
[1473.44s -> 1476.48s]  of examples of what you might learn from predicting the context?
[1481.52s -> 1488.72s]  Okay, okay, cool. So a very simple way to think about pre-training is pre-training is language
[1488.72s -> 1492.64s]  modeling. So we saw language modeling earlier in the course, and now we're just going to say,
[1493.44s -> 1497.44s]  instead of using my language model just to provide probabilities over the next word,
[1497.44s -> 1500.64s]  I am going to train it on that task, right? I'm going to actually model
[1501.44s -> 1508.48s]  the distribution p theta of the word t, given all the words previous.
[1509.92s -> 1514.48s]  And there's a ton of data for this, right? There's an amazing amount of data for this
[1514.48s -> 1518.56s]  in a lot of languages, especially English. There's very little data for this in actually
[1518.56s -> 1523.12s]  most of the world's languages, which is a separate problem. But you can pre-train just
[1523.12s -> 1527.04s]  through language modeling, right? So I'm going to sort of do the teacher forcing things,
[1527.04s -> 1532.56s]  so I have IRL, I predict goes, I have goes, I predict two, and I'm going to train my sort
[1532.56s -> 1537.04s]  of LSTM or my transformer to do this task, and then I'm just going to keep all the weights.
[1538.48s -> 1540.32s]  Okay, I'm going to save all the network parameters.
[1544.16s -> 1547.76s]  And then once I have these parameters, right, instead of generating from my language model,
[1547.76s -> 1553.68s]  I'm just going to use them as an initialization for my parameters. So I have this pre-training,
[1553.68s -> 1559.92s]  fine-tuning paradigm, two steps. Most of you, I think, in your, well, maybe not this year,
[1559.92s -> 1563.76s]  let's say a large portion of you this year in your final projects will be doing the pre-training,
[1563.76s -> 1567.44s]  fine-tuning sort of paradigm where someone has done the pre-training for you, right? So
[1567.44s -> 1573.20s]  you have a ton of text, you learn very general things about the distribution of words and sort
[1573.20s -> 1578.08s]  of the latent things that that tells you about the world and about language. And then in step
[1578.08s -> 1583.92s]  two, you've got some task, maybe sentiment analysis, and you have maybe not very many
[1583.92s -> 1589.76s]  labels. You have a little bit of labeled data and you adapt the pre-trained model to the task
[1589.76s -> 1595.12s]  that you care about by further doing gradient steps on this task. So you give it the movie
[1595.12s -> 1602.24s]  was, you predict happy or sad, and then you sort of continue to update the parameters based
[1602.24s -> 1608.64s]  on the initialization from the pre-training. And this just works exceptionally well. I mean,
[1608.64s -> 1613.60s]  unbelievably well compared to training from scratch. Intuitively, because you've taken a lot
[1613.60s -> 1618.32s]  of the burden of learning about language, learning about the world off of the data that
[1618.32s -> 1622.56s]  you've labeled for sentiment analysis. And you're sort of giving that task of learning all
[1622.56s -> 1627.12s]  this sort of very general stuff to the much more general task of language modeling. Yes.
[1627.12s -> 1632.16s]  You said we didn't have much data in other languages. What do you mean by data? Is it just
[1632.80s -> 1635.84s]  text in that language? Is it labeled in some way?
[1636.48s -> 1640.72s]  The question is, you said we have a lot of data in English, but not in other languages.
[1642.16s -> 1645.92s]  What do you mean by data that we don't have a lot of in other languages? Is it just text?
[1645.92s -> 1651.84s]  It's literally just text, no annotations, because you don't need annotations to do language
[1651.84s -> 1656.00s]  model pre-training, right? The existence of that sequence of words that someone has written
[1657.20s -> 1663.68s]  provides you with all these pairs of input and output. Input iro, output goes. Input iro goes,
[1663.68s -> 1669.36s]  output two. Those are all labels sort of that you've constructed from the input just existing.
[1669.36s -> 1675.36s]  But in most languages, even on the entire internet, I mean, there's about 7,000-ish languages
[1675.36s -> 1682.16s]  on earth. And most of them don't have the sort of billions of words you might want
[1682.48s -> 1683.76s]  to train these systems on.
[1691.76s -> 1693.60s]  The question is, if you're pre-training the entire thing,
[1693.60s -> 1697.68s]  do you still learn one vector representation per word? You learn one vector representation
[1697.68s -> 1702.88s]  that is the non-contextual input vector. So you have your vocabulary matrix,
[1702.88s -> 1708.80s]  you've got your embedding matrix that is vocabulary sized by model dimensionality.
[1708.80s -> 1714.00s]  And so, yeah, iro has one vector, goes has one vector. But then the transformer that
[1714.00s -> 1718.16s]  you're learning on top of it takes in the sequence so far and sort of gives
[1718.16s -> 1721.60s]  a vector to each of them that's dependent on the context in that case.
[1721.60s -> 1724.88s]  But still, at the input, you only have one embedding per word.
[1728.08s -> 1732.24s]  Yeah, so what sort of metrics would you use to evaluate a pre-trained model? It's supposed to
[1732.24s -> 1736.40s]  be general, right? But there's application-specific metrics, which one do you use?
[1736.96s -> 1740.16s]  Yeah, so the question is, what metric do you use to evaluate pre-trained models,
[1740.16s -> 1744.96s]  since it's supposed to be so general, but there are lots of very specific evaluations you could
[1744.96s -> 1750.80s]  use? We'll get into a lot of that in the rest of the lecture. While you're training it,
[1750.80s -> 1754.72s]  you can use simple metrics that sort of correlate with what you want but aren't actually what you
[1754.72s -> 1760.32s]  want, just like the probability quality, right? So you can evaluate the perplexity of
[1760.32s -> 1763.44s]  your language model, just like you would have when you cared about language modeling.
[1763.44s -> 1768.24s]  And it turns out to be the case that better perplexity correlates with all the stuff that's
[1768.24s -> 1773.52s]  much harder to evaluate, like lots and lots of different tasks. But also, the natural language
[1773.52s -> 1779.84s]  processing community has built very large sort of benchmark suites of varying tasks to try to
[1779.84s -> 1783.76s]  get at sort of a notion of generality, although that's very, very difficult. It's sort of
[1783.76s -> 1788.80s]  ill-defined, even. And so when you develop new pre-training methods, what you often do is
[1788.80s -> 1793.28s]  you try to pick a whole bunch of evaluations and show that you do better on all of them,
[1793.28s -> 1801.04s]  you know? And that's your argument for generality. Okay, so why should this sort
[1801.04s -> 1808.72s]  of pre-training, fine-tuning, two-part paradigm help? This is still an open area of research,
[1808.72s -> 1813.92s]  but the intuitions are all you're going to take from this course. So pre-training provides
[1813.92s -> 1819.44s]  some sort of starting parameters, L theta, so this is like all the parameters in your network,
[1819.44s -> 1824.40s]  right? From trying to do this minimum over all possible settings of your parameters of the
[1824.40s -> 1831.12s]  pre-training loss. And then the fine-tuning process takes your data for fine-tuning,
[1831.12s -> 1836.24s]  you've got some labels, and it tries to approximate the minimum through gradient descent
[1836.24s -> 1841.52s]  of the loss of the fine-tuning task of theta, but you start at theta hat, right? So you
[1841.52s -> 1846.80s]  start gradient descent at theta hat, which your pre-training process gave you, and then
[1848.16s -> 1854.16s]  if you could actually solve this min and wanted to, it sort of feels like the starting point
[1854.16s -> 1863.04s]  shouldn't matter, but it really, really, really does. It really does. And we'll talk a bit more
[1863.04s -> 1869.12s]  about this later, but the process of gradient descent, maybe it sticks relatively close to the
[1869.12s -> 1875.84s]  theta hat during fine-tuning, right? So you start at theta hat and then you walk
[1875.84s -> 1880.80s]  downhill with gradient descent until you hit a valley, and that valley ends up being really
[1880.80s -> 1885.12s]  good because it's close to the pre-training parameters, which were really good for a lot
[1885.12s -> 1891.44s]  of things. This is a cool place where practice and theory are meeting, where optimization
[1891.44s -> 1897.20s]  people want to understand why this is so useful, and LP people just want to build better systems.
[1897.20s -> 1904.64s]  So yeah, maybe the stuff around theta hat tends to generalize well. If you want to
[1904.64s -> 1907.36s]  work on this kind of thing, you should talk about it. Yeah?
[1907.36s -> 1911.52s]  So if stochastic gradient descent sticks relatively close,
[1911.52s -> 1916.08s]  but what if we were to use a different optimizer? How would that change our results?
[1916.08s -> 1920.16s]  The question is, if stochastic gradient descent sticks relatively close,
[1920.16s -> 1922.80s]  what if we use a different optimizer? I mean, if we use sort of any
[1923.44s -> 1927.68s]  common variant of gradient descent, like any first order method, like Adam,
[1927.68s -> 1933.28s]  which we use in this course, or AdaGrad, they all have this very, very similar properties.
[1934.72s -> 1939.60s]  Other types of optimization we just tend to not use, so who knows. Yeah?
[1939.60s -> 1944.72s]  Yeah, that sounds a little unclear on why the pre-training plus fine-tuning works better than
[1944.72s -> 1949.52s]  just fine-tuning, but making it more powerful, adding more layers, more data, etc.
[1949.52s -> 1953.44s]  Yeah, the question is, why does the pre-trained fine-tuned paradigm
[1953.44s -> 1958.00s]  work better than just making the model more powerful, adding more layers, adding more data
[1958.00s -> 1966.32s]  to just the fine-tuning? The simple answer is that you have orders of magnitude more data
[1966.32s -> 1973.92s]  that's unlabeled. That's just text that you found. Then you do carefully labeled data and
[1973.92s -> 1978.32s]  the tasks that you care about, right? Because that's expensive to get. It has to be examples
[1978.32s -> 1981.76s]  of your movie reviews or whatever, that you've had someone label carefully.
[1983.12s -> 1991.92s]  So you have something like, on the internet, at least five trillion, maybe ten trillion words
[1991.92s -> 1997.68s]  of this, and you have maybe a million words of your labeled data or whatever over here.
[1997.68s -> 2002.80s]  So it's just the scale is way off. But there's also an intuition that
[2003.68s -> 2009.92s]  learning to do a very, very simple thing like sentiment analysis is not going to get you
[2010.88s -> 2018.88s]  a very generally able agent in a wide range of settings compared to language modeling.
[2018.88s -> 2023.84s]  So it's hard to get, how do I put it? Even if you have a lot of labeled data
[2023.84s -> 2026.80s]  of movie reviews of the kind that people are writing today,
[2028.56s -> 2031.60s]  maybe tomorrow they start writing slightly different kinds of movie reviews,
[2031.60s -> 2036.32s]  and your system doesn't perform as well. Whereas if you pre-trained on a really diverse set of
[2036.32s -> 2042.88s]  text from a wide range of sources and people, it might be more adaptable to seeing stuff
[2042.88s -> 2046.08s]  that doesn't quite look like the training data you showed it, even if you showed it a ton
[2046.08s -> 2051.84s]  of training data. So one of the big takeaways of pre-training is that you get this huge
[2051.84s -> 2056.96s]  amount of variety of text on the internet, and you have to be very careful.
[2056.96s -> 2061.28s]  I mean, you should be very careful about what kind of text you're showing it and what
[2061.28s -> 2066.32s]  kind of text you're not, because the internet is full of awful text as well.
[2067.76s -> 2072.00s]  But some of that generality just comes from how hard this problem is and how much data you
[2072.00s -> 2073.28s]  can show it.
[2073.84s -> 2080.48s]  Is this a pre-trained model that's trying to have so much data? How do you then train it so that
[2080.48s -> 2085.12s]  it considers the stuff that you're fine-tuning it with as more important, more salient to the
[2085.12s -> 2089.68s]  task it's trying to do, rather than just one in a billion articles of data?
[2090.48s -> 2094.48s]  Yeah, that's a good question. So the question is, given that the amount of data on the
[2094.48s -> 2098.16s]  pre-training side is orders of magnitude more than the amount of data on the fine-tuning side,
[2098.16s -> 2102.16s]  how do you sort of get across to the model that, okay, actually the fine-tuning task
[2102.16s -> 2107.12s]  is what I care about, so focus on that. It's about the fact that I did this first,
[2107.12s -> 2113.20s]  the pre-training first, and then I do the fine-tuning second, right? So I've gotten
[2113.20s -> 2118.32s]  my parameter initialization from this, I've set it somewhere, and then I fine-tune, I move to
[2118.32s -> 2123.84s]  where the parameters are doing well for this task afterward. And so, well, it might just
[2124.40s -> 2128.72s]  forget a lot about how to do this, because now I'm just asking it to do this at this point.
[2129.68s -> 2135.92s]  I should move on, I think. But we're going to keep talking about this in much more detail
[2135.92s -> 2145.60s]  with more concrete elements. Okay, so let's talk about model pre-training. Oh, wait.
[2146.96s -> 2149.28s]  That did not advance the slides.
[2153.68s -> 2157.04s]  Nice, okay. Let's talk about model pre-training three ways.
[2157.92s -> 2164.08s]  In our Transformers lecture Tuesday, we talked about encoders, encoder decoders,
[2164.08s -> 2169.92s]  and decoders. And we'll do decoders last, because actually many of the largest models
[2170.56s -> 2175.68s]  that are being used today are all decoders, and so we'll have a bit more to say about them.
[2176.88s -> 2181.36s]  Right, so let's just recall these three. So encoders get bidirectional context.
[2181.36s -> 2185.84s]  You have a single sequence, and you're able to see the whole thing, kind of like an encoder
[2185.84s -> 2192.80s]  in machine translation. Encoder decoders have one portion of the network that gets
[2192.80s -> 2197.84s]  bidirectional context, so that's like the source sentence of my machine translation system. And
[2197.84s -> 2202.32s]  then they're sort of paired with a decoder that gets unidirectional context, so that I
[2202.32s -> 2207.28s]  have this sort of informational masking where I can't see the future, so that I can do things
[2207.28s -> 2211.52s]  like language modeling. I can generate the next token of my translation, whatever. So you could
[2211.52s -> 2216.08s]  think of it as, you know, I've got my source sentence here, and my partial translation here,
[2216.08s -> 2221.36s]  and I'm sort of decoding out the translation. And then decoders only are things like language
[2221.36s -> 2225.92s]  models. We've seen a lot of this so far, and there's pre-training for all three sort of
[2225.92s -> 2232.24s]  large classes of models, and how you pre-train them and then how you use them depends on
[2232.24s -> 2237.04s]  the properties and the proclivities of the specific architecture. So let's look at encoders
[2237.04s -> 2243.12s]  first. So we've looked at language modeling quite a bit, but we can't do language modeling with an
[2243.12s -> 2250.56s]  encoder because they get bidirectional context. So if I'm down here at i, and I want to
[2250.56s -> 2257.36s]  predict the next word, it's a trivial task at this level here to predict the next
[2257.36s -> 2262.88s]  word, because in the middle, I was able to look at the next word, and so I should just know.
[2262.88s -> 2266.32s]  There's nothing hard about learning to predict the next word here, because I could just look at
[2266.32s -> 2272.72s]  it, see what it is, and then copy it over. So when I'm training an encoder in something
[2272.72s -> 2278.08s]  for pre-training, I have to be a little bit more clever. In practice, what I do is
[2278.08s -> 2283.44s]  something like this. I take the input, and I modify it somewhat. I mask out words sort of like
[2283.44s -> 2289.36s]  I did in the examples I gave at the beginning of class. So I blank to the blank. And then
[2289.36s -> 2295.12s]  I have the network predict with its whole... I have it build contextual representations,
[2295.12s -> 2299.84s]  so now this vector representation of the blank sees the entire context around it
[2301.04s -> 2309.84s]  here, and then I predict the word went, and then here the word store. Any questions?
[2314.08s -> 2318.80s]  Okay, and you can see how this is doing something quite a bit like language modeling,
[2318.80s -> 2323.36s]  but with, you know, bidirectional context. I've removed the network's information
[2323.36s -> 2326.96s]  about the words that go in the blanks, and I'm training it to reconstruct that.
[2327.60s -> 2332.64s]  So I only have loss terms, but I only ask it to actually do the prediction, compute the loss,
[2332.64s -> 2337.44s]  backpropagate the gradients for the words that I've masked out. And you can think of this
[2337.44s -> 2342.96s]  as, you know, instead of learning probability of X, where X is like a sentence or a document,
[2342.96s -> 2348.64s]  this is learning the probability of X, the real document, given X tilde, which is this sort
[2348.64s -> 2356.16s]  of corrupted document with some of the information missing. Okay, and so we get the sequence of
[2356.16s -> 2362.64s]  vectors here, one per word, which is the output of my encoder in blue, and then I'd say that for
[2362.64s -> 2368.24s]  the words that I want to predict Y i, I draw them. This sim means the probability is
[2368.96s -> 2375.68s]  proportional to, you know, my embedding matrix times my representation of it.
[2376.32s -> 2380.88s]  So it's just a linear transformation of that last thing here. So this A plus B is this red
[2380.88s -> 2384.96s]  portion here, and then do the prediction, and I train the entire network to do this. Yes.
[2386.96s -> 2394.08s]  So the words that we mask out, do we just select them randomly, or is there some thing to it?
[2394.08s -> 2398.24s]  The question is, do we just choose words randomly to mask out, or is there a scheme?
[2398.24s -> 2402.00s]  Mostly randomly. We'll talk about a slightly smarter scheme in a couple of slides,
[2402.00s -> 2405.68s]  but yeah, just mostly randomly. Yeah.
[2406.96s -> 2412.48s]  What was that last part on the bottom, X, the masked version, if it's the first
[2413.20s -> 2414.80s]  or the very last sentence?
[2416.48s -> 2423.44s]  Yeah, so I'm saying that I'm defining X tilde to be this input part, where I've got the
[2423.44s -> 2427.84s]  masked version of the sentence with these sort of words missing, and then I'm defining a
[2427.84s -> 2433.60s]  probability distribution that's the probability of a sequence conditioned on the input being
[2433.60s -> 2436.08s]  the sort of corrupted sequence, the masked sequence.
[2439.60s -> 2447.84s]  Okay. So this brings us to a very, very popular and sort of NLP model that you need to know
[2447.84s -> 2453.44s]  about. It's called BERT, and it was the first one to popularize this masked language modeling
[2453.44s -> 2459.44s]  objective, and they released the weights of this pre-trained transformer that they pre-trained via
[2459.44s -> 2464.24s]  something that looks a lot like masked language modeling, and so you can download, you can use
[2464.24s -> 2470.16s]  them via code that's released by the company Huggingface that we have continued to bring up.
[2470.16s -> 2475.44s]  Many of you will use a model like BERT in your final project because it's such a useful builder
[2475.44s -> 2480.00s]  of representations of language and context. So let's talk a little bit about the details
[2480.00s -> 2487.36s]  of masked language modeling in BERT. First, we take 15% of the subword tokens. So remember,
[2487.36s -> 2493.12s]  all of our inputs now are subword tokens. I've made them all look like words, but just like
[2493.12s -> 2497.52s]  we saw at the very beginning of class, each of these tokens could just be some portion,
[2497.52s -> 2502.56s]  some subword, and I'm going to do a couple of things with it. Sometimes I am going to just
[2502.56s -> 2511.36s]  mask out the word and then predict the true word. Sometimes I'm going to replace the word
[2511.36s -> 2517.28s]  with some random sample of another word from my vocabulary and predict the real word that was
[2517.28s -> 2524.08s]  supposed to go there, and sometimes I'm going to not change the word at all and still predict it.
[2524.08s -> 2529.84s]  The intuition of this is the following. If I just had to build good representations
[2530.80s -> 2536.80s]  of in the sort of middle of this network for words that are masked out, then when I actually
[2536.80s -> 2543.36s]  use the model at test time on some real review to do sentiment analysis on, well, there are never
[2543.36s -> 2547.60s]  going to be any tokens like this, so maybe the model won't do a very good job because it's
[2547.60s -> 2551.92s]  like, oh, I have no job to do here because I only need to deal with the masked tokens.
[2553.44s -> 2558.32s]  By giving it sequences of words, or sometimes it's the real word that needs to be predicted,
[2558.32s -> 2563.60s]  sometimes you have to detect if the word is wrong. The idea is that now when I give it a sentence
[2564.88s -> 2568.64s]  that doesn't have any masks, it actually sort of does a good job of representing
[2568.64s -> 2573.04s]  all the words in context because it has this chance that it could be asked to predict anything
[2573.04s -> 2585.52s]  at any time. Okay, so the folks at Google who were defining this had a separate additional
[2586.32s -> 2593.36s]  task that is sort of interesting to think about. So this was their BERT model from their paper.
[2593.36s -> 2598.48s]  They had their position embeddings just like we saw from our Transformers lecture, token
[2598.48s -> 2602.24s]  embeddings just like we saw from the Transformers lecture, but then also they had this thing
[2602.24s -> 2606.96s]  called a segment embedding where they had two possible segments, segment A and segment B,
[2608.56s -> 2614.56s]  and they had this additional task where they would get a big chunk of text for segment A
[2614.56s -> 2618.64s]  and a big chunk of text for segment B and then they would ask the model,
[2618.64s -> 2624.96s]  is segment B a real continuation of segment A? Was it the text that actually came next
[2625.60s -> 2630.48s]  or did I just pick this big segment randomly from somewhere else? And the idea was that this
[2630.48s -> 2635.92s]  should teach the network some notion of sort of long distance coherence, right, about sort of
[2635.92s -> 2640.56s]  the connection between a bunch of text over here and a bunch of text over there. Turns out
[2640.56s -> 2646.80s]  it's not really necessary, but it's an interesting idea. And sort of similar things have continued
[2646.80s -> 2652.08s]  to have some sort of influence since then. But again, you should get this intuition that
[2652.08s -> 2656.48s]  we're trying to come up with hard problems for the network to solve such that by solving
[2656.48s -> 2662.00s]  them it has to learn a lot about language and we're defining those problems by making
[2662.00s -> 2666.72s]  simple transformations or removing information from text that just happened to occur.
[2666.72s -> 2670.16s]  Questions?
[2672.48s -> 2677.60s]  Yeah. The plus signs, do we concatenate the vectors or do we do an element-wise addition?
[2678.56s -> 2681.92s]  The question is for these plus signs, do we concatenate the vectors or do element-wise
[2681.92s -> 2689.76s]  addition? We do element-wise addition. You could have concatenated them. However, one of the big
[2689.76s -> 2694.08s]  sort of conventions of all these networks is that you always have exactly the same number
[2694.08s -> 2698.24s]  of dimensions everywhere at every layer of the network. It just makes everything very simple.
[2698.24s -> 2703.92s]  So just saying everything's the same dimension and then doing addition just ends up being simpler.
[2707.04s -> 2709.36s]  So why is the next sentence prediction not necessary?
[2710.96s -> 2713.76s]  Yeah, why is the next sentence prediction not necessary? I mean,
[2714.32s -> 2717.04s]  one thing that it does that's a negative is that now
[2717.68s -> 2727.60s]  the effective context length for a lot of your examples is halved. So one of the things that's
[2727.60s -> 2731.84s]  useful about pre-training seemingly is that you get to build representations of very long
[2731.84s -> 2737.36s]  sequences of text. This is very short, but in practice segment A was going to be something
[2737.36s -> 2743.52s]  like 250 words and segment B was going to be 250 words and in the paper that sort of let us
[2743.52s -> 2749.36s]  know that this wasn't necessary, they always had a long segment of 500 words and it seemed to
[2749.36s -> 2755.60s]  be useful to always have this very long context because longer contexts help give you more
[2755.60s -> 2760.40s]  information about the role that each word is playing in that specific context. If I see one
[2760.40s -> 2764.96s]  word, it's hard to know. If I just see record, it's hard to know what it's supposed to mean,
[2764.96s -> 2769.28s]  but if I see a thousand words around it, it's much clearer what its role is in that context.
[2770.24s -> 2772.96s]  Yeah, it cuts the effective context size is one answer.
[2777.36s -> 2780.88s]  Another thing is that this is actually much more difficult. This is a much more recent paper
[2781.68s -> 2785.44s]  that I don't have in the slides, but it's been shown since then that these models are really,
[2785.44s -> 2791.92s]  really bad at the next sentence prediction task. So it could be that maybe it just was too hard
[2791.92s -> 2798.16s]  at the time and so it just wasn't useful because the model was failing to do it at all.
[2799.68s -> 2802.00s]  So I'll give the link for that paper later.
[2804.40s -> 2808.72s]  Why do we need to do a next sentence prediction? What about just masking and predicting the next?
[2812.24s -> 2815.28s]  Yeah, so the question is, why do we need to do next sentence prediction? Why not just do the
[2815.28s -> 2819.60s]  masking we saw before? That's the thing. You seem to not need to do next sentence prediction,
[2819.60s -> 2824.32s]  but as sort of like history of the research, it was thought that this was useful.
[2825.28s -> 2831.76s]  And the idea was that it required you to develop this sort of pairwise, like do these two segments
[2831.76s -> 2836.48s]  of text interact? How do they interact? Are they related? The sort of longer distance notion.
[2837.04s -> 2841.84s]  And many NLP tasks are defined on pairs of things and they thought that might be useful.
[2842.88s -> 2846.16s]  And so they published it with this and then someone else came through,
[2846.16s -> 2849.36s]  published a new model that didn't do that and it sort of did better.
[2850.24s -> 2855.52s]  So, you know, this is just, yeah. So, yeah. There are intuitions as to why it could work.
[2855.52s -> 2862.56s]  It just didn't. It was doing both. It was doing both this next sentence. So BERT was
[2862.56s -> 2868.40s]  doing both this next sentence prediction training as well as this masking training
[2868.96s -> 2876.16s]  all at the same time. And so you had to have a separate predictor head on top of BERT,
[2876.16s -> 2882.00s]  a separate predictor sort of classification thing. And, you know, so one detail there is that
[2882.00s -> 2888.16s]  there's this special word at the beginning of BERT in every sequence that's CLS. And, you know,
[2888.16s -> 2893.28s]  you can define a predictor on top of that sort of fake word embedding that was going to say,
[2893.28s -> 2898.64s]  is the next sentence real or fake or not? Yeah. Okay. I'm going to move on.
[2900.40s -> 2903.84s]  And so this gets at sort of the question that we had earlier about how do you evaluate
[2903.84s -> 2910.00s]  these things? There's a lot of different NLP tasks out there. Gosh. And, you know,
[2910.00s -> 2914.40s]  when people were defining these papers, they would look at a ton of different evaluations
[2914.40s -> 2918.24s]  that had been sort of compiled as a set of things that are still hard for today's systems.
[2918.80s -> 2923.52s]  So are you detecting paraphrases between questions? Are two Quora questions actually the
[2923.52s -> 2930.32s]  same question? That turns out to be hard. You know, can you do sentiment analysis on this hard
[2930.32s -> 2935.20s]  data set? Can you tell if sentences are linguistically acceptable? Are they grammatical
[2935.20s -> 2940.80s]  or not? Are two sequences similar semantically? Do they mean sort of vaguely the similar thing?
[2942.24s -> 2945.92s]  And we'll talk a bit about natural language inference later, but that's the task of
[2945.92s -> 2952.72s]  defining sort of if I say, you know, I saw the dog that does not necessarily mean I saw
[2952.72s -> 2958.48s]  the little dog, but saying I saw the little dog does mean I saw the dog. So that's sort of
[2958.48s -> 2963.60s]  this natural language inference task. And, you know, the striking the difference between
[2963.60s -> 2970.48s]  sort of pre-pre-training days where you had this sort of this row here before you had
[2970.48s -> 2977.20s]  substantial amounts of pre-training and BERT was just like the field was taken aback in a way
[2977.20s -> 2983.60s]  that's hard to describe. You know, very carefully crafted architectures for each individual task
[2983.60s -> 2986.80s]  where everyone is designing their own neural network and doing things that they thought
[2986.80s -> 2990.64s]  were sort of clever as to how to define all the connections and the weights and whatever
[2990.64s -> 2994.56s]  to do their tasks independently. Everyone was doing a different thing for each one of these
[2994.56s -> 3001.36s]  tasks, roughly. All of that was blown out of the water by just build a big transformer
[3001.36s -> 3005.36s]  and just teach it to predict the missing words a whole bunch and then fine tune it on each
[3005.36s -> 3011.28s]  of these tasks. So this was just a sea change in the field. People were, I mean,
[3011.28s -> 3015.68s]  amazed. It's a little bit less flashy than chat GPT, I'll admit, but it's really part of
[3015.68s -> 3020.56s]  the story that gets us to it, you know. Okay, questions.
[3024.00s -> 3030.88s]  So like to get stuff out of the like the during the encoder pre-training stage,
[3031.60s -> 3038.16s]  encoder usually outputs like some sort of hidden values. How do we correlate those to
[3038.72s -> 3044.24s]  words that we are trying to test against? So the question is, you know, the encoder
[3044.32s -> 3051.36s]  output is a bunch of hidden values. How do we actually correlate those values to stuff that
[3051.36s -> 3056.00s]  we want to predict? I'm going to go on to the next slide here to bring up this example here.
[3056.00s -> 3062.56s]  So the encoder gives us for each input word token a vector of that token that represents
[3062.56s -> 3068.32s]  the token in context. And the question is, how do we get these representations and turn them
[3068.32s -> 3076.08s]  into sort of answers for the tasks that we care about? And the answer comes back to
[3082.64s -> 3083.44s]  something like this.
[3083.44s -> 3092.32s]  Something like this, maybe.
[3097.92s -> 3102.48s]  Ah, sure. So when we were doing the pre-training, right, we had the transformer that was giving us
[3102.48s -> 3108.00s]  our representations, and we had this little last layer here, this little sort of affine
[3108.64s -> 3113.76s]  transformation that moved us from the encoder's hidden state size to the vocabulary to do our
[3113.76s -> 3119.60s]  prediction. And we just removed this last prediction layer here. And let's say we want
[3119.60s -> 3126.16s]  to do something that is classifying the sentiment of the sentence. We just pick arbitrarily maybe
[3126.16s -> 3132.64s]  the last word in the sentence, and we stick a linear classifier on top and map it to positive
[3132.64s -> 3140.56s]  or negative, and then fine tune the whole thing. OK. So yeah, the BERT model had two
[3140.56s -> 3145.60s]  different models. One was 110 million parameters. One was 340 million. Keep that sort of in the
[3145.60s -> 3149.60s]  back of your head sort of percolating as we talk about models with many, many more
[3149.60s -> 3159.52s]  parameters later on. It was trained on 800 million words, plus that is definitely
[3159.52s -> 3165.60s]  wrong, maybe 25 million words, but on the order of less than a billion words of text,
[3165.60s -> 3172.16s]  quite a bit still. And it was trained on what was considered at the time to be a whole lot of
[3172.16s -> 3176.08s]  compute. Just, you know, it was Google doing this, and they released it, and we were like,
[3176.08s -> 3180.24s]  oh, who has that kind of compute but Google, although nowadays it's not considered to be very
[3180.24s -> 3185.68s]  much. But fine tuning is practical and common on a single GPU. So you could take the BERT
[3185.68s -> 3189.92s]  model that they've spent a lot of time training and fine tune it yourself on your task
[3190.56s -> 3197.60s]  on even sort of a very, very sort of small GPU. OK.
[3200.88s -> 3206.32s]  So one question is like, well, this seems really great. Why don't we just use this for
[3206.32s -> 3214.56s]  everything? Yeah. And the answer is, well, you know, what is the sort of pre-training
[3214.56s -> 3220.08s]  objective? What's the structure of the pre-trained model good for? BERT is really good for sort of
[3220.08s -> 3226.40s]  filling in the blanks, but it's much less naturally used for actually generating text,
[3226.40s -> 3231.04s]  right? So I wouldn't want to use BERT to generate a summary of something because it's
[3231.04s -> 3236.16s]  not really built for it. It doesn't have a natural notion of predicting the next word,
[3236.16s -> 3240.00s]  given all the words that came before it. So maybe I want to use BERT if I want a good
[3240.00s -> 3245.44s]  representation of, say, a document to classify it, give it one of a set of topic labels or
[3245.44s -> 3250.64s]  say it's toxic or non-toxic or whatever. But I wouldn't want to use it to generate a whole sequence.
[3253.04s -> 3257.28s]  OK, some extensions of BERT. So we had a question earlier of whether you just
[3257.28s -> 3264.16s]  mask things out randomly. One thing that seems to work better is you mask out sort of whole
[3264.16s -> 3272.80s]  contiguous spans because sort of the difficulty of this problem is much easier than it would
[3272.80s -> 3278.32s]  otherwise be because sort of this is part of irresistibly and you can tell very easily
[3278.32s -> 3284.48s]  based on the sort of sub words that came before it. Whereas if I have a much longer sequence,
[3284.48s -> 3288.88s]  it is a trade-off. But, you know, this might be a harder problem and it ends up being better
[3288.96s -> 3293.92s]  to do this sort of span-based masking than random masking. And that might be because
[3293.92s -> 3299.04s]  sub words make very simple prediction problems when you mask out just one sub word of a word
[3299.04s -> 3304.64s]  versus all the sub words of a word. OK, so this ends up doing much better.
[3305.20s -> 3309.52s]  There's also a paper called the Roberta paper, which showed that the next sentence prediction
[3311.20s -> 3315.28s]  wasn't necessary. They also showed that they really should have trained it on a lot
[3315.28s -> 3320.88s]  more text. So Roberta is a drop-in replacement for BERT. So if you're thinking of using BERT,
[3320.88s -> 3324.96s]  just use Roberta, it's better. And it gave us this intuition that we really don't know a whole
[3324.96s -> 3328.56s]  lot about the best practices for training these things. You sort of train it for as
[3328.56s -> 3335.12s]  long as you're willing to and things do good stuff and whatever. So this is very,
[3335.12s -> 3339.04s]  it was very difficult to do sort of iteration on these models because they're big,
[3339.04s -> 3344.16s]  it's expensive to train them. Another thing that you should know for your
[3344.16s -> 3349.12s]  final projects in the world ahead is this notion of fine-tuning all parameters of the network
[3349.12s -> 3354.72s]  versus just a couple of them. So what we've talked about so far is you pre-train all the parameters
[3354.72s -> 3359.92s]  and then you fine-tune all of them as well. So all the parameter values change. An alternative,
[3359.92s -> 3365.84s]  which you call parameter efficient or lightweight fine-tuning, you sort of choose little bits of
[3365.84s -> 3369.84s]  parameters or you choose the very smart way of keeping most of the parameters fixed and only
[3369.84s -> 3375.12s]  fine-tuning others. And the intuition is that these pre-trained parameters were really good
[3376.48s -> 3381.04s]  and you want to make the minimal change from the pre-trained model to the model that does what
[3381.04s -> 3384.24s]  you want so that you keep some of the generality, some of the goodness of the
[3384.24s -> 3390.24s]  pre-training. So one way that this is done is called prefix tuning. Prompt tuning is very
[3390.24s -> 3394.40s]  similar where you actually freeze all the parameters of the network. So I've pre-trained
[3394.40s -> 3401.12s]  my network here and I never change any of the parameter values. Instead, I make a bunch of
[3401.12s -> 3407.92s]  fake sort of pseudo-word vectors that I prepend to the very beginning of the sequence and I train
[3407.92s -> 3413.28s]  just them. Sort of unintuitive. It's like these would have been like inputs to the network
[3413.28s -> 3418.56s]  but I'm specifying them as parameters and I'm training everything to do my sentiment analysis task
[3418.64s -> 3421.44s]  just by changing the values of these sort of fake words.
[3423.20s -> 3427.52s]  This is nice because I get to keep all the good pre-trained parameters
[3428.80s -> 3436.24s]  and then just specify the sort of diff that ends up generalizing better. This is a very open
[3436.24s -> 3441.28s]  field of research but this is also cheaper because I don't have to compute the gradients
[3441.28s -> 3446.40s]  or I don't have to store the gradients and all the optimizer state with respect to all these
[3446.40s -> 3449.84s]  parameters. I'm only training a very small number of parameters.
[3460.96s -> 3464.80s]  In a decoder you have to put them at the beginning because otherwise
[3464.80s -> 3467.52s]  you don't see them before you process the whole sequence.
[3468.40s -> 3478.72s]  Can we just attach the new layers at the top of this and only train those? Absolutely.
[3478.72s -> 3483.28s]  This works a bit better. Another thing that works well, sorry we're running out of time,
[3484.32s -> 3489.60s]  is taking each weight matrix. I have a bunch of weight matrices in my transformer
[3489.60s -> 3496.00s]  and I freeze the weight matrix and learn a very low rank little diff and I set the
[3496.00s -> 3504.16s]  weight matrix's value to be the original value plus my very low rank diff from the
[3504.16s -> 3510.56s]  original one and this ends up being a very similarly useful technique. The overall idea
[3510.56s -> 3516.32s]  here is that I'm learning way fewer parameters than I did via pre-training and freezing most
[3516.32s -> 3524.32s]  of the pre-training parameters. For encoder decoders we could do something like language
[3524.32s -> 3530.40s]  modeling. I've got my input sequence here, encoder output sequence here and I could say
[3530.40s -> 3536.32s]  this part is my prefix for sort of having bi-directional context and I could then predict
[3536.32s -> 3541.76s]  all the words that are sort of in the latter half of the sequence just like a language model
[3541.76s -> 3548.32s]  and that would work fine. This is something that you could do. You take a long text,
[3548.32s -> 3552.80s]  split it into two, give half of it to the encoder and then generate the second half with
[3552.80s -> 3560.16s]  the decoder. But in practice what works much better is this notion of span corruption.
[3560.16s -> 3565.76s]  Span corruption is going to show up in your assignment five and the idea here is a lot
[3565.76s -> 3572.40s]  like BERT but in a sort of generative sense where I'm going to mask out a bunch of words
[3572.40s -> 3581.04s]  in the input thank you mask token one me to your party mask token two weak and then at
[3581.04s -> 3587.68s]  the output I generate the mask token and then what was supposed to be there but the mask token
[3587.68s -> 3593.52s]  replaced it right so thank you then predict for inviting at the output me to your party
[3593.52s -> 3601.12s]  last week and what this does is that it allows you to have bi-directional context right I get to
[3601.12s -> 3607.92s]  see the whole sequence except I can generate the parts that were missing so this feels a little
[3607.92s -> 3612.80s]  bit like BERT you mask out parts of the input but you actually generate the output as a sequence
[3612.80s -> 3617.20s]  like you would in language modeling so this might be good for something like machine translation
[3617.20s -> 3622.08s]  where I have an input that I want bi-directional context in but then I want to generate an output
[3622.08s -> 3626.08s]  and I want to pre-train the whole thing so this was shown to work better than language
[3626.08s -> 3632.00s]  modeling at the scales that these folks at Google were able to test back in 2018 this is still
[3632.00s -> 3639.20s]  quite quite popular yeah there's a lot of numbers it works better than the other stuff
[3639.92s -> 3646.48s]  I'm not going to worry about it there's a fascinating property of these models also
[3646.48s -> 3653.04s]  so T5 was the model that was originally introduced with salient span masking and you
[3653.04s -> 3658.56s]  can think of you know at pre-training time you saw a bunch of things like Franklin D Roosevelt
[3658.56s -> 3665.28s]  was born in you know blank and you generated out the blank and there's this task called
[3666.08s -> 3671.12s]  open domain question answering which has a bunch of trivia questions like you know when was
[3671.12s -> 3675.60s]  Franklin D Roosevelt born and then you're supposed to generate out the answer as a string
[3675.60s -> 3679.68s]  just like just from your parameters right so you did a bunch of pre-training you saw a bunch
[3679.68s -> 3685.52s]  of text and you're supposed to generate these answers and what's fascinating is that this
[3685.52s -> 3693.20s]  sort of salient span masking method allowed you to pre-train and then fine-tune on some examples
[3693.20s -> 3700.16s]  of questions trivia questions and then when you tested on new trivia questions it would sort of
[3700.16s -> 3706.40s]  the model would sort of implicitly extract from its pre-training data somehow the answer to that
[3706.40s -> 3710.72s]  new question that it never saw explicitly at fine-tuning time so it learned this sort of
[3710.72s -> 3715.84s]  implicit retrieval sometimes sometimes you know less than 50 percent of the time or whatever but
[3715.84s -> 3721.76s]  you know much more than random chance yeah and that's just sort of fascinating right so you've
[3721.76s -> 3726.64s]  sort of learned to access this sort of latent knowledge that you stored up by pre-training
[3727.28s -> 3732.40s]  and so yeah you just pass it the text when was Roosevelt born and it would pass out an
[3732.40s -> 3736.80s]  answer and one thing to know is that the answers always look very fluent they always look very
[3736.80s -> 3741.92s]  reasonable but they're frequently wrong and that's still true of things like chat gpt
[3742.88s -> 3751.60s]  um yeah okay so that's that's like encoder decoder models um next up we've got decoders
[3751.60s -> 3756.08s]  and spend a long time on decoders so this is just our normal language model so i get a
[3756.08s -> 3761.84s]  sequence of hidden states for my decoder the the model the words can only look at themselves
[3761.84s -> 3768.00s]  not the future and then i predict you know the next word in the sentence and then here again
[3768.00s -> 3771.92s]  i can you know to do sentiment analysis maybe take the last state for the last word
[3771.92s -> 3777.52s]  and then predict happy or sad based on that last embedding back propagate the gradients
[3777.52s -> 3782.80s]  the whole network train the whole thing or do some kind of lightweight or parameter efficient
[3782.80s -> 3787.84s]  fine-tuning like we mentioned earlier so this is our our you know pre-training a decoder
[3787.84s -> 3794.56s]  and um you know i can just pre-train it on language modeling um so again you might want
[3794.56s -> 3802.32s]  to do this if you are wanting to generate generate texts generate things uh this is you
[3802.32s -> 3807.44s]  sort of can use this like you use an encoder decoder um but in practice as we'll see a lot of
[3807.44s -> 3814.56s]  the sort of biggest uh most powerful pre-trained models tend to be decoder only it's not really
[3814.56s -> 3821.60s]  clear exactly why except they seem a little bit simpler than encoder decoders um and you get to
[3821.60s -> 3826.56s]  share all the parameters in one big network for the decoder whereas an encoder decoder you have
[3826.56s -> 3831.60s]  to split them sort of some into the encoder some into the decoder so for the rest of this
[3831.60s -> 3838.56s]  lecture we'll talk only about decoders so even in modern things uh the biggest networks do
[3838.56s -> 3846.16s]  tend to be decoders so we're coming all the way back again to 2018 and the gpt model from open
[3846.16s -> 3854.96s]  ai was a big success it had 117 parameter a million parameters uh it had you know 768
[3854.96s -> 3863.04s]  dimensional hidden states and uh it had this vocabulary uh that was 40 000 ish words that
[3863.04s -> 3867.92s]  was defined via a method like what we showed at the beginning of class trained on books corpus
[3868.56s -> 3873.36s]  and um you know actually you know gpt never actually showed up in the original paper
[3873.36s -> 3881.28s]  uh it's sort of uh it's unclear what exactly it's supposed to refer to um but uh this model
[3881.28s -> 3887.04s]  was a precursor to all the things that you're hearing about nowadays uh if you move forward
[3888.16s -> 3889.44s]  uh oh yeah so if you
[3889.52s -> 3890.32s]  hmm
[3895.68s -> 3900.72s]  so if we wanted to do something like natural language inference right which says you know
[3900.72s -> 3906.40s]  take these pairs of sentences the man is in the doorway the person is near the door and uh
[3906.40s -> 3911.04s]  say that these mean that one entails the other the sort of premise entails the hypothesis that
[3911.04s -> 3915.28s]  i can believe the hypothesis if i believe the premise i just sort of concatenate them
[3915.28s -> 3922.08s]  together right so give it maybe a start token pass in one sentence pass in some delimiter
[3922.08s -> 3929.44s]  token pass in the other and then predict uh sort of yes no entailment not entailment fine tuning
[3930.08s -> 3936.24s]  gpt on this it worked really well um and then you know burt came after gpt burt did a bit
[3936.24s -> 3942.80s]  better it had bi-directional context um but you know it did it did uh sort of an excellent job
[3943.68s -> 3950.88s]  and then came gpt2 where they focused more on the generative abilities of the network so um right
[3950.88s -> 3957.36s]  we looked at uh now a much larger network we've gone from 117 million to 1.5 billion
[3957.36s -> 3963.12s]  and given some sort of prompt it could generate at the time a quite surprisingly coherent
[3963.12s -> 3968.64s]  continuation to the prompt so it's telling this sort of story about uh about scientists
[3969.04s -> 3975.92s]  and unicorns here um and this size of model is still sort of small enough that you can use
[3975.92s -> 3982.96s]  on a small gpu and fine tune and whatever and its capabilities of generating long coherent text
[3982.96s -> 3990.24s]  was just sort of exceptional at the time it was also trained on more data although i don't
[3990.24s -> 4000.32s]  know uh something like 9 billion words of text um and then so after gpt2 uh we come to gpt3
[4000.32s -> 4004.64s]  sort of walking through these models and then we come with a different way of interacting with the
[4004.64s -> 4010.40s]  models so we've interacted with pre-trained models in two ways so far we've sort of sampled
[4010.40s -> 4016.08s]  uh from the distribution that they define uh we've generated text via like a machine translation
[4016.08s -> 4020.64s]  system or whatever or you fine-tune them on a task that we care about and we take their predictions
[4022.56s -> 4032.64s]  um but gpt3 seems to have an interesting new ability it's much larger and it can
[4032.64s -> 4039.92s]  do some tasks without any sort of fine tuning whatsoever gpt3 is much larger than gpt2 right
[4039.92s -> 4048.48s]  so we went from gpt 100 ish million parameters gpt2 1.5 billion gpt3 175 billion much larger
[4048.48s -> 4054.64s]  uh trained on 300 billion words of text and this sort of notion of in-context learning that it
[4054.64s -> 4059.36s]  could define or figure out patterns in the training or in the example that it's currently
[4059.36s -> 4065.60s]  seeing and continue the pattern uh is called in-context learning so you got you know the word
[4065.60s -> 4070.80s]  thanks and i pass in this little arrow and say okay thanks goes to you know merci and then hello
[4070.80s -> 4075.44s]  goes to bonjour and then you know they give it all of these examples and ask it um what
[4075.44s -> 4082.00s]  you know otter should go to and it's learned to sort of continue the pattern and say that
[4082.00s -> 4087.44s]  this is the translation of otter so now remember this is a single sort of input that
[4087.44s -> 4092.72s]  i've given to my to my model and i haven't said oh do translation or fine tune it on
[4092.72s -> 4097.92s]  translation or whatever i've just passed in the input given it some examples and then it is able
[4097.92s -> 4103.52s]  to some to some extent uh do this seemingly complex task that's in-context learning
[4105.44s -> 4110.24s]  and here are more examples you know maybe you give it examples of of addition and then it can
[4110.24s -> 4116.16s]  do some uh some simple addition afterward you give it in this case this is sort of rewriting
[4116.16s -> 4121.68s]  typos it can figure out how to rewrite typos in-context learning for for machine translation
[4121.68s -> 4126.32s]  and this was the start of this idea that there were these emergent properties that showed up in
[4126.32s -> 4131.68s]  much larger models and it wasn't clear when looking at the smaller models that you'd get
[4131.68s -> 4139.68s]  this sort of new this qualitatively new behavior out of them right like it's not obvious from just
[4139.68s -> 4144.96s]  the language modeling signal right gpt3 is just trained on that decoder only just next predict
[4144.96s -> 4152.24s]  the next word that it would as a result of that training learn to perform seemingly quite complex
[4152.24s -> 4159.60s]  things as a function of its context um yeah okay one or two questions about that
[4166.88s -> 4171.84s]  it should be quite surprising i think right like so far we said talk about good representations
[4171.84s -> 4176.48s]  contextual representations meanings of words in context this is some very very high level
[4176.48s -> 4181.28s]  pattern matching right it's coming up with patterns in just the input data in that one
[4181.28s -> 4186.80s]  sequence of text that you passed it so far and it's able to sort of identify how to complete
[4186.80s -> 4192.24s]  the pattern and as you think what kinds of things can this solve what are its capabilities
[4192.24s -> 4196.88s]  what are its limitations this ends up being an open area of research sort of what are the
[4196.88s -> 4202.56s]  kinds of problems that you maybe saw in the training data lot maybe gpt3 saw a ton of pairs
[4202.56s -> 4207.20s]  of words right it saw a bunch of you know dictionaries bilingual dictionaries in its
[4207.20s -> 4210.96s]  training data so it learned to do something like this or is it doing something much more
[4210.96s -> 4216.24s]  general where it's really learning the task in context you know the actual story we're not
[4216.24s -> 4222.24s]  totally sure something in the middle it seems like it has to be tied to your training data in
[4222.24s -> 4228.00s]  ways that we don't quite understand but there's also a non-trivial ability to learn new sort of
[4228.56s -> 4233.44s]  at least types of patterns just from the context so this is a very interesting thing to work on
[4234.56s -> 4238.80s]  now we've talked a lot about the size of these models so far and as models have gotten
[4238.80s -> 4245.04s]  larger they've always gotten better we train them on more data right so gpt3 was trained on
[4245.04s -> 4253.36s]  300 billion words of text and it was 175 billion parameters and you know at that scale it costs
[4253.36s -> 4258.16s]  a lot of money to build these things and it's very unclear whether you're getting the best
[4258.16s -> 4261.60s]  use out of your money like it's bigger really what you should have been doing in terms of the
[4261.60s -> 4267.04s]  number of parameters so you know the cost of training one of these is roughly you take the
[4267.04s -> 4270.72s]  number of parameters you multiply it by the number of tokens that you're going to train it on the
[4270.72s -> 4276.80s]  number of words and some folks at deep mind i throughout the citation on this some folks at
[4276.80s -> 4284.96s]  deep mind realized through some experimentation that actually gpt3 was just comically oversized
[4284.96s -> 4290.72s]  right so chinchilla the model they trained is less than half the size and works better but
[4290.72s -> 4297.04s]  they just trained it on way more data and this is sort of an interesting sort of trade-off about
[4297.04s -> 4300.72s]  you know how do you best spend your compute i mean you can't do this more than a handful of
[4300.72s -> 4309.12s]  times even if you're you know google so you know open open questions there as well another
[4309.12s -> 4313.52s]  sort of way of interacting with these networks that's come out recently is called chain of
[4313.52s -> 4321.04s]  thought so the prefix right we saw in the in context learning slide that the prefix can help
[4321.04s -> 4326.32s]  sort of specify what task you're trying to solve right now and it can do even more so here's
[4326.32s -> 4331.60s]  standard sort of prompting we have a prefix of examples of questions and answers so you have
[4331.60s -> 4337.44s]  a question and then an example answer so that's your prompt that's specifying the task and then
[4337.44s -> 4341.20s]  you have a new question and you're having the model generate an answer and it generates it wrong
[4343.04s -> 4348.96s]  and chain of thought prompting says well how about in the example in the demonstration we
[4348.96s -> 4354.48s]  give we give the question and then we give this sort of decomposition of steps towards how
[4354.48s -> 4359.04s]  to get an answer right so i'm actually writing this out as part of the input i'm i'm giving
[4359.04s -> 4364.88s]  annotations as a human to say oh you know to solve this sort of word problem here's how you
[4364.88s -> 4370.72s]  could think it through ish and then i give it a new question and the model says oh i know
[4370.72s -> 4376.48s]  what i'm supposed to do i'm supposed to first generate a sequence of steps of intermediate
[4376.48s -> 4382.80s]  steps and then next say the answer is and say what the answer is and it turns out and this
[4382.80s -> 4390.72s]  should again can be very surprising that the model can tend to generate plausible sequences of steps
[4390.72s -> 4395.68s]  and then much more frequently generates the correct answer after doing so relative to trying
[4395.68s -> 4400.88s]  to generate the answer by itself so you can think of this as a scratch pad you can think
[4400.88s -> 4405.60s]  of this as increasing the amount of computation that you're putting into trying to solve the
[4405.60s -> 4412.24s]  problem sort of writing out your thoughts right as i generate each word of this continuation here
[4412.88s -> 4419.84s]  i'm able to condition on all the past words so far and so maybe it just uh yeah allows the
[4419.84s -> 4424.96s]  network to sort of decompose the problem into smaller simpler problems which is more able to
[4424.96s -> 4432.72s]  solve each no one's really sure why this works exactly either at this point with networks that
[4432.72s -> 4438.64s]  are this large their emergent properties are both very powerful and exceptionally hard to
[4438.64s -> 4444.32s]  understand and very hard you should think to trust because it's unclear
[4444.32s -> 4447.44s]  what its capabilities are and what its limitations are where it will fail
[4449.04s -> 4455.36s]  so what do we think pre-training is teaching gosh a wide range of things even beyond what
[4455.36s -> 4460.16s]  i've written in this slide which i mostly wrote two years ago right so it can teach
[4460.16s -> 4464.80s]  you trivia and syntax and co-reference and maybe some lexical semantics and sentiment
[4464.80s -> 4469.12s]  and some reasoning like way more reasoning than we would have thought even three years ago
[4470.24s -> 4476.16s]  and yet they also learn and exacerbate racism and sexism all manner of biases
[4477.92s -> 4483.84s]  more on this later but it's the generality of this is really i think what's taken many
[4483.84s -> 4490.88s]  people aback and so increasingly these objects are not just studied for the sake of using them
[4490.88s -> 4495.20s]  but studied for the sake of understanding anything about how they work and how they fail
[4496.80s -> 4498.80s]  uh yeah any questions
[4505.12s -> 4511.60s]  has anyone tried like benchmarking like gpt for like programming tasks like how
[4511.60s -> 4518.32s]  i could please does etc yeah the question is has anyone tried benchmarking gpt for programming
[4518.32s -> 4524.32s]  tasks anyone seen how well it does um yes so there's definitely examples of people using gpt
[4525.12s -> 4531.52s]  three four simple programming things and then you know the modern state-of-the-art competitive
[4531.52s -> 4538.00s]  programming bots are all based on ideas from language modeling and i think i think they're all
[4538.00s -> 4543.20s]  also based on pre-trained language models themselves like if you just take all of these ideas
[4543.20s -> 4549.84s]  and apply it to like github then you get some very interesting emergent behaviors relating to code
[4549.84s -> 4556.40s]  uh fallout and so yeah i think all of the best systems use this more or less there's lots of
[4556.40s -> 4563.60s]  benchmarking there for sure the basis for what like github the question is is this the
[4563.60s -> 4568.32s]  basis is that what we just mentioned the basis for the github co-pilot system yes absolutely
[4568.32s -> 4574.64s]  we don't know exactly what it is in terms of details but it's all these ideas
[4575.92s -> 4579.92s]  what if you have a situation where you have you know still a large amount of data for
[4579.92s -> 4584.64s]  you know general data and then you have also a large amount of data for your fine tuning task
[4584.64s -> 4589.76s]  at what point is it better to train a new model for that fine-tuning versus you know get
[4589.76s -> 4593.60s]  data from both yeah the question is what if you have a large amount of data for pre-training
[4593.60s -> 4598.80s]  and a large amount of data for fine tuning when is it better to do sort of a separate training
[4598.80s -> 4607.36s]  on just the fine tuning data um almost never if you have a bunch of data for the task that you
[4607.36s -> 4612.72s]  care about what's frequently done instead is three-part training where you pre-train on a
[4612.72s -> 4618.64s]  very broad corpus then you sort of continue to pre-train using something like language modeling
[4618.64s -> 4624.24s]  on an unlabeled version of the labeled data that you have you just like strip the labels off
[4624.24s -> 4629.20s]  and just treat it all as text and do language modeling on that adapt the parameters a little bit
[4629.20s -> 4633.20s]  and then do the final stage of fine tuning with the labels that you want and that works
[4633.20s -> 4637.28s]  even better this is an interesting paper called don't stop pre-training
[4639.12s -> 4645.52s]  nice uh final question that's a lot of questions so anyone new someone new
[4649.60s -> 4654.96s]  yeah um i was wondering do you know if there's like a lot of instances where
[4654.96s -> 4659.68s]  a pre-trained model can do some task that's not seen before even without
[4661.04s -> 4664.40s]  yeah so are there any instances of where a pre-trained model can do a task that it hasn't
[4664.40s -> 4669.12s]  seen before uh you know without fine tuning the question is what does hasn't seen before mean
[4669.68s -> 4675.52s]  right like uh these models especially gpt3 and similar very large models you know during
[4675.52s -> 4682.24s]  pre-training did it ever see something exactly like this sort of word problem arithmetic maybe
[4682.24s -> 4687.92s]  maybe not it's actually sort of unclear it's clearly able to recombine sort of bits and
[4687.92s -> 4692.64s]  pieces of tasks that it saw implicitly during pre-training we saw the same thing with trivia
[4692.64s -> 4696.88s]  right like language modeling looks a lot like trivia sometimes where you just read the first
[4696.88s -> 4701.36s]  paragraph of a wikipedia page and it's kind of like answering a bunch of little trivia
[4701.36s -> 4706.08s]  questions about where someone was born and when um but like it's never seen something quite like
[4706.08s -> 4710.24s]  this and it's actually still kind of astounding how much is able to do things that don't seem
[4710.24s -> 4714.32s]  like they should have shown up all that directly in the pre-training data quantifying
[4714.32s -> 4720.80s]  that extent is an open research problem okay that's it let's call it
