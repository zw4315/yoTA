# Detected language: en (p=1.00)

[0.00s -> 4.80s]  This video is a continuation of our first video on how packet switches work.
[4.80s -> 8.96s]  In the first video, we saw that there are two basic operations to a packet switch.
[8.96s -> 12.72s]  First, packet addresses have to be looked up into a forwarding table.
[12.72s -> 18.06s]  And then the packet has to be switched or transferred to the correct output port
[18.06s -> 21.36s]  so that it can be sent onto the correct outgoing link.
[21.36s -> 25.94s]  In the last video, we saw how addresses are looked up in tables for Ethernet switches
[25.94s -> 26.94s]  and Internet routers.
[26.94s -> 30.82s]  In this video, I'm going to explain how packets are switched to the correct egress port.
[30.82s -> 33.38s]  I'm going to look at a number of different techniques.
[33.38s -> 36.34s]  Output queuing, input queuing, and virtual output queues.
[36.34s -> 43.42s]  And we'll see and get a sense for how these packet switches are actually built.
[43.42s -> 49.46s]  I'm going to start with the basic vanilla switch, which is the one I showed you before.
[49.46s -> 53.34s]  We have the address lookup on the left over here.
[53.34s -> 57.30s]  And then this is the forwarding table where we look up the addresses.
[57.30s -> 62.06s]  And then we have the packet queuing logic and then the buffer memory where the packets
[62.06s -> 65.34s]  are held during times of congestion.
[65.34s -> 71.30s]  When packets arrive, here are three packets arriving with different egress ports indicated
[71.30s -> 73.18s]  by the color of the header of the packet.
[73.18s -> 79.58s]  So the red one at the top is going to the red port over here, the one in the middle.
[79.66s -> 84.90s]  When these packets traverse the backplane, we see that the blue one is able to go to
[84.90s -> 86.30s]  its output.
[86.30s -> 90.50s]  One of the red ones can be delivered immediately, and the other one is held in the output
[90.50s -> 92.90s]  queue, waiting for its turn.
[92.90s -> 102.02s]  So as soon as the first two have left, this one can then depart in FIFO order.
[102.02s -> 105.86s]  We often refer to a switch like this as an output queued switch because the queues
[105.86s -> 107.66s]  are at the output.
[107.66s -> 111.54s]  This has a certain ramification for the performance of the switch.
[111.54s -> 114.58s]  Let's take a look at that.
[114.58s -> 119.80s]  When we have packets arriving, it's possible in the worst case that all the packets coming
[119.80s -> 124.80s]  in at the same time from the outside will be wanting to go to the same output queue,
[124.80s -> 126.34s]  let's say this one here.
[126.34s -> 133.42s]  So if we have n ports, each running at rate r, and let's say there are n of them, then
[133.42s -> 139.54s]  in the worst case we could actually have a writing rate of n times r into this output
[139.54s -> 140.54s]  queue.
[140.54s -> 147.10s]  Similarly, and we always have a reading rate from this queue of rate r, so we say in the
[147.10s -> 153.06s]  output queued switch that this memory must run an aggregate, a total rate of up to n
[153.06s -> 158.78s]  plus 1 times r.
[158.78s -> 163.10s]  The somewhat annoying thing or frustrating thing about this is that long term, you can't
[163.10s -> 167.54s]  possibly be the case that we're writing into this queue at rate n times r, the system
[167.54s -> 169.02s]  could not sustain that.
[169.02s -> 174.00s]  This only really works if some mechanism is at play, like congestion control, to hold
[174.00s -> 178.86s]  the average rate of writing into this queue at no more than 1r.
[178.86s -> 185.14s]  So it feels as though the maximum rate that we should need is 2 times r.
[185.14s -> 186.70s]  That was what we would strive for.
[186.70s -> 189.88s]  Unfortunately we're paying this penalty of n, and n could be a large number, it could
[189.88s -> 191.78s]  be hundreds or even thousands.
[191.94s -> 195.86s]  This memory has to run much faster.
[195.86s -> 201.86s]  Output queued switches are said to be limited by this problem, that they have to have memories
[201.86s -> 203.96s]  that run very, very fast.
[203.96s -> 210.30s]  It becomes quite a challenge when building scalable output queued switches to find or
[210.30s -> 216.06s]  use memories, or create a memory hierarchy that will run fast enough.
[216.06s -> 220.82s]  One obvious way to solve this problem is to move the queues from the output over
[220.86s -> 221.86s]  to the input.
[221.86s -> 224.74s]  Let's take a look at what happens when we do this.
[224.74s -> 228.02s]  For obvious reasons, we call this an input queued packet switch.
[228.02s -> 234.10s]  Now the queues where packets will be held are at the input side of the switch.
[234.10s -> 240.50s]  The advantage of this will perhaps be obvious in a moment, if we consider packets arriving
[240.50s -> 243.16s]  to the switch.
[243.16s -> 246.74s]  Same pattern as before, two reds, one blue.
[246.74s -> 250.74s]  In this case, what we would do is all of the packets would come through the switch,
[251.66s -> 256.38s]  and only one of them needs to be held, that's the one down here, waiting for its turn to
[256.38s -> 257.46s]  go across the switch.
[257.46s -> 260.30s]  And that's because its output line is busy, and there's no queue at the output
[260.30s -> 261.36s]  to hold it.
[261.36s -> 265.92s]  So we hold it back at the input, and then later, when its turn comes, it can depart
[265.92s -> 267.70s]  just like it would from an output queued switch.
[267.70s -> 273.22s]  Oh, and the face of it, the good news is that things look like they work the same.
[273.22s -> 279.58s]  And the better news is that the buffer memory here now only has to accept one packet,
[279.62s -> 285.46s]  at most one packet from the ingress at a time, and has to only send one packet into the switch
[285.46s -> 286.94s]  in a packet time.
[286.94s -> 292.02s]  So its speed has been reduced from N plus one times R, just down to our minimum and
[292.02s -> 297.70s]  our goal, which was two times R. So a factor of almost N reduction.
[297.70s -> 299.74s]  So this makes a huge difference.
[299.74s -> 303.54s]  And for this reason, people often say that input queued switches are much more scalable.
[303.54s -> 308.82s]  And indeed, quite a few big switches are made this way, but with a caveat, and there
[308.86s -> 313.14s]  is a problem that we're going to have a take a look at right now.
[313.14s -> 317.30s]  In an input queued switch, the problem is something called head-of-line blocking.
[317.30s -> 319.82s]  And this problem is something that you'll see in many contexts.
[319.82s -> 324.22s]  I want to explain it here, so you'll recognize it when you see it in other environments.
[324.22s -> 327.22s]  Let me go through an example.
[327.22s -> 331.98s]  These are three inputs representing the inputs of the switch.
[331.98s -> 333.58s]  So these are the input buffers.
[333.58s -> 337.82s]  I've taken away everything else on the switch, just to make it a little bit clearer.
[337.82s -> 340.86s]  And we're going to see packets arrive to these.
[340.86s -> 343.78s]  Here they are.
[343.78s -> 346.70s]  Red ones going to the red output, black ones to the black output, green ones to
[346.70s -> 348.70s]  the green output.
[348.70s -> 354.98s]  And imagine that you have the task of deciding which packets to go.
[354.98s -> 359.22s]  And you look at the packets at the head-of-line of this and see that they're all red.
[359.22s -> 363.46s]  Problem is that you can only send one of them at a time.
[364.18s -> 368.38s]  So in this particular instance, we'd only be able to send the red one.
[368.38s -> 374.42s]  Even though there are green and black packets in the system that could go to these unused
[374.42s -> 379.82s]  outputs, because we've arranged everything as a single queue, we get this head-of-line
[379.82s -> 382.98s]  blocking effect.
[382.98s -> 386.70s]  Natural solution to this, which is pretty widely used, is something called virtual output
[386.70s -> 392.22s]  queues, where each input maintains a separate queue for each output.
[392.22s -> 394.26s]  So in this case, we have a three-by-three switch.
[394.26s -> 401.62s]  So this queue here is a FIFO queue of packets waiting to go to output one, the red output,
[401.62s -> 405.22s]  for output two and for output three.
[405.22s -> 409.34s]  So when packets arrive, here are the same set of packets arriving as before, but now
[409.34s -> 413.30s]  they get preclassified and placed into a queue corresponding to the output they're going
[413.30s -> 414.30s]  to.
[414.30s -> 415.98s]  That's why we call them virtual output queues.
[415.98s -> 420.38s]  It's a queue of packets all going to the same output.
[420.38s -> 425.62s]  The good news now is that because each queue holds packets going to the same output, no
[425.62s -> 430.98s]  packet can be held up by a packet ahead of it going to a different output.
[430.98s -> 437.58s]  So it can't be held up because its head-of-line is blocked by someone who is stuck.
[437.58s -> 444.10s]  So now we can look at this and say, aha, we have visibility into all of the head-of-line
[444.10s -> 448.90s]  packets, and we can deliver all three in one go, and therefore get a higher instantaneous
[448.90s -> 449.90s]  throughput.
[449.94s -> 451.98s]  It's an obvious solution.
[451.98s -> 454.70s]  It can be a little tricky to implement in practice.
[454.70s -> 458.88s]  But the nice thing is that it overcomes this head-of-line blocking entirely.
[458.88s -> 465.02s]  So the good news overall is we've reduced the speed of the queues to 2 times R, the
[465.02s -> 469.14s]  speed of the memories, because remember, we can only have one packet come in at a time
[469.14s -> 472.10s]  and only one packet depart at a time.
[472.10s -> 477.82s]  And we're able to sustain the same throughput performance as before.
[477.82s -> 480.90s]  Just to look at this on a graph, we often see graphs that look like this.
[480.90s -> 488.18s]  This is a plot of the average delay that a packet would experience as a function of
[488.18s -> 489.18s]  the load.
[489.18s -> 492.86s]  This is basically how busy the ingress lines are.
[492.86s -> 499.14s]  The best that any queuing system can achieve is this line here, and this corresponds to
[499.14s -> 503.66s]  a system in which as the load approaches 100%, the delay increases, or the average
[503.66s -> 508.14s]  delay increases and is asymptotic to 100%.
[508.14s -> 511.50s]  In fact, this is what we will see with an output-queued switch.
[511.50s -> 517.54s]  An output-queued switch is perfect in the sense that you can't achieve a higher throughput
[517.54s -> 521.26s]  or you can't achieve a lower average delay.
[521.26s -> 525.50s]  Let's take a look at the main properties of output-queued switches.
[525.50s -> 527.54s]  First we say they are work-conserving.
[527.54s -> 532.74s]  Work-conserving means that an output line is never idle when there is a packet in the
[532.74s -> 535.22s]  system waiting to go to it.
[535.22s -> 539.58s]  That means there is no blocking internally preventing a packet getting to that line.
[539.58s -> 545.18s]  Whenever that line is idle, there is no packet in the system waiting for it.
[545.18s -> 549.26s]  As a consequence, throughput is maximized because you cannot have a higher throughput
[549.26s -> 553.34s]  than keeping all the lines busy whenever there is a packet available for them, and the
[553.34s -> 557.16s]  expected delay is minimized because we are always doing useful work delivering packets
[557.16s -> 562.10s]  onto the outgoing line.
[562.10s -> 566.18s]  Just to recap the performance that we suffer from with head-of-line blocking, this was
[566.18s -> 571.28s]  our perfect output-queued switch here on the right, this nice performance here.
[571.28s -> 577.16s]  With head-of-line blocking, it is a well-known result that the throughput can be reduced,
[577.16s -> 582.22s]  in other words this asymptote when things fall apart, gets reduced to 2 minus square
[582.22s -> 584.26s]  root of 2 or approximately 58%.
[584.26s -> 588.66s]  We lose almost half the performance of the system as a consequence of this head-of-line
[588.66s -> 589.96s]  blocking.
[589.96s -> 594.82s]  The actual number will vary depending on the particular arrival pattern, but in general
[594.82s -> 597.20s]  it's pretty bad news.
[597.20s -> 604.44s]  But if we use virtual output queues, this 58% gets pushed back up again to the full
[604.44s -> 606.12s]  100% of the system.
[606.12s -> 608.20s]  It doesn't entirely match the output-queued switch.
[608.20s -> 612.76s]  The asymptote will still be 100% over here.
[612.76s -> 617.04s]  Actually with virtual output queues, the delay will be slightly higher, but the asymptote
[617.04s -> 620.16s]  is to 100%.
[620.16s -> 624.12s]  I'd like to say a few last words about virtual output queues.
[624.12s -> 628.08s]  Virtual output queues are actually used very widely, and you may even have noticed them
[628.08s -> 629.92s]  when driving on the street.
[629.92s -> 634.66s]  So in the US where we drive on the right, it's very common to have a left-hand turn
[634.66s -> 637.60s]  lane like the one that's shown here.
[637.60s -> 643.82s]  This is to hold cars that are arriving and that are blocked because of cars coming
[643.82s -> 644.88s]  the other way.
[644.88s -> 648.48s]  So these ones are blocked, and can't turn left until there's nothing coming the other
[648.48s -> 649.48s]  way.
[649.48s -> 654.08s]  However, cars in this lane here can keep going straight on, or can turn right.
[654.08s -> 658.72s]  They are not held up or blocked because of a packet ahead of it going to an output
[658.72s -> 663.62s]  that, in this case over here, which is temporarily unavailable.
[663.62s -> 667.62s]  So in countries where you drive on the left-hand side, then a right-hand turn lane is
[667.62s -> 669.12s]  quite common as well.
[669.12s -> 672.00s]  So next time you're driving and you see one of these, just remember, this is virtual
[672.00s -> 672.50s]  output queuing.
[675.24s -> 679.52s]  So in summary, we've seen that packet switches perform two basic operations.
[679.52s -> 681.52s]  They look up addresses in a forwarding table.
[681.52s -> 687.02s]  We saw examples of that in the last video for Ethernet switches and for Internet routers.
[687.02s -> 690.92s]  And they also, once they've decided where a packet is going, they have to switch it.
[690.92s -> 694.60s]  They have to deliver it to the correct egress port so it can go under the correct output
[694.60s -> 696.00s]  link.
[696.00s -> 700.64s]  The simplest and slowest switches use output queuing because this maximizes the throughput
[700.68s -> 706.00s]  and minimizes the expected delay of packets, whereas more scalable switches often use input
[706.00s -> 710.68s]  queues with virtual output queues to maximize the throughput.
[710.68s -> 711.60s]  That's the end of this video.
