# Detected language: en (p=1.00)

[0.00s -> 5.04s]  In this video I'm going to start out by telling you some of the shortcomings of a FIFO output
[5.04s -> 7.52s]  queue and some of the problems that it causes.
[7.52s -> 13.28s]  And I'm going to describe two alternatives, switches that provide strict priorities to
[13.28s -> 18.04s]  give high priority and low priority traffic, and those that can give a guaranteed rate
[18.04s -> 21.92s]  to each of the flows passing through it.
[21.92s -> 25.44s]  Let's start by reviewing what an output queued packet switch looks like.
[25.44s -> 29.56s]  This is an example we saw before where we had three packets arriving.
[29.56s -> 32.88s]  Their addresses will be looked up, and they will be switched to the correct output.
[32.88s -> 38.32s]  In this particular case, two red packets, meaning they'll go to that middle red output, and
[38.32s -> 40.40s]  a blue one to the top.
[40.40s -> 43.84s]  One of the red packets gets to go, the other one is held back waiting for the link to
[43.84s -> 47.28s]  be free, and then it goes on its way afterwards.
[47.28s -> 52.32s]  So implicit here is the assumption that the output queue is a FIFO.
[52.32s -> 53.72s]  First in, first out.
[53.72s -> 57.80s]  And that's a pretty reasonable assumption for most router queues.
[57.80s -> 61.16s]  But in what we're going to be looking at next, we're going to be focusing on this output
[61.16s -> 68.16s]  queue and seeing what some of the consequences are of it being a FIFO.
[68.16s -> 72.60s]  So a FIFO queue is sometimes called a free-for-all queue.
[72.60s -> 75.88s]  There are many packets flowing through this queue coming from the different inputs.
[75.88s -> 81.00s]  I've drawn here three inputs to the queue representing the three inputs to the other
[81.00s -> 82.48s]  inputs to the switch.
[82.48s -> 85.80s]  So these are all packets that are coming through that are part of flows that are going
[85.80s -> 87.18s]  to this one output.
[87.18s -> 92.06s]  So we'll see packets coming out of here coming from all of those inputs.
[92.06s -> 96.26s]  And presumably at any one time when we have congestion, we'll see packets queued up in
[96.26s -> 99.22s]  this FIFO queue.
[99.22s -> 104.18s]  If there are many flows passing through the queue, whoever sends at the highest rate,
[104.18s -> 109.46s]  whoever sends the most packets, will in fact receive the highest usage of this output
[109.46s -> 110.52s]  link.
[110.52s -> 114.26s]  So in other words, if this one up here is able to get a whole load of packets into
[114.26s -> 119.92s]  here, while this one down here, the bottom input, is only able to get a small number,
[119.92s -> 123.90s]  this guy up here is going to hog this output link.
[123.90s -> 128.32s]  So if there's a really big hog of a flow going through, a little flow could easily
[128.32s -> 131.22s]  get squeezed out completely.
[131.22s -> 136.38s]  People say that this kind of FIFO queue, while it's nice and simple, it encourages
[136.38s -> 137.80s]  bad behavior.
[137.80s -> 141.58s]  Because the best thing for a flow to do is to try and crowd out every other flow
[141.58s -> 142.90s]  by sending as fast as it can.
[143.02s -> 145.74s]  It would be a little bit like when you're downloading a web page.
[145.74s -> 149.76s]  The thing to do would be to try and get your packets sent towards you at the highest
[149.76s -> 154.64s]  possible rate to maximize the amount of the queue that you can get.
[154.64s -> 161.04s]  It's not very friendly behavior and it doesn't provide a good incentive for good behavior.
[161.04s -> 164.30s]  Now imagine that some of the traffic was very urgent.
[164.30s -> 166.34s]  For example, some control traffic.
[166.34s -> 172.78s]  So let's say we had some urgent red packets here, and then we had some less urgent green
[172.78s -> 174.92s]  packets elsewhere.
[174.92s -> 177.98s]  Maybe there's another one that's squeezed in in front of that, and a green packet down
[177.98s -> 182.12s]  here, and a green packet down here, and maybe let's just add in another red packet
[182.12s -> 183.62s]  for good measure.
[183.62s -> 189.32s]  So if these ones were more important, what the FIFO queue will do is simply send them
[189.32s -> 191.76s]  out in the order in which they came in.
[191.76s -> 196.82s]  So if we were to number the order in which they arrived, say this was number one, this
[196.82s -> 200.82s]  was number two, this was the third one to arrive, this one was the fourth one,
[200.86s -> 204.26s]  this is the fifth one and the sixth one, they're obviously just going to go out in that order
[204.26s -> 208.30s]  of one, two, three, four, five, six on the outgoing line.
[208.30s -> 213.14s]  So not very good for the urgent control traffic, or perhaps it's some important video
[213.14s -> 214.14s]  traffic.
[214.14s -> 217.48s]  So the FIFO doesn't have any way to distinguish important, it just says if you got here
[217.48s -> 222.26s]  first and there was room in the queue, you are the most important packet.
[222.26s -> 230.08s]  So we can't say anything meaningful about the rate of each flow sharing this queue.
[230.08s -> 234.88s]  One little observation that's going to prove useful later, and it's why I've labelled
[234.88s -> 239.66s]  this as the size of the queue as B and the rate at which it's being served, the outgoing
[239.66s -> 241.00s]  link R.
[241.00s -> 245.48s]  Notice that if a packet does make it into the queue, so if I have a packet that does
[245.48s -> 249.60s]  make it into the queue, let's say this one ends up at the tail of the queue, the
[249.60s -> 257.40s]  maximum time it has to wait is B over R. So the delay through that queue we know
[257.40s -> 259.68s]  is less than or equal to B over R.
[259.68s -> 262.88s]  So we're going to remember this, we're going to use this observation later.
[262.88s -> 267.56s]  In this video, I'm going to describe two alternatives to simple FIFO queuing.
[267.56s -> 273.60s]  The first one is called strict priorities, where we give higher priority to some flows
[273.60s -> 274.84s]  over others.
[274.84s -> 278.32s]  And the second one is rate guarantees, where we give a guaranteed rate or a guaranteed
[278.32s -> 282.22s]  fraction of the outgoing link to each of the flows.
[282.22s -> 286.40s]  So basically we're going to take our single FIFO that we had before and replace it with
[286.40s -> 287.96s]  a more complicated mechanism.
[287.96s -> 292.32s]  Here we've simply replaced it with a high priority queue and a low priority queue.
[292.32s -> 294.40s]  So the inputs are just the same as before.
[294.40s -> 297.54s]  These are where packets arrive from the incoming links.
[297.54s -> 301.32s]  But now, as a packet arrives, we're going to decide whether we're to place it into
[301.32s -> 305.32s]  the high priority queue or into a low priority queue, and we do this based on bits in
[305.32s -> 306.32s]  the header.
[306.32s -> 309.32s]  So when a packet arrives, it might have a bit in the header.
[309.32s -> 312.36s]  And in IP, there's a specific field for this.
[312.36s -> 315.56s]  It's called the type of service field.
[315.56s -> 321.00s]  And we might use that to decide which traffic is high priority and which is low priority.
[321.00s -> 326.80s]  We might do this, for example, to say that video traffic is more important than email.
[326.80s -> 330.96s]  So we might want to put the video in the high priority queue and the email in the
[330.96s -> 331.96s]  low priority queue.
[331.96s -> 337.16s]  Or we might say that control traffic is more important than data, because we always
[337.16s -> 342.44s]  want to have high priority for the management traffic on the network.
[342.44s -> 347.48s]  Or an operator might say that they're gold users, that their traffic takes strict preference
[347.48s -> 350.88s]  over their silver customers.
[350.88s -> 357.38s]  And so that's a way of classifying users and giving preference to those who pay more.
[357.38s -> 362.24s]  The way that this actually works is, when the packets arrive, so they would be placed,
[362.24s -> 366.28s]  and I'm going to put red packets in here, and I'm going to put green packets in here
[366.28s -> 371.84s]  for lower priority, the basic discipline is this.
[371.84s -> 377.28s]  Here's a scheduler that sits at the output here, and it's always going to take packets
[377.28s -> 380.00s]  from the high priority if they are there.
[380.00s -> 385.36s]  And it's only going to serve the low priority if there's nothing in the high priority queue.
[385.36s -> 390.08s]  The consequence is that high priority traffic doesn't see the low priority traffic.
[390.08s -> 391.58s]  It's unaffected by it.
[391.58s -> 396.52s]  Because we only serve the low priority queue if the high priority queue is empty.
[396.52s -> 399.80s]  It's as if the high priority traffic has its own private network and doesn't see
[399.80s -> 402.04s]  the low priority traffic at all.
[402.04s -> 407.08s]  This is great for many occasions where we want to give strict preference to another
[407.08s -> 411.12s]  one, but it does run the danger of starving out the low priority traffic.
[411.12s -> 415.12s]  So you can only use it when there's a reasonably small amount of high priority traffic.
[415.12s -> 419.76s]  We don't want to completely hog the link and starve out this low priority traffic
[419.76s -> 420.76s]  at all.
[420.76s -> 425.68s]  But it is quite widely used, and many switches and routers support this capability today.
[425.68s -> 431.20s]  What if instead of strict priorities, we wanted to have weighted priorities?
[431.20s -> 436.92s]  What I mean by that is, if a packet arrives into this queue here, and packets arrive
[436.92s -> 444.24s]  into this queue here, I want the, in some sense, for the traffic here to be considered
[444.24s -> 447.32s]  to be twice as important as this here.
[447.32s -> 453.32s]  Not always having strict preference, but having twice as many opportunities to send.
[453.32s -> 458.84s]  More precisely, I'm going to say that the rate at which this queue is served is going
[458.84s -> 466.00s]  to be 2 over 2 plus 1, so in other words 2 is a fraction of the total rate of the egress
[466.00s -> 467.00s]  link.
[467.00s -> 472.56s]  Likewise, I'm going to say that the rate that this one is going to be served is 1,
[472.56s -> 478.32s]  that's its weight, divided by the total weight times the outgoing link rate.
[478.32s -> 480.20s]  That's what I'm trying to accomplish.
[480.20s -> 485.96s]  I can generalize this to many queues, as follows, this is simply just increasing it
[485.96s -> 496.12s]  from 2 to n, where queue i is going to receive w sub i bits of service.
[496.12s -> 499.16s]  And that's the, that's its weight.
[499.16s -> 507.88s]  So for example, w1 here will have a rate, r1, is w1 divided by the sum of all of the
[507.88s -> 508.88s]  weights.
[508.88s -> 517.16s]  I'll write that as the sum over i of wi times r, all the way down to of course w sub n
[517.16s -> 523.04s]  just as before, r of n equals w of n over the sum, I'll just write it like that,
[523.04s -> 526.30s]  times r of the outgoing link.
[526.30s -> 530.24s]  If all the packets were of the same length, this would actually be pretty easy.
[530.24s -> 538.16s]  We would simply visit each of the queues in turn, and we will call that a round.
[538.16s -> 541.76s]  So one round is when we visited all of the queues in turn.
[541.76s -> 548.48s]  And then we would send w sub i units, so they could be bits or complete packets from
[548.48s -> 550.82s]  each queue in each round.
[550.82s -> 557.56s]  So on the outgoing line, we could have w sub 1 bits from here, then we would have
[557.56s -> 562.60s]  all the way through to w sub n bits from this one, and then all the intervening queues
[562.60s -> 563.60s]  as well.
[563.60s -> 567.32s]  And so this would be a round when we visited all of the queues.
[567.32s -> 572.24s]  And you can see that the proportion that each queue has been served in that round
[572.24s -> 576.08s]  is in proportion to the weights, which is exactly what we wanted.
[576.08s -> 580.72s]  So if we could serve the packets as bits at a time, and actually send them out as
[580.72s -> 584.20s]  bits at a time, which of course we can't, but if we could, then this would actually
[584.20s -> 586.96s]  be pretty easy to accomplish what we wanted.
[586.96s -> 593.04s]  We would simply classify the packets as they arrive into the queue that they are destined for.
[593.04s -> 599.04s]  And then we would serve those queues according to the w sub i bits in each round,
[599.04s -> 601.42s]  and then send them out.
[601.42s -> 606.68s]  Of course, packets are variable length, and they don't consist of single bits.
[606.68s -> 614.72s]  And the problem is that real packets vary in size from something like 64 bytes all the
[614.72s -> 619.52s]  way to, in the case of Ethernet, about 1500 bytes.
[619.52s -> 621.72s]  There are jumbo frames that are even longer than this.
[621.72s -> 627.48s]  But even here we've got two orders of magnitude difference in packet size.
[627.48s -> 630.96s]  So if we were to serve this packet by packet instead of bit by bit, it would really
[630.96s -> 635.00s]  mess up the weights, and we wouldn't accomplish what we were trying to do.
[635.00s -> 639.16s]  Clearly, we must take into account the packet lengths if we want to prevent long packets
[639.16s -> 642.96s]  from creating out the short ones.
[642.96s -> 645.24s]  So let me describe how we do this.
[645.24s -> 649.92s]  I'm going to first describe it in terms of a kind of a thought experiment.
[649.92s -> 654.84s]  I'm going to use this notion of rounds again, where we visit each queue in turn in
[654.84s -> 664.20s]  a round, and then we're going to send w sub i bits from each queue during that round.
[664.20s -> 669.60s]  But I'm going to assume that in addition to the queues that I have here, that I have
[669.60s -> 674.80s]  another, I'm going to call it a magic queue, just to remind us that this isn't really
[674.80s -> 675.80s]  a queue.
[675.80s -> 679.48s]  It's just going to be a processing element, just to help us think about the problem.
[679.48s -> 681.72s]  We're going to get rid of this in a minute.
[681.72s -> 687.44s]  So in a round, the first queue gets to send w sub 1 bits, and the last queue gets
[687.44s -> 690.76s]  to send w n bits.
[690.76s -> 695.16s]  What we're going to do is we're going to imagine that we're going to serve each
[695.16s -> 699.20s]  of these queues by that number of bits in each round.
[699.20s -> 705.40s]  And then when we get to an end-of-packet marker, which is the last bit in a packet,
[705.40s -> 709.28s]  so this would be the end-of-packet here, and then let's say this is the end-of-packet
[709.28s -> 714.44s]  here, once we've got to that end-of-packet marker, we will construct complete packets
[714.44s -> 716.16s]  and send them onto the outgoing link.
[716.16s -> 718.28s]  So that's what this magic queue is going to do.
[718.28s -> 722.72s]  It's going to turn those bit-by-bit into packet-by-packet.
[722.72s -> 729.16s]  And so this will be the end-of-packet bit here, and this is the end-of-packet bit here.
[729.16s -> 732.88s]  But this is recognizing that we can't send them out as bits.
[732.88s -> 737.44s]  We wait until a full packet is accumulated and then send them out.
[737.44s -> 742.44s]  So the question is, in what order should we be sending these out onto the line?
[742.44s -> 744.44s]  When should we send them?
[744.44s -> 750.12s]  Because our goal is to meet the rate guarantees where each queue gets that weighted fair share
[750.12s -> 752.00s]  of the outgoing line.
[752.00s -> 755.76s]  So in what order should we send these packets in order to accomplish that?
[755.76s -> 758.32s]  I'm going to describe that next.
[758.32s -> 763.12s]  So just as before, we're going to assume that time proceeds in rounds.
[763.12s -> 766.26s]  So our unit of time is going to be rounds.
[766.26s -> 773.54s]  And we're going to figure out, if we were to service the packets bit-by-bit, which
[773.54s -> 775.72s]  round would they have finished in?
[775.72s -> 782.24s]  Okay, so if we were to serve them bit-by-bit, which round would they have finished in?
[782.24s -> 789.44s]  Well, I'm going to start by making an observation that will give you a clue as to how we're
[789.44s -> 791.28s]  going to use this.
[791.28s -> 794.92s]  Let's consider a packet here that's waiting to go.
[794.92s -> 799.00s]  And let's consider the round in which it starts, we'll call that s-of-k, and the
[799.00s -> 803.68s]  round in which it finishes, and we'll call that f-of-k.
[803.68s -> 808.80s]  Because we're serving everything in rounds and time progresses in rounds, we can say
[808.80s -> 815.80s]  the finishing time of this packet is its starting time in rounds plus the length of the packet
[815.80s -> 819.20s]  divided by w-of-1.
[819.20s -> 825.76s]  That's because that first queue will receive exactly w-sub-1 bits of service in each round.
[825.76s -> 829.78s]  So its finishing round is its starting round plus its length divided by the number of
[829.78s -> 832.04s]  bits it gets served per round.
[832.04s -> 836.32s]  So this is the finishing time here.
[836.32s -> 840.08s]  Now let's think about what happens when a packet arrives.
[840.08s -> 845.16s]  So we're going to try and calculate the starting time of that packet when it arrives.
[845.16s -> 848.68s]  In other words, what time will it enter service?
[848.68s -> 850.80s]  And what time will it finish service?
[850.80s -> 853.60s]  Might be surprising that we can do this, but I'm going to show you a way we can
[853.60s -> 858.52s]  calculate both its starting time and its finishing time when it arrives.
[858.52s -> 863.68s]  So the starting time of that packet, the time at which it starts to enter service,
[863.72s -> 866.60s]  is going to depend on what's ahead of it in the queue.
[866.60s -> 872.28s]  So if we're keeping track of the finishing time of this packet in rounds, and we want
[872.28s -> 878.76s]  to know what the starting time of this next packet here, and its finishing time,
[878.76s -> 880.12s]  we can do the following calculation.
[880.12s -> 883.24s]  So this is the finishing time of the k-1 packet.
[883.24s -> 885.36s]  It's the one that's ahead of it in queue.
[885.36s -> 890.74s]  We can say that the starting time of the packet k is going to be simply the finishing
[890.78s -> 897.58s]  time of the packet ahead of it, because we're proceeding in rounds, so it will be immediately
[897.58s -> 902.06s]  entering service as soon as the one ahead of it is finished, unless the queue happens
[902.06s -> 905.70s]  to be empty and there's nothing ahead of us, in which case it will enter service
[905.70s -> 906.70s]  now.
[906.70s -> 910.66s]  There's going to be the max of these two values, the max of the finishing time
[910.66s -> 914.50s]  of the packet ahead of it, and now.
[914.50s -> 919.34s]  The second thing that we can say is that the finishing time of k is its starting
[919.38s -> 925.30s]  time plus L over W1, just as before, because we know that's how much service.
[925.30s -> 931.06s]  So the combination of these two gives us a recursion that so long as we keep calculating
[931.06s -> 935.82s]  the finishing time and keep track of that, we can calculate the starting time and the
[935.82s -> 938.26s]  finishing time of packets as they arrive.
[938.26s -> 942.26s]  That's a pretty neat property.
[942.26s -> 944.78s]  And then what we're going to do is we're going to service the packets.
[944.78s -> 948.42s]  In other words, we're going to take the packets from head of line, so here is a scheduler.
[948.42s -> 952.70s]  It's going to be examining the head of line packets, and it's going to pick the one with
[952.70s -> 958.18s]  the lowest f, the lowest finishing time, so it'll pick the packet with the lowest finishing
[958.18s -> 959.18s]  time.
[959.18s -> 960.90s]  So that's what the scheduler does.
[960.90s -> 966.62s]  So we calculated the f of k when the packet came in, and then when it gets to the head
[966.62s -> 970.98s]  of line, it's competing with all the head of line packets to leave, and the scheduler
[970.98s -> 975.46s]  is simply going to pick the one with the lowest finishing time.
[975.46s -> 980.46s]  This has the nice property that finishing times can be determined when the packet arrives,
[980.46s -> 984.30s]  and the packets are served in order of their finishing time, which at least intuitively
[984.30s -> 985.98s]  seems like the best thing to do.
[985.98s -> 990.54s]  It turns out that it's more than just intuitively a good thing to do.
[990.54s -> 996.02s]  I'll show you why it actually is the right thing to do.
[996.02s -> 1001.90s]  If we plot the finishing time of the packets, if they were being served bit by bit, in
[1001.90s -> 1006.90s]  other words the time that the end-of-packet bit would leave on the outgoing line if the
[1006.90s -> 1012.90s]  packet was being served bit by bit, it might look something like this.
[1012.90s -> 1017.30s]  If we look at the finishing time of the packet-by-packet scheme, where each packet
[1017.30s -> 1022.30s]  goes and must wait for the packet head of it to finish before it can go on the line,
[1022.30s -> 1025.26s]  packets could end up actually departing a little bit later because they could be held up
[1025.26s -> 1027.62s]  by a packet that's still in service.
[1027.62s -> 1032.22s]  So there could be a little bit of a delta between when the packet would finish bit by
[1032.22s -> 1033.22s]  bit.
[1033.22s -> 1037.86s]  And we know if it's bit by bit, then the rate that it will receive, the QI would
[1037.86s -> 1045.54s]  receive, will be W sub i over the sum of the Wj's, in other words the weight that
[1045.54s -> 1047.26s]  we're looking for of the outgoing line.
[1047.26s -> 1052.38s]  So if this was being met, then we know that would be true.
[1052.38s -> 1056.18s]  In the packet-by-packet scheme, it can be proved that the difference in the time
[1056.74s -> 1062.74s]  that it will depart under the packet-by-packet scheme is no more than Lmax, the maximum length
[1062.74s -> 1068.54s]  packet divided by R, later than under the bit-by-bit scheme.
[1068.54s -> 1072.58s]  And that's true for every single packet in the system.
[1072.58s -> 1077.78s]  So this is really useful, because over a long period of time, this tells us that the same
[1077.78s -> 1081.10s]  number of bits will have departed as under the bit-by-bit scheme.
[1081.10s -> 1084.36s]  They'll just be jiggled around a little bit, there'll be little variants in the
[1084.56s -> 1087.24s]  actual departure time, but measured over a long period of time.
[1087.24s -> 1093.78s]  So under this packet-by-packet scheme, R of i will be the same as it was before.
[1093.78s -> 1101.80s]  So it'll be that weight of i divided by the sum over Wj of R. So this will accomplish
[1101.80s -> 1105.74s]  exactly the rates that we want.
[1105.74s -> 1110.28s]  This scheme is often called WFQ, or weighted fair queuing.
[1110.28s -> 1113.38s]  Weighted fair queuing is a pretty famous technique.
[1113.38s -> 1115.98s]  You can find lots of references to it.
[1115.98s -> 1120.42s]  It's also known as packet-by-packet generalized processor sharing, but if you look under WFQ
[1120.42s -> 1122.42s]  you can find lots of references to it.
[1122.42s -> 1126.58s]  But what it essentially tells us is, it tells us a specific mechanism for calculating
[1126.58s -> 1131.66s]  the finishing time of packets and scheduling them, so that we can give weighted fairness,
[1131.66s -> 1139.46s]  weighted usage of the outgoing link, and rate guarantees to each of the flows.
[1139.46s -> 1142.54s]  In summary, FIFO queues are a bit of a free-for-all.
[1142.54s -> 1147.98s]  They have no priorities, no guaranteed rates, and there's an incentive for a flow to send
[1147.98s -> 1152.74s]  as many packets as it can into the queue, so that it maximizes its share of the outgoing
[1152.74s -> 1153.74s]  link.
[1153.74s -> 1155.82s]  So they kind of encourage bad behavior.
[1155.82s -> 1159.42s]  So instead, it's quite common to use strict priorities.
[1159.42s -> 1164.86s]  High priority traffic could see or experience a network which appears to have no low priority
[1164.86s -> 1165.86s]  traffic at all.
[1165.86s -> 1167.02s]  It's unaffected by it.
[1167.02s -> 1171.18s]  This is useful if we have limited amounts of high priority traffic, like control traffic
[1171.18s -> 1172.62s]  in the network.
[1172.62s -> 1176.74s]  But if we want to do something which is more of a weighted priority, then we need to use
[1176.74s -> 1181.02s]  something like weighted fair queuing, which lets us give each flow a guaranteed service
[1181.02s -> 1185.58s]  rate, and we do that by scheduling the packets in order of their bit-by-bit finishing
[1185.58s -> 1186.58s]  times.
[1186.58s -> 1187.98s]  That's the end of the video.
