# Detected language: en (p=1.00)

[0.00s -> 6.00s]  In the last video, I told you about different types of congestion, the different timescales
[6.00s -> 9.56s]  that it can occur at, and what some of the consequences might be.
[9.56s -> 15.92s]  We then looked a little bit at the characteristics of congestion control algorithms that we
[15.92s -> 18.06s]  might like to try and design.
[18.06s -> 21.90s]  So we said that we wanted high throughput, we wanted them to be fair amongst the flows
[21.90s -> 26.30s]  competing for the bottleneck links, we wanted the control of congestion to be distributed
[26.30s -> 28.82s]  so that it can scale.
[28.82s -> 34.24s]  In this video, we're going to start looking at basic approaches to controlling congestion.
[34.24s -> 38.22s]  We're going to consider whether the congestion control should take place in the network,
[38.22s -> 42.68s]  with specific support at the routers, or whether it should be done at the end hosts.
[42.68s -> 45.28s]  And then I'm going to tell you a little bit about how TCP does it.
[45.28s -> 50.26s]  We're going to start with the basic mechanism called AIMD, or Additive Increase Multiplicative
[50.26s -> 54.50s]  Decrease, and we're going to study how that works over the next couple of videos,
[54.50s -> 60.20s]  before we look in more detail at how TCP congestion control really works in practice.
[60.20s -> 64.16s]  So I'm going to start with a consideration of where to put congestion control.
[64.16s -> 69.72s]  In fact, you may have already been wondering why it is that we can't simply use fair
[69.72s -> 71.08s]  queuing.
[71.08s -> 74.40s]  Notice that we've already seen a way to give everyone a fair share of the outgoing
[74.40s -> 82.04s]  link by simply decomposing the output buffer into per-flow queues, as are shown here,
[82.04s -> 86.38s]  so that if we've got multiple flows going through the network, then each flow would
[86.38s -> 91.46s]  be placed into its own queue, and then we use a fair queuing scheduler to divide
[91.46s -> 96.58s]  up that egress rate, let's say R, amongst all of the flows that are contending for
[96.58s -> 97.58s]  it.
[97.58s -> 101.08s]  And so if they're all wanting to send at rate greater than R over 2, then they
[101.08s -> 105.74s]  would each receive R over 2, because that's what a fair queuing scheduler would do.
[105.74s -> 111.74s]  And in fact, this will give us not only a fair behavior, it will actually give us the
[111.76s -> 116.28s]  max-min-fair at every link across the flows, it'll give us good throughput whenever
[116.28s -> 120.32s]  there is work to be done, it will always keep the outgoing line busy.
[120.32s -> 125.36s]  We say it's work conserving, so it'll give good throughput on each of the links.
[125.36s -> 128.06s]  So what's wrong with this basic mechanism?
[128.06s -> 131.96s]  Well the first thing is that it isn't responsive.
[131.96s -> 135.16s]  It's simply going to divide up the links.
[135.16s -> 139.32s]  But there's nothing here that will tell the sources the rate at which they should
[139.32s -> 143.02s]  send, or give them any indication of how many packets they should send.
[143.02s -> 147.54s]  In fact if they do send, so if these are each trying to send at the full blast rate,
[147.54s -> 153.38s]  so if there are packets coming in from all directions trying to use these links,
[153.38s -> 158.10s]  then packets will simply be dropped onto the floor as the buffers overflow, and we'll
[158.10s -> 163.34s]  end up wasting a lot of the upstream bandwidth delivering packets over links that eventually
[163.34s -> 165.30s]  get dropped downstream.
[165.30s -> 171.86s]  So we need a way of signalling back to the sources to say, give them some indication
[171.86s -> 175.84s]  of the rate at which they should send, or the number of outstanding packets that they
[175.84s -> 179.64s]  can have in the network.
[179.64s -> 184.64s]  So in network-based congestion control, there is explicit feedback that comes from the routers
[184.64s -> 187.62s]  to indicate congestion in the network.
[187.62s -> 196.56s]  So for example, if I have a source A and a destination B, and then some routers in between,
[196.56s -> 201.82s]  with some links like this, let's imagine that there are some flows in the network
[201.82s -> 206.82s]  that are coming in from different directions going through this router, causing some congestion
[206.82s -> 210.30s]  to take place right here.
[210.30s -> 214.06s]  One thing that we can do is, if there is congestion, is to try and signal back
[214.06s -> 221.58s]  to A some signal to say, there is congestion in the network, you need to reduce the number
[221.58s -> 225.78s]  of packets that you have outstanding, or reduce the rate at which you send them.
[225.78s -> 232.34s]  And so the question is, what would we send, and how would we get it back to A?
[232.34s -> 237.82s]  We could for example say, I'm dropping a packet, or it could be an indication of
[237.82s -> 243.74s]  the occupancy of the buffer, or it could mark that we've just crushed some threshold,
[243.74s -> 246.62s]  and so we're getting more congested.
[246.62s -> 250.18s]  Any of these would be examples of congestion.
[250.18s -> 255.06s]  Another one might be that the outgoing link has a certain amount of capacity left over,
[255.06s -> 260.26s]  and as the capacity gets used up, we send a signal back to say how much of that capacity
[260.26s -> 261.38s]  is available.
[261.38s -> 265.64s]  Or it could be a function of all of the signals that I've just mentioned.
[265.64s -> 269.74s]  So the next question is, how do we get that signal back, and how many bits do we use
[269.74s -> 271.62s]  to represent it?
[271.62s -> 276.50s]  If we're sending back the whole queue occupancy, we'd really like to be able to send a sizable
[276.50s -> 279.34s]  integer value to indicate what the current occupancy is.
[279.34s -> 283.18s]  That would take a lot of bits, and it might be complicated, so in practice, generally
[283.18s -> 289.46s]  people look for schemes that use one or a couple of bits to signal back to the source.
[289.46s -> 292.50s]  And then the next question is, how do you get them back to the source?
[292.50s -> 296.22s]  There's no point in creating a whole packet just to send it back to the source if we
[296.22s -> 299.06s]  can piggyback on packets that are already going by.
[299.50s -> 306.98s]  It's fairly common to use packets, for example, if there's a TCP packet that's coming through,
[306.98s -> 312.38s]  or some kind of two-way communication, to piggyback onto packets going in one direction,
[312.38s -> 317.90s]  such that they get sent back in the acknowledgments, and eventually get back to the source.
[317.90s -> 322.64s]  There's one particular technique that's called ECN, or Explicit Congestion Notification,
[322.64s -> 328.04s]  in which the routers indicate whether they have some degree of congestion, for example
[328.04s -> 329.56s]  crossing a threshold.
[329.56s -> 334.64s]  They then mark bits in packets going towards the destination, which then copies those bits
[334.64s -> 338.52s]  back into the acknowledgments going in the other direction.
[338.52s -> 344.00s]  The original scheme that was designed to work somewhat like this was called DECBit,
[344.00s -> 351.28s]  that was proposed more than 20 years ago as a single bit mechanism to signal to the
[351.28s -> 356.00s]  source to slow down.
[356.00s -> 358.96s]  A nice advantage of a scheme like this is it's simple to understand.
[358.96s -> 364.00s]  We can see that the signal will directly control the behaviour of the source.
[364.00s -> 368.68s]  It should be pretty responsive to change, because we can detect the onset of congestion
[368.68s -> 372.60s]  in the network, and be able to tell the source.
[372.60s -> 376.16s]  It's distributed in the sense that the signal is coming back from all of the routers
[376.16s -> 382.16s]  in the network, and it only affects the source, and so the source can make up its mind
[382.16s -> 384.96s]  on how it will process that signal.
[384.96s -> 389.04s]  And it can be made to be maxmin-fair, so it can be made to be fair.
[389.04s -> 393.20s]  For example, measure the rate of each flow through the router, and pass back the maxmin-fair
[393.20s -> 394.76s]  allocation for each flow.
[394.76s -> 398.36s]  There are other ways that are simpler, for example using fair queuing as I described
[398.36s -> 399.36s]  before.
[399.36s -> 404.20s]  So network-based could certainly work.
[404.20s -> 410.56s]  On the other hand, it's worth asking the question of whether we actually need the network
[410.56s -> 412.92s]  to provide any congestion notification.
[412.92s -> 418.28s]  In other words, can we support congestion control without any support from the network
[418.28s -> 422.94s]  at all, merely by implementing a mechanism at the end hosts, where it's just going
[422.94s -> 426.96s]  to simply observe the network behaviour?
[426.96s -> 434.56s]  So going to the example that I had before, if I have end hosts A and B, and then routers
[434.56s -> 444.56s]  in between, if I am able to observe behaviour of the network, such that it's enough to
[444.56s -> 448.80s]  be able to decide at what rate I send, or how many outstanding packets I have in the
[448.80s -> 454.76s]  network, then perhaps we can implement a congestion control mechanism this way.
[454.76s -> 459.96s]  This is nice because if it doesn't depend on the behaviour of the routers, or it doesn't
[459.96s -> 464.52s]  behave on them sending specific information back, we can evolve and adapt it over time
[464.52s -> 467.16s]  without having to change the network in between.
[467.16s -> 470.16s]  We're going to see that TCP does this.
[470.16s -> 475.48s]  TCP actually does congestion control purely at the end host by observing the network
[475.48s -> 476.78s]  behaviour.
[476.78s -> 483.20s]  What it's going to do is, if packets are dropped along the way, it's going to observe
[483.20s -> 489.80s]  this through either a timeout, or it will see a sequence of acknowledgements that are
[489.80s -> 495.56s]  all the same coming back, because the data was missing, and so B is going to keep acknowledging
[495.56s -> 501.04s]  an earlier piece of data, which we can interpret as data missing, and therefore
[501.04s -> 503.00s]  needing to retransmit it.
[503.00s -> 507.32s]  So if there's been data that's dropped, A could interpret this as congestion, and
[507.32s -> 512.52s]  then slow down the rate, or have a fewer number of outstanding packets, so that it will
[512.56s -> 517.04s]  reduce the congestion in the network.
[517.04s -> 522.88s]  So basically, A is going to observe, it's going to, it's a little bit like it's observing
[522.88s -> 528.08s]  the behaviour in the network, and seeing what happens in terms of timeouts and duplicate
[528.08s -> 532.76s]  acknowledgements and anything that indicates a drop, it could also see an increase in
[532.76s -> 537.64s]  delay or variance, any of the things that would indicate to it that congestion is occurring
[537.64s -> 541.48s]  so that it can change its behaviour accordingly.
[541.48s -> 547.00s]  In TCP's case, it actually has to do this, because IP offers no support by default.
[547.00s -> 550.60s]  IP offers no indication of congestion in the network.
[550.60s -> 555.44s]  So when TCP was first conceived, it was actually by necessity that it would control
[555.44s -> 557.24s]  congestion this way.
[557.24s -> 560.88s]  So let me give you a quick introduction to TCP congestion control.
[560.88s -> 566.60s]  TCP implements congestion control at the end host, because the network provides no support.
[566.60s -> 569.84s]  It reacts to events observable at the end host.
[569.84s -> 574.08s]  In particular, it's going to use packet loss, or if it believes that there were packets
[574.08s -> 576.04s]  that were dropped.
[576.04s -> 581.84s]  It's going to exploit TCP's sliding window that we use for flow control and retransmissions.
[581.84s -> 586.04s]  It's going to exploit the fact that that's there, and it's going to overload it with
[586.04s -> 590.60s]  a means to control congestion, and I'm going to be explaining that shortly.
[590.60s -> 595.12s]  And the way it's going to do this is it's going to try and figure out how many packets
[595.12s -> 599.64s]  it can safely have outstanding in the network at any time.
[599.64s -> 601.48s]  This is an important concept.
[601.48s -> 602.48s]  Let me repeat it.
[602.48s -> 607.08s]  It's going to try and figure out how many packets it can safely have outstanding in
[607.08s -> 609.64s]  the network at any time.
[609.64s -> 613.76s]  Now we're familiar with this already with the sliding window used in TCP, and this
[613.76s -> 617.72s]  is just a reminder of how the sliding window works.
[617.72s -> 624.12s]  Recall that the window is sliding over a stream of bytes, so this is the underlying stream
[624.12s -> 626.20s]  of bytes that we're sending.
[626.20s -> 632.72s]  And that is increasing to the right, so byte 0 was somewhere over here.
[632.72s -> 638.68s]  And the window is telling us data that has been acknowledged.
[638.68s -> 643.16s]  So this is earlier data which has been fully acknowledged.
[643.16s -> 647.84s]  This is outstanding data that has been sent, but not yet acknowledged.
[647.84s -> 649.44s]  This is data that's OK to send.
[649.44s -> 653.04s]  In other words, it's data that we perhaps haven't sent yet, but because it's inside
[653.08s -> 656.28s]  the window we're allowed to send it if we want.
[656.28s -> 659.92s]  And then there is data that is not OK to send yet because it's ahead of the window.
[659.92s -> 663.52s]  The window hasn't slid over the top of this yet, because we're still waiting
[663.52s -> 667.12s]  for outstanding acknowledgements over here.
[667.12s -> 673.72s]  OK, so the sliding window tells us not only which bytes can be outstanding, but also
[673.72s -> 674.92s]  how many bytes.
[674.92s -> 676.32s]  That's the window size.
[676.32s -> 681.72s]  And you will recall that the receiver is going to send back information about what's called
[681.72s -> 686.64s]  the receive window to tell us how many bytes we can have outstanding, such that we don't
[686.64s -> 688.64s]  overrun the receiver.
[688.64s -> 691.68s]  And we're going to see in a minute that we're going to reuse that mechanism in
[691.68s -> 695.36s]  a different way at the sender.
[695.36s -> 700.52s]  But just to give a rough idea of what's going on, with the TCP sliding window, here
[700.52s -> 705.28s]  is a view on a timeline of what's taking place when packets are sent and received.
[705.28s -> 709.36s]  And it's going to give us a feeling for how this is going to work.
[709.36s -> 715.52s]  So A is allowed to send up to a window's worth of data and have it outstanding before it
[715.52s -> 717.28s]  receives any acknowledgements.
[717.28s -> 721.44s]  So here is that window of data.
[721.44s -> 726.48s]  And when those packets are sent, of course each one of them is going to lead to an
[726.48s -> 727.48s]  acknowledgement.
[727.48s -> 731.64s]  So sometime later, we are going to get the acknowledgements, and then we're going
[731.64s -> 734.44s]  to send the next window's worth of data.
[734.44s -> 740.72s]  So if the round-trip time is much bigger than the window size, in other words the time
[740.72s -> 744.28s]  is much bigger than the amount of data that it takes to fill that pipe, then there will
[744.28s -> 749.12s]  be this big delay in between, and TCP will basically move forward by sending a window
[749.12s -> 754.64s]  in a burst, pausing and waiting for acknowledgements, sending a window in a burst, having a pause,
[754.64s -> 756.28s]  and then just repeating like that.
[756.28s -> 759.00s]  So that's in this particular case.
[759.00s -> 764.80s]  Now let's consider a different case, and that is when the round-trip time equals the
[764.80s -> 765.80s]  window size.
[765.80s -> 769.40s]  In other words, the window is exactly able to fill up the pipe.
[769.40s -> 772.96s]  The number of outstanding packets that we're allowed to have in the network precisely
[772.96s -> 774.86s]  fills the pipe.
[774.86s -> 779.96s]  In this particular case, the first acknowledgement will come back just after the last packet
[779.96s -> 784.08s]  has been sent, and so we're able to send in a continuous stream.
[784.08s -> 788.24s]  And so there are no pauses, therefore we're using the network more fully than in this
[788.28s -> 791.96s]  case when we've got this idle time.
[791.96s -> 796.34s]  So this gives us a hint as to our ability to keep the network full.
[796.34s -> 801.04s]  Some people would interpret this as a rate, because it's the window size divided by the
[801.04s -> 805.40s]  round-trip time, and we're going to consider that a little bit later.
[805.40s -> 809.70s]  So that's the basic idea of how this is going to work.
[809.70s -> 815.48s]  More specifically, with TCP congestion control, TCP is going to vary the number of outstanding
[815.48s -> 819.44s]  packets in the network by varying the window size.
[819.44s -> 824.56s]  And it's going to set the window size instead of just being the advertised window, which
[824.56s -> 830.20s]  is what it used before, which came from the receiver to stop overwhelming the receiver,
[830.20s -> 834.44s]  it's also going to take into consideration something called the congestion window.
[834.44s -> 837.48s]  This is something which is calculated at the source.
[837.48s -> 841.30s]  So the advertised window comes from the receiver, and at the source or the transmitter,
[841.30s -> 846.78s]  it's going to calculate the congestion window that's often abbreviated to CWND, CWND stands
[846.78s -> 849.38s]  for congestion window.
[849.38s -> 851.50s]  And then it will take whichever is the smaller value.
[851.50s -> 855.38s]  In other words, if the network is congested, then it's going to use CWND, and if the
[855.38s -> 860.36s]  network is not congested, then it will be dominated by the receive window, the one
[860.36s -> 862.66s]  advertised by the receiver.
[862.66s -> 866.54s]  So the next question to ask is, OK, how do we decide the value for CWND?
[866.54s -> 871.82s]  How are we going to use CWND in order to change the window size to control congestion
[871.82s -> 873.66s]  in the network?
[873.66s -> 877.62s]  And the scheme that we're going to use is called AIMD.
[877.62s -> 882.98s]  And this is a sort of a classic technique in networking that's used for controlling
[882.98s -> 887.14s]  congestion in a TCP network.
[887.14s -> 890.66s]  And it could be used in any network that uses sliding windows.
[890.66s -> 895.94s]  AIMD stands for additive increase and multiplicative decrease.
[895.94s -> 897.98s]  Let's start with the additive increase.
[897.98s -> 904.22s]  The way that the window size is going to evolve is as follows, or rather, CWND.
[904.22s -> 909.44s]  If every time a packet is received correctly by the sender, it's going to increase the
[909.44s -> 916.90s]  window size, in fact CWND, by 1 over W. What this means is that every time a complete
[916.90s -> 924.02s]  window's worth of data has been correctly received and acknowledged, then the sender
[924.02s -> 927.30s]  is going to increase its window size by 1.
[927.30s -> 931.78s]  It'll increase it by 1 over W for every packet, because there are W packets, then by the
[931.78s -> 934.98s]  end of the window it will have increased it by 1.
[934.98s -> 936.38s]  So this is the additive increase.
[936.38s -> 939.74s]  It's going to slowly increase when things are going well.
[939.74s -> 943.20s]  If things are going badly and packets are dropped, then it's going to use this as
[943.20s -> 947.08s]  a signal of congestion.
[947.08s -> 951.74s]  And if that happens, it's going to reduce the CWND by a factor of 2.
[951.74s -> 955.38s]  It's going to halve it.
[955.38s -> 961.14s]  What this will look like is, if we draw the window as a function of time, so this will
[961.14s -> 968.90s]  be the CWND as a function of time, it's going to start by increasing every time
[968.90s -> 970.42s]  we have a success.
[970.42s -> 974.94s]  And then when we have a drop, so here is the drop taking place here, it's going
[974.94s -> 979.58s]  to drop down to half of its value.
[979.58s -> 987.18s]  So if this is the peak value, then this value down here would be Wpeak over 2.
[987.18s -> 990.74s]  And then it's going to start increasing again and increasing and increasing until it has
[990.74s -> 992.08s]  another drop.
[992.08s -> 994.06s]  And then it's going to increase again and increase again.
[994.06s -> 998.14s]  And it could go up to a higher value, because now the network may be allowed more outstanding
[998.14s -> 1001.26s]  packets, come down to a different value, and then go up, and then there might be
[1001.26s -> 1002.26s]  another drop.
[1002.26s -> 1006.74s]  So it isn't always going to go in this nice, neat symmetrical sawtooth.
[1006.74s -> 1008.66s]  This is where the drops are taking place.
[1008.74s -> 1010.54s]  And it's halving at each case.
[1010.54s -> 1014.52s]  So here is the additive increase, here is the multiplicative decrease.
[1014.52s -> 1017.62s]  The additive increase, the multiplicative decrease.
[1017.62s -> 1022.70s]  This is often referred to as the TCP sawtooth or the AIMD sawtooth, just because of its
[1022.70s -> 1024.46s]  shape.
[1024.46s -> 1028.38s]  If we zoom in, let's take a closer look at what's going on at each step.
[1028.38s -> 1030.70s]  So let's take a closer look at what's going on here.
[1030.70s -> 1036.18s]  This is actually proceeding by going in steps.
[1036.38s -> 1041.62s]  Remember it's going in steps such that every packet time it's going to increase by 1 over
[1041.62s -> 1043.10s]  w.
[1043.10s -> 1049.30s]  I'm going to simplify that by saying every RTT, this horizontal dimension is time, it's
[1049.30s -> 1051.10s]  going to increase by 1.
[1051.10s -> 1053.38s]  The window sizes are going to increase by 1.
[1053.38s -> 1058.82s]  Because every time we've acknowledged a complete window's worth of data, it's going to increase
[1058.82s -> 1060.34s]  the window size by 1.
[1060.34s -> 1065.86s]  So it's going to go forward in the steps of RTT along the horizontal part of the stair
[1065.86s -> 1070.34s]  and then it's going to go up by 1 and then RTT and so on.
[1070.34s -> 1075.16s]  So this leads to what's often called the AIMD sawtooth or the TCP sawtooth that can look
[1075.16s -> 1076.16s]  like this.
[1076.16s -> 1081.76s]  This is an evolution of Seawind, remember that's the congestion window, as a function
[1081.76s -> 1082.78s]  of time.
[1082.78s -> 1086.54s]  So here was the additive increase, we had a drop, we dropped down to half the value,
[1086.54s -> 1090.34s]  we had an additive increase, we dropped down because of a drop that took place, packet
[1090.34s -> 1092.14s]  drop that took place here.
[1092.14s -> 1095.74s]  Then we go up again through the additive increase and you can see here that the available
[1095.74s -> 1099.54s]  window size, in other words the amount of data that the source can have outstanding
[1099.54s -> 1103.80s]  in the network, is varying presumably because the network conditions are changing.
[1103.80s -> 1107.70s]  There are other flows in the network or maybe even the capacity of the links is changing,
[1107.70s -> 1110.46s]  maybe they're wireless links for example.
[1110.46s -> 1114.42s]  So in summary, we have choice when we're implementing a congestion control algorithm.
[1114.42s -> 1118.78s]  We can implement it in the network or we can implement it at the end host.
[1118.78s -> 1124.02s]  TCP controls congestion from the end host because IP offers it no support by default,
[1124.02s -> 1128.84s]  so it gives it no signals or indication of congestion other than dropping packets.
[1128.84s -> 1134.62s]  So it merely reacts to events that are observable at the end host, in particular packet loss.
[1134.62s -> 1140.86s]  It exploits TCP's sliding window that's used for flow control and it's going to
[1140.86s -> 1146.42s]  overload that sliding window by changing the window size to try and control congestion.
[1146.42s -> 1149.94s]  It tries to figure out how many packets it can safely have outstanding in the network
[1149.94s -> 1154.82s]  at a time and it's going to vary that window size according to the additive increase
[1154.82s -> 1159.18s]  multiplicative decrease algorithm and we're going to be studying that more in the next
[1159.18s -> 1160.02s]  two videos.
