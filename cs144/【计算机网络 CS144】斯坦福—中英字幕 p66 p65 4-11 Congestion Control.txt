# Detected language: en (p=1.00)

[0.00s -> 4.80s]  In this unit, you've seen how transport and packet switching interact through congestion
[4.80s -> 6.68s]  control.
[6.68s -> 8.92s]  Flow control is about the end hosts.
[8.92s -> 13.08s]  It ensures that the source host doesn't overwhelm the destination host by sending
[13.08s -> 15.84s]  more than it can receive.
[15.84s -> 19.16s]  Congestion control, on the other hand, is about preventing the source hosts from
[19.16s -> 22.32s]  overwhelming the links and routers in between.
[22.32s -> 26.52s]  When a source host put too many packets into the network, or when lots of sources
[26.52s -> 31.28s]  put packets into the network, they can fill up the router queues until they overflow.
[31.28s -> 36.72s]  In TCP, a congestion control algorithm running on the sending host tells it how many packets
[36.72s -> 41.36s]  it can have outstanding in the network, so as not to overfill the router queues.
[41.36s -> 46.90s]  TCP will always lead to some packets being dropped, because this is the feedback signal
[46.90s -> 50.06s]  it uses to know when the router queues are full.
[50.06s -> 55.72s]  But when it's working well, TCP keeps the packet drop rate low, links nice and full,
[55.72s -> 59.08s]  and allows the flow to have a high throughput.
[59.08s -> 64.16s]  First, Nick explained the principles of network congestion.
[64.16s -> 68.28s]  We learned what happens when a router starts receiving packets faster than it can
[68.28s -> 69.28s]  send them.
[69.28s -> 73.68s]  If the congestion is short-lived, then a router can absorb the section traffic into
[73.68s -> 75.92s]  a queue and drain the queue.
[75.92s -> 80.28s]  If the congestion is long-lived, long-lived enough that the queue overflows, then the
[80.28s -> 83.32s]  router has to drop some packets.
[83.32s -> 86.00s]  Nick introduced a very valuable way to think about this.
[86.00s -> 91.60s]  Rather than come up with a scheme for dropping packets, think about what you want the overall
[91.60s -> 93.64s]  network behavior to be.
[93.64s -> 98.12s]  We want the network to be fair, and explain what that means by introducing the concept
[98.12s -> 100.76s]  of max-min fairness.
[100.76s -> 104.50s]  Max-min fairness says that the network is fair if you can't increase the rate
[104.50s -> 109.24s]  of a flow without decreasing the rate of a flow with a lower rate.
[110.16s -> 115.48s]  There are a lot of ways to achieve this goal, and networks today have many different mechanisms.
[115.48s -> 120.76s]  But we focused on one in particular, how TCP can control the number of outstanding
[120.76s -> 122.12s]  packets in the network.
[122.12s -> 127.28s]  We learned the basic algorithm TCP uses, called additive increase, multiplicative decrease,
[127.28s -> 129.84s]  or AIMD.
[129.84s -> 134.52s]  When running smoothly, TCP increases the number of bytes it can have outstanding by one
[134.52s -> 137.58s]  segment size per round trip time.
[137.58s -> 143.72s]  When TCP detects a packet is dropped, it halves the number of bytes it can have outstanding.
[143.72s -> 147.54s]  You learned what this behavior looks like using a TCP sawtooth diagram.
[147.54s -> 152.90s]  While each individual flow has a sawtooth, over a link that it shares many flows, these
[152.90s -> 157.06s]  all average out to a consistently high use of the link.
[157.06s -> 162.38s]  Using the sawtooth, we derived TCP's throughput using symbol AIMD.
[162.38s -> 167.22s]  If you assume that the network drops packets at a uniform rate p, then the throughput
[167.22s -> 175.74s]  of a TCP flow is the square root of 3 halves times the inverse of the RTT times the square
[175.74s -> 177.74s]  root of p.
[177.74s -> 181.60s]  If you increase the round trip time, throughput goes down.
[181.60s -> 185.58s]  This equation makes a lot of simplifying assumptions, but it turns out to be generally
[185.58s -> 189.74s]  pretty accurate in many cases, and so a very valuable tool when thinking about how a
[189.74s -> 193.06s]  network might behave.
[193.06s -> 196.70s]  You've learned how TCP realizes these principles in practice.
[196.70s -> 201.42s]  Phil told you about the Internet collapsing in the late 1980s due to congestion, and the
[201.42s -> 205.34s]  fixes made to TCP, which are still in use today.
[205.34s -> 212.14s]  You learned about three versions of TCP, TCP Tahoe, TCP Reno, and TCP New Reno.
[212.14s -> 218.30s]  The first important idea we covered is that a TCP endpoint maintains a congestion window.
[218.30s -> 223.38s]  A TCP flow can have n unacknowledged bytes outstanding in the network, where n is the
[223.38s -> 227.78s]  minimum of its flow control window and its congestion control window.
[227.78s -> 232.10s]  You don't put more packets into the network than the other end can handle, or more than
[232.10s -> 235.58s]  the links and routers can handle in between.
[235.58s -> 239.82s]  You learned how TCP controls the size of this congestion control window using two
[239.82s -> 243.66s]  states, slow start and congestion avoidance.
[243.66s -> 248.98s]  Slow start lets TCP quickly find something close to the right congestion window size,
[248.98s -> 253.06s]  while congestion avoidance uses AIMD.
[253.06s -> 257.86s]  TCP starts in slow start, and transitions to congestion avoidance when it first detects
[257.86s -> 259.86s]  a loss.
[259.86s -> 263.14s]  You learned how TCP estimates the round-trip time of its connection.
[263.14s -> 266.38s]  It needs this estimate to figure out when an acknowledgement times out.
[266.38s -> 269.66s]  By keeping track of both the average, as well as the variance of how long it takes
[269.66s -> 274.58s]  to receive an ACK for a segment, TCP can avoid unnecessary retransmissions, as well
[274.58s -> 277.18s]  as not wait too long.
[277.18s -> 280.58s]  You learned how TCP controls when it puts packets into the network using a technique
[280.58s -> 282.76s]  called self-clocking.
[282.76s -> 286.56s]  You first saw self-clocking when I showed you an animation of TCP's behavior.
[286.56s -> 288.90s]  Phil then walked you through some examples.
[288.90s -> 292.64s]  With self-clocking, TCP only puts a new packet into the network when it receives
[292.64s -> 294.76s]  an acknowledgement or when there's a timeout.
[294.76s -> 299.68s]  This is really helpful in preventing congestion, as it means TCP only puts packets into
[299.68s -> 302.56s]  the network when packets have left the network.
[302.56s -> 310.68s]  Finally, we covered three optimizations added in TCP Reno and TCP New Reno.
[310.68s -> 315.60s]  Fast Retransmit lets TCP keep on making progress when only one packet has been dropped.
[315.60s -> 320.56s]  Rather than wait for a timeout, TCP retransmits a segment when it detects three duplicate
[320.56s -> 322.72s]  acknowledgements for the previous segment.
[322.72s -> 327.12s]  This is a sign that TCP is continuing to receive segments, but hasn't received that
[327.12s -> 329.46s]  particular one.
[329.46s -> 334.68s]  Using fast recovery, TCP Reno doesn't drop back into slow start or on three duplicate
[334.68s -> 339.36s]  ACKs, it just cuts the congestion window in half and stays in congestion's avoidance.
[339.56s -> 345.12s]  Finally, TCP New Reno adds an additional optimization, window inflation, such that
[345.12s -> 350.04s]  three duplicate ACKs don't cause TCP to lose an RTT worth of transmissions, as
[350.04s -> 352.84s]  it waits for the missing segment to be ACKed.
[355.40s -> 360.00s]  Now what's really fascinating about congestion was that it's something that was discovered
[360.00s -> 361.92s]  as the internet evolved.
[361.92s -> 365.72s]  Nobody had really thought something like this might happen, or how to control it.
[365.76s -> 370.72s]  It was an emergent behavior once the network became large and heavily used enough.
[370.72s -> 375.60s]  Nowadays it's a basic concept in networking, seen as critical for building robust systems
[375.60s -> 378.68s]  that have high performance.
[378.68s -> 383.70s]  Modern versions of TCP are a bit more advanced than what we've talked about in class.
[383.70s -> 387.84s]  But mostly, they've evolved to handle much, much faster networks.
[387.84s -> 393.88s]  The TCP versions shipped in operating systems have TCP Reno or TCP New Reno in their algorithms,
[393.88s -> 398.16s]  with new additional features and modes of operations to handle very fast networks.
[398.16s -> 402.20s]  Take a look at the Linux source code, and you'll see these algorithms in there.
[404.60s -> 408.44s]  But what's also neat is that these nitty-gritty algorithms have a sound conceptual basis
[408.44s -> 409.96s]  and theory behind them.
[409.96s -> 414.80s]  On one hand, we can talk about RTT variance estimation, fast recovery, and self-clocking.
[414.80s -> 419.80s]  On the other, we're also talking about AAMD flows that can converge to maximum unfairness.
