# Detected language: en (p=1.00)

[0.00s -> 6.00s]  Hey, David, and thanks for making this call.
[6.00s -> 9.00s]  Hey, you bet, Phil. Good to see you again.
[9.00s -> 16.00s]  Yeah, good to see you. So first of all, funny story, we were trying to do this online video call recording.
[16.00s -> 21.00s]  I tried lots of stuff, and then of course, David, given who he is, just snapped some fingers,
[21.00s -> 26.00s]  and then some magical people made it all happen using some pretty awesome technology.
[26.00s -> 29.00s]  So David, what do you do, and how did you get there?
[29.00s -> 36.00s]  So I'm the chief architect at Cisco, and the story how I got here is actually quite,
[36.00s -> 41.00s]  it's probably longer than it is interesting, to be honest, but nonetheless,
[41.00s -> 47.00s]  I started off in grad school back at the University of Minnesota in the mid-90s,
[47.00s -> 54.00s]  and I was working on a project directing a research center trying to prove
[54.00s -> 60.00s]  that a cluster of workstations around the University of Minnesota connected over a high-speed network
[60.00s -> 64.00s]  could outcompete and outcompute supercomputers.
[64.00s -> 68.00s]  And so what was interesting about this is that we ended up building and deploying
[68.00s -> 74.00s]  the world's first production ATM network amongst a bunch of Sun and SGI workstations,
[74.00s -> 78.00s]  and I know most people on this call probably have only heard about those companies
[78.00s -> 81.00s]  perhaps in the annals of history or in some dusty textbook,
[81.00s -> 86.00s]  but nonetheless, we proved, in fact, given the right experiment,
[86.00s -> 89.00s]  that we could outcompete a supercomputer,
[89.00s -> 94.00s]  but the problem was to be able to prove this, we needed to connect supercomputers together.
[94.00s -> 99.00s]  So to do that, we built the first production wide area network, ATM network,
[99.00s -> 104.00s]  to connect Pittsburgh supercomputer, Minnesota, San Diego, all the big ones together,
[104.00s -> 107.00s]  but then there was a small problem that once we built this out,
[108.00s -> 110.00s]  and this high-speed network was called VBNS, again,
[110.00s -> 113.00s]  one of the very first big WAN networks.
[113.00s -> 118.00s]  But the problem was is that supercomputers spoke to each other over this protocol called HIPI,
[118.00s -> 121.00s]  High Performance Parallel Interface,
[121.00s -> 125.00s]  and there weren't a whole lot of HIPI interfaces available for supercomputers,
[125.00s -> 128.00s]  so we had to invent a HIPI switch,
[128.00s -> 132.00s]  and then we, with a group of folks at the startup, invented HIPI over ATM.
[132.00s -> 135.00s]  Now, what was really interesting about this bill, actually,
[135.00s -> 141.00s]  from an engineering point of view, is that a HIPI packet in those days was two gigs,
[141.00s -> 147.00s]  and an ATM cell was 53 bytes, and across the wide area network,
[147.00s -> 152.00s]  when you dropped 53 bytes, here comes two gigs again,
[152.00s -> 155.00s]  and so we learned a whole lot about how to build
[155.00s -> 158.00s]  extremely good wide area network packet generators.
[158.00s -> 163.00s]  But as I moved forward, I found out in my experiment, in my work,
[163.00s -> 168.00s]  that we also needed to upgrade the Midwest University Network,
[168.00s -> 172.00s]  which was called CICNet, and how the University of Minnesota was connected to the Internet,
[172.00s -> 174.00s]  which was NSFNet in those days,
[174.00s -> 179.00s]  and so we started working and upgraded Minnesota's connection to NSFNet.
[179.00s -> 182.00s]  Again, this is all pre-commercial Internet, which, again,
[182.00s -> 184.00s]  are down in the annals of history,
[184.00s -> 187.00s]  so we had to do an upgrade of that wide area network.
[187.00s -> 191.00s]  And towards this end, we started building, again, more WAN interfaces
[191.00s -> 196.00s]  and more high-speed LAN pieces, and then I found, scratching my head,
[196.00s -> 202.00s]  wow, the PhD I was getting in physical organic chemistry, that's hard,
[202.00s -> 207.00s]  and this networking stuff is kind of easier, and it's really kind of cool,
[207.00s -> 210.00s]  and while doing all this high-speed networking,
[210.00s -> 212.00s]  I actually met the woman who became my wife, so I'm like,
[212.00s -> 214.00s]  wow, you can pick up girls, too.
[214.00s -> 218.00s]  This is all working out pretty well.
[218.00s -> 223.00s]  So then I went to a startup that was building these hippie switches,
[223.00s -> 225.00s]  and we thought we had a great business plan.
[225.00s -> 230.00s]  We are going to network all of the supercomputers on the planet.
[230.00s -> 232.00s]  There was one small problem with this business plan.
[232.00s -> 238.00s]  One was that you only need one switch to connect like all of NASA Goddard,
[238.00s -> 240.00s]  all of these massive supercomputers.
[240.00s -> 241.00s]  You only need one switch.
[241.00s -> 244.00s]  Second business problem, if it wasn't volume,
[244.00s -> 247.00s]  it was that a supercomputer goes for $35 million
[247.00s -> 250.00s]  or some ungodly number of tens of millions of dollars,
[250.00s -> 255.00s]  and a switch goes for about $350,000, and we sold 12 of them,
[255.00s -> 258.00s]  and so we're like, oh, man, that didn't quite work out,
[258.00s -> 259.00s]  but wait a minute.
[259.00s -> 263.00s]  There's this whole internet thing going on over here that looks kind of popular.
[263.00s -> 266.00s]  The internet had just gone commercial out of NSFNet,
[266.00s -> 272.00s]  and we had built this distributed processing or distributed forwarding switch thing
[272.00s -> 276.00s]  that, again, had the first FPGAs and then the first ASICs
[276.00s -> 278.00s]  to build out these distributed line cards.
[278.00s -> 283.00s]  And wait a minute, if we had a Unix machine controlling this,
[283.00s -> 285.00s]  where the Unix machine was actually running BSD,
[285.00s -> 288.00s]  an open source operating system that was really popular at the time,
[288.00s -> 292.00s]  and we were using open source tools and routing tools like Gate-D,
[292.00s -> 295.00s]  which most folks don't use now, but nonetheless was super popular then
[295.00s -> 297.00s]  and actually ran NSFNet.
[297.00s -> 301.00s]  And so we built a Unix machine on top of this cage of ASICs and FPGAs
[301.00s -> 306.00s]  and built the world's first distributed forwarding router at that time.
[306.00s -> 309.00s]  Then what ended up kind of happening was that, wait a minute,
[309.00s -> 313.00s]  this thing actually works really, really kind of well and is really fast
[313.00s -> 318.00s]  and has a much different scaling property than other centralized switches
[318.00s -> 321.00s]  at the time or specialized routers at the time.
[321.00s -> 325.00s]  And that startup ended up getting sold to a company called Ascend Communications,
[325.00s -> 331.00s]  which back in the day was the world's biggest ISDN modem builder,
[331.00s -> 333.00s]  which everyone had in their homes at the time,
[333.00s -> 335.00s]  upgrading from those old dial-up motives and stuff.
[335.00s -> 338.00s]  They went to ISDN and then they went on to cable
[338.00s -> 340.00s]  and other technologies, of course, as well.
[340.00s -> 345.00s]  But nonetheless, I left Ascend and went to a small startup
[345.00s -> 349.00s]  that Cisco acquired in 1999, and I've been designing systems
[349.00s -> 351.00s]  and ASICs and operating systems for quite a while.
[351.00s -> 354.00s]  But one thing I just wanted to mention about that bit of history
[354.00s -> 356.00s]  was that when I was doing my PhD work
[356.00s -> 362.00s]  and I was working on this cluster of high-performance servers
[362.00s -> 364.00s]  connected to high-speed networking,
[364.00s -> 367.00s]  my real work was in parallel programming
[367.00s -> 369.00s]  and parallel processing and operating systems
[369.00s -> 375.00s]  and built out parallel virtual machine, which was using IPCs and RPCs
[375.00s -> 377.00s]  across a high-speed network
[377.00s -> 380.00s]  and with different QoS levels for the different types of messages.
[380.00s -> 382.00s]  And so I was a major league OS head.
[382.00s -> 384.00s]  Ah, good for you.
[384.00s -> 386.00s]  Yeah, I know. Well, back in the day, it was cool.
[386.00s -> 390.00s]  And no offense to my other friends over there in Stanford.
[390.00s -> 394.00s]  But what ended up coming out of this was a deep understanding
[394.00s -> 398.00s]  for multithreading modular operating systems.
[398.00s -> 402.00s]  And that, in turn, with the systems experience I had
[402.00s -> 405.00s]  building chips and systems like I just described,
[405.00s -> 410.00s]  when I showed up at Cisco in 1999, or December 20, 1999,
[410.00s -> 413.00s]  in fact, at 8 a.m. over in the big exec-u office,
[413.00s -> 415.00s]  they handed me a pen and they said,
[415.00s -> 418.00s]  okay, tell us what the next router is going to be.
[418.00s -> 420.00s]  And from there, it's just been a great opportunity
[420.00s -> 424.00s]  to explore the different design patterns that I've learned about
[424.00s -> 425.00s]  and that I truly believe in
[425.00s -> 429.00s]  and turn that into a lot of equipment that is now running on the Internet.
[429.00s -> 434.00s]  Cool, yeah. Well, it's funny you said about the chemistry is hard
[434.00s -> 436.00s]  and stuff, but writing code is fun.
[436.00s -> 438.00s]  So David Moliere, who's faculty here,
[438.00s -> 440.00s]  once said he started as a math major.
[440.00s -> 442.00s]  And he said, you know, math is kind of hard, right?
[442.00s -> 443.00s]  Programs are fun, right?
[443.00s -> 446.00s]  So that's why he ended up going into computer science.
[446.00s -> 453.00s]  Cool. And so you did a lot of work on a bunch of Cisco's major routers.
[453.00s -> 455.00s]  Do you want to just mention a little bit about that?
[455.00s -> 459.00s]  Sure. So I was the designer and chief architect of iOSXR,
[459.00s -> 464.00s]  which is the current multi-chassis parallel programming modular operating system
[464.00s -> 466.00s]  running on the service provider line,
[466.00s -> 469.00s]  but also was the co-architect of the CRS-1,
[469.00s -> 473.00s]  which then turned into the CRS-3, which then turned into the CRS-10
[473.00s -> 478.00s]  as we advanced ASIC designs, have designed and helped do the functional design
[478.00s -> 482.00s]  of probably on the order of about 60 ASICs now,
[482.00s -> 486.00s]  also worked on the ASR-9K, the ASR-1000,
[486.00s -> 488.00s]  most of the optical kit that we sell,
[488.00s -> 493.00s]  as I was very much into optics and physics in the day
[493.00s -> 499.00s]  and incorporating in particular optical processing in routers themselves
[499.00s -> 501.00s]  and taking the transponder shelves,
[501.00s -> 503.00s]  which are such a huge cost of building the internet
[503.00s -> 508.00s]  and building those effectively directly into routers to reduce costs.
[508.00s -> 512.00s]  From there, and all the while even back at Ascend
[512.00s -> 514.00s]  and doing things at the university,
[514.00s -> 518.00s]  you have to realize that the NSFNET community
[518.00s -> 522.00s]  and regional meetings turned into the North American Network Operators Group.
[522.00s -> 526.00s]  So I had to be deeply involved in there,
[526.00s -> 528.00s]  and then as we were building out NSFNET
[528.00s -> 530.00s]  and then building out the internet with all the providers
[530.00s -> 532.00s]  when it went commercial,
[532.00s -> 537.00s]  we needed a lot of protocol extensions to be able to peer with each other
[537.00s -> 540.00s]  and build out services and came up with effectively
[540.00s -> 543.00s]  the current network architecture of the internet,
[543.00s -> 546.00s]  which includes exchange points and how to do regional peering
[546.00s -> 548.00s]  and how to do settlement-free peering.
[548.00s -> 551.00s]  Frankly, we invented all the protocols
[551.00s -> 557.00s]  of now what we know of being BGP, OSPF, ISI, it's multicast, MPLS, et cetera,
[557.00s -> 561.00s]  to fundamentally solve some of the problems that we were facing on the internet.
[561.00s -> 564.00s]  When you couldn't get circuits delivered fast enough
[564.00s -> 567.00s]  and you had constrained bandwidth on your link,
[567.00s -> 570.00s]  you needed to be able to reserve bandwidth and have traffic engineering.
[570.00s -> 572.00s]  Here comes MPLS.
[572.00s -> 575.00s]  When you want to connect businesses in their own address space,
[575.00s -> 578.00s]  it's really, really hard to have overlapping address space
[578.00s -> 580.00s]  and do the correct IP routing
[580.00s -> 583.00s]  and voila, here come Layer 3 VPNs.
[583.00s -> 586.00s]  It was a lot of invention by necessity at the time
[586.00s -> 588.00s]  and just having a ton of fun,
[588.00s -> 591.00s]  not only, of course, writing the code, defining the specs,
[591.00s -> 594.00s]  but watching it all get deployed really, really fast.
[594.00s -> 597.00s]  It was a ton of fun, and it still is a ton of fun.
[597.00s -> 600.00s]  I'm still cranking away on new stuff.
[600.00s -> 602.00s]  I work with a lot of folks there at Stanford
[602.00s -> 606.00s]  and helped get open flow out of Gates 104 and build up the ONF
[606.00s -> 610.00s]  and now really focusing a lot out of the embedded OSes
[610.00s -> 613.00s]  and focusing at controller layers, orchestration,
[613.00s -> 615.00s]  network function virtualization,
[615.00s -> 619.00s]  and trying to tie together server storage switching and routing
[619.00s -> 622.00s]  and get more services on the internet.
[622.00s -> 626.00s]  I mean, basically, Phil, my red and white blood cells
[626.00s -> 628.00s]  are completely packetized in IP packets.
[631.00s -> 633.00s]  I never thought about our blood being packetized,
[633.00s -> 636.00s]  but actually, Martin Castado came and gave a guest lecture
[636.00s -> 638.00s]  in the class to talk about SDN,
[638.00s -> 641.00s]  present the whole sort of SDN vision of having controllers
[641.00s -> 644.00s]  and this global graph which you can operate on
[644.00s -> 646.00s]  to then push out to the switches
[646.00s -> 650.00s]  and sort of that separation of control logic and forwarding logic.
[650.00s -> 653.00s]  And so I think a lot of what you said will make sense, actually,
[653.00s -> 655.00s]  to a bunch of the students in this class.
[655.00s -> 657.00s]  They're pretty excited about the SDN stuff.
[657.00s -> 660.00s]  Well, I have a slightly different view maybe than Martin
[660.00s -> 662.00s]  and certainly when he was working at Stanford
[662.00s -> 665.00s]  is that having a centralized view of the topology
[665.00s -> 669.00s]  is a great augmentation of the current dynamically routed system.
[669.00s -> 672.00s]  And if you don't mind me to opine just for a little bit second
[672.00s -> 674.00s]  just on this philosophic point,
[674.00s -> 680.00s]  a distributedly routed autonomous acting set of routers forming the internet,
[680.00s -> 684.00s]  in my opinion, does provide the fastest protection
[684.00s -> 687.00s]  and restoration for topology failures.
[687.00s -> 688.00s]  Now, when we started out,
[688.00s -> 690.00s]  or the forefathers before me of the internet started up,
[690.00s -> 694.00s]  they were worried about, frankly, thermonuclear war,
[694.00s -> 697.00s]  and they wanted to have a consistent communication platform
[697.00s -> 700.00s]  that no matter what happened to any city or any node,
[700.00s -> 701.00s]  the network could stay up.
[701.00s -> 704.00s]  Well, this translates into modern time.
[704.00s -> 706.00s]  Hopefully we don't have to worry about that problem anymore,
[706.00s -> 708.00s]  but what we do have to worry about are backhoes
[708.00s -> 712.00s]  and other real problems of things in our transmission network,
[712.00s -> 714.00s]  which as far as the network is concerned,
[714.00s -> 716.00s]  they can't tell the difference what caused a fiber cut,
[716.00s -> 718.00s]  whether it's a backhoe or, you know,
[718.00s -> 721.00s]  whatever it is, somebody with a forklift in a data center.
[721.00s -> 725.00s]  So I still firmly believe in the tenets and power
[725.00s -> 727.00s]  of a distributedly routed network,
[727.00s -> 729.00s]  and we can get into this more in just a second,
[729.00s -> 732.00s]  but I really find polar conversations
[732.00s -> 735.00s]  that everything is distributedly routed
[735.00s -> 738.00s]  or everything is centrally managed
[738.00s -> 741.00s]  as really something best left for the bar.
[741.00s -> 744.00s]  It's not an interesting conversation for me.
[744.00s -> 746.00s]  And it's not an interesting conversation
[746.00s -> 751.00s]  because when you build and deploy transport networks
[751.00s -> 753.00s]  or ATM networks or frame relay networks
[753.00s -> 756.00s]  or even X25 networks, if I really want to date myself,
[756.00s -> 758.00s]  you do this with a centralized planner,
[758.00s -> 761.00s]  and what you get to see is you get to see
[761.00s -> 764.00s]  across summarization boundaries, and to scale the Internet,
[764.00s -> 766.00s]  we've got summarization boundaries of IP addressing
[766.00s -> 769.00s]  all over the place between autonomous systems,
[769.00s -> 771.00s]  whether it's Comcast, AT&T, or wherever you are in the world
[771.00s -> 773.00s]  between those big providers,
[773.00s -> 775.00s]  but also within those networks,
[775.00s -> 777.00s]  there are multiple autonomous systems,
[777.00s -> 779.00s]  and within those autonomous systems,
[779.00s -> 781.00s]  there's multiple areas like OSPF areas,
[781.00s -> 783.00s]  and frankly, those are the definitions
[783.00s -> 785.00s]  of IP address summarization.
[785.00s -> 787.00s]  Now, all that's cool.
[787.00s -> 788.00s]  Scaling is great.
[788.00s -> 790.00s]  We all love a bigger, better Internet,
[790.00s -> 793.00s]  but if I wanted to build a tunnel
[793.00s -> 796.00s]  between me and you using MPLS or using anything,
[796.00s -> 798.00s]  and I can't see your IP address
[798.00s -> 801.00s]  because it's been summarized into a larger block,
[801.00s -> 805.00s]  man, we spent years inventing 42 different ways
[805.00s -> 808.00s]  to leak prefixes and do all this fancy stuff
[808.00s -> 811.00s]  and protocols when, what's the answer,
[811.00s -> 814.00s]  have a logically centralized view of the entire topology,
[814.00s -> 817.00s]  and if I want to build a path between Dave and Phil,
[817.00s -> 820.00s]  I see your address and your topology
[820.00s -> 823.00s]  behind your area boundary and your summarization boundary,
[823.00s -> 825.00s]  and I can see mine over on this side,
[825.00s -> 827.00s]  and voila, I can build an end-to-end path
[827.00s -> 829.00s]  with quality of service, et cetera, et cetera,
[829.00s -> 833.00s]  and that augmentation of a centralized SDN controller
[833.00s -> 835.00s]  or a centralized controller
[835.00s -> 837.00s]  in addition to a dynamically-routed network
[837.00s -> 839.00s]  is something that I'm really focusing on,
[839.00s -> 842.00s]  and frankly, it might be a conversation
[842.00s -> 844.00s]  you might expect to have with a guy like me
[844.00s -> 846.00s]  given that I kind of came out of that world,
[846.00s -> 848.00s]  but I also came out of the world
[848.00s -> 850.00s]  where trying to do everything centralized
[850.00s -> 853.00s]  for ATM or frame, and I was building,
[853.00s -> 856.00s]  and we were building permanent virtual circuits
[856.00s -> 858.00s]  all across the planet.
[858.00s -> 862.00s]  I watched the scaling properties of that just explode,
[862.00s -> 864.00s]  and when you're trying to deal with
[864.00s -> 866.00s]  not just tens of thousands,
[866.00s -> 869.00s]  but millions of potential circuits or slices
[869.00s -> 872.00s]  in current parlance because circuits are bad
[872.00s -> 874.00s]  and now slices are good,
[874.00s -> 878.00s]  the scaling properties to scale to internet-sized stuff
[878.00s -> 882.00s]  is really something that has to be well understood,
[882.00s -> 884.00s]  and that's what dynamic routing
[884.00s -> 886.00s]  and topology exchange is really, really good at.
[886.00s -> 888.00s]  It's just really, really bad
[888.00s -> 890.00s]  at seeing across those summarization boundaries,
[890.00s -> 892.00s]  so the two together, the two together
[892.00s -> 894.00s]  are just rock and roll.
[894.00s -> 896.00s]  There's no disco involved in that one.
[896.00s -> 898.00s]  They're just cranking away.
[898.00s -> 899.00s]  Cool.
[899.00s -> 901.00s]  So we've talked a lot about the past
[901.00s -> 903.00s]  and maybe some of the things at the end
[903.00s -> 904.00s]  that's happening now,
[904.00s -> 906.00s]  but what are the things that you're most excited about
[906.00s -> 907.00s]  that's going on in routing
[907.00s -> 909.00s]  and sort of the wide area in the internet today?
[909.00s -> 910.00s]  I know for a little while
[910.00s -> 912.00s]  you were working on securing BGP.
[912.00s -> 915.00s]  Is that work still going on, or what are you doing now?
[915.00s -> 917.00s]  No, I am still working on securing BGP,
[917.00s -> 919.00s]  but that's turned into a political quagmire
[919.00s -> 922.00s]  of who's gonna hold the certificate,
[922.00s -> 924.00s]  and is it the Department of Commerce
[924.00s -> 927.00s]  which actually has this thing called the IANA function
[927.00s -> 930.00s]  which holds all the roots of all of our naming
[930.00s -> 931.00s]  and numbering.
[931.00s -> 933.00s]  It's just a political nightmare
[933.00s -> 935.00s]  internationally and nationally
[935.00s -> 937.00s]  because nobody can figure out, frankly,
[937.00s -> 938.00s]  I'll summarize it this way, Phil.
[938.00s -> 940.00s]  Nobody can figure out who's gonna make money off it,
[940.00s -> 942.00s]  and I'm not interested in solving that problem
[942.00s -> 946.00s]  because that problem is a outside commercial space.
[946.00s -> 948.00s]  Somebody needs to hold the root.
[948.00s -> 950.00s]  Somebody needs to handle all the addressing.
[950.00s -> 952.00s]  Somebody needs to handle all the certificates
[952.00s -> 953.00s]  associated with BGP routes,
[953.00s -> 956.00s]  and I continue to work on this with the FCC
[956.00s -> 958.00s]  and other governmental organizations,
[958.00s -> 959.00s]  whether it's the EU or others,
[959.00s -> 961.00s]  and the regional registries,
[961.00s -> 964.00s]  but it didn't make enough progress.
[964.00s -> 966.00s]  It didn't go fast enough for me
[966.00s -> 968.00s]  to get through these issues,
[968.00s -> 970.00s]  so instead, what I wanted to talk to you about today
[970.00s -> 973.00s]  of things that are extremely cool
[973.00s -> 975.00s]  going on in the internet, especially if you ask me,
[975.00s -> 979.00s]  have to do with a technology called segment routing,
[979.00s -> 981.00s]  and what's different about this
[981.00s -> 983.00s]  and my goal with this technology
[983.00s -> 985.00s]  and our goal with the group of folks
[985.00s -> 986.00s]  who are working on it,
[986.00s -> 988.00s]  including lots and lots of providers,
[988.00s -> 993.00s]  is to simplify MPLS networking,
[993.00s -> 996.00s]  traffic engineering, and VPN services
[996.00s -> 999.00s]  by actually removing protocols,
[999.00s -> 1001.00s]  and the reason why this is a bit of a shocker
[1001.00s -> 1005.00s]  is that, look, I've spent decades of my career
[1005.00s -> 1007.00s]  actually extending and advancing these protocols,
[1007.00s -> 1009.00s]  so you'd think that I might be a guy
[1009.00s -> 1011.00s]  who would just hold onto them religiously,
[1011.00s -> 1012.00s]  like, this is a hammer.
[1012.00s -> 1013.00s]  Everything is a nail.
[1013.00s -> 1014.00s]  Let me go find more nails,
[1014.00s -> 1016.00s]  or at least things that look close enough like nails
[1016.00s -> 1017.00s]  that I can go beat them,
[1017.00s -> 1019.00s]  but in fact, what's really interesting
[1019.00s -> 1022.00s]  is that when you take a look at traffic engineering
[1022.00s -> 1024.00s]  and wide area networking,
[1024.00s -> 1027.00s]  the operational overhead and the complexity of it
[1027.00s -> 1030.00s]  is in fact a major leak problem,
[1030.00s -> 1032.00s]  and part of the problem is that
[1032.00s -> 1035.00s]  we require the network to hold state,
[1035.00s -> 1038.00s]  so when I set up an RSVP traffic engineer tunnel,
[1038.00s -> 1041.00s]  I use RSVP, hence the name,
[1041.00s -> 1044.00s]  but I use RSVP, which signals hop by hop
[1044.00s -> 1045.00s]  through the entire network.
[1045.00s -> 1046.00s]  I'd like to reserve a gig.
[1046.00s -> 1047.00s]  I'd like to reserve a gig.
[1047.00s -> 1049.00s]  I'd like to reserve a gig.
[1049.00s -> 1050.00s]  When everybody says there's a gig
[1050.00s -> 1051.00s]  and make up your favorite number
[1051.00s -> 1053.00s]  if you want more bandwidth,
[1053.00s -> 1056.00s]  then you get go, start sending traffic,
[1056.00s -> 1060.00s]  and every device in this path holds that state,
[1060.00s -> 1063.00s]  and so then we were running into issues that,
[1063.00s -> 1065.00s]  well, how many midpoint labels can you hold,
[1065.00s -> 1067.00s]  and how much state can you hold,
[1067.00s -> 1068.00s]  and then we invented protocols
[1068.00s -> 1070.00s]  that didn't require that kind of state,
[1070.00s -> 1072.00s]  and they were called the label distribution protocol
[1072.00s -> 1074.00s]  where we'd say, hey, here are all the labels
[1074.00s -> 1077.00s]  I'm advertising, and another protocol we'd say,
[1077.00s -> 1079.00s]  like OSPF or ISRs, we'd say,
[1079.00s -> 1080.00s]  here are all the next tops
[1080.00s -> 1082.00s]  that you're supposed to bind them to,
[1082.00s -> 1087.00s]  and we built a stateless MPLS network out of that,
[1087.00s -> 1091.00s]  but the problem was labels and traffic engineer tunnels,
[1091.00s -> 1094.00s]  or sorry, those tunnels were only following
[1094.00s -> 1096.00s]  best path routing.
[1096.00s -> 1098.00s]  Well, what about all those other paths out there
[1098.00s -> 1100.00s]  that aren't passing my traffic?
[1100.00s -> 1101.00s]  What am I gonna do now?
[1101.00s -> 1103.00s]  I've got all this bandwidth deployed in the network.
[1103.00s -> 1104.00s]  I'd really like to use it.
[1104.00s -> 1106.00s]  Oh, and as a matter of fact,
[1107.00s -> 1112.00s]  what if I would like to also use equal cost multipath,
[1113.00s -> 1116.00s]  and when you take a look philosophically at,
[1116.00s -> 1117.00s]  if you wanna think of it that way,
[1117.00s -> 1120.00s]  philosophically at how MPLS networks are built,
[1120.00s -> 1123.00s]  the notion that there are link aggregation groups at L2,
[1123.00s -> 1126.00s]  like multiple ethernet interfaces in between routers,
[1126.00s -> 1128.00s]  or at equal cost multipath,
[1128.00s -> 1132.00s]  which is multiple layer three connectivity between routers,
[1132.00s -> 1135.00s]  best of luck trying to balance out your traffic
[1135.00s -> 1138.00s]  across those equal cost multipath links,
[1138.00s -> 1141.00s]  one, because they've been summarized in your IGP,
[1141.00s -> 1144.00s]  and you can actually see all those multiple adjacencies,
[1144.00s -> 1146.00s]  and two, how are you gonna organize
[1146.00s -> 1148.00s]  a hashing algorithm network-wide
[1148.00s -> 1150.00s]  across lots and lots of boxes of different eras
[1150.00s -> 1152.00s]  and different vendors and different ASICs
[1152.00s -> 1154.00s]  and all the rest of it,
[1154.00s -> 1157.00s]  and so MPLS, I would say right now,
[1157.00s -> 1160.00s]  fundamentally doesn't solve one of the major tenets
[1160.00s -> 1161.00s]  of the way the internet's built,
[1161.00s -> 1163.00s]  which is equal cost multipath,
[1163.00s -> 1166.00s]  adding lots and lots of bandwidth between two routers.
[1166.00s -> 1169.00s]  For example, if you need 400 gigs of bandwidth
[1169.00s -> 1171.00s]  between two routers, let's say,
[1171.00s -> 1173.00s]  whatever, between Boston and New York,
[1173.00s -> 1176.00s]  you can use 40, 10 gig interfaces
[1176.00s -> 1179.00s]  or 400 gig interfaces or whatever the case is,
[1179.00s -> 1181.00s]  now how do I balance traffic across those?
[1181.00s -> 1183.00s]  That's the problem that I'm trying to solve,
[1183.00s -> 1186.00s]  so what segment routing does is,
[1186.00s -> 1189.00s]  let's not use RSVP and hold all that state,
[1189.00s -> 1192.00s]  let's not use LDP and try and match up
[1192.00s -> 1194.00s]  and only use best path routing.
[1194.00s -> 1199.00s]  Instead, when I pass the IP address of a router
[1199.00s -> 1202.00s]  and I pass the IP addresses of all the interfaces
[1202.00s -> 1205.00s]  off the router in OSPF and flood that information
[1205.00s -> 1207.00s]  between all the other routers,
[1207.00s -> 1209.00s]  so that way we know, quote,
[1209.00s -> 1211.00s]  the topology of the network and all the adjacencies,
[1211.00s -> 1214.00s]  I'm already passing, here's the interface,
[1214.00s -> 1217.00s]  here's the adjacency, and here's the IP address.
[1217.00s -> 1219.00s]  Well, what if I also just snuck in,
[1219.00s -> 1221.00s]  you know, what's four bytes between friends,
[1221.00s -> 1223.00s]  what if I snuck in a label in there too?
[1223.00s -> 1228.00s]  I fundamentally haven't added really a whole lot
[1228.00s -> 1230.00s]  of traffic to the control plane, obviously.
[1230.00s -> 1234.00s]  You know, it's literally individual megs of RAM
[1234.00s -> 1238.00s]  on a router versus the hundreds of megs of RAM
[1238.00s -> 1240.00s]  or gigs of RAM required to hold the state
[1240.00s -> 1243.00s]  for RSVP tunnels, so let me back up, Phil.
[1243.00s -> 1246.00s]  What I've done is I've now flooded IP addresses
[1246.00s -> 1249.00s]  for all the routers, IP addresses for all the interfaces
[1249.00s -> 1253.00s]  and adjacencies, and I've flooded a label for everything.
[1253.00s -> 1257.00s]  So now I've flooded a complete link state topology,
[1257.00s -> 1260.00s]  including IP connectivity and label connectivity,
[1260.00s -> 1262.00s]  and what I do differently,
[1262.00s -> 1264.00s]  and here's how SDN becomes involved,
[1264.00s -> 1267.00s]  what I do differently is instead of sending
[1267.00s -> 1269.00s]  what would be called an explicit route object
[1269.00s -> 1272.00s]  to the head end to trigger RSVP TE,
[1272.00s -> 1274.00s]  which says I wanna follow this route,
[1274.00s -> 1277.00s]  this link to that router and that link to that router
[1277.00s -> 1278.00s]  and that link to that router,
[1278.00s -> 1281.00s]  that's the explicit route object that defines that,
[1281.00s -> 1285.00s]  I instead program the head end with a label stack,
[1285.00s -> 1288.00s]  and that label stack I've already learned from the network,
[1288.00s -> 1291.00s]  I've allocated to every device, and voila,
[1291.00s -> 1296.00s]  I can use just by programming the head end label stack
[1296.00s -> 1298.00s]  a complete path through the network
[1298.00s -> 1302.00s]  utilizing Equicast multipath using protection restoration,
[1302.00s -> 1305.00s]  and I think this is, although not being discussed
[1305.00s -> 1309.00s]  at the usual SDN places and not being discussed
[1309.00s -> 1311.00s]  maybe lots and lots within academia yet,
[1311.00s -> 1314.00s]  this is without a doubt perhaps the biggest revolution
[1314.00s -> 1316.00s]  to happen in routing and routing protocol
[1316.00s -> 1318.00s]  since MPLS was invented,
[1318.00s -> 1320.00s]  simply because it solves some fundamental
[1320.00s -> 1324.00s]  architectural problems of Equicast multipath,
[1324.00s -> 1328.00s]  but it preserves fast reroute, traffic engineering,
[1328.00s -> 1332.00s]  and it does it all SDN-esque,
[1332.00s -> 1334.00s]  but instead of passing down, say, a classifier,
[1334.00s -> 1336.00s]  you're passing down a label stack.
[1336.00s -> 1338.00s]  Easy peasy.
[1338.00s -> 1342.00s]  And so what I've been working on is, of course, pardon me,
[1342.00s -> 1344.00s]  how to link this up into RSVP networks,
[1344.00s -> 1347.00s]  how to link this up within LDP-based networks
[1347.00s -> 1349.00s]  or labeled BGP networks,
[1349.00s -> 1351.00s]  and what this allows me to do,
[1351.00s -> 1352.00s]  and the way I'd like folks to think about it
[1352.00s -> 1353.00s]  and you to think about it,
[1353.00s -> 1357.00s]  when I combine dynamically routing networks
[1357.00s -> 1359.00s]  with a centralized view of the topology
[1359.00s -> 1361.00s]  and a full known explicit path of labels
[1361.00s -> 1362.00s]  through the network program
[1362.00s -> 1364.00s]  and that head and label stack,
[1364.00s -> 1367.00s]  I, sorry, this technology fundamentally
[1367.00s -> 1370.00s]  would be called a soft permanent virtual circuit
[1370.00s -> 1371.00s]  in ATM or frame terms.
[1371.00s -> 1373.00s]  It's a soft PBC.
[1373.00s -> 1374.00s]  Program the head ends
[1374.00s -> 1377.00s]  and let signaling figure out the path in between,
[1377.00s -> 1379.00s]  and believe it or not.
[1379.00s -> 1381.00s]  So it's interesting that we're going to,
[1381.00s -> 1383.00s]  I mean, we have, I'm willing to pack it,
[1383.00s -> 1384.00s]  and I can pack it underneath,
[1384.00s -> 1386.00s]  but just for needs of,
[1386.00s -> 1388.00s]  I mean, core ISPs and traffic engineering
[1388.00s -> 1390.00s]  and all of this stuff in the middle,
[1390.00s -> 1391.00s]  we want to go to a soft circuit.
[1391.00s -> 1393.00s]  Well, let's face it.
[1393.00s -> 1394.00s]  What we should have remembered
[1394.00s -> 1397.00s]  that the most popular feature of those technologies
[1397.00s -> 1399.00s]  was, in fact, soft PBCs,
[1399.00s -> 1402.00s]  and we never invented it for IP and PLS.
[1402.00s -> 1404.00s]  It was a huge gaping hole, in my opinion.
[1404.00s -> 1406.00s]  Instead, we jumped all the way
[1406.00s -> 1408.00s]  to switch virtual circuits,
[1408.00s -> 1411.00s]  which was full topology awareness
[1411.00s -> 1413.00s]  of everybody at the head end.
[1413.00s -> 1415.00s]  Now, Phil, the other thing I wanted to mention,
[1415.00s -> 1417.00s]  and this is, again, a philosophic point,
[1417.00s -> 1420.00s]  is that it's a different point about SDN
[1420.00s -> 1423.00s]  and the requirement for logically centralized control
[1423.00s -> 1424.00s]  than others make,
[1424.00s -> 1425.00s]  but I think it's just because
[1425.00s -> 1427.00s]  it hasn't entered the conversation on the internet yet,
[1427.00s -> 1431.00s]  which is that today's IP MPLS networks
[1431.00s -> 1436.00s]  do load design via head end only views of the topology.
[1436.00s -> 1439.00s]  And so by doing head end load design.
[1439.00s -> 1440.00s]  Can you explain what that means?
[1440.00s -> 1443.00s]  I'm not sure every student will be able to do that.
[1444.00s -> 1445.00s]  Can you explain what that means?
[1445.00s -> 1448.00s]  I'm not sure every student will quite get what that means.
[1448.00s -> 1449.00s]  Yeah, so what that means is
[1449.00s -> 1452.00s]  when I want to build a path between me and you,
[1452.00s -> 1456.00s]  and I want to understand
[1456.00s -> 1459.00s]  how can I fit 10 gig circuit
[1459.00s -> 1462.00s]  or 10 gig tunnel between me and you,
[1462.00s -> 1465.00s]  I have no idea about the load on the rest of the network.
[1465.00s -> 1466.00s]  I have no idea of the load
[1466.00s -> 1469.00s]  that any other entry point into the network
[1469.00s -> 1470.00s]  would be putting onto the network.
[1470.00s -> 1472.00s]  All I know is what I see.
[1473.00s -> 1476.00s]  And all I see is a set of the topology.
[1476.00s -> 1478.00s]  I don't know the utilization.
[1478.00s -> 1480.00s]  I only know the reserved bandwidth.
[1480.00s -> 1483.00s]  What if I don't care about the reserved bandwidth?
[1483.00s -> 1484.00s]  What if I actually just want to know
[1484.00s -> 1487.00s]  where can I place 10 gigs of load on the network?
[1487.00s -> 1491.00s]  You can't do this from a head end based approach.
[1491.00s -> 1494.00s]  And all of MPLS and all of IP
[1494.00s -> 1495.00s]  is based upon the head end
[1495.00s -> 1497.00s]  having full information about the network
[1497.00s -> 1498.00s]  on how to place load.
[1498.00s -> 1499.00s]  So by that, the head end,
[1499.00s -> 1502.00s]  you mean the person who's initiating the tunnel,
[1502.00s -> 1504.00s]  basically saying I'm the head of this tunnel,
[1504.00s -> 1506.00s]  and therefore I'm the person.
[1506.00s -> 1508.00s]  Yeah, you're absolutely right, Paul.
[1508.00s -> 1509.00s]  So on the head end of this tunnel,
[1509.00s -> 1510.00s]  I'm the originator of this tunnel,
[1510.00s -> 1512.00s]  and I have to have full knowledge
[1512.00s -> 1514.00s]  of load on the network.
[1514.00s -> 1517.00s]  The second thing that's a problem
[1517.00s -> 1519.00s]  with our current IP MPLS networks
[1519.00s -> 1521.00s]  and why a combination of a centralized view
[1521.00s -> 1524.00s]  and the distributed routing is critical
[1524.00s -> 1528.00s]  is that there is zero admission control
[1529.00s -> 1532.00s]  today available onto the wide area network.
[1532.00s -> 1534.00s]  The only notion of admission control
[1534.00s -> 1538.00s]  is on load engineering using historical mechanisms
[1538.00s -> 1541.00s]  and trying to use past history
[1541.00s -> 1543.00s]  to predict future performance.
[1543.00s -> 1545.00s]  Now, it doesn't work very well in the stock market
[1545.00s -> 1547.00s]  and it doesn't work very well on networks
[1547.00s -> 1550.00s]  because we are subject to complete instantaneous,
[1550.00s -> 1554.00s]  unknowingly caused fluctuations in load on the network.
[1554.00s -> 1557.00s]  Yes, we can see diurnal patterns,
[1557.00s -> 1560.00s]  but we don't want to engineer only to diurnal patterns,
[1560.00s -> 1563.00s]  but why did we ever leave admission control
[1563.00s -> 1565.00s]  and load admission control out of IP MPLS?
[1565.00s -> 1567.00s]  You know, I can't tell you.
[1567.00s -> 1569.00s]  I've been working on this stuff for 20 years.
[1569.00s -> 1570.00s]  Why didn't we ever do soft PVCs?
[1570.00s -> 1571.00s]  I can't tell you.
[1571.00s -> 1576.00s]  We went all the way to switch virtual circuits
[1576.00s -> 1580.00s]  and left a couple of the most fundamental building blocks
[1580.00s -> 1582.00s]  of network engineering and network planning
[1582.00s -> 1584.00s]  not even on the table.
[1584.00s -> 1589.00s]  If you go back to the design of the Internet Protocol,
[1589.00s -> 1591.00s]  David Clark's paper,
[1591.00s -> 1594.00s]  he talks about here are the design considerations or goals.
[1594.00s -> 1598.00s]  And certainly, resource accounting is one of them,
[1598.00s -> 1600.00s]  but it's low down on the list.
[1600.00s -> 1603.00s]  And so it kind of didn't end up being taken care of very much.
[1603.00s -> 1604.00s]  But it's really nice.
[1604.00s -> 1607.00s]  A couple of weeks ago, he gave a guest lecture
[1607.00s -> 1610.00s]  and went back and revisited these principles
[1610.00s -> 1612.00s]  and these ideas and kind of threw out saying,
[1612.00s -> 1614.00s]  hey, we never thought about security.
[1614.00s -> 1616.00s]  Maybe that's something we should have thought originally.
[1616.00s -> 1619.00s]  So really, I think back when it was starting,
[1619.00s -> 1621.00s]  it didn't seem like a big deal,
[1621.00s -> 1623.00s]  but now some of those principles
[1623.00s -> 1627.00s]  which weren't given a high priority
[1627.00s -> 1629.00s]  are now becoming more and more important.
[1629.00s -> 1630.00s]  Oh, yeah.
[1630.00s -> 1632.00s]  And frankly, load-based engineering is one of them.
[1632.00s -> 1636.00s]  Now let's talk just a quick trip down history lane.
[1636.00s -> 1639.00s]  Back in the day, even before there was NSFnet,
[1639.00s -> 1640.00s]  there was ARPANET.
[1640.00s -> 1643.00s]  And a lot of my colleagues worked on ARPANET.
[1643.00s -> 1647.00s]  And what they tried to do was based upon delay
[1647.00s -> 1650.00s]  or queuing delay across the network,
[1650.00s -> 1653.00s]  they would fiddle with an IGP metric like OSPF.
[1653.00s -> 1655.00s]  And if there was high delay,
[1655.00s -> 1657.00s]  they would give a link worse preference.
[1657.00s -> 1658.00s]  And if there was low delay,
[1658.00s -> 1660.00s]  they would give it very high preference,
[1660.00s -> 1661.00s]  meaning attracting traffic.
[1661.00s -> 1663.00s]  Now translated.
[1663.00s -> 1664.00s]  Yeah, of course.
[1664.00s -> 1665.00s]  But you know what, man?
[1665.00s -> 1667.00s]  That was back in the 80s.
[1667.00s -> 1670.00s]  And to this day, from that lesson,
[1670.00s -> 1674.00s]  we have not been able to have an adult conversation
[1674.00s -> 1676.00s]  between consenting adults even
[1676.00s -> 1678.00s]  about guided load engineering.
[1678.00s -> 1682.00s]  And look, it's taken, I'll be honest,
[1682.00s -> 1683.00s]  the next thing that,
[1683.00s -> 1684.00s]  and I want to get to the next topic,
[1684.00s -> 1686.00s]  but the next thing we've been working on, of course,
[1686.00s -> 1690.00s]  is adding useful information into these protocols
[1690.00s -> 1692.00s]  that are flooding this around the network.
[1692.00s -> 1694.00s]  So here's where I'm building this up.
[1694.00s -> 1696.00s]  We just added, here's a shocker fill,
[1696.00s -> 1700.00s]  here's a shocker fill, the delay of a link
[1700.00s -> 1702.00s]  to be passed with the traffic engineering mechanism,
[1702.00s -> 1703.00s]  traffic engineering metrics.
[1703.00s -> 1705.00s]  Here is the utilization of a link.
[1705.00s -> 1706.00s]  Here is the jitter.
[1706.00s -> 1709.00s]  In fact, here's some packet loss information
[1709.00s -> 1713.00s]  that frankly, we're dropping packets across this link.
[1713.00s -> 1716.00s]  Now, without getting into the ARPANET war,
[1716.00s -> 1717.00s]  I said, look, I'm not even trying
[1717.00s -> 1719.00s]  to create an SPF out of this.
[1719.00s -> 1720.00s]  I'm not even trying to run a Dijkstra
[1720.00s -> 1721.00s]  and reroute your traffic.
[1721.00s -> 1725.00s]  Instead, if you could use this really modern
[1725.00s -> 1729.00s]  computer science idea of pushing this data
[1729.00s -> 1732.00s]  out of the network up to a centralized controller
[1732.00s -> 1734.00s]  instead of that goddamn protocol SNMP
[1734.00s -> 1737.00s]  where I need to go poll every device to get the load
[1737.00s -> 1740.00s]  and then somehow through some serious machinations
[1740.00s -> 1741.00s]  match that up with the topology
[1741.00s -> 1745.00s]  to understand what the load looks like on my network.
[1745.00s -> 1747.00s]  Hey man, I'm pushing traffic,
[1747.00s -> 1750.00s]  I'm pushing this data out in real time.
[1750.00s -> 1752.00s]  I'm pushing this data out
[1752.00s -> 1754.00s]  and I see the real time state of the topology
[1754.00s -> 1758.00s]  and the utilization and the delay and the packet loss.
[1758.00s -> 1760.00s]  I now can take that centralized controller
[1760.00s -> 1762.00s]  and do some really cool shit.
[1762.00s -> 1765.00s]  And what I can do is place traffic on the network
[1765.00s -> 1769.00s]  overcoming best path routing or shortest path routing
[1769.00s -> 1772.00s]  and I can fully utilize all the bandwidth
[1772.00s -> 1774.00s]  that I have deployed
[1774.00s -> 1777.00s]  and I can make new protection restoration decisions.
[1777.00s -> 1780.00s]  But what becomes really interesting is
[1780.00s -> 1782.00s]  if I know when I wanna place load on the network,
[1782.00s -> 1785.00s]  I can program that load to be placed on the network.
[1785.00s -> 1787.00s]  Basic stuff, back up my data center,
[1787.00s -> 1789.00s]  load my content caches.
[1789.00s -> 1791.00s]  Here comes an elephant of traffic
[1791.00s -> 1792.00s]  that I wanna send down my network.
[1792.00s -> 1796.00s]  Man, I just can't do this, dialing all the way back,
[1796.00s -> 1798.00s]  I just can't do this with a head end view
[1798.00s -> 1800.00s]  of load engineering onto a network.
[1800.00s -> 1803.00s]  I can't do this via polling interfaces
[1803.00s -> 1805.00s]  to 10,000 or more devices
[1805.00s -> 1808.00s]  and trying to create some crazy matrix in the sky.
[1808.00s -> 1810.00s]  That'll never be real time.
[1811.00s -> 1815.00s]  Instead, we'll use these modern computer science concepts
[1815.00s -> 1817.00s]  and tongue is in my cheek of
[1817.00s -> 1819.00s]  push the information out of the network
[1819.00s -> 1820.00s]  that you want to,
[1820.00s -> 1822.00s]  have it in a logically centralized place
[1822.00s -> 1825.00s]  and in particular, start removing states
[1825.00s -> 1827.00s]  because once I have that centralized view
[1827.00s -> 1829.00s]  and I'm not doing head end traffic engineering
[1829.00s -> 1831.00s]  or load engineering, I don't need RSVP.
[1831.00s -> 1833.00s]  I don't need these protocols.
[1833.00s -> 1836.00s]  I can do it in a much, much more efficient way.
[1836.00s -> 1838.00s]  And so these fundamental concepts
[1838.00s -> 1840.00s]  are really at the forefront of routing protocols,
[1840.00s -> 1842.00s]  routing technology and the WAN.
[1844.00s -> 1847.00s]  This was, as always, the fire hose
[1847.00s -> 1849.00s]  of fantastically cool information, David.
[1849.00s -> 1851.00s]  I can't thank you enough.
[1851.00s -> 1854.00s]  But I think I've taken enough of your time.
[1854.00s -> 1856.00s]  You gotta go save the internet.
[1856.00s -> 1858.00s]  I gotta go do something, that's for sure.
[1858.00s -> 1859.00s]  All right.
[1859.00s -> 1861.00s]  Hey, if you've got students out there
[1861.00s -> 1864.00s]  or other folks that wanna get involved
[1864.00s -> 1866.00s]  in this type of work,
[1866.00s -> 1867.00s]  we're doing it all in the open.
[1867.00s -> 1868.00s]  It's all in standards bodies.
[1868.00s -> 1870.00s]  We're doing quite a bit in open source
[1870.00s -> 1873.00s]  and a lot of it is available.
[1873.00s -> 1875.00s]  Again, I'd like to move the conversation
[1875.00s -> 1878.00s]  about SDN and orchestration away from, say,
[1878.00s -> 1879.00s]  just the data center stuff
[1879.00s -> 1882.00s]  and what you can do with some of these other projects.
[1882.00s -> 1885.00s]  But routing is still cool, man.
[1885.00s -> 1886.00s]  Will do.
[1886.00s -> 1887.00s]  It is.
[1887.00s -> 1888.00s]  Cool.
[1888.00s -> 1889.00s]  Well, have a great night
[1889.00s -> 1890.00s]  and thank you again so much.
[1890.00s -> 1891.00s]  Hey, talk to you soon, Phil.
[1891.00s -> 1892.00s]  See you.
[1892.00s -> 1893.00s]  Bye.
[1897.00s -> 1899.00s]  Thank you.
