# Detected language: en (p=1.00)

[0.00s -> 4.94s]  In this video I'm going to continue the description of packet switching and in
[4.94s -> 8.74s]  particular I'm going to tell you about how we can guarantee delays from one
[8.74s -> 12.00s]  end of a network to another. This may come as a bit of a surprise because in
[12.00s -> 16.40s]  earlier videos I was telling about how the queuing delay is variable and
[16.40s -> 20.78s]  so we generally cannot control the delay through the network but we're
[20.78s -> 24.24s]  going to use special techniques that rely on the weighted fair queuing that
[24.24s -> 27.02s]  we learned in the rate guarantee video so you should make sure you
[27.02s -> 33.18s]  watch that one first. Let me start with giving you some intuition on how delay
[33.18s -> 38.26s]  guarantees are going to work. So recall our end-to-end delay equation which
[38.26s -> 43.22s]  tells us the delay from one end of the network to the other as a function of
[43.22s -> 49.02s]  the packetization delay that is the fixed component of the packet length
[49.02s -> 54.26s]  divided by the rate plus the propagation delay which is the length
[54.26s -> 58.90s]  of the link divided by the propagation delay or the speed of light plus the
[58.90s -> 64.90s]  queuing delay. And the first two terms are defined as fixed functions
[64.90s -> 70.16s]  of the network. They depend on things under our control. Normally the queuing
[70.16s -> 73.88s]  delay is not under our control so if we want to provide an end-to-end delay
[73.88s -> 77.70s]  guarantee then we're going to have to provide a delay for the queue through
[77.70s -> 83.78s]  every router along the path. Okay so the basic idea is if we know the upper
[83.78s -> 88.86s]  bound on q1, q2 and q3 then we know the upper bound of the end-to-end delay
[88.86s -> 96.22s]  overall from this equation. So how do we do that? So if in a router I know
[96.22s -> 103.10s]  which queues a packet passes through and I know the size of the buffer so
[103.10s -> 106.66s]  let's say I'm looking at packets that are going to go through this queue
[106.66s -> 111.90s]  here inside the router I know the size of the buffer it's going to go
[111.90s -> 116.70s]  through and I know the rate at which that buffer is going to be served then I
[116.70s -> 121.54s]  know the maximum delay that a packet can encounter through this router because
[121.54s -> 126.66s]  WFQ, weighted fair queuing, that we saw in the in the rate guarantee video
[126.66s -> 133.22s]  gives me a rate r1 and then I can simply say that the delay through this
[133.22s -> 138.48s]  router will be bounded by the size of the buffer divided by r1, the rate
[138.48s -> 145.96s]  that it achieved. And remember that that r1 equals the weight that I was
[145.96s -> 150.08s]  going to give to that first one divided by the sum of all the weights
[150.08s -> 159.48s]  times r. So I can control r, r1, I can pick a B therefore I can pick the
[159.48s -> 165.04s]  delay through the router. How do I actually do this in practice? Let's take
[165.08s -> 170.76s]  a look at that. So we can control the delay of the packets from things that we
[170.76s -> 174.60s]  already know. What we already know how to control is the rate at which a queue
[174.60s -> 181.56s]  is served using WFQ and the size of each queue. This suggests a model for a
[181.56s -> 186.64s]  router where we classify the packets as they come in. So this is the arriving
[186.64s -> 190.88s]  packets and I'm going to classify them. I'm going to decide the flow to which
[190.88s -> 195.44s]  they belong and then I'm going to stick them into that queue. Alright so if they
[195.44s -> 199.40s]  had gone into another queue down here that might have been possible, which is
[199.40s -> 204.16s]  going to be serviced at that rate r1 and this one would be at rate rn
[204.16s -> 207.56s]  and then they're going to come together and go on to the outgoing
[207.56s -> 215.24s]  line of rate r. So if I can set this at the correct rate, I can set the size
[215.24s -> 219.36s]  of the buffer, then I can control the delay. Any packet arriving to
[219.36s -> 222.60s]  the router will have a bounded delay and if we add up all the components of
[222.60s -> 225.36s]  delay correctly using our equation then we can make it work end-to-end
[225.36s -> 230.08s]  according to what we know. This works for packets that make it all the
[230.08s -> 237.48s]  way through, but here's the problem. What if a packet arrives at such a rate
[237.48s -> 242.48s]  that it overflows the buffer here and falls under the ground? In other words
[242.48s -> 246.08s]  that we fill up this buffer just because of the arrival process to this
[246.08s -> 250.36s]  queue. So this is really the remaining problem that we have to solve because
[250.36s -> 253.40s]  there's no point in having end-to-end delay guarantee if packets are going to
[253.40s -> 257.96s]  get dropped along the way. That's not really a delay guarantee. So we need to
[257.96s -> 261.28s]  figure out how we can how we can prevent that buffer from overflowing
[261.28s -> 267.60s]  and if we can do this then we've solved our overall problem. So how do
[267.60s -> 271.40s]  we make sure that no packets are dropped? So we're going to zoom in on one
[271.40s -> 274.68s]  queue and take a look at this and we're going to go back to something
[274.88s -> 281.08s]  that we saw before which was our simple deterministic model of the
[281.08s -> 285.80s]  dynamics of a queue. So you'll remember that we had for a queue like this we
[285.80s -> 293.56s]  could model it as the cumulative bytes. So cumulative bits or bytes. Actually
[293.56s -> 297.76s]  I'll say bits because it's going to make it a little easier to explain as a
[297.76s -> 304.24s]  function of time. So this is the time evolution and you remember that we
[304.24s -> 309.08s]  said we have the cumulative arrival process which might look something like
[309.08s -> 315.08s]  this. And then we're going to have the cumulative departure process which is
[315.08s -> 318.68s]  going to be the times at which the queue is empty. We're going to serve it
[318.68s -> 322.66s]  at some fixed rate and then it'll pause until there's enough
[322.66s -> 327.20s]  accumulated, some accumulated and then so on. And then it'll pause and so the
[327.20s -> 331.68s]  rate at this point is slightly less than the arrival rate. So here we have
[331.92s -> 338.20s]  A of t, the arrival process, the cumulative arrival process, and here we have D of t,
[338.20s -> 342.48s]  the cumulative departure process. And you'll remember that if we are
[342.48s -> 346.56s]  interested in in the delay of individual packets through this queue or
[346.56s -> 351.76s]  in this case individual bits, we take the horizontal distance. This is the
[351.76s -> 355.56s]  time that a bit arrived. This is the time that that bit departed because
[355.56s -> 362.24s]  it's a FIFO queue that we have. Then the little D of t, the delay, is simply the
[362.24s -> 368.44s]  horizontal distance. And the maximum size of the buffer that we need is the
[368.44s -> 371.36s]  maximum horizontal distance between these two lines because that's the
[371.36s -> 378.04s]  maximum distance between what's arrived and what's departed. So if that vertical
[378.04s -> 384.56s]  distance grows too big, grows to larger than B, so this here is Q of t, right,
[384.56s -> 389.24s]  the amount that we have in the queue at only one time. If Q of t grows to be
[389.24s -> 393.64s]  greater than or equal to B, then we're gonna have packets that are gonna get
[393.64s -> 397.40s]  dropped onto the floor. How are we gonna make sure that this doesn't happen?
[397.40s -> 401.36s]  This is what we're gonna look at next. And as I said, if we can solve this
[401.36s -> 404.32s]  then we can provide the delay guarantee through the router that we're
[404.32s -> 412.32s]  looking for. The way that we're going to approach it is as follows. I'm gonna
[412.36s -> 420.16s]  re-sketch this deterministic queue model. So this is our, and I'm
[420.16s -> 425.76s]  gonna focus now just on the arrival process. So this is a cumulative arrival
[425.76s -> 434.24s]  process A of t. It just needs to be non-decreasing to be plausible. And
[434.24s -> 439.76s]  we're gonna say that in any time interval, so in any time interval, let's
[439.76s -> 448.72s]  take a time interval like this, if we can guarantee that no more than B plus R1
[448.72s -> 457.44s]  times t, where t is the time interval, we can say that no more than B plus R1
[457.44s -> 465.16s]  times t bits arrive, then the buffer can't possibly overflow. So this would be
[465.16s -> 470.84s]  over any time interval t. Because we know it's being drained at rate R1, so R1
[470.84s -> 474.20s]  times t will have departed. We just need to make sure that we haven't
[474.20s -> 477.96s]  accumulated more than B in any time interval. So if we make this t any
[477.96s -> 482.96s]  value and we never violate this, then we can be sure that we've never
[482.96s -> 493.04s]  overflowed the queue. So in other words, A of t at the time t plus capital T
[493.04s -> 498.72s]  minus the occupancy or the cumulative arrivals at time t is less
[498.72s -> 507.92s]  than or equal to this expression here, B plus R1 times t. So if we can make
[507.92s -> 512.96s]  sure that this guarantee is met, in other words, this expression defined
[512.96s -> 519.98s]  here is met, then the queue will never overflow. So we know the delay is
[519.98s -> 524.86s]  guaranteed because we're serving at rate R1. And we've given quite a lot of
[524.86s -> 529.90s]  leeway to the arrival process A of t. We've constrained it to make sure that
[529.90s -> 534.82s]  it must fit within this requirement that in this time frame no more than B
[534.82s -> 539.58s]  plus R1 t, but we've given it some leeway on how it accomplishes this. So
[539.58s -> 545.34s]  let's look at this in a bit more detail. We're going to
[545.34s -> 548.78s]  constrain the traffic and we're going to use a fairly well-known
[549.02s -> 554.22s]  technique for doing this. It's something that's called sigma-row regulation. I'll
[554.22s -> 557.70s]  tell you what sigma-row regulation is. It's basically the idea I just told
[557.70s -> 562.66s]  you. So if this blue squiggly line here is our arrival process, our
[562.66s -> 567.66s]  cumulative arrival process A of t, I'm going to say that the number of bits
[567.66s -> 573.18s]  that can arrive in any period of length t is bounded plus by sigma plus
[573.18s -> 580.18s]  rho t. So this is just like my B plus rho 1 t equation just now. We can think of
[580.18s -> 586.42s]  this as in any time we can draw that sigma plus rho t
[586.42s -> 592.42s]  by this blue line here. And we could start it at any point and you can see
[592.42s -> 596.38s]  that it's basically saying that if we touch it down on any point of A of t, A
[596.38s -> 601.42s]  of t in the future must lie below that line. So wherever we start, wherever we
[601.42s -> 607.18s]  slide this, it must always lie, the A of t must always lie underneath it. If that
[607.18s -> 613.42s]  is true, then this equation holds and we say that A of t is sigma-row
[613.42s -> 619.18s]  regulated. You can see that A of t has quite a lot of leeway on how it
[619.18s -> 623.18s]  fits under that. It just must never exceed it starting from any one point.
[623.18s -> 627.30s]  So in our example sigma equals B and rho equals r1 and the only reason I'm
[627.30s -> 630.46s]  telling you about sigma-row regulation is that you'll find it commonly
[630.50s -> 634.50s]  described in textbooks as sigma-row regulation. And in our example, I just
[634.50s -> 639.50s]  happened to use B and r1 for the queue I was looking at.
[639.50s -> 644.50s]  Okay, to reiterate this point then, if I've got sigma-row constrained
[644.50s -> 649.50s]  arrivals and a minimum service rate, so my minimum service rate here is r1,
[649.50s -> 654.50s]  that's the rate at which the queue is being served. I've got a cumulative
[654.50s -> 659.50s]  arrival process here, the green-blue line, and then the departure
[659.54s -> 664.54s]  process here, the red line, and I'm going to constrain A of t
[664.54s -> 669.54s]  to always lie below this sigma-row line, the sigma plus rho t line.
[669.54s -> 674.54s]  Remember that this constraint must be held for all
[674.54s -> 679.54s]  times wherever I start the sigma plus rho t. So if I slide it along, for example starting
[679.54s -> 684.54s]  here and starting here, it must be true on all of those occasions. But if I do that,
[684.58s -> 689.58s]  then I know that the distance between A of t and D of t is less
[689.58s -> 694.58s]  than the distance between this blue line and D of t. And so
[694.58s -> 699.58s]  therefore I can constrain both Bmax, that's the maximum queue occupancy I need
[699.58s -> 704.58s]  to never overflow, and Dmax, the maximum delay of any bit through
[704.58s -> 709.58s]  the queue. So in summary, for no packet loss, I need that
[709.62s -> 714.62s]  B is greater than or equal to sigma. And if the rate at which I'm serving is greater than
[714.62s -> 719.62s]  this rho, then the delay is less than or equal to B over r1.
[719.62s -> 724.62s]  So I've now bounded the delay based on things that I can control, B and r1.
[724.62s -> 729.62s]  It still doesn't tell me how I'm going to do this. It just tells me that if
[729.62s -> 734.62s]  I can constrain A of t, then this will hold. And so what I'm going to tell you
[734.66s -> 739.66s]  next and describe is that if the flows are what we're going to call leaky
[739.66s -> 744.66s]  bucket constrained, and the routers use weighted fair queuing, then end-to-end delay guarantees
[744.66s -> 749.66s]  are possible. So what is this leaky bucket constraint? Well, it turns out the
[749.66s -> 754.66s]  leaky bucket is something that implements the sigma-rho regulator, and therefore
[754.66s -> 759.66s]  makes all of this work. Let's take a look at what that leaky bucket is.
[759.70s -> 764.70s]  So the leaky bucket regulator looks like this. Packets are going to arrive,
[764.70s -> 769.70s]  so here are my packets arriving here, and they're going to go into the packet buffer. And the rule
[769.70s -> 774.70s]  is that I can send or I can take packets out of the buffer only if
[774.70s -> 779.70s]  there are enough tokens, and the tokens are being made available
[779.70s -> 784.70s]  here at a particular rate, rho, and the token bucket size is of sigma.
[784.74s -> 789.74s]  So the tokens here are just a scheduling mechanism. The tokens don't go out onto the wire.
[789.74s -> 794.74s]  This token bucket is just a way of holding and implementing the scheduling
[794.74s -> 799.74s]  mechanism that constrains the traffic. So this is something we're going to do at the source.
[799.74s -> 804.74s]  At A, when it's sending the packets onto the line, we're going to make sure that they are sigma-rho constrained
[804.74s -> 809.74s]  using the leaky bucket regulator. It can send them onto the wire if and only if
[809.78s -> 814.78s]  there are enough tokens in its bucket. So it's going to accumulate
[814.78s -> 819.78s]  at rate rho with burst in a sigma. In other words, the maximum that it can have
[819.78s -> 824.78s]  in that bucket is sigma. And then it will send a packet if there are sufficient tokens that
[824.78s -> 829.78s]  allow to send a packet of that size. So if the tokens are in bits, then you have to have enough
[829.78s -> 834.78s]  tokens to represent the packet that you're trying to send. And as soon as you send them, then you use
[834.82s -> 839.82s]  the tokens and you've got to wait for more to be put in. And you can probably see how this will make
[839.82s -> 844.82s]  sure that we're allowing for bursts of up to sigma, but over on the long term
[844.82s -> 849.82s]  rate is only a rho. And so that will meet the constraint that we want.
[849.82s -> 854.82s]  So putting it all together then, what have we got? We have a
[854.82s -> 859.82s]  sigma-rho constrained traffic here. So this would be our sigma-rho
[859.86s -> 864.86s]  constrained traffic going in that's coming out of our leaky bucket at
[864.86s -> 869.86s]  A. Each router is going to run WFQ,
[869.86s -> 874.86s]  waited for queuing, in order to make sure that we get a service rate of
[874.86s -> 879.86s]  r1 for that particular flow and a buffer size of at least the B that we need.
[879.86s -> 884.86s]  And that will be at each one, so we'll have r2 here and B.
[884.90s -> 889.90s]  And along the path, we're going to take the packets and make sure
[889.90s -> 894.90s]  they're going into the correct queue that is being serviced at that rate. So we call that packet
[894.90s -> 899.90s]  classification to put it into the correct queue and then eventually it will find its way.
[899.90s -> 904.90s]  Then we can use our
[904.90s -> 909.90s]  equation for end-to-end delay in order to calculate the entire delay
[909.94s -> 914.94s]  along the path. So you may be wondering how these
[914.94s -> 919.94s]  values of sigma and rho and the values of
[919.94s -> 924.94s]  r1 and B and r2 and B get told to the routers and the source along the way.
[924.94s -> 929.94s]  So there's actually a protocol for doing this, for setting this up initially.
[929.94s -> 934.94s]  And this is something that's called RSVP or the Resource Reservation Protocol.
[934.98s -> 939.98s]  And there is an ITF RFC that will tell us
[939.98s -> 944.98s]  all about what we're supposed to do, and it's number 2205. You can find this in any
[944.98s -> 949.98s]  textbook. You can go and look at the RFC, or if you look it up in Wikipedia, you'll find a good
[949.98s -> 954.98s]  description of this. So this is how we populate these values in the first place.
[954.98s -> 959.98s]  How it is that end-to-end, the control system is able to install
[960.02s -> 965.02s]  these values along the path. So let's look at a worked example of this now.
[965.02s -> 970.02s]  So imagine that in the network below, we want
[970.02s -> 975.02s]  an application to be able to send at a rate of 10 megabits per second
[975.02s -> 980.02s]  and have an end-to-end delay of less than 5 milliseconds when it sends
[980.02s -> 985.02s]  1000 byte packets. So let's see how we would do that.
[985.06s -> 990.06s]  So first of all, let's calculate the fixed portion, because there's nothing we can do about the fixed portion of the end-to-end delay.
[990.06s -> 995.06s]  That's going to be made up by the packetization delay and the propagation delay.
[995.06s -> 1000.06s]  So let's look at the packetization delay first. Packetization delay
[1000.06s -> 1005.06s]  end-to-end is going to be, well, it's the
[1005.06s -> 1010.06s]  sum of the packetization delay across all of the links. So on the first link
[1010.10s -> 1015.10s]  I've got 1000 byte packets that I'm sending. So they are 1000 times
[1015.10s -> 1020.10s]  8 bits divided by the rate that they're going to go over the first link
[1020.10s -> 1025.10s]  which is a 1 gigabit per second. So that's 10 to the power of 9.
[1025.10s -> 1030.10s]  And plus, oops, plus, I need
[1030.10s -> 1035.10s]  the same thing for the 1000 byte packets
[1035.14s -> 1040.14s]  going over the 100 megabit per second, which is 10 to the 8.
[1040.14s -> 1045.14s]  And then I'm going to have the same thing over here for this. I'm just going to multiply this by 2
[1045.14s -> 1050.14s]  because the last link is the same. And then I've got the
[1050.14s -> 1055.14s]  propagation delay, the propagation delay
[1055.14s -> 1060.14s]  along all of these links, which is going to be the
[1060.18s -> 1065.18s]  length of the link, which is 10 kilometers, divided by
[1065.18s -> 1070.18s]  the rate. So I've got 120 kilometers in total.
[1070.18s -> 1075.18s]  120 kilometers, so that's 10 times 10 to the 3 meters, divided by the
[1075.18s -> 1080.18s]  propagation speed, which I'm just going to assume is 2 times 10 to the 8 meters per second.
[1080.18s -> 1085.18s]  And so if I add these two together, I've already done that, and that comes out as
[1085.22s -> 1090.22s]  0.696 milliseconds.
[1090.22s -> 1095.22s]  So I've got a fixed propagation delay of 0.696 milliseconds, and so therefore
[1095.22s -> 1100.22s]  I need the sum of all the queuing delays along the path to be less than or equal to
[1100.22s -> 1105.22s]  the difference between that and 5 milliseconds. So that needs to be less than
[1105.22s -> 1110.22s]  4.304 milliseconds.
[1110.26s -> 1115.26s]  So we're going to remember this number because I'm going to clear this, and then we're going to figure out how to make that work.
[1115.26s -> 1120.26s]  So let's choose to split the delay,
[1120.26s -> 1125.26s]  the queuing delay, equally amongst the two routers.
[1125.26s -> 1130.26s]  I can actually do this in any way I want. It's going to make the math easier if I split it equally.
[1130.26s -> 1135.26s]  In other words, it's going to be just over 2 milliseconds for each one.
[1135.30s -> 1140.30s]  So let's clear that. So I'm going to make it so that the delay through here
[1140.30s -> 1145.30s]  is less than or equal to
[1145.30s -> 1150.30s]  152 milliseconds, and the same thing here.
[1150.30s -> 1154.30s]  That's the same value.
[1154.30s -> 1159.30s]  Now, I know what the rate is of the flow. It's 10 megabits per second, so I know the rate at which
[1159.30s -> 1164.30s]  the queue is going to be serviced. That's going to be 10 megabits per second.
[1164.34s -> 1169.34s]  Now I'm trying to figure out how big B should be, how big should the buffer be.
[1169.34s -> 1174.34s]  So B, the size of the buffer, the buffer in each one,
[1174.34s -> 1179.34s]  B needs to be large enough that I never drop anything.
[1179.34s -> 1184.34s]  And so it's got to be greater than 10 megabits per second, because that's the rate at which it's going to be serviced,
[1184.34s -> 1189.34s]  times the duration of a bit through it. Well, we already know that's
[1189.38s -> 1193.38s]  2.152 milliseconds.
[1193.38s -> 1197.38s]  And so this ends up as
[1197.38s -> 1201.38s]  21,520 bits,
[1201.38s -> 1206.38s]  which is 2,690 bytes.
[1206.38s -> 1210.38s]  So I've got roughly 3 packets worth of delay that I must have.
[1210.38s -> 1215.38s]  So basically what this tells me is, using weighted fair queuing, I will serve
[1215.42s -> 1218.42s]  these queues at a rate of 10 megabits per second.
[1218.42s -> 1223.42s]  I know I need to do that in order for the system to meet this 10 megabit per second requirement.
[1223.42s -> 1228.42s]  And I will assign a buffer at each of those routers to be at least
[1228.42s -> 1233.42s]  2,690 bytes at each of those along the path.
[1233.42s -> 1238.42s]  Then, if I add up all of the delays, my overall end-to-end delay will be
[1238.42s -> 1243.42s]  less than or equal to 5 milliseconds, which is what I was trying to achieve.
[1245.42s -> 1250.42s]  ...
[1250.42s -> 1255.42s]  ...
[1255.42s -> 1260.42s]  ...
[1260.42s -> 1265.42s]  ...
[1265.42s -> 1270.42s]  ...
[1270.46s -> 1275.46s]  ...
[1275.46s -> 1280.46s]  ...
[1280.46s -> 1285.46s]  So in practice, as I've shown you, it's technically possible
[1285.46s -> 1290.46s]  to provide this end-to-end delay, but actually very few networks actually control the end-to-end delay.
[1290.46s -> 1295.46s]  In other words, this is not really used very widely in practice. Why is that?
[1295.50s -> 1300.50s]  It turns out it's quite complicated to make it work. It requires coordination
[1300.50s -> 1305.50s]  amongst all of the players, all of the network operators,
[1305.50s -> 1310.50s]  the end-to-end, the routers along the way. And in practice, this hasn't
[1310.50s -> 1315.50s]  really taken off. And in most networks, a combination of over-provisioning and
[1315.50s -> 1320.50s]  priorities of traffic, so giving high priority to those that need special treatment, has proved to work well enough
[1320.54s -> 1325.54s]  in most cases. I wanted to go through and tell you this because if you can understand how
[1325.54s -> 1330.54s]  this whole weighted fair queuing mechanism works and how we can provide end-to-end delay guarantees, you
[1330.54s -> 1335.54s]  understand a lot about the queuing dynamics of a packet switch network. And also, it's
[1335.54s -> 1340.54s]  quite likely that some of these ideas will be used in some networks in the future, so they should prove
[1340.54s -> 1345.54s]  useful to you. And so, in summary, if we know
[1345.58s -> 1350.58s]  the size of a queue and the rate at which it's being served, then we can bound the delay
[1350.58s -> 1355.58s]  through it. We can pick the size of the queue, and using weighted fair queuing,
[1355.58s -> 1360.58s]  we can pick the rate at which the queue is served. Therefore, we just need a way to
[1360.58s -> 1365.58s]  prevent packets from being dropped along the way. For this, we use a leaky bucket regulator.
[1365.58s -> 1370.58s]  And with that, we can therefore bound the end-to-end delay.
[1370.58s -> 1372.58s]  That's the end of the video.
