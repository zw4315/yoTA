# Detected language: en (p=1.00)

[0.00s -> 4.00s]  So, in this video about the physical and link layers, I'm going to talk about
[4.00s -> 7.00s]  forward error correction, or FEC. So, recall that for a given signal to
[7.00s -> 11.00s]  noise ratio and modulation scheme, we can compute what the expected bit error rate
[11.00s -> 13.00s]  is. Now, this bit error rate, while it can
[13.00s -> 17.00s]  become very, very small for, say, a high, very high signal strength, it will
[17.00s -> 20.00s]  never reach zero. There's always a chance that there will
[20.00s -> 23.00s]  be a bit error. So, in practice, what this means is
[23.00s -> 28.00s]  that because these bit errors, although uncommon, you expect them to happen.
[28.00s -> 32.00s]  Directly turning your link layer bits into bits at the physical layer is very, very
[32.00s -> 36.00s]  inefficient in that the sparsity of the modulation you need or the speed of the
[36.00s -> 40.00s]  symbols that you need in order to have a very, very low bit error rate so that
[40.00s -> 44.00s]  packet bit errors are uncommon is really, really inefficient.
[44.00s -> 47.00s]  It's going to be very, very far from the Shannon limit.
[47.00s -> 50.00s]  That's not a good way to build a high throughput system.
[50.00s -> 53.00s]  So, instead, what you want to do is coding.
[53.00s -> 57.00s]  So, coding is a way where you add a little bit of redundancy to the data
[57.00s -> 60.00s]  In this case, we're talking about the physical error, which you can do in all
[60.00s -> 64.00s]  kinds of situations. So, you add a little bit of redundancy
[64.00s -> 68.00s]  to make up for these expected uncommon bit errors.
[68.00s -> 72.00s]  And in adding in this little bit of redundancy, its cost is much, much
[72.00s -> 75.00s]  smaller than its benefit. By doing this, you can greatly, greatly
[75.00s -> 78.00s]  improve your link throughput because just by adding a little bit of redundancy, all
[78.00s -> 81.00s]  your packets get through as opposed to almost none of your packets get
[81.00s -> 83.00s]  through. And this is true in theory as well as practice.
[83.00s -> 86.00s]  So, we talk about coding. We also talk about coding gain,
[86.00s -> 90.00s]  which is the ratio of bits, in this case, the link layer to the bits at the
[90.00s -> 92.00s]  physical layer. Coding gains more general than that,
[92.00s -> 95.00s]  but within networks, usually we're talking about the physical layer and the link
[95.00s -> 97.00s]  layer. So, one half code means that we turn
[97.00s -> 100.00s]  one link layer bit into two physical layer bits.
[100.00s -> 102.00s]  So, there's one redundant bit for every bit.
[102.00s -> 106.00s]  Three quarter code is we turn three link layer bits into four bits at the
[106.00s -> 109.00s]  physical layer. And this process is, is forward error
[109.00s -> 112.00s]  correction. The idea is, proactively add some
[112.00s -> 117.00s]  additional redundant data to prot, to protract and be able to correct potential
[117.00s -> 120.00s]  errors. It's called forward error correction
[120.00s -> 122.00s]  because you're doing it proactively, forward.
[122.00s -> 125.00s]  You're saying, look, I don't know if we're going to have any errors, but
[125.00s -> 128.00s]  beforehand, I'm just going to add a bit of redundancies that you can recover
[128.00s -> 130.00s]  from them. This is nice because you don't need
[130.00s -> 132.00s]  any exchanges. The recipient will be able to just
[132.00s -> 134.00s]  decode the data. It's not going to have to say, oh,
[134.00s -> 136.00s]  there was an error. Can you resend this part?
[136.00s -> 139.00s]  And so, it saves you the cost of those kinds of message exchanges.
[140.00s -> 142.00s]  So, how do you do this? Well, there's all kinds of coding
[142.00s -> 145.00s]  algorithms out there. There's lots of different ones, all
[145.00s -> 149.00s]  kinds of different trade-offs. Here's just a couple of them.
[149.00s -> 153.00s]  So, in this video, I'm going to talk about one in particular, Reed-Solomon.
[153.00s -> 156.00s]  So, I'm going to talk about Reed-Solomon because it turns out, compared to
[156.00s -> 160.00s]  many of these others, it's actually mathematically pretty simple.
[160.00s -> 166.00s]  It's also tremendously commonly used. CDs use Reed-Solomon, DVDs, DSL lines,
[166.00s -> 171.00s]  WiMAX, RAID 6 storage arrays, all of these systems, all of these communication
[171.00s -> 176.00s]  storage systems, use Reed-Solomon. Furthermore, compared to some of these
[176.00s -> 180.00s]  other coding algorithms, Reed-Solomon is actually pretty mathematically simple.
[180.00s -> 183.00s]  I'm not going to go into all of the details.
[183.00s -> 187.00s]  The basic concept is simple. Actually designing it so that you can
[187.00s -> 191.00s]  implement it very fast involves a bit more math, which I won't go into, but
[191.00s -> 195.00s]  the basic concept is very simple. The key idea behind Reed-Solomon is
[195.00s -> 200.00s]  that if I have a polynomial like here, a polynomial like here where I have a
[200.00s -> 206.00s]  parabola, right, so I have some ax squared plus bx plus c, then any
[206.00s -> 214.00s]  polynomial of degree k, or here k equals two, is uniquely determined by k
[214.00s -> 220.00s]  plus one points. So that means is that if I give you
[220.00s -> 227.00s]  three xy points, so three xy points will uniquely determine for a parabola a,
[227.00s -> 232.00s]  b, and c. So why is this useful?
[232.00s -> 237.00s]  So what I can do is I have some data that I want to encode with Reed-Solomon.
[237.00s -> 242.00s]  So I take k chunks of this data. These k chunks become coefficients of a k
[242.00s -> 247.00s]  minus one degree polynomial. So for example, I take three chunks of
[247.00s -> 253.00s]  this data, and those three chunks become a, b, and c, these three coefficients.
[253.00s -> 259.00s]  Then what I do is I compute n points along this polynomial, right, where n is
[259.00s -> 264.00s]  greater than or equal to k minus one, so it should be minus one.
[264.00s -> 269.00s]  Right, so I compute these n points, and that's what I send.
[269.00s -> 275.00s]  I send those points along the polynomial. Now what happens is I'm sending n
[275.00s -> 280.00s]  points, but because the original polynomial is of degree k minus one, any
[280.00s -> 285.00s]  of those k points, where k is less than n, would allow you to uniquely and
[285.00s -> 288.00s]  correctly determine what these coefficients were.
[288.00s -> 294.00s]  So for example, I have a second degree polynomial, which I generated from three
[294.00s -> 300.00s]  chunks of data, a, b, and c. Then what I do is I compute point zero,
[300.00s -> 308.00s]  point two, say point three, point four, point five, and point six.
[308.00s -> 312.00s]  I now send p zero, p one, p two, p three, p four, p five, p six.
[312.00s -> 317.00s]  Now it turns out that with any of these three points, right, with p one,
[317.00s -> 322.00s]  p five, and p six, or with p four, p one, and p three, you can, if you know
[322.00s -> 326.00s]  this is a parabola, you can determine what a, b, and c are.
[326.00s -> 330.00s]  So that's the basic math. I compute points along the polynomial whose
[330.00s -> 335.00s]  coefficients are the original data, and then if I'm able to recover enough of
[335.00s -> 340.00s]  those points to the polynomial, I can reconstitute what those coefficients
[340.00s -> 342.00s]  were. There's some complications to this,
[342.00s -> 345.00s]  like, I, I can't just, like, p two can't be a million, because how would I
[345.00s -> 348.00s]  represent a million? It's going to take up more space.
[348.00s -> 351.00s]  It's, there's some thing where, there's some complications where the actual
[351.00s -> 355.00s]  numbers you use aren't just basic integers, they're things, numbers in a, in a
[355.00s -> 359.00s]  finite field, which means sort of a mathematical construct where it's closed
[359.00s -> 362.00s]  over operations. Which essentially means that, hey, I can
[362.00s -> 365.00s]  represent each of these points in a finite number of bits.
[365.00s -> 370.00s]  It's not like suddenly a is a, a million, and then that means that p six is
[370.00s -> 375.00s]  something totally off the charts, and I can't represent it in a finite space.
[375.00s -> 379.00s]  And so that's one of the other complications that I'm not going to go into.
[379.00s -> 383.00s]  But this is the basic idea, is that I represent the data as coefficients
[383.00s -> 386.00s]  of a polynomial. I compute points along the polynomial,
[386.00s -> 390.00s]  then I send those points, and then the other side from those points can
[390.00s -> 393.00s]  reconstitute the coefficients. Now a little bit more detail in terms
[393.00s -> 396.00s]  of what Reed-Solomon can do for you. So it turns out, in these kinds of
[396.00s -> 399.00s]  systems, there are two kinds of errors we care about.
[399.00s -> 402.00s]  And it's important to distinguish them. The first are erasures.
[402.00s -> 405.00s]  So these are errors where we know that they occurred.
[405.00s -> 407.00s]  Like that piece of data is missing. It's an erasure.
[407.00s -> 410.00s]  The other is a general error. We don't know that where the error
[410.00s -> 412.00s]  occurred. So this is what we normally think of in
[412.00s -> 415.00s]  terms of, say, bit errors, whereas erased values are, say, you know, oh gosh,
[415.00s -> 418.00s]  this disk didn't answer. Oh, we missed that packet.
[418.00s -> 421.00s]  And so what Reed-Solomon does is you're taking k chunks of data, and you're
[421.00s -> 425.00s]  coding it into n chunks, where n is greater than or equal to k, where if n is
[425.00s -> 428.00s]  equal to k, you're not actually doing any coding.
[428.00s -> 432.00s]  And what Reed-Solomon will do is it can correct up to n minus k erasures,
[432.00s -> 435.00s]  right? Because remember, if we code into n
[435.00s -> 439.00s]  chunks, if we have k points, and we know those k points are correct, because
[439.00s -> 442.00s]  they're just missing the others, we can reconstitute the data.
[442.00s -> 446.00s]  But it can also correct up to n minus k divided by two errors.
[446.00s -> 453.00s]  So, like, let's say we have a very common Reed-Solomon code, is 223255.
[453.00s -> 459.00s]  Which means that we take 223 bytes of data and turn it into 255 coded bytes
[459.00s -> 464.00s]  of data. Well, 255 minus 223 is equal to 32.
[464.00s -> 472.00s]  So, I mean, this particular Reed-Solomon code can protect against 32 erasures or
[472.00s -> 477.00s]  16 errors. Where 32 of the 8-bit code words can be
[477.00s -> 482.00s]  missing, as long as we've got 223, we can reconstitute the original data.
[482.00s -> 485.00s]  That's 223 points on our 222 degree polynomial.
[485.00s -> 491.00s]  Or if 16 of them are, have bit errors in them, we can still reconstitute the
[491.00s -> 492.00s]  original data.
[492.00s -> 498.00s]  So, here's that conceptually. Let's take 223 8-bit values.
[498.00s -> 504.00s]  So, we take our data break it into bytes, take 223 bytes, and we're going to
[504.00s -> 508.00s]  consider those now the coefficients of a 222 degree polynomial p.
[508.00s -> 512.00s]  We then compute p of zero, p of one, p of two, et cetera, et cetera, et
[512.00s -> 515.00s]  cetera as 8-bit values. There's this, again, this idea of
[515.00s -> 517.00s]  using a field rather than arbitrary numbers.
[517.00s -> 521.00s]  There's that mathematical concept. So we can then represent them as 8-bit
[521.00s -> 527.00s]  values. We then send these 255, 255 points along the polynomial.
[527.00s -> 533.00s]  And so this is a 255, 223 code. Each of these 255 code words come from
[533.00s -> 536.00s]  223 data words. 32 erasures, 16 errors.
[536.00s -> 540.00s]  So we send these 255 values. And that's the basic idea is that if I
[540.00s -> 544.00s]  have up to 16 errors, I can still figure out which ones are wrong and I can
[544.00s -> 547.00s]  reconstitute the polynomial. I can reconstitute the polynomial
[547.00s -> 550.00s]  coefficients. Or I could be missing 32 of them.
[550.00s -> 555.00s]  As long as I get 223 if I have 32 erasures, I can still reconstitute the
[555.00s -> 558.00s]  original polynomial. So as I said before, this isn't
[558.00s -> 561.00s]  exactly what's done in practice for a bunch of reasons.
[561.00s -> 564.00s]  These values have to be in a concept called a field.
[564.00s -> 569.00s]  It turns out that this exact scheme is actually pretty expensive to decode.
[569.00s -> 574.00s]  You have to consider all possible parameters and do an interpolation.
[574.00s -> 578.00s]  But it's giving you the basic idea. The more modern ones are a bit, a bit more
[578.00s -> 580.00s]  complex. But the basic idea holds of you're
[580.00s -> 584.00s]  using the original data as coefficients on a polynomial, computing points along
[584.00s -> 587.00s]  that polynomial, and then sending those points.
[587.00s -> 592.00s]  So here is a simple example. I take these six bytes that say hello.
[592.00s -> 595.00s]  I'm breaking them up into chunks of data.
[595.00s -> 602.00s]  I then do my Reed-Solomon coding here. Here's the basic concept, right?
[602.00s -> 606.00s]  I'm adding my forward error correction here, some redundant data.
[606.00s -> 611.00s]  Then when that data arrives, I can decode it and get the original data.
[611.00s -> 614.00s]  So let's walk through this as a specific example.
[614.00s -> 617.00s]  So in this case, let's say we're using a 7-5 code.
[617.00s -> 621.00s]  What this means is that each initial data word that we're going to use is
[621.00s -> 624.00s]  three bits long. That's determined by the seven, two
[624.00s -> 627.00s]  to the three minus one. So we're breaking the data up into
[627.00s -> 630.00s]  little chunks of three bits. And so we're going to start with
[630.00s -> 635.00s]  five of these data words, so 15 bits. And each 15 bits are going to be turned
[635.00s -> 639.00s]  into seven code words. So here we have the first 15 bits,
[639.00s -> 643.00s]  the next 15 bits, the next 15 bits. So it turns out we have 48 bits here.
[643.00s -> 647.00s]  48's not evenly divisible by 15. So we have 45 plus three.
[647.00s -> 651.00s]  So we'll just pad this with zeros. So original data is going to grow
[651.00s -> 656.00s]  from 48 to 60 bits. We then perform the Reed-Solomon
[656.00s -> 661.00s]  decoder to turn these 15 bits into 21 bits.
[661.00s -> 669.00s]  And then send these total of 84 bits along say the wire or on the wireless.
[669.00s -> 675.00s]  Those 84 bits are transmitted, received. The other side gets these 84 coded
[675.00s -> 681.00s]  bits, then runs a Reed-Solomon decoder, and from that can regenerate the
[681.00s -> 687.00s]  original data. So since this is a seven, five code,
[687.00s -> 693.00s]  that means that we can recover from one error or two erasures.
[693.00s -> 698.00s]  But note that these errors and erasures are for code words.
[698.00s -> 703.00s]  And so it's possible in fact that if we have, let's say we have bit errors
[703.00s -> 706.00s]  that look like this. Here's a bit error.
[706.00s -> 709.00s]  Here's a bit error. Here are two bit errors.
[710.00s -> 714.00s]  Can I ask the question, will we be able to recover from these bit errors?
[714.00s -> 717.00s]  So for this first code block, the answer is yes.
[717.00s -> 721.00s]  There's a single bit error. A single bit error will corrupt at
[721.00s -> 725.00s]  most one code word. We can recover from one error, one
[725.00s -> 729.00s]  code word with an error. And so this first block will be able
[729.00s -> 738.00s]  to successfully recover. The same with the second code block.
[738.00s -> 743.00s]  We have one bit error, that means one code word will be an error, so we can
[743.00s -> 748.00s]  recover from that. For the third code block, well this is
[748.00s -> 752.00s]  a tricky question. So, one of our code words can be an
[752.00s -> 755.00s]  error. And we have two bit errors.
[755.00s -> 758.00s]  So this now depends on where those bit errors fall.
[758.00s -> 761.00s]  If, let's say here are code words spaced out.
[761.00s -> 764.00s]  Here's two, three, four, five, six, seven.
[764.00s -> 770.00s]  Let's say they look like this. Well, if the two bit errors are in the
[770.00s -> 775.00s]  same code word, then we'll be okay, because we just have one error, one
[775.00s -> 780.00s]  code word with an error. So in that case, we'll be alright.
[780.00s -> 788.00s]  However, if the bit errors are in different code words, then we will not
[789.00s -> 795.00s]  be alright, because we'll have two errors, we can't recover from two errors.
[800.00s -> 804.00s]  So this might not seem very good, but compared to what happens if we were
[804.00s -> 808.00s]  not to use any coding whatsoever, here we've been able to protect against at
[808.00s -> 812.00s]  least two bit errors, and maybe even up to four.
[813.00s -> 816.00s]  So there's one other trick that you can use.
[816.00s -> 819.00s]  This is very common. This, for example, is done in CD.
[819.00s -> 823.00s]  So, one of the assumptions people, you often have, is that errors in the
[823.00s -> 826.00s]  physical layer, you know, some of them are, are just interspersed and
[826.00s -> 828.00s]  random. Those are easy to handle.
[828.00s -> 832.00s]  What's hard are bursts of errors. And so the way you can make your
[832.00s -> 836.00s]  coding scheme more robust to bursts of errors is through something called
[836.00s -> 839.00s]  interleaving. So, imagine for example, I have, here's
[839.00s -> 842.00s]  my data, and there's twelve chunks of the 7-5 code.
[842.00s -> 847.00s]  So that means that each of these chunks is, in terms of the coded data, is
[847.00s -> 851.00s]  twenty-one bits long. Remember from the prior showing it's
[851.00s -> 854.00s]  seven code words that are three bits long each.
[854.00s -> 860.00s]  And so here I have twelve code blocks from A to L with bits A0 to A20 and
[860.00s -> 865.00s]  say L0 to L20. So let's think about what happens when
[865.00s -> 870.00s]  we have a burst of errors. So it's possible with, let's say, a burst
[870.00s -> 874.00s]  of one error. Well, I know for sure I'm going to be
[874.00s -> 878.00s]  able to recover from that, because that's going to corrupt at most one
[878.00s -> 881.00s]  code word in one block, and I can recover from one error.
[881.00s -> 883.00s]  So that will be okay. One bit error.
[892.00s -> 893.00s]  I'm okay.
[895.00s -> 901.00s]  How about two bit errors? Well, again, it's going to matter on
[901.00s -> 907.00s]  whether they fall in the same code word or if they say fall on different code
[907.00s -> 910.00s]  words. In fact, it's possible that it could
[910.00s -> 913.00s]  each fall on, I have a burst of two errors.
[913.00s -> 917.00s]  One falls on a code word in one block, that block can be recovered.
[917.00s -> 921.00s]  Another falls on a code word in another block, that can be recovered.
[921.00s -> 925.00s]  We would be okay. So in this case, we would be okay.
[925.00s -> 933.00s]  If they both fell within a single code word, we would be okay.
[933.00s -> 939.00s]  But, if they fell on two different code words within the same block, then we
[939.00s -> 945.00s]  won't be able to recover from that. There'll be two errors within a single
[945.00s -> 948.00s]  block. In fact, it turns out, if we were to
[948.00s -> 953.00s]  recover, what's the, the longest string of bit errors that we could possibly
[953.00s -> 957.00s]  recover from? That's a good question.
[957.00s -> 966.00s]  So it turns out that the longest that we can recover from is six bit errors.
[966.00s -> 974.00s]  And we can recover from a burst of six bit errors if and only if it happens to
[974.00s -> 980.00s]  be that three of them fall on the last code word of one block, and then three fall
[980.00s -> 983.00s]  on the first code word of the next block.
[983.00s -> 986.00s]  So you can imagine if we add one more, it's going to corrupt a second code
[986.00s -> 988.00s]  word. So that is possible, but unlikely.
[988.00s -> 993.00s]  This is the longest possible string of errors we could correct from using just a
[993.00s -> 996.00s]  basic coding. So what we can do instead is
[996.00s -> 1000.00s]  something called interleaving. So rather than send our data as A through
[1000.00s -> 1005.00s]  A0 through A20, B through B20, where then a burst of errors is concentrated on a
[1005.00s -> 1009.00s]  block, instead what we send is these bits interleaved.
[1025.00s -> 1029.00s]  For example, we send A0, B0, C0, D0, etc., etc., etc.
[1029.00s -> 1033.00s]  to L0. A1, B1, C1, dot, dot, dot to L1.
[1033.00s -> 1036.00s]  Then at the end, we're going to send K20, L20.
[1036.00s -> 1041.00s]  And so now it happens that a burst of errors at the physical layer is spread
[1041.00s -> 1045.00s]  out across all of the different code blocks.
[1045.00s -> 1050.00s]  So in this case, if we have even a string of 12 bit errors, a string of 12
[1050.00s -> 1055.00s]  bit errors is going to do, cause one bit error on every single one of the code
[1056.00s -> 1058.00s]  blocks. So it'll cause one bit error in A, one
[1058.00s -> 1061.00s]  bit error in B, one bit error in C, one bit error in L.
[1061.00s -> 1064.00s]  And so in that way, we'll suddenly become more robust.
[1064.00s -> 1070.00s]  So with interleaving, by interleaving all of these bits, we can recover up to
[1070.00s -> 1074.00s]  12 bit errors with interleaving.
[1078.00s -> 1082.00s]  Where even, where we didn't have interleaving, even two bit errors can
[1082.00s -> 1086.00s]  cause our system to fail, and six bit errors the most we could ever do.
[1086.00s -> 1090.00s]  But if all we do is interleave these bits, we can deterministically, knowingly
[1090.00s -> 1094.00s]  recover a burst up to 12 bits long. So interleaving is a very common and a
[1094.00s -> 1098.00s]  very popular technique. So this concludes the video on coding.
[1098.00s -> 1102.00s]  If you want to read more, there's all kinds of interesting coding algorithms
[1102.00s -> 1105.00s]  out there. I also suggest, if you want, dig into
[1105.00s -> 1109.00s]  the math of Reed-Solomon in terms of how it works in practice to make
[1109.00s -> 1111.00s]  efficient coding and decoding.
[1112.00s -> 1114.00s]  Thank you.
