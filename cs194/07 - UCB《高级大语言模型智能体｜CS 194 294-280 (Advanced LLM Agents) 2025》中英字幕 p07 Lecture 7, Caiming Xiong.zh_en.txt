# Detected language: en (p=1.00)

[0.00s -> 8.24s]  Hello everyone, I'm Sami Hui. I'm very glad to get the invite to present this multimodal agent area.
[8.80s -> 16.80s]  I saw you already listened to the similar exact same area multimodal agent last week.
[18.40s -> 25.76s]  Hopefully I would bring something new in this direction. So first I want to show this plot
[26.64s -> 34.64s]  to everyone. So you can see how crazy it is, right? In the past four, five years,
[36.96s -> 44.56s]  almost all the benchmarks get saturated. And now we have the last one, which is
[48.00s -> 53.20s]  the one proposed this year, the human last exam. Seems like when we solve this one,
[53.28s -> 61.84s]  we solved the HDR. I'm not sure whether we can solve it this year, but I believe we'll have a
[61.84s -> 67.52s]  better, much better result for this data set, this benchmark by the end of this year.
[69.52s -> 76.16s]  Okay, since we have so much, so powerful frontier foundation models,
[76.80s -> 82.72s]  then what we should do on those models, how we could leverage them to make our world,
[82.72s -> 91.12s]  make the society better? I think the answer is agent. We believe the multimodal AI agent
[91.12s -> 101.04s]  is the next generation of applications. Why? It's because we want agent not just reading text.
[101.04s -> 111.44s]  We want the agent can hear, can see, can speak, can feel even. And we want the agent
[112.24s -> 121.76s]  to process a task like a human. Because most of the tasks that when we do in computer,
[122.56s -> 130.64s]  they involve multiple apps interfaces. So power with those, the new advanced
[132.88s -> 138.40s]  language model or the large foundation model, which is vision language action models.
[139.76s -> 148.16s]  It could make the digital interactions more acceptable, more automated and increase the
[148.96s -> 154.72s]  humans productivity. So, okay, so what are the status of the multimodal agent?
[155.92s -> 165.04s]  How far it is? Actually, it's not far away. We already see a lot of the multimodal agents
[165.04s -> 172.00s]  be used in the real world, or some of them is used in real world, I would say,
[172.00s -> 178.32s]  some of them are in the research. Like, for example, the coding agent. I think some of you,
[178.32s -> 185.52s]  maybe a lot of you are using the coding assistant or agent products like the cursor,
[185.52s -> 198.96s]  web plate or daily work or research. And those agents, it's not only that can accept the code
[198.96s -> 206.72s]  text, they also can let you to upload the images, and they could to generate the code
[207.28s -> 215.36s]  or fix your back through the images you send. Web agents. The web agents is also a
[215.36s -> 223.60s]  multimodal agent that the agent could help you, could automate some control in your browser
[223.60s -> 231.52s]  or apps. Same as the mobile agents and even physical agents. Super amazing, right?
[232.40s -> 240.16s]  So this is a robot called the Helix from Figure AI. It not just shows
[242.24s -> 247.04s]  you interact with a single robot, even they could let robot and robot collaborate together
[247.68s -> 256.24s]  through the power for the foundation models. So all those agents, some of them in production,
[256.24s -> 259.76s]  some of them is in research, some of them is a demo stage.
[262.08s -> 268.32s]  And I believe we are going to have more and more this type of applied use cases of
[269.28s -> 273.20s]  multimodal agents. So the question is,
[275.36s -> 281.84s]  how are we going to develop them? And what are the challenges for developing those multimodal
[281.84s -> 288.32s]  agents? And what is the important component? What do we should focus on? So this is today's
[288.32s -> 296.40s]  agenda. I will talk about the multimodal agents from these three areas. First is development
[296.40s -> 304.48s]  benchmark. We need an interactive environment. This environment can be more reconfigurable,
[304.48s -> 310.80s]  explainable, not just static benchmarks for multimodal agents. We need data.
[312.72s -> 316.80s]  How we get data to train our multimodal agent.
[319.20s -> 325.60s]  The third one is what model we should build on. We interact with computers every day.
[327.12s -> 336.16s]  I would use that to perform a lot of different tasks, including like web browsing, editing some
[336.16s -> 345.04s]  personal videos, managing our files, doing some analysis, or doing coding to implement some
[345.04s -> 353.60s]  software. And for those tasks, they are not just, for those tasks, it's not just
[353.84s -> 359.68s]  not just interacting with a single app. It is a workflow that will involve multiple applications
[360.72s -> 366.32s]  through GUI or CLI. And with the multimodal agents,
[368.08s -> 377.36s]  it could have a big potential that can help us to revolutionize this interaction between
[378.16s -> 382.96s]  this computer environment. The agents can follow our high-level
[383.60s -> 389.52s]  natural language instruction and automatically do those interactions.
[392.56s -> 398.72s]  For example, in this image, in this slide, there's a task called
[398.96s -> 406.96s]  an update bookkeeping sheet with my recent transactions over the past few days in the
[406.96s -> 414.24s]  provided folder. It is a very important task. For this type of task, I think everyone would
[414.24s -> 424.16s]  like to have an agent to help them process those tasks. However, to build those agents,
[424.72s -> 432.64s]  you need, how can you trust those agents? We need to have a benchmark. So currently,
[432.64s -> 442.16s]  there is a major challenge in developing those multimodal agents that we don't have a benchmark
[443.04s -> 453.04s]  that depends on the real interactive environment, that this environment can cover the more
[453.68s -> 463.20s]  diverse and complex real-world computer use scenarios and also can support different operating
[463.20s -> 471.68s]  systems, interfaces, apps. So with this type of benchmark, after the agent passes that benchmark,
[471.68s -> 476.24s]  we can trust this agent for our daily use.
[476.24s -> 488.24s]  So currently, they already have some good benchmarks, but they do not have
[488.24s -> 495.36s]  these properties I just mentioned, like the mental web. The mental web is a real good
[495.36s -> 503.36s]  benchmark. They include a lot of good kinds of real testing. However, for each,
[503.92s -> 513.52s]  the test in the mental web, it has only single demonstrations without the executable environment.
[515.36s -> 524.48s]  So the function from the mental web is, for each task, there is only a single solution,
[524.48s -> 531.20s]  which is not true 100%. And with that benchmark, it would
[537.92s -> 543.68s]  paralyze some alternative to correct solution. The Web Arena is another good benchmark.
[544.24s -> 552.72s]  It is an executable development. However, it simplifies the observations and action space
[553.28s -> 559.28s]  that it only covers very limited tasks and applications or domains,
[559.92s -> 566.24s]  like it only covers the web navigation on a few websites in the domain.
[567.28s -> 576.16s]  So in that one, it restricts the testing use cases for the agents.
[576.16s -> 587.68s]  And also, it cannot be used to evaluate some open-man tasks in a complex real-world
[587.68s -> 596.80s]  scenario. That may be required, some tasks require the navigating between
[597.76s -> 601.04s]  multiple different applications and interfaces in the open domain.
[601.68s -> 613.52s]  To address this gap between the development, the real-world use case and the model development,
[618.08s -> 622.16s]  the simulation environment. So we introduce the OS world.
[622.16s -> 628.64s]  This is the first scalable and the real computer environment
[629.60s -> 634.96s]  for development and the benchmarking of multi-modal agents.
[637.92s -> 641.92s]  Why it is, as we say, it is a real computer environment,
[643.92s -> 650.40s]  and it could be used for both development and the benchmarking of the multi-modal
[650.40s -> 660.00s]  agents. Because the OS world, this development allows the free from the law, free from the
[660.00s -> 669.84s]  keyboard, mouse control of the real computer applications. And if not the fixed evaluation
[669.84s -> 684.24s]  set, you could to config your task and execute those tasks through this interactive way.
[684.88s -> 693.28s]  So this one, then you can use the OS world to evaluate some open-ended computer tasks
[694.24s -> 701.68s]  and with the arbitrary applications, so it's not fixed. Maybe you can do ranging from
[701.68s -> 708.00s]  different use case like a software development use case or image editing use case.
[708.56s -> 716.80s]  The OS world can serve as a unified, a real world, real computer developments that
[717.36s -> 724.24s]  allow the users, so in this way, they can allow users to define the agent task,
[725.68s -> 736.08s]  define a new agent task without to build this new domain-specific simulation environment.
[736.96s -> 745.76s]  So it's as an environment, the OS world is more flexible and gives the users more control
[745.84s -> 757.60s]  for the development design for their scenarios they care. So let's see how we're going to use
[757.60s -> 769.04s]  the OS world to create the different tasks. The first, for each task, you have a natural
[769.04s -> 779.12s]  language task instruction for the task. So you see here you have a task, you want to create this
[781.52s -> 787.12s]  testing environment, like updating, this one is I just mentioned this task, updating this
[788.00s -> 796.40s]  bookkeeping sheet. And with this instruction, in the OS world they provide the initial state
[796.40s -> 804.16s]  setup for this instruction. So the setup is, you just need to create this file.
[806.24s -> 813.12s]  In this file, they include multiple components. The red part is you need to
[813.68s -> 822.16s]  config the initial tasks, like this excel files means the task.
[824.40s -> 835.28s]  And also you need to add what is your post processing and those configurations.
[835.28s -> 845.28s]  And also where you can retrieve the files and the information, the yellow part, and the last
[845.28s -> 854.96s]  one is how you're going to evaluate after you finish your task in the environment. After you
[854.96s -> 865.12s]  configure your initial state, the setup, the OS world will initialize this
[867.36s -> 874.16s]  environment with a virtual machine. And in the virtual machine, they will initialize,
[874.16s -> 880.56s]  so the initialization will start to run in this virtual machine and you get the initial states
[881.52s -> 888.32s]  of the test environment. It's very convenient. So for example,
[892.00s -> 900.64s]  at this moment, you could use this virtual machine to restore the task again and again,
[900.64s -> 909.04s]  just to reuse this virtual machine. And because of this virtual machine,
[909.60s -> 913.20s]  it's a safe, isolated environment. You don't need to worry about
[914.88s -> 924.48s]  some damage, irreversible damages when you're testing. So when the task
[925.20s -> 934.00s]  environment gets set up, then you can start to run your agent to interact with this environment.
[934.00s -> 942.88s]  For example, you can use TPT4WAY to create an agent and interact with this environment,
[944.16s -> 947.68s]  or you can use any other open source model to build an agent.
[949.12s -> 957.04s]  The agent will receive or observe different information that can be
[957.60s -> 962.24s]  provided by the environment. First, of course, is the user query,
[962.24s -> 971.84s]  the natural language instruction from the user. And also, it can receive the observations
[971.84s -> 977.12s]  from the environment, such as the screenshot, the accessibility tree,
[979.04s -> 982.00s]  and some customized streams, maybe the terminal outputs.
[984.48s -> 991.84s]  So here is an example. The left side is called the setup mask, which is a screenshot
[991.84s -> 1000.16s]  plus the information you expect from the accessibility tree that you can
[1000.96s -> 1011.52s]  add for those, the label in the image, so they can provide more information on the screenshot.
[1012.48s -> 1019.68s]  And also, the right side is the DOM tree. It's an accessibility tree, but the left side
[1019.68s -> 1025.84s]  is the DOM tree. So after the agent receives those observations, of course,
[1026.40s -> 1030.56s]  it's not like every agent needs all the observations. It depends on your agent.
[1030.56s -> 1039.04s]  If you are a vision-based agent, maybe you only use the screenshot or SOM. If it's
[1039.04s -> 1044.96s]  a text-based agent, maybe you only use an accessibility tree. It depends on what agent
[1045.92s -> 1048.64s]  you're going to build to interact with the environment.
[1050.32s -> 1057.92s]  So after you receive those observations, the agent will, it depends on how you create the
[1057.92s -> 1064.32s]  agent, but the agent finally will generate some equitable actions, like you click some
[1064.32s -> 1075.68s]  position or you will check some content, check some box, and et cetera, to manipulate the keyboard
[1076.56s -> 1089.12s]  and the mouse. And then each of those actions will be represented using a co-string
[1090.24s -> 1100.08s]  generated by the agent. And then the evaluator will execute those co-strings in the virtual
[1100.08s -> 1110.88s]  machine, and you will get the new observations now. Also, we are using the Pi auto GUI3 to
[1112.00s -> 1117.92s]  control the mouse and the keyboard. This is where we used the library.
[1118.48s -> 1130.00s]  So this interaction loop will continue based on your instruction. It will repeat multiple steps
[1130.48s -> 1140.64s]  until it terminates. Maybe it's done or failed, or it's reached the maximum number of steps
[1140.64s -> 1155.92s]  you set. Our experiment with that is 15 and 15. So now you can set up the task until you
[1156.00s -> 1162.48s]  learn, you run your task, and then after the task you stop, you get a result, then you will
[1162.48s -> 1169.12s]  do evaluation on your task in this environment, in the OS world. The OS world is different from
[1169.12s -> 1176.16s]  other benchmarks. We implement this execution-based reward function.
[1176.24s -> 1188.40s]  So when the final step reaches the goal, of course, they'll get the positive reward,
[1189.76s -> 1196.08s]  or even if it's partially achieved, they'll get some number between zero and one. If not,
[1196.08s -> 1202.80s]  it's totally failed, it will get zero. To get this reward, we are not just using
[1203.28s -> 1218.16s]  the single evaluation metric. So here, to evaluate the successful execution of those tasks,
[1220.96s -> 1231.12s]  we think there is no single way to evaluate the successful or failed task.
[1231.84s -> 1239.60s]  So we make this evaluation more open. What is open? Open means that you can
[1239.60s -> 1245.36s]  construct the evaluation script by yourself in OS world development.
[1246.96s -> 1256.56s]  Because some tasks you need to retrieval, some tasks you need to match to evaluate your task.
[1257.04s -> 1267.28s]  So we use this example-wise evaluation to design the evaluation metric for a task.
[1267.28s -> 1272.64s]  So not just a fixed one, so it's an open one, and you can base on different tasks
[1273.44s -> 1283.12s]  to design your different evaluation metrics. So that you can facilitate the diversity of the task.
[1284.00s -> 1292.08s]  And it could make the evaluation more reliable, especially for these complex, real-world,
[1292.08s -> 1302.08s]  open-ended tasks. So we created this powerful OS world development so that the people can use
[1302.08s -> 1310.48s]  this development to benchmark their multimodal agents. Besides, we build this development,
[1311.04s -> 1317.52s]  besides we build this development, of course, we also build a benchmark on top of the OS world.
[1319.12s -> 1335.60s]  Here we introduce the 369 real-world computer tasks that involve widely used web or desktop
[1335.60s -> 1346.40s]  apps like Office, OS, daily use, workflow, and professionals.
[1348.56s -> 1356.56s]  All of them, each task, we did a lot of research on the internet.
[1357.12s -> 1361.20s]  So all of them is based on the real-world computer use cases, the experience.
[1361.20s -> 1373.60s]  And then we ask the people who had a fierce background to interact with the
[1375.36s -> 1378.80s]  initial state, the initialized development in the OS world,
[1382.24s -> 1389.04s]  and interact with the environment and record those trajectories.
[1389.92s -> 1400.80s]  And also handcrafted a lot of execution evaluation scripts,
[1400.80s -> 1407.60s]  execution-based evaluation scripts to verify the task completion. So in the left side,
[1407.60s -> 1413.84s]  you can see the distribution of the different tasks. So total they have 369
[1414.64s -> 1420.32s]  and 100 is a workflow task, which is a multi-app, means this task need to
[1423.52s -> 1430.48s]  jump across different apps, multiple apps. And then they have the 268, which is roughly
[1430.48s -> 1440.16s]  70 percent is a single app. And in those tasks, some of them, which is roughly
[1440.24s -> 1445.52s]  20 percent is through the other data set like Lab Arena.
[1447.76s -> 1454.16s]  And there's a special here is we also add the 30 tasks, which is infeasible because
[1455.20s -> 1462.80s]  you need to also know how to say no to the user.
[1462.80s -> 1470.08s]  So we think adding the infeasible task is important to make it more comprehensive.
[1474.24s -> 1480.56s]  And we also have a small number of 43 tasks for Windows, not just on Ubuntu.
[1482.24s -> 1490.96s]  And as I said, because we're thinking this example based the evaluation script, I mean,
[1491.84s -> 1503.76s]  it's very important for the real world evaluation on multi-modal agents. We construct the 134
[1504.64s -> 1514.00s]  evaluation script on 369 example, the sample. So you can see the ratio is very big.
[1514.80s -> 1522.80s]  So here is some examples, samples with samples from our 300
[1525.36s -> 1534.80s]  the facts of our benchmark. So you can see their car was very diverse, the apps and the
[1535.52s -> 1542.72s]  cost different the user intent. So we compared our benchmark with the
[1543.52s -> 1553.60s]  current existing popular benchmarks from the different perspective, such as
[1555.12s -> 1563.28s]  whether it provides the controllable executable environment or whether it's easy to scale to the
[1563.28s -> 1571.52s]  new environment, or you can support a multi-modal or whether it could be to the cross app task,
[1571.52s -> 1580.24s]  cross app task or whether it could start up from the intermediate, the interest data and so on.
[1580.88s -> 1591.36s]  So we can see our benchmark is to have more powerful, more, I mean, the cover
[1591.36s -> 1599.84s]  define the scenarios, the request as a benchmark for the multi-modal agents.
[1601.68s -> 1607.92s]  And then on the right side, we also did some analysis, how complicated the task
[1608.88s -> 1617.04s]  in the OS Word benchmark. We did some comparison with the Web Arena. So the green, sorry,
[1617.92s -> 1622.24s]  the blue is Web Arena and the yellow is OS Word.
[1626.40s -> 1632.80s]  The median completion time for Web Arena is roughly 30, 40 seconds.
[1633.68s -> 1643.84s]  And for the OS Word, the median completion time is roughly 100, 100, maybe 110
[1643.84s -> 1653.12s]  seconds. And they also have some significant, the number of demos maybe need even more,
[1654.16s -> 1661.12s]  you will need 900 seconds or even more. So the complexity of this benchmark is also
[1661.12s -> 1666.96s]  higher than the existing benchmarks. After we create the benchmark, we test
[1667.76s -> 1677.52s]  the benchmark with the state of art, the LM or VLM from the open source and also cross source
[1679.12s -> 1685.52s]  the foundation models like GPT-4, Gemini, Cloud and et cetera.
[1688.00s -> 1694.32s]  And the left side is the prompts we used. We designed the four different testing the settings
[1694.32s -> 1704.56s]  through based on different observation. They are, the one is only half accessibility tree,
[1704.56s -> 1714.08s]  which could be the LM can directly to write. And the second one is only screenshot,
[1714.08s -> 1719.04s]  which is pure vision. And the third one is a screenshot plus accessibility tree.
[1719.76s -> 1730.48s]  And the fourth one is set of marks. We did some simplification on the accessibility tree,
[1730.48s -> 1738.56s]  because usually accessibility tree is very long and include a lot of redundant information.
[1738.56s -> 1748.24s]  So we did some simplification to reduce them and to shock the length of this text representation.
[1749.04s -> 1761.04s]  So this is the result, the numbers we get. Let's do some analysis on this table.
[1762.56s -> 1768.16s]  So first, at the bottom of the table is the human performance,
[1768.16s -> 1775.60s]  roughly 70 to 75 percent, percent rate.
[1777.20s -> 1788.56s]  And if you go to see the performance of those, the state of art open source and
[1789.92s -> 1797.76s]  frontier cross source model, it's still far behind, far from the real human performance.
[1798.24s -> 1804.00s]  On those daily use task, it's not like a super hard task, it's a daily use task.
[1806.48s -> 1815.84s]  And second, you see like from different scenarios, human performance is kind of stable.
[1816.80s -> 1827.92s]  It's past minus three to five percent. But for those frontier models,
[1829.44s -> 1834.40s]  large foundation models, the performance changed dramatically.
[1835.20s -> 1841.52s]  And even the model from same company, like you see, for example,
[1841.68s -> 1852.32s]  in the set of mark, the GPT-4V and GPT-4O, the performance gap is huge, right?
[1852.32s -> 1860.24s]  It's 10 percent gap, even the performance, since the performance is only just 20 percent,
[1860.24s -> 1867.76s]  10 percent gap. And of course, so this is table, the result is we get last year,
[1867.84s -> 1876.16s]  and this year they have new models from the co, from the, from those
[1877.28s -> 1882.88s]  vendors, the OpenAI and the Trockik. They have some bigger improvements on there,
[1882.88s -> 1888.00s]  but still it's far away from the human performance. We'll see.
[1889.52s -> 1896.64s]  We did some analysis on what factors influence the performance.
[1897.20s -> 1903.76s]  The first, we did some analysis on the screenshot resolution. So we found,
[1906.80s -> 1916.64s]  so first we, so one point, so in the y-axis is a successful rate and the x-axis is a
[1916.72s -> 1926.24s]  downsampling rate. So we, so we, because of VRM, most of the VRM is still trying a very small
[1926.24s -> 1931.52s]  resolution, not very small, but it's, comparing this now we have better resolution, but usually
[1931.52s -> 1937.36s]  the VRM not trying at the full resolution, it's trying a smaller one. So we do some
[1937.36s -> 1944.96s]  downsampling on the resolution. And then we calculate, then we run an evaluation.
[1945.76s -> 1952.88s]  In a GPT four-way screenshot, as you improve the screenshot, it'll get a better performance.
[1953.84s -> 1961.52s]  But in SOM, it's, I think the trend is still improved, but this is a noise here,
[1962.16s -> 1968.88s]  but still improved, but not as much as, not as clear as the screenshot. I think it's got SOM
[1968.88s -> 1976.40s]  include, also include some, they have some extra information it gets from the accessibility tree
[1976.40s -> 1982.64s]  based on, so they have some text information inside, so it decrease the impact of the
[1982.64s -> 1993.68s]  resolution. And then we also doing some analysis on the, how much the historic context will
[1994.56s -> 2008.00s]  influence the final performance. So we did this, we choose different number of the trajectory
[2008.00s -> 2021.20s]  length. We found in the GPT-4-way SOM, it improved as you add more and more the historical
[2021.68s -> 2031.28s]  the information, the steps. But in the three-shot one, it doesn't even make it, it's even worse.
[2033.20s -> 2040.72s]  We think it's mainly it's because the SOM have, like I said, it include those,
[2040.72s -> 2047.84s]  the information from the text representation accessibility tree. And our M is perform,
[2048.80s -> 2055.84s]  it's the current foundation model is perform better on the text rather than the pure original
[2055.84s -> 2060.88s]  image. So I think, so we think that is the reason it getting better and better with
[2060.88s -> 2066.64s]  more and more context. But for the screenshot, it's getting worse. I think it's still is
[2066.64s -> 2076.32s]  because of the current foundation model, it's not be well-trained for the computer use
[2076.56s -> 2090.08s]  scenarios. So it means we need to, if they have a gap, like the open source
[2090.08s -> 2096.56s]  community or closed source companies need to improve those vision capabilities in the
[2096.56s -> 2103.36s]  foundation model if we want to get a better multimodal agent for computer use. And then
[2103.36s -> 2109.84s]  in the right side, it's a length distribution of the accessibility tree. So although you can
[2109.84s -> 2116.24s]  add more and more those text representation in the multimodal agent, and it can get better
[2116.24s -> 2123.12s]  performance, but they have an issue, the accessibility tree is too long.
[2123.92s -> 2133.12s]  So roughly is that 90% percentile, it's the 6k. So if we want to add more,
[2134.48s -> 2143.84s]  the history, the context, then we need to find a way to get a more simplified,
[2143.84s -> 2149.52s]  obviously more concise accessibility tree. So maybe they have some work on how to select
[2150.08s -> 2155.60s]  the relevant information in the accessibility tree before you fit into the system,
[2155.60s -> 2160.96s]  but when you do this step, maybe your system will be slow because you need to do this
[2160.96s -> 2166.72s]  information, maybe you need to train another big model to do this information processing
[2167.68s -> 2172.72s]  on, you know, that added the extra latency there.
[2183.52s -> 2188.00s]  And we also analyzed the model performance on the Ubuntu and Windows.
[2188.72s -> 2192.32s]  We found that their correlation is pretty high, it's 0.7,
[2193.12s -> 2202.24s]  and so which means like we could use the OS Word framework, it would be effective,
[2203.28s -> 2211.60s]  you could to get, if you can type the model in code in one, in one scenario it should be,
[2213.04s -> 2216.24s]  it should be effectively transferred to another environment.
[2216.32s -> 2225.92s]  And then we also did some operation, like if when we change the position,
[2225.92s -> 2235.52s]  size of the windows in the different app windows in the screen, the results
[2235.52s -> 2241.92s]  also show very different. So which means, it still means that the model is not,
[2242.32s -> 2251.36s]  the foundation model is not very robust for this type of disturbance.
[2252.08s -> 2259.68s]  So yeah, so through this we can find that there is a lot of work we should, we need to improve.
[2263.44s -> 2269.68s]  And the good news is recently they have a lot of the new progress in these areas.
[2270.64s -> 2279.12s]  Anthropic, they have this new cloud 3.5 and also 3.7, and we can see the very
[2279.12s -> 2291.20s]  significant improvement from here, 14, 15 percent to 22 percent, but still far away from a human,
[2291.20s -> 2299.44s]  72 percent. Of course, if you increase the number of steps from 15 to 50, it's getting better,
[2299.44s -> 2306.88s]  but they have a bigger. And OpenAI also have this new model called operator, the CUD.
[2311.68s -> 2318.80s]  And in the recent performance, CUD, sorry, in the recent performance, we can see
[2319.52s -> 2324.40s]  OpenAI is significantly better than the cloud 3.5.
[2328.08s -> 2336.96s]  And at the 200 steps, they can get even 38 percent, comparing which like maybe only have
[2336.96s -> 2347.76s]  15 percent four months ago. So it's improved super fast. But this is like you have 200 steps,
[2347.76s -> 2357.12s]  but most of task in the OS world is roughly, it should be, most of it can be solved under the
[2357.12s -> 2365.92s]  15 or even 20 steps. So which means they still have a big improvement. They have big space to
[2365.92s -> 2374.08s]  improve the multi-modal agent for this computer use. Okay, so this is through the
[2374.08s -> 2381.68s]  environment and the benchmark discussion. You know, we have made a big, a lot of
[2382.56s -> 2389.68s]  great progress on this computer use areas for multi-modal agent, but still they have
[2389.68s -> 2396.64s]  a lot of the stuff we need to improve. They still have a lot of opportunities there.
[2397.60s -> 2404.80s]  Now after this, let's go to the data part, since we still need the data to train our model.
[2406.72s -> 2412.88s]  For the agent training, I think we have a bigger challenge than we do in the LM training.
[2415.68s -> 2424.80s]  Why? Because the agent model is not a single step, it's a multi-step, it's a trajectory.
[2425.76s -> 2434.08s]  Collecting the trajectory data is super expensive. And the second is, it's very hard to directly
[2435.52s -> 2444.32s]  to get the trajectory data by the internet. And second, even if you say you have enough money,
[2444.32s -> 2451.68s]  I can just hire people to collect data. But to do those annotations, to collect the trajectory
[2451.68s -> 2460.88s]  data, it's very time consuming, and it limits the scalability. Not good for the scaling law.
[2462.00s -> 2468.48s]  The cost and the time consuming of the data collection through human, and also there's no
[2469.04s -> 2473.76s]  free large scale trajectory, how fast you can crawl, then what should we do?
[2473.84s -> 2482.16s]  Okay, why don't we let the model to synthesize? Here I want to discuss this paper called
[2482.16s -> 2492.72s]  Agent Trick, which is to synthesize the agent trajectories through this guiding play
[2492.72s -> 2500.96s]  way, a strategy with a web tutorial. So what does it mean? First, we know
[2501.60s -> 2510.48s]  they have a lot of the large collection of the tutorial-like text in the internet.
[2511.68s -> 2520.16s]  And those texts, they provide the step-by-step guidance on how to perform different tasks.
[2520.72s -> 2526.88s]  So we read those tutorials to learn how to interact with different apps and complete
[2526.96s -> 2539.60s]  different tasks in our GUI developments. And then those tutorials, they cover different
[2540.72s -> 2553.28s]  scenarios. So we could systematically filter those instruction contents from the internet.
[2553.28s -> 2560.00s]  And of course, those data, it's not that clean because it's written for humans. So
[2560.00s -> 2566.80s]  they have a lot of noise, text, descriptions, and of course, HTML formats there.
[2570.56s -> 2582.80s]  So we need to extract the valuable, the knowledges, the steps from those tutorial-like
[2582.80s -> 2595.52s]  text. And then transform those unstructured text into a structured agent trajectories
[2596.80s -> 2609.84s]  with the definition or high-level description of the task that make it to allow the agent to use,
[2610.48s -> 2620.56s]  to try and mimic the human task completion. So in the agent tree, there are three stages
[2620.56s -> 2628.64s]  to do this. The first stage is called automatic tutorial collection from the internet.
[2630.32s -> 2637.92s]  So in this step, we just graphically extract these tracks and filter those tutorial data from
[2637.92s -> 2646.40s]  the internet using a bunch of heuristic, the filtering, and some classifier like
[2646.40s -> 2659.28s]  with the fast text, and then transform it into some structured format with using LAM.
[2659.68s -> 2671.84s]  And the step two is the guided replay for those trajectory generation, which is using
[2672.80s -> 2680.24s]  VLM, vision language model agent, to execute those tutorials based on the structure tutorial,
[2680.24s -> 2687.20s]  the structure step. And the one is step by step. And through that, we were to collect the
[2687.20s -> 2692.24s]  high-level trajectories, including the high-level description of the task,
[2693.12s -> 2699.60s]  the observation of each step, and the reasoning of each step and the final actions.
[2701.36s -> 2712.32s]  And then given those data, we will run a separate evaluator to evaluate,
[2712.96s -> 2720.32s]  to assess the quality of the trajectory to ensure, to filter out some bad trajectories
[2720.96s -> 2727.52s]  or low-quality ones to maintain, to make sure it's, to ensure the effectiveness of
[2727.52s -> 2737.20s]  the data for the training. And the step three is we need to verify those agents by training
[2737.20s -> 2749.20s]  a model and fine-tune the agent models to demonstrate the improvements, the significant
[2749.20s -> 2756.08s]  performance improvement across different benchmarks. So that is roughly the steps for three stages
[2756.08s -> 2761.52s]  for the agent trick. So like I described these three steps, let's go a little bit deeper about
[2761.52s -> 2769.20s]  each step. The first step is a tutorial collection from the internet. So we start from some
[2770.08s -> 2783.20s]  the big, large corpus, like a write-by-drama, DCLM. Then we did the, using maybe some
[2783.20s -> 2788.72s]  filtering with, based on keywords or some structured features.
[2791.12s -> 2797.92s]  And then we did, after this step, we filter maybe 99% of data.
[2800.16s -> 2809.36s]  And then for the remaining data, we extract the subset, a subset of the remaining data,
[2809.44s -> 2817.28s]  the pre, the filter data, and use the LM to did some annotation.
[2818.24s -> 2827.04s]  It is a good, it is a good data, is a bad data for, for, for the, as a tutorial.
[2828.72s -> 2837.52s]  And then after this annotation, we turn the fast text, fast file, and use that one to
[2837.52s -> 2848.24s]  classify all the whole, the filter data. And then when you get this tutorial data,
[2848.24s -> 2856.72s]  we run another one of LM to paraphrase those data to follow some predefined format
[2858.56s -> 2865.60s]  that used for the guided, for guided replay for the stage two.
[2865.60s -> 2871.92s]  So this is the, this is the data flow, the flow, the follow step one where you do the
[2871.92s -> 2878.80s]  pre-filtering. So you can see we use some, the, the realistic filters
[2880.48s -> 2886.32s]  to do the first step, the data filtering. And you can see, like,
[2888.32s -> 2894.32s]  just after this one, two, three, the six step, you could, you could filter out more than
[2894.32s -> 2902.24s]  99.9 percent data. And after you get this filter data, we, as we, we collect some data,
[2902.24s -> 2910.08s]  we collect a small, small number of the data, and use the prompt LM to
[2910.96s -> 2916.16s]  classify if it's a GUI tutorial or not. But this is the prompt we use.
[2917.12s -> 2925.20s]  After we do that, we, we also have the human to do some, to sample some of them to verify.
[2925.20s -> 2930.96s]  We find that f1 score is pretty high, it's like eight, they can get about 88, 89 percent.
[2931.76s -> 2945.76s]  And we find for some long text, very long text, the documents, the LM is performed better than
[2946.88s -> 2955.04s]  human when they embed the tutorial in a long context, which is normal, which is,
[2956.88s -> 2963.92s]  which is not surprising. But, so initially we watch after the LM on the long context
[2963.92s -> 2970.24s]  classification. Then after we filter those tutorial data, then we formalize
[2970.24s -> 2980.72s]  the tutorial data into a specific structured format that we want to use to, for the model,
[2981.28s -> 2986.64s]  for the model, for the replay, the guided replay.
[2989.04s -> 2997.36s]  Here we organize, so the last slide is the prompt we used. The goal, so we want to,
[2998.08s -> 3007.92s]  through this prompt, we want to convert the tutorial into these six, into five parts.
[3008.56s -> 3014.88s]  First is the platform and the target development,
[3014.88s -> 3019.76s]  that is specified by what is operating system, what software versions,
[3020.64s -> 3028.40s]  so that would be used for the initial state. And second one is the task description,
[3028.96s -> 3036.40s]  which provides concise problem statements, which is the instruction.
[3037.52s -> 3043.36s]  The third one is the prerequisites, which has the dependencies, background knowledges,
[3044.08s -> 3049.36s]  and the fourth one is this step-by-step instructions,
[3050.56s -> 3060.48s]  which include this guidance and command syntax. And the last one is the expected outcome,
[3060.48s -> 3066.88s]  which is used for the verification or success indicators.
[3067.68s -> 3076.80s]  So after we get this, after we paraphrase the tutorial, then we get this structured tutorial,
[3076.80s -> 3086.80s]  and then we create a VRM agent and fit into this step-by-step tutorial, and the agent
[3086.80s -> 3096.32s]  were to start to run the interact with the development step-by-step
[3098.08s -> 3100.64s]  through this observation action loop.
[3104.72s -> 3112.56s]  And through this, after the whole execution, we will record all the observation action,
[3112.56s -> 3118.00s]  in the median reasoning, the whole trajectories,
[3120.32s -> 3128.00s]  and then those will fit into another VRM evaluator to assess the quality or
[3128.00s -> 3134.96s]  correctness of the trajectory and ensure we got high-quality data.
[3135.84s -> 3146.08s]  So here is an example of the finalized data, the final version of the data,
[3146.08s -> 3148.40s]  after you're doing all those steps.
[3152.40s -> 3159.68s]  So you could see the left side is the observation sequence, and the right side
[3159.68s -> 3173.84s]  including the action, the thought for each step, and the actions, the action code for each step.
[3175.12s -> 3184.40s]  So comparing the data we collect using the AgentChick with the prior works,
[3185.20s -> 3190.96s]  the prior loads, the other data set people use to train model.
[3192.00s -> 3198.48s]  First, all the data could be, through AgentChick you can collect much more data.
[3200.00s -> 3206.96s]  Here we show this 10k, 10k is not the maximum. We just do some of that.
[3208.00s -> 3209.12s]  It could be much bigger.
[3209.12s -> 3221.12s]  And the second, the complexity of the data is not low. Actually, it's pretty good.
[3222.32s -> 3223.76s]  Every step is a tough step.
[3226.16s -> 3232.80s]  And you also will have a lot of the information.
[3233.76s -> 3240.00s]  You will have HTML, you will have accessibility, you also will have reasoning,
[3240.00s -> 3248.88s]  you could get videos, you can get screenshots, and you can cover different websites.
[3252.16s -> 3259.52s]  So AgentChick, it maintains the diversity and the realism because it's from the real world.
[3260.48s -> 3267.68s]  And it's also adjusting the scalability issues that the human annotated data may have
[3269.04s -> 3279.12s]  given the budget. So here is the distribution of the trajectories from the,
[3280.96s -> 3286.96s]  so this is the distribution, the left side is the distribution, the data,
[3287.28s -> 3294.64s]  we, the different number of trajectories from different websites. And the right side is
[3296.48s -> 3299.12s]  from the high level, different vertical, different
[3300.00s -> 3309.84s]  domain, what's the distribution in the data. So you can see the how,
[3309.84s -> 3320.16s]  you can see how the coverage of the dataset, the domains in the AgentChick,
[3322.56s -> 3330.40s]  I mean, the dataset created by the AgentChick. So we did some, like we said, we fine-tuned
[3330.40s -> 3343.12s]  model to track the performance, to see how good this synthesized data can train with other
[3344.72s -> 3353.52s]  human annotated data in the benchmarks to train the agent in the benchmarks.
[3353.52s -> 3358.16s]  So we did it on this comparison on a web arena dataset, on the benchmark.
[3360.80s -> 3365.76s]  And the models trained on AgentChick data significantly out of
[3367.84s -> 3374.88s]  both open source and even closed source typically for all models, which demonstrate
[3376.72s -> 3384.00s]  just synthesized trajectories also, it's also high quality and it's also used to train
[3384.00s -> 3393.28s]  the better model. This is about how we do it using AgentChick to do the data synthesis for the
[3394.00s -> 3398.64s]  GUI agent. I think the takeaway from this AgentChick work is
[3401.28s -> 3409.60s]  leveraging the internet corpus is still an important way because they have diverse task
[3409.60s -> 3413.12s]  resources. And second is,
[3420.16s -> 3431.60s]  second is the reasoning and the reflection. The reasoning and the reflection is very important,
[3433.36s -> 3439.20s]  the factor for the trajectory, which we will talk about in the model side later.
[3440.56s -> 3448.16s]  Yeah, but however, you still have an issue here is because you, for each instruction,
[3448.72s -> 3458.32s]  you still only get a single trajectory, which is an issue that, not like you only have
[3458.32s -> 3465.12s]  a single solution for each, for the query. It could have multiple instructions.
[3465.12s -> 3472.96s]  So you still have this issue here because it's only an imitation learning process.
[3474.80s -> 3481.76s]  But I think we have a solution for it. It always works. So you can use AgentChick
[3483.84s -> 3488.72s]  to create a data for FFT training. And then you can use that high level instruction
[3489.36s -> 3496.64s]  and write in the OS-1 for the second stage, the IO-chain. And then you can solve this
[3496.64s -> 3505.76s]  exploitation exploration process. Great. So we try to answer the question that we can
[3505.76s -> 3514.32s]  use the data synthesis to make the existing real vision language model perform better on the
[3514.88s -> 3527.76s]  the GUI agent tasks. Then, so the second, the next I want to discuss is when you do data
[3527.76s -> 3535.84s]  synthesis for, to improve this action capability, action code capability for agent for external
[3535.84s -> 3547.20s]  tasks. How about it could also help for your internal, the task is still like understanding.
[3548.16s -> 3553.76s]  We know that the vision language model is trained for the understanding.
[3556.00s -> 3562.00s]  And if we want to do it, can we call from web data, website, if the website has this type
[3562.00s -> 3569.84s]  of data? I think the answer is not really. Then how can we synthesize it? So first,
[3569.84s -> 3575.60s]  let me see like why we need to do it. Why we want to synthesize this type of action data
[3575.60s -> 3587.52s]  for multimodal LM to understand. Understanding should be the capability or skills it should have.
[3587.92s -> 3595.20s]  But actually, it's not perfect. Or it's far from perfect on the open source model.
[3596.00s -> 3599.36s]  So here is the four examples from the four different
[3600.80s -> 3609.28s]  perspectives to explain why we need the model to itself can do the action calling to improve
[3610.24s -> 3617.84s]  understanding. All those tasks either they need more fine-grained vision capability,
[3617.84s -> 3622.08s]  or they need some reasoning capabilities, or they need some external knowledge.
[3622.88s -> 3632.88s]  So here we introduce TACU, which is synthesized with chain of thought and action data to
[3633.84s -> 3641.52s]  improve the multimodal understanding capability through adding its action calling capabilities
[3642.40s -> 3648.08s]  skills in a model. So this is some examples we showed.
[3651.04s -> 3657.84s]  After adding this action calling capability, which is trained through the
[3657.84s -> 3665.92s]  synthetic data we will discuss later, it can generate the thoughts and the actions
[3666.64s -> 3670.48s]  and through the observations, they will do it again until they get the results.
[3671.36s -> 3678.56s]  So this is the one example we just showed is failed in this lava model.
[3678.56s -> 3688.96s]  And this another one is called the localization functions, visual functions.
[3690.00s -> 3696.24s]  And this is to query the external knowledge, the text knowledge, so not just the vision model
[3696.24s -> 3707.12s]  only. And this makes the vision OCR and some that can calculate the capabilities.
[3707.84s -> 3715.20s]  Okay, so let's go back to how we created the synthetic pipeline
[3716.72s -> 3724.40s]  for the synthesized, to generate this data for the COT chain.
[3725.12s -> 3729.52s]  There is a lot of details here, but don't worry, that explains the number one.
[3730.64s -> 3731.84s]  It's not that complicated.
[3731.84s -> 3738.56s]  The first for this data synthesis pipeline, they have two parts.
[3739.20s -> 3743.12s]  First part is called the model-based generation, and the bottom part is the
[3744.80s -> 3756.56s]  programmatic generation. So in the model-based generation, first we take the existing image QA,
[3757.28s -> 3765.60s]  and we get those image and QA pairs as input, and we prompt them from the model,
[3765.60s -> 3772.88s]  the frontier models. I say that you have this image and the QA pair that generate
[3773.60s -> 3782.48s]  the jump thought and action, given those the action or called action list.
[3782.48s -> 3794.64s]  Or jump thought, if they do not have the relevant action in the list.
[3797.36s -> 3805.12s]  And then it will generate, after it generates either the jump thought with action or jump
[3805.12s -> 3812.88s]  thought without action, then we will verify the outcome, the output.
[3814.56s -> 3816.32s]  If it's correct, then we will pass it.
[3818.64s -> 3825.60s]  If it's not, then we will convert this pair as the direct answer pair.
[3826.40s -> 3832.32s]  So in this way, you will get the data, not all of them have the action,
[3832.32s -> 3836.32s]  some of them have action, some of them only have jump thought, some of them
[3837.04s -> 3845.44s]  without this direct answer. So that is the first part, the top part.
[3846.64s -> 3851.52s]  And in the bottom part, we do the programmatic generation.
[3852.96s -> 3861.68s]  This way, we do not have the QA pairs in this programmatic generation process.
[3862.32s -> 3869.04s]  We only have the image. Then we use the model, maybe the detection model, depth model,
[3869.76s -> 3878.72s]  OCR model, and different models. And also, it could be human to label some data, some box.
[3880.48s -> 3886.56s]  And we get those dense annotations in the image, and we design manually
[3887.12s -> 3892.08s]  templates, and we use that to generate the QA pairs.
[3894.80s -> 3903.36s]  And through that one, we can synthesize what actions from the answer to question to answer.
[3903.36s -> 3911.92s]  So this is the template we use to do the programmatic generation. So some of them is
[3911.92s -> 3918.40s]  about the counting, some of them is about the 2D spatial reasoning or 3D spatial reasoning,
[3919.12s -> 3924.48s]  some of them is the mixed understanding and the counting. So we try to
[3926.40s -> 3935.44s]  mimic different reasoning or the action call needed to answer different types of
[3936.00s -> 3947.84s]  the questions. And then here, the bottom is the action sets, which cover OCR,
[3949.20s -> 3955.92s]  get objects, localization, depth estimation, CLOP, regular to-me, to-mout,
[3956.64s -> 3966.24s]  and not to-me, and query the language model, accurate, stop the math equation. It's pretty
[3967.20s -> 3975.68s]  diverse. So through these two ways, we collect the data. And of course, in this data,
[3975.68s -> 3982.80s]  we need to, some of them is the chain of thought action data, some of them is not.
[3982.80s -> 3989.52s]  Then we need to do some analysis on those data and test on the benchmarks.
[3990.80s -> 3999.52s]  So first one we, through our experiment, first one we found to get the model, get the
[4000.88s -> 4006.64s]  multimodal foundation models to have this reasoning or action call capabilities,
[4007.28s -> 4015.52s]  you need to use fine-tuning. In the results, we found a few shots or in-context learning
[4015.52s -> 4026.96s]  and doesn't work. And the second is, we found that they have comparing for the tasks
[4026.96s -> 4034.80s]  with this action, adding this action capability, it's really to improve their capabilities to
[4035.76s -> 4041.44s]  solve the task you cannot solve before. We improve their understanding capability,
[4041.44s -> 4050.72s]  which means that even understanding this capability, it also could be enhanced by
[4052.88s -> 4059.76s]  adding external tools and make it through this function call or tool calling to improve it,
[4059.76s -> 4066.80s]  which is not a surprise, but it's also interesting to see these positive results.
[4068.16s -> 4079.76s]  So it can give us a different, another direction, how could we improve our models
[4080.00s -> 4090.32s]  on understanding. And this is the second observation we found, the best of the CoT data,
[4091.20s -> 4100.72s]  so the best of the CoT data, the best one, which can almost,
[4101.36s -> 4115.84s]  to test on the different scenarios that you see, it has to only say, okay, we QA, blink,
[4116.64s -> 4125.92s]  math with time, MMU. It's almost improved all the tasks, which means that it lists,
[4126.00s -> 4134.00s]  so adding this action capability in the model, in a data set, to create this chain of thought
[4134.00s -> 4140.88s]  action data, it can improve, and with this data to train the model, in a multiple model,
[4140.88s -> 4149.04s]  it can improve the model understanding capability in general, not like a job improve one model.
[4150.00s -> 4159.68s]  And the third observation from this, the TACO experiments on the CoT, the chain of thought
[4159.68s -> 4165.52s]  action data set, we find that the quality is more important than quantity, not just add more,
[4165.52s -> 4174.24s]  you will get a better performance. Okay, so this paper is to talk about how we could to
[4175.12s -> 4178.80s]  add in the action core capability into the multi-model,
[4179.52s -> 4185.92s]  multi-model, or foundation model, and it, and first you need to fine tune the model,
[4185.92s -> 4193.04s]  not through the few shot, and second is CoT data is consistent to improve those, the models,
[4196.88s -> 4202.48s]  through this fine tuning on this instruction, this sensor size data.
[4202.48s -> 4208.00s]  And the fourth one is the quality, it's more important than quantity, which is not
[4208.88s -> 4214.32s]  new, but it's some lessons we learned from here.
[4217.12s -> 4225.28s]  So here is, we have discussed about the environment benchmarking, and the data synthesis.
[4225.28s -> 4232.00s]  I think now you already have the comprehensive view, collective view about
[4234.08s -> 4240.40s]  this multi-agent areas, if you want to test the agent, if you want to generate the data,
[4241.12s -> 4248.32s]  and then if you want to do the IO in the environment, you already have those tools,
[4248.32s -> 4255.44s]  and you can start to create your multi-model. So the last one, I want to spend some time
[4255.44s -> 4261.76s]  talking about the model, although I'm not saying that it's the best model, but I think
[4261.76s -> 4265.84s]  you can, maybe you can learn from some insights from this model, I think.
[4266.40s -> 4277.68s]  So right now, every day they have new models, so they always have a better one,
[4277.68s -> 4283.76s]  so I hope that we can discuss some different things about the model here. So I want to
[4283.76s -> 4292.80s]  discuss this paper with our paper, which is a unified pure vision model for the GUI agent.
[4293.52s -> 4297.04s]  For the GUI agents, we found that they have three,
[4299.84s -> 4303.28s]  not problems, three issues that we want to solve.
[4303.28s -> 4313.84s]  First is, most of the GUI agents are text-based, user text representation,
[4313.84s -> 4323.76s]  like an accessibility tree. Second, the video grounding and performance capability is not good.
[4323.76s -> 4331.04s]  I think it's because most of the models are not really to challenge their model for the
[4331.52s -> 4340.40s]  use. And the third one is, most of the model is to generate an action directly without it.
[4341.12s -> 4348.96s]  So here are some more concrete slides about these three observations or issues.
[4351.28s -> 4359.20s]  The first one, you can see, they have different text representation
[4361.28s -> 4365.20s]  some like, you know, web, it's HTML, maybe DOM tree,
[4365.20s -> 4369.68s]  and in the OS, they have another type of accessibility tree, and the mobile has different ones.
[4372.72s -> 4376.88s]  And most of them, it's super long, super lengthy,
[4378.72s -> 4384.80s]  and it could be longer and longer at the complexity of your GUI.
[4384.80s -> 4397.04s]  So comparing with the pure vision, the vision is just one. So whatever your background code
[4397.04s -> 4407.92s]  looks like, the image is the same. So that is one about the benefit of the GUI agent.
[4408.64s -> 4417.04s]  The second one is because different systems use different tools,
[4417.60s -> 4424.72s]  the action space is very different. It's not a big issue, but it's still an issue that it
[4424.72s -> 4431.68s]  will prevent this effective cross-development learning. And also that you cannot leverage
[4431.68s -> 4440.32s]  different data, the training data together, which is not good for the scalability.
[4441.36s -> 4449.28s]  Then it's this visual grounding capability. So here we tested on the mobile,
[4449.28s -> 4460.96s]  desktop, and the web scenarios. We found those cross-sourcing models from surprising
[4462.64s -> 4472.32s]  bad, I would say. So it means that the model doesn't really optimize for the computer use
[4472.32s -> 4479.92s]  scenarios by then. This is DPD4, DPD4O. The new one, operator, or cloud 3.5, 3.7.
[4481.60s -> 4489.52s]  Here is an example. The current existing methods, they train the agent to generate
[4490.72s -> 4497.36s]  the low-level actions directly, like the pi, the director of the code,
[4498.00s -> 4510.64s]  rather than the sophisticated reasoning concept, reasoning the text, which would
[4512.64s -> 4521.04s]  get those models or those agents very struggle on the complex scenarios in the real world.
[4521.68s -> 4531.92s]  That may require more careful planning, which those planning knowledge could be transferred
[4532.48s -> 4546.00s]  from the text corporates, which train in LMD. So given these three issues, we propose this
[4546.00s -> 4555.36s]  algorithm model. First, it's pure vision-based, so it doesn't matter what accessibility tree
[4555.36s -> 4567.36s]  format it's using in the system or in the web browser. Second, we improve the grounding model
[4567.36s -> 4577.52s]  to combine, we define a new uniform format to combine all the 15 data and train the model
[4578.40s -> 4585.04s]  to improve the grounding. And third one is we explicitly include this reasoning process,
[4585.60s -> 4592.32s]  we call it, in a monologue into our training, the model training for the action generation.
[4593.28s -> 4603.04s]  As I said, for the algorithm, we eliminate these dependencies on the platform specific,
[4603.04s -> 4608.48s]  the textual representation, because it is vision, pure vision-based, it's directly
[4608.48s -> 4617.68s]  operated on the screen images, so it makes the model more generalizable into different interfaces.
[4617.76s -> 4634.96s]  And we developed a standardized action space that we could combine the different
[4636.00s -> 4644.72s]  data set into the same action format so that we can leverage more data, the existing data.
[4644.72s -> 4652.88s]  And the more crucial is the right side is we incorporate this in the monologue during our
[4652.88s -> 4666.80s]  training, which can develop these sophisticated reasoning patterns and which through this,
[4667.68s -> 4674.00s]  by leveraging the knowledge of memory patterns is already memorized in the ALM.
[4675.52s -> 4684.56s]  I think this is important to build a more robust action model.
[4685.60s -> 4696.24s]  So in each interaction step, our agent has to generate this, in the monologue,
[4696.24s -> 4702.16s]  the two steps, the new two parts, the sort and the low-level instructions
[4703.12s -> 4708.40s]  between your visual observation and the final actions.
[4709.76s -> 4716.56s]  So through this way, you could, through this sort and the low-level instruction and action,
[4716.56s -> 4726.56s]  you can consider this enables the more the agent can do it step by step from the different
[4727.20s -> 4732.08s]  complexity, because at the beginning, you get a high-level instruction and you're doing this
[4733.12s -> 4740.00s]  sort, the general sort, so you decompose the problem into some small steps.
[4741.12s -> 4746.88s]  And with those steps, you can generate this low-level instruction for each step
[4746.88s -> 4757.04s]  about action, and then you can get a better concrete code to access questions.
[4757.04s -> 4765.76s]  So then it could make the agent be more robust, more accurate.
[4766.40s -> 4771.28s]  So that is the motivation for the training. We have two steps.
[4771.28s -> 4776.32s]  First step is the grounding, and the second step is planning and reasoning.
[4776.32s -> 4783.76s]  And for grounding, we did some tricks, which is grounding packing strategy.
[4783.76s -> 4790.24s]  What do you mean a grounding packing strategy? It's like we were to find all the grounding
[4790.24s -> 4796.00s]  pairs, the grounding samples, which based on the same image, and we contacted them at the
[4796.00s -> 4804.72s]  market and grounding at one sample. We found that can improve the training efficiency and even
[4805.68s -> 4814.00s]  to have a big improvement on the performance. And in the second stage,
[4816.16s -> 4829.04s]  we add this in the monologue in our training, which make the agent code to generate this
[4829.04s -> 4837.20s]  intermediate sort and the low-level instructions before they generate the action directly.
[4838.16s -> 4847.76s]  In the data part, as I said, we, because it's pure vision-based, we can easily combine
[4847.76s -> 4854.96s]  and define the data set together and define the same format for the outcome.
[4855.92s -> 4862.56s]  Then we can leverage all the most of the existing data and create a new data set to train our model.
[4864.72s -> 4872.64s]  And for the monologue, we use, we prompt the frontier model as a typical O,
[4873.60s -> 4885.92s]  given this observation image and highlight the UI element with the instruction and the final goal
[4886.48s -> 4896.96s]  and the final out the code, the action code, and from GPT4O to generate in the monologue
[4896.96s -> 4907.44s]  component. So this is an example of previous instruction, previous action,
[4907.44s -> 4913.76s]  of course the observation, we don't add here, and this action. Then later, after this step,
[4916.24s -> 4923.28s]  we will have the sort and the low-level instructions in the new data, in the new training.
[4924.16s -> 4932.80s]  So this way, we create the arc width collection, which is combine a number of different data sets.
[4934.80s -> 4941.44s]  Then we finally, we get, for stage one, we get the one million the GUI grounding data,
[4941.44s -> 4949.92s]  and for stage two, we get the 35k Markov step trajectory, which is explicit in the monologue
[4950.56s -> 4955.76s]  reasoning data. It's great. And then we do the same thing as we did before. We
[4957.60s -> 4963.12s]  run our model, we train our model on this agent arc width collection data set.
[4965.60s -> 4973.04s]  And then we demonstrate, we show this arc width model perform exceptional on the
[4974.00s -> 4985.36s]  the grounding task, so the top is the grounding task, so there's no planner, so we can see the
[4985.36s -> 4998.56s]  significant output from the existing models on those tasks. And we also run the two-stage
[4998.56s -> 5007.84s]  training, so we also have the model, single model to both grounding and planning reasoning action.
[5009.44s -> 5017.04s]  We found after you did the second stage training, it even improve this grounding task
[5019.68s -> 5025.68s]  and achieve the state-of-the-art performance. And then we also did the development,
[5026.64s -> 5034.40s]  if the evaluation on the offline setting on mental lab, it also showed it to
[5035.04s -> 5041.68s]  achieve the superior performance across different metrics on the offline setting,
[5042.64s -> 5049.36s]  and also on the Android setting. We also did the this evaluation on the online setting, which
[5049.36s -> 5057.68s]  is mental lab and the Android world, which is this mental lab life is different to mental lab,
[5057.68s -> 5064.00s]  mental lab, it provides a dynamic lab-based development to do the evaluation.
[5066.40s -> 5075.84s]  And we can see the performance of all the arc width models, the superior performance.
[5079.60s -> 5089.20s]  Even better than so. And then one thing I want to emphasize is, if you see here is the arc width
[5089.20s -> 5099.92s]  7B model with GPT4O planner, it could perform the state-of-the-art in these tasks,
[5099.92s -> 5108.56s]  which means that the arc width model have a better grounding capabilities,
[5108.56s -> 5118.56s]  and this definitely can improve the final performance of this planner task, this planning
[5118.56s -> 5126.32s]  task. And this is not, it's also we found like they still have a big space to improve
[5127.20s -> 5133.20s]  the grounding to improve, which will improve the final performance on the
[5134.08s -> 5137.60s]  different temperature use time. And we do some analysis on
[5140.08s -> 5145.60s]  for different components in the arc width on a performance. Both stage one, stage two is
[5145.60s -> 5152.24s]  important. In the MOOC, it's also very crucial for both the high level reasoning
[5152.24s -> 5156.00s]  and the low level grounding task. And we find that in the monologue,
[5156.80s -> 5164.16s]  actually helps solving the harder task, which in the right side you see,
[5164.16s -> 5170.72s]  one is self-plan, one is enforced plan. Enforced plan means we enforce it, it must
[5170.72s -> 5180.48s]  generate in the monologue. And it can achieve the 20% of, resolve 20% of the grounding errors.
[5182.72s -> 5189.36s]  And the left side is two examples, with and without in the monologue. And we also
[5190.48s -> 5195.76s]  see the cross-platform benefits in the arc width. The arc width is trained on the
[5195.76s -> 5204.56s]  web and the mobile data. The other one is the computer use. We find it is remarkably,
[5204.56s -> 5216.00s]  although we didn't train exclusively on the computer use scenario, the model still performed
[5216.00s -> 5229.12s]  pretty good. And using arc width 72b as the grounding part before, it's even better than
[5229.12s -> 5241.28s]  how computer use model. So here I want to take, here I want to, you can take away. First,
[5242.96s -> 5255.76s]  the arc width model is a unified framework. It can perform pretty robust across different
[5255.76s -> 5265.12s]  platforms with only visual observation. And I think the visual, this pure vision paradigm,
[5266.64s -> 5276.56s]  it has a big advantages on the computer use, which UI use case, which I think OpenAI already
[5276.56s -> 5282.40s]  agree with my comment, because they have the operator.
[5284.08s -> 5290.72s]  And the second is, although we have a lot of progress, we still need to improve both grounding
[5290.72s -> 5295.04s]  and the structural reasoning. Don't just try to improve reasoning. Actually,
[5295.04s -> 5300.24s]  they have bigger, bigger space there. You need to improve in a grounding.
