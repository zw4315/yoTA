# Detected language: en (p=1.00)

[0.00s -> 4.24s]  Hello everyone, now let's get started for the first lecture of this class,
[4.24s -> 10.72s]  Advanced Large Language Model Agents. I'm Xinyin Chen, I'm a research scientist at Google
[10.72s -> 15.84s]  DeepMind, and I've been starting this course with Professor Dong Song here at UC Berkeley
[15.84s -> 21.44s]  and Caoyou Yang from META. We also have an amazing TA team who help us a lot on sort
[21.44s -> 28.32s]  out the cost logistics with the head TA Alex and readers Tara, Ashwin, and Jason. Last semester
[28.32s -> 34.48s]  we launched the first version of this alarm agents course both here at UC Berkeley and also on MOOC.
[35.68s -> 40.32s]  Last semester we had a very amazing line of guest speakers talking about very broad
[40.32s -> 47.28s]  topics about alarm agents, so we discussed about the fundamental reasoning techniques,
[47.28s -> 54.56s]  the agent workflows, and different applications, etc. And we had around 15,000 people registered
[54.56s -> 61.36s]  on MOOC. We also launched a hackathon which attracted around 3,000 people participating globally,
[62.00s -> 67.52s]  and we also we are very grateful for this general sponsorship from our industry partners
[67.52s -> 74.64s]  who gave us around $200,000 support in prizes and resources. We will announce the hackathon
[74.64s -> 81.92s]  winners very soon, so stay tuned. Now let's go back to alarm agents. So the call of a
[81.92s -> 87.52s]  large language model based agent is that the kind of the brain of this agent is a large
[87.52s -> 93.36s]  language model which is going to perform reasoning and planning to take action at each
[93.36s -> 98.88s]  time step. Then with this like action to interact with the environment it will receive
[98.88s -> 104.72s]  the feedback and revise its internal memory so that it can better plan for the next step to take.
[106.24s -> 111.36s]  So with this like agent framework it allows the alarm to also utilize this external
[111.36s -> 117.84s]  feedback and tools whenever available so it is can further expand its capability and to give to
[117.84s -> 124.48s]  get more informed decision in its own decision making process. So some common components in the
[124.48s -> 130.00s]  agent framework include tool use and retrieval. Then why do we want to empower these large
[130.00s -> 136.00s]  language models with the agent framework? Because solving real-world tasks typically involves a
[136.00s -> 143.04s]  trial and error process especially when we deploy this alarm agent in a like a new environment
[143.04s -> 148.72s]  it really needs to like interact with the environment to understand some success or failure
[148.72s -> 153.60s]  modes in different actions so that it is able to like revise its own internal memory.
[154.72s -> 159.68s]  At the same time leveraging external tools and retrieving from external knowledge
[159.76s -> 165.76s]  further expands alarm's capabilities. And with the more like with this agent
[165.76s -> 171.12s]  workflow design becomes more mature it also for these cities facilitates the accomplishment
[171.12s -> 178.88s]  of complex tasks. This includes task decomposition, allocation of subtasks, division of labor for
[178.88s -> 184.08s]  collaboration between different specialized modules and multi-agent generation.
[184.08s -> 194.32s]  Many people are saying that this year 2025 will be the year of agents. So nowadays we have
[194.32s -> 199.92s]  already seen explosion of alarm agents for various applications and in many domains
[199.92s -> 205.36s]  actually these agents transformed how previous people are thinking about like all these like
[205.36s -> 211.68s]  AI models can how these AI models can behave. So some very successful demonstrations and also
[211.68s -> 218.72s]  practical use cases include code generation, computer use, personal assistant, aerobatics
[218.72s -> 224.32s]  and these of applications will keep like growing in the future as well and also there
[224.32s -> 230.40s]  are some like domains like education, finance, etc. where we already have seen an emergence of
[230.40s -> 235.76s]  a pretty nice alarm based agents. Another factor that further drives this excitement about
[236.08s -> 240.56s]  agent development is about this like recent rapid progress of reasoning models
[241.12s -> 247.52s]  starting from September last year with the release of OpenAI 01. And starting from that
[247.52s -> 253.84s]  a model we have also seen a lot of like great reasoning progress during the meantime.
[253.84s -> 260.40s]  For example last month we saw the release of the Gemini 2.0 flash syncing model and also the
[260.40s -> 266.48s]  OpenAI 03 model and this month we saw the release of DeepSeek R1 and Kimi K1.5.
[267.20s -> 271.44s]  And also within the same model series we can see a very drastic improvement
[271.44s -> 276.80s]  from different versions of the model. So for example there is a rapid progress of performance
[276.80s -> 281.84s]  from OpenAI 01 to 03 and also if we compare the current Gemini syncing model
[281.84s -> 286.72s]  to our initial version released last month there are also like a lot of improvement we can see.
[287.52s -> 293.12s]  And if we think about this like progress in reasoning the most impressive performance we
[293.12s -> 298.64s]  have seen so far are in maths and coding especially for solving those very challenging
[298.64s -> 305.20s]  problems. For example last year with the alpha proof and alpha geometry system developed by my
[305.20s -> 310.64s]  colleagues at Google DeepMind they received the silver medal or nearly gold medal performance
[311.04s -> 318.32s]  in IMO last year. And also with the more recent OpenAI 03 model it shows that it can be ranked
[318.32s -> 324.24s]  among top 200 among human participants on code forces competitive programming contests.
[325.68s -> 330.72s]  If we think about these milestones probably they are even very hard to imagine a couple
[330.72s -> 335.84s]  of months ago before they actually happen. So for this semester our course is called
[335.84s -> 341.76s]  advanced large language model agents. So we will do a deeper dive into the methodology part
[341.76s -> 346.96s]  especially on the reasoning techniques. So we will talk about fundamental techniques for
[346.96s -> 353.52s]  inference time for inference time scaling training techniques and also search and planning.
[354.72s -> 360.16s]  And for the application side we will focus more on software engineering and mathematics.
[360.72s -> 367.84s]  So this includes code generation and verification and also auto-formalization and serial improving.
[368.80s -> 371.60s]  After this deep dive on the methodology we will talk about
[372.32s -> 377.36s]  alarm agents use cases for real-world enterprise application and also more advanced
[378.00s -> 384.16s]  agentic workflow design and also topics about safety and ethics for real-world deployment
[384.16s -> 391.04s]  of these agents. Now I will hand over to our head TA Alex to talk about cost logistics.
[391.04s -> 397.52s]  Okay so the course is pretty similar to last week's the or last semester's the main difference
[397.52s -> 403.68s]  is we are moving the lab to a later due date. So last year the lab was a bit earlier and is
[403.68s -> 407.68s]  so much shorter this year the lab is going to be this semester the lab is going to be more
[407.68s -> 413.12s]  involved and it will be due or it will be released later and you'll have more time to work on it.
[413.84s -> 420.08s]  The other main thing apart from not having a hackathon is we're also going to sort of open
[420.08s -> 427.12s]  up the project. So last semester the project was sort of open-ended this time we'll have two
[427.12s -> 434.16s]  tracks an applications track and a research track. I think I wrote about it here so the
[434.16s -> 440.88s]  applications track is pretty much the same content as last semester the research track is new
[440.88s -> 447.20s]  basically if you want to or are interested in submitting a conference publication or a workshop
[447.20s -> 452.40s]  publication you can apply to the research track we'll have a google form out shortly
[452.40s -> 457.60s]  and if we select you then you can sort of work with under the supervision of like some
[457.60s -> 464.32s]  of Don's postdocs and grad students to basically try to submit a workshop paper for this
[464.32s -> 470.32s]  class. Other than that everything else is sort of the same as last year I would just refer to
[471.60s -> 477.20s]  the final like everything is on this master website and we won't be having a syllabus just
[477.20s -> 482.72s]  keep track of the changes on here and on edstem. So that's it for logistics.
[483.60s -> 489.04s]  Yeah and also like same as last semester we will also invite like a very great like speakers
[489.04s -> 496.00s]  to talk about different topics of advanced like agents my story and applications so we
[496.00s -> 500.80s]  will we are still finalizing the guest speaker schedule so also check out this course website
[500.80s -> 505.68s]  for the most up-to-date information. In this lecture I will talk about inference time
[505.68s -> 511.12s]  techniques for alarm reasoning so in the inch of the cross we briefly talk about the
[511.12s -> 516.00s]  advancement of reasoning models which is kind of one of the highlights of large-range model
[516.00s -> 522.16s]  development in last year and so here I will like go more into details about what we can see
[522.16s -> 528.24s]  from these reasoning models. So on the slides I'm showing these like results which was like
[528.24s -> 534.56s]  quoted from the 01 blog. So from here we can see that actually OpenAI 01's model started to
[534.56s -> 540.00s]  achieve pretty impressive performance across various very challenging class where previous
[540.00s -> 545.04s]  models are really struggling with so this includes the competition level math problems
[545.04s -> 550.48s]  like in the ME contest last year and also competitive programming contest on code forces.
[553.12s -> 557.68s]  And in the recent like all three results they further show this like the performance can
[557.68s -> 563.20s]  further improve with more inference time compute. For example here I'm showing these like
[563.20s -> 569.60s]  curves they demonstrated on a benchmark called ARCAGI. So this is a benchmark that includes
[569.60s -> 576.64s]  a bunch of puzzles to test how well humans or AI models can learn some reasoning patterns
[576.96s -> 583.12s]  given the demonstration examples. So this was a very challenging task so for those existing models
[583.12s -> 588.88s]  before 01 if we do not apply any special inference time techniques and just do the like
[588.88s -> 595.20s]  very regular inference then all existing models can achieve like less than 25 percent accuracy
[595.20s -> 600.72s]  and even with the 01 model it can only achieve around 30 percent accuracy but with the
[600.72s -> 608.24s]  recent 03 model if we set the inference time budget to around 20 dollars per task
[608.24s -> 613.76s]  it can achieve a performance that kind of matching the average human annotators or mechanical
[613.76s -> 618.88s]  workers and if we further scale out this inference time cost to a crazy number like one
[618.88s -> 624.88s]  thousand dollars to solve one problem then actually the model can achieve 87.5 percent accuracy.
[624.88s -> 629.76s]  So definitely this is a very expensive way to solve this particular task but
[629.76s -> 635.92s]  it shows how this model can perform in the ideal case when we do not care so much
[635.92s -> 640.96s]  about inference time compute. Then here on the slides I'm showing a very simple like opening
[640.96s -> 647.60s]  at 01 demo where basically I call the model to solve a planning problem. So what is different
[647.60s -> 654.56s]  for this 01 model? So the main difference is that for the existing ARAM before the 01 model
[654.56s -> 659.28s]  when we enter a user query then the model will directly generate this response.
[659.84s -> 666.24s]  But in the 01 interface they include this thought process so and basically this process can take
[666.24s -> 670.80s]  different amount of time depending on the difficulty of the task. For example for the
[670.80s -> 677.20s]  specific question I asked the model it takes slightly more than one minute to generate the
[677.20s -> 684.16s]  final solution and in the 01 interface they didn't show the like a the raw thought process
[684.88s -> 690.16s]  instead they show a summarized version of the hidden thought. So from this hidden thought for
[690.16s -> 695.44s]  this specific planning problem we can see that it summarizes some key stages in the reasoning
[695.44s -> 702.56s]  process including laying out initial plan evaluate it based on the model's internal belief
[702.56s -> 708.96s]  and then revise the plan so that it can satisfy the constraints and improve the optimality.
[708.96s -> 719.76s]  Then with the Gemini 2.0 syncing model we also like make these sort tokens visible to users.
[720.48s -> 726.24s]  So on the slides I'm showing a demo where actually this question includes an image input.
[726.88s -> 731.44s]  So this question is the math part of this question is pretty simple so basically
[732.24s -> 736.96s]  there are four balls in the image and the question is asking the model to take three out
[736.96s -> 744.48s]  of these like numbers so that the sum is 30. So the thought here as you can see is pretty long
[744.48s -> 749.20s]  so after we enter the query the model immediately generates these thoughts and we don't
[749.20s -> 754.08s]  need to read the reasoning process token by token I know it's pretty hard to read but the
[754.08s -> 760.00s]  most interesting step we can see from this multi-model query is that there is a so at the
[760.00s -> 765.84s]  beginning of the this like a image there are there's a specific number which is nine
[766.48s -> 772.40s]  so at some point the model figures out a very important step for this question so based because
[772.40s -> 776.96s]  if you want to get the sum of 30 actually we need to turn this nine into six by rolling
[776.96s -> 782.80s]  this ball so this is a very interesting like a feature of this of this query when we put
[782.80s -> 790.80s]  these numbers on some balls and also like this like step itself it's not that like
[792.48s -> 798.08s]  kind of difficult mathematically but actually it is an interesting trick and if we and earlier
[798.08s -> 802.32s]  if we do not allow the model to generate this long sort this specific step will be very
[802.32s -> 810.56s]  hard to figure out and also like besides the these models from open ai and google didn't mind
[810.56s -> 814.40s]  there are other like reasoning models out there recently like at the beginning of the
[814.40s -> 818.88s]  lecture I mentioned there are also models from the deep seek team and also the kimi team
[819.68s -> 825.28s]  and out of all these like a breakthrough in reasoning models one core shared idea is that
[825.28s -> 830.24s]  we need some way to trigger the large language model to generate long chain of thought before
[830.24s -> 835.84s]  it actually concludes with the final solution in the literature there are different approaches
[835.84s -> 841.60s]  to trigger this child soul generation like in the original child so the paper they designed
[841.60s -> 847.36s]  a few short like pointed scheme which basically demonstrates this thought process in exemplars
[848.80s -> 852.96s]  and later on there are some works showing that we can apply an instruction prompting
[852.96s -> 856.48s]  to achieve this like we can use the instruction let's think step by step
[859.04s -> 864.24s]  and for the reason that chat models we can see that even if we do not give any special
[864.24s -> 869.12s]  instructions to the model the model also tends to generate some thoughts before coming up with
[869.12s -> 874.48s]  the final answer no matter whether the story is like a short or long so there are different
[874.48s -> 879.92s]  ways to train the model to achieve this like in for instruction tuning where we actually put
[879.92s -> 885.44s]  this chain of solid data into our supervised fine-tuning mixture and also like all you can
[885.44s -> 890.00s]  see from recent reasoning models we can apply reinforcement learning to achieve this goal
[890.80s -> 896.32s]  in this lecture i will focus more on inference time techniques for scaling token budget
[896.32s -> 901.68s]  and in the future lectures we will also deep dive deeper into reasoning techniques
[901.68s -> 905.76s]  and there will be some other guest speakers who will come here to share their thoughts
[909.44s -> 914.80s]  so from this lecture i will discuss like three parts about reasoning techniques at inference time
[914.80s -> 919.52s]  i will first discuss the basic prompting techniques and the common idea among these
[919.52s -> 923.92s]  prompting techniques is that we want to allow the model to use more token budget
[923.92s -> 930.40s]  to generate a single solution then the second part i will talk about techniques to perform
[930.40s -> 935.92s]  search and selection from multiple candidates and the common idea here is that this want
[935.92s -> 942.80s]  to increase the width to explore the solution space in the last part i will talk about
[943.76s -> 947.44s]  in the last part i will talk about iterative cell improvement
[947.44s -> 951.04s]  we'll allow the model to increase its depth to reach the final solution
[952.72s -> 958.64s]  now after all these discussions of different parts we will also see how we can like best utilize
[958.64s -> 961.84s]  different combinations of these techniques to seek the best
[962.88s -> 967.04s]  way to utilize the inference time compute to improve the alarm reasoning performance
[967.04s -> 973.36s]  so let's get started on the first part about basic prompting techniques
[975.76s -> 979.60s]  before we talk about more advanced prompting techniques let's first reveal
[979.60s -> 984.64s]  the background about standard prompting basically in **standard prompting** let's say
[984.64s -> 989.52s]  we want to solve a math problem as shown in the slides before line of works on channel
[989.52s -> 996.16s]  solve prompting the common practice is that we just put some question answer pairs into the prompt
[996.16s -> 1001.92s]  like here the question is the math question and the answer is the final answer so before
[1001.92s -> 1006.80s]  the advancement in post-training techniques if we directly apply this standard prompting
[1006.80s -> 1013.20s]  then the performance is pretty poor on reasoning benchmarks the fundamental issue is that if we
[1013.20s -> 1018.80s]  look at these standard future examples they only provide information on the final solution format
[1018.80s -> 1023.28s]  but it doesn't tell the model about the rationale to derive the solution now you can
[1023.28s -> 1029.20s]  imagine that if we like if we are taking a class and the teachers are basically just teaching you
[1029.20s -> 1034.16s]  with the final like official solution without any more context then if you do not have
[1035.04s -> 1039.68s]  relative background knowledge yourself then it will make your teaching a lot more difficult
[1042.56s -> 1048.00s]  so for the child sort of prompting the idea is that we will include these like a future
[1048.00s -> 1054.00s]  exemplars with sorts again let's let's take a look at this example of solving math problems
[1054.56s -> 1060.08s]  so here instead in exemplars we will provide these calculation steps to basically demonstrate
[1060.08s -> 1066.64s]  the model how we should like um just how we should derive this like a solution step by step
[1066.64s -> 1072.64s]  to to achieve the final answer then after giving these exemplars with a train of thought
[1072.64s -> 1076.40s]  originales then the model is able to solve this problem correctly
[1079.84s -> 1085.76s]  in practice how this how this like um these models can benefit more from chance of prompting
[1085.76s -> 1090.24s]  compared to standard prompting is also dependent on the model's fundamental
[1090.24s -> 1095.92s]  capabilities so here on the sides i'm showing the scaling curves presented by the original
[1095.92s -> 1101.44s]  chance of papers and also some other related papers about these like um chain of thought
[1101.44s -> 1107.84s]  properties so we can see that among like different model families one common trend
[1107.84s -> 1113.20s]  is that this clt performance improves more significantly with the increase of the model
[1113.20s -> 1119.92s]  sites in particular if we look at this that gap of chance of prompting versus standard prompting
[1120.48s -> 1125.04s]  learn better models benefit more with clt generation and there we suggest
[1125.04s -> 1129.28s]  the improvement on reasoning performance once the model reaches a certain scale
[1131.44s -> 1137.92s]  i also want to know that here for all for these curves they are from papers in 2022 so these
[1137.92s -> 1144.16s]  experiments are done using return only alarms so this is why it is important to apply some
[1144.16s -> 1150.00s]  prompting so that the model can generate this source immediately and also with these like
[1150.00s -> 1154.64s]  recent post-training techniques they might show some different scaling curves like we have seen
[1154.64s -> 1161.20s]  some like strong but very lightweight alarms recently but then the main conclusions will
[1161.20s -> 1166.16s]  still hold we will still see this overall trend basically well the once the model becomes
[1166.16s -> 1172.16s]  better it will benefit more from this like um this allowance of the child generation then in
[1172.16s -> 1177.76s]  a follow-up work called large language models are still short listeners they further find that
[1177.76s -> 1182.72s]  even with pre-trained only large language models we do not have to provide the model with
[1182.72s -> 1189.36s]  examples to elicit this clt generation instead we can achieve this with an instruction like in
[1189.36s -> 1194.00s]  the paper they demonstrate this instruction let's think step by step then after we apply
[1194.00s -> 1199.52s]  this instruction before we ask the model to generally answer the model can also generate some
[1199.52s -> 1206.96s]  thoughts before generating the final solution and they empirically demonstrate that this still
[1207.20s -> 1213.68s]  clt significant outperform social performance especially on harder like math and symbolic
[1213.68s -> 1220.72s]  reasoning tasks so zero short clt is more convenient than future clt in the sense that
[1220.72s -> 1227.04s]  we no longer need to manually annotate source ourselves as exemplars but if we compare their
[1227.04s -> 1234.32s]  performance zero short clt performance is still much worse than future clt then a very natural
[1234.32s -> 1239.76s]  question we want to ask is how can we get the best of the both worlds how to improve clt
[1239.76s -> 1247.20s]  performance without manually laboring samplers so this is all we want to achieve with our paper
[1247.20s -> 1253.20s]  large language models as analytical reasoners this paper was published at iclea last year
[1253.20s -> 1259.04s]  the idea is that instead of directly providing the model with exemplars we instruct the model
[1259.04s -> 1265.20s]  to first record relevant exemplars to solve the test problem then based on this like model
[1265.20s -> 1269.92s]  generating samplers then the model will try to solve the test problems like just
[1270.48s -> 1277.68s]  imagining that this self-generated samplers are just some clt exemplars provided by external sources
[1280.56s -> 1286.80s]  the benefit of this design lies in two folds first these exemplars are self-generated by
[1286.80s -> 1292.64s]  the large language models so we fulfill our original goal we don't need to do any manual
[1292.64s -> 1299.44s]  labeling here on the other hand because this instruction is applied to each test question
[1299.44s -> 1303.60s]  so basically for the for different test question the model is able to generate different
[1303.60s -> 1308.40s]  exemplars which are more relevant to the concrete topics covered in the test question
[1308.40s -> 1313.68s]  so these exemplars are more tailored to individual problems we call our approach
[1313.68s -> 1319.04s]  analogical prompting because the at least methodologies actually can be motivated by
[1319.04s -> 1326.24s]  human analogical reasoning if we think about future prompting it is not supernatural if we
[1326.24s -> 1331.52s]  think about in a human reasoning way because humans are not explicitly given demonstrations
[1331.52s -> 1338.64s]  every time for for each single new problem and instead humans also intrinsically recall
[1338.64s -> 1344.88s]  from past relevant experience so in a classic reasoning methodology book called how to solve
[1344.88s -> 1349.76s]  it from poya they have a very nice description about this reasoning method
[1349.76s -> 1355.84s]  named do you know a related problem so the idea is that when solving a problem we look
[1355.84s -> 1361.36s]  for a pre-formally solved problem which is linked to our present one by generalization
[1361.36s -> 1368.40s]  specialization or analogy and want to learn from these like past results or their methods so
[1368.40s -> 1376.88s]  that we can help us solve new test problems better if we think about it from the human
[1376.88s -> 1382.80s]  reasoning perspective learn besides exemplars we can also install a model to generate some
[1382.80s -> 1388.96s]  higher level knowledge which can be like a more like better summary of what we need to solve
[1388.96s -> 1394.24s]  the new problems and this generating knowledge can complement the problems with broader insights
[1395.20s -> 1402.24s]  so here let's take solving coding problems as an example so given a new coding problem
[1402.24s -> 1406.64s]  we can first like instruct the model to self-generate some knowledge which is
[1406.64s -> 1411.20s]  more of like higher level tutorial about the algorithms we need to solve the problem
[1411.20s -> 1414.88s]  and also the model needs to identify what are the main algorithms we need
[1414.88s -> 1419.36s]  so it can come up with related example of problems with their corresponding code
[1420.32s -> 1425.92s]  then after this instruction and also once the model generate this related like a background
[1425.92s -> 1431.28s]  knowledge including higher level tutorial and exemplars then the model goes back to solve
[1431.28s -> 1436.80s]  the initial test problem so in the results we show that basically analogical prompting
[1436.80s -> 1442.56s]  not only outperforms zero short cot but actually outperforms manual future cot as well
[1442.56s -> 1448.00s]  and we show this performance improvement or in like um mass problem solving co-generation
[1448.64s -> 1454.88s]  and also like big bench reasoning tasks if you look at these results at the first time you
[1454.88s -> 1460.00s]  might be a bit surprised because knowledge here all these example of problems or knowledge
[1460.00s -> 1467.12s]  are generated by the model so intuitively it might contain more errors than human annotated
[1467.12s -> 1472.88s]  exemplars empirically we indeed observe this trend we see that for example for those like
[1472.88s -> 1478.56s]  examples generated by the model for mass problems around 70 percent of them are accurate and also
[1478.56s -> 1483.60s]  they are relevant but the rest can have some like um like issues like they are completely
[1483.60s -> 1488.80s]  irrelevant or there are some actually wrong calculation steps but even though there are
[1488.80s -> 1492.56s]  some noises in the demonstrations the model still benefits a lot from it
[1496.24s -> 1501.76s]  so given this like a finding you you might imagine that this means at least the model
[1501.84s -> 1506.16s]  should be able to generate some reasonable exemplars or otherwise it cannot learn from it at all
[1507.12s -> 1512.08s]  so based on this intuition we also conduct a like pretty rough like a scaling
[1512.08s -> 1518.72s]  like study for this analogical prompting so here we are comparing like models among like
[1518.72s -> 1524.08s]  different among the gpt test model families i know these models are all shut down and
[1524.08s -> 1530.72s]  deprecating now but um but this trend will still hold so basically we can see that we
[1530.72s -> 1536.16s]  call large language models benefit less from a logical prompting although if we compare the
[1536.16s -> 1542.32s]  performance to zero short chain of thought it's still uh it's on par or still outperforms
[1542.32s -> 1548.24s]  their performance and with stronger large language models like starting from text
[1548.24s -> 1553.28s]  eventually 002 in a gbt family and logical prompting starts to outperform channel
[1553.28s -> 1559.52s]  sort prompting with manually designed exemplars and notice that actually this approach can even
[1559.52s -> 1565.04s]  outperform retrieved examples so for retrieval method what we did is that basically for each
[1565.04s -> 1570.64s]  test problem we implement a retrieval approach so that it will find those most relevant
[1570.64s -> 1579.12s]  problems in the in the GSM AK mass corpus training data so one explanation here is that
[1579.12s -> 1585.28s]  this generated channel by the model can be more tailored to the underlying reasoning style of
[1585.28s -> 1591.20s]  these like a quick trend or instruction tuned large language models so this finding actually
[1591.20s -> 1596.16s]  is also in line with the recent trend like when we are trying to develop the reasoning models
[1596.72s -> 1601.84s]  instead of like um putting the in putting like um human written source we can have some
[1601.84s -> 1608.32s]  training methods so that the model is able to discover the best like reasoning strategies by
[1608.32s -> 1615.76s]  the cell now let's go back to the instruction let's think step by step so why do we have to
[1615.76s -> 1622.08s]  use this sentence is there something special about it so in the original zero short cot paper
[1622.08s -> 1626.80s]  they conducted this ablation study of different variants of these instructions to trigger the
[1626.80s -> 1633.52s]  channel generation so some of them do not make much sense so it is kind of um not surprising
[1633.52s -> 1638.88s]  that they do not perform well but if we look at those different instructions in the top category
[1638.88s -> 1645.04s]  instructive if we mask out these accuracy numbers it is it might be not that intuitive to
[1645.04s -> 1651.60s]  know which instruction should be performed better and it can be like kind of um not very good to
[1651.60s -> 1656.48s]  see that even if you just change a couple of words then accuracy number can change very
[1656.48s -> 1662.08s]  dramatically so this shows that current large range models are pretty sensitive to prompt design
[1663.76s -> 1670.00s]  because um and also in many cases there's no clear principle about how to write optimum forms
[1670.00s -> 1674.88s]  and this is why nowadays there are still many people who are trying writing a lot of guidelines
[1674.88s -> 1681.92s]  about how to have have the best practice to do from engineering so given this observation
[1682.80s -> 1688.56s]  a very natural question i want to ask next is how we can reduce this manual work for writing problems
[1689.52s -> 1695.20s]  so one idea is that since our large-range models can solve like many like challenging reasoning
[1695.20s -> 1700.88s]  problems and also answering other queries so why don't we also use it to help us do the prompt
[1700.88s -> 1707.28s]  engineering or like prompt design in general so in iClear 2023 there is a paper called large
[1707.28s -> 1712.96s]  language models are human level prompt engineers so the head of idea is basically to apply a
[1712.96s -> 1718.72s]  large-range model to propose like problems and so that we hope this model generated problems can
[1718.72s -> 1724.40s]  also perform even better than human written problems so here i'm showing the complete
[1724.40s -> 1730.08s]  diagram of their method so there are two important stages the first is this proposal
[1730.08s -> 1735.60s]  generation where we can leverage this large-range model to generate initial instructions given
[1736.32s -> 1740.16s]  to generate initial instructions given our task description
[1741.92s -> 1746.96s]  then once the model generate different like proposed problems we learn we'll score each
[1746.96s -> 1752.08s]  instruction based on the prediction correctness on a small set of problems so here basically we
[1752.08s -> 1759.12s]  will hold out a small validation set which can tell us the quality of each prompt so here
[1759.12s -> 1763.76s]  the setup might be like more more similar to a traditional machine learning task where we
[1763.76s -> 1769.36s]  have this like separation of like training and test but actually we do not need to include so
[1769.36s -> 1776.24s]  many like examples here for this validation purpose like in most cases it is sufficient to
[1776.24s -> 1784.24s]  include tens of examples or just around 100 problems then our work published at iClear last
[1784.24s -> 1790.48s]  year we go further where we try to use this large-range model not only just to propose some
[1790.48s -> 1796.08s]  like some instructions and do some kind of a search and mutation we also want to apply
[1796.08s -> 1803.84s]  it as an optimizer to iteratively improve the prompt so so the idea of our approach is that
[1803.84s -> 1809.84s]  we want to install ARM to leverage the past operation trajectory represented as sorted solution
[1809.84s -> 1815.84s]  score pairs so that it can learn to iteratively generate better prompts with more steps
[1815.84s -> 1820.48s]  so in this work we frame it as a more marginal position framework in the natural language space
[1820.48s -> 1825.60s]  but in this specific use case of prone optimization basically this means that we will
[1825.60s -> 1831.84s]  have two large-range models as optimizers and evaluators so basically this optimizer is the
[1831.84s -> 1837.36s]  large-range model which is responsible for proposing a new instruction given old ones and
[1837.36s -> 1843.76s]  also the task examplers then the evaluator is the scholar which will evaluate the accuracy
[1844.40s -> 1847.20s]  or performance of a given instruction
[1850.80s -> 1855.04s]  now let's take a look at how we can design the meta-prompt to achieve this goal
[1855.60s -> 1860.64s]  so on the stats I'm showing an example meta-prompt for GSM-AK to solve the math problems
[1861.28s -> 1866.40s]  the problem itself looks pretty long but basically there are two like main components
[1867.04s -> 1871.84s]  the first one is this like trajectory including the instructions we have
[1872.24s -> 1877.76s]  explored in the past steps and with their corresponding accuracies and in practice we will
[1877.76s -> 1882.64s]  sort them in the ascending order and we can remove those older ones if the
[1882.64s -> 1888.24s]  context is not enough to put all of them in history and we also provide these
[1888.24s -> 1893.36s]  examplers to show what's the task we want to optimize and basically here for example for
[1893.36s -> 1898.32s]  math problems we only need to include this like QA pairs and with the final answer
[1898.32s -> 1904.08s]  so again there's no need to annotate any chain of thought or any like explanations of how we
[1904.08s -> 1913.12s]  can derive this solution then for the results basically again we are testing all these like
[1913.12s -> 1918.64s]  instructions or pre-trained only large-range models and we use like PAM tool at that time
[1918.64s -> 1924.00s]  and we also try using different models as the optimizers including PAM model and the GBT model
[1924.00s -> 1932.08s]  so for all these like approaches we are starting from the initial instruction with let's solve the
[1932.08s -> 1938.56s]  problem which achieves an accuracy of around 61 percent for this GSM-AK task and this is a pretty
[1938.56s -> 1944.00s]  general purpose instruction it is around like 10 percent lower than the instruction that's seen
[1944.00s -> 1952.48s]  step by step and if we look at the best alarm generated problem among these lists we can see
[1952.48s -> 1957.92s]  that actually this instruction take a deep breath and work on this problem step by step achieves
[1957.92s -> 1963.92s]  like around like eight percent like higher than this next step by step instruction
[1963.92s -> 1969.36s]  and actually if we compare this performance to future CLT it matches the performance like
[1969.36s -> 1974.00s]  reported in a PAM tool paper where the chain of thought examplers are written by humans
[1974.88s -> 1980.08s]  so there are two things we can we can think about here one is that basically here we are
[1980.08s -> 1986.00s]  seeing another way to basically achieve very competitive performance to future CLT
[1986.00s -> 1991.20s]  without manually writing examplers ourselves another perspective is that is that with this
[1991.20s -> 1998.08s]  like alarm-based problem optimization it doesn't only save us time to so that we do not need
[1998.08s -> 2002.88s]  to spend so much time to to like manually tune the problems ourselves but actually when it
[2002.88s -> 2009.28s]  comes with new problems it also provides some sometimes surprising different angles like I can
[2009.28s -> 2016.24s]  assume that for example it's like turns take a deep breath at least for me if I'm trying to
[2016.24s -> 2024.00s]  like revise the problem by myself I won't feel that this phrase will be so so helpful but
[2024.00s -> 2028.32s]  empirically the large range model with this optimization loop actually it can
[2029.44s -> 2033.44s]  somehow come up with this instruction that fits pretty well for this model
[2034.24s -> 2040.00s]  and know that in this like alarm-based optimization process actually this is not a real
[2040.00s -> 2045.76s]  optimization in the sense that we do not have any training here it is completely prompting only
[2046.32s -> 2051.20s]  so there's no guarantee that the model can just keep like coming up with better and better
[2051.20s -> 2059.12s]  prompts but in practice we find that actually this like optimization curve holds for different
[2059.12s -> 2065.52s]  models so we can see that basically this like it was more optimization steps which is basically
[2065.52s -> 2070.96s]  this iterative prompting steps the accuracy increases with more steps and then protrudes
[2070.96s -> 2075.52s]  so it looks pretty like similar to the traditional optimization graphs
[2079.28s -> 2086.40s]  okay now let's go back to child thought so from the methodology perspective what does child
[2087.12s -> 2092.72s]  perspective what does child so really bring into our reasoning why is it so effective
[2094.24s -> 2098.48s]  if we think about it chance of prompting is basically giving us a way
[2099.04s -> 2104.48s]  so to perform this variable computation of the thought process which where these number
[2104.48s -> 2111.60s]  of tokens used for problem will be adapted to us of different difficulty levels so if we
[2111.60s -> 2118.32s]  think about standard prompting basically it for the same task if the answer learns are the same then
[2118.32s -> 2122.56s]  basically if we just provide the final answer directly it basically means that we are forcing
[2122.56s -> 2127.28s]  the model to use the same amount of time same amount of inference time budget to solve each
[2127.28s -> 2133.44s]  problem regardless of its difficulty but with this like a child thought basically it shows that
[2133.44s -> 2139.68s]  for more complex questions because it also require more reasoning steps to solve the problem like for
[2139.68s -> 2144.88s]  math problems it might require more calculation steps so basically with this channel so idea it
[2144.88s -> 2151.76s]  allows the model to basically also perform more reasoning steps in the token space so that it can
[2151.76s -> 2158.00s]  better solve them the test problems and in the chance of paper they also show that even if the
[2159.68s -> 2165.44s]  difficulty of the problems is in exemplars are pretty simple mass problems but actually the
[2165.44s -> 2169.76s]  same problem can also generalize to test problems that require much more reasoning steps
[2173.12s -> 2175.68s]  then if we think about the high level reasoning strategies
[2176.32s -> 2181.36s]  channels thought also incorporates a lot of like reasoning process that are very important
[2181.36s -> 2188.24s]  for us humans who reason about difficult tasks such as this decomposition planning and etc
[2191.12s -> 2195.36s]  so if you want to further improve the model performance and especially if we know what our
[2195.36s -> 2200.96s]  some good guidelines on the reasoning strategies we need to use for a specific task we can also
[2200.96s -> 2205.44s]  explicitly instruct the arm with the desired reasoning strategies for problem solving
[2208.32s -> 2213.76s]  so one idea to further approach this decomposition strategy is called least to most
[2213.76s -> 2220.56s]  prompting and the goal of this approach is to achieve easy to hard generalization by explicitly
[2220.56s -> 2226.48s]  telling the model what's the best practice to decompose the original problem so on the slides
[2226.48s -> 2232.08s]  i'm showing this example for solving mass problems and basically in this original least
[2232.08s -> 2238.32s]  to most prompting paper from my colleagues at google demine it includes two stages so in the
[2238.32s -> 2243.92s]  first stage it performs a problem reduction like given a mass problem it first reduces the
[2243.92s -> 2250.00s]  problems into simpler subproblems so each each sub mass problem can be easier to solve requires
[2250.00s -> 2256.24s]  fewer calculation steps then given these sub questions the model will be able to sub
[2256.24s -> 2261.12s]  to like sequentially solve these sub problems and combine the solutions to solve the original
[2261.12s -> 2268.80s]  problem the most impressive results from this work is that actually with this least to most
[2268.80s -> 2274.08s]  prompting design it shows for the first time that we do not need to like design the
[2274.08s -> 2281.04s]  neural symbolic frameworks or requiring any like program execution took but it can still
[2281.04s -> 2286.16s]  solve these like compositional transition benchmarks with a nearly perfect accuracy
[2286.96s -> 2294.00s]  so the results showing their paper is on this scan benchmark so scan is the synthetic data
[2294.00s -> 2299.52s]  about like translating programmatically generated natural language commands into action sequences
[2300.48s -> 2306.48s]  the most challenging part of this benchmark is about this length split basically in there like
[2306.48s -> 2312.16s]  test splits all the actual sequences are longer than the training samples so this makes it very
[2312.16s -> 2317.36s]  challenging for the traditional like sequence to sequence learning if we directly do this like
[2317.36s -> 2322.24s]  training on on their training set and then on this length split the test accuracy will be
[2322.24s -> 2328.48s]  no more than 25 unless we apply some neural symbolic techniques but then basically with
[2328.48s -> 2334.16s]  this the most prompting they show that if they use them like the latest models at that time
[2334.80s -> 2340.40s]  which is that code eventually 002 they're able to achieve nearly perfect test accuracy
[2341.04s -> 2348.00s]  at the same time they only need to use like 0.1 percent of the training samples as examplers
[2348.00s -> 2354.32s]  so it is like both like more like test efficient like like data efficient to adapt to new tasks
[2354.32s -> 2360.64s]  and while also the performance is very competitive learning our follow-up work published at iClear last
[2360.64s -> 2366.32s]  year we further show that we can extend this list to most prompting idea to solve like more
[2366.32s -> 2373.28s]  complicated compositional transition in the case of translating text to more realistic programming
[2373.28s -> 2378.00s]  languages like here on the slides I'm showing this example of translating natural language
[2378.00s -> 2384.96s]  questions to spark your queries so the main challenge here is that for more real world
[2384.96s -> 2390.72s]  languages including natural language or programming languages there are more complicated grammar rules
[2390.72s -> 2397.44s]  and also the vocabulary size is larger this means that especially if we do not have
[2397.44s -> 2403.36s]  a model with a super long context then a single prompt will be no longer enough to cover all the
[2403.36s -> 2410.72s]  grammar rules needed to solve the task so actually with this challenge it shows one benefit of
[2410.72s -> 2416.80s]  decomposition by allowing the model to use different prompts for different reasoning stage
[2416.80s -> 2423.20s]  it also enables the model to use customized prompts for each sub problem or each sub task
[2423.20s -> 2429.28s]  in the context of agent development so here is an overview of our dynamic list to most prompting
[2429.28s -> 2434.40s]  approach so some details do not matter too much here like how we do the decomposition
[2434.40s -> 2440.80s]  our menu stage we add is this like so-called dynamic selection of examples for each sub problem
[2440.80s -> 2447.60s]  so basically here for each of the like test like a sentence we can discover those example
[2447.60s -> 2452.48s]  questions with the most relevant grammar rules so like so basically we can assure that for each
[2452.48s -> 2457.92s]  part of the sentence we can always retrieve for the most most like useful grammar rules to solve
[2457.92s -> 2461.20s]  the task so that the model can have enough context to solve the problem
[2464.40s -> 2471.12s]  so on this like a compositional like a freebase question benchmark the cfq data set again we
[2471.12s -> 2475.52s]  show that actually our dynamic list to most prompting approach outperforms all these like
[2475.52s -> 2482.16s]  baselines which where we need to train a model on the complete training set and again if we
[2482.16s -> 2486.72s]  compare this like dynamic list to both prompting with channel sort of prompting and other prompting
[2486.72s -> 2493.84s]  approaches it also scales better when we increase this example of form to generate these future
[2493.84s -> 2500.64s]  examples so far we have talked about how we can instruct the model to solve the problem
[2500.64s -> 2506.16s]  in in some like risk with some reasoning strategies indicated by the future examples
[2507.28s -> 2512.56s]  by way if we think about it we can assume that for different reasoning tasks they can
[2512.56s -> 2518.40s]  require very different reasoning structures like there will be different ways to decompose the task
[2518.40s -> 2523.60s]  and plan for each stage at the same time we might not want to annotate all these like
[2523.60s -> 2528.56s]  best practice for each type of task like again we don't want to do this manual labeling of
[2528.56s -> 2535.52s]  of examples for every single problem so in our self-discover work we published at NeurIPS
[2535.52s -> 2541.36s]  last year we basically instruct the model to compose task specific reasoning structures
[2541.36s -> 2547.04s]  without manually written demonstrations so in practice how we do is that basically we will list
[2547.04s -> 2552.72s]  a list of kind of tens of best practice of reasoning strategies including like a channel sort
[2552.72s -> 2558.00s]  of reasoning doing decomposition doing some self-reflection etc then given a different new
[2558.00s -> 2564.72s]  task the model will think about how it can like utilize the reasoning strategies that are
[2564.72s -> 2569.60s]  best suited for the current task and compose a reasoning structure follow it to solve the problem
[2571.60s -> 2575.84s]  and for evaluation we also show that with this self-discover methodology
[2575.84s -> 2580.48s]  it outperforms chain of thought prompting and also other base all the base lines on
[2580.48s -> 2585.92s]  a different benchmarks including big bench hard reasoning task mass benchmark and also
[2585.92s -> 2589.28s]  some other benchmarks for social so social agent reasoning
[2592.16s -> 2597.68s]  so to summarize in the first part i talk about i first talk about chain of thought generation
[2598.64s -> 2604.56s]  and the fundamental idea of allowing the model to perform variable computation to like trigger
[2604.56s -> 2609.92s]  the sort of process and this thought process can adapt to tasks of different difficulty levels
[2612.88s -> 2616.64s]  we talk about how to improve the channel performance at inference time
[2617.60s -> 2621.36s]  so in original chain of thought paper we talk about the future prompting idea with
[2621.36s -> 2627.04s]  stapling of sorts we talk about instruction prompting to trigger this channel generation
[2628.56s -> 2631.76s]  and also we talk about how we can ensure the large-range model
[2631.76s -> 2635.20s]  to automate the prompt design and discover better problems by themselves
[2638.88s -> 2643.92s]  so notice that the this best practice of interacting with large-range models can evolve
[2643.92s -> 2648.80s]  over time like for example recently with this post-change large-range models
[2648.80s -> 2653.60s]  you do not have to put examples for every single task for the model to learn how to do it
[2654.32s -> 2659.76s]  and also and is for some models for example it is better for you to just provide very clear
[2659.76s -> 2664.64s]  guidelines and instructions on what you want and then hope the model can just follow your
[2664.64s -> 2669.84s]  instructions and some models prefer those like an interactive discussion and some models might
[2669.84s -> 2676.08s]  prefer just a very long detailed instruction at once but the principles of how to discover
[2676.08s -> 2682.96s]  good prompting strategies for reasoning still hold here like two very important criteria is that
[2682.96s -> 2688.72s]  we want this approach to encourage the model to generate longer chain of thought for complex
[2688.72s -> 2695.20s]  tasks we want it to scale with different task difficulty and also we need this prompting
[2695.20s -> 2700.56s]  technique to be able to support reasoning strategies required for the task so now let's
[2700.56s -> 2705.84s]  start the second part about performance search and selection from multiple candidates and increasing
[2705.84s -> 2714.64s]  the ways to explore solution space so after we talk about the first part what is missing so far
[2715.20s -> 2719.52s]  yeah i guess this question is not so hard since i already showed the outline of these
[2719.52s -> 2725.20s]  slides so basically like basically from first part we can see that we should not limit the
[2725.20s -> 2730.40s]  large-range model to generate only one solution per problem because the mod because the model can
[2730.40s -> 2738.24s]  just um have some mistakes in its single path solution instead we should allow the model to
[2738.24s -> 2743.36s]  explore multiple branches which will allow the large-range model to recover from mistakes
[2743.36s -> 2749.36s]  in a single generation and there are two ways to basically increase the width of this solution
[2749.36s -> 2756.40s]  exploration one is that we can generate multiple candidate solutions per problem another is that
[2756.40s -> 2760.88s]  at each step we can allow the model to generate multiple potential next reasoning steps
[2763.04s -> 2769.52s]  it is not that hard to think that with this like multiple sample generation then the upper
[2769.52s -> 2773.68s]  bound of the performance will be definitely higher because we allow the model to
[2773.68s -> 2777.60s]  like a kind of um select an alternative probably better solution
[2778.40s -> 2783.60s]  our fundamental challenge for lead for approaches along this line is that how to
[2783.60s -> 2789.60s]  see that the best response from multiple candidates because in the real world use case
[2789.60s -> 2795.44s]  we do not have an oracle scholar at inference time so surely for example in a chat interface
[2795.44s -> 2800.00s]  we can just have the model during multiple like solutions and the user can keep whatever
[2800.00s -> 2806.16s]  they want then this is kind of a best of an acceleration case but it will be more ideal
[2806.16s -> 2811.60s]  if the model can just um do this kind of process internally so that we do not have to
[2811.60s -> 2816.40s]  review like tons of like solutions by ourselves to see that what we want
[2819.36s -> 2825.12s]  so along the line of these like ideas i want to first talk about this self-consistency
[2825.12s -> 2831.92s]  work from my colleagues at google demand so if we look at this idea nowadays we will find
[2831.92s -> 2837.36s]  that this is an incredibly simple idea but it is actually very effective and it is um
[2838.16s -> 2843.92s]  it actually can output can improve performance a lot with all the different like models
[2845.20s -> 2849.60s]  so let's say we want to also solve a math problem and this time we allow the model to
[2849.60s -> 2857.12s]  generate multiple responses so um basically after we have all these responses we still do not know
[2857.12s -> 2863.68s]  which one is accurate but here we have this consistency based selection criterion so basically
[2863.68s -> 2868.16s]  let's say for this that math problem with a single number as the final answer
[2868.16s -> 2873.04s]  we will look at these three solutions two of them have the answer 18 and one of them have
[2873.04s -> 2879.28s]  the answer 26 so basically 18 is the most consistent answer so we pick the response
[2879.28s -> 2886.96s]  corresponding to this like most frequently appeared answer and now that here one important
[2886.96s -> 2891.52s]  thing is that this selection is only based on the final answer it is not based on the
[2891.52s -> 2895.76s]  reasoning passes the model can come up with whatever reasoning passes it wants
[2895.76s -> 2899.92s]  so as long as they lead to the same final answer we consider them to be all good
[2901.12s -> 2906.08s]  this is a pretty simple idea but actually it also it puts the performance across different
[2906.08s -> 2912.88s]  models and benchmarks and here which here in their paper they also evaluate all the models
[2912.88s -> 2918.08s]  available at that time and especially on mass problems they show very significant
[2918.08s -> 2923.44s]  performance improvement now let's talk about this scaling effect again with the
[2923.44s -> 2931.60s]  self-consistency approach so for the baseline here in the curves they compare with sample and
[2931.60s -> 2936.96s]  rank baseline this is actually a pretty natural baseline which says that we can see that the
[2936.96s -> 2943.20s]  response with the highest log probability so then here the assumption is that if the model
[2943.20s -> 2947.68s]  like gives like higher probability to the current response it should be more confident about it
[2948.80s -> 2954.08s]  but actually if we look at these curves we can see that self-consistency performance scales much
[2954.08s -> 2959.68s]  better than probability based ranking so with more and more responses like like up to like
[2959.68s -> 2965.12s]  40 responses self-consistency performance is still improving but this example and rank
[2965.12s -> 2974.72s]  baseline has already stopped improving with around 10 responses already but there is an exception
[2974.72s -> 2979.92s]  so the exception is that if we train the model to be a very good verifier for the class we care
[2979.92s -> 2986.32s]  about we might be able to kind of outperform this like a scaling curve with self-consistency
[2986.32s -> 2992.08s]  and we will talk about it later another very important factor that contributes to
[2992.08s -> 2999.20s]  self-consistency success is that we need the model to like generate very diverse responses
[2999.20s -> 3005.84s]  to perform this consistency based selection so here basically in these like results they
[3005.84s -> 3011.12s]  also compare with different like baselines so the beam search is actually a very classic
[3011.12s -> 3018.88s]  search algorithm like in the nlp literature before the area so basically idea is that at each
[3018.88s -> 3024.72s]  decoding step we will keep the top k like tokens with the highest probabilities then in the
[3024.72s -> 3029.76s]  end we will we will use the like a sentence with the highest overall probability
[3032.00s -> 3037.68s]  and then they also like compare with this ensemble baselines where they apply graded
[3037.68s -> 3043.20s]  coding for like among like different variants of how you formulate the problem because earlier
[3043.20s -> 3047.36s]  we mentioned that the arms are sensitive to different problems also they compare the
[3047.36s -> 3052.08s]  performance to different ways to permute the exemplars orders in a prompt
[3052.64s -> 3057.68s]  so again they show that self-consistency using sampling scales better with more samples
[3058.64s -> 3064.56s]  so the important like factor here is that basically for the sampling master it needs to
[3064.56s -> 3070.32s]  ensure that the responses are very diverse so like we can do we can do the sampling with
[3070.32s -> 3076.16s]  a high temperature or we can perform nuclear sampling with like by setting the parameters
[3076.24s -> 3080.72s]  like by setting the parameters in the right way to enforce this diversity etc
[3081.84s -> 3087.52s]  one analysis from this work is actually pretty interesting it is related to model calibration
[3088.32s -> 3094.32s]  so this is about like actually how does self how does this consistency based selection
[3094.32s -> 3098.16s]  like improve the model performance what does it mean when the model is like
[3099.52s -> 3103.76s]  for a given task the pro the model can come up with a more consistent response
[3104.56s -> 3109.84s]  so here basically they show this curve about to show the correlation between like the accuracy
[3109.84s -> 3116.96s]  and consistency so here basically we can see that if it follows like a sample responses
[3116.96s -> 3121.76s]  if more of them lead to the same final answer this means that this consistency percentage is
[3121.76s -> 3127.68s]  higher then there are two like um conclusions we can see from here why is that this data
[3127.68s -> 3133.12s]  model is more certain with its final predicted conclusion even if they are like coming up with
[3133.44s -> 3138.88s]  the same conclusion with different reasoning passes and empirically we see that this aggregated
[3138.88s -> 3144.88s]  solution for this like more consistent responses can be more likely to be accurate the final
[3144.88s -> 3150.96s]  solution can can just be the right one for the original has problem so this can also explain
[3150.96s -> 3156.48s]  why this like consistency can be a very simple but very powerful criterion to select the final
[3156.48s -> 3162.48s]  solution so in a self-consistency paper they demonstrate this like um approach on those like a
[3162.48s -> 3168.48s]  test-based reasoning problems and actually this consistency based selection approach is also
[3168.48s -> 3175.52s]  very powerful for code generation and in our alpha code work um at google at google demight
[3176.56s -> 3181.20s]  basically this alpha core system itself is a pretty like a complicated one it includes
[3181.20s -> 3187.52s]  that model training and inference time techniques but here i will i will mainly focus on this
[3187.52s -> 3193.12s]  like inference stage called filtering and clustering and especially this clustering is
[3193.12s -> 3199.92s]  basically tries to like um do this like a code selection based on the consistency on execution
[3199.92s -> 3208.48s]  results so let's first briefly discuss about the background the problem setting of competitive
[3208.48s -> 3213.76s]  programming so basically in a competitive programming problem the it uh in the problem
[3213.76s -> 3218.32s]  description it will include a very long and complicated text to tell about the background
[3218.32s -> 3223.52s]  story about what we want for the problem but for the corresponding code and then also
[3223.52s -> 3230.80s]  includes a few improbable pairs as test cases then for the solution to be considered correct
[3230.80s -> 3236.48s]  the generated code needs to pass both the given test cases in the prompt and also usually
[3236.48s -> 3242.08s]  we will have some product test cases and these test cases are typically like of larger
[3242.08s -> 3247.68s]  scale and they will be more diff more difficult than this like example test cases and they will
[3247.68s -> 3253.68s]  cover some corner cases so since we already have some improv examples we can filter the
[3253.68s -> 3258.64s]  programs that fail the given test cases already but for the remaining programs they
[3258.64s -> 3263.68s]  might still fail on the hurdle test cases like for those of who of those of you who have
[3263.68s -> 3268.56s]  like participated in these like um coding contests you might feel the same way you feel that
[3268.56s -> 3273.76s]  your code is great but it just fail on some mysterious hold down test cases and you didn't
[3273.76s -> 3279.84s]  get the score you want so basically for this like for the alpha code system what we do is
[3279.84s -> 3285.68s]  that we will train another model to generate new test inputs for these test problems
[3287.52s -> 3293.20s]  then we will execute those like sample programs on all these like model generated inputs and then
[3293.20s -> 3300.08s]  we cluster all programs with the same outputs together the assumption here is basically if we
[3300.08s -> 3306.00s]  have the model to generate like enough that number of test inputs and also they are diverse
[3306.00s -> 3311.20s]  of of higher quality then actually we can assume that all programs in the same cluster
[3311.20s -> 3314.72s]  are semantically equivalent because they lead to the same execution results
[3317.28s -> 3321.44s]  and then finally we sample one program from each of the 10 largest clusters
[3322.16s -> 3326.72s]  in our relation we show that this clustering provides additional performance gain compared
[3326.72s -> 3331.84s]  to the filter only method but clearly there are still a gap from the oracle selection
[3331.84s -> 3336.88s]  because there's still no guarantee that the consistent response will be just the best one
[3336.88s -> 3343.12s]  out of the candidate pools now let's revisit this self-consistency decoding pipeline
[3343.76s -> 3347.52s]  so one limitation here is that for the original self-consistency
[3347.52s -> 3353.44s]  we need an answer extraction process so we can do this aggregation among the candidate responses
[3354.72s -> 3358.48s]  so if you want to like apply this like consistency based decoding to
[3358.48s -> 3365.20s]  like kind of broader use cases the natural next question is how we can do it for free
[3365.20s -> 3370.32s]  phone generation where it is not that natural to have this aggregation and answer extraction
[3370.32s -> 3376.16s]  process so this is what we want to achieve with our work on universal self-consistency
[3376.72s -> 3380.40s]  the idea is that instead of having this like answer extraction process
[3381.20s -> 3386.00s]  we ask the large range model to perform consistency based selection so specifically
[3386.00s -> 3390.96s]  we threw an instruction to the model asking it to select the most consistent response
[3390.96s -> 3395.92s]  based on the majority consensus and also it needs to take a look at all these candidate responses
[3397.04s -> 3401.52s]  so the intuition here is that clearly we know that it will be hard for the model to judge
[3401.52s -> 3406.80s]  the answer correctness by itself but consistency should be a simpler criterion to measure
[3409.68s -> 3413.92s]  then empirically we also evaluate this universal self-consistency method for different
[3413.92s -> 3419.92s]  applications so for those like free phone generation problems like summarization and
[3419.92s -> 3426.00s]  question answering or with with like of with like low factual like um final answers
[3426.00s -> 3431.92s]  we see that the universal self-consistency also still can improve over the baselines
[3431.92s -> 3435.12s]  where the original self-consistency is not directly applicable
[3437.12s -> 3441.36s]  and on mass reasoning and coding problems where we can apply this original
[3441.36s -> 3446.64s]  self-consistency idea actually universal self-consistency can match its performance
[3446.64s -> 3450.64s]  without the requirement of performing this answer extraction and code execution
[3450.64s -> 3457.84s]  and uh not just here clearly this universal self-consistency performance is bounded by
[3457.84s -> 3463.28s]  the long-context capability so at the time we are using a like an earlier model which
[3463.28s -> 3468.88s]  is not super great at long-context reasoning so we can see that when we include more responses
[3468.88s -> 3475.76s]  in the in the prompt the performance might not go like up as like well as the original
[3475.76s -> 3481.36s]  self-consistency but we can see that with the recent like development of long-context models
[3481.36s -> 3487.12s]  the least like will allow such kind of a model based selection approach to scale more with more
[3487.12s -> 3492.96s]  responses and also in practice for most like tasks it can be well solved within let's say
[3492.96s -> 3498.80s]  10 responses in a candidate pool so this will still showcase the least like usability of this
[3498.80s -> 3505.28s]  model based consistency selection previously we talked about like for the model based like a
[3505.28s -> 3512.00s]  selection using the log probability from the original model this approach of ranking doesn't
[3512.00s -> 3518.40s]  scale better than the consistency based selection so to improve further over this consistency based
[3518.40s -> 3524.88s]  selection one idea is that we can train a large range model as the ranker and we hope that at
[3524.88s -> 3531.36s]  least this ranker can perform better than this like a simple consistency criteria and
[3531.36s -> 3537.12s]  actually it can have some like a kind of sense of which responses are more likely to be accurate
[3538.24s -> 3543.68s]  so here on the slides I'm showing this like this like a kind of diagram from the original
[3543.68s -> 3550.24s]  OPM paper which introduced the GSM-AK data set and also the and also let's demonstrate this
[3550.56s -> 3556.56s]  method of how to train a verifier to show the to basically judge the correctness of the math
[3556.56s -> 3561.52s]  solution and also like later on they have they have this for our paper called let's verify
[3561.52s -> 3567.12s]  step by step so in total basically for this like line of error based ranking works
[3567.12s -> 3572.88s]  there are two pairs there are two ways to basically design the error based method to
[3572.88s -> 3579.28s]  judge the answer quality the first way is the most like in the initial and the most standard
[3579.28s -> 3584.24s]  approach which is called outcome supervised reward model basically it will verify the
[3584.24s -> 3589.36s]  correctness at the solution level like given a math problem the solution the model will tell
[3589.36s -> 3597.84s]  you whether the solution is correct or not another like approach introduced in this let's
[3597.84s -> 3603.76s]  verify step-by-step paper is called process supervised reward model so the idea is that
[3603.84s -> 3609.52s]  instead of verifying at the solution space we can also verify at a step level we can see whether
[3609.52s -> 3617.20s]  each step is accurate so in their inner work basically they show that this like a process
[3617.20s -> 3622.80s]  supervised reward model actually scales better with more samples compared to this outcome
[3622.80s -> 3629.68s]  based reward model and also this majority voting base time but clearly here the performance will
[3629.68s -> 3635.52s]  be highly dependent on the verifier quality and also the same verifier might not like generalize
[3635.52s -> 3640.48s]  across tasks because we know that this consistency based criteria is very simple and actually can be
[3640.48s -> 3645.68s]  directly applied to many tasks but for these dranker models you need to design a good training
[3645.68s -> 3650.96s]  like a strategy and also design a good training data so that it can really like work across
[3650.96s -> 3656.56s]  different domains you care about so far we have talked about response selection only at the
[3656.56s -> 3661.84s]  like solution level so we only perform this section after the full responses are generated
[3663.04s -> 3669.04s]  but assuming that we have a very good stepwise scholar then actually this like solution-based
[3669.04s -> 3674.40s]  response selection is kind of a waste of like um using this step by scholar because actually we
[3674.40s -> 3682.16s]  can we can do more with such kind of a more powerful and more fine-grained verifier so if
[3682.16s -> 3687.44s]  we already have this a good like stepwise scholar function then actually we can perform this like a
[3687.44s -> 3693.52s]  tree search with darjans model we are basically instead of waiting until the solution becomes
[3693.52s -> 3699.36s]  complete during the search process we can just we can prioritize those exploration of like
[3699.36s -> 3705.44s]  partial solutions at the step level and we can explore those more promising steps before exploring
[3705.44s -> 3714.08s]  other passes so on the slides i'm showing these like a like a least like um diagram from the
[3714.08s -> 3719.28s]  trail of salt prompting paper so from this illustration you can clearly see what's what
[3719.28s -> 3724.00s]  are the differences of different like um prompting methods like channel so prompting is basically
[3724.00s -> 3729.60s]  a trend this self-consistency measure is like multiple parallel trends and then for this
[3729.60s -> 3735.12s]  like tree of salt basically at each step it can have multiple branches to explore the next
[3735.12s -> 3742.08s]  step and you don't have to complete all these that passes until the end so here i will show
[3742.08s -> 3747.28s]  this like how we can apply this like a trail so idea with a simple example of gang of 24
[3747.84s -> 3754.40s]  so basically at each step there are two important like stages we want to do one is called salt
[3754.40s -> 3760.16s]  generation basically this step will try to we will prompt the large range model to propose
[3760.16s -> 3765.84s]  possible next sinking steps like in the count of 24 these like sinking steps are pretty like
[3765.84s -> 3769.92s]  simple in format you just need to select the two numbers to perform this calculation
[3771.76s -> 3776.80s]  then there's another stage called salt evaluation where again here we come from the large range
[3776.80s -> 3782.24s]  model to evaluate how promising the current state is like for the for the count of 24 you need to
[3782.24s -> 3788.96s]  decide whether it is possible to reach the final number of 24 after the these all the manipulations
[3790.16s -> 3796.72s]  so previously we show that how we we can show how we can like do this that evaluation at the
[3796.72s -> 3803.28s]  at the step level so it will determine the broad quality of each individual step but
[3803.28s -> 3808.80s]  related to our previous like a consistency based selection discussion for this like scoring
[3808.80s -> 3813.92s]  a function we can also think about it as a way to just select the best state among the
[3813.92s -> 3819.12s]  candidates we don't need to just have the model to make this like decision at the point level
[3819.76s -> 3825.36s]  so idea is that basically we can ask that large range model to like select the best
[3825.36s -> 3830.96s]  responses among all these like possible next like steps and then basically we can also do
[3830.96s -> 3836.56s]  this voting multiple times because since all this like selection here is based on large
[3836.56s -> 3841.68s]  range model sampling then with all these like different votes the model can finally like select
[3842.16s -> 3844.88s]  the majority vote as the final choice to proceed
[3848.32s -> 3853.92s]  here for the results i'm showing the list again of 24 evaluation and in the original paper they
[3853.92s -> 3860.16s]  also show some other benchmarks so we can see that with trail of thought using breadth-first
[3860.16s -> 3865.04s]  search it scales better than the standard prompting and trial of thought with respect to
[3865.04s -> 3873.44s]  token budget and going further clearly we can integrate some more advanced search algorithms
[3873.44s -> 3880.24s]  like in this original chart so paper they just they use the most basic bfs and dfs algorithms but
[3880.24s -> 3885.44s]  in the later on we also see the some like papers which are coming up with these multi-color
[3885.44s -> 3890.24s]  tree search methods and some works are also trying to train a separate evaluation function
[3890.24s -> 3896.00s]  instead of just prompting the model again here the important consideration is that we
[3896.00s -> 3901.44s]  need to design a good language model and prompting scheme so that it can be effective
[3901.44s -> 3910.40s]  for self evaluation so to summarize in this second part i i saw that we can further scale
[3910.40s -> 3914.88s]  this inference time compute by sampling multiple branches in the solution space
[3915.84s -> 3921.44s]  and we talk about consistency based selection which is a pretty simple but effective and general
[3921.44s -> 3928.00s]  principle we talk about this self consistency where we can marginalize out reasoning passes
[3928.00s -> 3933.44s]  and do the selection based on the final answer and for co-generation we can also do this
[3933.44s -> 3941.36s]  re-ranking based on execution consistency when this large-range model self evaluation works well
[3942.08s -> 3947.76s]  so that it can really judge the quality of partial solutions or structure quality at the
[3947.76s -> 3953.84s]  step level we can also apply this as such method in this case sometimes it will scale
[3953.84s -> 3959.76s]  better and also you can reduce the token cost because it can already eliminate those
[3959.76s -> 3965.28s]  non-promising passes during the meantime so finally i will talk about iterative cell
[3965.28s -> 3969.28s]  improvements which will try to increase the depth to reach the final solution
[3972.24s -> 3977.36s]  so even with the most recent like advanced models including those reasoning models
[3977.36s -> 3982.80s]  we can see that they can achieve pretty amazing like numbers on those competitive
[3982.80s -> 3988.80s]  like challenging benchmarks for solving hard math and coding questions but they can still
[3988.80s -> 3994.00s]  sometimes make pretty obvious mistakes and people are keeping complaining about it but
[3994.00s -> 3999.12s]  if you think about us as humans we also tend to make sometimes pretty trivial mistakes
[3999.52s -> 4008.08s]  at first thought so previously we talk about we can sample multiple solutions to help us reduce
[4008.08s -> 4013.84s]  these mistakes from a single prediction but if we think about again it is actually a pretty
[4013.84s -> 4019.68s]  sub-optimal way to do this error correction because all these responses are generated in
[4019.68s -> 4024.88s]  parallel so we do not really learn from the past mistakes we can just come having the model
[4024.88s -> 4028.24s]  generate the same wrong predictions again and again
[4031.68s -> 4037.28s]  so basically in this part about inference time cell improvement i will talk about techniques
[4037.28s -> 4044.16s]  using our models to iteratively improve its own response for the given task and also
[4044.16s -> 4048.40s]  this will align better with us humans like error correction process
[4048.40s -> 4058.16s]  so the first papers about these ideas include this like reflection and self-refine so basically
[4058.16s -> 4063.44s]  in this process there are also like two important steps after generating each solution
[4064.64s -> 4070.00s]  so basically after the model generates a solution the model will also generate some feedback based on
[4070.00s -> 4074.88s]  what it observes in this step it can use those external evaluation when available
[4075.52s -> 4081.28s]  so for example in the original reflection paper they focus more on this agentic setup where
[4081.28s -> 4086.24s]  basically the arm as the agent will propose actions to the environment and the environment
[4086.24s -> 4092.56s]  will basically like put like gives the model some kind of like changes of the observations
[4092.56s -> 4097.12s]  so this will be external signal to tell the model whether the current step is promising
[4098.40s -> 4103.92s]  then based on this like model generating reflection feedback and also these external observations
[4104.88s -> 4111.04s]  then this large-range model will refine its like output to revise its next prediction step
[4111.04s -> 4118.32s]  given both these internal and external signals in the in the least reflection paper they basically
[4118.32s -> 4124.16s]  show that this self-reflection and self-refinement works pretty well especially when we can have a
[4124.16s -> 4130.32s]  scheme so that so that basically the model can utilize pretty high quality evaluation or
[4130.32s -> 4138.08s]  when the external signals are very reliable so for example in the case of AFF world it is
[4138.08s -> 4144.72s]  basically a navigation task so there will be clear signals about observations at each step
[4146.08s -> 4152.88s]  and they also evaluate on their approach on hop-hop QA but they kind of repurpose this like
[4152.88s -> 4158.88s]  QA task into an agent environment in the sense that after the model generates the solution they
[4158.88s -> 4163.12s]  assume that this environment will give a signal about whether the current solution is correct or
[4163.12s -> 4169.68s]  not then this is why you can keep seeing this like improvement with more trials we will revisit
[4169.68s -> 4178.32s]  this assumption later and if we think about this like self-improvement loop for applications
[4178.32s -> 4184.48s]  then code generation is actually a pretty natural use case because of when we are writing code
[4185.28s -> 4190.80s]  for us for example we also like debug our code better we see an IDE and we do not just write
[4190.80s -> 4195.84s]  the code and we stop we actually really rely on this like interactive loop when we are trying
[4195.84s -> 4202.16s]  to like investigate all these code execution results and see whether it makes sense and also
[4202.16s -> 4207.92s]  which parts we should revise later and this is also like this one fundamental idea of the
[4207.92s -> 4216.24s]  recent coding agents so in our paper self-debugging we also like do a deeper dive into what kind of
[4216.24s -> 4222.88s]  feedback formats will be helpful for the model to better correct its own mistakes so for example
[4222.88s -> 4230.64s]  the most simple feedback message is basically a short universal information about whether
[4230.64s -> 4236.40s]  current code is correct or not without more information and then for unit test feedback
[4236.40s -> 4242.24s]  basically we also include the execution results like whether the current unit test has passed
[4242.24s -> 4249.28s]  or whether there are any runtime errors then for co-explanation basically at each step we
[4249.28s -> 4254.80s]  also ask the model to self-generate some line-by-line explanation of the implementation in
[4254.80s -> 4260.56s]  natural language so this idea is that kind of motivated by human robot debugging trying to
[4260.56s -> 4264.16s]  ask the model to see like what is trying to do in a code
[4266.56s -> 4272.00s]  and finally in a trace feedback basically it we ask the model to perform some line-by-line
[4272.00s -> 4277.84s]  simulation of the execution traced by itself this is also one way of debugging by human programmers
[4280.88s -> 4287.28s]  empirically we evaluated the like the like latest large-range models at the time of paper
[4287.28s -> 4292.08s]  publication last year so basically we can see that with this self-debugging loop
[4292.08s -> 4296.24s]  it consistently improves performance across different large-range models of different
[4296.24s -> 4301.44s]  capabilities and also if we give more informative feedback it can further improve the
[4301.44s -> 4310.08s]  debugging performance now let's go back to this like self-correction for question answering
[4310.08s -> 4316.88s]  tasks previously we talk about these results of hop-hop QA and also in some other words it can
[4316.88s -> 4323.68s]  it also show this improvement of self-question on other reasoning tasks like mass problem solving
[4324.40s -> 4330.88s]  but all of this significant improvement comes from the assumption with an oracle verifier
[4333.36s -> 4339.76s]  but in practice this oracle verifier will not be available in many scenarios especially in the
[4339.76s -> 4344.64s]  case of solving reasoning problems you can imagine that when you are taking a mass test
[4344.64s -> 4349.36s]  you don't really have the official solution at hand otherwise it will be just trivial for you to
[4349.36s -> 4355.20s]  finish the exam so basically the question here we want to study is how do these large-range
[4355.20s -> 4361.44s]  models perform if we do not give the model such very like explicit and accurate external
[4361.44s -> 4363.60s]  feedback about the response correctness
[4366.48s -> 4372.24s]  so in our paper published last year at iClear called large-range models cannot self-correct
[4372.24s -> 4379.28s]  reasoning yet we actually show some negative results from this study so we compare with this
[4379.28s -> 4384.32s]  oracle like baseline where we basically utilize the ground truth answer for question
[4384.32s -> 4388.00s]  and same as previous works we show this huge improvement
[4388.56s -> 4394.32s]  then we study the case of the real self-question where basically we do not allow the model to
[4394.32s -> 4399.12s]  utilize any oracle feedback the model needs to judge the response correctness by themselves
[4400.24s -> 4405.52s]  then these are the results we show in the bottom table we can see that the main issue here
[4405.52s -> 4410.32s]  is that because large-range models can really judge the correctness of its predictions
[4410.32s -> 4416.08s]  it often turns an originally a correct solution into a wrong one so this is why actually
[4416.32s -> 4423.20s]  so this is why actually in this like self-question without external evaluation this process can lead
[4423.20s -> 4432.96s]  to worse performance after each round so with this study one natural question is probably we
[4432.96s -> 4438.64s]  didn't tune the feedback prompt well so this is why it fails so to address this question we
[4439.60s -> 4447.20s]  also designed several variants of this feedback prompt and notice that all these feedback
[4447.20s -> 4452.80s]  problems are pretty general they are not like specifically tuned for one task so we can see
[4452.80s -> 4457.52s]  that if we just have this like simple general purpose feedback prompt if we do not have any
[4457.52s -> 4463.20s]  more like of the model training then editing this feedback prompt indeed can affect the
[4463.20s -> 4468.72s]  self-question behavior but this is more about adjusting the tendency for the model
[4468.72s -> 4475.76s]  to keep the initial response so in some cases actually this like a feedback after the feedback
[4475.76s -> 4480.64s]  prompt update there's no much regression on the reasoning performance but then in this case
[4481.28s -> 4485.12s]  this also doesn't help the model to improve over the initial performance
[4485.76s -> 4490.64s]  so previously we discussed about this comparison if we just do this self-question over one
[4490.64s -> 4496.08s]  initial response and we further like compare with another method where basically we do this
[4496.08s -> 4501.92s]  like self-question among multiple responses so for this we compare with the multi-agent debate
[4501.92s -> 4507.28s]  baseline the idea is that we will we will have the model generate like several responses
[4507.28s -> 4512.96s]  in parallel ones then we prompt the large-range model to review these multiple responses and give
[4512.96s -> 4519.36s]  an updated one and for this multi-agent debate we will compare with self-consistency
[4519.36s -> 4523.20s]  recall if self-consistency is just completely generated solutions in parallel
[4523.20s -> 4530.24s]  and select the response with the most common final answer so in the previous multi-agent
[4530.24s -> 4535.20s]  debate papers they are showing some improvement over self-consistency but we find that the
[4535.20s -> 4540.96s]  major issue is that they do not like have a very rigorous comparison in terms of the token
[4540.96s -> 4547.68s]  budget so actually if we because in the multi-agent debate each round for this specific experiment
[4547.68s -> 4554.88s]  has like three responses but basically if we keep the same number of response budgets
[4554.88s -> 4560.56s]  we will see that actually still like self-consistency scales better than the multi-agent
[4560.56s -> 4566.24s]  debate because at some point like this again this multi-agent debate performance stops improving
[4566.24s -> 4571.04s]  but with more responses self-consistency performance is still like improving on this
[4572.00s -> 4573.28s]  mass problem benchmark
[4576.96s -> 4581.20s]  so giving all this discussion then the question here is that how we can
[4581.20s -> 4585.44s]  best utilize the token budget by integrating all these different methods together
[4586.88s -> 4592.24s]  so the fundamental question is here basically we need to understand how to balance the
[4592.24s -> 4596.72s]  inference budget for generating multiple samples in parallel or sequentially
[4597.28s -> 4602.96s]  so actually this is a pretty difficult question because it really depends on the task and the
[4602.96s -> 4610.24s]  model and basically in essence it's about like whether the model for evaluation is good at
[4610.24s -> 4617.44s]  self-reflection and correction for your specific task so here i'm showing an example study from
[4617.44s -> 4622.40s]  my colleagues at google where they are trying to like basically in these specific experiments
[4622.40s -> 4627.84s]  they are trying to find the optimal way to scale this token budget to solve the mass problems
[4627.84s -> 4633.84s]  so actually here they train a model which is supposed to be more specialized at correcting
[4633.84s -> 4640.88s]  mass solutions and also basically here for for the different difficulty level problems
[4640.88s -> 4646.72s]  they study what's the optimal ratio to like perform this parallel generation versus
[4646.72s -> 4653.04s]  parallel generation versus sequential generation so i will say this that concrete numbers will
[4653.04s -> 4658.48s]  really vary a lot among different models but basically here the overall conclusion is that
[4659.20s -> 4665.04s]  for for simple problems it like like actually the model can benefit more from self-question
[4665.04s -> 4670.80s]  because it knows more clear about where the current solution is wrong and how it should revise
[4670.80s -> 4677.28s]  but for harder problems actually there is a better point in like in the middle about
[4677.28s -> 4681.52s]  how much you should perform parallel generation and also this like sequential refinement
[4683.20s -> 4688.96s]  and then with this like kind of compute optimal like factor for different like
[4688.96s -> 4693.28s]  task difficulty of problems they show that this like compute optimal curve
[4693.28s -> 4696.96s]  can also scale better than this complete parallel generation
[4696.96s -> 4701.52s]  and when we think about inference time compute another factor we should consider is about the
[4701.52s -> 4707.84s]  model size because intuitively with the same false budget we can sample much more solutions
[4707.84s -> 4714.00s]  from a lighter model so if a if a like more expensive model does not perform much better than
[4714.00s -> 4719.76s]  a cheaper model then it will not be a very optimal way to do the inference by calling the
[4719.76s -> 4723.60s]  very optimal way to do the inference by calling the more expensive model
[4726.24s -> 4732.16s]  so basically from this like empirical like a study from the paper inference scaling loss
[4732.72s -> 4737.52s]  so basically again they will show that with the optimal model with different inference like
[4737.52s -> 4741.68s]  different inference budget can be like a different like when you have like a smaller
[4741.68s -> 4746.56s]  inference time budget maybe you want to call rely more on the like lighter models but
[4746.56s -> 4750.88s]  basically when you have more budget probably you can utilize the more expensive models which
[4750.88s -> 4755.28s]  can probably give you some boost on those very challenging problems where for with the
[4755.28s -> 4759.92s]  smaller models it might like it will be very hard to come up with the final solution anyway
[4763.52s -> 4768.08s]  so basically in this lecture we have covered like different ways to to perform this like
[4768.08s -> 4773.60s]  inference time scaling with like the more token budget so we talk about the basic prompting
[4773.60s -> 4779.68s]  techniques about this like search and selection to increase the ways of exploring the solution
[4779.68s -> 4787.20s]  space and also we talk about iterative cell with improvement and again the best practice
[4787.20s -> 4792.64s]  to interact with a large-edge model should be adapted according to its capabilities so you
[4792.64s -> 4797.28s]  really need to consider the underlying model capability and also the tasks in your hand
[4797.28s -> 4801.36s]  about to think about how you can best like combine all these like different techniques
[4801.36s -> 4807.44s]  and design a more like efficient and effective paradigm for your task
[4809.68s -> 4815.12s]  and finally i also want to talk about the general principle of how to design effective reasoning
[4815.12s -> 4819.92s]  techniques and this actually will be helpful for both inference time techniques and training
[4819.92s -> 4825.84s]  time techniques so i guess many of you might have heard about this beta lesson from
[4825.84s -> 4831.84s]  Richard Sutton but if not i highly encourage you to read this like a web page actually i would
[4831.84s -> 4837.44s]  say this is a very important guideline of for many of them like this like modern machine
[4837.44s -> 4842.64s]  learning works and especially with the recent development of large-edge models and reasoning
[4842.64s -> 4849.36s]  techniques so this like article itself has a lot of like tests but i will quote some most
[4849.36s -> 4855.28s]  interesting like a text in my opinion here so basically what we really want to develop as a
[4855.28s -> 4863.12s]  method is basically those that can continue to scale with increased computation and when we
[4863.12s -> 4869.60s]  want to like improve the AI agents we want to like basically teach the model to discover like
[4869.60s -> 4875.84s]  what we can't not which content what we have discovered so it is more about like having some
[4875.84s -> 4882.72s]  like a method so that we can we can kind of trigger the model to know how it can like keep
[4882.72s -> 4887.44s]  improving itself and how to like have the right way to perform the reasoning
[4888.16s -> 4891.76s]  so this is the end of this lecture thanks for listening
