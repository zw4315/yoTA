# Detected language: en (p=1.00)

[0.00s -> 2.90s]  And hey, it's great to see you here.
[2.90s -> 6.30s]  I'm Caillou, and I will talk about formal reasoning
[6.30s -> 12.06s]  meets ALMs towards AI for mathematics and verification.
[12.06s -> 16.62s]  OK, so we are now kind of in an arm race of large language
[16.62s -> 22.38s]  models in which every leading company releases a new model
[22.38s -> 25.02s]  in every few months.
[25.02s -> 29.86s]  So at the center of this arm race is math and coding.
[29.88s -> 33.24s]  For example, when OpenAI introduces O1,
[33.24s -> 35.76s]  you typically see this kind of chart
[35.76s -> 40.18s]  showing that its capability on various benchmarks.
[40.18s -> 44.32s]  And you see the math competition like AI-ME
[44.32s -> 46.88s]  and the coding competition like CodeForces
[46.88s -> 48.94s]  usually highlighted.
[48.94s -> 51.84s]  And similarly, when Google released Gemini
[51.84s -> 55.60s]  and when XAI released Grok, they also
[55.60s -> 57.84s]  highlight the capability in math and coding.
[60.28s -> 70.36s]  And in November 2023, XTX Markets, which is a British hedge fund,
[70.36s -> 72.64s]  launched the AI MO prize.
[72.64s -> 78.16s]  That's $10 million for AI that can win a gold medal in IMO.
[78.16s -> 82.00s]  And not long after that, in July 2024,
[82.00s -> 86.64s]  Google DeepMind announced that AlphaProof and AlphaGeometry
[86.64s -> 89.28s]  have achieved the silver medal level.
[89.30s -> 93.38s]  I guess that's the content of the last lecture.
[93.38s -> 97.54s]  And if we want to go beyond high school competition math,
[97.54s -> 100.14s]  we have this Apple AI.
[100.14s -> 102.62s]  It introduced a benchmark called
[102.62s -> 106.90s]  Frontier Maths, which includes research level
[106.90s -> 109.10s]  mathematical questions that's curated
[109.10s -> 112.82s]  by professional mathematicians.
[112.82s -> 116.34s]  And not long after that, OpenAI 03
[116.36s -> 119.52s]  announced that it can solve more than 20%
[119.52s -> 122.00s]  of that, which came as a surprise.
[122.00s -> 125.60s]  Because when Frontier Maths benchmark was released,
[125.60s -> 130.36s]  people thought, OK, it's going to take a few years
[130.36s -> 133.08s]  for language models to make real progress.
[133.08s -> 135.36s]  But really, after less than one year,
[135.36s -> 139.88s]  we have groundbreaking results from OpenAI.
[139.88s -> 144.24s]  So why are people interested in math and coding?
[144.26s -> 146.90s]  Why are they often highlighted?
[146.90s -> 149.70s]  One reason is that math and coding
[149.70s -> 153.72s]  are proxies for complex reasoning and planning.
[153.72s -> 155.58s]  And reasoning and planning, they're
[155.58s -> 158.06s]  not only important for human intelligence,
[158.06s -> 161.06s]  but they are also one of the major challenges
[161.06s -> 164.16s]  for current language models.
[164.16s -> 167.28s]  And they can really unlock unlimited applications
[167.28s -> 170.90s]  if you can make language models be able to reason
[170.90s -> 172.26s]  and be able to plan.
[172.28s -> 175.86s]  For example, you can have them do travel planning for you
[175.86s -> 181.92s]  and also have language model agent scheduling calendar
[181.92s -> 184.28s]  invites for you.
[184.28s -> 185.72s]  And the second reason that people
[185.72s -> 187.72s]  are interested in math and coding
[187.72s -> 191.28s]  is they are relatively easy to evaluate.
[191.28s -> 193.40s]  For a math problem, you can check
[193.40s -> 195.24s]  if the answer is correct.
[195.24s -> 197.36s]  And for coding, you can run the unit test
[197.36s -> 200.12s]  to see if they can pass all the tests.
[200.12s -> 201.66s]  But if you ask your language model
[201.66s -> 205.28s]  to write a crime fiction, or you
[205.28s -> 208.08s]  ask it to compose a symphony, it's
[208.08s -> 211.48s]  going to be much, much harder to evaluate.
[211.48s -> 217.24s]  You can argue that crime fiction also involves reasoning.
[217.24s -> 220.56s]  And composing a symphony definitely needs planning.
[220.56s -> 222.32s]  So they also need reasoning and planning.
[222.32s -> 227.96s]  But they are just not as easy to evaluate.
[227.96s -> 230.72s]  So I put it relatively easy here
[230.72s -> 233.54s]  because, as we will come back, we
[233.54s -> 236.14s]  don't think checking the answers
[236.14s -> 239.06s]  or running the unit test is a perfect evaluation.
[239.06s -> 242.06s]  We're just arguing that it's a reasonable evaluation.
[242.06s -> 244.62s]  And we will revisit this point later
[244.62s -> 246.26s]  in the rest of this lecture.
[249.02s -> 251.54s]  So we know language models are racing
[251.54s -> 254.90s]  to be good at math and coding.
[254.90s -> 258.46s]  And next, we want to briefly introduce the main techniques
[258.46s -> 259.34s]  behind them.
[259.36s -> 263.84s]  How do we train language models to solve math problems?
[263.84s -> 265.08s]  Here, we focus on math.
[265.08s -> 269.52s]  But the discussion largely also applies to coding as well.
[269.52s -> 272.08s]  And we will see later that math and coding
[272.08s -> 274.92s]  are deeply connected.
[274.92s -> 277.28s]  And state-of-the-art math language models
[277.28s -> 280.44s]  are trained using mainly two techniques.
[280.44s -> 283.36s]  One is supervised fine-tuning, or SFT.
[283.36s -> 287.12s]  And the other is reinforcement learning, or RL.
[287.14s -> 289.34s]  We're going to present a very high-level view
[289.34s -> 296.00s]  and emphasizing the need for data and for verifiability.
[296.00s -> 299.10s]  Starting with SFT, so in SFT, you
[299.10s -> 303.44s]  have a base model that's a generic language
[303.44s -> 307.82s]  model which ends on internet-scale data sets.
[307.82s -> 311.02s]  And to learn mathematics, you give it additional data
[311.02s -> 313.70s]  that's focused on mathematics.
[313.70s -> 317.02s]  So you can grab web pages from math overflow
[317.02s -> 321.72s]  or you can have papers from archive.
[321.72s -> 323.96s]  And the result of this fine-tuning
[323.96s -> 327.58s]  is called the base math language model.
[327.58s -> 329.60s]  Well, it has seen a lot of mathematics,
[329.60s -> 334.36s]  but it still hasn't yet learned how to solve problems.
[334.36s -> 338.36s]  So the next step is to give it a lot of problems
[338.36s -> 344.32s]  and with detailed step-by-step solutions for these problems.
[344.32s -> 347.06s]  And optionally, in the solution,
[347.06s -> 350.06s]  you can use external tools.
[350.06s -> 353.14s]  So here, the model is trained to produce
[353.14s -> 354.74s]  not only natural language reasoning,
[354.74s -> 359.34s]  but also it can call external tools like Python.
[359.34s -> 362.80s]  So this is an example of a problem it can solve.
[366.70s -> 370.30s]  So the most important thing in this pipeline, as you can see,
[370.30s -> 372.98s]  is data set.
[373.00s -> 376.10s]  You need data set for continued pre-training.
[376.10s -> 380.76s]  That's the web documents collected from web,
[380.76s -> 383.30s]  from the mass forums like Stack Overflow.
[383.30s -> 387.00s]  And you also have to annotate a lot of data
[387.00s -> 390.64s]  for further fine-tuning consisting of problems
[390.64s -> 393.12s]  with step-by-step solution.
[393.12s -> 397.24s]  And I would say collecting this data set
[397.24s -> 400.88s]  is one of the most expensive part in this pipeline
[400.88s -> 403.58s]  because it cannot be fully automated.
[403.58s -> 407.70s]  You need human to annotate the data, to clean the data,
[407.70s -> 414.68s]  and to transform the data into the data set that you can use.
[414.68s -> 419.82s]  So what if our data set has only the final answer,
[419.82s -> 423.14s]  but not the intermediate steps?
[423.14s -> 426.34s]  This is a scenario where you don't want humans
[426.34s -> 429.86s]  to annotate data very expensively,
[429.88s -> 434.18s]  but maybe you can more cheaply get the data that
[434.18s -> 436.28s]  has only the final answer.
[436.28s -> 440.04s]  So can we still use this data set to train the model?
[440.04s -> 441.16s]  And the answer is yes.
[441.16s -> 442.72s]  We can use reinforcement learning
[442.72s -> 446.72s]  to train the model to generate solution
[446.72s -> 449.76s]  with only the problem and the final answer.
[449.76s -> 451.08s]  Here's how it works.
[451.08s -> 452.92s]  So you have the problem, and you give it
[452.92s -> 455.24s]  to the language model.
[455.24s -> 457.94s]  And the model generates a solution
[457.96s -> 460.44s]  that has intermediate steps, and that also
[460.44s -> 462.88s]  has a final solution.
[462.88s -> 466.48s]  And you can check if the final solution is correct
[466.48s -> 468.96s]  by comparing it with the ground truth.
[468.96s -> 472.96s]  So here, it's a correct solution.
[472.96s -> 475.44s]  And the model can also generate incorrect solution,
[475.44s -> 477.76s]  and you can check.
[477.76s -> 480.92s]  So the result of this verification
[480.92s -> 484.48s]  can be used as reward signal in reinforcement learning.
[484.50s -> 488.34s]  And you can use algorithms like GRPO
[488.34s -> 492.06s]  to optimize the model to achieve high rewards.
[492.06s -> 495.86s]  Well, you can treat the reward for a correct solution
[495.86s -> 498.30s]  to be 1, and if it's an incorrect solution,
[498.30s -> 500.10s]  the reward is going to be 0.
[500.10s -> 502.72s]  So this approach is a very simple approach
[502.72s -> 505.12s]  and a very powerful idea.
[505.12s -> 508.78s]  It's actually, the idea is there for several decades,
[508.78s -> 513.34s]  but it was first shown to work for training reasoning
[513.36s -> 515.86s]  language models by DeepSeq R1.
[515.86s -> 517.68s]  That's fairly recently.
[517.68s -> 521.06s]  And this idea is getting a lot of momentum.
[521.06s -> 522.94s]  Just to see how much momentum it has,
[522.94s -> 527.78s]  just one month ago, two pioneers in reinforcement learning,
[527.78s -> 531.20s]  Richard Sutton and Andrew Batto,
[531.20s -> 533.16s]  won the ACM Turing Award.
[534.90s -> 537.16s]  So here, in reinforcement learning,
[537.16s -> 540.76s]  the key is really the verifiability.
[540.78s -> 543.80s]  So you have to be able to verify the results.
[543.80s -> 545.44s]  You have to be able to tell
[545.44s -> 547.62s]  if the solution is correct or not.
[547.62s -> 549.52s]  Otherwise, you don't have the reward.
[550.40s -> 554.28s]  So if the solution has a final answer, that's a number.
[554.28s -> 556.42s]  If it has a numerical solution,
[556.42s -> 559.36s]  you can always check by comparing with the ground truth.
[559.36s -> 561.80s]  But what if it's a theorem proving problem?
[561.80s -> 564.22s]  So you are asked to generate a proof,
[564.22s -> 569.22s]  but there's really no easy way to tell
[569.66s -> 571.34s]  if the proof is correct or not.
[571.34s -> 573.64s]  Can we still do reinforcement learning?
[573.64s -> 577.40s]  We will revisit this point in the later part of this talk.
[578.54s -> 581.26s]  So here, we have covered the main techniques
[581.26s -> 584.26s]  to train language models for math,
[584.26s -> 587.06s]  supervised learning and reinforcement learning.
[587.06s -> 591.78s]  I would say any state-of-the-art math language models
[591.78s -> 596.78s]  is kind of like a combination of a very strong base model
[597.70s -> 601.26s]  and these two techniques for post-training
[601.26s -> 605.76s]  and maybe more importantly, marvelous engineering efforts.
[605.76s -> 608.26s]  So the algorithms are very straightforward.
[608.26s -> 611.34s]  Pretty much every leading industry lab
[611.34s -> 613.40s]  is using the same algorithm,
[613.40s -> 615.38s]  but what really matters is
[615.38s -> 617.14s]  who has the better infrastructure,
[617.14s -> 619.52s]  better engineering and a better data set.
[621.94s -> 626.66s]  So then we will ask, will AI soon solve mathematics?
[627.42s -> 629.46s]  Since we have this very powerful recipe.
[630.46s -> 632.22s]  And the answer is definitely no.
[632.22s -> 634.42s]  So we have a few gaps.
[634.42s -> 636.74s]  So the first gap is how do we go
[636.74s -> 640.18s]  from pre-college math to advanced math?
[641.54s -> 644.60s]  Existing successes, as we see,
[644.60s -> 648.28s]  they're most limited on pre-college math,
[648.28s -> 651.20s]  like IMO or AME.
[651.20s -> 652.72s]  And if you give language models
[652.72s -> 655.50s]  more advanced mathematical problems, they still struggle.
[655.50s -> 657.94s]  So this quote is from Terry Tao,
[657.94s -> 661.94s]  who is one of the most prominent mathematicians nowadays.
[662.90s -> 666.96s]  And he said he played with OpenAI 01.
[666.96s -> 669.22s]  And he finds they still struggle
[669.22s -> 672.40s]  with the most advanced research mathematical tasks.
[673.50s -> 676.38s]  Well, this is kind of expected
[676.38s -> 680.50s]  because mathematical research is by definition novel
[680.50s -> 682.22s]  and since it's novel,
[682.22s -> 685.02s]  you don't have a lot of data to train language models.
[686.12s -> 691.12s]  And also O3's results on frontier math,
[692.62s -> 693.92s]  as we mentioned before,
[694.94s -> 697.26s]  they are on advanced mathematical problems,
[697.26s -> 699.02s]  but they comes with a few caveats.
[700.06s -> 703.78s]  Well, the first caveat is that there are financial ties
[703.78s -> 708.78s]  between Epoch AI and OpenAI that were later disclosed.
[709.86s -> 711.82s]  But I would say more importantly,
[712.70s -> 715.34s]  there's a difference between frontier math
[716.04s -> 717.74s]  and the real mathematical research.
[717.74s -> 720.90s]  The problems in frontier math
[720.90s -> 723.70s]  are designed to have numerical solutions.
[723.70s -> 726.14s]  So they are very advanced math,
[726.14s -> 728.90s]  but they are designed to have a numerical solution.
[728.90s -> 730.62s]  So this is kind of counterintuitive
[730.62s -> 735.14s]  because we know most problems in advanced math,
[735.14s -> 737.86s]  they don't have numerical solutions.
[737.86s -> 740.82s]  But they designed the benchmark in this way
[740.82s -> 744.38s]  in order for the models to be evaluated.
[746.30s -> 748.94s]  And so this is related to the second gap.
[750.18s -> 753.42s]  Current models are very good at guessing the answer,
[753.42s -> 755.98s]  but how do we get it to write proofs?
[757.42s -> 760.46s]  And the recent study has found language models
[760.46s -> 763.46s]  still struggle in writing even very basic proofs.
[764.70s -> 766.22s]  So Putnam Bench,
[766.22s -> 770.22s]  Putnam is the mathematical competition for undergrads
[771.18s -> 775.94s]  and people recently tried using OpenAI's reasoning model
[775.94s -> 777.22s]  on Putnam.
[777.22s -> 779.26s]  And here's the results.
[779.26s -> 781.90s]  I'm very unimpressed by Chad GPT
[781.90s -> 783.86s]  in the recent Putnam exam.
[785.04s -> 788.84s]  And it can, most answers are worth one or two points
[788.84s -> 790.46s]  out of 10 points.
[790.46s -> 794.62s]  So this is checked by human mathematicians.
[794.62s -> 798.02s]  And recently in another paper,
[798.02s -> 802.46s]  researchers evaluated language models proofs
[802.46s -> 805.68s]  for the USA MO exam.
[805.68s -> 810.68s]  And they showed that all the models scored around 5%,
[810.74s -> 811.78s]  which is pretty low.
[813.38s -> 817.86s]  And the errors those language models are making can be,
[817.86s -> 819.94s]  they can be pretty easy to spot
[819.94s -> 822.58s]  once you pointed them out.
[822.58s -> 824.10s]  But if it's in a long proof,
[824.10s -> 825.74s]  it's very hard to detect them.
[825.74s -> 827.58s]  So here's the example.
[828.10s -> 831.78s]  We are asking the model to prove a very simple inequality.
[832.78s -> 835.10s]  So if ABC is satisfied some condition
[835.10s -> 838.42s]  and then there should be some, you have some results.
[838.42s -> 840.74s]  And here it's making an error.
[840.74s -> 845.34s]  When it tries to multiply an existing inequality
[845.34s -> 847.88s]  by a positive number,
[847.88s -> 851.30s]  it should preserve the direction of the inequality,
[851.30s -> 853.62s]  but it flipped the direction.
[853.62s -> 857.26s]  So that's a very simple error if you point it out.
[860.42s -> 865.28s]  So we argue that current language models,
[865.28s -> 867.96s]  the current approach for training language models,
[868.94s -> 871.96s]  it's not sufficient for solving these problems.
[872.96s -> 877.96s]  Because it relies heavily on data and on verifiability.
[878.86s -> 881.10s]  And when you work with advanced mathematics,
[881.10s -> 883.02s]  as we said, you don't have enough data.
[883.30s -> 884.90s]  And the data is very scarce.
[885.94s -> 887.38s]  And when you work with proofs,
[887.38s -> 888.94s]  you don't have verifiability.
[889.98s -> 893.10s]  And in advanced mathematics,
[893.10s -> 895.30s]  most problems require proof.
[897.74s -> 900.42s]  So if language models are not enough,
[900.42s -> 901.58s]  what else do we need?
[903.10s -> 906.50s]  In a recent position paper we released on archive,
[906.50s -> 909.58s]  we argue that one important missing point
[909.58s -> 911.62s]  is formal reasoning.
[912.18s -> 917.18s]  Here formal reasoning means you grant mathematical reasoning
[917.22s -> 918.90s]  in formal systems.
[918.90s -> 920.74s]  And formal systems they can include
[920.74s -> 923.72s]  like first-order logic, higher-order logic,
[923.72s -> 925.18s]  dependent types theory,
[925.18s -> 928.00s]  or even computer programs and their semantics.
[929.12s -> 930.94s]  And the role of formal system here
[930.94s -> 933.44s]  is to try to verify the proofs
[933.44s -> 936.14s]  and provide automatic feedback on the proofs.
[937.18s -> 940.74s]  So this is very useful because the feedback it provides
[940.74s -> 943.54s]  can mitigate data scarcity.
[943.54s -> 946.58s]  You can have the model learn from automatic feedback
[946.58s -> 949.06s]  and you don't need to collect
[949.06s -> 951.42s]  as much human-created data.
[951.42s -> 954.26s]  And second, now you can do verification
[954.26s -> 958.60s]  which enable you to evaluate the model rigorously.
[959.94s -> 962.98s]  So therefore we argue that to move forward
[962.98s -> 965.80s]  we really need to integrate formal reasoning
[965.80s -> 967.26s]  into the current approach.
[967.26s -> 972.26s]  So we're going to very briefly introduce a link
[973.40s -> 976.30s]  which is a kind of formal system
[976.30s -> 979.26s]  we focus on in this talk.
[979.26s -> 981.74s]  And it's also used in alpha proof
[981.74s -> 984.58s]  so I guess people already have some familiarity on link.
[985.62s -> 988.98s]  So here is a programming link.
[988.98s -> 992.50s]  In this program you first define natural numbers.
[992.50s -> 994.38s]  So a natural number is either zero
[994.38s -> 997.10s]  or a successor of another natural number.
[997.80s -> 999.54s]  And then you define addition between two natural numbers.
[999.54s -> 1002.00s]  So this is just a recursive function.
[1002.00s -> 1004.66s]  And then you can state and prove some theorems
[1004.66s -> 1006.66s]  about natural numbers and addition.
[1006.66s -> 1010.78s]  For example you can prove A plus B equals to B plus A.
[1012.46s -> 1016.54s]  And this here is only one file in link
[1016.54s -> 1019.90s]  and if you are using link to write formal mathematics
[1019.90s -> 1022.46s]  it's similar to using Python
[1022.46s -> 1025.90s]  or another programming language to develop software.
[1025.90s -> 1030.82s]  So you have one file and this file has a proof
[1030.82s -> 1034.10s]  and the proof it has semantics.
[1034.10s -> 1037.62s]  So you are starting, it corresponds to a proof tree.
[1038.62s -> 1041.46s]  At the root of this tree is the original theorem
[1041.46s -> 1042.48s]  you want to prove.
[1043.58s -> 1047.82s]  And every step in this proof decomposes the proof goal
[1047.82s -> 1050.84s]  into several simpler sub-goals.
[1050.84s -> 1052.46s]  And you can do that recursively
[1052.46s -> 1054.54s]  until all the sub-goals are solved.
[1055.48s -> 1057.06s]  And this is one file.
[1057.06s -> 1059.50s]  And files are organized into larger units
[1059.50s -> 1062.76s]  like a project can have many files.
[1063.98s -> 1066.90s]  And the project can be open sourced on GitHub.
[1066.90s -> 1071.30s]  For example in your project you can define real numbers.
[1071.30s -> 1073.86s]  And somewhere else project can build on your definitions
[1073.86s -> 1077.70s]  of real number and they can formalize for example
[1077.70s -> 1079.10s]  like optimization theory.
[1080.18s -> 1082.62s]  So this is similar to how you develop software.
[1083.62s -> 1088.62s]  And one example of using AI with link is alpha proof.
[1089.50s -> 1092.42s]  That's the content of the last lecture.
[1092.42s -> 1095.94s]  And it's a very successful example
[1095.94s -> 1100.70s]  and it uses link to generate a lot of theorems
[1100.70s -> 1104.90s]  and a lot of proofs and verify those proofs
[1104.90s -> 1107.94s]  and do search and reinforce learning to learn the model.
[1108.78s -> 1111.62s]  So in the rest of this lecture we will be talking
[1111.66s -> 1116.66s]  about mainly two tasks at the center of using link and AI.
[1118.74s -> 1120.84s]  So first is theorem proving.
[1120.84s -> 1125.24s]  In theorem proving your input is a theorem statement.
[1125.24s -> 1128.78s]  And you want the model to generate a proof.
[1128.78s -> 1130.66s]  Here both the statement and the proof
[1130.66s -> 1133.12s]  are written formally in link.
[1133.12s -> 1136.02s]  And the link can check if the proof is correct or not.
[1136.02s -> 1138.52s]  So there's no room for the model to hallucinate.
[1138.88s -> 1142.48s]  And the other task is auto-formalization.
[1142.48s -> 1145.56s]  In auto-formalization you have the informal mathematics
[1145.56s -> 1147.32s]  for example from a textbook.
[1147.32s -> 1149.48s]  And you want to use your model
[1149.48s -> 1151.92s]  to translate it into formal mathematics.
[1153.60s -> 1157.08s]  So next we will be first focusing on theorem proving.
[1158.76s -> 1161.00s]  A very straightforward way to do theorem proving
[1161.00s -> 1164.36s]  is you just give the model what you want to prove
[1164.36s -> 1168.22s]  and you ask the model to generate a step in the proof.
[1168.78s -> 1170.90s]  Or you can ask it to generate the entire proof.
[1170.90s -> 1175.12s]  And well because if you can generate the next step
[1175.12s -> 1179.02s]  you can kind of using search to chain steps together
[1179.02s -> 1180.70s]  into a complete proof.
[1180.70s -> 1184.42s]  So I would say like the central problem here
[1184.42s -> 1186.46s]  is how do you generate the next step?
[1189.40s -> 1191.94s]  And a very straightforward solution is
[1191.94s -> 1194.48s]  okay we have human written form of proofs.
[1195.48s -> 1199.64s]  And we can train a model to do supervised learning
[1199.64s -> 1202.04s]  on those data so that the model can learn
[1202.04s -> 1204.12s]  how to generate the next step.
[1204.12s -> 1208.12s]  And this idea has been actually here for a while.
[1210.40s -> 1213.64s]  And earlier work used classical machine learning algorithms
[1213.64s -> 1216.56s]  like k-nearest neighborhoods.
[1216.56s -> 1220.24s]  And around 2019 people have started to use
[1220.24s -> 1222.36s]  deep neural networks for this task.
[1222.36s -> 1224.16s]  And even later people are starting
[1224.16s -> 1225.80s]  to use large language models.
[1226.86s -> 1231.86s]  And I would say until the use of large language models
[1232.02s -> 1234.72s]  we really see a boost in the performance
[1234.72s -> 1237.84s]  because the model can acquire a lot
[1237.84s -> 1240.02s]  of mathematical knowledge during pre-training
[1240.02s -> 1242.72s]  and use that knowledge in theorem proving.
[1246.40s -> 1249.88s]  So we will be mainly talking about this paper
[1249.88s -> 1254.24s]  called Lin Dojo which was published in New York 2023.
[1255.28s -> 1260.28s]  So in this paper we provide open source data sets,
[1261.12s -> 1264.36s]  tools, and models for theorem proving in Lin.
[1264.36s -> 1266.92s]  And compared to previous approaches
[1266.92s -> 1270.42s]  the main difference is first we are completely open.
[1270.42s -> 1273.48s]  Previous approaches are private
[1273.48s -> 1276.56s]  and they don't release the data set and the models.
[1276.76s -> 1279.96s]  And they were done in private labs.
[1280.88s -> 1283.44s]  And second we propose a new model
[1283.44s -> 1287.40s]  to generate the next step in the proof.
[1287.40s -> 1288.64s]  So here is the overview.
[1289.80s -> 1294.80s]  So in Lin we can extract data from human written Lin code.
[1295.60s -> 1300.60s]  We extract about 10,000 theorems and proofs.
[1301.00s -> 1304.30s]  And for each proof we have the steps in the proof.
[1304.30s -> 1308.14s]  And we also have the lemmas used in the proof
[1308.14s -> 1311.02s]  and also the definitions of this lemmas.
[1311.02s -> 1312.78s]  And after extracting this data set
[1312.78s -> 1315.76s]  we use this data set to train a machine learning model.
[1315.76s -> 1317.34s]  And once the model is trained
[1317.34s -> 1321.86s]  the model can interact with Lin to prove new theorem.
[1321.86s -> 1325.30s]  And so here is the model we introduced for this task.
[1325.30s -> 1328.72s]  It's called retrieval augmented prover or reprover.
[1329.66s -> 1332.30s]  The motivation is that when you prove a theorem
[1332.34s -> 1335.14s]  you almost never start from scratch.
[1335.14s -> 1337.10s]  You use existing lemmas.
[1338.70s -> 1340.42s]  And in order to use existing lemmas
[1340.42s -> 1343.18s]  the model have to know what lemmas are there
[1343.18s -> 1345.94s]  and try to decide whether a lemma is useful.
[1345.94s -> 1348.10s]  So here given a proof state
[1348.10s -> 1349.78s]  that's the goal you want to prove.
[1349.78s -> 1352.38s]  And also given a library of lemmas.
[1352.38s -> 1354.22s]  So we first do retrieval.
[1355.16s -> 1358.58s]  In this phase we encode everything into vectors.
[1358.58s -> 1360.82s]  So the state is now a vector
[1360.82s -> 1363.66s]  and each lemma becomes a vector.
[1363.66s -> 1365.38s]  And we compute the similarity
[1365.38s -> 1368.62s]  between the state and each lemma.
[1368.62s -> 1372.66s]  And we use the result to retrieve the top lemmas.
[1373.62s -> 1375.38s]  And after the lemmas are retrieved
[1375.38s -> 1378.14s]  we combine the lemmas with the state
[1378.14s -> 1381.78s]  and use another language model to generate the next step.
[1384.52s -> 1386.90s]  So here the model is very simple
[1386.90s -> 1389.28s]  but the really important point
[1389.28s -> 1391.52s]  is how do you get the data set.
[1391.52s -> 1394.18s]  And as we said in Lin Dojo
[1394.18s -> 1397.60s]  you can extract the data set from Lin.
[1397.60s -> 1399.94s]  Like from Lin products like mass lib
[1399.94s -> 1401.14s]  and other Lin products.
[1402.72s -> 1407.18s]  So here is a summary of a typical neural theorem prover.
[1408.52s -> 1410.08s]  So on the left you see a tree.
[1410.08s -> 1413.60s]  So that's the tree doing proof search.
[1413.60s -> 1414.96s]  You have an initial state
[1414.96s -> 1419.08s]  and your model can generate multiple potential next steps.
[1419.08s -> 1420.40s]  And you can run those steps
[1420.40s -> 1423.00s]  which gives you the next proof state.
[1423.00s -> 1426.36s]  And you can do that repeatedly until you find a proof.
[1426.36s -> 1428.44s]  So for this part you can use any such algorithm
[1428.44s -> 1431.16s]  like DFS or Monte Carlo tree search.
[1432.30s -> 1436.24s]  And the second part is how do you generate
[1436.24s -> 1437.36s]  the next step.
[1437.36s -> 1440.60s]  So on the right you see at each node
[1440.60s -> 1444.58s]  in this proof search tree you have the current state.
[1444.58s -> 1447.26s]  So you give the current state to the language model
[1447.26s -> 1450.26s]  and it generates suggestions for you.
[1450.26s -> 1452.26s]  And here optionally the language model
[1452.26s -> 1455.38s]  can take a formal mass library
[1455.38s -> 1458.04s]  which is the library of lemmas you can use.
[1461.40s -> 1463.88s]  And so after the Lin Dojo work
[1463.88s -> 1468.18s]  we've seen a bunch of work trying to improve on that.
[1468.18s -> 1469.82s]  For example one thing you could do
[1469.82s -> 1473.22s]  is what we call expert iteration.
[1473.34s -> 1477.62s]  So which is, so you have a prover.
[1477.62s -> 1479.58s]  Even if it's not very good you can use it
[1479.58s -> 1483.32s]  to prove some, to attempt to prove some new theorem.
[1483.32s -> 1486.26s]  And in this process you will get some new proofs.
[1486.26s -> 1487.70s]  And once you get the new proofs
[1487.70s -> 1489.66s]  you can add them into your training set.
[1489.66s -> 1491.42s]  So now you have a better training set.
[1491.42s -> 1492.86s]  And you can use the better training set
[1492.86s -> 1494.54s]  to train a better prover.
[1494.54s -> 1495.70s]  And you can iterate.
[1496.54s -> 1499.42s]  So in this way we show that
[1499.42s -> 1502.10s]  if you run for more iteration
[1502.10s -> 1503.80s]  the performance usually improves
[1503.80s -> 1506.50s]  until you hit some saturation point.
[1508.02s -> 1511.06s]  So we've seen a very simple way
[1511.06s -> 1513.94s]  to build a theorem proving model.
[1513.94s -> 1516.54s]  You just extract human written proofs
[1516.54s -> 1518.66s]  and you train a language model
[1518.66s -> 1520.66s]  to generate the next step in the proof.
[1521.70s -> 1526.06s]  Okay but we would say it's a great step
[1526.06s -> 1528.10s]  but it comes with limitations.
[1528.14s -> 1532.42s]  So the first limitation is still data.
[1532.42s -> 1534.02s]  Because language models work well
[1534.02s -> 1537.30s]  only if you have a lot of training data.
[1537.30s -> 1539.86s]  And the amount of human written proofs
[1539.86s -> 1541.76s]  on the internet is very limited
[1541.76s -> 1544.20s]  compared to say Python code.
[1545.46s -> 1547.98s]  And also another limitation is
[1547.98s -> 1550.40s]  inherent to the task of theorem proving.
[1550.40s -> 1553.74s]  So if you think about other tasks
[1553.74s -> 1556.72s]  that has been successful like playing Go.
[1556.72s -> 1559.12s]  So what's the difference between playing Go
[1559.12s -> 1560.88s]  and proving a theorem?
[1561.92s -> 1565.40s]  The main difference is in Go you have a 19 by 19 board
[1565.40s -> 1567.40s]  so the possible action you can take
[1567.40s -> 1571.10s]  is constrained by the size of the board, it's finite.
[1571.10s -> 1574.36s]  But in mathematics arguably you can,
[1574.36s -> 1576.38s]  your action space is infinite.
[1576.38s -> 1577.96s]  So there is infinite number of things
[1577.96s -> 1580.52s]  you could potentially do when proving a theorem.
[1581.74s -> 1583.88s]  So since the action space is infinite
[1583.88s -> 1586.76s]  it's really hard to cover this entire space
[1587.76s -> 1591.60s]  if you only learn from human created data sets.
[1591.60s -> 1594.32s]  And even if you use reinforcement learning
[1594.32s -> 1597.08s]  it's very hard to explore the space efficiently
[1597.08s -> 1599.36s]  during reinforcement learning.
[1599.36s -> 1600.20s]  That's a good question.
[1600.20s -> 1604.68s]  So in the work we have talked up to now
[1604.68s -> 1607.44s]  we are just using supervised learning.
[1607.44s -> 1610.16s]  So there's no curriculum involved.
[1610.16s -> 1611.68s]  But if you use the reinforcement learning
[1611.68s -> 1614.08s]  it's natural to have the model first learn
[1614.08s -> 1618.08s]  from relatively simple examples and then try to generalize.
[1619.16s -> 1621.80s]  And if you look at more recent work
[1621.80s -> 1624.88s]  in using reinforcement learning for theorem proving
[1624.88s -> 1628.64s]  one major difficulty is how do you choose
[1629.76s -> 1631.80s]  examples to present to the model?
[1631.80s -> 1634.68s]  So you want examples to be not too easy
[1634.68s -> 1635.88s]  but also not too hard.
[1636.84s -> 1637.68s]  Mm-hmm.
[1640.00s -> 1643.40s]  So how do we solve the limitation
[1643.40s -> 1646.36s]  regarding the action space?
[1646.36s -> 1649.16s]  So in more recent work we try to take a step
[1649.16s -> 1653.92s]  in that direction but we choose a very limited domain
[1653.92s -> 1656.94s]  as a playground.
[1656.94s -> 1660.14s]  So we prove inequalities in mass olympia.
[1661.08s -> 1663.00s]  So inequalities in mass olympia
[1663.04s -> 1667.20s]  it's like you have a few real numbers like A, B, C
[1667.20s -> 1669.48s]  and they satisfy some precondition
[1669.48s -> 1671.96s]  and you want to prove a post condition like this.
[1672.88s -> 1675.56s]  And we choose this domain because the proofs
[1675.56s -> 1678.50s]  in this domain is highly regular.
[1678.50s -> 1680.80s]  So here is what humans would do.
[1680.80s -> 1683.20s]  Humans would do typically two things.
[1683.20s -> 1686.96s]  One is you can apply an existing lemma
[1686.96s -> 1691.16s]  like AM-GM inequality or Cauchy-Schwarz inequality.
[1691.20s -> 1692.96s]  And the other thing people typically do
[1692.96s -> 1697.68s]  is to rewrite the current formula into a form
[1697.68s -> 1700.40s]  so that an existing lemma becomes applicable.
[1701.80s -> 1703.80s]  But even with these two kind of actions
[1703.80s -> 1706.12s]  the action space is still very big.
[1706.12s -> 1710.24s]  For example, for proving this particular inequality
[1710.24s -> 1712.60s]  you can apply AM-GM.
[1712.60s -> 1715.04s]  And AM-GM can be applied in more than one way
[1715.04s -> 1719.18s]  so you have more, you have very big branching factor.
[1719.18s -> 1722.14s]  And you can also apply other inequality
[1722.14s -> 1724.38s]  like the T2 inequality.
[1724.38s -> 1727.50s]  And you can also potentially choose to rewrite
[1727.50s -> 1729.50s]  the formula into this form.
[1729.50s -> 1734.46s]  So here the actions you can take is essentially infinite.
[1734.46s -> 1739.08s]  So how do we navigate this kind of infinite action space?
[1740.08s -> 1743.50s]  So one thing we do before trying to solve the problem
[1743.50s -> 1747.70s]  is okay let's try to see how O1 solve this problem.
[1747.70s -> 1750.70s]  And we give a lot of inequality problems to O1
[1750.70s -> 1752.54s]  and we manually check their proofs.
[1753.54s -> 1757.18s]  So as a result we find a lot of errors
[1757.18s -> 1759.64s]  in the O1 produced proofs.
[1760.54s -> 1763.82s]  And we checked 20 examples.
[1764.98s -> 1769.02s]  O1 preview proved none of those examples.
[1769.02s -> 1772.94s]  And the more powerful O3 and also DeepSeq-R1
[1773.86s -> 1777.42s]  also only proved three or four out of 20 examples.
[1777.98s -> 1780.18s]  Whereas a typical gold medalist,
[1780.18s -> 1783.54s]  like a human gold medalist can prove at least 15.
[1783.54s -> 1786.42s]  So then let's come to our method.
[1786.42s -> 1790.86s]  So as we said we categorize the steps you can take
[1790.86s -> 1794.14s]  in this particular problem into two kinds.
[1795.06s -> 1797.16s]  First we call it scaling.
[1798.18s -> 1803.18s]  Scaling is you apply an existing lemma like Cauchy-Schwatz
[1804.26s -> 1807.22s]  to a part of the current formula.
[1807.86s -> 1811.06s]  So in this example you find that the left part
[1811.06s -> 1816.06s]  of the formula can be used for Cauchy-Schwatz
[1817.38s -> 1820.66s]  which gives you the result on the right hand side.
[1822.80s -> 1826.18s]  So what's special about scaling is the number
[1826.18s -> 1829.22s]  of scaling you can do is finite.
[1829.22s -> 1832.80s]  This is because you only have a finite number of lemmas
[1832.80s -> 1834.26s]  and each lemma can be applied
[1834.26s -> 1836.78s]  in only a finite number of ways.
[1837.22s -> 1840.58s]  So it's actually possible to enumerate all the scaling,
[1841.42s -> 1844.58s]  all the possible scalings in this space.
[1844.58s -> 1849.38s]  And besides scaling another category is called rewriting.
[1849.38s -> 1851.74s]  Rewriting is you are given a formula
[1851.74s -> 1854.58s]  and you want to transform the current formula
[1854.58s -> 1856.38s]  into another form.
[1856.38s -> 1858.74s]  And rewriting is very different from scaling
[1858.74s -> 1860.74s]  in the sense that the space is infinite.
[1860.74s -> 1862.74s]  So given any formula there are infinite number
[1862.74s -> 1864.94s]  of ways you can rewrite it.
[1865.78s -> 1869.56s]  So the key insight here is that after we make
[1869.56s -> 1872.70s]  this categorization our algorithm can enumerate
[1872.70s -> 1877.02s]  all the possible scalings using a symbolic algorithm
[1877.02s -> 1880.98s]  whereas the rewriting like the infinite space
[1880.98s -> 1882.98s]  can be handled by large language model.
[1883.94s -> 1888.62s]  So we combine the next step generated
[1888.62s -> 1891.86s]  by the symbolic algorithm with language models
[1891.86s -> 1895.26s]  and we use some heuristics to filter the results
[1895.26s -> 1898.22s]  to reduce the number of goals you have to improve search.
[1899.26s -> 1904.26s]  And as a result we solved much more problems
[1905.16s -> 1909.82s]  compared to DeepSeq R1 and also O3.
[1909.82s -> 1913.66s]  So here on the top you see R1 proved four theorems.
[1913.66s -> 1917.24s]  Humans can prove 15 whereas our system
[1917.24s -> 1919.46s]  called Leaps can prove 16 theorems.
[1920.46s -> 1923.34s]  And at the bottom we evaluate our system
[1923.34s -> 1928.34s]  with a few benchmarks and compare with existing methods.
[1928.74s -> 1931.30s]  So here's one example, one interesting example
[1931.30s -> 1933.06s]  proof discovered by our system.
[1934.74s -> 1937.66s]  So here it's interesting because it's from
[1937.66s -> 1941.86s]  Evan Chen who is the IMO coach for the Team USA.
[1941.86s -> 1945.34s]  And he has a problem set and he said
[1945.42s -> 1949.90s]  this problem has no hope to be solved by AMGM
[1949.90s -> 1953.26s]  but we find that the proof discovered by our system
[1953.26s -> 1958.04s]  uses only AMGM combined with clever rewriting.
[1958.04s -> 1959.74s]  So it can actually find proofs
[1959.74s -> 1962.16s]  that humans do not know how to find.
[1963.82s -> 1967.90s]  So here's the takeaway, the challenge in theorem proving
[1967.90s -> 1970.66s]  is how do you efficiently explore
[1970.66s -> 1972.52s]  a potentially infinite action space?
[1973.44s -> 1975.60s]  And while this is a very hard problem
[1975.60s -> 1979.72s]  but if you are working on a specific domain of mathematics
[1979.72s -> 1983.00s]  you can use domain insight to structure the action space
[1984.12s -> 1986.84s]  so that the action space become more feasible.
[1987.96s -> 1991.60s]  But it is an open problem of whether you can
[1991.60s -> 1994.12s]  generalize this approach to be domain general.
[1995.20s -> 1997.84s]  So after talking about theorem proving
[1997.84s -> 2000.76s]  we will be going to auto-formalization
[2000.76s -> 2005.76s]  and feel free to bring it up any questions, any time.
[2008.08s -> 2010.92s]  So in auto-formalization we have informal math
[2010.92s -> 2012.70s]  and we have formal math and we want the model
[2012.70s -> 2015.40s]  to be able to build a bridge between them
[2015.40s -> 2017.84s]  like translating from informal to formal.
[2019.92s -> 2022.88s]  And here actually we have two slightly
[2022.88s -> 2024.76s]  different sub-problems.
[2024.76s -> 2027.80s]  First is how do we auto-formalize the theorem statement?
[2028.72s -> 2032.36s]  And second is how do we auto-formalize the proofs?
[2033.24s -> 2035.04s]  In statement auto-formalization
[2035.04s -> 2036.64s]  the model only take the statement
[2036.64s -> 2039.24s]  and produce the statement in link.
[2040.36s -> 2042.40s]  But in proof auto-formalization
[2042.40s -> 2045.16s]  the model have the informal statement,
[2045.16s -> 2048.28s]  the informal proof and also the formal statement
[2048.28s -> 2050.28s]  and it only produce the formal proof.
[2052.52s -> 2057.52s]  So these two sub-problems have different challenges.
[2057.68s -> 2060.16s]  Let's talk about statement formalization first.
[2060.16s -> 2062.04s]  For formalizing the statement
[2062.04s -> 2064.20s]  it's really hard to do the evaluation.
[2065.52s -> 2069.48s]  Because for example here the informal statement says
[2069.48s -> 2072.20s]  there exists an infinite number of primes.
[2072.20s -> 2076.04s]  And the formal statement says for any natural number n
[2076.04s -> 2080.40s]  there exists a p that's greater than or equal to n,
[2080.40s -> 2081.64s]  p is prime.
[2081.64s -> 2084.44s]  So this is obviously a correct translation
[2084.44s -> 2086.44s]  of the informal statement
[2086.44s -> 2089.28s]  because it implies the infinity.
[2089.28s -> 2092.68s]  But the problem is it's not the only correct solution.
[2092.68s -> 2096.48s]  So if we produce this solution
[2096.48s -> 2099.40s]  we change p greater than or equal to n
[2099.40s -> 2101.64s]  to p strictly greater than n.
[2102.48s -> 2104.32s]  Well essentially it's the same.
[2104.32s -> 2106.40s]  And we can even make it more different.
[2106.40s -> 2109.52s]  We can make n also a prime.
[2109.52s -> 2112.12s]  And for any human you can easily see
[2112.12s -> 2114.16s]  okay this three statements
[2114.16s -> 2115.70s]  well they are slightly different
[2115.82s -> 2117.78s]  but they are essentially the same thing.
[2118.66s -> 2120.62s]  But this creates a problem for machine learning.
[2120.62s -> 2123.46s]  So if the machine learning model produces one of them
[2123.46s -> 2126.94s]  how do you know if it's equivalent to the ground truth?
[2128.30s -> 2131.18s]  You may want to say okay can I like
[2131.18s -> 2132.86s]  check the logical equivalent?
[2133.78s -> 2137.46s]  But the reality is this kind of checking the equivalence
[2137.46s -> 2140.22s]  between two theorems is computationally infeasible
[2140.22s -> 2141.06s]  in general.
[2142.38s -> 2145.26s]  And in practice researchers have resulted
[2145.82s -> 2148.10s]  to heuristics or human evaluation.
[2148.10s -> 2150.86s]  But if you rely on human evaluation
[2150.86s -> 2153.82s]  it's going to be very expensive and not reproducible.
[2153.82s -> 2158.22s]  And if you result to proxy matrix or heuristics
[2158.22s -> 2159.78s]  it's not going to be accurate.
[2161.30s -> 2162.86s]  So that's the challenge for evaluating
[2162.86s -> 2164.78s]  the autoformulized statement.
[2164.78s -> 2165.66s]  How about proofs?
[2166.86s -> 2169.38s]  Proofs we have another challenge.
[2169.38s -> 2172.70s]  Human written proofs even if they are very rigorous
[2172.70s -> 2175.78s]  they are not rigorous enough to be formalized.
[2175.78s -> 2177.18s]  They often have holes.
[2177.18s -> 2181.70s]  Like one kind of hole is like mathematicians often say
[2181.70s -> 2184.22s]  okay this case is left to the reader.
[2184.22s -> 2186.98s]  So if it's left to the reader your model
[2186.98s -> 2189.54s]  has no way to figure it out from the text.
[2189.54s -> 2191.58s]  It has to figure it out by their own.
[2192.46s -> 2194.74s]  And there are more subtle holes in the proofs
[2194.74s -> 2196.42s]  that we will cover later.
[2196.42s -> 2199.86s]  But for the model to be able to autoformulize the proof
[2199.86s -> 2203.50s]  it kind of have to fill in all those kind of gaps
[2203.50s -> 2205.38s]  which can be very difficult.
[2205.38s -> 2208.78s]  So we've covered two key challenges in autoformulization.
[2208.78s -> 2210.94s]  For statement we don't have a reliable
[2210.94s -> 2212.70s]  automatic evaluation.
[2212.70s -> 2215.10s]  And for proofs the model have to fill in
[2215.10s -> 2216.78s]  all the reasoning gaps.
[2216.78s -> 2220.86s]  And in this project we want to address both problems.
[2220.86s -> 2223.42s]  So that sounds very intractable.
[2223.42s -> 2224.98s]  But a lesson we learned in this project
[2224.98s -> 2228.22s]  is that since intractable in general
[2228.26s -> 2230.82s]  can be made tractable if you restrict
[2230.82s -> 2232.22s]  to a very specific domain.
[2233.30s -> 2235.84s]  And the domain we choose is Euclidean geometry.
[2236.82s -> 2241.22s]  Euclidean geometry is one of the oldest branches
[2241.22s -> 2243.74s]  in mathematics and it has been used
[2243.74s -> 2247.46s]  to test human intelligence for more than 2,000 years.
[2247.46s -> 2252.46s]  And recently it has become interested by the AI community
[2252.86s -> 2255.02s]  because of the alpha geometry work
[2255.02s -> 2259.70s]  where they have an AI that can solve IMO level geometry.
[2261.86s -> 2266.38s]  So in this work we show that Euclidean geometry
[2266.38s -> 2269.94s]  is not only a good domain for theorem proving,
[2269.94s -> 2272.82s]  it's also a good domain for autoformulization.
[2272.82s -> 2275.82s]  And more concretely we collect a benchmark
[2275.82s -> 2279.68s]  called Lin Euclid in Euclidean geometry.
[2279.68s -> 2284.68s]  It consists of 48 problems from Euclid's Elements Book One.
[2285.46s -> 2287.78s]  And more than 100 problems from UniGeo
[2287.78s -> 2290.10s]  which is an existing data set.
[2291.36s -> 2294.84s]  So let's look at one example in our data set.
[2296.62s -> 2300.12s]  So this is directly from Euclid's Elements.
[2300.12s -> 2302.50s]  It's proposition one from Book One.
[2302.50s -> 2305.78s]  It's basically you have two points A and B
[2305.78s -> 2307.78s]  and you want to show that it's possible
[2307.78s -> 2309.88s]  to construct a equilateral triangle.
[2310.98s -> 2313.90s]  So for each problem you have the diagram
[2313.90s -> 2316.42s]  and the informal theorem statement
[2316.42s -> 2319.14s]  and also the informal proof.
[2319.14s -> 2321.10s]  And when constructing the benchmark
[2321.10s -> 2324.78s]  we try to manually formalize this problem in Lin.
[2325.94s -> 2329.06s]  So on the right hand side is our formalization in Lin.
[2330.30s -> 2333.10s]  As you can see it corresponds very closely
[2333.10s -> 2335.82s]  to the informal version.
[2335.82s -> 2340.66s]  Like the first step is you draw a circle, BCD,
[2340.66s -> 2345.26s]  that's centered at A and B should be on the circle.
[2345.26s -> 2347.98s]  And the second step is you draw another circle
[2347.98s -> 2350.24s]  and you try to find the intersection
[2350.24s -> 2352.38s]  and you connect everything
[2352.38s -> 2354.38s]  and then that's a equilateral triangle.
[2354.38s -> 2357.82s]  So there are only like five or six steps
[2357.82s -> 2359.78s]  in the formal proof.
[2359.78s -> 2362.26s]  And each step corresponds perfectly
[2362.26s -> 2366.34s]  to what's being done in the informal proof.
[2366.66s -> 2370.76s]  And we did this for all the 48 theorems and proofs
[2370.76s -> 2373.86s]  in Euclid's Elements Book One.
[2373.86s -> 2377.54s]  And this is aesthetically very pleasing for me
[2377.54s -> 2380.30s]  because to the best of our knowledge
[2380.30s -> 2384.62s]  we are actually the first to formalize Euclid's Elements
[2384.62s -> 2386.18s]  in a proof assistance like Lin.
[2387.86s -> 2391.98s]  And while we want to keep the proofs very faithful
[2391.98s -> 2394.46s]  in the sense that there is a direct correspondence
[2394.50s -> 2396.86s]  between formal and informal.
[2396.86s -> 2401.74s]  And as we will see later this is not always trivial.
[2403.06s -> 2406.02s]  And during this process we find a few
[2406.02s -> 2408.32s]  very interesting errors in Euclid's Elements.
[2409.50s -> 2412.38s]  I think we can probably go over one of those error
[2412.38s -> 2415.54s]  just to see how formalization can help you check
[2415.54s -> 2418.90s]  mathematical proofs rigorously and uncover bugs
[2418.90s -> 2422.66s]  in a proof that you may think is already very rigorous.
[2423.66s -> 2427.54s]  So this is proposition 24 from Euclid's Book One.
[2428.50s -> 2430.46s]  It states if you have two triangles
[2431.64s -> 2436.64s]  and then the two triangles have two pairs
[2437.46s -> 2439.22s]  of edges equal to each other.
[2439.22s -> 2444.22s]  So AC equals to DF and AB equals to DE.
[2445.58s -> 2449.04s]  So blue equals to blue, red equals to red.
[2449.04s -> 2452.28s]  And the angle A is bigger than angle D.
[2453.38s -> 2458.38s]  So you want to show that BC is greater than EF.
[2459.38s -> 2463.20s]  Well this doesn't look very difficult
[2463.20s -> 2467.54s]  and I'm going to show the original proof by Euclid.
[2468.66s -> 2472.58s]  So first you try to move the triangle from the left
[2472.58s -> 2475.18s]  to overlap with the triangle on the right.
[2476.10s -> 2478.04s]  So C will become G.
[2478.04s -> 2483.04s]  And then you try to connect F and G.
[2483.64s -> 2488.64s]  So now the problem becomes proving EF greater than EG.
[2491.32s -> 2493.60s]  This is easier because EF and EG
[2493.60s -> 2496.16s]  are now in the same triangle.
[2496.16s -> 2498.68s]  And according to a previously proved theorem
[2498.68s -> 2502.48s]  in the same book, in the same triangle
[2502.48s -> 2507.16s]  a bigger angle corresponds to a bigger edge.
[2507.16s -> 2511.00s]  So you only have to prove the relationship
[2511.00s -> 2513.40s]  between these two angles.
[2513.40s -> 2515.66s]  You have to prove the big angle here
[2515.66s -> 2518.36s]  is indeed bigger than the small angle.
[2518.36s -> 2519.20s]  So how?
[2520.70s -> 2524.70s]  Just notice that we have a isosceles triangle here.
[2524.70s -> 2526.96s]  We have DGF.
[2526.96s -> 2528.96s]  DG equals to DF.
[2529.86s -> 2532.32s]  And by a previously proved lemma,
[2532.32s -> 2536.68s]  in an isosceles triangle you have two angles
[2537.12s -> 2538.72s]  equal to each other.
[2538.72s -> 2543.72s]  So then you can argue that the bigger angle
[2543.76s -> 2548.12s]  is bigger than the EFG
[2548.12s -> 2552.12s]  because it's a superset of the EFG.
[2552.12s -> 2555.40s]  And the EFG is equal to DGF.
[2555.40s -> 2559.12s]  And the DGF is bigger than the small angle.
[2559.12s -> 2560.72s]  Then you have proved the bigger angle
[2560.72s -> 2562.60s]  is bigger than the small angle.
[2562.60s -> 2564.52s]  Okay so this is very trivial
[2564.52s -> 2567.00s]  but the proof is actually wrong.
[2567.88s -> 2568.88s]  Anyone knows why?
[2569.88s -> 2570.72s]  No, okay.
[2572.36s -> 2574.88s]  Well technically it's not wrong for this diagram
[2574.88s -> 2576.48s]  but what if they look like this?
[2578.34s -> 2580.42s]  So let me try to replay the proof.
[2581.36s -> 2585.32s]  First, move the triangle from the left to the right.
[2585.32s -> 2588.40s]  Second, connect G and F.
[2588.40s -> 2593.40s]  Now we want to prove EF greater than EG
[2594.68s -> 2597.80s]  and we want to use this isosceles triangles alpha.
[2597.80s -> 2602.24s]  So this alpha are equal and we want to prove
[2602.24s -> 2606.36s]  X greater than Y using alpha
[2606.36s -> 2608.64s]  which does not work directly
[2608.64s -> 2612.32s]  because in the previous diagram alpha is a part of X
[2612.32s -> 2614.56s]  but now alpha is outside X and Y.
[2614.56s -> 2617.84s]  So you cannot use it as an angle in the middle anymore.
[2617.84s -> 2621.80s]  So the original argument breaks in this diagram.
[2621.80s -> 2623.32s]  So how do we fix this proof?
[2623.32s -> 2624.52s]  Well luckily it's fixable.
[2624.52s -> 2629.28s]  Otherwise we will probably publish a bigger paper.
[2631.08s -> 2635.92s]  So first notice that the angle DGE,
[2635.92s -> 2638.80s]  that's alpha plus Y.
[2638.80s -> 2640.60s]  It's an inner angle in a triangle
[2640.60s -> 2642.28s]  so it's smaller than pi.
[2642.28s -> 2643.32s]  That's trivial.
[2643.32s -> 2647.52s]  And second, two pi minus alpha minus X,
[2647.52s -> 2651.32s]  that's DFE is also smaller than pi.
[2651.36s -> 2653.96s]  If you add these inequalities together,
[2653.96s -> 2658.12s]  alpha will cancel out and you get X greater than Y.
[2658.12s -> 2661.08s]  So it can be fixed but as you see,
[2661.08s -> 2664.56s]  this proof is quite different from Euclid's proof.
[2664.56s -> 2668.04s]  So that's why we think Euclid's have made an error here
[2668.04s -> 2669.88s]  or technically it's not an error.
[2669.88s -> 2673.28s]  It's like this proof has two cases.
[2673.28s -> 2675.84s]  These two cases are slightly different
[2675.84s -> 2678.60s]  and one case is more challenging than the other
[2678.60s -> 2682.52s]  but Euclid omitted the more challenging case
[2682.52s -> 2684.08s]  and focused on the easy case.
[2685.84s -> 2688.52s]  Okay, maybe that's a very interesting digression
[2688.52s -> 2691.72s]  that's not related to AI but it showed
[2691.72s -> 2695.36s]  how AI and the link can make things more rigorous.
[2695.36s -> 2697.64s]  So let's go back to AI.
[2699.04s -> 2704.04s]  So in our benchmark, the first thing good about it
[2704.64s -> 2708.28s]  is it enables you to check whether the auto-formalized
[2708.28s -> 2710.00s]  theorem statement is correct.
[2710.00s -> 2712.68s]  So here's how we check if the statement is correct.
[2712.68s -> 2714.68s]  We have a ground truth and we want to check
[2714.68s -> 2717.32s]  if it's logically, if the model generated statement
[2717.32s -> 2720.40s]  is logically equivalent to the ground truth.
[2720.40s -> 2723.36s]  So given two theorems, T1 and T2,
[2723.36s -> 2727.04s]  we check it by proving T1 implies T2
[2727.04s -> 2729.88s]  and the T2 also implies T1.
[2729.88s -> 2732.68s]  And here we use a automated reasoning tool
[2732.68s -> 2733.96s]  called SMT solver.
[2735.20s -> 2738.00s]  But if you remember, I said this is infeasible
[2738.60s -> 2740.72s]  but how it becomes feasible now.
[2740.72s -> 2743.24s]  And it turns out now it's feasible
[2743.24s -> 2746.72s]  because the theorems in Euclidean geometry
[2746.72s -> 2748.36s]  takes a particular form.
[2748.36s -> 2750.80s]  They are not general theorems.
[2750.80s -> 2753.72s]  So problems in fact in general
[2753.72s -> 2756.28s]  can be made tractable in a specific domain.
[2759.20s -> 2762.72s]  And so that's how we check the theorem statement.
[2762.72s -> 2766.28s]  Another problem is auto-formalizing the proofs.
[2766.28s -> 2768.80s]  So here in Euclidean geometry,
[2768.80s -> 2773.80s]  we want to show that there is a well-known
[2774.64s -> 2776.34s]  kind of gaps in the proofs.
[2776.34s -> 2778.44s]  We call it diagrammatical reasoning gaps.
[2779.64s -> 2781.20s]  I'm going to show that using an example.
[2781.20s -> 2784.44s]  So proposition one from Euclid's Elements Book One.
[2784.44s -> 2786.68s]  Given two points, A and B,
[2786.68s -> 2790.08s]  you can construct a little equilateral triangle.
[2790.08s -> 2791.96s]  Here is Euclid's proof.
[2791.96s -> 2795.88s]  Draw a circle centered at A and B is on the circle.
[2796.60s -> 2800.44s]  Draw another circle centered at B, A is on the circle.
[2800.44s -> 2803.80s]  Take the intersection C and connect everything.
[2803.80s -> 2805.40s]  You get a equilateral triangle.
[2806.32s -> 2808.52s]  Anyone disagree with this proof?
[2808.52s -> 2809.36s]  No?
[2809.36s -> 2810.48s]  Okay.
[2810.48s -> 2814.56s]  The problem is did we prove C actually exists?
[2814.56s -> 2816.20s]  Did we prove these two circles
[2816.20s -> 2818.36s]  actually have an intersection?
[2819.32s -> 2820.44s]  We didn't.
[2820.44s -> 2823.24s]  And actually if you try to prove it,
[2823.24s -> 2824.84s]  so this is proposition one.
[2824.84s -> 2826.88s]  So to prove this proposition,
[2826.88s -> 2828.52s]  you can only use the axioms.
[2828.52s -> 2830.76s]  There's no existing lemmas.
[2830.76s -> 2833.60s]  And if you try to use Euclid's axioms
[2833.60s -> 2838.44s]  to try to prove these two circles intersect,
[2838.44s -> 2839.98s]  you cannot prove it actually.
[2840.86s -> 2842.30s]  So it's not provable.
[2843.68s -> 2845.28s]  But no one bothers to prove it
[2845.28s -> 2848.04s]  because it's just so obvious in the diagram.
[2848.92s -> 2849.92s]  It becomes a problem
[2849.92s -> 2852.46s]  if I want to translate this proof in Lin.
[2852.46s -> 2853.92s]  When I formulate the proof in Lin,
[2854.20s -> 2856.20s]  Lin does not know what is the diagram
[2856.20s -> 2858.04s]  or what is obvious in the diagram.
[2859.12s -> 2861.76s]  So I kind of have to formalize
[2861.76s -> 2863.62s]  what is obvious in the diagram.
[2863.62s -> 2868.62s]  I believe this, I was in the Zoom in the last lecture.
[2869.44s -> 2870.96s]  I believe someone asked a question
[2870.96s -> 2875.20s]  about how do we formalize Euclidean geometry in Lin.
[2875.20s -> 2878.32s]  I think this is very closely related to that question.
[2878.32s -> 2881.14s]  To formalize Euclidean geometry in Lin,
[2881.14s -> 2882.44s]  the most important thing,
[2882.48s -> 2884.52s]  or one of the most important thing
[2884.52s -> 2886.36s]  is how do you formalize the concept
[2886.36s -> 2888.48s]  of being obvious in the diagram?
[2889.92s -> 2894.92s]  And luckily, this problem has actually been solved before.
[2896.32s -> 2900.48s]  So this is a work from Abigail
[2900.48s -> 2904.96s]  with a professor in philosophy, now in CMU.
[2906.16s -> 2910.74s]  So Abigail essentially identified a set of axioms
[2910.74s -> 2912.94s]  that's not in Euclid's axiom system.
[2912.94s -> 2917.94s]  You can think of it as an unspoken axiom or hidden axiom.
[2917.94s -> 2922.18s]  And so what is being obvious in the diagram,
[2922.18s -> 2927.18s]  it is just if a fact is a consequence of those axioms,
[2927.18s -> 2929.78s]  then it's obvious in the diagram.
[2929.78s -> 2931.58s]  So now we have a computational meaning
[2931.58s -> 2933.42s]  of obviousness in the diagram,
[2933.42s -> 2937.58s]  and we can implement an automated reasoning algorithm
[2937.58s -> 2940.68s]  to try to derive things that are obvious.
[2941.74s -> 2944.34s]  So let me put everything together.
[2944.34s -> 2946.78s]  So in this work, we have a benchmark.
[2946.78s -> 2948.88s]  So in the benchmark, we have a diagram,
[2948.88s -> 2950.68s]  we have informal math,
[2950.68s -> 2953.42s]  and we have a ground truth formal statements
[2953.42s -> 2955.06s]  that's written by human.
[2955.06s -> 2959.24s]  So now we use a model to do auto-formalization.
[2959.24s -> 2962.34s]  It generates a formal theorem,
[2962.34s -> 2965.42s]  and we can check whether it's equivalent
[2965.42s -> 2968.22s]  to the ground truth using SMT solvers
[2968.22s -> 2971.06s]  to prove that they imply each other.
[2972.10s -> 2974.78s]  So after checking the statement equivalence,
[2974.78s -> 2978.42s]  the model can also do proof auto-formalization.
[2978.42s -> 2980.40s]  Say it generates this proof,
[2980.40s -> 2982.50s]  and we give this proof to Lin.
[2982.50s -> 2986.22s]  Lin will identify a set of holes in the proof.
[2987.06s -> 2991.18s]  And we give those holes to SMT solver,
[2991.18s -> 2994.62s]  which will tell us if the holes are obvious
[2994.62s -> 2995.98s]  in the diagram.
[2995.98s -> 2997.58s]  And if everything works,
[2997.58s -> 3001.42s]  then we conclude, okay, this is a correct formalization.
[3001.42s -> 3004.46s]  So what our benchmark really do here is
[3004.46s -> 3008.32s]  we provide a protocol that you can use
[3008.32s -> 3011.64s]  to rigorously evaluate auto-formalization.
[3013.06s -> 3016.30s]  And so after collecting this benchmark,
[3016.30s -> 3018.42s]  we also did some very simple experiments
[3019.46s -> 3023.04s]  using just GPT-4 and the GPT-4 vision.
[3023.04s -> 3024.98s]  And here we don't train the model,
[3024.98s -> 3028.70s]  we just give a few demonstrations to the model
[3028.70s -> 3031.30s]  and ask it to auto-formalize the theorem statement.
[3032.70s -> 3037.28s]  So blue here is GPT-4, and green is GPT-4V,
[3037.28s -> 3040.04s]  which also takes the diagram as input.
[3040.04s -> 3042.30s]  And so here are two takeaways.
[3042.30s -> 3045.46s]  One is if you look at the x-axis,
[3045.46s -> 3049.84s]  it's how many demonstrations you give to the model.
[3049.84s -> 3051.82s]  So if you give the model more demonstration,
[3051.82s -> 3053.74s]  it's going to do better.
[3053.82s -> 3058.26s]  But even if you give five demonstrations or even more,
[3058.26s -> 3062.72s]  the model get only about 20%, which is pretty low,
[3062.72s -> 3066.78s]  which shows auto-formalization is very challenging.
[3066.78s -> 3068.78s]  And if you compare green with blue,
[3068.78s -> 3071.42s]  you see green is a little bit better,
[3071.42s -> 3075.24s]  which shows if you give the diagram to the model,
[3075.24s -> 3078.78s]  it has a stronger understanding of the geometry problem.
[3080.54s -> 3082.64s]  And here are some takeaways.
[3082.64s -> 3085.80s]  We identified two challenges in auto-formalization.
[3085.80s -> 3089.10s]  For theorem statement, you don't know how to evaluate.
[3089.10s -> 3091.68s]  For the proofs, there are too many holes
[3091.68s -> 3093.60s]  in the human written proofs.
[3093.60s -> 3098.02s]  And we address them by limiting to a specific domain.
[3098.02s -> 3102.04s]  So in this domain, we can build a equivalence checker
[3102.04s -> 3105.28s]  for the statement, and we can build a tool
[3105.28s -> 3107.78s]  that can fill in all the gaps.
[3108.76s -> 3111.30s]  But still, the open problem is really
[3111.30s -> 3115.08s]  how do you generalize across a broader set
[3115.08s -> 3116.30s]  of mathematical domain?
[3117.50s -> 3120.82s]  So now we've covered both tasks,
[3120.82s -> 3123.26s]  auto-formalization and theorem proving.
[3123.26s -> 3125.36s]  And this is the end of this lecture,
[3125.36s -> 3127.06s]  and I'm happy to take questions.
