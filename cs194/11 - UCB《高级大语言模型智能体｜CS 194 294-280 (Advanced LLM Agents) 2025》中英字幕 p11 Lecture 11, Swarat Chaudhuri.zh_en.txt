# Detected language: en (p=1.00)

[0.00s -> 4.20s]  Try sharing your slides.
[4.20s -> 4.96s]  Yes.
[4.96s -> 5.68s]  One sec.
[5.68s -> 6.18s]  OK.
[24.60s -> 25.48s]  OK.
[25.48s -> 26.48s]  Let's see now.
[30.00s -> 45.20s]  OK.
[45.20s -> 46.64s]  Do you see this?
[46.64s -> 49.36s]  I see your speaker notes.
[49.36s -> 50.72s]  If you're OK with that.
[55.72s -> 56.42s]  Oh, OK.
[56.42s -> 56.92s]  Thanks.
[57.92s -> 59.40s]  Is that it?
[59.40s -> 60.12s]  Oh, OK.
[60.12s -> 61.16s]  Thanks.
[61.16s -> 63.24s]  Cool.
[63.24s -> 64.00s]  I see.
[64.00s -> 68.04s]  But this way, I don't get to see the speaker notes, which
[68.04s -> 69.24s]  I guess is OK.
[69.24s -> 71.92s]  But isn't there a way?
[71.92s -> 74.36s]  I thought there was a way of for me
[74.36s -> 76.72s]  to see the speaker notes, the presenter view,
[76.72s -> 80.72s]  and for you to see the actual screen.
[80.72s -> 82.64s]  I figured this out during the pandemic,
[82.64s -> 85.32s]  but I don't remember how to do it anymore.
[87.92s -> 88.42s]  Yeah.
[88.42s -> 88.92s]  Sorry.
[88.92s -> 90.20s]  Go on, Tara.
[90.20s -> 92.36s]  Go Alex.
[92.36s -> 97.40s]  Do you have in your Google or PowerPoint,
[97.40s -> 101.48s]  is there a button that says presenter view?
[101.48s -> 105.00s]  Yeah, that's what I used to get to the previous screen.
[105.00s -> 105.72s]  Oh, OK.
[105.72s -> 109.80s]  And then, so when you have presenter view on,
[109.80s -> 112.28s]  maybe stop sharing your screen.
[112.28s -> 118.04s]  Do you see two panes of both your actual slides and then
[118.04s -> 129.72s]  the, do you see both your slides and the speaker notes?
[129.72s -> 131.64s]  Yeah, that's what I see in the presenter view.
[131.64s -> 132.36s]  That's right.
[132.36s -> 134.72s]  OK, so now you have your two screens.
[134.72s -> 137.28s]  When you go to share screen, is it possible to just
[137.28s -> 145.24s]  select your slides instead of the speaker notes?
[145.24s -> 146.04s]  OK, I see.
[146.04s -> 151.88s]  So you're saying that I go to, so I first go to PowerPoint
[151.88s -> 156.12s]  and do what, sorry?
[156.12s -> 157.72s]  Go to presenter view.
[157.72s -> 160.08s]  So you should now see two windows,
[160.08s -> 162.64s]  one with your speaker notes and one with the slides.
[162.64s -> 164.08s]  Correct, yeah.
[164.08s -> 170.88s]  And then on Zoom, click share and only share your window
[170.88s -> 172.32s]  that corresponds to the slides.
[176.40s -> 178.72s]  I see, so that's not it, right?
[178.72s -> 181.60s]  Yeah, not that one.
[181.60s -> 184.12s]  OK, I think this is too complicated perhaps.
[184.12s -> 188.80s]  Let's just, yeah, I'll just use the slide show view.
[188.80s -> 190.52s]  OK, that works.
[194.08s -> 195.96s]  OK, so how does this work?
[195.96s -> 205.12s]  Cool, OK, so I think Xinyan will come on, hopefully, and.
[205.12s -> 206.44s]  Caillou will come on.
[206.44s -> 207.52s]  Oh, is Caillou today?
[207.52s -> 212.20s]  OK, well, Caillou will come on and introduce you at 10,
[212.20s -> 215.28s]  10, or like basically in 10, 7 minutes.
[215.28s -> 219.88s]  And you feel free to just go for as long as you want.
[219.88s -> 222.88s]  You'll have until, you haven't been able to see the slides
[222.88s -> 225.28s]  until, you have two hours basically,
[225.28s -> 229.36s]  but usually people finish in like an hour or like 90 minutes
[229.36s -> 234.76s]  and then there's just time for Q&A at the end.
[234.76s -> 240.72s]  And basically the Q&A will be, oh, Caillou's here.
[240.72s -> 244.68s]  Those are the Q&A's coming from our viewers online
[244.68s -> 246.52s]  and I can read out the questions or Caillou
[246.52s -> 248.68s]  can to have you answer them.
[248.68s -> 251.48s]  That sounds great.
[251.48s -> 252.76s]  Hey, Caillou.
[252.76s -> 255.28s]  Hey, Saad, how are you?
[255.28s -> 256.80s]  Good, good.
[256.80s -> 258.16s]  Thank you for coming.
[258.16s -> 258.80s]  Of course.
[265.16s -> 267.76s]  Yeah, and then Berkeley classes start 10 minutes
[267.76s -> 271.40s]  after the hour, so I guess you can close your camera
[271.40s -> 273.48s]  if you want and we'll get started at like 10, 10.
[276.84s -> 279.44s]  OK, and then just to make sure I understand,
[279.48s -> 284.12s]  so I won't get to see the questions, right?
[284.12s -> 286.12s]  Because they are showing up on what, YouTube
[286.12s -> 288.48s]  or is there a different platform?
[288.48s -> 289.84s]  We're using a different platform.
[289.84s -> 292.20s]  I can share with you the questions directly
[292.20s -> 295.52s]  if you wanted to read them out yourself and answer them,
[295.52s -> 297.88s]  but also generally we've had one of us
[297.88s -> 299.84s]  read out the questions to you and then you can answer them.
[299.84s -> 300.80s]  But if it's helpful to read it,
[300.80s -> 303.16s]  I can show you where the questions are coming from.
[303.16s -> 304.20s]  No, I think that's fine.
[304.20s -> 305.56s]  You can read it.
[305.56s -> 308.36s]  And is the norm that people ask questions
[308.36s -> 313.00s]  during the lecture or is it more like I just talk?
[313.00s -> 315.68s]  Generally, we save it all to the end.
[315.68s -> 316.44s]  OK.
[316.44s -> 317.80s]  If that's OK with you, yeah.
[317.80s -> 319.44s]  Yeah, that sounds good.
[319.44s -> 320.80s]  Sounds good.
[320.80s -> 324.16s]  And how many students are there?
[324.16s -> 328.16s]  We have, I believe, close to 300 Berkeley students
[328.16s -> 334.48s]  and then we have around 6,000 online MOOC students.
[334.48s -> 336.80s]  But I think in terms of live viewership,
[336.80s -> 340.16s]  it's usually around like the 400 mark-ish
[340.16s -> 342.96s]  because we're coming from international.
[342.96s -> 343.44s]  Right.
[343.44s -> 343.88s]  Time zones.
[343.88s -> 344.36s]  Mm-hmm.
[358.20s -> 361.72s]  Caoyou, is Dawn also joining?
[361.72s -> 364.48s]  I don't think so.
[364.48s -> 366.48s]  You usually think you will be joining,
[366.52s -> 368.72s]  but she has a conflict today.
[368.72s -> 370.80s]  So that's fine.
[370.80s -> 371.32s]  Right.
[382.08s -> 385.28s]  What time is it there?
[385.28s -> 387.28s]  Eight hours ahead of you.
[387.28s -> 387.96s]  Eight hours.
[391.56s -> 396.44s]  Yeah, you guys had Thomas speak a few weeks ago, right?
[396.44s -> 398.28s]  Yeah, that's right.
[398.28s -> 400.04s]  She talked about other proof.
[400.04s -> 401.80s]  That's right.
[426.44s -> 427.92s]  OK.
[456.44s -> 457.92s]  All right.
[486.44s -> 487.92s]  All right.
[516.44s -> 520.44s]  And what platform do you use for the MOOC?
[520.44s -> 521.44s]  Is this edX?
[523.44s -> 527.44s]  We host everything on our own course website
[527.44s -> 530.44s]  and then the livestream takes place over YouTube.
[530.44s -> 531.44s]  Yeah.
[531.44s -> 532.44s]  Yeah.
[532.44s -> 533.44s]  Yeah.
[533.44s -> 534.44s]  Yeah.
[534.44s -> 535.44s]  Yeah.
[535.44s -> 536.44s]  Yeah.
[536.44s -> 537.44s]  Yeah.
[537.44s -> 538.44s]  Yeah.
[538.44s -> 539.44s]  Yeah.
[539.44s -> 540.44s]  Yeah.
[540.44s -> 541.44s]  Yeah.
[541.44s -> 542.44s]  Yeah.
[542.44s -> 543.44s]  Yeah.
[543.44s -> 544.44s]  Yeah.
[544.44s -> 545.44s]  Yeah.
[546.44s -> 547.44s]  I see.
[547.44s -> 548.44s]  I see.
[548.44s -> 551.44s]  And then the questions, how do you set that up?
[551.44s -> 556.44s]  We use a platform called Slido where students can ask questions
[556.44s -> 559.44s]  and vote for their favorite questions.
[559.44s -> 560.44s]  Oh, cool.
[560.44s -> 561.44s]  OK.
[561.44s -> 562.44s]  Yeah.
[572.44s -> 575.44s]  And we have TA to edit the videos
[575.44s -> 577.44s]  and before uploading to YouTube.
[577.44s -> 578.44s]  OK.
[578.44s -> 579.44s]  That sounds good.
[581.44s -> 582.44s]  Yeah.
[582.44s -> 584.44s]  One thing that's a little bit disconcerting
[584.44s -> 589.44s]  about this sort of online talk is that I don't see students.
[589.44s -> 591.44s]  I don't know what the reaction is,
[591.44s -> 593.44s]  whether or not they're understanding,
[593.44s -> 595.44s]  but I guess that's just part of the MOOC.
[596.44s -> 597.44s]  Yeah.
[597.44s -> 598.44s]  Yeah.
[598.44s -> 599.44s]  It's definitely difficult.
[601.44s -> 602.44s]  Yeah.
[602.44s -> 604.44s]  I guess in the future we could set up a camera in the classroom
[604.44s -> 607.44s]  because I was physically in the classroom
[607.44s -> 610.44s]  like the week before last week.
[610.44s -> 612.44s]  There were a lot of students there.
[614.44s -> 615.44s]  Oh, I see.
[615.44s -> 617.44s]  So in the classroom there is a camera?
[618.44s -> 621.44s]  There isn't, but I was there.
[621.44s -> 623.44s]  So I know there were a lot of students.
[623.44s -> 624.44s]  Uh-huh.
[624.44s -> 625.44s]  OK.
[625.44s -> 626.44s]  Yeah.
[626.44s -> 628.44s]  Like some of our talks were able to host on Berkeley campus
[628.44s -> 630.44s]  if the speaker's able to come out,
[630.44s -> 632.44s]  but in other ways we're able to accommodate
[632.44s -> 634.44s]  just by having it all online.
[634.44s -> 635.44s]  Right.
[635.44s -> 636.44s]  Yeah.
[643.44s -> 648.44s]  So the online lectures are also being streamed on campus,
[648.44s -> 651.44s]  or is the audience going to be entirely from home?
[653.44s -> 655.44s]  We make sure our Berkeley students
[655.44s -> 657.44s]  are going to watch the recording
[657.44s -> 660.44s]  or the livestream on YouTube as well as our MOOC students.
[660.44s -> 661.44s]  Yeah.
[661.44s -> 663.44s]  No, but when they're watching it,
[663.44s -> 664.44s]  is there an actual classroom?
[664.44s -> 667.44s]  Is there an actual classroom where this is being streamed?
[667.44s -> 668.44s]  Oh, no.
[668.44s -> 671.44s]  We just have them watch it on their desktop at home
[671.44s -> 672.44s]  or wherever they're at.
[672.44s -> 673.44s]  OK.
[673.44s -> 674.44s]  Right, right, right.
[674.44s -> 675.44s]  That makes sense.
[685.44s -> 688.44s]  I think about the time we can probably get started,
[688.44s -> 693.44s]  and I can, well, if Tara thinks it's ready.
[694.44s -> 695.44s]  Yeah, yeah, go ahead.
[695.44s -> 696.44s]  Sure.
[696.44s -> 698.44s]  I can introduce the speaker today.
[698.44s -> 703.44s]  And Surat is a professor of computer science at UT Austin
[703.44s -> 706.44s]  and visiting research at Google DeepMind.
[706.44s -> 710.44s]  His research lies at the intersection of programming languages,
[710.44s -> 712.44s]  automated reasoning, and machinery.
[712.44s -> 715.44s]  He aims to develop a new class of intelligent systems
[715.44s -> 719.44s]  that are reliable, transparent, and secure by construction
[719.44s -> 722.44s]  and can solve reasoning-intensive tasks.
[722.44s -> 725.44s]  Surat has received various awards,
[725.44s -> 729.44s]  including an NSF career award and several dissertation award
[729.44s -> 732.44s]  and also several distinguished paper award.
[732.44s -> 734.44s]  And he also served as the program chair
[734.44s -> 736.44s]  for CAV and ICLR.
[736.44s -> 739.44s]  And yeah, let's welcome Surat.
[741.44s -> 744.44s]  Thank you very much, Caillou, for that introduction.
[744.44s -> 748.44s]  And even though I can't see any of the students,
[748.44s -> 751.44s]  thank you so much for your attention.
[751.44s -> 756.44s]  I hope that I'll be able to teach you something interesting today.
[758.44s -> 760.44s]  So in this course,
[760.44s -> 765.44s]  you have seen a lot of different kinds of uses of LLMs.
[765.44s -> 771.44s]  And today I'm going to emphasize the use of LLMs
[771.44s -> 775.44s]  as a tool of scientific and mathematical discovery.
[775.44s -> 778.44s]  And in particular, there's going to be an emphasis
[778.44s -> 783.44s]  on the ability of LLMs to discover abstract concepts
[783.44s -> 787.44s]  and how this can accelerate these discovery processes.
[790.44s -> 793.44s]  Okay, so let us dive in.
[793.44s -> 798.44s]  Let us start with the problem of mathematical discovery.
[798.44s -> 801.44s]  This is, of course, one of the grandest challenges
[801.44s -> 803.44s]  known to humanity.
[804.44s -> 810.44s]  Now, how does the process of mathematical discovery look like?
[810.44s -> 814.44s]  There is the process of modeling
[814.44s -> 817.44s]  where you are getting inspired
[817.44s -> 819.44s]  by some sort of a real-world phenomenon
[819.44s -> 821.44s]  and then modeling it
[821.44s -> 827.44s]  in terms of a variety of definitions and concepts.
[827.44s -> 830.44s]  Now, once you have a set of definitions,
[830.44s -> 835.44s]  at that point, you may want to conjecture
[835.44s -> 837.44s]  various properties of these definitions.
[837.44s -> 843.44s]  And then these are potentially candidate theorems and lemmas,
[843.44s -> 847.44s]  and then there is this process of rigorous reasoning
[847.44s -> 851.44s]  where you are actually proving these statements.
[851.44s -> 856.44s]  And this process is for human mathematicians intrinsically social.
[856.44s -> 861.44s]  You are convincing each other about the rigor of your reasoning
[861.44s -> 863.44s]  and the value of your conjectures, right?
[863.44s -> 865.44s]  But this process just continues
[865.44s -> 868.44s]  in this sort of an iterative loop,
[868.44s -> 871.44s]  so you may see proofs and counterexamples,
[871.44s -> 873.44s]  and these may influence your models
[873.44s -> 875.44s]  as well as your conjectures,
[875.44s -> 877.44s]  and this process just goes on, right?
[877.44s -> 882.44s]  That's the history of mathematics.
[883.44s -> 886.44s]  Now, the dream of the field of AI for math
[886.44s -> 890.44s]  is that you want to automate substantial pieces
[890.44s -> 893.44s]  of this process, and as a result,
[893.44s -> 895.44s]  you want to accelerate this process even more.
[895.44s -> 898.44s]  You want to be able to discover new kinds of math,
[898.44s -> 905.44s]  and this includes settling known conjectures
[905.44s -> 910.44s]  but also potentially coming up with new conjectures.
[910.44s -> 914.44s]  And then there is the setting of natural sciences
[914.44s -> 917.44s]  that is also going to be something I'll cover
[917.44s -> 919.44s]  in this lecture.
[919.44s -> 922.44s]  And here, unlike in math,
[922.44s -> 925.44s]  you are engaging with the real world.
[925.44s -> 927.44s]  There is this process of theory building
[927.44s -> 932.44s]  where you're coming up with hypotheses about the universe,
[932.44s -> 937.44s]  and then you are using these hypotheses
[937.44s -> 939.44s]  to design experiments,
[939.44s -> 942.44s]  and then you are experimentally validating them, right?
[942.44s -> 944.44s]  And this experimental validation process
[944.44s -> 947.44s]  creates new data,
[947.44s -> 951.44s]  and then this data leads to new kinds of theories,
[951.44s -> 955.44s]  for example, by refinement of the existing hypotheses,
[955.44s -> 957.44s]  or in some cases, you may want to just come up
[957.44s -> 962.44s]  with entirely new concepts and new predictions.
[962.44s -> 967.44s]  So the ambition of the area of AI for science
[967.44s -> 973.44s]  is to automate hypothesis generation and experiment design.
[973.44s -> 979.44s]  Now, the key ideas in the areas of AI for math
[979.44s -> 981.44s]  and AI for science at this point
[981.44s -> 986.44s]  are that, number one, search is a really powerful concept,
[986.44s -> 991.44s]  and so we want to imagine these spaces of hypotheses,
[991.44s -> 996.44s]  conjectures, and proofs, and systematically search them.
[996.44s -> 999.44s]  Now, this is, of course, a very massive space.
[999.44s -> 1004.44s]  There are all these, you know, potential statements
[1004.44s -> 1006.44s]  about various mathematical objects,
[1006.44s -> 1008.44s]  and there are all these hypotheses
[1008.44s -> 1010.44s]  that are possible in the universe,
[1010.44s -> 1013.44s]  and this space is, you know, clearly infinite.
[1013.44s -> 1018.44s]  Now, how do you explore this space in a tractable way?
[1018.44s -> 1020.44s]  Well, you use prior knowledge
[1020.44s -> 1022.44s]  to prioritize directions of search,
[1022.44s -> 1025.44s]  and that's also how humans proceed.
[1025.44s -> 1028.44s]  We don't just do discovery in a vacuum.
[1028.44s -> 1031.44s]  We read existing papers,
[1031.44s -> 1034.44s]  and sometimes we incrementally advance on them,
[1034.44s -> 1036.44s]  and sometimes we make a big leap,
[1036.44s -> 1041.44s]  but that prior knowledge is always of value.
[1041.44s -> 1044.44s]  But then also we learn from experience
[1044.44s -> 1048.44s]  how and how not to do this sort of exploration.
[1048.44s -> 1052.44s]  You know, you find out that you made certain choices
[1052.44s -> 1054.44s]  while building your theory,
[1054.44s -> 1056.44s]  and then you find that this just didn't work, right,
[1056.44s -> 1058.44s]  and that's experience for you,
[1058.44s -> 1060.44s]  and then you use this experience
[1060.44s -> 1066.44s]  to potentially take completely new directions.
[1066.44s -> 1069.44s]  And then another piece that is very important,
[1069.44s -> 1072.44s]  has been very important to the history of math and science,
[1072.44s -> 1077.44s]  has been to discover abstract concepts and tools.
[1077.44s -> 1081.44s]  So, for example, you know, the idea of calculus
[1081.44s -> 1084.44s]  that really sped up the progress of science
[1084.44s -> 1086.44s]  after it was invented.
[1086.44s -> 1088.44s]  Similarly with algebra,
[1088.44s -> 1092.44s]  and really all major branches of mathematics
[1092.44s -> 1096.44s]  came from the need to abstract
[1096.44s -> 1099.44s]  some sort of real-world process.
[1099.44s -> 1101.44s]  And then the value of these abstractions
[1101.44s -> 1105.44s]  is that they speed up both this process of exploration
[1105.44s -> 1109.44s]  and also this learning process that I previously mentioned.
[1109.44s -> 1115.44s]  Now, if you think of it, LLMs bring something to the table
[1115.44s -> 1118.44s]  regarding each of these criteria.
[1118.44s -> 1122.44s]  So, for example, you know, if you are doing a search
[1122.44s -> 1128.44s]  in a very large space of potential scientific hypotheses,
[1128.44s -> 1132.44s]  an LLM has inside it a lot of knowledge
[1132.44s -> 1135.44s]  about what sort of hypotheses
[1135.44s -> 1139.44s]  have historically tended to work in science,
[1139.44s -> 1145.44s]  and so this knowledge can potentially help you
[1145.44s -> 1148.44s]  direct the search process.
[1148.44s -> 1153.44s]  And then also, you know, in this course,
[1153.44s -> 1156.44s]  you have seen examples of LLM agents,
[1156.44s -> 1159.44s]  and in this class, you're going to see more of this.
[1159.44s -> 1162.44s]  So these are systems that contain inside them
[1162.44s -> 1164.44s]  LLMs as modules,
[1164.44s -> 1166.44s]  but they're also interacting with the world,
[1166.44s -> 1168.44s]  and they're learning from experience,
[1168.44s -> 1172.44s]  and this experience is gathered as data
[1172.44s -> 1175.44s]  and then often stuck into the prompt,
[1175.44s -> 1178.44s]  but sometimes you can even do reinforcement learning
[1178.44s -> 1182.44s]  or other kinds of, you know, modification of the LLM's weights.
[1182.44s -> 1185.44s]  But the point is that there are all these established methods
[1185.44s -> 1189.44s]  for doing learning combined with the use of language models,
[1189.44s -> 1192.44s]  and that is something that we can exploit here as well.
[1194.44s -> 1197.44s]  Now, the use of LLM's for abstraction,
[1197.44s -> 1201.44s]  this is something that's not well explored yet.
[1201.44s -> 1204.44s]  This is a really new idea,
[1204.44s -> 1207.44s]  but I'm going to give you some evidence
[1207.44s -> 1212.44s]  as to why there might be a lot of things going on here,
[1212.44s -> 1216.44s]  and there is certainly a lot more to be done
[1216.44s -> 1220.44s]  about the use of LLM's in constructing new concepts
[1220.44s -> 1222.44s]  and new tools.
[1222.44s -> 1229.44s]  So I think that LLM agents have tremendous promise
[1229.44s -> 1232.44s]  in mathematical and scientific discovery,
[1232.44s -> 1240.44s]  and the reason is that so far as all of these ideas go,
[1240.44s -> 1247.44s]  LLM's have a potential to contribute with regards to each.
[1248.44s -> 1251.44s]  So this may all seem somewhat abstract,
[1251.44s -> 1255.44s]  so I'm going to now jump into some concrete results,
[1255.44s -> 1258.44s]  and this lecture is divided into two pieces.
[1258.44s -> 1261.44s]  In the first part, we are going to talk about
[1261.44s -> 1263.44s]  AI for mathematical discovery,
[1263.44s -> 1265.44s]  and then in the second part,
[1265.44s -> 1268.44s]  we are going to talk about AI for scientific discovery,
[1268.44s -> 1270.44s]  and in each of these cases,
[1270.44s -> 1277.44s]  I'm going to show you some LLM agents
[1277.44s -> 1280.44s]  built using quite different algorithms,
[1280.44s -> 1286.44s]  but we'll see that these agents realize
[1286.44s -> 1288.44s]  many of these ideas that we talked about
[1288.44s -> 1291.44s]  in the previous slide.
[1291.44s -> 1295.44s]  Okay, so let's talk about AI for math.
[1295.44s -> 1297.44s]  Over the last few years,
[1297.44s -> 1300.44s]  there's been tremendous interest in this field.
[1300.44s -> 1303.44s]  Here's Terry Tao from 2023
[1303.44s -> 1307.44s]  talking about how AI from that year
[1307.44s -> 1310.44s]  can already generate suggestive hints
[1310.44s -> 1313.44s]  and help with the mathematical process,
[1313.44s -> 1316.44s]  and he predicted that by 2026,
[1316.44s -> 1319.44s]  AI can become a trustworthy co-author
[1319.44s -> 1324.44s]  in math research papers and in other fields as well.
[1324.44s -> 1327.44s]  Now, 2026 is not yet here,
[1327.44s -> 1331.44s]  but I would say that this prediction
[1331.44s -> 1335.44s]  seems very plausible as of now.
[1335.44s -> 1339.44s]  Now, in terms of concrete systems,
[1339.44s -> 1342.44s]  O1, when it was introduced,
[1342.44s -> 1347.44s]  one of the highlights was that it did really well
[1347.44s -> 1350.44s]  in various competition mathematics problems,
[1350.44s -> 1353.44s]  so here is a plot.
[1353.44s -> 1357.44s]  It shows how O1 does compared to GPD4O,
[1357.44s -> 1360.44s]  and as you see, accuracy in this particular competition,
[1360.44s -> 1366.44s]  AI-ME 2024, has gone up very substantially.
[1366.44s -> 1368.44s]  And then, of course, last year,
[1368.44s -> 1371.44s]  my colleagues at Google DeepMind,
[1371.44s -> 1374.44s]  they built a system that achieved
[1374.44s -> 1377.44s]  silver medal-level performance at the IMO,
[1377.44s -> 1380.44s]  and you've already heard her talk about that
[1380.44s -> 1381.44s]  in this course,
[1381.44s -> 1384.44s]  so I'm not going to go into further detail
[1384.44s -> 1387.44s]  or too much more detail about that in this talk,
[1387.44s -> 1391.44s]  but you know that it's impressive,
[1391.44s -> 1394.44s]  and it uses a lot of very impressive machinery.
[1397.44s -> 1403.44s]  Now, one question that I think is important to ask
[1403.44s -> 1408.44s]  is that is it all going to be done just with LLMs?
[1408.44s -> 1415.44s]  In other words, do you need any sort of additional machinery
[1415.44s -> 1420.44s]  that combines LLMs with, for example, actions?
[1420.44s -> 1422.44s]  This is really the characteristic
[1422.44s -> 1428.44s]  that distinguishes LLM agents from just basic LLMs,
[1428.44s -> 1435.44s]  and so I would say that there are certainly still believers
[1436.44s -> 1442.44s]  in the potential of just LLMs alone to do everything
[1442.44s -> 1444.44s]  after being trained on language,
[1444.44s -> 1448.44s]  but I am personally a little bit skeptical
[1448.44s -> 1450.44s]  about that possibility.
[1450.44s -> 1453.44s]  Now, that said, you know, one must admit
[1453.44s -> 1455.44s]  that over the last couple of years,
[1455.44s -> 1458.44s]  just LLMs have come a long way,
[1458.44s -> 1462.44s]  so here is the picture of how you train an LLM
[1463.44s -> 1466.44s]  for these sorts of mathematical settings,
[1466.44s -> 1469.44s]  so you're going to start with a pre-trained LLM,
[1469.44s -> 1471.44s]  then you're going to train it on, you know,
[1471.44s -> 1473.44s]  math-related web docs
[1473.44s -> 1476.44s]  and then, you know, problems with step-by-step solutions,
[1476.44s -> 1478.44s]  problems with tool-integrated solutions,
[1478.44s -> 1480.44s]  and so on and so forth,
[1480.44s -> 1483.44s]  and then ultimately you have this LLM that can use,
[1483.44s -> 1485.44s]  for example, Python APIs,
[1485.44s -> 1488.44s]  and that can solve math problems,
[1489.44s -> 1494.44s]  and this has, again, as I said, come a long way,
[1494.44s -> 1497.44s]  so, for example, here is a problem
[1497.44s -> 1500.44s]  from the Patna math competition from last year,
[1500.44s -> 1503.44s]  and Owen's response on this problem
[1503.44s -> 1506.44s]  is actually very reasonable,
[1506.44s -> 1511.44s]  and as these systems are getting better and better,
[1511.44s -> 1515.44s]  it seems like they're solving more and more problems
[1515.44s -> 1520.44s]  that are of this nature that are similarly hard or harder.
[1520.44s -> 1525.44s]  But that said, there are some fundamental weaknesses
[1525.44s -> 1528.44s]  of this neural-only approach
[1528.44s -> 1531.44s]  that you'd get with just a basic LLM,
[1531.44s -> 1536.44s]  and the reason is that, or one fundamental reason
[1536.44s -> 1540.44s]  is that there is this issue of data scarcity.
[1540.44s -> 1544.44s]  In order to do rigorous mathematical reasoning,
[1544.44s -> 1549.44s]  not just computing answers to well-posed questions,
[1549.44s -> 1554.44s]  you need to have a lot of high-quality traces
[1554.44s -> 1556.44s]  of mathematical reasoning,
[1556.44s -> 1560.44s]  or alternatively you would need high-quality reward functions,
[1560.44s -> 1564.44s]  and this is difficult to get just with human-produced data
[1564.44s -> 1568.44s]  beyond high school or competition settings.
[1568.44s -> 1570.44s]  And then a second reason fundamentally
[1570.44s -> 1574.44s]  is that natural language reasoning is hard to verify,
[1574.44s -> 1580.44s]  and this is something that is a fundamental weakness
[1580.44s -> 1583.44s]  of this language-only models,
[1583.44s -> 1585.44s]  and there are even settings
[1585.44s -> 1588.44s]  where you can apply this sort of mathematical reasoning
[1588.44s -> 1590.44s]  where, for example, when you are doing mathematical reasoning
[1590.44s -> 1594.44s]  about, you know, real-world systems
[1594.44s -> 1599.44s]  and the behaviors of these systems
[1599.44s -> 1601.44s]  have real consequences,
[1601.44s -> 1603.44s]  so edge cases are really critical,
[1603.44s -> 1605.44s]  so in these cases you want your mathematical reasoning
[1605.44s -> 1608.44s]  to be really robust and rigorous.
[1608.44s -> 1613.44s]  And unfortunately, language-based,
[1613.44s -> 1615.44s]  purely neural mathematical reasoning
[1615.44s -> 1620.44s]  tends to not be this way.
[1620.44s -> 1622.44s]  And so it is entirely possible
[1622.44s -> 1626.44s]  that scaling is going to solve all of these problems,
[1626.44s -> 1630.44s]  but as I said, I'm a little bit skeptical about this.
[1630.44s -> 1634.44s]  So in this talk, I'm going to demonstrate
[1634.44s -> 1638.44s]  an alternative strategy, which is similar in many ways
[1638.44s -> 1642.44s]  to alpha proof, which you saw previously,
[1642.44s -> 1646.44s]  but which is more of an LLM agent approach
[1646.44s -> 1652.44s]  that is an alternative to this sort of purely neural,
[1652.44s -> 1658.44s]  purely language-driven kind of model.
[1658.44s -> 1662.44s]  So this alternative involves formal representations,
[1662.44s -> 1665.44s]  just like in alpha proof.
[1665.44s -> 1670.44s]  So there's an entire literature that is building up
[1670.44s -> 1672.44s]  that follows this kind of strategy,
[1672.44s -> 1677.44s]  and so here what you do is that rather than just operating
[1677.44s -> 1679.44s]  in this space of informal problem statements
[1679.44s -> 1681.44s]  and informal solutions,
[1681.44s -> 1687.44s]  you'd be first doing what is called auto-formalization.
[1687.44s -> 1689.44s]  So you're going to go from the informal statement
[1689.44s -> 1692.44s]  to a formal statement in a language
[1692.44s -> 1695.44s]  like Lean or Coq or Isabel,
[1695.44s -> 1699.44s]  and Lean in particular is what is used for alpha proof,
[1699.44s -> 1702.44s]  but there are other frameworks like this as well
[1702.44s -> 1708.44s]  that have a very long history.
[1708.44s -> 1712.44s]  Now, the appeal of these sorts of formal frameworks
[1712.44s -> 1716.44s]  is that a proof can be written in a step-by-step way,
[1716.44s -> 1718.44s]  just like code,
[1718.44s -> 1721.44s]  and not only that, a proof can be checked.
[1721.44s -> 1725.44s]  You can check if a proof that you have written
[1725.44s -> 1731.44s]  in this formal language actually implies a certain property
[1731.44s -> 1733.44s]  that you started out with,
[1733.44s -> 1736.44s]  so this theorem that you started out with,
[1736.44s -> 1738.44s]  and so the idea is that you would then,
[1738.44s -> 1742.44s]  in the second phase, take this formal problem statement,
[1742.44s -> 1744.44s]  and then you would be feeding it to a neural prover,
[1744.44s -> 1747.44s]  which is going to come up with this sequence of steps,
[1747.44s -> 1752.44s]  which are also called tactics, these rules that you apply.
[1752.44s -> 1756.44s]  So these are tactics, and you apply them one by one,
[1756.44s -> 1762.44s]  and then at the end, you get to this QED stage
[1762.44s -> 1767.44s]  where you have nothing left to prove,
[1767.44s -> 1770.44s]  and so that's how this process would work,
[1770.44s -> 1773.44s]  and the advantage here is that if you think
[1773.44s -> 1777.44s]  of the verification challenge that I mentioned earlier
[1777.44s -> 1781.44s]  where you want to check whether a claimed proof
[1781.44s -> 1783.44s]  is actually a proof, this is something
[1783.44s -> 1786.44s]  that you can establish relatively easily
[1786.44s -> 1790.44s]  using a proof assistant, right?
[1790.44s -> 1793.44s]  And in terms of the data issues,
[1793.44s -> 1796.44s]  so the appeal of this kind of verifiability
[1796.44s -> 1798.44s]  is that during training time,
[1798.44s -> 1801.44s]  you can synthetically generate a very,
[1801.44s -> 1805.44s]  very large number of candidate proofs,
[1805.44s -> 1807.44s]  and then you can immediately get feedback
[1807.44s -> 1811.44s]  on whether or not these proofs are correct.
[1811.44s -> 1816.44s]  So this turns this difficult problem of proof
[1816.44s -> 1819.44s]  into this kind of a game-playing task
[1819.44s -> 1821.44s]  which you can approach with really a version
[1821.44s -> 1823.44s]  of reinforcement learning,
[1823.44s -> 1826.44s]  and this is what, for example, Alpha Proof did,
[1826.44s -> 1829.44s]  although I'll show here that you can also take
[1829.44s -> 1832.44s]  a more LLM-based approach to this problem,
[1832.44s -> 1838.44s]  and in fact, you can build up
[1838.44s -> 1841.44s]  on these pretrained language models
[1841.44s -> 1844.44s]  and only have a pretty thin layer on top of that,
[1844.44s -> 1849.44s]  and that still does quite interestingly well.
[1849.44s -> 1850.44s]  Okay.
[1850.44s -> 1853.44s]  So just to go into a little bit more detail
[1853.44s -> 1855.44s]  about these formal representations,
[1855.44s -> 1857.44s]  even though you may have seen this
[1857.44s -> 1860.44s]  in the Alpha Proof lecture,
[1860.44s -> 1866.44s]  I'll just, you know, quickly recap, recapitulate this.
[1866.44s -> 1870.44s]  So here is a theorem that we are,
[1870.44s -> 1872.44s]  let's say we are trying to prove,
[1872.44s -> 1875.44s]  and what this says is that if X is even,
[1875.44s -> 1877.44s]  then X squared is even as well.
[1877.44s -> 1879.44s]  So the theorem statement over here
[1879.44s -> 1883.44s]  says that X is a natural number,
[1883.44s -> 1886.44s]  and then it says that X being even
[1886.44s -> 1890.44s]  implies X squared is also even.
[1890.44s -> 1894.44s]  So modulo two, the remainder is zero.
[1894.44s -> 1896.44s]  So now in order to prove this,
[1896.44s -> 1900.44s]  you're going to apply this sequence of tactics,
[1900.44s -> 1902.44s]  as I said previously.
[1902.44s -> 1908.44s]  Now, it's important to note that the system,
[1908.44s -> 1914.44s]  the proof assistant, it has a certain state
[1914.44s -> 1917.44s]  that depends on what the original theorem is
[1917.44s -> 1920.44s]  and which tactics you have applied so far.
[1920.44s -> 1922.44s]  So for example, in the beginning,
[1922.44s -> 1927.44s]  you would have a state that looks like this.
[1927.44s -> 1929.44s]  Now, just to spell this out, what this means
[1929.44s -> 1931.44s]  it says that there is one goal
[1931.44s -> 1934.44s]  that you currently need to prove,
[1934.44s -> 1937.44s]  and that goal is that under this context,
[1937.44s -> 1939.44s]  which is that X is a natural number,
[1939.44s -> 1943.44s]  this is the variable that you're allowed to use,
[1943.44s -> 1946.44s]  and then this symbol turnstile means
[1946.44s -> 1948.44s]  that this is what you need to prove.
[1948.44s -> 1952.44s]  So this is basically the theorem that you're starting out with
[1952.44s -> 1954.44s]  before you have applied that tactic.
[1954.44s -> 1958.44s]  Now, you have the state, and then you apply the tactic,
[1958.44s -> 1960.44s]  which is, you can think of it,
[1960.44s -> 1964.44s]  if you're thinking of this as a reinforcement learning setup,
[1964.44s -> 1966.44s]  you are applying an action.
[1966.44s -> 1970.44s]  And so you apply that action, and that leads to a new state.
[1970.44s -> 1974.44s]  And then this state here looks like this.
[1974.44s -> 1976.44s]  So this intro tactic, what it does
[1976.44s -> 1980.44s]  is that it introduces this new hypothesis,
[1980.44s -> 1983.44s]  which over here now says that, okay,
[1983.44s -> 1986.44s]  let us assume that X is even,
[1986.44s -> 1989.44s]  along with, sorry, yeah, X is even,
[1989.44s -> 1991.44s]  along with X being a natural number.
[1991.44s -> 1993.44s]  And this property that X is even,
[1993.44s -> 1995.44s]  we are going to give it a name, H,
[1995.44s -> 1998.44s]  and then under this context,
[1998.44s -> 1999.44s]  which is really like, you know,
[1999.44s -> 2001.44s]  you have now declared a new variable,
[2001.44s -> 2004.44s]  which has a certain type attached to it.
[2004.44s -> 2006.44s]  So now you are trying to show
[2006.44s -> 2009.44s]  that this X squared is even as well.
[2009.44s -> 2012.44s]  And then you apply this next tactic,
[2012.44s -> 2015.44s]  which leads to another state,
[2015.44s -> 2018.44s]  so these tactics are written by humans,
[2018.44s -> 2024.44s]  and they encode various rules of simplification in mathematics.
[2024.44s -> 2026.44s]  And so what you are really doing
[2026.44s -> 2028.44s]  is that for each of these rewrite rules
[2028.44s -> 2031.44s]  that you're applying, the state is changing.
[2031.44s -> 2033.44s]  So for example, now the state has changed
[2033.44s -> 2037.44s]  to you showing that, you know, if,
[2037.44s -> 2041.44s]  so this X modulo two times X modulo two,
[2041.44s -> 2044.44s]  the whole thing modulo two, that is equal to zero.
[2044.44s -> 2048.44s]  And then the next rewrite is going to simplify this,
[2048.44s -> 2050.44s]  and then finally you will end up with,
[2050.44s -> 2054.44s]  you know, zero equals zero, basically.
[2054.44s -> 2057.44s]  And at that point, you're going to hit QED
[2057.44s -> 2060.44s]  because your goal will just disappear, right?
[2060.44s -> 2062.44s]  So each of these rules you can think of
[2062.44s -> 2064.44s]  as simplifying the state, and then at the end,
[2064.44s -> 2066.44s]  ideally, you want to end up in the state
[2066.44s -> 2068.44s]  where there are no open goals,
[2068.44s -> 2072.44s]  and at that point, you can end the proof.
[2073.44s -> 2076.44s]  So that's what these formal proof assistants look like.
[2076.44s -> 2078.44s]  Really, they're state machines
[2078.44s -> 2080.44s]  where you have the proof state
[2080.44s -> 2082.44s]  and then these actions, which are these tactics,
[2082.44s -> 2085.44s]  and then they lead to a new proof state,
[2085.44s -> 2089.44s]  and your goal really is to find a sequence of tactics
[2089.44s -> 2092.44s]  that take you from this initial state
[2092.44s -> 2096.44s]  of the prover to this QED state
[2096.44s -> 2100.44s]  where there are no more goals to be discharged.
[2102.44s -> 2104.44s]  Okay.
[2104.44s -> 2108.44s]  So now let's talk about the two phases
[2108.44s -> 2110.44s]  that I mentioned previously.
[2110.44s -> 2113.44s]  The auto-formalization phase is something like this.
[2113.44s -> 2116.44s]  You are starting with this informal math statement.
[2116.44s -> 2118.44s]  There exists an infinite number of primes,
[2118.44s -> 2121.44s]  and then there is this, you know,
[2121.44s -> 2123.44s]  proof of this as well in natural language,
[2123.44s -> 2126.44s]  and now you are applying your auto-formalization model,
[2126.44s -> 2128.44s]  and later I'll talk a little bit
[2128.44s -> 2130.44s]  about how this could be done,
[2130.44s -> 2133.44s]  but, you know, the version zero of this would be that
[2133.44s -> 2137.44s]  you throw a language model at this informal statement
[2137.44s -> 2140.44s]  and then out comes this formal theorem, right?
[2140.44s -> 2142.44s]  And so this one is in Lean.
[2142.44s -> 2145.44s]  Now what this says is that,
[2146.44s -> 2150.44s]  well, I won't read this thing in detail,
[2150.44s -> 2154.44s]  but maybe I can read the theorem statement.
[2154.44s -> 2158.44s]  It says that, so for all n, there exists a p
[2158.44s -> 2161.44s]  such that p is bigger than, greater than equal to n,
[2161.44s -> 2163.44s]  and then p is also prime, right?
[2163.44s -> 2164.44s]  So that's how you establish
[2164.44s -> 2166.44s]  that there's an infinite number of primes,
[2166.44s -> 2169.44s]  and then the rest of it is the proof.
[2172.44s -> 2174.44s]  So then let's assume that you have done
[2174.44s -> 2175.44s]  this auto-formalization.
[2175.44s -> 2178.44s]  You have a formal statement of a theorem,
[2178.44s -> 2180.44s]  and now you're trying to prove it.
[2180.44s -> 2184.44s]  So previously I showed you that there are these tactics,
[2184.44s -> 2186.44s]  these simplification rules that you could apply,
[2186.44s -> 2188.44s]  but now you can imagine a search problem
[2188.44s -> 2190.44s]  where at each state,
[2190.44s -> 2194.44s]  there is a set of potential tactics that you could apply,
[2194.44s -> 2197.44s]  and this leads to the creation of a search tree,
[2197.44s -> 2200.44s]  and then your goal is to basically find a path
[2200.44s -> 2203.44s]  in this tree that ends up at QED.
[2204.44s -> 2206.44s]  Now, of course, the complexity over here
[2206.44s -> 2208.44s]  is that at each point,
[2208.44s -> 2210.44s]  you can have a very, very large number
[2210.44s -> 2213.44s]  of potential branches in this tree,
[2213.44s -> 2216.44s]  and then also this tree can be quite deep, right,
[2216.44s -> 2217.44s]  because if you're talking
[2217.44s -> 2220.44s]  about really advanced mathematical theorems,
[2220.44s -> 2223.44s]  you could end up with proofs
[2223.44s -> 2226.44s]  that are thousands and thousands of lines long,
[2226.44s -> 2228.44s]  and so this is a far harder problem
[2228.44s -> 2230.44s]  than any game-playing problem
[2230.44s -> 2234.44s]  that we have been able to solve so far,
[2234.44s -> 2237.44s]  so it's remarkable to see the kind of progress
[2237.44s -> 2240.44s]  that has already happened in this task
[2240.44s -> 2242.44s]  over the last few years.
[2244.44s -> 2247.44s]  Now, alpha proof, of course, you have already seen.
[2247.44s -> 2249.44s]  You have seen this picture.
[2249.44s -> 2250.44s]  There's this formalizer network
[2250.44s -> 2253.44s]  that does this formalization into formal problems,
[2253.44s -> 2255.44s]  and then the solver network,
[2255.44s -> 2258.44s]  it tries to generate these formal proofs.
[2258.44s -> 2261.44s]  One big insight was that even misformalized problems
[2261.44s -> 2263.44s]  are still helpful for learning,
[2263.44s -> 2267.44s]  and this sort of proof, the solver network,
[2267.44s -> 2269.44s]  this is being trained with RL,
[2269.44s -> 2271.44s]  and then one big insight was that
[2271.44s -> 2273.44s]  you could complement RL training with test time RL
[2273.44s -> 2275.44s]  when you are actually doing the IMO
[2275.44s -> 2278.44s]  or any other, solving any other math problem.
[2278.44s -> 2281.44s]  You are, you could generate,
[2281.44s -> 2286.44s]  you could basically do this sort of test time RL on that,
[2286.44s -> 2288.44s]  but, again, you have seen all of this,
[2288.44s -> 2291.44s]  so I'll move on.
[2291.44s -> 2294.44s]  The approach that I'm going to talk about today,
[2294.44s -> 2296.44s]  this is called COPRA.
[2296.44s -> 2298.44s]  This was published in a paper
[2298.44s -> 2302.44s]  at the Conference of Language Models last year,
[2302.44s -> 2305.44s]  and this is led by my Ph.D. student
[2305.44s -> 2308.44s]  at UT Austin, Amita Ushtakur.
[2308.44s -> 2314.44s]  Now, this is the overall picture of COPRA.
[2314.44s -> 2319.44s]  It still has this view of interaction
[2319.44s -> 2325.44s]  between an LLM that is predicting the proof steps
[2325.44s -> 2331.44s]  and the actual prover that is executing the step
[2331.44s -> 2333.44s]  that is proposed by the LLM
[2333.44s -> 2337.44s]  and then giving you a new state.
[2337.44s -> 2340.44s]  However, the difference is that while alpha proof
[2340.44s -> 2343.44s]  does this sort of, you know,
[2343.44s -> 2345.44s]  advanced reinforcement learning for training,
[2345.44s -> 2347.44s]  so here you're just taking
[2347.44s -> 2349.44s]  this sort of an LLM agent approach
[2349.44s -> 2353.44s]  where you have a frontier LLM,
[2353.44s -> 2358.44s]  and the frontier LLM is being provided the proof state
[2358.44s -> 2363.44s]  and some information from the history of the search so far,
[2363.44s -> 2367.44s]  and then it is making a prediction
[2367.44s -> 2370.44s]  as to what's the next step that you should take,
[2370.44s -> 2371.44s]  and then you are taking that step,
[2371.44s -> 2373.44s]  and you are executing it,
[2373.44s -> 2377.44s]  and then you are getting back a new state of the proof,
[2377.44s -> 2380.44s]  and then this process is continuing, right?
[2380.44s -> 2381.44s]  Now, one important thing
[2381.44s -> 2383.44s]  is that there is an external search
[2383.44s -> 2385.44s]  that is going on, a depth-first search,
[2385.44s -> 2387.44s]  and then inside that depth-first search,
[2387.44s -> 2389.44s]  you are doing all this, right?
[2389.44s -> 2391.44s]  But it is fundamentally different in the sense
[2391.44s -> 2393.44s]  that everything is in context.
[2393.44s -> 2398.44s]  There is no actual adjustment to the way it's happening,
[2398.44s -> 2400.44s]  but what is interesting is that even
[2400.44s -> 2402.44s]  with a relatively small amount of compute,
[2402.44s -> 2405.44s]  this approach was still able to do quite well,
[2405.44s -> 2407.44s]  and so I think that this strategy
[2407.44s -> 2410.44s]  has a lot of long-term promise,
[2411.44s -> 2416.44s]  for one, because it immediately leverages advances in LLMs.
[2416.44s -> 2420.44s]  As, for example, you go from GPT-4 to, you know,
[2420.44s -> 2425.44s]  O1 to O3 and to, you know, maybe some GPT-5,
[2425.44s -> 2427.44s]  then you are going to immediately leverage
[2427.44s -> 2431.44s]  those advantages, advances,
[2431.44s -> 2435.44s]  and it also has this ability to integrate natural language
[2435.44s -> 2439.44s]  and formal language reasoning
[2439.44s -> 2442.44s]  because ultimately you are just using an LLM
[2442.44s -> 2448.44s]  to generate the proofs in the formal language,
[2448.44s -> 2450.44s]  but at the same time, the LLM has the ability
[2450.44s -> 2452.44s]  to do natural language as well,
[2452.44s -> 2455.44s]  so this is something you can benefit from,
[2455.44s -> 2457.44s]  and I'm going to show concrete examples
[2457.44s -> 2460.44s]  of this in a little bit.
[2460.44s -> 2462.44s]  Another thing is that it's also,
[2462.44s -> 2464.44s]  this strategy is also applicable
[2464.44s -> 2467.44s]  even if there is no corpus of training problems.
[2467.44s -> 2471.44s]  Now, one of the challenges in doing AI for math
[2471.44s -> 2474.44s]  once you go outside of settings
[2474.44s -> 2475.44s]  such as math competitions
[2475.44s -> 2479.44s]  is that you're really trying to solve a new problem, right?
[2479.44s -> 2480.44s]  So if you're a math researcher,
[2480.44s -> 2482.44s]  you're trying to solve a new problem
[2482.44s -> 2485.44s]  that by design is quite different from the problems
[2485.44s -> 2489.44s]  that a lot of other people have solved previously,
[2489.44s -> 2492.44s]  so in such a situation,
[2492.44s -> 2498.44s]  it's a bit challenging to create a corpus of training problems
[2498.44s -> 2504.44s]  and then do, you know, training of an RL agent.
[2504.44s -> 2508.44s]  However, with an in-context learning agent,
[2508.44s -> 2513.44s]  the appeal is that you can still use it.
[2513.44s -> 2517.44s]  The LLM has incited a ton of prior knowledge,
[2517.44s -> 2521.44s]  and that knowledge is something that you could benefit from, right,
[2521.44s -> 2523.44s]  and the LLM can, for example,
[2523.44s -> 2525.44s]  bring in knowledge about related problems
[2525.44s -> 2529.44s]  even though it's not been explicitly trained on them
[2529.44s -> 2533.44s]  beyond the pre-training stage.
[2533.44s -> 2535.44s]  And in the longer run, of course,
[2535.44s -> 2539.44s]  the two approaches of in-context learning
[2539.44s -> 2544.44s]  and explicit training, these are not incompatible,
[2544.44s -> 2547.44s]  so you could imagine the in-context learning agent
[2547.44s -> 2552.44s]  being used to generate a lot of different traces
[2552.44s -> 2555.44s]  by interaction with the proof assistant,
[2555.44s -> 2559.44s]  and then you could use them for doing fine-tuning.
[2559.44s -> 2563.44s]  Now, one question that you may have over here is that,
[2563.44s -> 2567.44s]  well, so do I really need to even have Lean in that case?
[2567.44s -> 2572.44s]  Couldn't I just rely on, you know, the LLM alone?
[2572.44s -> 2573.44s]  And I would say that the difference here
[2573.44s -> 2578.44s]  is that this interaction with the proof assistant
[2578.44s -> 2582.44s]  is still providing, you know, a very strong form of grounding.
[2582.44s -> 2586.44s]  So, for example, if you take a step predicted by the LLM
[2586.44s -> 2590.44s]  and then you find that this is actually an invalid step,
[2590.44s -> 2591.44s]  you're going to get that feedback
[2591.44s -> 2593.44s]  right away from the proof assistant,
[2593.44s -> 2595.44s]  and you can take that feedback
[2595.44s -> 2599.44s]  and you can then stick it back into the prompt of the LLM,
[2599.44s -> 2603.44s]  and then you can, you know, ask it to continue,
[2603.44s -> 2605.44s]  and then hopefully it'll learn from its mistake
[2605.44s -> 2607.44s]  and carry on, right?
[2607.44s -> 2610.44s]  So, in the long run, it's possible that
[2610.44s -> 2612.44s]  once you have generated a very, very,
[2612.44s -> 2616.44s]  very large number of traces by interaction with the prover,
[2616.44s -> 2620.44s]  maybe you have fine-tuned your LLM on all this data,
[2620.44s -> 2622.44s]  and then the LLM is progressively getting better
[2622.44s -> 2627.44s]  and better, and there is a point beyond which
[2627.44s -> 2631.44s]  it just doesn't make any mistakes with this kind of proof.
[2631.44s -> 2633.44s]  Maybe we'll get there someday,
[2633.44s -> 2638.44s]  but we are still pretty far away from that possibility.
[2638.44s -> 2641.44s]  So, at least as of now,
[2641.44s -> 2644.44s]  I think that this sort of interaction
[2644.44s -> 2646.44s]  between LLM and proof assistant
[2646.44s -> 2649.44s]  is still extremely valuable.
[2649.44s -> 2651.44s]  Okay, so here is this COPRA system
[2651.44s -> 2653.44s]  in a little bit more detail.
[2653.44s -> 2655.44s]  So, what is going on here is that
[2655.44s -> 2657.44s]  you're starting with a formal theorem,
[2657.44s -> 2660.44s]  but you could also have some informal hints.
[2660.44s -> 2663.44s]  You could imagine bringing auto-formalization
[2663.44s -> 2666.44s]  into this picture as well, but we didn't,
[2666.44s -> 2669.44s]  and so the first step is that you create a prompt
[2669.44s -> 2672.44s]  based on the statement of the theorem,
[2672.44s -> 2679.44s]  and then you feed it to a frontier LLM,
[2679.44s -> 2684.44s]  and you just ask it to generate some potential tactics
[2685.44s -> 2687.44s]  that would be of interest,
[2687.44s -> 2695.44s]  and now your model has predicted
[2695.44s -> 2700.44s]  a set of one of the actions or tactics in this case,
[2700.44s -> 2702.44s]  and then you parse that tactic
[2702.44s -> 2704.44s]  because it will be in the form of text,
[2704.44s -> 2707.44s]  so now you parse it into a structured format,
[2707.44s -> 2713.44s]  and then you just execute it via the proof environment,
[2713.44s -> 2716.44s]  and this could be Lean or Coq or SML,
[2716.44s -> 2720.44s]  and then after you do that, in the lucky case,
[2720.44s -> 2724.44s]  you already hit QED, and this is all done,
[2724.44s -> 2730.44s]  but if you fail, if this tactic is invalid
[2730.44s -> 2734.44s]  or there are still some open goals
[2734.44s -> 2736.44s]  that you still need to resolve,
[2736.44s -> 2739.44s]  in that case, you get some feedback,
[2739.44s -> 2743.44s]  and then you augment the prompt
[2743.44s -> 2745.44s]  and then backtrack if needed,
[2745.44s -> 2750.44s]  and then we also have this retrieval phase over here
[2750.44s -> 2755.44s]  which seemed to add to the performance of the system,
[2755.44s -> 2758.44s]  and here you are also able to query a lemma database
[2758.44s -> 2761.44s]  and bring in some relevant lemmas
[2761.44s -> 2763.44s]  and then add it back to the prompt,
[2763.44s -> 2766.44s]  and this process just continues over and over and over again
[2766.44s -> 2770.44s]  until you have resolved the original theorem
[2770.44s -> 2774.44s]  or alternatively, you have timed out.
[2774.44s -> 2777.44s]  So here's the prompt of the system
[2777.44s -> 2779.44s]  in a little bit more detail,
[2779.44s -> 2784.44s]  so here, as you see, there is the serialized proof state,
[2784.44s -> 2786.44s]  and here you have, you know,
[2786.44s -> 2789.44s]  what is the goal that you still have to prove
[2789.44s -> 2793.44s]  and what are the hypotheses that are being made,
[2793.44s -> 2795.44s]  so over here, for example,
[2795.44s -> 2797.44s]  the hypotheses include,
[2797.44s -> 2799.44s]  these are the assumptions you're making, right?
[2799.44s -> 2803.44s]  So this includes X being of type natural numbers
[2803.44s -> 2806.44s]  and then also, you know, X being even,
[2806.44s -> 2808.44s]  and then you have the goal,
[2808.44s -> 2813.44s]  which is X squared modulo 2 is equal to 0.
[2813.44s -> 2817.44s]  We also have this information about the last step,
[2817.44s -> 2819.44s]  so the way to read this is that from left to right,
[2819.44s -> 2823.44s]  you are looking at successive queries to the LLM,
[2823.44s -> 2826.44s]  so the first column over here,
[2826.44s -> 2828.44s]  this includes information about, you know,
[2828.44s -> 2830.44s]  what is the first query that you're making,
[2830.44s -> 2832.44s]  so the first thing that you have done,
[2832.44s -> 2838.44s]  so let's say that you already have made this decision
[2838.44s -> 2841.44s]  that you're going to have this intro H tactic, right?
[2841.44s -> 2843.44s]  So that's what created that hypothesis,
[2843.44s -> 2845.44s]  X squared modulo 2 equals 0.
[2845.44s -> 2847.44s]  So then this is the last step.
[2847.44s -> 2853.44s]  This is stored in a stack and fed as part of the prompt.
[2853.44s -> 2858.44s]  Now, then at this point, there has been no error,
[2858.44s -> 2864.44s]  so then, you know, you get this response from this LLM
[2864.44s -> 2866.44s]  after you feed this information
[2866.44s -> 2869.44s]  that the next thing that you should do is RWH,
[2869.44s -> 2872.44s]  and then now if you try to execute it,
[2872.44s -> 2875.44s]  you might get, you know, an error message
[2875.44s -> 2878.44s]  because the rewrite tactic failed,
[2878.44s -> 2881.44s]  and then, you know, this information,
[2881.44s -> 2884.44s]  this error feedback, you send it back to the LLM,
[2884.44s -> 2886.44s]  and then that's the next query.
[2886.44s -> 2888.44s]  You say that try again LLM,
[2888.44s -> 2890.44s]  and then the LLM says that, okay,
[2890.44s -> 2894.44s]  let's try apply nat dot mul mod write.
[2894.44s -> 2897.44s]  Now, the details of this tactic doesn't matter.
[2897.44s -> 2900.44s]  It's, you know, something involving modular multiplication,
[2900.44s -> 2902.44s]  but the point is this is also wrong,
[2902.44s -> 2903.44s]  so then again at the next step,
[2903.44s -> 2906.44s]  you get an error feedback, error message,
[2906.44s -> 2910.44s]  and this is something you stick back into the prompt,
[2910.44s -> 2912.44s]  and then you make a query again,
[2912.44s -> 2915.44s]  and now it says, oh, let's try this rewrite
[2915.44s -> 2920.44s]  with nat dot mul mod, and yay, this time it works,
[2920.44s -> 2922.44s]  so now you see that the interaction result
[2922.44s -> 2924.44s]  doesn't have any error message in it.
[2924.44s -> 2930.44s]  It's a success, and so this is now put into the stack,
[2930.44s -> 2932.44s]  and so the last step, as you see, has changed.
[2932.44s -> 2936.44s]  It's now rwnat dot mul mod,
[2936.44s -> 2940.44s]  so now you're going to try to generate the next step,
[2940.44s -> 2945.44s]  and then this time the LLM generates this rwh, right,
[2945.44s -> 2948.44s]  and so this is how this process is going to continue.
[2948.44s -> 2952.44s]  In a nutshell, you may query the LLM,
[2952.44s -> 2955.44s]  get some error message,
[2955.44s -> 2958.44s]  and then stick it back into the prompt,
[2958.44s -> 2960.44s]  and then continue, right,
[2960.44s -> 2964.44s]  and this process will just go on.
[2964.44s -> 2971.44s]  Okay, so this evaluation was done in 2024,
[2971.44s -> 2973.44s]  actually late 2023,
[2973.44s -> 2975.44s]  so, you know, the world has changed since then,
[2975.44s -> 2978.44s]  but at least at that point, we compared with GPT-4
[2978.44s -> 2983.44s]  and found that GPT-4 frequently gave incorrect errors,
[2983.44s -> 2985.44s]  incorrect proofs,
[2985.44s -> 2992.44s]  and by contrast, this feedback with the proof environment,
[2992.44s -> 2995.44s]  LEAN in this case, really helped.
[2995.44s -> 2998.44s]  COPRA, and so even though it was using
[2998.44s -> 3000.44s]  the same underlying model,
[3000.44s -> 3003.44s]  the interaction allowed it to create a correct proof
[3003.44s -> 3007.44s]  in this case.
[3007.44s -> 3009.44s]  Here are some experimental results.
[3009.44s -> 3012.44s]  You can see the details in the paper,
[3012.44s -> 3014.44s]  but in this picture on the x-axis,
[3014.44s -> 3016.44s]  you see the number of queries,
[3016.44s -> 3019.44s]  and on the y-axis, you see the number of theorems proved,
[3019.44s -> 3024.44s]  so what we saw over here is that pretty quickly,
[3024.44s -> 3027.44s]  the COPRA approach was able to prove a lot of theorems
[3027.44s -> 3030.44s]  with a relatively small number of queries,
[3030.44s -> 3039.44s]  whereas the other approach is that that didn't get that far,
[3039.44s -> 3042.44s]  and here is an evaluation on Mini-F2F test,
[3042.44s -> 3045.44s]  so the absolute numbers are not great,
[3045.44s -> 3048.44s]  but then the amount of compute we provided,
[3048.44s -> 3051.44s]  COPRA was also relatively small.
[3051.44s -> 3055.44s]  We just allowed 60 queries per proof attempt.
[3055.44s -> 3061.44s]  Now, what we saw here was that we got
[3061.44s -> 3063.44s]  quite reasonable performance,
[3063.44s -> 3067.44s]  comparable with the state-of-the-art methods.
[3068.44s -> 3072.44s]  Now, of course, since then,
[3072.44s -> 3074.44s]  the state-of-the-art on this benchmark
[3074.44s -> 3076.44s]  has moved quite a bit, so Mini-F2F,
[3076.44s -> 3078.44s]  for those of you who don't know, is, by the way,
[3078.44s -> 3084.44s]  it's a compilation of competition mathematics problems,
[3084.44s -> 3086.44s]  mostly at the high school level,
[3086.44s -> 3092.44s]  and so it's of the form that here is a theorem
[3092.44s -> 3095.44s]  and come up with a lean proof,
[3095.44s -> 3100.44s]  so here what we see is that there was this around 29%
[3100.44s -> 3103.44s]  or 30% success rate,
[3103.44s -> 3106.44s]  which is behind the current SOTA,
[3106.44s -> 3108.44s]  but I think one question would be that
[3108.44s -> 3110.44s]  if you take this approach and scale it up,
[3110.44s -> 3112.44s]  then where would you end up,
[3112.44s -> 3115.44s]  and there are no public results on that yet.
[3117.44s -> 3119.44s]  So in this talk, though,
[3119.44s -> 3124.44s]  I'm going to go beyond the method
[3124.44s -> 3129.44s]  that we just presented, which is the basic COPRA,
[3129.44s -> 3132.44s]  and talk about some results
[3132.44s -> 3135.44s]  that we just recently developed.
[3135.44s -> 3137.44s]  This is not published yet,
[3137.44s -> 3146.44s]  and here we are leveraging the ability of LLMs to abstract
[3146.44s -> 3150.44s]  and also to use natural language reasoning,
[3150.44s -> 3152.44s]  because I think that this is really
[3152.44s -> 3156.44s]  where the sort of approach that we just presented,
[3156.44s -> 3159.44s]  this shines because of the tremendous amount
[3159.44s -> 3162.44s]  of world knowledge that is stored inside LLMs.
[3162.44s -> 3168.44s]  We can really use them in a lot more powerful capacity
[3168.44s -> 3174.44s]  than just as a predictor of the next tactic,
[3174.44s -> 3178.44s]  and so here's a sample problem.
[3178.44s -> 3181.44s]  This is from IMO, and this is saying that, you know,
[3181.44s -> 3183.44s]  prove that this fraction is irreducible
[3183.44s -> 3186.44s]  for every natural number n.
[3186.44s -> 3193.44s]  So what we'll see is that the COPRA approach
[3193.44s -> 3196.44s]  can be extended a little bit
[3196.44s -> 3201.44s]  to, in fact, come up with a hierarchical solution
[3201.44s -> 3203.44s]  to this problem,
[3203.44s -> 3206.44s]  and so this will involve breaking down the problem
[3206.44s -> 3209.44s]  into subproblems and then solving hierarchically.
[3209.44s -> 3211.44s]  Okay, so how do we do this?
[3211.44s -> 3216.44s]  Well, first what we do is that we ask the LLM
[3216.44s -> 3220.44s]  to come up with an informal solution to this problem,
[3220.44s -> 3225.44s]  and so I don't know if this font is too small to read,
[3225.44s -> 3227.44s]  but basically what is going on here
[3227.44s -> 3231.44s]  is that this was fed to O3, a reasoning model,
[3231.44s -> 3233.44s]  and so it was able to come up with this,
[3233.44s -> 3235.44s]  you know, step-by-step plan.
[3235.44s -> 3239.44s]  So, for example, you first define the GCD,
[3239.44s -> 3242.44s]  and then you, you know, subtract to simplify.
[3242.44s -> 3244.44s]  Then you apply the Euclidean algorithm,
[3244.44s -> 3246.44s]  and then you conclude something about the GCD,
[3246.44s -> 3251.44s]  and then you have the final conclusion.
[3251.44s -> 3254.44s]  So then we say that, okay,
[3254.44s -> 3258.44s]  so let's take this informal decomposition
[3258.44s -> 3260.44s]  and put it in the context.
[3260.44s -> 3264.44s]  Now let's say that it's our goal
[3264.44s -> 3268.44s]  to prove this lemma in lean-for, right?
[3268.44s -> 3272.44s]  So this is the formal statement of the same problem,
[3272.44s -> 3275.44s]  and we are just saying that don't try to write the proof.
[3275.44s -> 3278.44s]  Okay, so let's just write the subgoals
[3278.44s -> 3280.44s]  that are needed to prove this,
[3280.44s -> 3285.44s]  but we are doing this conditioned on the informal proof
[3285.44s -> 3287.44s]  that we just generated,
[3288.44s -> 3292.44s]  and so here is the proof of,
[3292.44s -> 3295.44s]  or here is the decomposition of this lemma.
[3295.44s -> 3299.44s]  So what you see is that it comes up with,
[3299.44s -> 3302.44s]  you know, these three subgoals.
[3302.44s -> 3307.44s]  So first you show that, you know,
[3307.44s -> 3310.44s]  this GCD step one is true,
[3310.44s -> 3313.44s]  and it's basically saying that this GCD
[3313.44s -> 3315.44s]  of 21n plus 4 times 14n plus 3
[3315.44s -> 3317.44s]  is the same as the GCD of this, you know,
[3317.44s -> 3320.44s]  14n plus 3 times 7n plus 1.
[3323.44s -> 3325.44s]  So then there is the second one,
[3325.44s -> 3326.44s]  then there's the third one,
[3326.44s -> 3331.44s]  and so on and so forth,
[3331.44s -> 3333.44s]  and then finally you have the main theorem
[3333.44s -> 3335.44s]  that combines all of the above.
[3336.44s -> 3339.44s]  So the appeal of this sort of decomposition
[3339.44s -> 3344.44s]  is that now this very complex problem
[3344.44s -> 3348.44s]  has been simplified into these three subproblems
[3348.44s -> 3351.44s]  which are a lot more tractable.
[3351.44s -> 3353.44s]  So then in the next step,
[3353.44s -> 3358.44s]  we then ask COPRA to solve the subproblems one by one,
[3359.44s -> 3364.44s]  and what we see is that COPRA is in fact able
[3364.44s -> 3368.44s]  to come up with these proofs of the subproblems,
[3368.44s -> 3370.44s]  and here we are.
[3370.44s -> 3372.44s]  Here are the proofs.
[3372.44s -> 3375.44s]  Again, I'm not going to go into the details of the proofs.
[3375.44s -> 3377.44s]  This is something that, you know,
[3377.44s -> 3379.44s]  you have to stare at for a little bit,
[3379.44s -> 3383.44s]  and it's not actually especially complicated.
[3384.44s -> 3390.44s]  But then once you have proven these lemmas,
[3390.44s -> 3395.44s]  then you just put them all in the context,
[3395.44s -> 3398.44s]  and then you also put in the informal proof,
[3398.44s -> 3403.44s]  and then you ask COPRA to prove the full theorem,
[3405.44s -> 3407.44s]  and it is able to do that.
[3408.44s -> 3410.44s]  But COPRA is not able to prove this
[3410.44s -> 3415.44s]  if you were giving it just this entire theorem
[3415.44s -> 3418.44s]  all as one block and just asked it to solve.
[3420.44s -> 3424.44s]  So what we did here is really, you know,
[3424.44s -> 3429.44s]  something that is a highly powerful design pattern
[3429.44s -> 3432.44s]  in LLM agents.
[3434.44s -> 3436.44s]  LLM agents are extraordinarily flexible.
[3437.44s -> 3442.44s]  So with just some variations on the ways you do prompting,
[3443.44s -> 3446.44s]  you're able to make it do very, very different things,
[3446.44s -> 3449.44s]  things that would take a tremendous amount of training
[3449.44s -> 3452.44s]  if you weren't using these pre-trained models.
[3452.44s -> 3455.44s]  And so what we did over here is that we started
[3455.44s -> 3458.44s]  with a system that was just designed
[3458.44s -> 3463.44s]  to be a step-by-step predictor of low-level proof,
[3464.44s -> 3466.44s]  and then we made it do high-level reasoning
[3466.44s -> 3468.44s]  at this more abstract level, right,
[3468.44s -> 3472.44s]  because if you can think of these subgoals
[3472.44s -> 3476.44s]  as basically being abstractions of subproblems,
[3477.44s -> 3481.44s]  and so the LLM is able to create these abstractions.
[3482.44s -> 3485.44s]  And once you have created, made use of these abstractions,
[3486.44s -> 3489.44s]  your proof process becomes a lot simpler
[3489.44s -> 3491.44s]  because now you're able to go hierarchically.
[3493.44s -> 3495.44s]  And all of this was doable
[3495.44s -> 3498.44s]  without any substantial training.
[3499.44s -> 3503.44s]  You were just using the power of prompting.
[3505.44s -> 3508.44s]  And to me, this is a huge revelation,
[3508.44s -> 3513.44s]  the idea that you can use the power of language,
[3513.44s -> 3516.44s]  the power of all of the stored knowledge inside LLMs,
[3516.44s -> 3521.44s]  and then combine that with the sort of higher-level
[3521.44s -> 3524.44s]  algorithmics where you just put together the informal
[3524.44s -> 3526.44s]  and formal reasoning steps in various ways
[3526.44s -> 3528.44s]  and change the prompting strategy,
[3528.44s -> 3532.44s]  and you get something that is qualitatively different
[3532.44s -> 3533.44s]  in capability.
[3534.44s -> 3537.44s]  And now it turns out that the Lean feedback here
[3537.44s -> 3539.44s]  was also really important,
[3539.44s -> 3541.44s]  because at least with O3 Mini,
[3541.44s -> 3543.44s]  what we found is that there were hallucinated lemmas
[3543.44s -> 3546.44s]  and so on, and the grounding that was provided
[3546.44s -> 3549.44s]  by the Lean theorem prover
[3549.44s -> 3553.44s]  was absolutely critical to solve this problem.
[3555.44s -> 3559.44s]  So now I want to talk a little bit about different
[3560.44s -> 3562.44s]  application of the same idea.
[3562.44s -> 3565.44s]  This is in the context of formal system verification,
[3565.44s -> 3569.44s]  which is one of the most practical applications
[3569.44s -> 3572.44s]  of rigorous mathematical reasoning.
[3572.44s -> 3574.44s]  And so what you're doing in formal verification
[3574.44s -> 3576.44s]  is that you're mathematically modeling
[3576.44s -> 3580.44s]  a real-world system as a set of definitions,
[3581.44s -> 3584.44s]  and then you are modeling system properties
[3584.44s -> 3587.44s]  as theorems involving a set of definitions
[3587.44s -> 3589.44s]  and then you're proving these theorems.
[3590.44s -> 3595.44s]  These methods have led to extremely rigorous systems,
[3595.44s -> 3597.44s]  systems that are very difficult to
[3597.44s -> 3600.44s]  or almost impossible to hack.
[3600.44s -> 3602.44s]  For example, there was this time
[3602.44s -> 3607.44s]  in a program organized by the U.S. Department of Defense,
[3609.44s -> 3613.44s]  they used this formally verified Linux kernel
[3613.44s -> 3618.44s]  called SEL4, and they put this on a drone,
[3620.44s -> 3624.44s]  and then they got a red team to try to attack
[3624.44s -> 3627.44s]  this drone for a period of six weeks.
[3628.44s -> 3631.44s]  And they failed, right, because the rigor
[3631.44s -> 3634.44s]  provided by formal verification was just so much.
[3635.44s -> 3637.44s]  Now, historically, formal verification
[3637.44s -> 3640.44s]  has been extremely expensive,
[3640.44s -> 3643.44s]  and as a result, it's costs,
[3643.44s -> 3646.44s]  it's not really been widely deployed
[3646.44s -> 3648.44s]  because the costs are not justifiable
[3648.44s -> 3651.44s]  except in the most safety-critical scenarios.
[3653.44s -> 3657.44s]  However, with the rise of AI, especially AI for math,
[3657.44s -> 3661.44s]  the cost-benefit equation changes quite drastically.
[3662.44s -> 3667.44s]  And now you can imagine doing not only proof-of-the-art
[3667.44s -> 3670.44s]  but proofs of real-world systems of,
[3670.44s -> 3672.44s]  for example, correctness, security, performance,
[3672.44s -> 3677.44s]  and so on using this growing generation of AI tools.
[3679.44s -> 3681.44s]  So here I'm going to show you
[3681.44s -> 3685.44s]  a basic example of this idea.
[3685.44s -> 3688.44s]  This can be scaled up substantially
[3688.44s -> 3692.44s]  with more powerful models and more compute.
[3693.44s -> 3695.44s]  But here, what we are going to do
[3695.44s -> 3699.44s]  is to give you a very, very basic compiler
[3699.44s -> 3703.44s]  and then show how to formally verify its correctness.
[3704.44s -> 3706.44s]  Now, this is so simple, the compiler,
[3706.44s -> 3710.44s]  that it's almost a joke, but trust me,
[3711.44s -> 3714.44s]  when you go to more advanced compiler design,
[3714.44s -> 3715.44s]  the fundamentals don't change.
[3715.44s -> 3718.44s]  It's really the same thing.
[3718.44s -> 3719.44s]  The fundamentals don't change.
[3719.44s -> 3722.44s]  It's really the same sort of idea.
[3722.44s -> 3725.44s]  It's just that there is more complexity.
[3727.44s -> 3730.44s]  Now, okay, to do compiler verification,
[3730.44s -> 3732.44s]  you would need the following pieces.
[3732.44s -> 3735.44s]  So first of all, you would need to have
[3735.44s -> 3738.44s]  a notion of language,
[3738.44s -> 3742.44s]  and the language consists of a syntax and a semantic.
[3742.44s -> 3747.44s]  So syntax is a grammar that defines the form of the program.
[3748.44s -> 3750.44s]  And then semantics are the rules
[3750.44s -> 3753.44s]  that define the executions of programs.
[3753.44s -> 3755.44s]  And then a compiler is basically a set of rules
[3755.44s -> 3758.44s]  that translate programs from one language to another.
[3758.44s -> 3761.44s]  And then verifications is that you prove
[3761.44s -> 3764.44s]  that the rules preserve semantics.
[3766.44s -> 3768.44s]  So here we are going to talk about
[3768.44s -> 3771.44s]  a very basic expression language
[3771.44s -> 3776.44s]  in which you are allowed to do addition and multiplication.
[3776.44s -> 3778.44s]  So this is just basic arithmetic expressions.
[3778.44s -> 3781.44s]  And the syntax that you see over here,
[3781.44s -> 3784.44s]  this is in a language called Coq.
[3784.44s -> 3788.44s]  And here, it looks a lot like Lean.
[3788.44s -> 3790.44s]  It's a similar, you know,
[3790.44s -> 3794.44s]  dependently typed functional programming language.
[3794.44s -> 3796.44s]  And so what you're doing over here
[3796.44s -> 3798.44s]  is that you're defining this binary operator.
[3798.44s -> 3800.44s]  This can be either plus or times.
[3800.44s -> 3802.44s]  You're defining this notion of an expression.
[3803.44s -> 3806.44s]  And this is saying that it can be either a constant
[3806.44s -> 3809.44s]  or a binary operation.
[3809.44s -> 3812.44s]  And then there are some rules that tell you,
[3812.44s -> 3818.44s]  you know, how to actually evaluate these expressions.
[3818.44s -> 3820.44s]  You know, if you're given an expression,
[3820.44s -> 3823.44s]  what will be the result when you evaluate it?
[3823.44s -> 3825.44s]  And I'm not going to go into the rules in detail.
[3825.44s -> 3828.44s]  I'll share the slides, and if you're interested,
[3828.44s -> 3835.44s]  you can, well, you can look up many resources on the web.
[3835.44s -> 3839.44s]  But also, I'm happy to answer questions over email offline.
[3842.44s -> 3844.44s]  Or actually, after this talk.
[3844.44s -> 3846.44s]  Okay, so then you define the source language.
[3846.44s -> 3849.44s]  So here, what you see is that you can define,
[3849.44s -> 3852.44s]  you know, the rules that we saw previously.
[3852.44s -> 3855.44s]  They allow you to define the semantics
[3855.44s -> 3857.44s]  of quite complex expressions.
[3857.44s -> 3859.44s]  So here, what you're seeing is that, you know,
[3859.44s -> 3861.44s]  you have a const 2.
[3861.44s -> 3865.44s]  So an expression can be a constant or a binary operation
[3865.44s -> 3867.44s]  or the result of a binary operation.
[3867.44s -> 3869.44s]  And you see that, you know, this is the result
[3869.44s -> 3873.44s]  of applying a binary operation to two expressions, you know.
[3873.44s -> 3875.44s]  And so this is a recursive definition, really.
[3875.44s -> 3880.44s]  And then you are getting an expression as output.
[3880.44s -> 3883.44s]  So here, what you're seeing is that you are,
[3883.44s -> 3887.44s]  you can evaluate various complicated expressions
[3887.44s -> 3889.44s]  in this way.
[3889.44s -> 3892.44s]  And so now you have to define the target language, right?
[3892.44s -> 3894.44s]  So this is the, a compiler is a mapping
[3894.44s -> 3896.44s]  from a source language to a target language.
[3896.44s -> 3898.44s]  So now you have to define the target language.
[3898.44s -> 3900.44s]  And so what you did over here
[3900.44s -> 3903.44s]  is that you are defining the target language
[3903.44s -> 3906.44s]  as a list of instructions.
[3906.44s -> 3909.44s]  So a program is a list of instructions.
[3909.44s -> 3911.44s]  And an instruction is either this sort
[3911.44s -> 3915.44s]  of a constant expression or a binary operation expression.
[3915.44s -> 3917.44s]  So one way to see it
[3917.44s -> 3920.44s]  is that while the original expression
[3920.44s -> 3924.44s]  in the source language was defined in this recursive way,
[3924.44s -> 3929.44s]  in the target language,
[3929.44s -> 3931.44s]  when you are interpreting it,
[3931.44s -> 3934.44s]  you are doing it using a stack.
[3934.44s -> 3938.44s]  So this is a well-known idea in programming languages
[3938.44s -> 3942.44s]  that if you have a recursive expression,
[3942.44s -> 3944.44s]  let's say an arithmetic expression,
[3944.44s -> 3946.44s]  then the way you can execute it
[3946.44s -> 3948.44s]  is by maintaining a stack.
[3948.44s -> 3950.44s]  And you can keep pushing the current instruction
[3950.44s -> 3951.44s]  on the stack.
[3951.44s -> 3955.44s]  And you can basically do this kind of a low-level,
[3955.44s -> 3958.44s]  build up a low-level machine, a stack machine.
[3958.44s -> 3960.44s]  Actually a lot of early computers
[3960.44s -> 3961.44s]  were based on this idea.
[3961.44s -> 3965.44s]  And then the stack basically keeps pushing new things on it
[3965.44s -> 3968.44s]  and then you pop,
[3968.44s -> 3972.44s]  and then you read the value that was just popped.
[3972.44s -> 3976.44s]  So while very basic,
[3976.44s -> 3978.44s]  it still captures a lot
[3978.44s -> 3981.44s]  of important programming language concepts.
[3981.44s -> 3986.44s]  Now, the point is that I want to believe
[3986.44s -> 3989.44s]  that the mapping that I would do
[3989.44s -> 3992.44s]  between these two representations is actually correct.
[3992.44s -> 3996.44s]  But of course, first I have to define the compiler.
[3996.44s -> 3998.44s]  And so this is the compiler.
[3998.44s -> 4001.44s]  So what this says is that I am going to, you know,
[4001.44s -> 4004.44s]  take my input expression in the source language,
[4004.44s -> 4006.44s]  and then I'm going to do a pattern matching.
[4006.44s -> 4008.44s]  I'm going to consider these two cases,
[4008.44s -> 4009.44s]  either when it's a constant
[4009.44s -> 4011.44s]  or when it's a binary operation.
[4011.44s -> 4013.44s]  And in each of these cases,
[4013.44s -> 4017.44s]  I'm going to generate the target language program.
[4017.44s -> 4019.44s]  So in the target language program,
[4019.44s -> 4021.44s]  the entire sequence of instructions
[4021.44s -> 4023.44s]  is just written down as a list.
[4023.44s -> 4025.44s]  Now it's when you are executing the program
[4025.44s -> 4027.44s]  that you are maintaining a stack, right?
[4027.44s -> 4029.44s]  That's when you are doing, you know,
[4029.44s -> 4032.44s]  all of the kind of modeling the recursion.
[4032.44s -> 4035.44s]  But in the program itself,
[4035.44s -> 4037.44s]  you are just doing this kind of,
[4037.44s -> 4040.44s]  you're generating the instructions from E2,
[4040.44s -> 4041.44s]  you're generating the instructions from E1,
[4041.44s -> 4043.44s]  and then you are sticking this B,
[4043.44s -> 4044.44s]  and then you are just, you know,
[4044.44s -> 4046.44s]  building up a stack with,
[4046.44s -> 4050.44s]  or a sequence of instructions with all of this.
[4051.44s -> 4052.44s]  So for example, you know,
[4052.44s -> 4054.44s]  if you started with this expression,
[4054.44s -> 4056.44s]  you know, binup plus const two const three,
[4056.44s -> 4058.44s]  so this is, you know, two plus three,
[4058.44s -> 4060.44s]  is that's really the expression.
[4060.44s -> 4063.44s]  And so then in the lower level language,
[4063.44s -> 4066.44s]  it's going to look like this.
[4067.44s -> 4069.44s]  So then you have to define this correctness theorem,
[4069.44s -> 4072.44s]  which says that for all E in the,
[4072.44s -> 4076.44s]  so all programs, all expressions E in the source language,
[4076.44s -> 4078.44s]  upon compiling,
[4078.44s -> 4080.44s]  upon compiling this expression,
[4080.44s -> 4083.44s]  you get something that when you execute it
[4083.44s -> 4085.44s]  in the programming languages semantics,
[4085.44s -> 4088.44s]  you get the same thing as if you,
[4088.44s -> 4092.44s]  if you evaluated the original expression
[4092.44s -> 4095.44s]  in the source languages semantics, right?
[4095.44s -> 4097.44s]  So these two are the same.
[4098.44s -> 4100.44s]  Now, it turns out that
[4100.44s -> 4104.44s]  COPRA cannot prove this theorem just in one shot.
[4104.44s -> 4107.44s]  So what you do instead
[4107.44s -> 4110.44s]  is that you ask the LLM,
[4110.44s -> 4113.44s]  again, think of the IMO example that I showed.
[4113.44s -> 4115.44s]  This is quite similar.
[4115.44s -> 4118.44s]  So you ask the LLM to invent a lemma,
[4118.44s -> 4120.44s]  an intermediate goal.
[4120.44s -> 4123.44s]  So the LLM goes ahead
[4123.44s -> 4125.44s]  and thinks for a while,
[4125.44s -> 4127.44s]  and this is O one here,
[4127.44s -> 4130.44s]  and it comes up with this intermediate lemma,
[4130.44s -> 4135.44s]  which basically says that rather than trying to prove
[4135.44s -> 4139.44s]  the entire compiler correct,
[4139.44s -> 4141.44s]  what you instead do is that you just show,
[4141.44s -> 4145.44s]  you know, one step of compilation,
[4145.44s -> 4148.44s]  like for one sub-expression,
[4148.44s -> 4150.44s]  that's going to be correct.
[4152.44s -> 4155.44s]  So basically, that's the target theorem.
[4155.44s -> 4157.44s]  The auxiliary lemma is saying that
[4157.44s -> 4161.44s]  you can compile, you know, one instruction correctly first.
[4161.44s -> 4165.44s]  And so this is really just simplifying the problem,
[4165.44s -> 4167.44s]  breaking it down in a step-by-step way,
[4167.44s -> 4171.44s]  and then we ask COPRA to come up with a proof
[4171.44s -> 4173.44s]  of this one instruction,
[4173.44s -> 4175.44s]  and it's able to,
[4175.44s -> 4177.44s]  which is fantastic.
[4177.44s -> 4179.44s]  But then what we can do is,
[4179.44s -> 4181.44s]  just in the way we did previously,
[4181.44s -> 4184.44s]  we stick this back.
[4184.44s -> 4189.44s]  We take this proof, give it to the COPRA agent,
[4189.44s -> 4191.44s]  put it in the prompt, and then ask it to, you know,
[4191.44s -> 4193.44s]  prove the entire theorem correct,
[4193.44s -> 4196.44s]  and it's now able to do this.
[4198.44s -> 4204.44s]  So that's what I have on this particular topic.
[4204.44s -> 4207.44s]  In summary, I would say that mathematical discovery
[4207.44s -> 4211.44s]  with LLM agents is extremely promising.
[4211.44s -> 4215.44s]  I think that you can go a very long distance
[4215.44s -> 4219.44s]  with just, you know, in-context learning,
[4219.44s -> 4222.44s]  but you want to have the proof assistant feedback
[4222.44s -> 4224.44s]  for proper grounding.
[4224.44s -> 4226.44s]  And there is a lot of, you know,
[4226.44s -> 4228.44s]  higher-level agent design that you can do
[4228.44s -> 4232.44s]  where you can combine agents that are doing,
[4232.44s -> 4234.44s]  you know, for example, formalization
[4234.44s -> 4237.44s]  with agents that are doing hierarchical planning
[4237.44s -> 4240.44s]  with agents that are doing low-level stuff,
[4240.44s -> 4243.44s]  low-level proof generation.
[4243.44s -> 4245.44s]  And this is a very new space.
[4245.44s -> 4249.44s]  There is a lot to be done over here.
[4249.44s -> 4251.44s]  Now, you know, one question, again,
[4251.44s -> 4253.44s]  going back to something we asked previously,
[4253.44s -> 4256.44s]  can you get rid of this interaction
[4256.44s -> 4259.44s]  by just pushing the whole thing to training?
[4259.44s -> 4262.44s]  Maybe, you know, you generate a lot of training data
[4262.44s -> 4265.44s]  in this way by interaction with the prover,
[4265.44s -> 4268.44s]  and then you, you know,
[4268.44s -> 4271.44s]  just continue training the LLM with those traces,
[4271.44s -> 4274.44s]  and then at some point, the LLM just gets good enough.
[4274.44s -> 4276.44s]  We are not there yet,
[4276.44s -> 4280.44s]  but maybe this is a world that we can get to someday,
[4280.44s -> 4283.44s]  but currently we have to have
[4283.44s -> 4286.44s]  these sorts of agent architectures.
[4286.44s -> 4292.44s]  Now, there's this paper several of us recently wrote,
[4292.44s -> 4294.44s]  and you may have seen a reference
[4294.44s -> 4296.44s]  to this paper previously.
[4296.44s -> 4300.44s]  Caillou is the lead author of this,
[4300.44s -> 4304.44s]  and Don was the leader of the overall effort,
[4304.44s -> 4307.44s]  but if you are curious about this area,
[4307.44s -> 4310.44s]  you want to, you know, know more about, you know,
[4310.44s -> 4312.44s]  specific challenges, roadmaps,
[4312.44s -> 4316.44s]  then I would recommend that you go and look at this paper.
[4318.44s -> 4321.44s]  I had a few things to say about open challenges,
[4321.44s -> 4324.44s]  but I'm going to skip those in the interest of time,
[4324.44s -> 4329.44s]  and I'm going to move on to the next topic,
[4329.44s -> 4333.44s]  which is AI for scientific discovery,
[4333.44s -> 4337.44s]  because I want to emphasize
[4337.44s -> 4340.44s]  that this idea of LLM agents,
[4340.44s -> 4343.44s]  this has a lot of value in this sort of deductive mathematics,
[4343.44s -> 4346.44s]  where you are starting with a set of principles,
[4346.44s -> 4348.44s]  set of axioms, definitions,
[4348.44s -> 4351.44s]  and then you are proving statements in a step-by-step way,
[4351.44s -> 4355.44s]  but it also has tremendous potential in empirical discovery,
[4355.44s -> 4358.44s]  which is what happens in most of the natural sciences,
[4358.44s -> 4360.44s]  where you are starting with data,
[4360.44s -> 4365.44s]  and then you are trying to fit, you know,
[4365.44s -> 4370.44s]  some hypotheses or some experimental players
[4370.44s -> 4373.44s]  or some claims to this data.
[4373.44s -> 4376.44s]  Now, we'll see that this is also a place
[4376.44s -> 4378.44s]  where the abstraction abilities of LLMs
[4378.44s -> 4382.44s]  play a potentially very interesting role.
[4382.44s -> 4386.44s]  Okay, so very quickly,
[4386.44s -> 4389.44s]  this is what a natural science process looks like.
[4389.44s -> 4391.44s]  You know, you choose a research question.
[4391.44s -> 4393.44s]  You generate some hypotheses.
[4393.44s -> 4396.44s]  You do an experimental setup.
[4396.44s -> 4398.44s]  You collect some data.
[4398.44s -> 4399.44s]  You do some analysis of the data.
[4399.44s -> 4401.44s]  You interpret the data,
[4401.44s -> 4403.44s]  and then you report to the community, right?
[4403.44s -> 4409.44s]  And so just as an example of this,
[4409.44s -> 4413.44s]  you know, if you think of the world of astronomy,
[4413.44s -> 4417.44s]  back in the day, there was this problem
[4417.44s -> 4421.44s]  that was observed, which is that people believed
[4421.44s -> 4425.44s]  that the Earth was the center of the solar system,
[4425.44s -> 4430.44s]  but then some of the planets appeared to move backward,
[4430.44s -> 4433.44s]  and so this led to this creation of this new hypothesis
[4433.44s -> 4436.44s]  that, in fact, we have a heliocentric model,
[4436.44s -> 4438.44s]  and then this led to the creation,
[4438.44s -> 4440.44s]  the collection of more data.
[4440.44s -> 4443.44s]  Tycho Brahe, in particular, observed Mars
[4443.44s -> 4448.44s]  and then came up with a whole bunch of precise measurements
[4448.44s -> 4450.44s]  of the planets' positions,
[4450.44s -> 4455.44s]  and then Kepler chose to analyze this data
[4455.44s -> 4458.44s]  and then fit a wide variety of geometric shapes to this data,
[4458.44s -> 4462.44s]  and then he came up with this Kepler's third law,
[4462.44s -> 4468.44s]  and then this eventually led to Newton's law of gravitation.
[4469.44s -> 4472.44s]  So there was this process that happened in science,
[4472.44s -> 4477.44s]  which involved trading in abstractions,
[4477.44s -> 4479.44s]  you know, seeing some data,
[4479.44s -> 4482.44s]  and then coming up with this law
[4482.44s -> 4484.44s]  in terms of abstract principles,
[4484.44s -> 4487.44s]  but then, you know, further generalizing it
[4487.44s -> 4492.44s]  into a law that was even more explanatory.
[4492.44s -> 4496.44s]  So I think that LLM agents have tremendous potential
[4496.44s -> 4500.44s]  in automating a lot of pieces of this pipeline,
[4500.44s -> 4502.44s]  and in particular here, we are going to focus
[4502.44s -> 4506.44s]  on the problem of symbolic regression,
[4506.44s -> 4511.44s]  which is the task of coming up with equations
[4511.44s -> 4514.44s]  based on empirical observations.
[4514.44s -> 4518.44s]  So, for example, an instance of that would be
[4518.44s -> 4523.44s]  that if you were given these measurements of planet Mars
[4523.44s -> 4527.44s]  and then you wanted to discover Kepler's law,
[4527.44s -> 4531.44s]  how do you do that, right, from the data?
[4531.44s -> 4533.44s]  Now, one would think that LLMs
[4533.44s -> 4536.44s]  would not be a great fit for this problem
[4536.44s -> 4539.44s]  because the data is just really low-level, right?
[4539.44s -> 4542.44s]  This is just a bunch of numbers.
[4542.44s -> 4546.44s]  However, it turns out that LLMs
[4546.44s -> 4549.44s]  do have a tremendous potential over here
[4549.44s -> 4552.44s]  in the sense that they can bring in
[4552.44s -> 4555.44s]  prior scientific knowledge into the picture.
[4555.44s -> 4558.44s]  What you have to do, though, is that you have to complement
[4558.44s -> 4563.44s]  the use of LLMs with the use of other machinery
[4563.44s -> 4567.44s]  that is able to, for example, evaluate hypotheses on the data
[4567.44s -> 4573.44s]  and give you some notion of how well it performed.
[4573.44s -> 4576.44s]  So once you have that kind of a mechanism
[4576.44s -> 4579.44s]  for evaluating LLM-generated hypotheses,
[4579.44s -> 4581.44s]  you can create an agentic loop
[4581.44s -> 4585.44s]  where you have more and more feedback going to the LLM,
[4585.44s -> 4590.44s]  and then this process of hypothesis generation
[4590.44s -> 4592.44s]  is left to the LLM,
[4592.44s -> 4598.44s]  whereas hypothesis evaluation is done by some other entity.
[4598.44s -> 4600.44s]  And if you think of it, it's not fundamentally different
[4600.44s -> 4603.44s]  from the way we use Lean or the Proof Assistant
[4603.44s -> 4605.44s]  in the previous part of the lecture.
[4605.44s -> 4613.44s]  So one of the most established approaches
[4613.44s -> 4617.44s]  to symbolic regression is genetic programming,
[4617.44s -> 4619.44s]  and here are some visualizations
[4619.44s -> 4621.44s]  from a system called PISAR,
[4621.44s -> 4624.44s]  which is quite popular in this space,
[4624.44s -> 4627.44s]  and so what's going on here is that you have
[4627.44s -> 4630.44s]  a pool of candidate expressions,
[4630.44s -> 4632.44s]  candidate equations being maintained,
[4632.44s -> 4635.44s]  and then you have mutation
[4635.44s -> 4638.44s]  and crossover operations being applied to those,
[4638.44s -> 4642.44s]  and then you are generating,
[4642.44s -> 4645.44s]  you're updating this pool of expressions
[4645.44s -> 4648.44s]  using feedback from a separate machinery
[4648.44s -> 4650.44s]  that is evaluating these expressions
[4650.44s -> 4653.44s]  and telling you how good is the loss,
[4653.44s -> 4656.44s]  how well does this match the reality
[4656.44s -> 4660.44s]  that is there in the data.
[4661.44s -> 4665.44s]  So this has had a lot of impact.
[4665.44s -> 4667.44s]  There are applications in physics
[4667.44s -> 4670.44s]  and other areas as well,
[4670.44s -> 4673.44s]  and so here are some example papers.
[4675.44s -> 4678.44s]  So one thing that's interesting
[4678.44s -> 4680.44s]  is that if you just imagine
[4680.44s -> 4686.44s]  how does this process of genetic programming go on,
[4686.44s -> 4689.44s]  so there is this pool of expressions
[4689.44s -> 4691.44s]  that is being evolved,
[4691.44s -> 4695.44s]  and so what you see is that there are these islands
[4695.44s -> 4698.44s]  of expressions, so these clusters,
[4698.44s -> 4702.44s]  which contain expressions that are of,
[4702.44s -> 4705.44s]  that have something in common between them.
[4706.44s -> 4711.44s]  So the, one way to think about
[4711.44s -> 4714.44s]  how LLMs are relevant is that these clusters
[4714.44s -> 4716.44s]  or islands of expressions,
[4716.44s -> 4718.44s]  these capture various kinds of,
[4718.44s -> 4720.44s]  various classes of functions.
[4720.44s -> 4722.44s]  So for example, in this red one over here,
[4722.44s -> 4725.44s]  you may have exponential functions.
[4725.44s -> 4728.44s]  In this case, you may have trigonometric functions
[4728.44s -> 4731.44s]  where you have the cosines,
[4731.44s -> 4734.44s]  and so one way to imagine this
[4734.44s -> 4739.44s]  is that an LLM looking at these expressions
[4739.44s -> 4741.44s]  because LLMs are good at coding
[4741.44s -> 4743.44s]  and associating code with natural language
[4743.44s -> 4745.44s]  could perhaps come up with some explanation
[4745.44s -> 4748.44s]  of what this piece of code,
[4748.44s -> 4750.44s]  this expression is actually doing, right,
[4750.44s -> 4752.44s]  and that's the idea of abstraction
[4752.44s -> 4754.44s]  that we are going to use.
[4754.44s -> 4759.44s]  Okay, so this paper was at NeurIPS last year
[4759.44s -> 4761.44s]  and co-led by Arya Graili,
[4761.44s -> 4763.44s]  who was an undergrad at UT,
[4763.44s -> 4767.44s]  and Atharva Sehgal, who's a PhD student at UT,
[4767.44s -> 4772.44s]  and so the starting point here
[4772.44s -> 4774.44s]  is that we want to do this kind of concept
[4774.44s -> 4779.44s]  abstraction, and so what we want to be able to do
[4779.44s -> 4784.44s]  is that we want to look at these variety of laws,
[4784.44s -> 4787.44s]  which come from very different contexts,
[4787.44s -> 4790.44s]  but then we should be able to look at this
[4790.44s -> 4792.44s]  and kind of tell that what's going on here
[4792.44s -> 4798.44s]  is that you have this power law thing going on, right,
[4798.44s -> 4801.44s]  and so the other thing we want to do
[4801.44s -> 4804.44s]  is that we want to use background knowledge
[4804.44s -> 4806.44s]  that was created by a physicist
[4806.44s -> 4810.44s]  or that's there inside an LLM.
[4810.44s -> 4812.44s]  So, for example, here you have, you know,
[4812.44s -> 4816.44s]  an LLM concept that has been produced,
[4816.44s -> 4817.44s]  which is that, you know,
[4817.44s -> 4819.44s]  wave strain diminishes as distance increases.
[4819.44s -> 4821.44s]  This is the, let's say, property of,
[4821.44s -> 4824.44s]  you know, the black holes,
[4824.44s -> 4828.44s]  and so you want to be able to associate
[4828.44s -> 4831.44s]  this kind of information.
[4831.44s -> 4834.44s]  You know, this is a piece of common sense knowledge
[4834.44s -> 4836.44s]  to equations,
[4836.44s -> 4840.44s]  so this equation, for example, satisfies these properties,
[4840.44s -> 4844.44s]  but this is, you want to create a mapping
[4844.44s -> 4846.44s]  between these two things,
[4846.44s -> 4847.44s]  because, you know, if you imagine
[4847.44s -> 4849.44s]  how the scientific process is ongoing,
[4849.44s -> 4850.44s]  so let's say you have an AI
[4850.44s -> 4852.44s]  that is predicting equations based on data.
[4852.44s -> 4854.44s]  You also want it to be conditioned
[4854.44s -> 4856.44s]  on this sort of high-level knowledge
[4856.44s -> 4860.44s]  that is being given by a human expert, right?
[4860.44s -> 4865.44s]  And so you want to have this association between code,
[4865.44s -> 4868.44s]  that is, these equations, and language.
[4871.44s -> 4876.44s]  So the way we set up
[4876.44s -> 4878.44s]  this symbolic regression problem
[4878.44s -> 4882.44s]  using LLMs is something like this,
[4882.44s -> 4886.44s]  that we imagine that,
[4886.44s -> 4888.44s]  and actually here, it's not just LLMs.
[4888.44s -> 4890.44s]  It's more like, you know, a combination of LLMs
[4890.44s -> 4895.44s]  and this sort of abstraction into high-level concepts,
[4895.44s -> 4898.44s]  and so we imagine that there is this universe
[4898.44s -> 4899.44s]  of high-level concepts.
[4899.44s -> 4900.44s]  So, for example, you know,
[4900.44s -> 4902.44s]  wave strain diminishes as distance increases,
[4902.44s -> 4905.44s]  or power loss, or sinusoidal functions.
[4905.44s -> 4907.44s]  These are sort of high-level ideas
[4907.44s -> 4909.44s]  that we have in our head,
[4909.44s -> 4911.44s]  and there is a notion of a data set
[4911.44s -> 4914.44s]  which is the concrete data that you want to fit,
[4914.44s -> 4917.44s]  and then you have a space of programmatic hypotheses.
[4917.44s -> 4920.44s]  This is, for example, the, you know,
[4920.44s -> 4922.44s]  the landscape of equation,
[4922.44s -> 4925.44s]  or the set of equations expressible in Python
[4925.44s -> 4929.44s]  or, you know, in some restricted DSL inside Python.
[4930.44s -> 4932.44s]  And then what you're saying is that
[4932.44s -> 4936.44s]  you want to jointly infer, given the data,
[4936.44s -> 4939.44s]  this distribution over pi,
[4939.44s -> 4943.44s]  which is, you know, likely hypothesis and likely concepts.
[4943.44s -> 4946.44s]  So you don't want to just get likely programs,
[4946.44s -> 4947.44s]  given the data,
[4947.44s -> 4950.44s]  which would be the goal of traditional symbolic regression.
[4950.44s -> 4953.44s]  You want to get both the programs
[4953.44s -> 4956.44s]  and also the kinds of concepts that they represent
[4956.44s -> 4959.44s]  because, ultimately, you want to explain these programs
[4959.44s -> 4961.44s]  to a human scientist,
[4961.44s -> 4963.44s]  or, alternatively, maybe, you know,
[4963.44s -> 4966.44s]  this joint distribution gives you more information
[4966.44s -> 4969.44s]  than if you just operated in the program space.
[4970.44s -> 4973.44s]  And so this you can factorize in this way,
[4973.44s -> 4977.44s]  and then it turns out that, you know,
[4977.44s -> 4979.44s]  the way to think about it is that
[4979.44s -> 4982.44s]  P of D given pi is basically saying that
[4982.44s -> 4986.44s]  if I have a particular program pi,
[4986.44s -> 4990.44s]  what is the likelihood of the data?
[4990.44s -> 4993.44s]  So, you know, how well does the program explain the data?
[4993.44s -> 4996.44s]  In order to find this, you just execute the program, right,
[4996.44s -> 4999.44s]  and see what kinds of results come out
[4999.44s -> 5002.44s]  and what the likelihood of the data set is.
[5002.44s -> 5007.44s]  And P of pi given C is basically telling you that,
[5007.44s -> 5011.44s]  okay, here are these...
[5011.44s -> 5013.44s]  Here is a concept that I have,
[5013.44s -> 5015.44s]  so let's say I'm thinking about power loss, okay?
[5015.44s -> 5017.44s]  So conditioned on this concept
[5017.44s -> 5019.44s]  that I'm thinking about power loss,
[5019.44s -> 5021.44s]  what is the probability of this program
[5021.44s -> 5024.44s]  that, you know, this piece of code, right,
[5024.44s -> 5026.44s]  to what extent does this piece of code
[5026.44s -> 5028.44s]  represent a power loss?
[5028.44s -> 5030.44s]  And this is something that an LLM
[5030.44s -> 5033.44s]  has background knowledge inside it to do.
[5033.44s -> 5036.44s]  And then finally, I have this P of C,
[5036.44s -> 5038.44s]  which is a prior over concepts,
[5038.44s -> 5041.44s]  and this is also something that an LLM knows that, you know,
[5041.44s -> 5043.44s]  I have seen the literature of,
[5043.44s -> 5045.44s]  let's say, physics until this point,
[5045.44s -> 5050.44s]  so what is the likelihood of this concept, right?
[5050.44s -> 5056.44s]  So all of this together gives you ultimately
[5056.44s -> 5061.44s]  the ability to infer both a distribution
[5061.44s -> 5065.44s]  over both programs and concepts.
[5065.44s -> 5068.44s]  Now, in order to really implement this,
[5068.44s -> 5072.44s]  this is the loop that we came up with.
[5072.44s -> 5074.44s]  So this follows the basic structure
[5074.44s -> 5076.44s]  of an evolutionary algorithm,
[5076.44s -> 5078.44s]  but it does this sort of, you know,
[5078.44s -> 5080.44s]  this sort of Bayesian concept learning thing
[5080.44s -> 5084.44s]  guided by LLMs on top of that.
[5084.44s -> 5087.44s]  And so operationally, the way to think about it
[5087.44s -> 5090.44s]  is that you have, like in any evolutionary algorithm,
[5090.44s -> 5094.44s]  you're maintaining a population of hypotheses or programs,
[5094.44s -> 5098.44s]  and then at this point, you have a choice of doing,
[5098.44s -> 5101.44s]  you know, evolution either guided
[5101.44s -> 5103.44s]  by the classical symbolic method
[5103.44s -> 5105.44s]  where you're doing some random tweaks
[5105.44s -> 5107.44s]  to the abstract syntax tree of the program
[5107.44s -> 5110.44s]  or alternatively, you are doing this LLM guided evolution
[5110.44s -> 5112.44s]  where you are saying that here is a concept,
[5112.44s -> 5114.44s]  here is a program that I have,
[5114.44s -> 5116.44s]  and I want to generate a new program.
[5116.44s -> 5117.44s]  And when you are doing that,
[5117.44s -> 5120.44s]  you are being guided by this concept library
[5120.44s -> 5121.44s]  which tells you that, you know,
[5121.44s -> 5122.44s]  these are the concepts
[5122.44s -> 5124.44s]  that I'm thinking about right now, right?
[5124.44s -> 5126.44s]  So power laws seem really good,
[5126.44s -> 5129.44s]  really well suited to this condition,
[5129.44s -> 5132.44s]  this concept or the setting
[5132.44s -> 5136.44s]  or, you know, maybe trigonometric functions
[5136.44s -> 5139.44s]  seem especially well suited to this setting and so on, right?
[5139.44s -> 5141.44s]  So you are feeding all of that
[5141.44s -> 5143.44s]  to this LLM guided evolution process,
[5143.44s -> 5145.44s]  and that allows you to, you know,
[5145.44s -> 5153.44s]  basically mutate the current programs into new programs.
[5153.44s -> 5154.44s]  And then after you do that,
[5154.44s -> 5157.44s]  you are, like in the usual evolutionary way,
[5157.44s -> 5163.44s]  you are identifying the best candidates in this population,
[5163.44s -> 5165.44s]  and so this is where you use the fitness function
[5165.44s -> 5172.44s]  which is, you know, how well does your hypothesis pool
[5172.44s -> 5174.44s]  explain the data that you have, right?
[5174.44s -> 5180.44s]  And this is where that P of D given pi that comes in.
[5180.44s -> 5182.44s]  And so this is a little bit more detail
[5182.44s -> 5185.44s]  about how the hypothesis evolution works.
[5185.44s -> 5188.44s]  So, you know, you start with this initialization,
[5188.44s -> 5191.44s]  initial program population, and then you're evaluating,
[5191.44s -> 5192.44s]  you're getting the best program,
[5192.44s -> 5194.44s]  you're mutating or crossing over,
[5194.44s -> 5200.44s]  and then you're replacing the oldest program,
[5200.44s -> 5203.44s]  and then this process just carries on for a while.
[5205.44s -> 5207.44s]  So the important thing over here
[5207.44s -> 5209.44s]  is that you are using the LLM
[5209.44s -> 5212.44s]  for implementing this mutation and crossover operations.
[5212.44s -> 5216.44s]  So in particular, rather than, you know,
[5216.44s -> 5218.44s]  in the classical genetic algorithm,
[5218.44s -> 5220.44s]  what's going to happen is that you're given the AST
[5220.44s -> 5221.44s]  and you're going to say that, okay,
[5221.44s -> 5224.44s]  swap out this subtree by a different subtree.
[5224.44s -> 5227.44s]  Now you're going to say that I'm going to ask the LLM
[5227.44s -> 5229.44s]  to rewrite the program into something.
[5229.44s -> 5234.44s]  And this idea was also used in the FunSearch paper
[5234.44s -> 5237.44s]  from DeepMind last year,
[5237.44s -> 5242.44s]  and FunSearch has been very successful.
[5242.44s -> 5246.44s]  The way in which this laser work differs from FunSearch
[5246.44s -> 5249.44s]  is fundamental in its use of this concept library,
[5249.44s -> 5254.44s]  but a lot of the other ideas are closely related.
[5254.44s -> 5256.44s]  So this is just to highlight, you know,
[5256.44s -> 5258.44s]  the difference between the symbolic crossover
[5258.44s -> 5260.44s]  versus the LLM crossover.
[5260.44s -> 5263.44s]  So in this case, the LLM is basically doing all this.
[5263.44s -> 5265.44s]  It's being given a crossover prompt,
[5265.44s -> 5267.44s]  and then it's given some formatting instructions,
[5267.44s -> 5270.44s]  and then it's generating the new expression.
[5270.44s -> 5273.44s]  So after you do this, there is the...
[5273.44s -> 5275.44s]  So you've evolved your hypothesis,
[5275.44s -> 5276.44s]  so now you have this new pool,
[5276.44s -> 5279.44s]  and now you also evolve your concept library.
[5279.44s -> 5283.44s]  And so the concept library,
[5283.44s -> 5285.44s]  this evolution process has these two stages.
[5285.44s -> 5287.44s]  So first, you know, you take your new programs
[5287.44s -> 5290.44s]  that you've created, your new program pool,
[5290.44s -> 5293.44s]  and now you're asking the LLM to abstract this
[5293.44s -> 5295.44s]  into these sort of natural language,
[5295.44s -> 5297.44s]  high-level descriptions.
[5297.44s -> 5299.44s]  So for example, here, the LLM is telling you that,
[5299.44s -> 5305.44s]  oh, this equation represents exponential growth or decay.
[5305.44s -> 5309.44s]  And then the LLM, the concept evolution process
[5309.44s -> 5313.44s]  is just like the hypothesis or program evolution process,
[5313.44s -> 5317.44s]  except it now uses this, you know,
[5317.44s -> 5321.44s]  an LLM to basically combine,
[5321.44s -> 5324.44s]  mash up these different concepts into a new concept.
[5324.44s -> 5326.44s]  So for example, here you see that, you know,
[5326.44s -> 5329.44s]  exponential growth decay, this is one concept.
[5329.44s -> 5332.44s]  Another concept says depends on temperature.
[5332.44s -> 5335.44s]  And our LLM, because it has all this knowledge
[5335.44s -> 5338.44s]  about all sorts of concepts relevant in physics,
[5338.44s -> 5340.44s]  it comes up with this idea that, oh, you know,
[5340.44s -> 5342.44s]  this should be a Boltzmann distribution.
[5342.44s -> 5344.44s]  This is a useful concept to have.
[5344.44s -> 5346.44s]  And then this is going to influence
[5346.44s -> 5347.44s]  the hypothesis evolution,
[5347.44s -> 5350.44s]  which is going to influence concept abstraction
[5350.44s -> 5354.44s]  and evolution, and this loop will continue.
[5354.44s -> 5360.44s]  So one way to visualize this is that after the first phase,
[5360.44s -> 5362.44s]  when you have done this hypothesis evolution,
[5362.44s -> 5364.44s]  you may have these, you know, islands
[5364.44s -> 5366.44s]  of different kinds of expressions.
[5366.44s -> 5372.44s]  Now, in the concept generation process,
[5372.44s -> 5374.44s]  concept discovery process,
[5374.44s -> 5376.44s]  you're going to get these, you know,
[5376.44s -> 5380.44s]  natural language descriptions of these different islands.
[5380.44s -> 5383.44s]  So this is sinusoidal trends, this is power law trends,
[5383.44s -> 5387.44s]  this is linear trends, this is exponential trends, and so on.
[5387.44s -> 5394.44s]  And then what we found is that the use of this sort
[5394.44s -> 5397.44s]  of a high-level concept library helped us
[5397.44s -> 5399.44s]  in this equation discovery process.
[5399.44s -> 5402.44s]  So we evaluated this on this task of, you know,
[5402.44s -> 5406.44s]  equation discovery or discovering Feynman equations
[5406.44s -> 5411.44s]  just given the prompt that...
[5411.44s -> 5414.44s]  So there was the data, but then there was a prompt
[5414.44s -> 5417.44s]  that told you, you know, which chapter you were looking at
[5417.44s -> 5418.44s]  and so on.
[5418.44s -> 5422.44s]  And so what we found is that this approach proved
[5422.44s -> 5427.44s]  more effective than a version where you were just using
[5427.44s -> 5429.44s]  either a pure genetic programming or one
[5429.44s -> 5434.44s]  where you were not using any kind of concept learning.
[5434.44s -> 5437.44s]  And so the...
[5437.44s -> 5441.44s]  So it turns out that even with local language models,
[5441.44s -> 5444.44s]  when we didn't use a frontier model,
[5444.44s -> 5446.44s]  we were still able to outperform
[5446.44s -> 5450.44s]  this genetic programming approach.
[5450.44s -> 5454.44s]  And the overall finding was that,
[5454.44s -> 5456.44s]  and you can find more details in the paper,
[5456.44s -> 5459.44s]  is that the use of this sort of high-level concepts
[5459.44s -> 5463.44s]  generated by the LLM, it accelerates discovery.
[5463.44s -> 5470.44s]  Now, one appeal of this sort of LLM-based approach
[5470.44s -> 5475.44s]  is that it can very naturally take in user-provided hints.
[5475.44s -> 5483.44s]  So in this case, we gave the algorithm some hints,
[5483.44s -> 5486.44s]  which was, you know, this is the kind of equation
[5486.44s -> 5488.44s]  you are looking at.
[5488.44s -> 5492.44s]  So in the previous experiments, actually, we did not have hints.
[5492.44s -> 5495.44s]  So this was just, you know, purely based on data.
[5495.44s -> 5497.44s]  But then in this case, we had hints,
[5497.44s -> 5502.44s]  and then we found that hints actually provided some benefits.
[5502.44s -> 5505.44s]  So on the X-axis here, you have the number of iterations.
[5505.44s -> 5509.44s]  On the Y-axis, you have the fraction of equations solved.
[5509.44s -> 5511.44s]  And then we found that the use of the hints,
[5511.44s -> 5514.44s]  it provided a modest benefit,
[5514.44s -> 5518.44s]  at least earlier on in the process.
[5518.44s -> 5522.44s]  So here is an example of this in action.
[5522.44s -> 5526.44s]  So here, this is the classic Coulomb's law.
[5526.44s -> 5531.44s]  And then PISAR is this genetic programming approach.
[5531.44s -> 5537.44s]  And it gave this very unwieldy kind of expression
[5537.44s -> 5540.44s]  that it turns out is equivalent
[5540.44s -> 5542.44s]  to the ground truth expression,
[5542.44s -> 5546.44s]  but it required a lot of simplification for you to,
[5546.44s -> 5547.44s]  it would require a lot of simplification
[5547.44s -> 5550.44s]  for you to go from here to the ground truth.
[5550.44s -> 5555.44s]  Whereas the laser approach is able to basically,
[5556.44s -> 5560.44s]  because of the, you know, some of the knowledge
[5560.44s -> 5562.44s]  that is there in the LLM,
[5562.44s -> 5565.44s]  it's able to, you know, reduce to ground truth
[5565.44s -> 5567.44s]  after four steps of simplification.
[5567.44s -> 5569.44s]  Now you could say, wait a second, isn't the LLM cheating?
[5569.44s -> 5572.44s]  Because it already has Coulomb's law in its mind.
[5572.44s -> 5576.44s]  So how do you know that this is not just, you know, leakage?
[5576.44s -> 5578.44s]  And of course, in this case,
[5578.44s -> 5581.44s]  there is that possibility, but that said,
[5581.44s -> 5583.44s]  I would say that, you know, the attraction here
[5583.44s -> 5588.44s]  is that the input to the system is just this data, right?
[5588.44s -> 5591.44s]  It's just the input-output values
[5591.44s -> 5593.44s]  that would result from Coulomb's law.
[5593.44s -> 5596.44s]  And it's a very low level kind of input.
[5596.44s -> 5600.44s]  The fact that it was able to fit this to Coulomb's law
[5600.44s -> 5602.44s]  is kind of interesting.
[5602.44s -> 5605.44s]  But that said, we also did other experiments
[5605.44s -> 5608.44s]  on synthetic data, where you just manufactured
[5608.44s -> 5613.44s]  a completely weird equation and a weird data set
[5613.44s -> 5614.44s]  coming from that equation,
[5614.44s -> 5618.44s]  and then asked the system to regenerate this.
[5618.44s -> 5620.44s]  And we found that there were substantial benefits
[5620.44s -> 5621.44s]  even in that case.
[5623.44s -> 5627.44s]  So here is some information about some of the concepts
[5627.44s -> 5630.44s]  that you ended up getting from laser.
[5630.44s -> 5633.44s]  So you see there is stuff like, you know,
[5633.44s -> 5636.44s]  good mathematical expressions exhibit symmetry or regularity,
[5636.44s -> 5638.44s]  but that said, there were also some expressions
[5638.44s -> 5642.44s]  that didn't really seem to mean anything.
[5644.44s -> 5646.44s]  But at the same time, you know,
[5646.44s -> 5650.44s]  there is no guarantee about the quality of the concepts.
[5650.44s -> 5652.44s]  This is all natural language at this point.
[5652.44s -> 5654.44s]  An interesting possibility is to use
[5654.44s -> 5655.44s]  some kind of a formal language
[5655.44s -> 5658.44s]  to describe the high-level concepts.
[5659.44s -> 5663.44s]  Now, we also use this in a new discovery problem
[5663.44s -> 5667.44s]  because we were also worried about the possibility
[5667.44s -> 5669.44s]  of this being an artifact of leakage
[5669.44s -> 5671.44s]  in case of standard benchmarks
[5671.44s -> 5673.44s]  like discovering Feynman equations.
[5673.44s -> 5677.44s]  And so we used this method
[5677.44s -> 5681.44s]  to come up with a new scaling law for LLMs.
[5681.44s -> 5687.44s]  And so here what you see is the so-called Chinchilla law.
[5687.44s -> 5690.44s]  So this is a law that was postulated
[5690.44s -> 5693.44s]  in a DeepMind paper from a few years back.
[5693.44s -> 5695.44s]  And so here, basically, what you're trying to do
[5695.44s -> 5698.44s]  is to predict the loss of the LLM, you know,
[5698.44s -> 5702.44s]  based on these various hyperparameters.
[5702.44s -> 5704.44s]  And so this was the law.
[5704.44s -> 5705.44s]  So you postulate a scaling law,
[5705.44s -> 5708.44s]  you measure model loss with respect to hyperparameters,
[5708.44s -> 5710.44s]  and then you fit scaling law to data set.
[5710.44s -> 5711.44s]  And in order to do this, of course,
[5711.44s -> 5712.44s]  you would need to have a data set
[5712.44s -> 5715.44s]  where you have a lot of values of these hyperparameters
[5715.44s -> 5721.44s]  and corresponding accuracy values for the LLM.
[5721.44s -> 5725.44s]  Now, we did the same thing with LASER
[5725.44s -> 5727.44s]  rather than fitting a hand-fitted law,
[5727.44s -> 5730.44s]  which was done in the Chinchilla paper, right?
[5730.44s -> 5732.44s]  So they basically postulated a law like this,
[5732.44s -> 5735.44s]  and then they fit the parameters.
[5735.44s -> 5737.44s]  But by contrast, what we did
[5737.44s -> 5741.44s]  is that we just asked LASER to go find it.
[5741.44s -> 5745.44s]  And LASER was able to come up with this sort of a law
[5745.44s -> 5750.44s]  where we also had this parameter called number of shots,
[5750.44s -> 5752.44s]  which is that if you're doing prediction,
[5752.44s -> 5754.44s]  you know, what is the number of examples
[5754.44s -> 5757.44s]  that you're giving it in context?
[5757.44s -> 5763.44s]  And this parameter is available in some of these data sets,
[5763.44s -> 5764.44s]  right?
[5764.44s -> 5766.44s]  And so what we found is that there was this interesting
[5766.44s -> 5769.44s]  kind of law that came up.
[5769.44s -> 5771.44s]  Now, this is just, you know, one example.
[5771.44s -> 5774.44s]  It's not really very comprehensive yet,
[5774.44s -> 5776.44s]  but this just shows that you can make
[5776.44s -> 5780.44s]  these kinds of new discoveries using this law.
[5780.44s -> 5783.44s]  And so basically, if you interpret this,
[5783.44s -> 5786.44s]  the way to think about it is that a large number of shots
[5786.44s -> 5789.44s]  gives poor results for low-capability models,
[5789.44s -> 5792.44s]  but then once the models pass a capability threshold,
[5792.44s -> 5795.44s]  then having more shots helps, right?
[5795.44s -> 5796.44s]  And this is sort of a common-sense
[5796.44s -> 5801.44s]  linguistic interpretation of the law that came out.
[5801.44s -> 5808.44s]  So, you know, you can also do something after this.
[5808.44s -> 5811.44s]  So if you're thinking about thinking of yourself
[5811.44s -> 5813.44s]  as an LLM scaling law person,
[5813.44s -> 5816.44s]  you start with the Chinchilla law from the previous paper,
[5816.44s -> 5817.44s]  and then you say, okay, you know,
[5817.44s -> 5821.44s]  maybe I should bring in this insight about shots
[5821.44s -> 5825.44s]  into the Chinchilla law, combine the old Chinchilla law
[5825.44s -> 5828.44s]  with this new discovery that was made by laser,
[5828.44s -> 5832.44s]  and then you find that this modified Chinchilla
[5832.44s -> 5837.44s]  actually is able to do better than both the old Chinchilla
[5837.44s -> 5842.44s]  and also the, you know, the laser itself.
[5842.44s -> 5845.44s]  And so this is kind of interesting that,
[5845.44s -> 5847.44s]  and shows that, you know,
[5847.44s -> 5849.44s]  this discovery process using LLM agents,
[5849.44s -> 5850.44s]  and this is very much an agent, right?
[5850.44s -> 5852.44s]  This is not just a one-shot LLM.
[5852.44s -> 5857.44s]  You're using an LLM combined with this various other tools,
[5857.44s -> 5861.44s]  and so what you are getting out of this is,
[5861.44s -> 5864.44s]  can influence human discovery as well
[5864.44s -> 5865.44s]  and lead to something that is greater
[5865.44s -> 5868.44s]  than the sum of the parts.
[5868.44s -> 5872.44s]  So in the interest of time, I'm going to skip this next bit,
[5872.44s -> 5875.44s]  but what I said there is that these ideas
[5875.44s -> 5878.44s]  can also be applied to settings
[5878.44s -> 5880.44s]  where you have visual high-dimensional inputs
[5880.44s -> 5883.44s]  because now we have vision language models as well,
[5883.44s -> 5885.44s]  so you can go crazy
[5885.44s -> 5889.44s]  and build these kinds of evolution-based agents
[5889.44s -> 5892.44s]  where you have VLMs as part of the story,
[5892.44s -> 5896.44s]  and I'm extremely optimistic about this direction.
[5896.44s -> 5899.44s]  So in summary, I would say that LLM-directed evolution
[5899.44s -> 5901.44s]  is a powerful tool for empirical science,
[5901.44s -> 5906.44s]  and this point has already been made previously by FunSearch,
[5906.44s -> 5907.44s]  but now, you know,
[5907.44s -> 5909.44s]  we see that we can make it even more powerful
[5909.44s -> 5912.44s]  with these ideas of abstraction
[5912.44s -> 5917.44s]  that LLMs can give you basically for free,
[5917.44s -> 5919.44s]  and even, you know,
[5919.44s -> 5921.44s]  this can be even extended to visual settings
[5921.44s -> 5925.44s]  where you have to use now vision language models,
[5925.44s -> 5927.44s]  but hey, they're coming a long way.
[5927.44s -> 5928.44s]  They have come a long way,
[5928.44s -> 5932.44s]  and importantly, the abstraction abilities of LLMs
[5932.44s -> 5934.44s]  can also be really helpful here.
[5934.44s -> 5936.44s]  Open challenges, you know,
[5936.44s -> 5939.44s]  verification of hypothesis and concepts,
[5939.44s -> 5943.44s]  concept representations that go beyond natural language,
[5943.44s -> 5946.44s]  scaling to larger search spaces, input dimensions,
[5946.44s -> 5947.44s]  and then, you know,
[5947.44s -> 5949.44s]  even going beyond hypothesis generation
[5949.44s -> 5953.44s]  to other parts of the scientific discovery process,
[5953.44s -> 5954.44s]  but at the end, though,
[5954.44s -> 5960.44s]  I am very optimistic that LLM agents,
[5960.44s -> 5961.44s]  not just LLMs,
[5961.44s -> 5963.44s]  but these kinds of agentic workflows
[5963.44s -> 5967.44s]  where you have combinations of LLM queries
[5967.44s -> 5970.44s]  as well as other kinds of machinery,
[5970.44s -> 5974.44s]  they can be extremely powerful in scientific discovery.
[5974.44s -> 5978.44s]  They can help you come up with new scientific hypotheses
[5978.44s -> 5982.44s]  and even do experiment design,
[5982.44s -> 5983.44s]  and also, as we saw,
[5983.44s -> 5984.44s]  mathematical discovery,
[5984.44s -> 5986.44s]  they can be supremely powerful,
[5986.44s -> 5990.44s]  and again, we are just in the early stages
[5990.44s -> 5996.44s]  of what I think this line of research will end up being,
[5996.44s -> 6000.44s]  and this can have applications in math,
[6000.44s -> 6002.44s]  in science, in engineering,
[6002.44s -> 6004.44s]  as you saw, the formal verification application,
[6004.44s -> 6007.44s]  you know, building rigorous systems,
[6007.44s -> 6011.44s]  so the future is very bright,
[6011.44s -> 6012.44s]  and with that, I'll end.
[6012.44s -> 6017.44s]  I want to thank all my collaborators and funders,
[6017.44s -> 6019.44s]  and I'm happy to take questions.
[6023.44s -> 6024.44s]  Thank you, Sivarath,
[6024.44s -> 6027.44s]  for this very interesting and informative talk.
[6027.44s -> 6030.44s]  So we have a few questions from students,
[6030.44s -> 6034.44s]  so I'm going to read their question.
[6034.44s -> 6037.44s]  So first, about the second part of the talk,
[6037.44s -> 6039.44s]  the question is,
[6039.44s -> 6044.44s]  LLMs have most human knowledge memorized,
[6044.44s -> 6047.44s]  but why haven't they yet been applied
[6047.44s -> 6050.44s]  to make a single new connection
[6050.44s -> 6053.44s]  that leads to a new scientific discovery?
[6057.44s -> 6061.44s]  That's a great question.
[6061.44s -> 6065.44s]  I would say that there are a lot of folks
[6065.44s -> 6067.44s]  that are trying to make
[6067.44s -> 6071.44s]  new foundational scientific discoveries using LLMs,
[6071.44s -> 6073.44s]  so I would say that, you know,
[6073.44s -> 6076.44s]  the scaling law is a very basic example
[6076.44s -> 6079.44s]  of a new scientific discovery, right?
[6079.44s -> 6081.44s]  Now, this is, you know, maybe not...
[6081.44s -> 6084.44s]  It's a very contained domain, right,
[6084.44s -> 6086.44s]  but I think that the same sorts of ideas
[6086.44s -> 6091.44s]  can be applied to other, more challenging settings,
[6091.44s -> 6094.44s]  and there are a lot of folks that I know of
[6094.44s -> 6098.44s]  who are indeed applying these sorts of methods in science,
[6098.44s -> 6100.44s]  so just recently, there was a paper
[6100.44s -> 6106.44s]  where FunSearch was used for discovering models,
[6106.44s -> 6111.44s]  interpretable models of animal behavior
[6111.44s -> 6114.44s]  and also actually human behavior,
[6114.44s -> 6119.44s]  so this is an example of a discovery
[6119.44s -> 6121.44s]  that was made using LLMs, right,
[6121.44s -> 6122.44s]  but it wasn't just pure LLMs.
[6122.44s -> 6127.44s]  It was LLMs plus this kind of an agentic workflow,
[6127.44s -> 6131.44s]  and I would say that the methods that I shared today,
[6131.44s -> 6136.44s]  we have ongoing work on applying this to problems
[6136.44s -> 6138.44s]  that would excite a lot of scientists,
[6138.44s -> 6144.44s]  but this is not ready yet for publication.
[6144.44s -> 6147.44s]  Yeah, and a follow-up question is,
[6147.44s -> 6149.44s]  in which scientific domains
[6149.44s -> 6154.44s]  do you expect the first successful use of LLM agents?
[6154.44s -> 6155.44s]  This is a great question,
[6155.44s -> 6160.44s]  so I would say that you want a domain
[6160.44s -> 6168.44s]  where you can heavily leverage
[6168.44s -> 6172.44s]  the coding and symbolic abilities,
[6172.44s -> 6174.44s]  and so the appeal of physics to me
[6174.44s -> 6177.44s]  and related fields like astronomy and so on
[6177.44s -> 6184.44s]  is that a lot can be explained by symbolic models
[6184.44s -> 6187.44s]  whereas in settings like biology,
[6187.44s -> 6189.44s]  I think it's a lot more difficult.
[6189.44s -> 6192.44s]  You do not have this sort of,
[6192.44s -> 6195.44s]  you know, you can have neural networks do extremely well,
[6195.44s -> 6200.44s]  but I think that because the domains are less symbolic,
[6200.44s -> 6205.44s]  I think LLMs might find them, might have a harder time.
[6205.44s -> 6207.44s]  That said, I would say that, you know,
[6207.44s -> 6209.44s]  what we are seeing with vision language models
[6209.44s -> 6214.44s]  is that, or even, you know, vision transformers
[6214.44s -> 6219.44s]  is that if you train things, tokenize things properly,
[6219.44s -> 6222.44s]  you know, even data that doesn't look like text
[6222.44s -> 6226.44s]  can be, you can train LLMs to do well on that
[6226.44s -> 6229.44s]  and combinations of that and text.
[6229.44s -> 6231.44s]  But that said, my bet would be on domains
[6231.44s -> 6235.44s]  like physics and astronomy and so on.
[6235.44s -> 6236.44s]  Thanks.
[6236.44s -> 6242.44s]  And I think also a very interesting point you mentioned
[6242.44s -> 6246.44s]  is like the pipeline for scientific discovery
[6246.44s -> 6248.44s]  where you collect data and analyzing
[6248.44s -> 6251.44s]  and interpreting the data and draw hypotheses
[6251.44s -> 6254.44s]  and try to verify the hypotheses.
[6254.44s -> 6258.44s]  I think symbolic regression is probably analyzing the data,
[6258.44s -> 6262.44s]  but do you expect agents to do other paths as well?
[6262.44s -> 6266.44s]  Like are they going to do fully autonomous experiments
[6266.44s -> 6269.44s]  and collecting data from those experiments?
[6269.44s -> 6271.44s]  Absolutely. Absolutely.
[6271.44s -> 6274.44s]  So in the longer run, I would hope that, you know,
[6274.44s -> 6278.44s]  we would have these automated labs
[6278.44s -> 6283.44s]  where you'd have these AI agents that are able to,
[6283.44s -> 6285.44s]  you know, of course, within guardrails
[6285.44s -> 6287.44s]  that are able to come up with new hypotheses,
[6287.44s -> 6290.44s]  design experiments based on these hypotheses
[6290.44s -> 6295.44s]  and use robots to perform the experiments, right?
[6295.44s -> 6299.44s]  But we are very far away from that kind of a future.
[6299.44s -> 6305.44s]  However, I think smaller scale versions of this
[6305.44s -> 6310.44s]  could happen, I would say, in the near future.
[6310.44s -> 6312.44s]  That's really exciting.
[6312.44s -> 6317.44s]  We have two other questions about the early part of the talk.
[6318.44s -> 6322.44s]  You mentioned how COPRA was constrained by compute budget
[6322.44s -> 6325.44s]  in the mini-F2F evaluation.
[6325.44s -> 6329.44s]  What is known about how these systems scale
[6329.44s -> 6332.44s]  if you can add more compute during inference time?
[6334.44s -> 6338.44s]  Nothing public is known.
[6338.44s -> 6340.44s]  So if you are interested in this
[6340.44s -> 6342.44s]  and you have access to compute,
[6342.44s -> 6348.44s]  I would recommend, I would encourage you to scale this up.
[6348.44s -> 6354.44s]  But at this point, I do not know of any public data
[6354.44s -> 6357.44s]  on scaling this sort of method
[6357.44s -> 6360.44s]  to a very large number of inference queries.
[6363.44s -> 6370.44s]  The last question is, recent work shows language models
[6370.44s -> 6373.44s]  can assign meaning to embeddings
[6373.44s -> 6376.44s]  and even interpret it between the embeddings,
[6376.44s -> 6381.44s]  even if the embeddings do not directly map to concepts.
[6381.44s -> 6385.44s]  What are the implications for theorem proving?
[6388.44s -> 6391.44s]  So I want to understand this point a little bit better.
[6391.44s -> 6395.44s]  So when you say they don't directly map to concepts,
[6395.44s -> 6398.44s]  there are presumably some concepts that they're mapping, right?
[6398.44s -> 6403.44s]  It's just not concepts that are known to us necessarily at this point.
[6403.44s -> 6407.44s]  Is that a fair interpretation of this?
[6408.44s -> 6410.44s]  Well, I don't know for sure,
[6410.44s -> 6413.44s]  but my guess is maybe the student is asking about,
[6413.44s -> 6416.44s]  for example, in word embedding,
[6416.44s -> 6418.44s]  you have embedding corresponds to a word,
[6418.44s -> 6420.44s]  but you can interpret it between the embedding.
[6420.44s -> 6422.44s]  So that embedding in the middle
[6422.44s -> 6424.44s]  does not correspond to any specific word,
[6424.44s -> 6427.44s]  but it seems to carry some semantics.
[6428.44s -> 6430.44s]  Yeah, and I think that that is really powerful.
[6430.44s -> 6435.44s]  So what we can imagine is using these latent representations inside LLMs
[6435.44s -> 6440.44s]  to, for example, understand which kinds of theorems
[6440.44s -> 6442.44s]  can be solved by similar techniques
[6442.44s -> 6447.44s]  and then maybe that can drive the proof process, right?
[6447.44s -> 6450.44s]  So the kind of concept abstraction that I showed in COPRA
[6450.44s -> 6456.44s]  was very syntactic in nature,
[6456.44s -> 6461.44s]  like it was explicitly generating these subgoals as text
[6461.44s -> 6466.44s]  and then decomposing the problem in text
[6466.44s -> 6470.44s]  and then solving those subproblems in text.
[6470.44s -> 6471.44s]  But you could imagine a world
[6471.44s -> 6475.44s]  where you operate at the level of latent representations
[6475.44s -> 6477.44s]  and you take the latent representations
[6477.44s -> 6482.44s]  to cluster relevant problems.
[6482.44s -> 6487.44s]  Imagine an approach sort of like the laser approach,
[6487.44s -> 6489.44s]  right, but for theorem proving.
[6489.44s -> 6492.44s]  And I could imagine that such a process
[6492.44s -> 6497.44s]  could make use of the latent representations.
[6498.44s -> 6500.44s]  Yeah, thank you.
[6500.44s -> 6502.44s]  I think that's about everything
[6502.44s -> 6506.44s]  and let's say thanks again for giving this talk
[6506.44s -> 6511.44s]  and yeah, and later we will follow up on the slides
[6511.44s -> 6513.44s]  and also the video.
[6513.44s -> 6515.44s]  Sounds great, thank you very much.
[6515.44s -> 6517.44s]  Thank you.
[6517.44s -> 6518.44s]  Thank you so much.
[6518.44s -> 6520.44s]  Yeah, thanks, bye.
[6520.44s -> 6521.44s]  Bye.
[6521.44s -> 6522.44s]  Bye-bye.
[6541.44s -> 6542.44s]  Thank you.
[6571.44s -> 6586.44s]  Oh, Alex, there's like something.
[6586.44s -> 6589.44s]  There's a pop-up blocking the QR code.
[6589.44s -> 6592.44s]  All right, yeah, thank you.
[6592.44s -> 6594.44s]  Sorry about that.
[6622.44s -> 6624.44s]  Thank you.
[6652.44s -> 6654.44s]  Thank you.
[6682.44s -> 6684.44s]  Thank you.
[6712.44s -> 6714.44s]  Thank you.
[6742.44s -> 6744.44s]  Thank you.
[6772.44s -> 6774.44s]  Thank you.
[6802.44s -> 6804.44s]  Thank you.
[6832.44s -> 6834.44s]  Thank you.
[6862.44s -> 6864.44s]  Thank you.
[6892.44s -> 6894.44s]  Thank you.
[6922.44s -> 6924.44s]  Thank you.
[6952.44s -> 6954.44s]  Thank you.
[6982.44s -> 6984.44s]  Thank you.
[7012.44s -> 7014.44s]  Thank you.
[7042.44s -> 7044.44s]  Thank you.
[7072.44s -> 7074.44s]  Thank you.
[7102.44s -> 7104.44s]  Thank you.
[7132.44s -> 7134.44s]  Thank you.
[7162.44s -> 7164.44s]  Thank you.
