# Detected language: en (p=1.00)

[0.00s -> 0.84s]  Hello, everyone.
[0.84s -> 2.96s]  I'm really, really glad to be here,
[2.96s -> 5.44s]  be able to share a little bit more about our work.
[5.44s -> 8.84s]  And I would, of course, like to organize the organizers
[8.84s -> 11.08s]  for inviting me to be a part of this class.
[12.48s -> 14.64s]  All right, so, you know, three and a half years ago,
[14.64s -> 16.72s]  I had just finished two big projects.
[16.72s -> 17.96s]  One of them was alpha code.
[17.96s -> 19.68s]  The other one was alpha tensor.
[19.68s -> 21.08s]  I'll tell you a little bit more about those
[21.08s -> 21.92s]  in the future.
[21.92s -> 26.00s]  But basically during the Christmas holiday in 2021,
[26.00s -> 28.12s]  I stumbled on this talk from 2019
[28.16s -> 30.08s]  called The Future of Mathematics.
[30.08s -> 32.08s]  And the talk is over an hour.
[32.08s -> 34.96s]  The comments were extremely positive.
[34.96s -> 36.14s]  One person was saying, you know,
[36.14s -> 39.36s]  this is the best lecture and the best YouTube video ever.
[39.36s -> 41.96s]  I've never watched anything more enjoyable.
[41.96s -> 45.86s]  So, you know, I was pretty intrigued and I pressed play
[45.86s -> 49.60s]  and we started a chain of inelectable events
[49.60s -> 53.04s]  that basically takes me here in front of you today.
[53.04s -> 56.80s]  So in this talk, rather than focusing on the details,
[57.20s -> 59.96s]  I'd really like to focus on the big ideas
[59.96s -> 62.72s]  that allowed us to work on alpha proof
[62.72s -> 65.40s]  and why we believe it has an integral role to play
[65.40s -> 67.28s]  in the future of mathematics.
[67.28s -> 70.24s]  I will basically talk first about the rise
[70.24s -> 72.48s]  of formal mathematics and what that is,
[72.48s -> 75.40s]  and then about our experience in reinforcement learning
[75.40s -> 77.56s]  and the lessons we've took away
[77.56s -> 80.80s]  and how these two things meet with alpha proof.
[83.10s -> 85.72s]  So, Ganileo said quite a long time ago
[85.72s -> 87.42s]  that the laws of nature are written
[87.42s -> 89.48s]  in the language of mathematics.
[89.48s -> 92.04s]  It was an incredible prediction or insight
[92.04s -> 95.48s]  because we've made a lot of progress since Ganileo.
[95.48s -> 98.24s]  And indeed now just a few equations on a single page
[98.24s -> 102.08s]  can describe how the universe works extremely precisely.
[103.54s -> 105.40s]  Also in a famous essay
[105.40s -> 108.92s]  called The Unreasonable Effectiveness of Mathematics,
[108.92s -> 111.28s]  Eugene Wigner argues that, you know,
[111.28s -> 114.20s]  mathematics not only have like a descriptive power,
[114.24s -> 116.96s]  but they also have predictive power.
[116.96s -> 119.88s]  One example he gives, if I'm not mistaken,
[119.88s -> 123.24s]  is the development of matrix algebra
[123.24s -> 124.94s]  and its use in quantum mechanics.
[125.84s -> 127.52s]  Of course, once you can describe
[127.52s -> 132.04s]  and predict the natural world, you can start to shape it.
[132.04s -> 135.00s]  And if you look around us, you know,
[135.00s -> 136.88s]  in the world we live in, the civilization,
[136.88s -> 138.48s]  the culture, all the tech, you know,
[138.48s -> 140.64s]  that makes our current world,
[140.64s -> 142.20s]  well, basically mathematics is just
[142.24s -> 143.60s]  at the core of everything.
[145.04s -> 148.08s]  And maybe I would also like to argue that
[148.08s -> 151.08s]  mathematics is the root node to intelligence.
[151.08s -> 153.40s]  The root node is a word that
[153.40s -> 156.78s]  Demetris really likes to talk about.
[157.80s -> 161.52s]  And basically, you know, it means that from that road
[161.52s -> 165.32s]  is going across on the whole tree of possibilities.
[165.32s -> 167.80s]  And why could we argue that is because, you know,
[167.80s -> 170.88s]  mathematics requires reasoning and planning,
[170.88s -> 174.48s]  generalization and abstraction, knowledge and creativity,
[174.48s -> 177.66s]  open-ended and unbounded complexity.
[177.66s -> 180.04s]  And over the years, I also understood that
[180.04s -> 182.66s]  mathematics is not just this kind of
[182.66s -> 185.24s]  application of logical rules one after the other.
[185.24s -> 189.20s]  It's really a sort of art.
[189.20s -> 191.80s]  And so it even requires a knife of beauty.
[191.80s -> 193.44s]  And so I used a lot of keywords here,
[193.44s -> 197.12s]  and that you'll see them reappearing during my talk.
[197.12s -> 199.92s]  And I hope that will make this a bit more concrete.
[201.60s -> 205.60s]  In summary, I've tried to convey to you that,
[205.60s -> 207.44s]  you know, mathematics is a root node
[207.44s -> 209.16s]  to understand the natural world,
[209.16s -> 212.00s]  and that you also might be a root node for intelligence.
[213.40s -> 215.44s]  So I think it's worth kind of really thinking
[215.44s -> 218.88s]  about mathematics and see what can we do for that.
[218.88s -> 221.50s]  But before that, I'd like to give you
[221.50s -> 223.48s]  a really super brief history
[223.48s -> 224.96s]  of formalization in mathematics,
[224.96s -> 227.44s]  because basically mathematics hasn't always looked
[227.44s -> 228.96s]  the way it looks now.
[229.44s -> 232.32s]  About 500, you know, before Christ,
[232.32s -> 235.64s]  you know, the Greeks had a great revelation.
[235.64s -> 237.96s]  They understood the importance of the concept
[237.96s -> 239.60s]  of mathematical proof.
[239.60s -> 241.76s]  And in some sense, can you really imagine, you know,
[241.76s -> 244.26s]  it's like you find something that will tell you
[244.26s -> 245.92s]  with certainty that you have attained
[245.92s -> 247.08s]  kind of eternal truth,
[247.08s -> 250.32s]  and you can discover more and more truths of our universe.
[250.32s -> 252.36s]  It must have felt incredible.
[253.60s -> 254.88s]  And since then, you know,
[254.88s -> 257.40s]  proof is at the heart of mathematics,
[257.40s -> 259.16s]  and what constitutes a proof, however,
[259.16s -> 262.96s]  has evolved over the millenniums of mathematical history.
[262.96s -> 265.96s]  One of the first breakthroughs, I'm sure that you know,
[265.96s -> 268.52s]  is with Euclid and his book Elements
[268.52s -> 271.24s]  that have definitions, axioms and theorems.
[271.24s -> 275.28s]  So that's really kind of the start of axiomatization,
[275.28s -> 279.44s]  but that has really kind of been developed much more
[279.44s -> 282.76s]  in the early 20th century with Hilbert program
[282.76s -> 285.60s]  that really developed this to learn on what we have now.
[286.60s -> 289.48s]  So that's kind of axiomatization on one side.
[289.48s -> 292.48s]  On the other side, you know, like mathematics,
[293.58s -> 296.04s]  you know, at the beginning, it didn't really use symbols.
[296.04s -> 297.72s]  And that's for a very long time, you know,
[297.72s -> 300.28s]  even if the Greeks had taken a fine axiomatization
[300.28s -> 302.00s]  and started to use some symbols,
[302.00s -> 304.44s]  the rest of the world kind of described really
[304.44s -> 306.86s]  mathematics with a lot of words.
[306.86s -> 308.88s]  And so here you have like a square
[308.88s -> 311.48s]  and 10 times its root, I pull to 39.
[311.48s -> 313.28s]  So today we would write this as X squared
[313.28s -> 315.40s]  plus 10 X equals 39.
[316.16s -> 317.00s]  So you can look at this description
[317.00s -> 318.52s]  of how do you solve this thing?
[318.52s -> 319.82s]  Take the coefficient of the root,
[319.82s -> 321.94s]  divide it into two parts and multiply one of them
[321.94s -> 325.26s]  by itself at least to 39, et cetera, et cetera, right?
[325.26s -> 328.44s]  Now you just write kind of this concise formula.
[328.44s -> 331.46s]  And there are two things here that comes, you know,
[331.46s -> 333.72s]  is first, you know, replacing kind of, you know,
[333.72s -> 337.48s]  using special in the machine learning lingo,
[337.48s -> 340.82s]  using special tokens for numbers, right?
[340.82s -> 343.92s]  Like we present like the digits have special,
[344.36s -> 346.64s]  special kind of characters,
[346.64s -> 348.32s]  but also even more important,
[348.32s -> 349.84s]  I don't know which one is more important,
[349.84s -> 352.12s]  but that helps with also kind of,
[352.12s -> 354.80s]  I think only in the 17th century,
[354.80s -> 357.56s]  RenÃ© Descartes kind of comes up with the idea
[357.56s -> 360.60s]  of using kind of the early letter of the alphabets
[360.60s -> 363.92s]  to denote constants and the end of the letter
[363.92s -> 367.04s]  of the alphabets X, Y, Z to describe unknowns.
[367.04s -> 370.00s]  And so that's basically how you end up with this formula,
[370.00s -> 371.72s]  but it takes, you know, like, you know,
[371.72s -> 374.36s]  I don't know how many it takes until the 17th century
[374.36s -> 376.36s]  to really come up with those notations.
[378.04s -> 382.44s]  And I think it's not hard to argue that precise notation
[382.44s -> 385.04s]  and axiomatization really eliminated ambiguities
[385.04s -> 387.88s]  and logical error, that symbolic notation
[387.88s -> 390.40s]  made complex manipulations more tractable
[390.40s -> 393.04s]  and requires much less, you know, cognitive load.
[393.04s -> 396.92s]  It's also kind of packs information extremely densely.
[398.16s -> 399.12s]  In terms of notation,
[399.12s -> 401.36s]  I can't remember exactly who said that
[401.88s -> 403.04s]  and if it was a joke or not,
[403.04s -> 406.60s]  but someone was arguing that maybe the Einstein notations
[406.60s -> 409.48s]  was his biggest contribution.
[409.48s -> 410.96s]  Then maybe pushing it a little bit
[410.96s -> 415.48s]  with given like the impact of general relativity,
[415.48s -> 417.56s]  special relativity and his work
[417.56s -> 419.60s]  to introduce quantum mechanics.
[419.60s -> 423.60s]  But certainly kind of Einstein notations
[423.60s -> 424.68s]  are very important.
[424.68s -> 426.28s]  Actually, if you read machine learning code,
[426.28s -> 428.98s]  you'll see that appearing a lot as well.
[428.98s -> 431.82s]  But basically once you have those symbols
[431.82s -> 433.94s]  and you can lift yourself up
[433.94s -> 435.94s]  from kind of particular examples,
[435.94s -> 439.38s]  you start to be able to observe different common patterns
[439.38s -> 440.86s]  across different contexts.
[440.86s -> 443.90s]  So that's where abstraction and generalization comes up.
[443.90s -> 446.64s]  You can unify disparate areas of mathematics
[447.62s -> 449.98s]  and actually you can even create new fields.
[449.98s -> 453.62s]  And so my favorite example here is really the people
[453.62s -> 457.18s]  who are looking at the five axioms of Euclidean geometry
[457.22s -> 460.50s]  and actually, I don't know, 4,000 of year, I think,
[460.50s -> 463.74s]  trying to see if they could remove the fifth axiom.
[463.74s -> 466.58s]  And that actually led to non-Euclidean geometry
[467.66s -> 470.18s]  and in particular, Riemannian geometry,
[470.18s -> 474.94s]  which in the theme of general relativity
[474.94s -> 477.10s]  was essential to get general relativity.
[478.24s -> 483.24s]  So this formalization has helped mathematics,
[484.18s -> 486.30s]  but also the other sciences,
[486.30s -> 488.54s]  in particular, physics.
[488.54s -> 490.58s]  And so I'd like to argue that basically
[490.58s -> 492.14s]  computer formulation,
[492.14s -> 494.78s]  which basically means writing mathematics in code,
[494.78s -> 496.66s]  is just the next logical phase
[496.66s -> 499.26s]  in this grand arc of mathematical formalism.
[501.46s -> 503.24s]  So there's been quite a few languages
[503.24s -> 504.74s]  to describe mathematical reasoning,
[504.74s -> 506.42s]  such as Koch and Isabel,
[506.42s -> 508.90s]  and Lean is one of the most recent ones,
[508.90s -> 512.10s]  originally developed, I believe, for software education,
[512.10s -> 514.34s]  but also with mathematics in mind.
[514.66s -> 517.22s]  And mathematicians saw in Lean kind of the tool
[517.22s -> 518.70s]  that would make the next big step
[518.70s -> 522.42s]  in mathematical formalism.
[522.42s -> 523.88s]  It's a programming language,
[523.88s -> 526.48s]  it's a theorem prover and interactive proof assistant,
[526.48s -> 529.10s]  and Lean has a really vibrant open source community
[529.10s -> 530.32s]  of mathematicians.
[530.32s -> 533.74s]  It's been used to formalize cutting edge mathematics,
[533.74s -> 535.42s]  field metal mathematics,
[535.42s -> 538.58s]  and the mathematicians are on the huge quest
[539.62s -> 543.26s]  to basically build the math library of the future.
[543.78s -> 545.52s]  Let me go there.
[545.52s -> 546.90s]  It's called MathLib.
[546.90s -> 548.62s]  It's built open source
[548.62s -> 551.14s]  on basically the free time of mathematicians,
[551.14s -> 554.04s]  and it has two big goals.
[554.04s -> 555.94s]  It tends to be general and unified.
[555.94s -> 559.22s]  So general, it means that it has the weakest assumptions
[559.22s -> 561.02s]  and the strongest results,
[561.02s -> 562.88s]  and unified is that it only has
[562.88s -> 564.38s]  one definition per concept.
[564.38s -> 565.52s]  So I was actually pretty surprised
[565.52s -> 568.94s]  when I learned about this unification idea,
[568.94s -> 571.06s]  because I thought that mathematics
[572.04s -> 576.42s]  was written in a very structured way already.
[576.42s -> 580.74s]  But apparently it turns out that for a given concept,
[580.74s -> 582.14s]  you may have several papers
[582.14s -> 583.96s]  that basically talk about the same concept,
[583.96s -> 585.82s]  but that define it in a different way
[585.82s -> 588.50s]  that is not necessarily compatible with each other.
[588.50s -> 590.70s]  And so you need to do quite a bit of work
[590.70s -> 594.18s]  to actually kind of try to find the unifying definition
[594.18s -> 596.06s]  that underlies all those ideas.
[596.06s -> 597.94s]  And so that's actually what takes the most time
[597.94s -> 599.30s]  in building math.
[601.62s -> 604.66s]  Maybe you can see in the representation I gave here
[604.66s -> 606.90s]  that it's one monolithic library
[606.90s -> 608.94s]  where everything is connected together.
[608.94s -> 611.48s]  And so this is very important for modern math,
[611.48s -> 613.90s]  because basically modern math is about kind of
[613.90s -> 615.90s]  trying to connect all areas of math.
[615.90s -> 617.70s]  And there's been observations that actually,
[617.70s -> 621.86s]  like ideas from here can help ideas from there.
[621.86s -> 625.14s]  And for instance, maybe Fermat's last theorem's proof
[625.14s -> 626.62s]  is a great example of this
[626.62s -> 628.00s]  connecting two different areas
[628.48s -> 631.20s]  to try to prove one huge theorem.
[631.20s -> 633.72s]  And now there's also something called the Lang Langs program
[633.72s -> 635.60s]  that really explicitly tries to do that
[635.60s -> 636.76s]  and connect all these areas.
[636.76s -> 638.86s]  So it's really important that you have a library
[638.86s -> 642.88s]  that basically can connect all different areas of math.
[642.88s -> 645.84s]  Currently the library covers about 80%
[645.84s -> 647.40s]  of the undergrad curricula.
[648.40s -> 650.08s]  And most of the library, however,
[650.08s -> 651.88s]  is above the undergraduate,
[651.88s -> 655.68s]  but has a lot of irregular coverage.
[655.68s -> 658.48s]  And because I think particular examples
[658.48s -> 662.00s]  are complex analysis or PDE theory.
[663.40s -> 666.72s]  Okay, all great, but why do all of this, right?
[667.72s -> 670.20s]  Is because computer formalization
[670.20s -> 672.62s]  unlocks kind of enormous synergies.
[672.62s -> 674.36s]  And maybe to explain myself a little bit,
[674.36s -> 675.46s]  I have to step back.
[676.52s -> 678.78s]  And for those of you who are coders,
[678.78s -> 681.06s]  maybe just understanding the right code
[681.06s -> 683.08s]  and be able to use all the software stack,
[683.08s -> 686.28s]  that should be already a huge win for mathematics.
[686.28s -> 687.94s]  But basically the other property
[687.94s -> 690.28s]  of when you write mathematics in code
[690.28s -> 692.14s]  with languages such as lean
[692.14s -> 694.04s]  is that you get perfect verification.
[694.04s -> 696.44s]  So when you write a proof
[696.44s -> 697.72s]  and you write the correct proof,
[697.72s -> 699.52s]  lean is gonna tell you, yes, this is correct.
[699.52s -> 700.68s]  And when you've made a mistake,
[700.68s -> 702.68s]  lean is gonna tell you this is your proof
[702.68s -> 705.00s]  is not correct or is still incomplete.
[706.04s -> 710.24s]  So from these two things, you get some instant wins.
[710.24s -> 714.40s]  First, your correctness concerns completely disappear.
[714.40s -> 716.52s]  You can trust giant proofs
[716.52s -> 718.64s]  and you can delegate proof checking
[718.64s -> 719.92s]  entirely to the computer.
[721.56s -> 723.40s]  And so I think it's important to note
[723.40s -> 725.08s]  and it's my understanding that
[725.08s -> 727.68s]  it's not like all areas of mathematics
[727.68s -> 731.14s]  are suffering from concerns about correctness.
[731.14s -> 734.64s]  I think in reality, most maybe areas
[734.64s -> 737.00s]  where the mathematicians feel pretty comfortable
[737.00s -> 742.00s]  about their ability to discern correctness from mistakes.
[743.16s -> 746.28s]  But I've been hearing that in maybe some of the areas
[746.28s -> 748.08s]  that are more abstract areas,
[748.08s -> 751.42s]  actually they start to be real concerns for correctness.
[751.42s -> 755.04s]  So that's not everywhere, but that starts to be important.
[755.04s -> 756.64s]  So these are the instant wins.
[757.70s -> 761.72s]  But those perfect verification and software stack
[761.72s -> 764.00s]  has really kind of a knock-on effect
[764.00s -> 766.08s]  that could lead to game changers.
[767.60s -> 770.80s]  So first, I think for all of you who are coders
[770.80s -> 773.80s]  to bring all the tools that we have developed for coding
[773.80s -> 776.72s]  should already be a big, big win.
[776.72s -> 781.04s]  Next, I have transformed mathematics into a video game.
[781.04s -> 783.08s]  So what does this video game look like?
[783.08s -> 784.64s]  Well, you have, what are the levels?
[784.64s -> 787.32s]  The levels are the different theorems.
[787.32s -> 791.04s]  And then you can interact in those levels
[791.04s -> 792.74s]  and basically kind of lean will tell you,
[792.74s -> 794.72s]  yes, you've completed the level or not
[794.72s -> 798.00s]  with this perfect verification feedback.
[798.00s -> 800.04s]  You always have harder level
[800.04s -> 803.28s]  and maybe contrary to other games where they,
[803.28s -> 805.44s]  I don't know, you have to shoot other people.
[805.44s -> 809.36s]  This is a super meaningful, useful, and profound game.
[809.36s -> 811.64s]  You're basically adding new truths for it.
[812.80s -> 816.88s]  The lean people describe this as,
[816.88s -> 819.28s]  they could do this 14 hours a day
[819.28s -> 822.08s]  and they're constantly getting positive reinforcements.
[822.80s -> 826.72s]  And it also means that it's great for education.
[826.72s -> 830.00s]  First, a lot younger people now are really used
[830.00s -> 831.96s]  to using computers and softwares.
[831.96s -> 834.64s]  And it's great because it's interactive, right?
[834.64s -> 837.02s]  It's when it's fun, it's easy to learn.
[837.02s -> 840.36s]  And you can also create kind of software
[840.36s -> 844.48s]  such that the detail of the level of detail
[844.48s -> 846.84s]  is adapted to the person who is reading
[846.84s -> 851.84s]  rather than the level that the author
[852.52s -> 853.44s]  thought was interesting.
[853.44s -> 855.52s]  I don't know if you read a lot of mass papers
[855.52s -> 858.74s]  or you look at solutions of some problems
[858.74s -> 859.76s]  you are trying to solve.
[859.76s -> 863.96s]  Sometimes people say this is quite obvious,
[863.96s -> 865.20s]  but maybe it is not for you
[865.20s -> 868.48s]  and you would love to see more detail.
[868.48s -> 872.32s]  I have also an example from actually from my dad
[872.32s -> 874.84s]  who recently tried to learn kind of Galois theory
[874.84s -> 877.48s]  in a different online class.
[877.48s -> 879.88s]  And so he went through week one and week two
[879.88s -> 881.68s]  and week three are all good,
[882.16s -> 883.28s]  but then when he was going through week four
[883.28s -> 884.36s]  and trying to do the homework,
[884.36s -> 885.68s]  he realized that there was something
[885.68s -> 888.08s]  he didn't understand.
[888.08s -> 891.58s]  And he basically needed someone to explain to him
[891.58s -> 893.84s]  and he messaged the professor,
[893.84s -> 896.16s]  but unfortunately the professor didn't reply.
[896.16s -> 898.60s]  And so he was basically completely stuck.
[898.60s -> 901.64s]  But imagine that instead, he was doing this in Lean,
[901.64s -> 904.02s]  then Lean would have said exactly what is right,
[904.02s -> 905.56s]  what is wrong, what is missing, et cetera.
[905.56s -> 907.98s]  And he would be able to have like constant feedback
[907.98s -> 910.74s]  without having to ask his teacher.
[912.68s -> 916.88s]  And now I want to go to the last point.
[916.88s -> 920.02s]  It's actually a point that has been made by Terry Tao,
[920.02s -> 925.02s]  I think about a year and a half ago at least.
[925.82s -> 928.44s]  And he describes that basically using
[928.44s -> 932.12s]  computer formalization will unlock massive collaboration
[932.12s -> 933.66s]  between mathematicians.
[933.66s -> 934.68s]  And so why does he say that?
[934.68s -> 937.40s]  Well, apparently the current state of mathematics
[937.40s -> 941.48s]  kind of collaboration is that you can do
[942.32s -> 944.40s]  most of the papers are either a single author
[944.40s -> 945.76s]  or two author paper,
[945.76s -> 949.00s]  and kind of the max is really five author papers.
[949.00s -> 952.64s]  And the reason why is because every author
[952.64s -> 955.52s]  needs to check everyone else's work.
[955.52s -> 958.60s]  And so basically kind of having more and more
[958.60s -> 960.80s]  collaborators doesn't really scale,
[960.80s -> 963.44s]  but we still need to check everyone else's work.
[963.44s -> 965.56s]  And so that's where like computer formalization
[965.56s -> 967.44s]  and pathway verification comes in,
[967.44s -> 970.20s]  is that if you could trust the computer
[970.20s -> 971.46s]  to check the proofs,
[971.46s -> 973.32s]  then suddenly you can,
[973.32s -> 976.86s]  it opens up kind of working on mathematical projects
[976.86s -> 978.24s]  to, I don't know, thousands of people,
[978.24s -> 980.70s]  because you could have someone you've never met
[980.70s -> 982.22s]  who lived in the middle of nowhere,
[982.22s -> 984.26s]  who contributes a little bit to your proof,
[984.26s -> 985.22s]  a lemma there,
[985.22s -> 987.12s]  and then lean would say, yes, this is correct.
[987.12s -> 988.90s]  And then you can just move on.
[988.90s -> 992.10s]  And so there's been kind of maybe,
[992.10s -> 993.78s]  there's been a few projects like this.
[993.78s -> 996.74s]  I think mass maybe is maybe one of the coolest
[996.74s -> 998.90s]  demonstration where you have hundreds of mathematicians
[998.90s -> 1002.22s]  working together to build this library of the future.
[1002.22s -> 1003.94s]  But I think Tere-Tao is also arguing
[1003.94s -> 1006.50s]  that you could use that to do new mathematics
[1006.50s -> 1009.88s]  and has actually launched a project in the past
[1009.88s -> 1013.86s]  called universal algebra, where he was exploring that.
[1013.86s -> 1016.80s]  But basically, I think in ML at the moment,
[1016.80s -> 1018.26s]  we are all about scaling.
[1018.26s -> 1020.76s]  And here basically you could scale humans
[1020.76s -> 1023.44s]  to do mathematics and potentially maybe
[1023.44s -> 1025.24s]  tackle the harder problems.
[1026.70s -> 1028.02s]  All right.
[1028.02s -> 1030.18s]  So, you know, should everyone go to, you know,
[1030.18s -> 1032.38s]  computer formalization, it's awesome.
[1032.38s -> 1034.28s]  The reality is that there are still challenges
[1034.28s -> 1036.08s]  for computer formulation.
[1036.08s -> 1040.06s]  In my estimates, about only 0.1% to 1% of mathematicians
[1040.06s -> 1041.22s]  have adopted lean.
[1041.22s -> 1045.46s]  It's a small community of mathematicians, a vibrant one.
[1045.46s -> 1046.54s]  But why is that?
[1046.54s -> 1049.58s]  It's because it's actually not yet a great tool
[1049.58s -> 1051.36s]  for mainstream research.
[1051.36s -> 1055.46s]  Like a lot of, you know, programming languages,
[1055.46s -> 1057.58s]  it has a steep learning curve.
[1058.10s -> 1060.66s]  It also requires a significant time investment
[1060.66s -> 1063.72s]  because, you know, to get perfect verification,
[1063.72s -> 1067.04s]  you need to be able to write every single detail.
[1068.82s -> 1070.46s]  And so, you know, that takes a lot more time.
[1070.46s -> 1073.08s]  Maybe that's not the place where you want to focus.
[1073.08s -> 1075.34s]  You want to focus on the things you don't even know how
[1075.34s -> 1078.06s]  to solve, not on the things that are more obvious
[1078.06s -> 1078.90s]  to you.
[1079.80s -> 1082.50s]  And so that will be improved, you know,
[1082.50s -> 1085.18s]  the tooling and the library matchers.
[1086.06s -> 1089.22s]  But at the moment, there's still some things missing.
[1089.22s -> 1092.10s]  And obviously, you know, there is also a passive lack
[1092.10s -> 1094.26s]  of necessity for research.
[1094.26s -> 1095.98s]  But I'm hoping that, you know, the arguments
[1095.98s -> 1098.30s]  I've outlined in the slide before
[1098.30s -> 1102.42s]  are going to change a little bit at that point.
[1102.42s -> 1104.38s]  So now I just want to show you a little bit
[1104.38s -> 1106.36s]  what it looks like.
[1106.36s -> 1111.08s]  So, you know, this is a proof that you may have,
[1111.08s -> 1112.16s]  you may know about.
[1113.12s -> 1115.68s]  It's called, it's about how,
[1115.68s -> 1118.04s]  it's about proving that there is an infinite number
[1118.04s -> 1118.88s]  of primes.
[1120.36s -> 1123.16s]  And, okay, so this is what it looks like in me.
[1123.16s -> 1125.42s]  I hope you can see that it's big enough.
[1126.44s -> 1127.28s]  And so,
[1130.64s -> 1132.00s]  okay, here it is.
[1132.00s -> 1133.16s]  This is the theorem statement.
[1133.16s -> 1137.32s]  And I think what is kind of remarkable is that
[1137.32s -> 1139.36s]  because you can use Unicode,
[1139.36s -> 1141.84s]  it looks pretty much what you would write yourself
[1142.36s -> 1143.52s]  on a piece of paper.
[1143.52s -> 1145.70s]  There's that for all N natural numbers,
[1145.70s -> 1147.08s]  there is exist a P,
[1147.08s -> 1149.72s]  which that P is bigger than N and P is prime.
[1149.72s -> 1151.68s]  So that's how one way to say
[1151.68s -> 1153.88s]  that there is an infinite number of primes.
[1155.78s -> 1157.20s]  This is what the proof look like.
[1157.20s -> 1159.40s]  So fairly compact in this case.
[1160.44s -> 1162.66s]  And the fact that you can understand all of this
[1162.66s -> 1164.92s]  is because basically lean is a language based
[1164.92s -> 1168.78s]  high level syntax and mirroring normal mathematics.
[1169.78s -> 1174.38s]  So let me go a little bit on what it looks like
[1174.38s -> 1176.06s]  to interact with this.
[1176.06s -> 1178.06s]  So here is the first line of the proof.
[1178.06s -> 1181.38s]  And this is actually the main insight that Euclid had
[1181.38s -> 1184.24s]  is to consider first N factorial plus one
[1184.24s -> 1185.50s]  and the mean fact of it.
[1185.50s -> 1186.70s]  So what is the mean fact?
[1186.70s -> 1190.18s]  Is the mean factor of N factorial plus one.
[1190.18s -> 1193.06s]  And so I want to draw your attention on the box
[1193.06s -> 1195.78s]  on the right there for the tactic state.
[1195.78s -> 1197.94s]  And so basically that's what lean returns to you.
[1198.10s -> 1200.16s]  It says, oh, here are the things we know
[1200.16s -> 1201.90s]  and here are the things we assume you to prove.
[1201.90s -> 1204.36s]  So we know that N is a natural number.
[1204.36s -> 1208.82s]  We know that P is this mean factor of N factorial plus one
[1208.82s -> 1210.50s]  and you still need to show that there exists a P
[1210.50s -> 1212.86s]  such that P is bigger than N and P is prime.
[1214.74s -> 1216.36s]  So let's go a second line.
[1216.36s -> 1218.86s]  Second line says, oh, let's prove
[1218.86s -> 1221.38s]  that N factorial plus one is not one.
[1221.38s -> 1223.48s]  And you have actually the proof in line
[1223.48s -> 1225.26s]  with maybe a few more characters
[1225.30s -> 1228.98s]  and you really want to prove something this easy.
[1228.98s -> 1232.34s]  But on the right, you see that you have a new fact
[1232.34s -> 1237.34s]  in your arsenal is that N factorial plus one is not one.
[1239.98s -> 1241.70s]  And then we're going to say,
[1241.70s -> 1245.50s]  now let's prove that P is prime and the proof is say,
[1245.50s -> 1246.86s]  well, it's because it's basically
[1246.86s -> 1248.94s]  we took the minimum factor, right?
[1248.94s -> 1253.06s]  The minimum factor is kind of obvious that it's a prime
[1253.06s -> 1256.18s]  unless the thing you've taken off is one.
[1256.18s -> 1258.50s]  And so that's why you have this thing
[1258.50s -> 1260.02s]  you needed to prove before.
[1260.02s -> 1262.46s]  And then kind of the proof continues.
[1262.46s -> 1264.54s]  And at the end, you should see
[1264.54s -> 1266.42s]  in your tactic state, no goals.
[1267.52s -> 1270.46s]  And it used to be, I think, in a previous version of Lean
[1270.46s -> 1273.24s]  that you'd also received kind of this tada emoticon.
[1274.54s -> 1278.10s]  And I think, unfortunately, it's been removed
[1278.10s -> 1280.18s]  from the current version of Lean,
[1281.18s -> 1284.10s]  but basically, it's your little reward
[1284.10s -> 1285.86s]  for completing this level.
[1287.42s -> 1290.42s]  So just to recap, right, computer formalization.
[1290.42s -> 1294.06s]  And there is a really vibrant community
[1294.06s -> 1296.96s]  that has a huge enthusiasm for this.
[1296.96s -> 1299.18s]  They are building the library of the future
[1299.18s -> 1300.90s]  in their own time.
[1300.90s -> 1304.10s]  It has many, many ways in which
[1304.10s -> 1306.90s]  it can unlock enormous synergies,
[1306.90s -> 1309.22s]  but it still has some way to go.
[1309.64s -> 1312.22s]  And this community and myself as well,
[1314.02s -> 1315.98s]  we have the belief that this will inevitably
[1315.98s -> 1319.22s]  play an integral role in the future of mathematics.
[1319.22s -> 1322.44s]  Just this work kind of pushed the computer formalization.
[1323.68s -> 1326.38s]  All right, so now I want to talk
[1326.38s -> 1328.78s]  about some concurrent development in AI
[1328.78s -> 1331.38s]  and focus on something that was extremely popular
[1331.38s -> 1332.58s]  a few years ago,
[1332.58s -> 1334.58s]  got forgotten a bit in the last couple of years
[1334.58s -> 1338.42s]  and nowadays making a comeback big time.
[1338.42s -> 1341.66s]  So hopefully, you've picked up on my hints
[1341.66s -> 1344.82s]  with this little reward, this environment,
[1344.82s -> 1348.06s]  this interactiveness, this video game.
[1348.06s -> 1350.46s]  I want to talk about reinforcement learning.
[1352.42s -> 1357.42s]  So what's reinforcement learning, also known as RL?
[1358.18s -> 1360.74s]  Basically, it's just trial and error learning.
[1360.74s -> 1363.00s]  It's a very, very general framework
[1363.00s -> 1365.88s]  of an agent trying to achieve a goal.
[1365.88s -> 1367.46s]  So really, this agent learns
[1367.46s -> 1371.30s]  by interacting with an environment to maximize a reward.
[1371.30s -> 1374.46s]  And I think there is sometimes misconception
[1374.46s -> 1375.74s]  about this reward.
[1375.74s -> 1377.34s]  It has to be very dense
[1377.34s -> 1380.72s]  or it has to guide you towards a solution, et cetera.
[1380.72s -> 1384.54s]  Not at all in the framework of RL.
[1384.54s -> 1386.18s]  The reward can be basically,
[1386.18s -> 1388.68s]  have I achieved my goal or not?
[1388.68s -> 1391.54s]  So basically, in RL, we really need to have a goal,
[1391.54s -> 1394.18s]  but then your reward can be a goal achieved
[1394.18s -> 1396.74s]  equal than one, otherwise a zero all the time.
[1398.20s -> 1400.32s]  If you don't really have a goal
[1400.32s -> 1402.14s]  that you want to try to achieve,
[1402.14s -> 1403.94s]  then maybe that doesn't fit very well
[1403.94s -> 1405.66s]  into the RL framework.
[1405.66s -> 1407.08s]  But if you have a goal,
[1407.08s -> 1410.70s]  then super framework to work with,
[1411.54s -> 1415.62s]  especially since it has actually been proven
[1415.62s -> 1417.86s]  to work really, really well.
[1417.86s -> 1420.18s]  So here is a sample of the most successful stories
[1420.18s -> 1421.38s]  coming out of DeepMind
[1421.70s -> 1425.74s]  when RL was basically run at scale.
[1425.74s -> 1427.70s]  So as you can see, there are a lot of examples
[1427.70s -> 1430.22s]  of games and video games.
[1430.22s -> 1433.78s]  So for instance, the seminal work on Atari,
[1433.78s -> 1436.74s]  the work on Go and StarCraft and Stratego.
[1436.74s -> 1439.94s]  But as I said, RL is not confined to games.
[1439.94s -> 1442.10s]  It's about trying to achieve a goal.
[1442.10s -> 1445.70s]  And so here, like on the bottom row,
[1445.70s -> 1447.86s]  there is AlphaDev and AlphaTensor,
[1447.86s -> 1450.46s]  which were used to discover new algorithms
[1450.86s -> 1453.54s]  and the bottom on the right is basically,
[1453.54s -> 1457.58s]  RL was used to try to control in a tokamak
[1457.58s -> 1459.06s]  the shape of the plasma
[1459.06s -> 1461.34s]  to get better nuclear fusion reactions.
[1461.34s -> 1464.90s]  So you can see, it is a very general setup.
[1466.58s -> 1470.94s]  Now, I had myself the luck to spend my days
[1470.94s -> 1473.94s]  on AlphaGo and descendants.
[1473.94s -> 1477.22s]  And this is what I call the zero series.
[1477.66s -> 1481.18s]  And especially from AlphaGo Zero,
[1481.18s -> 1484.22s]  we showed that we could train an agent to master domains.
[1484.22s -> 1486.70s]  And when I say master, I actually mean,
[1486.70s -> 1489.02s]  being the world champion hands down
[1489.02s -> 1491.34s]  at the time when those things came out.
[1493.22s -> 1494.38s]  And we learned to do that
[1494.38s -> 1496.90s]  without any human input information,
[1496.90s -> 1499.74s]  or you basically learned everything from scratch.
[1499.74s -> 1502.74s]  And so I wanna talk to you about the zero philosophy,
[1502.74s -> 1506.82s]  which I think is a great philosophy for research.
[1508.06s -> 1509.82s]  And it's about this.
[1509.82s -> 1512.66s]  If an agent can learn to master an environment tabula rasa,
[1512.66s -> 1514.34s]  so without any information,
[1514.34s -> 1516.02s]  then basically you've demonstrably shown
[1516.02s -> 1518.58s]  that you have a system that is able to discover
[1518.58s -> 1520.34s]  and learn new knowledge by itself.
[1521.98s -> 1525.30s]  This also means that this algorithm is general
[1525.30s -> 1527.26s]  and should apply to other domains.
[1528.94s -> 1530.82s]  And I think the ability
[1530.82s -> 1533.62s]  of being able to discover new knowledge,
[1533.62s -> 1536.62s]  this is particularly important to advance science
[1536.62s -> 1538.42s]  because a big part of doing science
[1538.42s -> 1541.34s]  is to just explore areas that are not yet well understood
[1541.34s -> 1542.18s]  and advance knowledge.
[1542.18s -> 1543.78s]  And you need to accumulate knowledge,
[1543.78s -> 1546.78s]  continuously generate and discover new interesting source.
[1546.78s -> 1549.62s]  And that's a huge part of doing science.
[1551.30s -> 1553.26s]  So, especially for research,
[1553.26s -> 1557.14s]  I think this is a great way to demonstrate
[1557.14s -> 1560.02s]  that you can discover new things.
[1560.02s -> 1561.82s]  I would say that especially now
[1561.82s -> 1563.58s]  with the big success of LLMs,
[1565.02s -> 1566.58s]  maybe a more pragmatic point of view,
[1567.50s -> 1571.74s]  is to be able to start at a certain knowledge base,
[1571.74s -> 1573.98s]  and then you want to know that your algorithm
[1573.98s -> 1576.54s]  is gonna continue to discover new things from there.
[1576.54s -> 1578.62s]  So you don't really need to start from zero.
[1578.62s -> 1580.98s]  What you really need is to have this algorithm
[1580.98s -> 1582.46s]  that can discover new things.
[1582.46s -> 1584.66s]  And we've demonstrated that you can do this
[1585.98s -> 1589.90s]  with this research and this alpha zero six.
[1589.90s -> 1592.54s]  So let me go into some kind of examples,
[1592.54s -> 1594.26s]  alpha goes zero and alpha zero.
[1594.26s -> 1595.74s]  As I said, they learn simply
[1595.78s -> 1597.50s]  by playing against themselves,
[1597.50s -> 1599.18s]  starting completely from random play.
[1599.18s -> 1601.22s]  And what we observed is that
[1602.54s -> 1605.38s]  they basically rediscovered human patterns,
[1605.38s -> 1607.74s]  accumulating thousands of years of human knowledge,
[1607.74s -> 1609.70s]  maybe in a matter of days.
[1609.70s -> 1612.30s]  But ultimately, some of those patterns
[1612.30s -> 1615.54s]  were kind of discarded in preference of new discoveries
[1615.54s -> 1617.38s]  that humans don't even know about,
[1617.38s -> 1618.94s]  they don't even know about.
[1618.94s -> 1621.74s]  So in Go, I believe that, you know,
[1621.74s -> 1626.30s]  Alpha Go basically starting a new move at move,
[1626.30s -> 1627.46s]  I would say move five,
[1629.54s -> 1631.86s]  a move that wasn't even played by humans.
[1631.86s -> 1634.62s]  And I've heard, if I'm not mistaken,
[1634.62s -> 1636.90s]  that Magnus Carlsen was saying that
[1636.90s -> 1639.50s]  because his coach was really good at using engines
[1639.50s -> 1641.86s]  and he got to use alpha zero like engines
[1641.86s -> 1643.18s]  when those came out,
[1643.18s -> 1645.70s]  he got a big advantage momentarily
[1645.70s -> 1647.42s]  over the other chess players,
[1647.42s -> 1650.42s]  because basically these things was playing very differently
[1650.42s -> 1652.26s]  and people were not used to it
[1652.26s -> 1655.42s]  and they didn't know how to handle those new situations.
[1655.42s -> 1658.06s]  But that's kind of Go and chess.
[1658.06s -> 1660.14s]  And actually, if you look at, for instance,
[1660.14s -> 1662.54s]  the figure I showed on the bottom right,
[1662.54s -> 1664.06s]  this is kind of,
[1664.06s -> 1666.90s]  this is trying to show the frequency of alpha zero
[1666.90s -> 1668.94s]  playing that particular pattern called,
[1668.94s -> 1670.62s]  for instance, the Karakhan defense.
[1670.62s -> 1672.26s]  And you can see that, you know,
[1672.26s -> 1675.10s]  it basically discovers it after two hours of training,
[1675.10s -> 1677.86s]  it plays it for about four hours quite consistently.
[1677.86s -> 1678.70s]  And then, you know,
[1678.74s -> 1680.78s]  this kind of pattern starts to disappear
[1680.78s -> 1682.62s]  from the games alpha zero is playing,
[1682.62s -> 1685.90s]  probably because has found kind of a way
[1685.90s -> 1689.74s]  that makes this pattern not very interesting anymore.
[1689.74s -> 1693.10s]  While on the bottom, the Queen's Gambit seems,
[1693.10s -> 1694.14s]  you know, the frequency at which
[1694.14s -> 1696.18s]  the Queen's Gambit is played
[1696.18s -> 1699.02s]  is just increasing over time, over training time.
[1701.02s -> 1704.26s]  All right, now let me talk about alpha tensor.
[1704.26s -> 1707.58s]  And so this was a work that was trying to find
[1707.58s -> 1710.90s]  what is the best way to multiply two matrices together.
[1710.90s -> 1712.82s]  And so maybe the naive algorithm
[1712.82s -> 1715.94s]  that you may have learned at school
[1715.94s -> 1717.90s]  requires kind of N cube operations
[1717.90s -> 1720.90s]  when you're trying to multiply two N by N matrices.
[1720.90s -> 1722.14s]  But for two by two matrices,
[1722.14s -> 1723.90s]  you need eight multiplications
[1723.90s -> 1727.26s]  or four by four multiplications,
[1727.26s -> 1730.14s]  you need 64 multiplications, that's the choice.
[1730.14s -> 1732.70s]  And so there was at some point a big discovery
[1732.70s -> 1734.58s]  from a person named Strassen
[1734.58s -> 1736.66s]  that found that for two by two matrices,
[1736.70s -> 1739.90s]  actually you could do it with only seven multiplications.
[1739.90s -> 1742.46s]  And he also showed that this was optimal,
[1742.46s -> 1744.26s]  you couldn't do better than that.
[1745.22s -> 1748.26s]  But as soon as we went to four by four matrices,
[1749.34s -> 1751.78s]  basically we knew nothing more
[1751.78s -> 1754.42s]  than applying Strassen algorithm twice.
[1754.42s -> 1756.26s]  And so, you know, still reduces
[1756.26s -> 1759.22s]  from 64 to 49 multiplications.
[1759.22s -> 1762.98s]  And so in alpha tensor, we try to discover
[1762.98s -> 1765.98s]  a new efficient and provably correct algorithm
[1766.02s -> 1769.06s]  for basically tasks that look like matrix multiplication.
[1770.06s -> 1773.30s]  And, you know, it actually did.
[1773.30s -> 1777.66s]  It found new things in actually many different sizes,
[1779.18s -> 1780.94s]  discovering like brand new knowledge
[1780.94s -> 1783.46s]  that human never known about.
[1783.46s -> 1786.10s]  And so, you know, if you had only trained
[1786.10s -> 1787.62s]  on what human had known,
[1787.62s -> 1790.46s]  and then you would have never discovered those.
[1790.46s -> 1792.42s]  As a big bonus as part of this work,
[1792.42s -> 1793.54s]  we showed that basically, you know,
[1793.54s -> 1796.22s]  we could basically, alpha zero could tackle
[1796.22s -> 1797.94s]  huge action spaces.
[1797.94s -> 1800.02s]  So as I wrote here, 10 to the power of 30,
[1800.02s -> 1802.74s]  but basically for practical purposes,
[1802.74s -> 1805.66s]  this is just infinite.
[1805.66s -> 1807.06s]  And so all those successes, you know,
[1807.06s -> 1810.14s]  go, chess, tensor, decomposition,
[1810.14s -> 1812.58s]  are all specific to a certain task.
[1812.58s -> 1814.66s]  You know, if you avoid a certain level,
[1814.66s -> 1816.30s]  but under the hood is basically
[1816.30s -> 1817.86s]  the same alpha zero algorithm
[1817.86s -> 1820.42s]  that is running and making those discoveries.
[1820.42s -> 1822.06s]  And so what we had in mind is like,
[1822.06s -> 1823.78s]  is there an environment that captures
[1823.78s -> 1825.70s]  a much broader class of tasks
[1825.70s -> 1827.46s]  that are relevant for science?
[1827.46s -> 1831.02s]  And you guessed it, I would say yes, it's mathematics.
[1831.86s -> 1833.58s]  But just before we get there,
[1833.58s -> 1837.70s]  the lessons we've learned from all this work.
[1837.70s -> 1839.70s]  So what made those systems superhuman?
[1840.98s -> 1842.82s]  I've listed up what I believe
[1842.82s -> 1845.86s]  are the four most important properties.
[1845.86s -> 1848.62s]  First is to scale up trial and error.
[1848.62s -> 1850.92s]  And basically to discover new knowledge,
[1850.92s -> 1852.40s]  you need to try something new
[1852.40s -> 1855.68s]  and be able to learn based on the feedback you receive.
[1855.68s -> 1857.48s]  And if your feedback is not grounded,
[1857.48s -> 1860.44s]  you know, if you invent your own feedback,
[1860.44s -> 1863.28s]  then it's really, I think it's really hard
[1863.28s -> 1865.40s]  once you venture out of what's known.
[1865.40s -> 1867.48s]  And so you really need a way to measure
[1867.48s -> 1869.48s]  if you are achieving your goals or not,
[1869.48s -> 1874.48s]  where you are in your solution, et cetera, et cetera.
[1875.00s -> 1878.48s]  And also alpha zero uses a lot of search.
[1878.48s -> 1880.16s]  And I believe, it's my belief
[1880.16s -> 1881.56s]  that search can really accelerate
[1881.56s -> 1883.84s]  the discovery of new paths and skills.
[1883.84s -> 1886.00s]  And it's because most of the time,
[1886.00s -> 1889.00s]  when you sample a different action,
[1889.00s -> 1894.00s]  not the action that has the highest probability,
[1894.12s -> 1895.08s]  if you try that action,
[1895.08s -> 1897.42s]  most of the time it's gonna be bad,
[1897.42s -> 1899.58s]  but sometimes it's gonna be good.
[1899.58s -> 1901.32s]  And you need to discover
[1901.32s -> 1903.00s]  why that action might be good or not.
[1903.00s -> 1904.28s]  And so you need to follow up
[1904.28s -> 1907.64s]  on kind of playing well after you play that action.
[1907.64s -> 1910.16s]  And I think, you know, for instance,
[1910.16s -> 1914.44s]  AlphaGo and Move37 is a good example of that.
[1914.44s -> 1916.92s]  It had a very low probability of being played,
[1916.92s -> 1918.52s]  but it was still explored in the search.
[1918.52s -> 1919.74s]  And then it was discovered
[1919.74s -> 1922.44s]  that it was a really good move.
[1922.44s -> 1925.04s]  And so it became, you know, it was played
[1925.04s -> 1927.72s]  and it was described as being very creative.
[1929.08s -> 1932.60s]  The final one is you need to have a good curriculum
[1932.60s -> 1934.76s]  to take you all the way to superhema.
[1934.76s -> 1937.84s]  And in the AlphaZero case, in Go and Chance and Shoggy,
[1937.84s -> 1940.42s]  we relied on the fact that it was a two-player game.
[1940.42s -> 1942.08s]  And so you could do self-play.
[1942.08s -> 1944.12s]  You basically always have the right opponent
[1944.12s -> 1945.28s]  to play against.
[1945.28s -> 1946.88s]  And that was really important.
[1947.92s -> 1950.58s]  Okay, so now let me try to connect those two.
[1952.04s -> 1953.48s]  Scaled up, try, and all right.
[1953.48s -> 1956.60s]  We have, you know, Lean is the environment
[1956.60s -> 1959.42s]  to explore mathematics, completely in silico.
[1959.42s -> 1962.20s]  And the algorithm is gonna come from AlphaZero.
[1962.20s -> 1964.64s]  And again, Lean is gonna give us
[1965.40s -> 1966.24s]  kind of this grounded feedback signal.
[1966.24s -> 1967.46s]  He's gonna tell us, you know,
[1967.46s -> 1968.64s]  what have we proved so far?
[1968.64s -> 1970.52s]  What's left to be proved?
[1970.52s -> 1972.72s]  And he's gonna tell us when we are done,
[1973.76s -> 1976.04s]  when we get a correct proof.
[1976.04s -> 1980.60s]  For search, we can rely on AlphaZero's search algorithm.
[1980.60s -> 1982.48s]  It works pretty well.
[1982.48s -> 1985.00s]  And maybe kind of the question mark is,
[1985.00s -> 1986.80s]  what do we do with the curriculum?
[1987.92s -> 1989.50s]  And in some sense, you know,
[1989.50s -> 1991.20s]  where are the levels coming from?
[1992.20s -> 1995.04s]  But so you understood kind of the idea of AlphaProof
[1995.04s -> 1997.50s]  is to use AlphaZero for mathematics
[1997.50s -> 2000.92s]  and in particular, formal mathematics.
[2000.92s -> 2003.88s]  So, you know, let me make kind of a napkin
[2003.88s -> 2007.60s]  kind of master plan for AlphaProof.
[2007.60s -> 2010.12s]  So Lean gives us a way to scale up trial and error
[2010.12s -> 2011.80s]  with an environment to explore mathematics
[2011.80s -> 2013.20s]  completely in silico
[2013.20s -> 2016.16s]  and a perfect feedback signal for proving.
[2016.16s -> 2018.12s]  And then now, if we extrapolate from the work
[2018.12s -> 2019.82s]  we've done in the other domains,
[2020.18s -> 2023.62s]  I conclude we can therefore reach super human intelligence
[2023.62s -> 2024.68s]  and discover new truths,
[2024.68s -> 2027.04s]  just like in go chess tensor decomposition,
[2027.04s -> 2029.82s]  provided we can generate high enough quality
[2029.82s -> 2032.12s]  and quantity of problems and RL works.
[2033.34s -> 2036.50s]  So I'm very bullish on RL works.
[2036.50s -> 2039.66s]  And so, as I said, the kind of the question that remains
[2039.66s -> 2043.42s]  is how can we generate high enough quality
[2043.42s -> 2044.30s]  and quantity of problems?
[2044.30s -> 2045.94s]  Where do the problems come from?
[2046.94s -> 2051.30s]  And so I think one of the answer has to be that, you know,
[2051.30s -> 2054.06s]  the problems partly comes from the humans.
[2055.38s -> 2057.34s]  And maybe the most important reason why
[2057.34s -> 2060.18s]  is that it's because we want to help human mathematics.
[2062.30s -> 2065.12s]  We could try to create alien mathematics,
[2066.26s -> 2068.54s]  but really what we would like
[2068.54s -> 2070.02s]  is to help human mathematics
[2070.02s -> 2073.34s]  and kind of contribute in that community.
[2073.34s -> 2075.68s]  And maybe a more pragmatic reason
[2076.28s -> 2077.12s]  is that humans have already come up
[2077.12s -> 2080.04s]  with a lot of problems that are quite interesting.
[2081.56s -> 2083.56s]  But of course, we shouldn't throw away the second part
[2083.56s -> 2085.82s]  where half a proof was defined a problem.
[2086.78s -> 2090.56s]  And so I think there is maybe two ways to slice this.
[2090.56s -> 2093.24s]  One is half a proof defines a problem
[2093.24s -> 2094.68s]  around human mathematics.
[2094.68s -> 2096.68s]  And we've done a little bit of that already
[2096.68s -> 2098.84s]  with auto-formization and the same area.
[2098.84s -> 2099.76s]  So it's perfectly fine
[2099.76s -> 2102.20s]  if you don't understand what I mean here.
[2102.20s -> 2104.46s]  I'll try to explain those a bit later.
[2105.42s -> 2107.20s]  But the other part is basically
[2107.20s -> 2109.30s]  really augmenting human mathematics.
[2109.30s -> 2111.14s]  So, you know, ideally half a proof
[2111.14s -> 2114.02s]  would be able to discover new areas of mathematics,
[2114.02s -> 2115.42s]  build new theories, et cetera,
[2115.42s -> 2116.98s]  that humans find interesting.
[2119.70s -> 2122.50s]  Yeah, so these are maybe where the problems can come from.
[2122.50s -> 2126.22s]  So before we move on, I want to take a step back
[2126.22s -> 2129.54s]  because if we're thinking about AI for mathematics,
[2129.54s -> 2132.34s]  you know, there is potentially a choice.
[2132.34s -> 2135.42s]  There is a choice of doing it in natural language
[2135.42s -> 2136.26s]  or informal language.
[2136.26s -> 2139.32s]  So I've been arguing a long time now for the formal part,
[2140.26s -> 2145.02s]  but, you know, actually most effort in AI at the moment
[2145.02s -> 2146.72s]  is on informal mathematics
[2146.72s -> 2149.42s]  or doing basically mathematics in natural language.
[2149.42s -> 2153.38s]  And the reason why is because basically
[2153.38s -> 2157.34s]  all of the data of mathematics, you know,
[2157.34s -> 2159.18s]  by many orders of magnitude
[2159.18s -> 2162.10s]  is written in natural language, of course.
[2162.70s -> 2164.64s]  So you have this mass lib library that is growing
[2164.64s -> 2167.10s]  and contains quite a lot of mass,
[2167.10s -> 2170.38s]  but it's still a few orders of magnitude smaller
[2170.38s -> 2173.14s]  than the amount of mathematics you can find
[2174.18s -> 2178.14s]  in textbooks, on archive, on published papers, et cetera,
[2178.14s -> 2180.48s]  that are all written using natural language.
[2182.14s -> 2183.84s]  So that's kind of the trade-off you get
[2183.84s -> 2185.14s]  when you use formal mathematics
[2185.14s -> 2188.50s]  is you really have a very small amount of data.
[2188.50s -> 2189.50s]  That's when, again, you know,
[2189.82s -> 2192.76s]  you bump into the question of how do you get the problems
[2192.76s -> 2195.02s]  where the problems don't come?
[2195.02s -> 2199.42s]  However, I guess it's my opinion that the, you know,
[2200.42s -> 2202.22s]  because natural language mathematics,
[2202.22s -> 2205.34s]  at least the reasoning part is not verifiable.
[2206.34s -> 2207.74s]  I believe that this is an issue
[2207.74s -> 2209.60s]  if you're reading your goal is to build a system
[2209.60s -> 2213.02s]  that can go beyond human knowledge and create new knowledge.
[2214.10s -> 2215.40s]  You can, of course, you know,
[2215.40s -> 2218.58s]  improve the reliability of those systems.
[2218.58s -> 2221.50s]  But the problem in mathematics is really that any,
[2221.50s -> 2224.18s]  if you build on any false statement,
[2224.18s -> 2228.34s]  then, you know, it destroys everything you do afterwards.
[2228.34s -> 2230.54s]  So you need, you really need like a hundred percent
[2230.54s -> 2234.86s]  reliability as a property of mathematics.
[2236.30s -> 2238.90s]  And that's why I think that, you know,
[2238.90s -> 2241.20s]  verifiability is very important.
[2241.20s -> 2244.50s]  And basically lean is already superhuman at verification.
[2245.50s -> 2248.98s]  And I say all of this for mathematics,
[2248.98s -> 2250.48s]  but actually I believe this is true
[2250.48s -> 2252.44s]  for many important, impactful applications
[2252.44s -> 2254.54s]  that we cannot solve yet.
[2254.54s -> 2258.36s]  And the scientific problems in particular.
[2258.36s -> 2261.04s]  So the, you know, in the domains we cannot solve,
[2261.04s -> 2262.74s]  there is almost by construction,
[2262.74s -> 2264.60s]  there isn't much data available
[2264.60s -> 2267.30s]  because the domain is relatively poorly understood.
[2267.30s -> 2269.96s]  And then, you know, it could even be a completely new
[2269.96s -> 2272.10s]  domain that has just been discovered.
[2272.10s -> 2274.22s]  And you could also have applications
[2274.82s -> 2275.66s]  that are somewhat niche.
[2275.66s -> 2278.78s]  And maybe if you think about what AGI would do,
[2278.78s -> 2281.58s]  you could argue that maybe mathematics is a bit niche,
[2281.58s -> 2283.90s]  but it's still extremely impactful.
[2283.90s -> 2285.46s]  And it's conceivable that, you know,
[2285.46s -> 2288.14s]  generalization from all the other data
[2288.14s -> 2289.98s]  would be somewhat limited.
[2289.98s -> 2291.82s]  And so if we are data scarce,
[2291.82s -> 2294.30s]  we really need a process that can continuously generate
[2294.30s -> 2295.98s]  and discover new interesting facts
[2295.98s -> 2298.38s]  that are not yet represented in the data.
[2298.38s -> 2301.18s]  And you need a way to ground those new facts
[2301.18s -> 2303.70s]  so that you can check that you are basically
[2303.74s -> 2307.50s]  constantly adding new truths and building on top of new truths.
[2307.50s -> 2310.42s]  And so this is really our most foundational bet
[2310.42s -> 2313.34s]  is that perfect verification we need in the long run
[2313.34s -> 2315.84s]  be the most important property for mathematics.
[2317.26s -> 2318.66s]  Let me recap.
[2318.66s -> 2320.38s]  RL meets formal math.
[2320.38s -> 2322.82s]  So we have a proven recipe to be a legend
[2322.82s -> 2324.78s]  that can discover new knowledge by themselves
[2324.78s -> 2327.98s]  and reach superhuman intelligence in certain domains.
[2327.98s -> 2330.66s]  And the key ingredients I have highlighted
[2330.66s -> 2333.42s]  are scandal trial and error and grounded feedback.
[2333.98s -> 2335.78s]  And basically formal math provides us
[2335.78s -> 2337.26s]  with those key ingredients.
[2338.14s -> 2342.26s]  So that's, these are the big ideas that, you know,
[2342.26s -> 2344.02s]  behind that approach.
[2344.02s -> 2348.78s]  So now let me talk to you a little bit about the IMO
[2348.78s -> 2351.94s]  and our participation in 2024.
[2353.26s -> 2355.16s]  So first, what's the IMO?
[2355.16s -> 2356.38s]  And the IMO stands for
[2356.38s -> 2358.48s]  the International Mathematics Olympia.
[2358.48s -> 2360.54s]  It's basically the world championship level event
[2360.54s -> 2363.32s]  in mathematics that brings together the best young minds
[2364.16s -> 2365.44s]  from around the world.
[2365.44s -> 2369.32s]  So every country brings their top six high school students
[2369.32s -> 2371.12s]  and they try to solve six problems
[2371.12s -> 2373.78s]  in two sessions of four and a half hours.
[2373.78s -> 2376.34s]  So basically in total, they have like nine hours
[2376.34s -> 2377.60s]  to solve six problems.
[2378.92s -> 2381.42s]  In these six problems, they are two easy problems,
[2381.42s -> 2384.52s]  two medium problems and two hard problems.
[2384.52s -> 2387.28s]  So let me first try to illustrate what it means
[2387.28s -> 2390.68s]  to take the top six high school students for the USA.
[2390.68s -> 2394.42s]  So let's say there is about 17 million high school students.
[2396.22s -> 2399.48s]  It's not like kind of every kid loves math
[2399.48s -> 2401.60s]  and is gonna do a mass competition,
[2402.60s -> 2404.88s]  might instead play video games
[2404.88s -> 2406.64s]  and maybe in the future,
[2406.64s -> 2409.08s]  actually build a mass library of the future.
[2409.08s -> 2411.68s]  But anyways, at the moment, there's about 60,000 of them
[2411.68s -> 2415.00s]  that participate in these AMC competitions.
[2416.00s -> 2421.00s]  If they do well and they are still kind of into this,
[2421.20s -> 2424.98s]  they might go and participate in this AI competition.
[2426.04s -> 2427.92s]  And so for those of you who have been following
[2427.92s -> 2430.92s]  kind of the benchmarks that are used in AI,
[2430.92s -> 2434.08s]  and you may know about this mass benchmark
[2434.08s -> 2437.92s]  that basically contained a lot of AMC and AI problems.
[2437.92s -> 2441.24s]  And so the reason why is that basically AI need,
[2441.24s -> 2444.32s]  what you need to do in the AI need is to,
[2444.32s -> 2448.12s]  the solution is always an integers between one and 1,000.
[2448.12s -> 2452.28s]  And so it makes it easy to check whether kind of your,
[2452.28s -> 2454.48s]  your system has outputted the correct answer
[2454.48s -> 2456.46s]  or not like the correct number.
[2456.46s -> 2457.84s]  You can check that very easily
[2457.84s -> 2461.68s]  and kind of use that as a grounded feedback.
[2461.68s -> 2462.96s]  The problem is it doesn't tell you
[2462.96s -> 2466.76s]  whether your reasoning to get there is correct or not.
[2466.76s -> 2470.42s]  Now, like if you need to guess 177,
[2471.32s -> 2474.16s]  then I think, like in a team believe
[2474.96s -> 2478.04s]  that it's more likely that you have correct reasoning
[2478.04s -> 2479.76s]  when you come up with the right answer.
[2479.76s -> 2481.36s]  And so, you know, that might still help
[2481.36s -> 2482.20s]  to improve the reason.
[2482.20s -> 2483.52s]  And basically what's happening,
[2483.52s -> 2486.92s]  it's improving the reasoning of those models.
[2486.92s -> 2489.48s]  But anyway, when you do really well at the AME,
[2489.48s -> 2492.60s]  you may participate in the USA Mass Olympiad.
[2493.72s -> 2495.20s]  There's only 250 now.
[2496.08s -> 2497.62s]  Then if you are in the top there,
[2497.62s -> 2499.84s]  you may go to the IMO camp.
[2499.84s -> 2503.44s]  And from there only the top six people are selected.
[2503.60s -> 2505.06s]  So you can see there is like basically six
[2505.06s -> 2509.00s]  or seven orders of selection.
[2509.00s -> 2513.82s]  And told by my colleague who did the IMO for the UK
[2513.82s -> 2517.00s]  that, you know, they study for a thousand of hours,
[2517.00s -> 2519.04s]  maybe an average of three hours for three years.
[2519.04s -> 2522.40s]  So really intense training for this competition.
[2523.96s -> 2527.36s]  Now, let me, let me share a little bit about the problems
[2527.36s -> 2530.86s]  and I'll start with this quote from Poisson law.
[2530.86s -> 2532.92s]  The Poisson law is not anyone.
[2533.36s -> 2534.20s]  He is not new to the IMO.
[2534.20s -> 2537.24s]  He was actually the coach, the national coach for USA
[2537.24s -> 2539.12s]  for the last 10 years.
[2539.12s -> 2542.00s]  And he said, I tried this year's problems
[2542.00s -> 2543.48s]  while at the IMO.
[2543.48s -> 2545.56s]  It took me hours.
[2545.56s -> 2547.20s]  And in context about the IMO,
[2547.20s -> 2550.44s]  problems are specifically selected to be non-standard.
[2550.44s -> 2551.64s]  But during the IMO itself,
[2551.64s -> 2553.44s]  national coaches need to pick the problems
[2553.44s -> 2554.76s]  to appear on the exam.
[2554.76s -> 2557.92s]  One of the most important tasks of that group
[2557.92s -> 2559.68s]  is to avoid problems similar to problems
[2559.68s -> 2561.00s]  that appeared anywhere before.
[2561.00s -> 2564.10s]  And so they go through great lengths to ensure this.
[2566.00s -> 2569.84s]  And so that's why basically we really wanted to, you know,
[2569.84s -> 2572.72s]  we picked the IMO as a goal for us
[2572.72s -> 2574.68s]  is because we thought there was a really a test
[2574.68s -> 2577.54s]  of reasoning and not of knowledge.
[2578.50s -> 2581.00s]  And, you know, as Poisson law says,
[2581.00s -> 2583.72s]  even for specialists, even for the national coach
[2583.72s -> 2585.76s]  of the USA that has been doing this for 10 years,
[2585.76s -> 2588.64s]  it will take him hours to solve those problems.
[2589.12s -> 2593.64s]  The final property of participating at the 2024 IMO
[2593.64s -> 2596.56s]  was that those problems were not leaked, right?
[2596.56s -> 2601.56s]  And so you really needed to kind of be able
[2601.56s -> 2603.32s]  to reason to solve those problems.
[2604.26s -> 2605.72s]  As I said, you know, they are like easy,
[2605.72s -> 2607.52s]  medium and hard problems.
[2607.52s -> 2609.68s]  Easy is, you know, in fact,
[2609.68s -> 2611.72s]  those problems are not easy at all
[2611.72s -> 2616.34s]  because only half of the participants
[2616.34s -> 2618.00s]  solve two or more problems.
[2618.66s -> 2620.56s]  And so, you know, like you take the top six
[2620.56s -> 2623.44s]  from each country and half of those won't be able
[2623.44s -> 2624.80s]  to solve the easy problem.
[2626.42s -> 2628.32s]  All right, so now let's go really to, you know,
[2628.32s -> 2633.08s]  this IMO 2024 and I'd like to relive
[2633.08s -> 2635.04s]  a part of it with you.
[2635.04s -> 2638.24s]  So, you know, we had contacted the IMO organization
[2638.24s -> 2641.60s]  and we had agreed with them
[2641.60s -> 2643.24s]  that they will send us the problems.
[2643.24s -> 2645.24s]  We have contacted some judges,
[2645.24s -> 2647.96s]  also part of that competition.
[2648.80s -> 2650.20s]  So they could grade our solutions
[2653.06s -> 2654.82s]  and we try to participate.
[2654.82s -> 2656.64s]  So I just want to make sure, you know,
[2656.64s -> 2660.72s]  like kind of, I clarify exactly
[2660.72s -> 2663.56s]  what our IMO participation was.
[2663.56s -> 2665.60s]  And I think a good way to think about it
[2665.60s -> 2667.16s]  is more like an Apollo program.
[2667.16s -> 2669.28s]  And so what we really wanted to know is
[2669.28s -> 2671.12s]  can we actually reach the moon?
[2671.12s -> 2675.46s]  Can our system solve the 2024 IMO problem at all?
[2675.46s -> 2678.50s]  You know, given the computer that we can give us
[2678.50s -> 2682.62s]  at that time and enough time for doing a reason.
[2684.50s -> 2685.70s]  So it was really about this.
[2685.70s -> 2687.98s]  Like, can we, you know, can we build a system
[2687.98s -> 2690.38s]  that can solve those problems at all?
[2690.38s -> 2692.62s]  And I think at the time when we got started,
[2694.46s -> 2695.30s]  it was believed that, you know,
[2695.30s -> 2697.26s]  this was like five or 10 years away.
[2698.74s -> 2701.18s]  Okay, so now let me take you to, you know,
[2701.18s -> 2703.74s]  seven months before January, 2024.
[2703.74s -> 2704.98s]  We are seven months away
[2705.54s -> 2707.26s]  and we need to make a decision about geometry.
[2707.26s -> 2708.98s]  So why geometry?
[2708.98s -> 2710.18s]  It's because, you know,
[2710.18s -> 2712.94s]  mathlib is very sparse in 2D Euclidean geometry.
[2712.94s -> 2715.14s]  It doesn't have much of what you need to know
[2715.14s -> 2716.90s]  about Euclidean geometry,
[2716.90s -> 2719.72s]  mostly because, you know, researchers don't really care
[2719.72s -> 2722.98s]  about 2D Euclidean geometry that is useful for the IMO.
[2722.98s -> 2727.02s]  And so we're a bit unsure what to do.
[2727.02s -> 2728.56s]  We are debating whether maybe, you know,
[2728.56s -> 2730.66s]  we actually don't do geometry at all,
[2730.66s -> 2734.42s]  or we try to fill in that part in mathlib
[2734.70s -> 2736.82s]  so that our agent has the basic knowledge
[2736.82s -> 2739.16s]  that is expected every students to have.
[2740.14s -> 2742.78s]  And so we are debating this when, you know,
[2742.78s -> 2746.26s]  miraculously another group within Google DeepMind
[2746.26s -> 2748.14s]  turned out to have been working exactly
[2748.14s -> 2749.82s]  the IMO geometry problem.
[2749.82s -> 2752.58s]  And they already have a system that is very, very strong.
[2752.58s -> 2754.94s]  You know, their system was already approaching
[2754.94s -> 2759.76s]  gold level metal performance on the geometry problems.
[2759.76s -> 2762.06s]  And we learned about that because they publish
[2762.06s -> 2765.50s]  the paper in Nature in early January, 2024.
[2765.50s -> 2769.26s]  So miracle, but we decided to join forces
[2769.26s -> 2770.62s]  with them for July.
[2770.62s -> 2772.50s]  And so our geometry will tackle geometry
[2772.50s -> 2774.66s]  and our proof will tackle algebra,
[2774.66s -> 2776.42s]  and the theory and combinatorics.
[2778.34s -> 2780.74s]  A couple of months later, we are debating another thing.
[2780.74s -> 2784.22s]  So we've been training our proof on proving theorems,
[2784.22s -> 2786.96s]  but not all questions are in the form prove that.
[2788.06s -> 2790.42s]  Actually, some require an answer to be determined,
[2790.42s -> 2793.62s]  like find all x such that, you know,
[2793.62s -> 2796.26s]  and this is not just a, you know,
[2796.26s -> 2798.66s]  an integer like the AME, it could be, you know,
[2798.66s -> 2802.34s]  it's the set of, you know, like these numbers
[2802.34s -> 2804.74s]  that have this property, et cetera.
[2804.74s -> 2807.58s]  And so this actually not obvious how to do well
[2807.58s -> 2810.46s]  in formal language, because you have like these kind of
[2810.46s -> 2812.70s]  tautological case where, you know, he says,
[2812.70s -> 2815.38s]  well, what are all the x such that property P holds
[2815.38s -> 2819.46s]  and says, well, it's the set of numbers where P holds.
[2819.70s -> 2822.06s]  Then you have like a really easy time to prove that,
[2822.06s -> 2825.30s]  but that's definitely not what kind of the judges
[2825.30s -> 2826.48s]  were after.
[2828.06s -> 2829.86s]  And so actually, you know, the answer itself
[2829.86s -> 2833.10s]  is a bit of a subjective thing.
[2833.10s -> 2835.50s]  And we debate, you know, what we should do.
[2835.50s -> 2837.16s]  Either we do what's, you know,
[2837.16s -> 2839.18s]  was known outside as the EV mode.
[2840.42s -> 2842.90s]  We basically feed in the correct answer to the,
[2842.90s -> 2846.32s]  to have a proof and we ask have a proof to prove it.
[2846.32s -> 2849.02s]  Or we go with hard mode where we try to determine
[2849.54s -> 2851.38s]  the answer, we have something that then is the answer
[2851.38s -> 2852.82s]  and we prove it.
[2852.82s -> 2855.92s]  And I personally don't really like easy and hard
[2855.92s -> 2859.26s]  kind of keywords here because, you know,
[2859.26s -> 2862.86s]  it's fairly well known that kind of the beat
[2862.86s -> 2866.02s]  that is the hardest for the MO is the proving part
[2866.02s -> 2868.18s]  is not really to guess the answer.
[2869.94s -> 2872.38s]  But anyway, so we are debating,
[2872.38s -> 2875.58s]  we are not sure exactly what to do,
[2875.58s -> 2877.76s]  but you know, after thinking about it through,
[2877.76s -> 2880.60s]  it's pretty clear that we need to do hard mode.
[2880.60s -> 2883.92s]  And what's cool is that actually kind of getting the answer
[2883.92s -> 2887.24s]  is exactly what kind of Gemini is being trained on.
[2887.24s -> 2889.56s]  And so we decided that we will generate candidates
[2889.56s -> 2892.10s]  and serve with Gemini and we'll attempt to prove
[2892.10s -> 2895.84s]  and disprove all possible candidates without a proof.
[2895.84s -> 2897.92s]  That's how we're going to participate.
[2897.92s -> 2900.36s]  Now, the final thing we are debating about
[2900.36s -> 2904.68s]  is about kind of the input that the system should receive.
[2904.68s -> 2906.72s]  When you receive the natural language description
[2906.72s -> 2909.80s]  of the problem, and you know, for instance, in English,
[2909.80s -> 2912.16s]  actually, you know, if you participate at the MO,
[2912.16s -> 2913.36s]  you can choose the language
[2913.36s -> 2915.08s]  in which you will receive the problems.
[2915.08s -> 2917.64s]  You know, not everyone has to read it in English.
[2917.64s -> 2920.14s]  You can have it in Russian or in Chinese or in Japanese,
[2920.14s -> 2922.08s]  you can choose your own language.
[2922.08s -> 2924.44s]  Unfortunately, you know, they don't propose lean
[2924.44s -> 2926.00s]  as a language.
[2926.00s -> 2928.12s]  And so we are debating whether, you know,
[2928.12s -> 2931.12s]  we will send kind of English to the system
[2931.12s -> 2935.40s]  or we'll ask our experts internally
[2935.40s -> 2937.28s]  to manually formalize the problem
[2937.28s -> 2940.16s]  and send that as an input to the system.
[2941.32s -> 2944.64s]  And so we debate this a little bit with our judges
[2944.64s -> 2948.92s]  and, you know, very quickly, we have the same opinion
[2948.92s -> 2950.92s]  because what we really care about is, you know,
[2950.92s -> 2954.56s]  the mathematical reasoning and problem solving at the MO.
[2954.56s -> 2957.20s]  And that's what we work, therefore,
[2957.20s -> 2959.48s]  not really about our capabilities
[2959.48s -> 2963.58s]  or actually translating the problem in formal language.
[2964.50s -> 2968.16s]  And so we decide that it's okay for what we want to achieve
[2968.16s -> 2971.30s]  that we will give the formalized problems as input.
[2971.30s -> 2973.22s]  And so we ask our lean experts
[2973.22s -> 2976.54s]  to manually formalize the problems for the system.
[2976.54s -> 2978.78s]  So if I recap, you know,
[2978.78s -> 2981.26s]  our protocol is gonna look something like this.
[2981.26s -> 2984.06s]  So we are gonna officially receive the problems at 1 p.m.
[2985.62s -> 2987.90s]  The lean experts will first manually
[2987.90s -> 2989.90s]  formalize the problems.
[2989.90s -> 2991.42s]  And then that's where kind of the,
[2991.42s -> 2993.22s]  after that, the AI comes in.
[2994.22s -> 2997.72s]  We'll generate hundreds of answer candidates with Gemini.
[2997.72s -> 2999.86s]  Alpha proof will try to filter
[2999.86s -> 3002.02s]  all the easily disprovable ones.
[3002.02s -> 3004.58s]  And then we are gonna run test time error.
[3004.58s -> 3006.40s]  Again, I haven't told you what is test time error,
[3006.40s -> 3007.74s]  but I'm gonna get there.
[3009.22s -> 3012.38s]  Okay, day one, we are there.
[3013.38s -> 3016.66s]  Tuesday at 1 p.m. we receive the problem.
[3016.66s -> 3018.94s]  P1 is an algebra, P2 is a number theory,
[3018.94s -> 3020.90s]  and P3 is a combinatorics.
[3020.90s -> 3023.94s]  And P1, P2 actually requires an answer to be guessed.
[3026.02s -> 3028.78s]  And so, you know, we ask our experts
[3028.78s -> 3030.68s]  to manually formalize the problem,
[3030.68s -> 3033.58s]  then Gemini to guess answer candidates.
[3033.58s -> 3034.62s]  And then we get alpha proof
[3034.62s -> 3037.58s]  to try to disprove all those answer candidates.
[3037.58s -> 3042.18s]  And so we have already very good signs
[3042.18s -> 3045.34s]  is that alpha proof is able to disprove 99% basically
[3045.34s -> 3048.94s]  of guesses for those two problems in a matter of minutes.
[3048.94s -> 3050.14s]  So this is a really good sign
[3050.22s -> 3051.50s]  because we observed in the past
[3051.50s -> 3054.16s]  that when alpha proof understands the problem,
[3054.16s -> 3056.40s]  it's gonna be able to disprove a lot of kind
[3056.40s -> 3057.62s]  of false index.
[3057.62s -> 3060.70s]  But for problems that it doesn't understand at all,
[3060.70s -> 3063.94s]  it wouldn't be able to disprove much at all.
[3063.94s -> 3067.38s]  And so that's really good sign for day one.
[3067.38s -> 3069.54s]  On day two, right, we receive again
[3069.54s -> 3071.46s]  at 1 p.m. the problems.
[3071.46s -> 3074.34s]  We knew actually that P4 and P5 had to be geometry
[3074.34s -> 3076.06s]  and combinatorics because there is a rule
[3076.06s -> 3079.02s]  that the easy and medium problems, you know,
[3079.02s -> 3082.46s]  all categories have to appear in easy and medium problems.
[3082.46s -> 3084.30s]  And, but we are pretty happy to see
[3084.30s -> 3088.46s]  that P6 was the algebra functional equation
[3088.46s -> 3090.88s]  because we knew that also alpha proof
[3090.88s -> 3095.30s]  was particularly strong in functional equation problems.
[3095.30s -> 3098.06s]  That said, P6 is in the hard category
[3099.14s -> 3101.06s]  and it's the hardest problem.
[3101.06s -> 3103.32s]  So even if it's, you know, kind of a good category
[3103.32s -> 3107.68s]  for alpha proof, nothing is guaranteed at all.
[3108.68s -> 3111.48s]  Turns out that, you know, alpha geometry is able
[3111.48s -> 3114.08s]  to solve the geometry problem, I believe in seconds.
[3115.16s -> 3116.76s]  So really, really powerful.
[3118.22s -> 3121.24s]  You get there, so basically, you know, they came in,
[3121.24s -> 3123.12s]  they run that thing for a few seconds
[3123.12s -> 3125.38s]  and then it was done for them.
[3126.32s -> 3128.86s]  The P5 combinatorics was actually very tricky
[3128.86s -> 3131.48s]  and our human experts failed to formalize
[3131.48s -> 3132.96s]  that problem on the day.
[3132.96s -> 3135.92s]  And the P6 algebra, you know,
[3135.96s -> 3138.32s]  it's also requires sensors to be determined
[3138.32s -> 3140.52s]  and Gemini generated a bunch of answers.
[3140.52s -> 3143.16s]  And when we try to disprove it,
[3143.16s -> 3145.42s]  it's gonna only disprove 7% of guesses.
[3146.36s -> 3149.72s]  So it doesn't have zero traction on the problem,
[3149.72s -> 3152.88s]  but it doesn't have probably as good an understanding
[3152.88s -> 3154.24s]  as on P1 and P2.
[3156.44s -> 3160.02s]  All right, so, you know, day three, Thursday,
[3161.16s -> 3163.36s]  alpha proof hasn't proved anything yet,
[3163.36s -> 3165.68s]  but we had a way to track progress.
[3166.12s -> 3169.80s]  And so we observed on the day one problems that P1,
[3169.80s -> 3171.36s]  you know, alpha proof had actually solved
[3171.36s -> 3173.04s]  half of the problem.
[3173.04s -> 3175.06s]  And so actually in that proof,
[3175.06s -> 3176.24s]  there are at least, you know,
[3176.24s -> 3178.70s]  two ways to split that problem in two.
[3178.70s -> 3181.68s]  You'd, you know, case on the fact that something
[3181.68s -> 3186.38s]  is either even or odd or that it's rational or irrational.
[3186.38s -> 3188.86s]  And so alpha proof had kinda, you know,
[3188.86s -> 3191.80s]  come up with these ideas of splitting this problem
[3191.80s -> 3193.06s]  in two by itself.
[3193.06s -> 3195.16s]  And for instance, in the even odd case,
[3195.52s -> 3197.56s]  it had proved the even case
[3197.56s -> 3199.80s]  and hadn't finished proving the odd case.
[3200.64s -> 3203.56s]  It wasn't able to prove the odd case yet.
[3203.56s -> 3205.96s]  In P2, when we look at it,
[3206.92s -> 3209.72s]  alpha proof had arguably solved the whole problem.
[3209.72s -> 3210.94s]  And so I don't know if you know,
[3210.94s -> 3212.62s]  but in the IMO there is, you know,
[3212.62s -> 3216.02s]  there is a whole section where after the competition is,
[3216.02s -> 3218.72s]  you know, after the students have submitted their answers,
[3218.72s -> 3222.56s]  then the team captains can argue with the,
[3222.56s -> 3224.92s]  with the judging committee
[3225.52s -> 3226.36s]  that actually, you know,
[3226.36s -> 3228.08s]  like the student has done this much
[3228.08s -> 3230.36s]  and so therefore they deserve that much points.
[3230.36s -> 3233.40s]  And so, you know, if we had followed that process,
[3233.40s -> 3236.18s]  we believe that with whatever alpha proof had,
[3236.18s -> 3239.00s]  we may be able to argue for the whole problem.
[3239.00s -> 3241.76s]  So let's say, you know, six out of seven points there.
[3243.12s -> 3245.72s]  And then on P3, the combinatorics problem,
[3245.72s -> 3247.94s]  alpha proof hadn't made any progress.
[3249.08s -> 3250.80s]  On the day two problem, as I said, you know,
[3250.80s -> 3254.16s]  alpha geometry had already solved in seconds
[3254.44s -> 3256.60s]  the problem, so seven out of seven points.
[3256.60s -> 3261.16s]  The P5 combinatorics, we had made no progress.
[3261.16s -> 3264.50s]  I think by that time we had a formalization of it.
[3264.50s -> 3268.44s]  But what was really cool is that actually on P6,
[3268.44s -> 3270.16s]  alpha proof had made progress
[3270.16s -> 3272.88s]  and had proved a particular case
[3272.88s -> 3275.52s]  that we now know is worth two out of seven points.
[3276.58s -> 3278.28s]  So probably on Thursday,
[3278.28s -> 3279.88s]  it had actually made quite a bit of progress
[3279.88s -> 3281.48s]  on all the problems.
[3281.60s -> 3284.48s]  We may have already have like about 18 points.
[3286.16s -> 3289.12s]  Now, Friday, we received this email.
[3290.60s -> 3293.84s]  That's the name of my colleague, Rishi,
[3293.84s -> 3296.60s]  that was actually sent by alpha proof.
[3296.60s -> 3298.40s]  And alpha proof is sending an email
[3298.40s -> 3301.04s]  each time it solves one of the IMO problems.
[3301.04s -> 3304.16s]  It says, I have discovered a truly marvelous proof
[3304.16s -> 3308.58s]  of IMO 2024 P6, but this email is too short to contain.
[3309.26s -> 3312.12s]  And of course, it's a joke.
[3312.12s -> 3315.26s]  And the whole proof is there in the email.
[3315.26s -> 3317.58s]  And so when we see that, we are extremely excited
[3317.58s -> 3321.70s]  because, you know, we know we've kind of half P2,
[3321.70s -> 3325.34s]  alpha geometry has P4, we're making progress on P1.
[3325.34s -> 3327.70s]  And we have, it seems like it looks like
[3327.70s -> 3331.46s]  we have solved the hardest problem, P6.
[3331.46s -> 3333.28s]  So, you know, extremely excited.
[3333.28s -> 3336.94s]  I have run to my colleague's desk, Oliver Nash,
[3336.98s -> 3338.66s]  who is one of our Lean experts.
[3338.66s -> 3342.94s]  And so we basically copy paste that proof to official Lean.
[3342.94s -> 3346.86s]  And it sinks a little bit and says, yeah, all good, proved.
[3346.86s -> 3349.26s]  And so I run back to my desk.
[3349.26s -> 3350.62s]  I don't think too much.
[3350.62s -> 3354.60s]  I just forward that message to Demis and Sergey.
[3354.60s -> 3357.40s]  So he said, Demis has a business, Sergey Bary.
[3360.94s -> 3365.52s]  And I want you to share that for two things.
[3365.52s -> 3369.24s]  One, I think it shows you the trust I have in Lean.
[3369.24s -> 3370.28s]  If I didn't trust Lean,
[3370.28s -> 3373.68s]  I would probably wouldn't have sent that email
[3373.68s -> 3376.36s]  kind of seconds after kind of getting confirmation
[3376.36s -> 3378.52s]  from Lean that this was all good.
[3378.52s -> 3381.02s]  And the second thing I thought a little bit after that
[3381.02s -> 3383.68s]  is like, ooh, was this like a stupid email?
[3383.68s -> 3385.28s]  But I think actually we are in a company
[3385.28s -> 3387.02s]  where it's cool to do this.
[3387.02s -> 3388.76s]  And Demis and Sergey were following
[3388.76s -> 3391.56s]  a little bit what was happening during this competition.
[3391.56s -> 3396.56s]  And a little bit later in the day, actually,
[3397.20s -> 3400.72s]  we receive another mail and it's a full proof of P2.
[3400.72s -> 3402.88s]  So now there is nothing to argue about,
[3404.56s -> 3406.66s]  perfect proof solved by alpha proof.
[3406.66s -> 3408.20s]  And a little bit later in the day,
[3408.20s -> 3409.28s]  actually on that same day,
[3409.28s -> 3411.72s]  we also received the full proof of P1.
[3413.20s -> 3416.52s]  So in conclusion, final results,
[3416.52s -> 3419.60s]  P1, P2 and P6 were fully solved by alpha proof
[3419.60s -> 3422.48s]  and P4 was fully solved by alpha geometry.
[3422.48s -> 3425.48s]  And we actually tried to keep running over the weekend
[3425.48s -> 3427.32s]  in the hope of getting one point on P3
[3427.32s -> 3430.48s]  because actually the agent made some progress on it,
[3430.48s -> 3433.52s]  but it turned out that it was not enough
[3433.52s -> 3434.62s]  for a partial point.
[3435.96s -> 3438.96s]  So we reached the score of a silver medalist
[3438.96s -> 3441.56s]  and missed the gold threshold by one point.
[3443.80s -> 3446.08s]  Of course, I just wanna reiterate that, you know,
[3446.08s -> 3449.00s]  we definitely use more time and more compute
[3449.28s -> 3451.60s]  than was available to the human participant.
[3453.86s -> 3456.08s]  I wanna come back a little bit on this P6 problem
[3456.08s -> 3458.04s]  because it turned out that it was arguably
[3458.04s -> 3459.24s]  one of the hardest problems
[3459.24s -> 3461.36s]  in the last years at the IMO.
[3461.36s -> 3465.64s]  Only five out of 609 of the participant
[3465.64s -> 3467.24s]  solved this problem fully.
[3467.24s -> 3469.88s]  And you know, this is the top six people
[3469.88s -> 3471.16s]  of every country who comes in
[3471.16s -> 3475.00s]  and only five in total solved the problem fully.
[3475.00s -> 3478.56s]  And Sir Timothy Gowers, who is field medalist
[3479.12s -> 3480.92s]  and IMO gold medalist as well,
[3480.92s -> 3482.86s]  he said that he actually spent a couple of hours
[3482.86s -> 3484.88s]  on this problem and did not solve it.
[3484.88s -> 3486.94s]  And I find the fact that the program can come up
[3486.94s -> 3488.88s]  with a somewhat complicated construction like this
[3488.88s -> 3490.24s]  very impressive.
[3490.24s -> 3492.36s]  So in no ways am I saying that, you know,
[3492.36s -> 3497.26s]  we are close to Tim Gowers kind of math abilities
[3497.26s -> 3498.92s]  and he may not have spent, you know,
[3498.92s -> 3500.60s]  these two hours really trying to solve this
[3500.60s -> 3502.56s]  and these things, you know, you need to, you know,
[3502.56s -> 3503.60s]  these are kind of,
[3503.60s -> 3506.42s]  you need to practice to solve these things.
[3506.42s -> 3507.88s]  And so in no ways am I saying
[3508.16s -> 3510.32s]  that we are close to fields medal level,
[3510.32s -> 3512.92s]  but I think this, we all interpreted this
[3512.92s -> 3515.56s]  as a really, really encouraging sign for the future
[3515.56s -> 3517.96s]  to be able to do something really non-trivial.
[3520.32s -> 3522.16s]  Let me conclude this IMO part
[3522.16s -> 3524.48s]  with some of the challenges we've observed.
[3524.48s -> 3527.04s]  The first is the gaps in math.
[3527.04s -> 3530.36s]  And so the most blatant one is geometry actually.
[3530.36s -> 3533.40s]  If our geometry was not there, you know,
[3533.40s -> 3536.56s]  like kind of, you may have required us
[3536.56s -> 3539.28s]  to actually completely write that part in mathly
[3539.28s -> 3544.28s]  so that we could do the geometry problems
[3544.60s -> 3546.20s]  or we may have decided not even
[3546.20s -> 3548.12s]  to try the geometry problem.
[3548.12s -> 3551.92s]  And also I want to say that even in P1,
[3551.92s -> 3554.40s]  we observed that actually there were some gaps in mathly
[3554.40s -> 3556.84s]  that we suffered from.
[3556.84s -> 3558.76s]  And so in one of the proofs where you have to prove
[3558.76s -> 3561.76s]  that it's, you know, either rational or irrational,
[3561.76s -> 3563.88s]  AFA proof had proved the rational part
[3563.88s -> 3566.52s]  and was struggling on the irrational part.
[3566.52s -> 3571.08s]  And this is not definitive, but in fact,
[3571.08s -> 3573.20s]  we observed that, you know, like the natural way
[3573.20s -> 3575.84s]  to prove this would be to use a certain theorem
[3575.84s -> 3578.04s]  that turned out to not exist in mathly.
[3579.16s -> 3580.88s]  So, you know, this is not an excuse.
[3580.88s -> 3583.36s]  AFA proof could have conjectured that theorem,
[3583.36s -> 3586.12s]  proved it in line and use it to finish that proof.
[3587.08s -> 3588.04s]  But it turned out that, you know,
[3588.04s -> 3592.12s]  that problem was more difficult than it needed to be.
[3593.04s -> 3596.00s]  Now Combinatrix problems were also very difficult.
[3596.84s -> 3599.60s]  You know, just P5 was extremely hard to formalize.
[3599.60s -> 3601.52s]  A lot of, you know, the necessary API
[3601.52s -> 3604.48s]  to represent those problems about, you know,
[3604.48s -> 3607.68s]  actually in the ironically about agents, environments
[3607.68s -> 3611.28s]  and strategies is not present in mathly
[3611.28s -> 3614.64s]  and that was made that problem very hard to formalize.
[3614.64s -> 3617.32s]  And the reality is that even when we had formalized it,
[3617.32s -> 3620.40s]  we did not manage to make progress on P3 and P5.
[3620.40s -> 3625.40s]  So Combinatrix were difficult for AFAQ and of course,
[3626.12s -> 3627.52s]  we use many orders of magnitude,
[3627.52s -> 3630.18s]  more compute than human contestants.
[3630.18s -> 3633.00s]  In some way, you know, I think, you know,
[3633.00s -> 3634.84s]  we actually successfully landed on the moon.
[3634.84s -> 3637.24s]  I think that is the actually the important part
[3638.12s -> 3641.64s]  and we will want to be more efficient for the next trip.
[3643.28s -> 3647.56s]  All right, so now let me go into, you know, the methods
[3648.52s -> 3649.52s]  and before we get there,
[3649.56s -> 3652.04s]  I want to remind you that, you know,
[3652.04s -> 3653.74s]  when we're talking about the methods,
[3653.74s -> 3656.70s]  there were two important aspects we need to think about.
[3656.70s -> 3658.96s]  One is where do the problems come from
[3658.96s -> 3662.80s]  and how are we going to have an agent
[3662.80s -> 3664.92s]  that learns to prove those problems?
[3666.28s -> 3668.72s]  So we trained several models.
[3668.72s -> 3670.76s]  One model is a formalizer model.
[3670.76s -> 3672.28s]  So it takes as an input,
[3672.28s -> 3674.44s]  a problem describing natural language
[3674.44s -> 3676.78s]  in output a gene formalization.
[3676.78s -> 3678.52s]  So this is based on the idea that, you know,
[3678.52s -> 3681.40s]  we are going to lean onto human defined problems
[3682.52s -> 3684.68s]  instead of inventing our own problems.
[3684.68s -> 3687.00s]  And we are going to try to translate them in lean
[3687.00s -> 3688.92s]  because, you know, they all exist in natural language,
[3688.92s -> 3690.40s]  but they don't exist in lean.
[3690.40s -> 3693.44s]  So we're going to try to do that, you know,
[3693.44s -> 3696.56s]  trying to do that with humans, don't scale at all.
[3696.56s -> 3697.40s]  It takes a long time.
[3697.40s -> 3700.44s]  And so instead we want to try to train a model
[3700.44s -> 3702.04s]  that can do this automatically.
[3703.40s -> 3705.92s]  Okay, that's the formalizer model.
[3705.92s -> 3707.48s]  And now we also have a proven model.
[3707.48s -> 3709.72s]  So the architecture of a proven model
[3709.72s -> 3711.96s]  is that it takes this kind of, you know,
[3711.96s -> 3714.20s]  if you remember in my demonstration before
[3714.20s -> 3717.16s]  this kind of yellow box, the tactic state,
[3717.16s -> 3718.92s]  the lean state that tells you, you know,
[3718.92s -> 3719.76s]  what do you know now?
[3719.76s -> 3721.04s]  What do you need to prove?
[3721.04s -> 3722.80s]  This is the thing we're going to send
[3722.80s -> 3724.88s]  as an input to the proven model.
[3724.88s -> 3727.68s]  And then the proven model is going to sample
[3727.68s -> 3730.38s]  and continuation for the proof.
[3730.38s -> 3733.00s]  You know, the same things you might consider
[3733.84s -> 3737.48s]  now potentially part of the proof and then alternatives.
[3737.48s -> 3739.16s]  And then you take this proven model
[3739.16s -> 3741.28s]  and you put it in the alpha zero search.
[3741.28s -> 3743.64s]  So you're going to search over actions,
[3743.64s -> 3745.72s]  which are basically one line in the proof.
[3745.72s -> 3747.50s]  Each time you add one line in the proof,
[3747.50s -> 3749.84s]  you send that to lean and lean tells you,
[3749.84s -> 3751.76s]  oh, what do I know now?
[3751.76s -> 3753.20s]  What do I need to prove now?
[3755.64s -> 3758.52s]  And that's basically how the search works.
[3758.52s -> 3760.36s]  If you know a little bit about alpha zero
[3760.36s -> 3761.82s]  and MCTS like search,
[3761.82s -> 3763.22s]  they try to exploit, you know,
[3763.22s -> 3764.84s]  what kind of the networks tell you,
[3764.84s -> 3766.22s]  the neural network tell you,
[3766.22s -> 3769.38s]  they exploit the high prior and high value path,
[3769.38s -> 3773.12s]  but it has a mechanism also to explore low visited path.
[3773.12s -> 3773.96s]  For instance, you know,
[3773.96s -> 3776.90s]  that's how move 37 and half ago is discovered.
[3776.90s -> 3777.86s]  And it's because, you know,
[3777.86s -> 3779.58s]  it might have a low prior,
[3779.58s -> 3783.12s]  might not be considered the right move by the network,
[3783.12s -> 3786.26s]  but they were still going to try to visit it
[3786.26s -> 3788.86s]  in case it might be an interesting one.
[3788.86s -> 3791.22s]  Okay, so now let me put this thing together
[3791.50s -> 3792.54s]  in the pipeline.
[3792.54s -> 3794.32s]  Step one, we do auto-formalization.
[3794.32s -> 3795.90s]  So we train a formalization model
[3795.90s -> 3798.30s]  and auto-formalize human creative problems.
[3800.48s -> 3802.10s]  So this is really the answer to the question
[3802.10s -> 3806.00s]  where the problems come from around human mathematics.
[3806.00s -> 3808.98s]  And here, I want to highlight a few advantages
[3808.98s -> 3811.86s]  of using kind of formalization of the problem.
[3811.86s -> 3813.42s]  First is that when, you know,
[3813.42s -> 3814.98s]  like when you send the problem to lean
[3814.98s -> 3817.70s]  and lean says, this is the valid syntax.
[3818.62s -> 3821.18s]  It means that, you know, your question makes sense.
[3823.18s -> 3825.06s]  It's not kind of something that doesn't make sense.
[3825.06s -> 3826.58s]  So it's either true or false.
[3826.58s -> 3828.02s]  You know, you're generating something
[3828.02s -> 3829.62s]  that is either true or false.
[3830.76s -> 3832.82s]  Okay, the problems might not be interesting.
[3832.82s -> 3836.38s]  You know, they might be truly true or truly false,
[3836.38s -> 3839.10s]  but they are either, you know, true or false.
[3839.10s -> 3840.50s]  And so one way to see this
[3840.50s -> 3843.38s]  is you have the human problem in the middle
[3843.38s -> 3846.46s]  and you try to generate kind of lean kind of variance
[3847.14s -> 3851.54s]  and, you know, if your model is really good,
[3851.54s -> 3853.46s]  then it would do the perfect transition.
[3853.46s -> 3855.34s]  But maybe also it will generate problems
[3855.34s -> 3858.62s]  kind of that are kind of look like that problem, right?
[3860.42s -> 3862.58s]  And so when we have this formalization model,
[3862.58s -> 3865.14s]  the way we use it is that we were able to get
[3865.14s -> 3866.86s]  about a million informal problems.
[3866.86s -> 3870.38s]  And there's a lot of problems to train for the,
[3870.38s -> 3871.68s]  about a million of them.
[3871.68s -> 3874.02s]  We pass them through our formalized model
[3874.06s -> 3877.26s]  and we generate roughly for every, you know,
[3877.26s -> 3878.54s]  probably describe the natural language,
[3878.54s -> 3883.50s]  we generated roughly a hundred possible translation for it.
[3883.50s -> 3886.54s]  And now we've just created a huge database
[3886.54s -> 3887.96s]  of formal problems.
[3889.72s -> 3891.58s]  Okay, now we need to have,
[3891.58s -> 3894.70s]  we need to train the prover to be good at proving.
[3894.70s -> 3896.46s]  And so that's where, you know,
[3896.46s -> 3900.70s]  you don't really want to be completely
[3900.70s -> 3902.10s]  in the alpha zero philosophy
[3902.10s -> 3903.94s]  where you really start from scratch
[3904.82s -> 3905.94s]  and here you, you know, you,
[3905.94s -> 3907.02s]  you want to be pragmatic
[3907.02s -> 3909.38s]  and you want to start with something that is already,
[3909.38s -> 3911.48s]  you know, can do a few things.
[3911.48s -> 3913.30s]  So you take a pre-trained model,
[3913.30s -> 3916.38s]  I know a case it was pre-trained on a lot of code.
[3916.38s -> 3919.82s]  And then kind of the next step is you're going to
[3919.82s -> 3922.38s]  train the prover model supervised on mathlib.
[3922.38s -> 3925.08s]  The mathlib has about like a hundred thousand definitions,
[3925.08s -> 3929.02s]  200,000 theorems, 200,000 lines of proof.
[3929.02s -> 3931.54s]  And basically you are really going to do supervised.
[3931.54s -> 3933.06s]  So, you know, you are going to place yourself
[3933.10s -> 3934.58s]  inside of a human proof.
[3934.58s -> 3936.76s]  You're going to look at what is the mean state there.
[3936.76s -> 3938.90s]  Then you are going to look at what the human said,
[3938.90s -> 3940.82s]  what's the next step in the proof.
[3940.82s -> 3943.44s]  And you're going to train the model towards it.
[3943.44s -> 3944.50s]  So in some sense,
[3944.50s -> 3947.54s]  very similar to regular kind of pre-training,
[3947.54s -> 3949.90s]  but you get much more information from me.
[3949.90s -> 3951.82s]  And so when you've done that, you know,
[3951.82s -> 3955.42s]  your prover model has learned a really good prior over,
[3955.42s -> 3957.70s]  you know, the actions to take.
[3957.70s -> 3961.00s]  Unfortunately, you know, mathlib is about, you know,
[3961.00s -> 3961.84s]  I don't know,
[3961.84s -> 3963.84s]  especially if you want to do the IMO,
[3963.84s -> 3968.16s]  mathlib doesn't contain much IMO like kind of problems.
[3968.16s -> 3970.48s]  And it's all about research math.
[3970.48s -> 3972.36s]  And so if you take that prover model
[3972.36s -> 3974.76s]  and you try to apply it on the IMO,
[3976.92s -> 3980.68s]  you get around 0% solve rate on the IMO.
[3980.68s -> 3984.60s]  I'm not going to say how 0% I actually, I'm not sure,
[3984.60s -> 3987.36s]  but really close to 0%.
[3987.36s -> 3989.40s]  And so that's why you start to have to discover
[3989.44s -> 3990.84s]  new knowledge about, you know,
[3990.84s -> 3993.40s]  how to solve those IMO problems.
[3993.40s -> 3996.64s]  And so that's where you enter the next stage
[3996.64s -> 3999.20s]  of the alpha zero reinforcement learning stage.
[3999.20s -> 4002.66s]  And so you are going to train the prover model by ARRA.
[4002.66s -> 4004.42s]  And the recipe is very simple.
[4004.42s -> 4007.90s]  It's ARRA recipe, you know, for each form of problem,
[4007.90s -> 4009.88s]  you are going to generate experience
[4009.88s -> 4012.40s]  or either proving or disproving that problem
[4012.40s -> 4013.92s]  by searching over lean steps.
[4014.98s -> 4018.28s]  Once you've come up with what you think is a proof,
[4018.28s -> 4021.16s]  you're going to use lean to verify the proofs.
[4021.16s -> 4023.00s]  Lean is going to say, this is not a proof,
[4023.00s -> 4023.84s]  this is a proof.
[4023.84s -> 4025.28s]  And for the things that are proof,
[4025.28s -> 4028.52s]  you're going to reinforce your proven network
[4028.52s -> 4030.20s]  on those proofs.
[4030.20s -> 4031.72s]  And so what happens is that, you know,
[4031.72s -> 4034.36s]  at the very beginning, the prover model
[4036.00s -> 4039.08s]  is able to prove the very, the easiest problems,
[4039.08s -> 4040.84s]  really, really easy problems.
[4040.84s -> 4042.98s]  It's going to learn from that experience
[4042.98s -> 4045.32s]  and kind of climb the next one on the ladder
[4045.32s -> 4047.56s]  that leads all the way to IMO strengths, right?
[4047.72s -> 4049.40s]  It's going to be able to then, you know,
[4049.40s -> 4051.64s]  solve a few more problems a bit harder.
[4051.64s -> 4052.76s]  And by learning from those,
[4052.76s -> 4055.24s]  it will go and get better and better and better
[4055.24s -> 4057.20s]  and all the way to the IMO.
[4057.20s -> 4061.12s]  And so from this stage, we decided to do a kind of,
[4061.12s -> 4063.44s]  really the alpha zero kind of pipeline,
[4064.66s -> 4066.88s]  because what we wanted to see is that, you know,
[4066.88s -> 4068.72s]  like, of course we want to solve the IMO,
[4068.72s -> 4071.00s]  but we want to do it in a way that we think,
[4071.00s -> 4073.84s]  we know we generalize way beyond the IMO.
[4073.84s -> 4078.76s]  So, you know, like our real goal is to do real mathematics,
[4078.76s -> 4080.68s]  if not to stop at the IMO.
[4080.68s -> 4085.68s]  So we want to have a method that is not tuned to the IMO.
[4085.84s -> 4088.52s]  We want to have a method that can discover new knowledge.
[4088.52s -> 4090.68s]  And so all these proving strategies for the IMO,
[4090.68s -> 4093.84s]  this is completely discovered by the model by itself.
[4095.50s -> 4096.92s]  So that's step three.
[4096.92s -> 4099.92s]  And then final step is this thing called test time error.
[4100.92s -> 4103.16s]  And so to give you a bit of context,
[4103.32s -> 4104.72s]  as I alluded before, you know,
[4104.72s -> 4108.04s]  even for a portion of the USA coach or Tim Gowers
[4108.04s -> 4109.48s]  or Terry Tao, you know,
[4109.48s -> 4111.68s]  the problems at the IMO are not obvious.
[4111.68s -> 4114.92s]  So even if you have like fields, metal level,
[4114.92s -> 4117.20s]  neural network of mathematic nutrition,
[4117.20s -> 4120.72s]  you cannot really solve those IMO problems instantly.
[4120.72s -> 4123.64s]  Or then even for them, it takes hours to solve.
[4123.64s -> 4125.34s]  And why is that?
[4125.34s -> 4126.18s]  It's because, you know,
[4126.18s -> 4128.48s]  those problems have been designed this way
[4128.48s -> 4130.56s]  and you really want to learn something
[4130.56s -> 4131.76s]  about those problems.
[4131.76s -> 4134.28s]  So it's almost like kind of a real scientific
[4134.28s -> 4135.12s]  kind of experiment.
[4135.12s -> 4137.34s]  You know, you first need to think of experiments.
[4137.34s -> 4139.26s]  In this case, it's like, what could be true?
[4139.26s -> 4140.68s]  What could be false?
[4140.68s -> 4142.86s]  And then you need to run the experiments, you know,
[4142.86s -> 4145.06s]  try to prove or disprove those problems
[4145.06s -> 4147.00s]  and learn from this experience,
[4147.00s -> 4148.84s]  learn about that particular problem.
[4149.84s -> 4154.68s]  And the way we did it is that, you know,
[4154.68s -> 4157.44s]  alpha proof is gonna generate variants of the problem
[4157.44s -> 4158.62s]  we are caring about.
[4158.62s -> 4159.46s]  So in this case, you know,
[4159.46s -> 4161.20s]  we have these six problems at the IMO.
[4161.64s -> 4163.38s]  These are the problems we care about.
[4163.38s -> 4165.64s]  Can we generate variants of those problems?
[4165.64s -> 4168.88s]  And basically then run RL exactly as in the previous step
[4168.88s -> 4170.42s]  to try to discover knowledge,
[4171.30s -> 4173.64s]  especially about those problems.
[4173.64s -> 4178.32s]  So I'm trying to illustrate this in some hyperplane
[4178.32s -> 4180.68s]  where you can have the agent and the problem.
[4183.04s -> 4185.16s]  So yeah, try to imagine this a little bit.
[4185.16s -> 4186.84s]  So on the bottom left,
[4186.84s -> 4189.74s]  basically you have a bubble of problems
[4189.78s -> 4193.04s]  that the agent can already solve easily.
[4193.04s -> 4195.10s]  And so it's a generous checkpoint.
[4195.10s -> 4196.90s]  Every like little black dots here
[4196.90s -> 4198.30s]  represent kind of a problem
[4198.30s -> 4201.22s]  that the agent already knows how to do.
[4201.22s -> 4204.12s]  And then, you know, like very far on the top right,
[4204.12s -> 4206.50s]  you have this very hard problem that you wanna solve.
[4206.50s -> 4209.98s]  So imagine maybe it's the P6 problem at the IMO.
[4209.98s -> 4213.54s]  And so as I described, the first thing we did
[4213.54s -> 4215.14s]  was we generated variants on that thing.
[4215.14s -> 4217.54s]  So we basically explored kind of that problem
[4217.54s -> 4219.62s]  into many, many variants.
[4220.46s -> 4222.22s]  And if you are doing this well,
[4222.22s -> 4225.74s]  you may hope that it will generate one variant
[4225.74s -> 4228.50s]  that is actually already solvable by the agent.
[4229.66s -> 4231.22s]  And so you solve that one
[4231.22s -> 4234.58s]  and you learn from that proof exactly like before.
[4234.58s -> 4237.34s]  And you learn a little bit about that problem.
[4237.34s -> 4238.82s]  And then you repeat that process.
[4238.82s -> 4240.32s]  You repeat that process.
[4240.32s -> 4241.82s]  You learn about that problem
[4241.82s -> 4246.46s]  until you reach that problem and you can solve it.
[4246.46s -> 4249.42s]  And so that's how actually P1 and P2 and P6
[4250.30s -> 4252.14s]  require the same amount to be solved.
[4253.82s -> 4256.46s]  Okay, so I want to finish this section
[4256.46s -> 4258.66s]  with some challenges for AlphaProve.
[4260.98s -> 4264.86s]  So we inherit some challenges from formal mathematics.
[4264.86s -> 4265.68s]  As I said, you know,
[4265.68s -> 4268.50s]  most of human data is in natural language,
[4268.50s -> 4270.34s]  but we started to have a good handle
[4270.34s -> 4272.62s]  on how to translate that human data
[4272.62s -> 4274.30s]  so that we could learn about it.
[4275.26s -> 4277.90s]  So it's still a big challenge,
[4277.98s -> 4279.82s]  especially as we move forward.
[4279.82s -> 4281.78s]  But maybe we have some handling.
[4282.78s -> 4285.38s]  Maybe something that is a bit harder
[4285.38s -> 4288.38s]  is that we cannot easily learn and work on areas
[4288.38s -> 4290.38s]  that are not supported by massive.
[4290.38s -> 4291.52s]  And so for instance, you know,
[4291.52s -> 4293.12s]  like in the geometry case,
[4293.12s -> 4295.38s]  where you have nothing about it, right?
[4295.38s -> 4297.54s]  You can't even express kind of, you know,
[4297.54s -> 4300.98s]  the geometry problems that humans have come up with.
[4300.98s -> 4304.30s]  And so that's kind of a real challenge to think about.
[4304.94s -> 4308.62s]  And so this could be solved if, you know,
[4308.62s -> 4311.70s]  you start to understand how to do creative building
[4311.70s -> 4313.96s]  of new objects and theories.
[4313.96s -> 4316.86s]  You understand interestingness and beauty in mathematics.
[4318.30s -> 4323.30s]  But this is definitely a really cool challenge
[4323.56s -> 4324.40s]  for future work.
[4325.86s -> 4326.70s]  Cool.
[4326.70s -> 4330.22s]  So, you know, what's next for AlphaProve?
[4330.22s -> 4332.94s]  As I said, you know, like, you know,
[4332.94s -> 4336.62s]  from the very beginning, actually, of the AlphaProve project,
[4336.62s -> 4340.50s]  the idea was to try to see if we could, you know,
[4340.50s -> 4342.18s]  learn about mathematics
[4342.18s -> 4344.48s]  and potentially go beyond human mathematics.
[4345.92s -> 4349.50s]  The IMO was a really, really cool milestone
[4349.50s -> 4352.46s]  that I think was extremely correlating with that goal.
[4352.46s -> 4355.30s]  But that's not where we want to stop.
[4355.30s -> 4358.60s]  So obviously what we want to do is we want to broaden
[4358.60s -> 4360.10s]  to the entire mathematical landscape.
[4360.26s -> 4363.22s]  We don't want to be stuck at high school mathematics.
[4363.22s -> 4366.26s]  And we want in the future to be able to contribute
[4366.26s -> 4368.34s]  to the frontiers of research maths.
[4368.34s -> 4370.94s]  And in going back to, you know,
[4370.94s -> 4375.94s]  to the understanding that the Greek got 2,500 years ago
[4376.30s -> 4378.18s]  of the importance of proof.
[4378.18s -> 4381.90s]  And it would be fantastic if we could democratize
[4381.90s -> 4383.54s]  kind of this idea of the proof
[4383.54s -> 4385.34s]  and share that with every thinker
[4386.22s -> 4388.50s]  from an elite mathematician
[4388.50s -> 4391.22s]  to, you know, any other person.
[4391.22s -> 4393.58s]  And so can we have AlphaProve as a useful tool
[4393.58s -> 4394.64s]  for every thinker?
[4396.06s -> 4398.74s]  I want to conclude saying that, you know,
[4398.74s -> 4402.22s]  the big ideas, they're really important,
[4402.22s -> 4404.18s]  but equally important, if not more,
[4404.18s -> 4406.50s]  is the people you are doing this with
[4406.50s -> 4409.48s]  and the mindset that everyone has on the team.
[4410.70s -> 4412.94s]  I was pleasantly surprised to hear that, you know,
[4412.94s -> 4415.74s]  in mathematics as well, you need to be really optimistic.
[4416.74s -> 4419.02s]  Otherwise you will never be able to prove anything.
[4419.02s -> 4422.18s]  I think this is also true in AI and machine learning.
[4422.18s -> 4425.26s]  You need to be absolutely kind of optimistic
[4425.26s -> 4427.98s]  to be able to achieve kind of absurd goals.
[4427.98s -> 4430.42s]  And really, you know, individually,
[4430.42s -> 4432.14s]  this was almost impossible,
[4432.14s -> 4434.78s]  but, you know, we had this belief that together
[4434.78s -> 4436.10s]  it was impossible to fail.
[4436.98s -> 4439.42s]  So these were the team members
[4439.42s -> 4441.94s]  that contributed to this amazing project.
[4442.94s -> 4445.86s]  And yeah, let me conclude that
[4445.86s -> 4447.62s]  and thank you for listening.
