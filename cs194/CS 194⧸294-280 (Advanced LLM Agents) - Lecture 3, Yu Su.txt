# Detected language: en (p=1.00)

[0.00s -> 3.04s]  I'm super excited here today to talk about a topic
[3.04s -> 6.08s]  that is really close to my heart, language agents.
[6.08s -> 9.80s]  I'm very excited about this topic in the past two years
[9.80s -> 12.16s]  or so, like it's really fascinating
[12.16s -> 15.12s]  and I've devoted most of my time thinking about this.
[15.12s -> 17.92s]  So I have to share some thoughts here today.
[17.92s -> 19.40s]  Whether you like it or not,
[19.40s -> 21.68s]  well, since you are in this lecture,
[21.68s -> 23.16s]  so most likely you like it,
[23.16s -> 28.16s]  but you probably have heard about the term agents a lot.
[28.64s -> 31.20s]  Many people are super enthusiastic about it,
[31.20s -> 34.76s]  like Bill Gates said that it will bring about
[34.76s -> 39.04s]  the biggest revolution since essentially GUI.
[39.04s -> 42.12s]  Then Andrew Ng said that the agenda workflows
[42.12s -> 44.56s]  will drive massive air progress.
[44.56s -> 47.12s]  And then Sam Osman said that 2025
[47.12s -> 50.32s]  is the year when agents will work.
[50.32s -> 54.20s]  However, there is definitely also a lot of voice
[54.20s -> 56.04s]  from the other side of the aisle.
[57.04s -> 60.60s]  Like many people think that the current LLMs
[60.60s -> 63.80s]  are just thin wrappers around LLM,
[63.80s -> 65.88s]  agents are just thin wrappers around LLM.
[65.88s -> 69.20s]  So there is nothing fundamental here.
[69.20s -> 72.64s]  Well, others think other aggressive LLMs
[72.64s -> 75.32s]  can never truly reason or plan.
[76.92s -> 81.92s]  And then AutoGBT was one of the early prototypes
[82.12s -> 83.60s]  of these agent systems,
[83.64s -> 86.04s]  but it didn't, it was like very popular.
[86.04s -> 91.04s]  So it's the fastest growing repository in GitHub history
[91.56s -> 93.28s]  in terms of the number of stores.
[93.28s -> 95.48s]  But it didn't take long for people to realize
[95.48s -> 97.92s]  that it was, at least at the time,
[97.92s -> 99.72s]  mostly more of a prototype.
[99.72s -> 101.84s]  And there was a lot of limitations
[101.84s -> 104.16s]  far from a production system.
[105.12s -> 107.80s]  If we take one step back and think about,
[107.80s -> 109.60s]  hey, what is agents?
[109.60s -> 112.20s]  It's really not a new thing, right?
[112.20s -> 115.68s]  It's one of the very first topic
[115.68s -> 118.80s]  we cover in AI 101, agents,
[118.80s -> 123.40s]  and especially using these classic schematic illustration
[123.40s -> 125.08s]  from Russo and Novick.
[125.08s -> 129.04s]  And it has been part of the core pursuits of AI
[129.04s -> 130.52s]  since the very beginning.
[130.52s -> 132.28s]  So why all of a sudden it became
[132.28s -> 134.12s]  so much more popular again?
[136.04s -> 140.04s]  So let's try to define these modern agents.
[140.04s -> 141.64s]  Many people hold this view
[141.68s -> 144.36s]  that, hey, you have a damaged model.
[144.36s -> 147.24s]  It has a text-in, text-out interface.
[147.24s -> 150.32s]  So the kind of things that it can do is quite limited.
[150.32s -> 152.56s]  It's still a lot, but it's limited
[152.56s -> 155.56s]  because it's not connected to these external environments.
[155.56s -> 159.16s]  And then once you connect that to an external environment,
[159.16s -> 161.68s]  it can perceive information from the environment
[161.68s -> 164.88s]  and it can exert impact on the environment.
[164.88s -> 166.32s]  Then it becomes an agent.
[168.92s -> 171.00s]  That seems reasonable,
[171.00s -> 172.12s]  but at the same time,
[172.12s -> 176.60s]  it also seems a bit oversimplified and incomplete.
[176.60s -> 179.80s]  We often hear things like, hey, self-reflection,
[179.80s -> 184.80s]  that an agent or LIM looks at its own reasoning process
[185.72s -> 187.04s]  and then decide to do,
[187.04s -> 189.32s]  autonomously decide to do something else.
[189.32s -> 192.12s]  Or you can do multi-agent simulation
[192.12s -> 193.96s]  or a lot of other things
[193.96s -> 197.40s]  that don't necessarily involve an external environment,
[197.48s -> 201.48s]  or at least that's not part of the core defining traits.
[203.64s -> 207.84s]  Fundamentally, I think there are two main competing views
[207.84s -> 209.48s]  in a community.
[209.48s -> 214.48s]  The dominant view, I call it an LM first view.
[214.68s -> 216.80s]  So we first have an LM.
[216.80s -> 217.76s]  It's amazing.
[217.76s -> 218.92s]  It's very powerful.
[218.92s -> 220.16s]  It knows a lot of things.
[220.16s -> 222.28s]  So let's make that into an agent.
[223.36s -> 224.88s]  Then if you take this view,
[224.92s -> 227.12s]  naturally the implications are,
[227.12s -> 231.00s]  you think of building a scaffold on top of an LM
[231.00s -> 233.28s]  and it will be a lot of prompting
[233.28s -> 235.52s]  and it will be heavy on engineering.
[235.52s -> 240.40s]  And I think that's also where a lot of these sentiments
[240.40s -> 242.84s]  that the agents are just thing wrappers
[242.84s -> 244.44s]  around LM's come from.
[246.24s -> 247.92s]  I tend to take a different view.
[247.92s -> 250.88s]  I take an agent first view.
[250.88s -> 254.32s]  So as I said, AI agents have been the core pursuits
[254.32s -> 256.64s]  of AI since the very beginning.
[256.64s -> 259.52s]  LG's just now become an upgrade.
[259.52s -> 263.52s]  Now we integrate one or more LM's into this AI agent
[263.52s -> 265.84s]  so they gain a new capability.
[265.84s -> 267.36s]  So we've used the language
[267.36s -> 269.44s]  with reasoning and communication.
[271.08s -> 272.48s]  If you hold this view,
[272.48s -> 275.80s]  then the implications are all the same challenges
[275.80s -> 277.96s]  faced by the previous AI agents,
[277.96s -> 280.12s]  like how to perceive the environments,
[280.12s -> 282.32s]  how to reason about the environmental states,
[282.32s -> 284.32s]  how to build all the models
[284.32s -> 289.08s]  to model the state transition dynamics in the environment
[289.08s -> 291.64s]  and how to use that for better planning.
[291.64s -> 294.04s]  All of these challenges still remain,
[294.04s -> 296.24s]  but we need to re-examine them
[296.24s -> 299.40s]  through the new dance of LM's.
[299.40s -> 301.96s]  And also we need to tackle the new challenges
[301.96s -> 303.32s]  and opportunities.
[303.32s -> 305.88s]  Like now we can do synthetic data at a scale,
[305.88s -> 307.60s]  we can do several reflection
[307.60s -> 311.04s]  and we can also do O1 style internalizer search.
[313.32s -> 314.28s]  Okay.
[314.28s -> 316.20s]  If we agree on that view,
[316.20s -> 317.72s]  then agent first view,
[317.72s -> 320.76s]  then we can look at what's really fundamentally different
[320.76s -> 324.32s]  now once we're integrating these LM's.
[324.32s -> 327.60s]  I think it's really the capability of using the language
[327.60s -> 330.36s]  as a vehicle for reasoning and communication.
[332.12s -> 334.40s]  The power of using the language for communication
[334.40s -> 336.16s]  is probably more clear.
[337.56s -> 339.88s]  All of us remember the vividly,
[340.08s -> 342.60s]  the viral success of a chat to BT.
[342.60s -> 347.04s]  It was the fastest app to grow to 100 million users
[347.04s -> 348.08s]  in human history.
[349.36s -> 351.28s]  But we also knew that
[351.28s -> 353.04s]  in terms of the fundamental capabilities,
[353.04s -> 355.16s]  chat to BT was not significantly better
[355.16s -> 356.64s]  than the previous generation,
[356.64s -> 359.44s]  but openly I decided to tune it
[359.44s -> 362.36s]  to be more chatty and release it to the public.
[362.36s -> 364.56s]  So I think that actually really shows you
[364.56s -> 367.40s]  the power of using language for communication.
[368.40s -> 370.32s]  You get instruction following,
[370.32s -> 372.08s]  you get in context of learning
[372.08s -> 375.60s]  and you can easily customize the output formats.
[377.36s -> 380.48s]  But perhaps a bit more unique this time around,
[380.48s -> 382.48s]  because using language for communication,
[382.48s -> 387.12s]  we have been doing that in decades in dialogue systems.
[387.12s -> 390.56s]  But this new capability of using language for reasoning
[390.56s -> 393.72s]  is probably a bit more unique this time around.
[394.56s -> 398.08s]  It used to be the case that I have to do
[398.08s -> 400.84s]  a lot of justification about why this is unique,
[400.84s -> 403.32s]  but now with all these new reasoning models,
[403.32s -> 405.16s]  I think this has become much easier.
[406.16s -> 410.00s]  So, but let's still consider this example.
[410.00s -> 412.28s]  Most of the people would look at this example
[412.28s -> 415.08s]  and think GPT-4 was being instituted
[415.08s -> 416.92s]  and contradicting itself.
[416.92s -> 418.60s]  I first thought the answer is no,
[418.60s -> 420.88s]  but there was a lot, a bit more thinking
[420.92s -> 424.04s]  and then you realize that the answer is actually yes.
[424.96s -> 426.76s]  But the way I see this example,
[426.76s -> 430.60s]  it really shows the power of using language for reasoning.
[430.60s -> 433.60s]  So you can decide, the agent,
[433.60s -> 435.84s]  the model can decide on the fly
[435.84s -> 440.52s]  to use the adaptable amount of the compute
[440.52s -> 441.84s]  for different problems.
[441.84s -> 443.92s]  So you've got a second chance,
[443.92s -> 446.20s]  unlike in traditional ML models
[446.20s -> 450.08s]  where you pay a fixed amount of compute for any decision.
[450.24s -> 453.04s]  So you only have one shot and you have to commit to it.
[454.16s -> 457.08s]  And it's specifically in the context of agents.
[457.08s -> 459.80s]  Reasoning is not just for the sake of reasoning,
[459.80s -> 461.88s]  but it's not just for math or coding.
[461.88s -> 464.20s]  Reasoning is for better acting.
[464.20s -> 465.36s]  So you can use the reasoning
[465.36s -> 468.12s]  to infer about the environmental states.
[468.12s -> 469.56s]  You can do self reflection.
[469.56s -> 471.52s]  You can do dynamic re-planning,
[471.52s -> 475.16s]  but all for the same purpose for better acting,
[475.16s -> 477.12s]  better planning in the environment.
[480.60s -> 482.92s]  Now we're ready to reconcile
[482.92s -> 485.60s]  this new generation of language agents
[485.60s -> 487.32s]  with the previous agents.
[488.16s -> 492.32s]  All we need to do is to add a new type of action
[492.32s -> 495.44s]  called the reasoning by generating tokens.
[495.44s -> 498.96s]  This in contrast with actions in external environments.
[500.12s -> 500.96s]  Then for that,
[500.96s -> 504.00s]  we also need to add an internal environment
[504.00s -> 507.40s]  just like in the model log where reasoning happens.
[507.40s -> 511.72s]  With that change,
[511.72s -> 515.72s]  now we can also reconcile things like self reflection.
[515.72s -> 518.72s]  Now it becomes just a meta reasoning action.
[518.72s -> 521.32s]  Like it's a reasoning over the reasoning process,
[521.32s -> 523.56s]  just like the meta cognitive functions.
[525.24s -> 527.00s]  And reason is for better acting.
[527.00s -> 527.96s]  And at the same time,
[527.96s -> 530.16s]  due to the power of these LLMs,
[530.16s -> 532.64s]  the percept and the external action spaces
[532.64s -> 534.76s]  are drastically expanded as well.
[534.76s -> 538.00s]  Okay, before we proceed,
[538.00s -> 541.08s]  let me just provide a little bit of justification
[541.08s -> 543.16s]  of the use of reasoning here.
[544.04s -> 546.44s]  Because this is such an overloaded term.
[546.44s -> 548.64s]  If we don't properly define it,
[548.64s -> 550.48s]  then people will frown upon it.
[551.80s -> 555.40s]  So I'm sympathetic of the position
[555.40s -> 558.44s]  of you calling all of these as reasoning
[558.44s -> 560.32s]  because for people who are familiar
[560.32s -> 565.32s]  with the due process mental model of human cognition,
[567.16s -> 571.04s]  by the famous late Daniel Kahneman
[571.04s -> 572.68s]  in thinking fast and slow.
[572.68s -> 575.80s]  You could be familiar with concepts like a perception,
[575.80s -> 577.44s]  then intuitively influenced,
[577.44s -> 580.12s]  and then a symbolic reasoning.
[580.12s -> 582.80s]  Intuitively influenced is fast, effortless,
[582.80s -> 586.28s]  and then symbolic reasoning is slow and effortful.
[586.28s -> 589.56s]  But LLMs, they don't have
[589.56s -> 593.92s]  all of those different cognitive subscripts
[593.92s -> 597.72s]  for these different type of cognitive functions.
[597.72s -> 600.84s]  All it has is mostly just one mechanism,
[600.84s -> 602.88s]  which is token generation.
[602.88s -> 605.80s]  It's mostly, we will talk about this later
[605.80s -> 607.56s]  because we will talk about intrinsic,
[607.56s -> 609.28s]  like implicit reasoning,
[609.28s -> 612.60s]  but mostly reasoning happens through token generalization,
[612.64s -> 615.28s]  through explicit language production.
[616.40s -> 619.12s]  So if you look at this example from 24O,
[619.12s -> 621.32s]  when it looks at this image,
[621.32s -> 623.36s]  it's generation incorporates
[623.36s -> 626.76s]  all of these different mental processes,
[626.76s -> 628.72s]  what we would call perception,
[628.72s -> 630.04s]  like what's in there,
[630.04s -> 631.52s]  and intuitively influenced,
[631.52s -> 634.24s]  what you can immediately infer from what you see,
[634.24s -> 636.08s]  and also some symbolic reasoning,
[636.08s -> 638.84s]  some more complex reasoning.
[638.84s -> 640.20s]  So I think for this reason,
[640.20s -> 644.60s]  because LLMs really only have this one mechanism,
[644.60s -> 647.00s]  and that blends all this together.
[647.00s -> 650.36s]  So I think it's proper to call this reasoning.
[651.40s -> 655.32s]  And then one may alternatively call these thoughts
[655.32s -> 657.60s]  to avoid this overloaded term,
[657.60s -> 659.12s]  but then the risk is that
[659.12s -> 662.40s]  with the further anthropomorphize these machines.
[662.40s -> 664.44s]  So I prefer reasoning over thoughts.
[664.44s -> 665.28s]  All right.
[667.48s -> 670.48s]  Another thing that I want to get behind us
[670.48s -> 673.44s]  before we proceed is the name.
[673.44s -> 674.92s]  There are so many different names
[674.92s -> 676.60s]  where these agents right now,
[676.60s -> 679.28s]  these AI agents, autonomous agents,
[679.28s -> 681.64s]  LM agents, and so on and so forth.
[681.64s -> 684.60s]  But I truly think that like language agents
[684.60s -> 688.24s]  is probably the most discrete,
[688.24s -> 689.84s]  like characteristic name
[689.84s -> 692.12s]  for this kind of generation of agents,
[692.12s -> 693.40s]  because the language
[693.40s -> 695.72s]  is really their most salient traits.
[697.04s -> 699.36s]  And what about multi-modal agents?
[699.36s -> 702.44s]  Well, there is perception in other modalities
[702.44s -> 703.80s]  and they are very important,
[703.80s -> 706.48s]  but the language is still doing the heavy lifting,
[706.48s -> 708.52s]  like the reasoning and communication part.
[709.72s -> 711.44s]  And what about LM agents?
[711.44s -> 714.56s]  That's probably the most popular name out there.
[714.56s -> 716.80s]  I think what's really needed here
[716.80s -> 719.76s]  or characteristic of this generation of agents
[719.76s -> 724.56s]  is this capability of universal language
[724.56s -> 726.36s]  understanding and production,
[726.36s -> 729.20s]  which turns out to be extremely important for agents.
[730.80s -> 735.20s]  But this capability doesn't have to come from an LM.
[735.20s -> 737.68s]  Maybe in a few days we will go beyond LM,
[737.68s -> 740.84s]  but these needs, this capability will remain.
[740.84s -> 743.96s]  So LM's could be a means to an end.
[743.96s -> 747.08s]  In that sense, it's less as fundamental.
[747.08s -> 751.56s]  So language agents is still a more appropriate name.
[754.44s -> 755.48s]  If we take one step back
[755.48s -> 758.96s]  and think about the evolution of AI,
[758.96s -> 762.92s]  I think we're really entering a new evolutionary stage
[762.92s -> 765.12s]  of machine intelligence.
[765.12s -> 768.44s]  So human intelligence is truly a marvel made by nature.
[768.44s -> 771.72s]  Somehow our brain can take raw inputs
[771.72s -> 773.80s]  from different sensory organs
[773.80s -> 777.80s]  and we present them in a unified neural representation
[777.80s -> 780.12s]  to reconstruct the world around us
[780.12s -> 782.72s]  and also to support symbolic reasoning
[782.72s -> 784.96s]  and decision-making, right?
[784.96s -> 787.68s]  So that's truly fantastic.
[787.68s -> 789.64s]  Then throughout the history of AI,
[789.64s -> 794.08s]  we have made many attempts to approach human intelligence
[794.08s -> 798.72s]  and manifest machine intelligence into these AI agents.
[798.72s -> 801.08s]  But earlier generations of AI agents
[801.08s -> 804.24s]  were only able to capture some limited facets
[804.24s -> 807.20s]  of human intelligence, like symbolic reasoning,
[807.20s -> 810.08s]  like perception in single modalities.
[811.16s -> 814.48s]  Only recently was this multimodal LM's
[814.48s -> 817.16s]  and the language agent built on top of them.
[817.16s -> 818.88s]  For the first time,
[818.88s -> 823.88s]  we have a model that can encode multi-sensor inputs
[823.88s -> 826.60s]  into a unified neural representation
[826.60s -> 829.92s]  that is also conducive to symbolic reasoning
[829.92s -> 831.56s]  and communication.
[831.56s -> 835.52s]  So this drastically improve the expressiveness,
[835.52s -> 839.48s]  the reasoning ability and the adaptivity of AI agents.
[841.00s -> 843.96s]  And that's, I think, what makes this kind of generation
[843.96s -> 845.72s]  of AI agents so exciting.
[847.12s -> 849.16s]  We can do a more detailed comparison
[849.16s -> 850.88s]  of these different generations of agents,
[850.88s -> 852.56s]  but in the interest of time,
[852.56s -> 854.72s]  I will just quickly go through this
[854.72s -> 857.08s]  and just focus on language agents.
[857.20s -> 860.28s]  The expressiveness is pretty high.
[860.28s -> 862.68s]  For example, you can compare with the logical agents
[862.68s -> 864.72s]  where the expressiveness is bounded
[864.72s -> 867.00s]  by the logical language we use.
[867.00s -> 869.68s]  Here, the agent or the model
[869.68s -> 872.16s]  can essentially encode almost everything,
[873.08s -> 875.92s]  especially the verbalizable parts of the world.
[875.92s -> 877.84s]  But for the non-verbalizable parts,
[877.84s -> 879.56s]  like how to recognize a face,
[879.56s -> 884.32s]  we still have multimodal input is to capture those.
[885.16s -> 886.64s]  And then for reasoning,
[886.64s -> 888.96s]  now we do the language-based reasoning
[888.96s -> 891.48s]  instead of logical inferences.
[891.48s -> 896.48s]  So it's fuzzy and it's flexible
[896.52s -> 898.56s]  and it's semi-explicit.
[898.56s -> 900.76s]  So it's not entirely implicit.
[900.76s -> 902.36s]  You still have this chain of thought
[902.36s -> 904.08s]  that you can see what's going on.
[904.08s -> 906.44s]  And the fuzziness, I want to emphasize
[906.44s -> 908.84s]  that it's not necessarily a bad thing
[908.84s -> 910.68s]  because the world around us,
[910.68s -> 914.28s]  it has a lot of fuzziness in it.
[914.28s -> 918.08s]  If we resort to something strictly very sound
[918.08s -> 919.32s]  and very rigid,
[919.32s -> 923.32s]  then it comes as a cost of sacrificing
[923.32s -> 925.96s]  the expressiveness a lot.
[925.96s -> 927.12s]  And then the adaptivity,
[927.12s -> 930.28s]  which is a hallmark of intelligence,
[930.28s -> 932.76s]  is also very high in language agents
[932.76s -> 935.92s]  because there's a strong prior captured by MLM
[935.92s -> 938.64s]  and the language use in general is very flexible.
[941.36s -> 942.80s]  So before we proceed,
[942.80s -> 945.60s]  let me share with you my own conceptual framework
[945.60s -> 946.84s]  for language agents.
[946.84s -> 948.20s]  This is what I have been using
[948.20s -> 950.16s]  to guide my own research agenda
[950.16s -> 952.12s]  in the past two years or so.
[953.56s -> 958.28s]  The most important things are what I call core competencies
[958.28s -> 961.40s]  and I try to arrange them in those hierarchy.
[961.40s -> 963.08s]  So each box here,
[963.08s -> 966.56s]  you can roughly find the corresponding cognitive function
[966.56s -> 967.80s]  in the human brain.
[967.80s -> 971.68s]  And then the bottom ones are the more fundamental ones,
[971.68s -> 974.32s]  like perception, memory, and embodiment,
[974.32s -> 977.56s]  and the upper ones built on top of the bottom ones,
[977.56s -> 980.64s]  like planning built on top of reasoning and word models
[980.64s -> 983.36s]  and reasoning built on top of perception and memory
[983.36s -> 985.04s]  and source and force.
[985.04s -> 987.44s]  We also have many cross-cutting issues,
[987.44s -> 990.60s]  like safety, evaluation, synthetic data
[990.60s -> 993.72s]  is both a challenge and an opportunity,
[993.72s -> 996.80s]  and efficiency and many new applications.
[996.80s -> 998.20s]  So with this framework,
[998.24s -> 1002.00s]  now you can look at any new paper about agents
[1002.00s -> 1007.00s]  and try to map its main claims and main contributions
[1007.04s -> 1008.68s]  into this framework.
[1012.04s -> 1014.48s]  Right, so that's the introduction.
[1014.48s -> 1015.96s]  And for the rest of the talk,
[1015.96s -> 1020.96s]  we'll try to cover three main aspects of language agents.
[1022.36s -> 1024.76s]  How we model long-term memory,
[1024.76s -> 1028.32s]  for which I will use our recent work at Hiporac.
[1028.32s -> 1032.16s]  Then how do these language models fully reason?
[1032.16s -> 1035.24s]  And I will use our work on graph to transform it.
[1035.24s -> 1037.48s]  And then finally, I will talk about planning,
[1037.48s -> 1040.64s]  especially word-based planning using a word model.
[1041.52s -> 1044.00s]  Okay, so let's start with memory.
[1047.16s -> 1048.92s]  We'll talk about Hiporac.
[1048.92s -> 1051.60s]  The main content will be from Hiporac,
[1051.60s -> 1054.20s]  but we'll talk about a lot of other things as well.
[1054.68s -> 1057.96s]  It's a new biologically inspired long-term memory mechanism
[1057.96s -> 1059.16s]  for large language models.
[1059.16s -> 1062.32s]  It's that by my student, Bono,
[1062.32s -> 1064.68s]  and in collaboration with Michi from Stanford.
[1067.20s -> 1070.88s]  Right, so if we look at how humans
[1070.88s -> 1075.24s]  or really most other animals that have a numerous system,
[1075.24s -> 1078.76s]  how they learn, it's really fascinating
[1078.76s -> 1083.76s]  that we are all like this 24-7 nonstop lifelong learners.
[1085.20s -> 1089.16s]  And all we learn is stored in our memory.
[1090.36s -> 1095.28s]  So Eric Kandel, the Nobel Prize winner
[1095.28s -> 1098.76s]  who got the prize for his contribution
[1098.76s -> 1101.00s]  in the study of memory,
[1101.00s -> 1104.12s]  especially the neurobiological foundation of memory,
[1104.12s -> 1107.48s]  once said that memory is everything.
[1107.48s -> 1109.72s]  Without it, we have nothing,
[1109.72s -> 1112.44s]  which I think is very profound
[1112.48s -> 1116.20s]  because really anything we learn
[1116.20s -> 1119.64s]  has to be encoded in our memory
[1119.64s -> 1124.56s]  and through a process called synaptic plasticity, right?
[1124.56s -> 1127.04s]  So basically we can change our synapses
[1127.04s -> 1129.04s]  to capture, to memorize things
[1129.04s -> 1131.16s]  and to capture all the things we're experiencing
[1131.16s -> 1132.60s]  and we're doing.
[1132.60s -> 1136.92s]  And there are different ways of changing the synapses.
[1136.92s -> 1139.80s]  Like for example, you could change the strength
[1139.80s -> 1144.80s]  of a synapses by either adding more receptors here
[1146.60s -> 1150.40s]  or reducing more neurotransmitters.
[1151.36s -> 1156.36s]  Or you can do structural changes to a synapses
[1156.48s -> 1159.24s]  like by growing new synapses
[1159.24s -> 1162.44s]  for the same neuron, for example.
[1162.44s -> 1166.28s]  So this is used more often in forming long-term memory.
[1167.24s -> 1170.48s]  Right, so we are really 24-7 learners.
[1170.48s -> 1172.24s]  Even when we're sleeping,
[1172.24s -> 1176.72s]  we are replaying what happens during the day, right?
[1176.72s -> 1181.36s]  That's how this long-term memory gets consolidated, right?
[1181.36s -> 1186.36s]  Ideally, we want that the same kind of learning capacity
[1186.40s -> 1189.56s]  in machines, especially in agents as well.
[1189.56s -> 1190.52s]  Because these agents,
[1190.52s -> 1193.72s]  they are supposed to explore the world,
[1193.72s -> 1197.36s]  do things and then accumulate a lot of learning from that
[1197.36s -> 1200.84s]  and self-improve in some sense, right?
[1200.84s -> 1205.84s]  However, that's very hard for the current technologies.
[1207.56s -> 1210.80s]  For these newer networks in general and LLM,
[1210.80s -> 1213.80s]  these gigantic LLMs in particular,
[1213.80s -> 1218.80s]  they have the notorious issue of catastrophic forgetting.
[1220.52s -> 1222.32s]  So when you're learning new things,
[1222.32s -> 1226.44s]  because of these highly distributional representation
[1226.44s -> 1228.44s]  of these models, in these models,
[1228.44s -> 1230.08s]  when you are learning new things,
[1230.08s -> 1233.60s]  it often has unintended side effects,
[1233.60s -> 1238.40s]  like some other things like unexpectedly got changed
[1238.40s -> 1241.88s]  during that new learning process, right?
[1241.88s -> 1245.64s]  And to use this, we can use,
[1245.64s -> 1247.76s]  and in the LLM context,
[1247.76s -> 1251.12s]  we can consider what people have been doing,
[1251.12s -> 1256.12s]  trying to edit an LLM to inject or alter
[1256.20s -> 1257.64s]  a specific effect.
[1257.64s -> 1261.20s]  So that's called knowledge model editing, right?
[1261.20s -> 1266.16s]  And such topic in LLM model editing
[1266.16s -> 1269.28s]  is the so-called repo effect, right?
[1269.28s -> 1273.20s]  So if you want to change just this one fact
[1273.20s -> 1276.00s]  in a counterfactual change,
[1276.00s -> 1277.32s]  like you want to say,
[1277.32s -> 1282.20s]  hey, Leonardo is a citizen of Syria
[1282.20s -> 1285.36s]  instead of United States, right?
[1285.36s -> 1289.44s]  And you have some expected repo effects, right?
[1289.44s -> 1291.88s]  Because now the citizenship has changed,
[1291.88s -> 1296.88s]  so he should speak Arabic instead of English.
[1297.04s -> 1300.80s]  But if you actually look at what has changed
[1300.80s -> 1303.00s]  after this edit, you will see that
[1303.00s -> 1305.88s]  a lot of other unexpected things have changed,
[1305.92s -> 1309.00s]  or the things that should have changed,
[1309.00s -> 1311.20s]  but didn't change, right?
[1311.20s -> 1313.28s]  So if you look at the negation
[1313.28s -> 1315.24s]  of these original statements,
[1315.24s -> 1318.96s]  you will see that it will still predict Syria,
[1318.96s -> 1320.44s]  which is wrong.
[1320.44s -> 1323.96s]  You want it to be United States because of the negation.
[1323.96s -> 1325.76s]  And then there is this,
[1328.04s -> 1331.40s]  like the language here also becomes Syria,
[1331.40s -> 1334.36s]  but you want it to be Arabic and so on and so forth.
[1334.36s -> 1336.92s]  So all of these just tell you that
[1336.92s -> 1341.32s]  because of this highly distributional representation
[1341.32s -> 1343.80s]  of these artificial neural networks,
[1343.80s -> 1346.12s]  that makes continual learning very hard.
[1349.04s -> 1353.20s]  The human brain and most somehow figure out a way to do
[1353.20s -> 1357.04s]  that, but we don't quite understand how that happens yet.
[1357.04s -> 1360.76s]  So we cannot replicate that in machines.
[1360.76s -> 1362.60s]  But that's highly desired.
[1362.60s -> 1365.16s]  This kind of continual learning is highly desired
[1365.16s -> 1367.68s]  for agents and for LRMs.
[1367.68s -> 1368.64s]  So what can we do?
[1370.56s -> 1374.32s]  The good news is that for LRMs,
[1374.32s -> 1378.28s]  it's possible to use an alternative form of memory
[1378.28s -> 1380.52s]  called non-harmetric memory.
[1381.40s -> 1385.12s]  So instead of like directly changing the parameters
[1385.12s -> 1388.28s]  of a model with the new experiences,
[1388.28s -> 1392.32s]  we can just hold the new experience external to the model.
[1392.88s -> 1394.20s]  So it's non-harmetric.
[1394.20s -> 1399.00s]  And then just retrieve them using some mechanism as we go.
[1400.92s -> 1403.84s]  But for that to work, then of course,
[1403.84s -> 1407.48s]  this is called a RAG, retrieval augmented generation.
[1407.48s -> 1411.68s]  But for that to work, there is a prerequisite condition.
[1412.64s -> 1415.20s]  That is when you retrieve something,
[1415.20s -> 1418.56s]  some external information and you say,
[1418.56s -> 1419.92s]  hey, this is your memory.
[1419.92s -> 1422.72s]  So you should take it.
[1422.72s -> 1425.20s]  All your decision-making should be based on it.
[1425.20s -> 1429.12s]  Then it's based on the condition that these LRMs
[1429.12s -> 1434.12s]  actually will be receptive to such external information.
[1436.24s -> 1440.64s]  So we did this study called adaptive chameleon
[1440.64s -> 1442.88s]  or stubborn thoughts,
[1442.88s -> 1445.88s]  where we specifically study this behavior.
[1445.88s -> 1449.76s]  When you have this external evidence like from RAG
[1450.64s -> 1452.08s]  and then when that directed conflict
[1452.08s -> 1455.92s]  within the LRMs parametric memory, what would happen?
[1455.92s -> 1459.80s]  Will they resist that or will they be receptive to that?
[1461.40s -> 1464.24s]  Perhaps now not too surprisingly at this point,
[1464.24s -> 1466.64s]  but what's very surprising at this time
[1466.64s -> 1470.88s]  is that these LRMs turned out to be highly receptive
[1470.88s -> 1472.68s]  to external evidence,
[1472.68s -> 1476.64s]  even when that conflict with their parametric memory.
[1476.64s -> 1478.40s]  So for the two examples here,
[1478.44s -> 1480.44s]  we have a question and the ground truth answer.
[1480.44s -> 1484.92s]  In this case, the parametric memory of the LRM is correct.
[1484.92s -> 1489.24s]  But if you give it a coherent counter memory like this,
[1489.24s -> 1491.72s]  then the LRM will happily accept this
[1491.72s -> 1493.28s]  and change its answer.
[1493.28s -> 1496.76s]  And in this case, the parametric memory was wrong
[1496.76s -> 1498.04s]  and you can correct that
[1498.04s -> 1501.16s]  by giving it the correct counter memory.
[1501.16s -> 1503.48s]  So that kind of paves the way
[1503.48s -> 1506.48s]  for non-parametric memory for LRMs
[1506.48s -> 1509.92s]  because of their being highly receptive
[1509.92s -> 1512.40s]  to this external evidence.
[1512.40s -> 1515.32s]  Of course, this has other implications like safety.
[1515.32s -> 1518.20s]  Like maybe this means that these LRMs
[1518.20s -> 1520.00s]  could be highly callable,
[1520.00s -> 1523.00s]  but that's not the focus today.
[1526.24s -> 1529.16s]  Now let's talk about how to really make
[1529.16s -> 1532.80s]  long-term memory work for LRMs.
[1532.80s -> 1537.08s]  Most people know that RAG is the defector solution today.
[1539.68s -> 1541.64s]  Given something that is, for example,
[1541.64s -> 1543.24s]  the LRM doesn't know
[1543.24s -> 1546.12s]  or is beyond its knowledge cutoff date,
[1546.12s -> 1548.20s]  then it will retrieve from the internet
[1548.20s -> 1550.16s]  and then use the retrieved information
[1550.16s -> 1554.30s]  as a kind of long-term memory to answer the question.
[1556.32s -> 1559.56s]  But if you think about how RAG works,
[1560.12s -> 1563.20s]  you embed the evidence into vectors,
[1563.20s -> 1566.84s]  then you do vector-based similarity to retrieve them.
[1566.84s -> 1569.96s]  That seems far simpler,
[1570.84s -> 1574.44s]  far less sophisticated than the human memory system
[1574.44s -> 1579.44s]  where we can recognize patterns in massive data,
[1579.52s -> 1580.80s]  raw experiences.
[1580.80s -> 1584.76s]  We can create a lot of associations across them
[1584.76s -> 1587.08s]  and then we can dynamically retrieve them
[1587.48s -> 1590.96s]  for the current context and so on and so forth.
[1590.96s -> 1594.84s]  So to illustrate some of the limitations
[1594.84s -> 1597.72s]  of the current embedding-based RAG system,
[1597.72s -> 1599.28s]  let's consider this example.
[1600.28s -> 1603.92s]  Let's say you have a query
[1603.92s -> 1605.68s]  which a Stanford professor works on
[1605.68s -> 1608.08s]  the neuroscience of Alzheimer's.
[1608.08s -> 1610.20s]  So you have two salient concepts,
[1610.20s -> 1612.62s]  Stanford and Alzheimer's.
[1612.62s -> 1615.40s]  Then let's assume just hypothetically
[1615.40s -> 1618.12s]  you have a bunch of passages.
[1618.12s -> 1622.60s]  Each passage only contains part of the information.
[1622.60s -> 1624.56s]  So you don't happen to have a passage
[1624.56s -> 1629.56s]  that tells you this person is both a Stanford professor
[1630.28s -> 1632.28s]  and works on Alzheimer's.
[1632.28s -> 1635.96s]  Instead, what you have is these separate passages.
[1635.96s -> 1639.64s]  Okay, this passage says this person works at Stanford.
[1639.64s -> 1643.68s]  This passage says that this person works on Alzheimer's.
[1643.68s -> 1646.88s]  And then you will see there is one person
[1646.88s -> 1650.54s]  who works at Stanford and on Alzheimer's,
[1650.54s -> 1653.08s]  but it's in separate documents.
[1653.08s -> 1656.26s]  You don't happen to have one document that tells you all.
[1657.80s -> 1658.76s]  So what would happen
[1658.76s -> 1661.28s]  if you just use embedding-based RAG?
[1662.62s -> 1666.88s]  Now you embed each of these passages into a vector.
[1666.88s -> 1669.08s]  You embed your query into a vector.
[1669.08s -> 1672.24s]  You do compare these similarities one by one.
[1672.24s -> 1675.08s]  Then you will find all of these passages
[1675.08s -> 1678.44s]  are equally likely because each of them
[1678.44s -> 1682.88s]  captures precisely 50% of the information.
[1683.88s -> 1688.88s]  And as a result, the model has to manually
[1689.04s -> 1691.00s]  go through all of these passages
[1691.00s -> 1694.48s]  to figure out which ones are the correct ones.
[1695.80s -> 1698.84s]  And you can imagine there could be thousands of professors
[1698.84s -> 1701.96s]  working at Stanford and even more people
[1702.56s -> 1703.80s]  working on Alzheimer's.
[1703.80s -> 1707.10s]  So it's a one-to-many, many-to-one relationship.
[1708.08s -> 1711.26s]  But our human memory doesn't work that way.
[1711.26s -> 1714.48s]  Somehow, if we have come across
[1714.48s -> 1717.68s]  like this person works on Alzheimer's,
[1717.68s -> 1719.96s]  this person also works at Stanford,
[1719.96s -> 1723.56s]  then we will build some sort of association model
[1723.56s -> 1725.96s]  such that when this query comes,
[1725.96s -> 1728.40s]  we can very easily and quickly find that,
[1728.40s -> 1731.92s]  hey, this person is connected with both Stanford
[1732.88s -> 1734.48s]  and Alzheimer's, so it's dancing.
[1734.48s -> 1736.40s]  We don't have to go through,
[1736.40s -> 1739.04s]  recall all of this other information
[1739.04s -> 1742.10s]  to derive at that conclusion.
[1743.64s -> 1745.92s]  So in that sense, embedding basic rack
[1745.92s -> 1748.44s]  works very differently from human memory.
[1751.64s -> 1756.64s]  Now to study exactly how the human memory works
[1757.64s -> 1760.20s]  in these regards, so we turn to
[1760.24s -> 1763.88s]  this well-established theory of human long-term memory
[1763.88s -> 1766.84s]  called the hippocampal indexing theory.
[1766.84s -> 1768.24s]  It basically goes like this.
[1768.24s -> 1772.50s]  I'll summarize it in an overly simplified way.
[1773.40s -> 1775.56s]  It says that your wrong memory
[1775.56s -> 1778.16s]  is stored on your neocortex,
[1778.16s -> 1783.16s]  particularly in where the memory was first generated.
[1785.16s -> 1789.48s]  So when episodic memory, the auditory part of it
[1789.60s -> 1792.16s]  will be stored in the auditory cortex,
[1792.16s -> 1794.12s]  the visual part of it will be stored
[1794.12s -> 1796.46s]  in the visual cortex, and so on and so forth.
[1796.46s -> 1799.26s]  That's why when you are trying to recall something,
[1799.26s -> 1801.36s]  you were like reliving the moment,
[1801.36s -> 1805.88s]  like because it's just re-triggering the same neurons
[1805.88s -> 1809.68s]  as if you were perceiving them, right?
[1809.68s -> 1814.32s]  But importantly, you also have a structured index
[1814.32s -> 1816.96s]  stored in the hippocampus.
[1816.96s -> 1819.36s]  That creates a much closer,
[1819.36s -> 1822.38s]  much essentially a shortcut
[1822.38s -> 1826.54s]  that associates all these disparate memory units together.
[1828.72s -> 1831.62s]  And that's what we are trying to mimic here.
[1834.12s -> 1837.00s]  This kind of separation and structure index
[1837.00s -> 1841.22s]  give you two important faculties of human memory.
[1841.22s -> 1843.92s]  It allows you to do pattern separation
[1843.92s -> 1847.64s]  so you can differentiate memories
[1847.64s -> 1849.08s]  in a very fine-grained way,
[1849.08s -> 1851.70s]  certainly beyond just vectors,
[1851.70s -> 1854.10s]  at least at the concept level, right?
[1854.10s -> 1856.34s]  If you think about your episodic memory,
[1856.34s -> 1860.00s]  the second, this second and the second before,
[1860.00s -> 1862.00s]  they are very similar to each other.
[1862.00s -> 1864.68s]  How can you differentiate the differences?
[1864.68s -> 1868.24s]  That requires a very fine-grained separation of patterns.
[1869.08s -> 1871.96s]  But more importantly, which is more relevant here,
[1871.96s -> 1873.96s]  is pattern completion, right?
[1873.96s -> 1878.96s]  We can recover complete memories using just partial queues.
[1879.56s -> 1881.44s]  Like the previous example,
[1881.44s -> 1884.52s]  the partial queues are Stanford and Alzheimer's.
[1884.52s -> 1888.00s]  And then you can quickly recall this whole fact
[1888.00s -> 1891.16s]  that this person is associated with both of them.
[1891.16s -> 1895.82s]  And that's due to this structure index in the hippocampus.
[1897.84s -> 1901.40s]  Right, so that's what we're trying to mimic
[1901.44s -> 1903.76s]  in Hippo-Rag.
[1903.76s -> 1907.06s]  We're trying to build a similar structured index
[1907.06s -> 1911.48s]  for Rag systems to enjoy some of the same benefits
[1911.48s -> 1913.38s]  of the human long-term memory system.
[1916.28s -> 1919.24s]  I won't get into too much details,
[1919.24s -> 1921.88s]  but so I'll just at a high level,
[1923.48s -> 1926.60s]  put this in the Hippo-Campbell indexing theory.
[1926.60s -> 1928.60s]  There are three main parts, right?
[1928.60s -> 1931.44s]  There's the neural cortex and the hippocampus,
[1931.44s -> 1933.56s]  and then there's the parapixel regions
[1933.56s -> 1935.16s]  that connects the two.
[1935.16s -> 1940.04s]  So the neural cortex is more about pattern separation.
[1940.04s -> 1942.80s]  So it will process the raw experience,
[1942.80s -> 1946.22s]  extract patterns out of these concepts
[1946.22s -> 1947.60s]  and so on and so forth.
[1947.60s -> 1951.28s]  And then the hippocampus is more like the structured index.
[1951.28s -> 1954.12s]  So it's the indexing auto-association.
[1954.12s -> 1955.84s]  And then the parapixel regions
[1955.88s -> 1958.76s]  is more like the working memory that connects the two
[1958.76s -> 1963.66s]  and do some more iterative thinking on there.
[1965.72s -> 1970.72s]  So to mimic this process, we have two spaces.
[1971.40s -> 1973.52s]  We have the offline indexing phase
[1973.52s -> 1975.66s]  and the online query phase.
[1975.66s -> 1977.60s]  For the offline indexing,
[1977.60s -> 1980.70s]  let's say you have these passages as the inputs.
[1980.70s -> 1984.48s]  We use an LN to serve as a neural cortex
[1984.48s -> 1987.90s]  that will do open information extraction
[1987.90s -> 1990.48s]  to extract these triplets,
[1990.48s -> 1993.04s]  like the concepts, noun phrases,
[1993.04s -> 1995.66s]  and their relationships, like the verb phrases.
[1996.88s -> 2000.30s]  So extract these triplets.
[2000.30s -> 2003.62s]  Then we try to build a knowledge graph.
[2003.62s -> 2006.80s]  Particularly, this is a schema-less knowledge graph.
[2006.80s -> 2008.44s]  So we don't have a nounatology
[2008.44s -> 2011.16s]  or a predefined schema or anything like that.
[2011.16s -> 2016.04s]  Everything is extracted by an LN from the raw experiences.
[2017.32s -> 2021.24s]  So we built a knowledge graph by consolidating
[2021.24s -> 2024.00s]  all these newly extracted concepts
[2024.00s -> 2026.98s]  and phrases as nodes and edges.
[2026.98s -> 2030.16s]  That's the offline indexing phase.
[2030.16s -> 2034.62s]  And this becomes our artificial hippocampal index.
[2036.68s -> 2040.36s]  Then we use a dense retriever here
[2040.48s -> 2042.64s]  as an encoder to consolidate things,
[2042.64s -> 2047.24s]  like to identify which concepts are similar to each other
[2047.24s -> 2048.92s]  or synonymous to each other.
[2050.16s -> 2053.24s]  Then in the online retrieval phase,
[2053.24s -> 2058.08s]  when a query comes in, like the Stanford Alzheimer's example
[2058.08s -> 2060.38s]  then we'll identify the concept.
[2060.38s -> 2062.24s]  We use named entity recognition.
[2062.24s -> 2065.12s]  So now we'll get the Stanford Alzheimer's.
[2065.12s -> 2068.40s]  We still use a dense retriever
[2068.40s -> 2073.40s]  to find the similar nodes in my index.
[2075.76s -> 2080.40s]  And then those nodes become the seed nodes
[2080.40s -> 2082.36s]  for the graph search process.
[2082.36s -> 2084.56s]  So we'll use these as seeds
[2084.56s -> 2086.68s]  to do a search process on the graph
[2086.68s -> 2090.38s]  to find the most related things.
[2091.40s -> 2095.50s]  And the particular graph search algorithm we're using here
[2095.50s -> 2097.44s]  is personalized page rank.
[2099.40s -> 2102.00s]  So for people who don't quite remember
[2102.00s -> 2105.28s]  what a personalized page rank works,
[2105.28s -> 2107.28s]  I'll do a quick recap.
[2107.28s -> 2110.28s]  So it's a random walk process on the graph
[2110.28s -> 2112.32s]  where you start with some seed nodes.
[2112.32s -> 2115.64s]  So those start with probability one.
[2115.64s -> 2119.08s]  Then you use a random walk starting from the seed nodes
[2119.08s -> 2122.00s]  to disperse the probability mass
[2122.00s -> 2124.32s]  to their neighboring nodes.
[2124.32s -> 2128.16s]  So the nodes that are closer to this seed nodes
[2128.86s -> 2132.64s]  especially those are in intersection of multiple seed nodes
[2132.64s -> 2135.88s]  will naturally end up with a higher weights.
[2135.88s -> 2138.84s]  Then in this case, Professor Thomas
[2138.84s -> 2141.88s]  who is connected to both Stanford and Alzheimer's
[2141.88s -> 2145.68s]  will naturally stand out and getting the highest weights.
[2145.68s -> 2148.36s]  Now we can use these weights over concept
[2148.36s -> 2151.46s]  to re-weight the original passages
[2151.46s -> 2155.44s]  then to retrieve the most highly weighted passage.
[2156.44s -> 2159.16s]  All right, so that's how people wrapped works
[2159.16s -> 2160.20s]  in a nutshell.
[2160.20s -> 2162.18s]  Of course, there are a lot of details
[2162.18s -> 2165.76s]  that we're now covering, but this is the main gist.
[2168.08s -> 2172.40s]  And it turns out like some simple strategy
[2172.40s -> 2176.80s]  like this biologically plausible strategy
[2176.80s -> 2180.52s]  works also extremely well in practice, right?
[2180.52s -> 2181.38s]  It's not enough.
[2181.38s -> 2184.60s]  I think it is of course a neat idea like,
[2184.60s -> 2187.16s]  hey, this is biologically inspired.
[2187.16s -> 2188.76s]  How cool is that?
[2188.76s -> 2190.56s]  But I think it is equally important
[2190.56s -> 2194.68s]  for it to actually work, to work very well
[2194.68s -> 2198.72s]  and better than existing solutions in practice.
[2198.72s -> 2201.14s]  And that's what we showed was a people rack.
[2201.14s -> 2202.44s]  So we'll compare people rack
[2202.44s -> 2207.44s]  with the state-of-the-art dense retrievers at the time
[2207.68s -> 2211.40s]  and show that a multi-top QA datasets
[2211.40s -> 2214.36s]  three standard multi-top QA datasets
[2215.20s -> 2218.58s]  it performs much better by a large margin.
[2218.58s -> 2221.84s]  And also you can compare people rack
[2221.84s -> 2226.84s]  with this iterative retrieval methods like IRCOT
[2226.88s -> 2231.44s]  and it's also works better than this iterative method.
[2231.44s -> 2233.80s]  And also because of people rack
[2233.80s -> 2236.56s]  the nature of really this structured index
[2236.56s -> 2238.12s]  and this graph search.
[2238.12s -> 2242.70s]  So it's highly complimentary to all these existing methods.
[2242.74s -> 2247.58s]  You can easily integrate it with other methods like IRCOT
[2247.58s -> 2250.78s]  and then you get a big boost as well.
[2254.46s -> 2256.66s]  And to better understand
[2256.66s -> 2260.34s]  where does this power of people rack comes from
[2260.34s -> 2262.74s]  we can consider a type of questions
[2262.74s -> 2266.52s]  what do we call path-finding questions
[2266.52s -> 2270.64s]  like the example running example we have been working with.
[2270.64s -> 2274.64s]  So if you think about in this information space
[2274.64s -> 2278.44s]  like this huge graph of everything connected to everything
[2278.44s -> 2282.46s]  what's the solution path of structure
[2282.46s -> 2284.72s]  for this question looks like
[2284.72s -> 2289.72s]  you will see it's not just like a one-to-one-to-one
[2290.08s -> 2291.80s]  kind of path.
[2291.80s -> 2294.64s]  It first has start with a one-to-many relation
[2294.64s -> 2298.26s]  like Stanford there are many professors working at Stanford.
[2298.26s -> 2302.72s]  Then among those is our answer professor Thomas
[2302.72s -> 2307.72s]  and then for other people to researchers to Alzheimer's
[2309.34s -> 2312.22s]  that's also a many-to-one relationship.
[2312.22s -> 2315.10s]  So there are many people working on Alzheimer's.
[2315.10s -> 2317.70s]  If you don't have like prior knowledge
[2317.70s -> 2319.98s]  about the answer you're looking for
[2319.98s -> 2322.82s]  then naturally you'd have to search through
[2322.82s -> 2326.86s]  all these hundreds of thousands of candidates
[2326.86s -> 2329.86s]  for you to find professor Thomas.
[2329.86s -> 2333.42s]  And people wrap by explicitly building extracting
[2333.42s -> 2337.74s]  and building these associations from the raw inputs
[2337.74s -> 2340.82s]  that allows you to kind of create this shortcut
[2340.82s -> 2343.10s]  to quickly find the true answer.
[2343.10s -> 2346.06s]  Well, if you use a copier or like LCLT
[2346.06s -> 2348.62s]  then they won't be able to find it efficiently.
[2351.78s -> 2353.58s]  Okay, so that was people rack
[2354.50s -> 2356.40s]  that was published in New York's
[2356.40s -> 2358.70s]  which is just two months ago
[2358.70s -> 2362.68s]  but in today's pace it feels like ages ago.
[2363.62s -> 2368.62s]  So as I'm very excited to share that
[2368.66s -> 2371.52s]  people rack v2 is coming very soon.
[2371.52s -> 2375.60s]  And so a problem with people rack
[2375.60s -> 2377.54s]  or all of these recent structured
[2377.54s -> 2380.24s]  augmented rack message like graph rack,
[2380.24s -> 2383.34s]  light rack, rapture and so on and so forth.
[2383.90s -> 2387.98s]  If you compare favorite body
[2387.98s -> 2391.78s]  with this like simpler baselines
[2391.78s -> 2394.70s]  like this small dense retrievers
[2394.70s -> 2397.94s]  like a cobra v2 or confider
[2397.94s -> 2401.86s]  but recently there have been many large embedding models
[2401.86s -> 2404.54s]  like a grid, like a main embed
[2404.54s -> 2406.62s]  and if you actually compare
[2406.62s -> 2409.34s]  these structured augmented rack message was done
[2409.34s -> 2411.96s]  you will see that they are much worse
[2411.96s -> 2413.24s]  including hippo rack.
[2414.56s -> 2417.88s]  They are working compare okay
[2417.88s -> 2420.44s]  on this multi-hop QA tasks
[2420.44s -> 2424.08s]  because they were mostly designed
[2424.08s -> 2425.40s]  for these kinds of tasks
[2425.40s -> 2428.08s]  but if you just look at other scenarios
[2428.08s -> 2430.16s]  like just some very simple QA
[2430.16s -> 2431.90s]  or some discourse understanding
[2431.90s -> 2433.16s]  and so on and so forth
[2433.16s -> 2434.84s]  like they don't work that well.
[2434.84s -> 2437.88s]  So as a result, it's very hard for this message
[2437.88s -> 2440.60s]  to become just a drop-in replacement
[2440.60s -> 2444.12s]  of like embedding basic message.
[2444.12s -> 2446.04s]  Then in hippo-rack v2
[2446.04s -> 2450.56s]  we did a bunch of clever upgrades
[2450.56s -> 2452.64s]  to hippo-rack v1
[2452.64s -> 2456.88s]  and the results is that now v2 is comparable
[2456.88s -> 2460.96s]  or better than the best large embedding models
[2460.96s -> 2462.72s]  across the board.
[2462.72s -> 2465.56s]  So then it's much more possible
[2465.56s -> 2467.72s]  to just use hippo-rack v2
[2467.72s -> 2471.44s]  as a drop-in replacement for your rack system.
[2472.44s -> 2474.36s]  We hope to release this very soon.
[2476.60s -> 2480.76s]  Right, so for the memory part, the takeaways.
[2480.76s -> 2483.40s]  So memory is really central to human learning
[2485.12s -> 2488.88s]  and long-term memory through parametric continued learning
[2488.88s -> 2491.28s]  for LMs is very hard
[2491.28s -> 2493.92s]  but fortunately non-parametric memory
[2493.92s -> 2496.76s]  like rack could be a promising solution.
[2497.16s -> 2500.44s]  And the recent trend in rack is to add more structures
[2500.44s -> 2502.96s]  to embeddings like hippo-rack or graph-rack
[2502.96s -> 2506.10s]  to enhance like the sense-making capability
[2506.10s -> 2507.16s]  of these models
[2507.16s -> 2510.44s]  like the ability to integrate larger, more complex
[2510.44s -> 2512.48s]  and uncertain context
[2512.48s -> 2515.48s]  and the associativity of this memory
[2515.48s -> 2518.88s]  like the capacity to draw multiple connections
[2518.88s -> 2521.38s]  between disparate pieces of information.
[2521.38s -> 2526.38s]  Okay, so that's memory.
[2527.06s -> 2529.82s]  And I think we're still quite far away
[2529.82s -> 2533.90s]  from developing a very sophisticated memory system
[2533.90s -> 2536.82s]  but we are getting there.
[2537.82s -> 2539.42s]  But there are still many gaps
[2539.42s -> 2541.98s]  like how to handle like episodic memory
[2541.98s -> 2544.64s]  like the spatial temporal aspects of things
[2544.64s -> 2547.14s]  which is central to human memory.
[2547.14s -> 2549.50s]  We don't have a good solution yet.
[2549.54s -> 2553.86s]  Okay, so memory is the most fundamental aspects
[2553.86s -> 2555.26s]  in my opinion.
[2555.26s -> 2560.26s]  Then let's get back to another very fundamental aspect
[2561.10s -> 2565.52s]  which can be built on top of memory, which is reasoning.
[2566.76s -> 2569.02s]  And our discussion today will be mainly based
[2569.02s -> 2569.86s]  on this paper,
[2569.86s -> 2573.94s]  Gracking of Implicit Relations in Transformers.
[2573.94s -> 2577.98s]  It's led by the student from our group, Boshi Wang
[2577.98s -> 2581.18s]  and it's published in NeurIPS this year, last year.
[2583.90s -> 2587.40s]  So we will be talking about implicit reasoning.
[2588.94s -> 2592.82s]  In implicit reasoning, we don't do chain of thoughts.
[2592.82s -> 2596.78s]  So there is no verbalized chain of thoughts explicitly.
[2596.78s -> 2601.34s]  We ask the LRM to directly predict the NC.
[2601.34s -> 2604.20s]  In this example of compositional reasoning,
[2604.52s -> 2609.52s]  you let's say the language model has memorized
[2609.52s -> 2612.28s]  or know these two atomic facts
[2612.28s -> 2614.72s]  that Barack's wife is Michelle
[2614.72s -> 2617.84s]  and Michelle was born in 1964.
[2617.84s -> 2621.64s]  Then the model will be given this input
[2621.64s -> 2624.20s]  like Barack's wife born in
[2624.20s -> 2628.48s]  and is asked to directly predict the NC 1964
[2628.48s -> 2631.22s]  in this compositional reasoning fashion.
[2631.86s -> 2633.22s]  Right?
[2633.22s -> 2634.44s]  So if you do chain of thoughts,
[2634.44s -> 2635.94s]  of course you can try to recall,
[2635.94s -> 2638.50s]  hey, Barack's wife is Michelle,
[2638.50s -> 2641.46s]  Michelle was born in this, so therefore the answer is this.
[2641.46s -> 2645.34s]  But we want to push the model to implicitly
[2645.34s -> 2648.42s]  just using its parameters in a single forward pass
[2648.42s -> 2649.76s]  to do this reasoning.
[2650.70s -> 2651.86s]  Why?
[2651.86s -> 2655.34s]  Like COT, especially this long COT
[2655.34s -> 2659.26s]  is all the rage right now, like in R1 or O1.
[2659.26s -> 2662.16s]  Then why does implicit reasoning matter?
[2664.50s -> 2667.14s]  This is also what I would mean in the beginning,
[2667.14s -> 2670.74s]  like why this reasoning mechanism of these LRM's
[2670.74s -> 2674.22s]  is mostly just token generation,
[2674.22s -> 2676.46s]  but implicit reasoning is still part
[2676.46s -> 2678.36s]  of the reasoning repertoire.
[2679.78s -> 2682.50s]  So why does implicit reasoning matter?
[2682.50s -> 2687.50s]  First of all, this is the default mode of training
[2688.50s -> 2692.18s]  or pre-training because during training time,
[2692.18s -> 2696.18s]  when the model is asked to predict the next token
[2696.18s -> 2699.90s]  with cross entropy loss, there is no COT, right?
[2699.90s -> 2701.38s]  As it is not for now.
[2701.38s -> 2706.38s]  So in training time, model has to compress the data,
[2706.94s -> 2709.30s]  has to do implicit reasoning
[2709.30s -> 2714.30s]  in order to reasonably predict the next token.
[2714.62s -> 2716.14s]  So no matter what,
[2716.14s -> 2718.78s]  I think it's important to understand
[2718.78s -> 2722.54s]  the implicit reasoning capability of managed models.
[2723.86s -> 2726.54s]  Also like the implicit reasoning fundamentally determines
[2726.54s -> 2730.62s]  how well these models acquire structured representations
[2730.62s -> 2734.06s]  of the facts and the rules from their training data.
[2734.06s -> 2738.62s]  And finally, there's a lot of speculation
[2738.62s -> 2742.86s]  about how does this O1 or O1 style long COT
[2742.86s -> 2744.62s]  emerge from RL.
[2745.18s -> 2749.46s]  I think one possible hypothesis as it is in my mind
[2749.46s -> 2754.46s]  is like this, you start with a capable base model,
[2754.50s -> 2756.70s]  if the base model is not capable enough,
[2756.70s -> 2759.18s]  then RL won't do much.
[2759.18s -> 2762.78s]  So you start with a capable base model, like deep C3,
[2762.78s -> 2764.90s]  and it probably has already learned
[2764.90s -> 2769.90s]  all of these basic constructs or strategies for reasoning.
[2770.18s -> 2773.10s]  So like that reflection, like analogical reasoning
[2773.10s -> 2774.52s]  and so on and so forth.
[2775.40s -> 2778.28s]  Maybe in an implicit reasoning fashion,
[2778.28s -> 2782.64s]  just some kind of reasoning circuits in the parameters.
[2782.64s -> 2786.56s]  Then reinforcement learning with these verifiable rewards
[2786.56s -> 2789.60s]  is just to incentivize the model
[2789.60s -> 2793.08s]  to learn to use the right combination of strategies.
[2793.08s -> 2797.52s]  But it's not learning new reasoning strategies through RL.
[2797.52s -> 2800.32s]  And then it also encourages the model to keep trying,
[2800.32s -> 2801.20s]  don't be lazy.
[2801.92s -> 2805.76s]  Right, so if this hypothesis is true,
[2805.76s -> 2809.10s]  then understand how these models,
[2809.10s -> 2811.40s]  how these different reasoning strategies work
[2811.40s -> 2814.16s]  within the model becomes even more important.
[2816.76s -> 2820.12s]  Right, so back to implicit reasoning.
[2820.12s -> 2822.60s]  Before our work, there were a bunch of work,
[2822.60s -> 2826.48s]  a great work that shows that RLMs truly struggle
[2826.48s -> 2828.06s]  with implicit reasoning.
[2828.98s -> 2831.14s]  Some people show that it struggles
[2831.14s -> 2834.26s]  with compositional reasoning like this.
[2834.26s -> 2837.34s]  There is a famous compositionality gap.
[2837.34s -> 2840.18s]  And then some other people show that RLMs,
[2840.18s -> 2842.14s]  even like G54 at the time,
[2842.14s -> 2845.74s]  struggled with simple things like comparative reasoning
[2845.74s -> 2846.78s]  or comparison.
[2846.78s -> 2850.92s]  Like Trump is 78, Biden is 82,
[2850.92s -> 2855.02s]  then Trump is younger or older than Biden.
[2855.02s -> 2860.02s]  So that was like the previous conclusions,
[2863.90s -> 2868.44s]  but we had different opinions.
[2868.44s -> 2872.86s]  It seems to us like these all like kind of contribute
[2872.86s -> 2876.26s]  to like this autoregressive RLMs
[2876.26s -> 2880.74s]  and never truly reason or plan kind of narrative.
[2880.74s -> 2885.46s]  But we had some different belief.
[2885.46s -> 2888.30s]  We have more faith, more confidence
[2888.30s -> 2890.62s]  in language models and transformers.
[2890.62s -> 2895.62s]  So we want to more carefully study this problem
[2895.78s -> 2898.74s]  than to see whether we can get some new insights.
[2898.74s -> 2901.12s]  So we started with these research questions.
[2901.12s -> 2905.54s]  First, can transformers learn to reason implicitly?
[2905.54s -> 2907.96s]  Or are there fundamental limitations
[2907.96s -> 2911.74s]  that prohibit robust acquisition of this skill?
[2912.72s -> 2914.76s]  And then what factors,
[2914.76s -> 2916.60s]  like is that the scale of the training data,
[2916.60s -> 2918.46s]  the distribution of the training data,
[2918.46s -> 2921.96s]  or the model architecture that control the acquisition
[2921.96s -> 2923.76s]  of implicit reasoning?
[2925.42s -> 2929.38s]  So with these two questions, let's discuss the setup.
[2931.82s -> 2935.20s]  The model and optimization are pretty standard.
[2935.32s -> 2939.52s]  We want to, I don't want to introduce questions here.
[2939.52s -> 2941.38s]  So it's just like a standard decoder
[2941.38s -> 2945.64s]  only transformer in GPT-2's architecture.
[2945.64s -> 2947.96s]  And we'll also show that the results are pretty robust,
[2947.96s -> 2949.32s]  two different model scales.
[2949.32s -> 2951.12s]  So you can have a deeper model,
[2951.12s -> 2953.42s]  but the conclusions are the same.
[2953.42s -> 2956.96s]  And then the optimization is also pretty standard.
[2958.70s -> 2963.58s]  Then for the data we'll be using will be synthetic data
[2963.58s -> 2967.26s]  because we want to carefully control all the factors
[2967.26s -> 2968.86s]  in this investigation
[2968.86s -> 2972.62s]  so that we can separate out the problem
[2972.62s -> 2973.62s]  we want to study.
[2974.86s -> 2977.60s]  So you can imagine, like let's just consider composition.
[2977.60s -> 2979.78s]  We study both composition and comparison,
[2979.78s -> 2982.68s]  but let's focus on composition for now.
[2982.68s -> 2985.74s]  The data will look like just a random large graph.
[2985.74s -> 2989.74s]  It has ifs, ats, ands, ands, ors, ors, ors.
[2989.74s -> 2991.58s]  We set r to 200.
[2992.42s -> 2994.74s]  So then it's a bunch of notes
[2994.74s -> 2997.08s]  and then these different relations like this.
[2998.48s -> 3002.28s]  Then it has, these are the atomic facts, right?
[3002.28s -> 3006.98s]  Like superstition, the scenario is Stevie.
[3006.98s -> 3009.42s]  And then we can use these atomic facts
[3009.42s -> 3011.86s]  to get some inferred facts
[3011.86s -> 3013.98s]  or these two hop compositions
[3013.98s -> 3016.78s]  following the composition rule.
[3016.78s -> 3021.22s]  Like if the head entity, r1, bridge entity,
[3021.74s -> 3024.40s]  and then bridge entity, r2, tail entity,
[3024.40s -> 3026.58s]  then we would have these composition effects
[3026.58s -> 3029.62s]  like head entity, r1, r2, tail entity.
[3029.62s -> 3034.62s]  So it's like Barack wife born in 1964, right?
[3038.90s -> 3042.46s]  So this composition and you can think like comparison,
[3042.46s -> 3044.30s]  the data for comparison is similar.
[3046.82s -> 3049.26s]  Then the important setup here
[3049.26s -> 3052.90s]  is that we want to study inductive learning
[3052.90s -> 3055.92s]  of deduction rules, right?
[3055.92s -> 3059.10s]  A fancy phrase, but let me just decompose this for you.
[3060.30s -> 3061.90s]  First of all, we want the model,
[3061.90s -> 3064.54s]  which is the decoder-only transformer
[3064.54s -> 3066.06s]  to learn inductively.
[3066.06s -> 3068.46s]  So just give a bunch of training examples
[3068.46s -> 3071.82s]  we want to learn these rules
[3071.82s -> 3074.10s]  and particularly these are deduction rules.
[3074.10s -> 3077.14s]  So this is a typical deduction rule, right?
[3077.14s -> 3078.62s]  You start from some premises
[3078.82s -> 3082.66s]  and then you can deduce a new facts, right?
[3082.66s -> 3085.40s]  So this is inductive learning of deduction rules.
[3087.38s -> 3092.38s]  Then there are two generalization settings,
[3094.02s -> 3096.84s]  what I would call the in-distribution setting
[3096.84s -> 3099.14s]  and then the out-of-distribution setting
[3099.14s -> 3101.38s]  or this setting can also be called
[3101.38s -> 3103.90s]  systematic generalization.
[3103.90s -> 3106.18s]  So it goes like this.
[3106.18s -> 3107.56s]  For the atomic facts,
[3107.56s -> 3109.36s]  like those edges in the log graph,
[3110.38s -> 3112.44s]  we split it into two sets,
[3112.44s -> 3116.38s]  the ID set and the OOD set.
[3116.38s -> 3118.86s]  Then they all go through the same rule,
[3118.86s -> 3120.68s]  like the composition rule.
[3120.68s -> 3124.86s]  You get to the corresponding inferred facts, right?
[3124.86s -> 3127.40s]  Then for this set of inferred facts
[3127.40s -> 3129.76s]  from the ID atomic facts,
[3129.76s -> 3131.68s]  we split it into two sets,
[3131.68s -> 3136.68s]  the training set and the type ID test set, right?
[3137.80s -> 3142.80s]  So for these inferred facts in the ID test sets,
[3146.16s -> 3148.16s]  even though we have not seen
[3148.16s -> 3152.56s]  these exact inferred facts in our training data,
[3152.56s -> 3154.64s]  so it's an unseen inferred fact,
[3154.64s -> 3158.36s]  but we have seen all of its constitutes,
[3158.36s -> 3161.84s]  like those atomic facts being composed
[3161.84s -> 3166.84s]  with other atomic facts in these ID training sets, right?
[3167.12s -> 3169.48s]  I know it's a bit of a mouthful,
[3169.48s -> 3171.16s]  but just bear with me.
[3171.16s -> 3176.00s]  So for any of these inferred facts in the ID test set,
[3176.00s -> 3179.96s]  like Barack's wife,
[3179.96s -> 3182.52s]  born in 1964,
[3182.52s -> 3186.40s]  maybe you have not seen that exact inferred fact
[3186.40s -> 3187.72s]  in your training data,
[3187.72s -> 3190.52s]  but you have seen, for example, the relation,
[3192.08s -> 3195.00s]  like Barack's wife, Michelle,
[3195.00s -> 3197.32s]  you have seen these atomic facts
[3197.32s -> 3199.92s]  being composed with other relations.
[3199.92s -> 3203.56s]  Also, similarly, you have seen the other atomic facts,
[3203.56s -> 3206.26s]  like Michelle, born in 1964,
[3206.26s -> 3209.48s]  being composed with other atomic facts.
[3209.48s -> 3211.78s]  You just haven't happened to see
[3211.78s -> 3213.36s]  that exact inferred fact,
[3213.36s -> 3218.28s]  like Barack's wife, born in 1964,
[3218.28s -> 3221.98s]  so that you still need some generalization to capture them.
[3222.98s -> 3226.18s]  But for the OD test sets,
[3226.18s -> 3229.74s]  then you still have seen all of the atomic facts
[3229.74s -> 3231.58s]  because otherwise the model doesn't know
[3231.58s -> 3234.74s]  that these facts exist,
[3234.74s -> 3237.90s]  but you have never seen any of those atomic facts
[3238.90s -> 3241.90s]  being composed in a compositional fashion,
[3241.90s -> 3244.88s]  like being composed with other relations, right?
[3244.88s -> 3246.26s]  So that, in that sense,
[3246.26s -> 3248.34s]  is a stronger generalization setting,
[3248.34s -> 3251.18s]  so that's why it's called a systematic generalization.
[3251.22s -> 3253.16s]  If the model can do this,
[3253.16s -> 3255.02s]  then essentially it means that the model
[3255.02s -> 3257.94s]  has truly learned this rule, this deduction rule,
[3257.94s -> 3259.90s]  so that you can just apply it
[3259.90s -> 3264.18s]  to any arbitrary new facts, right?
[3264.18s -> 3265.56s]  So that's really the goal
[3265.56s -> 3268.94s]  for this learning of deduction rules.
[3271.22s -> 3274.58s]  Now, with all this DANFI setup in mind,
[3274.58s -> 3276.50s]  let's look at some thumb parts,
[3276.50s -> 3280.98s]  look at some interesting takeaways from our investigation.
[3281.74s -> 3285.18s]  The first surprising conclusion we found
[3285.18s -> 3290.18s]  is that transformers can learn to reason implicitly,
[3290.66s -> 3294.56s]  but only through a process called grokking.
[3295.66s -> 3296.66s]  What is grokking?
[3298.08s -> 3302.42s]  So let's look at this figure for now.
[3302.42s -> 3304.82s]  This curve is the training accuracy.
[3304.82s -> 3307.50s]  You can see it quickly goes to 100%,
[3307.50s -> 3309.74s]  that means the model,
[3309.74s -> 3313.06s]  the training has overfit at these points.
[3313.06s -> 3317.94s]  But if you look at the test, the ID test curve, right?
[3317.94s -> 3321.14s]  When the model first overfits here,
[3321.14s -> 3323.72s]  the test accuracy was still very low.
[3324.64s -> 3327.30s]  And if you keep training the model
[3327.30s -> 3331.98s]  way beyond the saturation or overfitting,
[3331.98s -> 3335.38s]  and the overfitting happens around 10,000 steps,
[3335.38s -> 3340.38s]  and if you train it like 20 times more optimization steps,
[3341.98s -> 3344.10s]  like to these points,
[3344.10s -> 3347.80s]  this is large scale, for example, keep in mind,
[3347.80s -> 3352.74s]  then all of a sudden, generalization happens, right?
[3352.74s -> 3356.74s]  This, at least the ID generalization happens, right?
[3356.74s -> 3359.78s]  So the model gets to 100% test accuracy
[3359.78s -> 3362.58s]  on composition in the ID splits.
[3363.50s -> 3365.26s]  Similarly for comparison,
[3366.26s -> 3368.58s]  the overfitting happens very early on,
[3368.58s -> 3373.34s]  but it takes about 20 more times of train optimization
[3373.34s -> 3375.90s]  steps for generalization to happen.
[3375.90s -> 3378.18s]  And interestingly for comparison,
[3378.18s -> 3381.66s]  all the test accuracy also get to 100%.
[3381.66s -> 3386.10s]  So that's a problem we'll look into later, right?
[3387.52s -> 3390.86s]  So that's quite interesting, right?
[3390.86s -> 3395.46s]  These transformers, they can learn to reason implicitly,
[3395.46s -> 3398.16s]  but only through blocking.
[3398.16s -> 3400.42s]  And it's one of the first studies
[3400.42s -> 3403.62s]  that establishes this connection between blocking
[3403.62s -> 3405.76s]  and implicit reasoning, I believe.
[3407.44s -> 3411.14s]  And we will investigate why that happens in a second.
[3412.50s -> 3416.80s]  Another immediate takeaway here is that, as we just said,
[3416.80s -> 3421.80s]  the level of system necessity of generalization varies
[3422.72s -> 3424.36s]  by the reason type.
[3424.36s -> 3426.60s]  For compositional reasoning,
[3426.60s -> 3431.08s]  it never managed to learn to generalize OOD.
[3431.08s -> 3433.12s]  Well, for comparative reasoning,
[3433.12s -> 3436.14s]  OOD generalization did happen.
[3436.14s -> 3438.44s]  So we want to understand why
[3438.44s -> 3440.20s]  there is such a difference as well.
[3442.22s -> 3446.76s]  Then another interesting takeaway here is that
[3447.68s -> 3450.80s]  before this study, there were already some studies
[3450.80s -> 3452.56s]  that look at blocking,
[3452.56s -> 3456.58s]  and then I'll try to understand why blocking happens
[3456.58s -> 3460.52s]  and under what conditions would blocking happen.
[3460.52s -> 3463.56s]  Then one common belief in the literature
[3463.56s -> 3468.46s]  was that there is a critical data size.
[3468.46s -> 3470.76s]  Like once your total amount of data
[3470.76s -> 3474.08s]  surpasses a certain threshold, then blocking happens,
[3474.08s -> 3475.60s]  otherwise it won't happen.
[3476.52s -> 3479.00s]  But in our study, we tried to study
[3479.00s -> 3483.24s]  this hypothesis very carefully.
[3483.24s -> 3486.84s]  Then we find that actually it's not the data size,
[3486.84s -> 3490.80s]  but the data distribution that matters.
[3490.80s -> 3493.82s]  So in these experiments, on the right,
[3498.04s -> 3500.64s]  so there is two variables.
[3500.64s -> 3502.32s]  There is the total data size,
[3502.32s -> 3505.00s]  which is controlled by the number of entities,
[3505.28s -> 3507.28s]  and it's proportional to the total data,
[3507.28s -> 3508.78s]  number of training examples.
[3508.78s -> 3511.36s]  And then there is bi,
[3511.36s -> 3516.36s]  which is the ratio of the inferred versus atomic facts.
[3517.20s -> 3519.12s]  So if this number is larger,
[3519.12s -> 3521.44s]  that means that you have more inferred facts
[3521.44s -> 3524.20s]  than you have atomic facts.
[3525.12s -> 3528.40s]  So on the right, we keep the ratio fixed
[3528.40s -> 3530.98s]  and increase the number of entities
[3530.98s -> 3532.60s]  or the total data size.
[3532.60s -> 3535.88s]  Then you will see the speed of generalization
[3535.88s -> 3537.64s]  is roughly the same
[3537.64s -> 3541.30s]  that when you increase the total data size.
[3543.96s -> 3545.80s]  It's more or less the same.
[3545.80s -> 3550.80s]  However, when you keep the data size fixed,
[3550.96s -> 3554.04s]  so it's the same number of entities,
[3554.04s -> 3555.96s]  you just change the ratio file,
[3556.92s -> 3560.74s]  like from 3.6 to 18.
[3560.74s -> 3564.50s]  Then you see the speed of generalization
[3566.46s -> 3569.86s]  strongly positively correlates with this ratio.
[3570.74s -> 3572.26s]  If you have a higher ratio,
[3572.26s -> 3576.00s]  then the generalization will happen faster.
[3576.00s -> 3578.10s]  And at some point it will be as fast
[3578.10s -> 3581.10s]  as the overfitting of the training data.
[3582.42s -> 3583.70s]  So this really shows that
[3583.70s -> 3586.42s]  it's the critical data distribution that matters,
[3586.42s -> 3588.80s]  not for the sheer size of your training data.
[3591.58s -> 3594.86s]  Oh, with all those interesting takeaways,
[3595.76s -> 3598.46s]  our job is not done here yet
[3598.46s -> 3601.46s]  because there are still some very important questions
[3601.46s -> 3602.68s]  we have not answered.
[3603.70s -> 3606.18s]  So why does blocking happen?
[3606.18s -> 3610.62s]  What exactly happens during this long blocking process?
[3610.62s -> 3613.66s]  What's going on within the model?
[3613.66s -> 3616.94s]  And why does the level of systematicity
[3616.94s -> 3620.54s]  in generalization vary by the recent time?
[3621.18s -> 3622.74s]  So all of these questions require
[3622.74s -> 3625.10s]  a deeper look inside of the model.
[3626.26s -> 3630.24s]  That's the mechanistic interpretation part of this study.
[3632.52s -> 3635.08s]  So we will use some popular
[3635.08s -> 3639.14s]  and nowadays very standard mechanistic interpretation tools.
[3639.14s -> 3641.78s]  One is a modular lens.
[3641.78s -> 3646.02s]  So here we apply the internal states
[3646.02s -> 3650.66s]  at some position within the transformer
[3650.66s -> 3655.14s]  and multiply it with the output embedding metrics
[3655.14s -> 3657.86s]  so that we can get a distribution
[3657.86s -> 3659.34s]  or with output vocabulary.
[3659.34s -> 3661.18s]  So we kind of get a peek
[3661.18s -> 3665.08s]  into what this internal representation is about.
[3666.24s -> 3670.50s]  Then we will also use so-called causal tracing.
[3670.78s -> 3675.78s]  In a nutshell, this technique allows you
[3678.34s -> 3683.34s]  to quantitatively measure the amount of impact
[3686.42s -> 3691.42s]  of a certain internal state to the outputs.
[3692.48s -> 3695.70s]  So it ranges from zero to one.
[3695.70s -> 3697.26s]  If it's closer to one,
[3697.26s -> 3699.98s]  then that means that this position,
[3699.98s -> 3702.84s]  this internal state has a larger impact
[3702.84s -> 3704.32s]  on the final decision.
[3705.50s -> 3708.42s]  I won't get into the very details
[3708.42s -> 3710.38s]  because that will take too much time.
[3712.96s -> 3716.48s]  So with this mechanistic interpretation techniques,
[3716.48s -> 3720.82s]  now we can try to find or discover rather
[3720.82s -> 3724.58s]  that the corresponding generalizing circuits
[3725.46s -> 3729.30s]  formed for different reasoning types.
[3729.30s -> 3733.24s]  Then this is what we found, very, very beautiful.
[3733.24s -> 3737.66s]  We'll find that for composition, after grocking,
[3737.66s -> 3742.38s]  the transformer will learn a generalizing circuit like this
[3742.38s -> 3745.38s]  it's a kind of stage circuit, right?
[3745.38s -> 3747.18s]  So this is the input to the transformer.
[3747.18s -> 3750.02s]  You have the head entity R1 and R2
[3750.02s -> 3752.14s]  and then this is the layer zero,
[3752.14s -> 3754.62s]  this is layer eight, the final layer.
[3755.66s -> 3760.66s]  Then the first few layers of the transformer
[3760.92s -> 3762.64s]  will essentially do two things.
[3763.62s -> 3767.42s]  For these parts, it will process the first hop
[3767.42s -> 3770.26s]  essentially to look at H and R1
[3770.26s -> 3773.50s]  and then find the bridge entity P.
[3773.50s -> 3775.34s]  Here we're using logical dance here.
[3776.66s -> 3780.74s]  So it has memorized these atomic facts
[3780.74s -> 3783.18s]  in the first few layers to the extent
[3783.18s -> 3787.32s]  that it can reliably predict P at layer five.
[3788.50s -> 3793.22s]  Then another thing that needs to be done here
[3793.22s -> 3797.70s]  is that it needs to defer the processing of R2.
[3797.70s -> 3799.98s]  So you cannot forget about R2
[3799.98s -> 3802.30s]  because R2 needs to be used later.
[3802.30s -> 3804.86s]  Even though it doesn't need to use R2
[3804.86s -> 3808.26s]  in the earlier layers, it needs to use R2
[3808.26s -> 3811.70s]  when it has found B, the bridge entity.
[3811.70s -> 3814.78s]  So once the bridge entity is identified
[3814.78s -> 3817.22s]  then in the internal states,
[3817.22s -> 3819.66s]  then it can be combined with R2
[3819.66s -> 3822.90s]  and then it has memorized the atomic facts
[3822.90s -> 3826.80s]  that B, R2 is T.
[3826.80s -> 3829.42s]  So then you can combine the bridge entity B
[3829.42s -> 3833.26s]  and the delayed processing of R2 to predict T.
[3834.34s -> 3838.10s]  So that's the generalizing circuit or composition.
[3838.78s -> 3840.78s]  It has two clear stages
[3840.78s -> 3843.90s]  and that will also determine its generalization behavior
[3843.90s -> 3845.48s]  that it will analyze later.
[3846.54s -> 3848.18s]  Well, on the other hand,
[3848.18s -> 3851.48s]  the comparison relationship or reasoning
[3851.48s -> 3854.86s]  has a different circuit, what we call a parallel circuit.
[3855.76s -> 3857.26s]  Remember for comparison,
[3857.26s -> 3860.86s]  it's like you have two entities, E1 and E2.
[3860.86s -> 3864.50s]  You know some value of some attributes of this entity,
[3864.50s -> 3868.86s]  like this is Trump, this is Biden,
[3868.86s -> 3873.86s]  and then Trump is V1, is 78, Biden is 82.
[3874.26s -> 3878.10s]  Then the prediction is who is older.
[3878.10s -> 3883.10s]  It's like Trump is younger or older or equals than Biden.
[3886.64s -> 3890.46s]  Then the circuit here is more of a parallel circuit.
[3890.46s -> 3893.22s]  So the first few layers of a transformer
[3893.42s -> 3898.42s]  learn to in parallel retrieve the attribute values,
[3898.70s -> 3901.30s]  like 78 and 82.
[3901.30s -> 3906.00s]  Then the upper layers will use this to retrieve the values
[3906.00s -> 3911.00s]  to do the comparison and then to predict the final answer,
[3911.16s -> 3913.74s]  whether it's smaller than, equal or larger.
[3916.18s -> 3920.22s]  So given the reason through mechanics interpretation,
[3920.22s -> 3924.38s]  we can find the generalizing circuits configuration
[3924.38s -> 3928.78s]  of different reasoning types, and they are indeed different.
[3928.78s -> 3932.22s]  And this different configuration will determine
[3932.22s -> 3935.76s]  their level of systematicity of generalization.
[3938.98s -> 3943.98s]  Before that, let me share a simple way to fix
[3944.16s -> 3947.82s]  or to improve this systematic generalization,
[3947.82s -> 3950.40s]  especially for composition.
[3952.70s -> 3955.14s]  So as we showed earlier for composition,
[3955.14s -> 3958.62s]  all the generalization never happened.
[3958.62s -> 3959.62s]  Why?
[3959.62s -> 3962.86s]  If you look at this circuit and think about it,
[3962.86s -> 3966.50s]  that becomes kind of obvious.
[3967.46s -> 3971.22s]  So for the old generalization to happen,
[3972.46s -> 3975.10s]  the model needs to do a few things.
[3975.10s -> 3979.26s]  It needs to first memorize a copy
[3979.26s -> 3983.66s]  of the first hop atomic effects, like H, R1B
[3983.66s -> 3986.80s]  in the lower layers of the transformer.
[3986.80s -> 3990.34s]  So that you can find the bridge entity at the layer five.
[3990.34s -> 3994.78s]  Then it also needs to store a copy
[3994.78s -> 3998.02s]  of the second hop atomic effect,
[3998.02s -> 4000.46s]  but in the upper layers,
[4000.46s -> 4003.76s]  because the second hop has delayed processing.
[4003.76s -> 4007.84s]  So it needs to somehow store the atomic effect,
[4007.84s -> 4011.76s]  B, R2, T here, not in the lower layer,
[4011.76s -> 4013.00s]  but in the upper layer.
[4014.28s -> 4016.78s]  However, for the OOD generalization,
[4016.78s -> 4020.00s]  remember our definition of OOD is that
[4020.00s -> 4023.00s]  none of these atomic effects has been seen
[4023.00s -> 4026.22s]  during training to be composed with other facts.
[4027.36s -> 4032.00s]  In that situation, the model has no incentive
[4032.00s -> 4036.16s]  to store the second hop atomic effect in the upper layers.
[4037.16s -> 4041.16s]  It only seen this atomic effect individually,
[4041.16s -> 4045.66s]  so it can easily memorize them in the lower layers,
[4045.66s -> 4050.08s]  but it doesn't have the incentive to store another copy
[4050.08s -> 4054.68s]  in the upper layers, because that requires extra effort.
[4054.68s -> 4059.68s]  That's why OOD generalization of composition never happens.
[4059.72s -> 4061.38s]  Well, for comparison,
[4061.38s -> 4063.74s]  you don't have this stage of processing.
[4063.74s -> 4067.76s]  So you only need to store one copy of the atomic effect,
[4067.76s -> 4070.02s]  and you can retrieve that the value
[4070.02s -> 4074.04s]  in the lower layers here for the comparison.
[4074.04s -> 4075.68s]  So you don't have that issue,
[4075.68s -> 4078.76s]  and that's why OOD generalization of comparison
[4078.76s -> 4079.60s]  doesn't happen.
[4081.48s -> 4084.50s]  And to further validate these hypothesis,
[4084.50s -> 4087.62s]  we did an intervention here.
[4087.62s -> 4090.10s]  So if that is indeed the case,
[4090.10s -> 4092.66s]  that is because the model doesn't have an incentive
[4092.66s -> 4096.70s]  to store this atomic effect in the upper layers,
[4096.70s -> 4099.94s]  then we just need to do some parameter sharing,
[4099.94s -> 4102.50s]  some cross-layer parameter sharing.
[4102.50s -> 4106.88s]  So you kind of tie the weights of the lower layers
[4106.88s -> 4108.86s]  with the weights of the upper layer,
[4108.86s -> 4110.98s]  so it's parameter sharing.
[4110.98s -> 4113.66s]  And then if the lower layer has memorized
[4113.66s -> 4114.62s]  the atomic effects,
[4114.66s -> 4118.06s]  then the upper layer will get a copy of it as well.
[4118.06s -> 4119.62s]  So we did that intervention,
[4119.62s -> 4122.14s]  and it turns out, boom,
[4122.14s -> 4126.74s]  OOD generalization starts to happen for composition.
[4126.74s -> 4129.22s]  So this further validates the hypothesis.
[4129.22s -> 4130.42s]  It's a bit dense,
[4130.42s -> 4131.82s]  but I think that this is probably
[4131.82s -> 4133.58s]  one of the most interesting slides
[4133.58s -> 4136.12s]  of this analysis of this work.
[4138.40s -> 4141.80s]  So remember,
[4141.80s -> 4146.44s]  we try to understand
[4146.44s -> 4150.68s]  what exactly is going on during grocking.
[4150.68s -> 4153.64s]  We said that the grocking process
[4153.64s -> 4158.64s]  is when these generalizing circuits start to form,
[4159.50s -> 4161.48s]  but in what exact way?
[4161.48s -> 4163.08s]  That's still unclear.
[4163.08s -> 4167.22s]  So through causal tracing,
[4167.22s -> 4170.10s]  we can actually identify
[4170.10s -> 4172.92s]  what exactly is going on during grocking.
[4175.52s -> 4180.52s]  Let's first focus on this figure.
[4190.64s -> 4193.06s]  Or let me first say this.
[4193.06s -> 4198.06s]  So we believe grocking is the phase transition
[4198.72s -> 4201.94s]  from root learning to generalization.
[4201.94s -> 4206.94s]  So grocking starts when the model
[4207.52s -> 4209.42s]  has overfit the trained data.
[4210.26s -> 4212.78s]  But at that point,
[4212.78s -> 4216.22s]  the model has only done root learning.
[4216.22s -> 4219.78s]  So it just would have forced the memorized
[4219.78s -> 4220.88s]  or the training data,
[4220.88s -> 4222.88s]  including the input facts,
[4222.88s -> 4225.84s]  but it's not actually in a generalizable way.
[4226.62s -> 4228.22s]  Because it has memorized all of them,
[4228.22s -> 4230.00s]  so the training loss will be zero.
[4231.52s -> 4232.34s]  But it doesn't mean
[4232.34s -> 4235.40s]  that it has captured this generalizing circuit.
[4235.40s -> 4238.84s]  Then the grocking process is essentially
[4238.84s -> 4243.24s]  the phase transition from the initial root learning
[4243.24s -> 4245.32s]  to the generalization.
[4245.32s -> 4248.34s]  It's where these generalizing circuits get formed.
[4249.18s -> 4253.34s]  And now let's analyze how exactly it gets formed.
[4253.86s -> 4258.12s]  The first thing we will look at is this figure.
[4259.66s -> 4263.36s]  We look at two things.
[4263.36s -> 4268.36s]  So S5, R1 is this position.
[4271.34s -> 4274.86s]  So this is S stands for state.
[4274.86s -> 4279.86s]  So this is a state of layer five as the position of R1.
[4279.90s -> 4282.64s]  So it's this one, S5, R1.
[4284.22s -> 4288.94s]  Then we can use logic advance to decode
[4288.94s -> 4293.90s]  what's captured in that state representation,
[4293.90s -> 4295.50s]  in that state.
[4295.50s -> 4298.42s]  You can see the grocking process.
[4299.34s -> 4302.94s]  And similarly, we can do that for S5, R2.
[4302.94s -> 4304.58s]  So this position.
[4305.82s -> 4308.98s]  So once grocking starts,
[4308.98s -> 4311.74s]  the model already has,
[4311.74s -> 4313.14s]  and this is MRR,
[4313.78s -> 4315.70s]  so it's the main receiver called rank.
[4315.70s -> 4320.62s]  But basically it just tells you like this bridge-end CP,
[4320.62s -> 4325.02s]  the ranking of the bridge-end CP in the logic advance.
[4326.02s -> 4329.02s]  If it's one, it means that the state here
[4329.02s -> 4332.70s]  will always predict the bridge-end CP.
[4332.70s -> 4335.46s]  So that means that it has encoded this information.
[4335.46s -> 4339.14s]  So when the grocking starts, B is already there.
[4339.14s -> 4341.10s]  The MRR equals one.
[4341.10s -> 4343.38s]  So that means this state here
[4343.38s -> 4345.82s]  always predict the bridge-end CP,
[4345.82s -> 4347.92s]  which is what we want, right?
[4347.92s -> 4352.14s]  We want the first hop to get us the bridge-end CP.
[4352.14s -> 4353.48s]  That is great.
[4353.48s -> 4356.42s]  But remember, we also need the model
[4356.42s -> 4360.24s]  to do a delay the processing of R2,
[4360.24s -> 4361.58s]  so that at this point,
[4361.58s -> 4366.04s]  you can combine B and R2 to predict the tail entity.
[4366.04s -> 4371.04s]  But if you look at MRR of R2 at this position,
[4374.48s -> 4376.12s]  when grocking starts,
[4377.12s -> 4381.44s]  it does not have R2 there.
[4381.44s -> 4384.48s]  So it has a B here, but it doesn't have R2 here.
[4385.88s -> 4389.44s]  Despite that, the model has a train,
[4389.44s -> 4391.28s]  so this is on the training set.
[4391.28s -> 4394.62s]  So the model has a training loss of one, of zero.
[4394.62s -> 4399.62s]  So the model can perfectly predict the tail entity T,
[4399.78s -> 4404.78s]  despite not using R2 here to combine with B.
[4404.82s -> 4409.38s]  That means the model is just doing rote learning.
[4409.38s -> 4411.14s]  So it just memorized that,
[4411.14s -> 4416.14s]  hey, whenever I see edge R1, R2, then I will predict T.
[4416.30s -> 4419.58s]  But it's not actually doing this stage thing,
[4419.58s -> 4421.34s]  like getting the bridge entity
[4421.34s -> 4424.10s]  and then combine with R2 to get another,
[4424.62s -> 4427.66s]  use another atomic facts and do the reasoning.
[4427.66s -> 4429.70s]  But through the grocking process,
[4430.74s -> 4434.98s]  you can see the R2 gradually increases.
[4434.98s -> 4439.98s]  So at the end of grocking, it always predicts R2.
[4440.22s -> 4442.78s]  Then you have B here, you have R2 here,
[4442.78s -> 4447.10s]  then you can actually do recall the atomic facts here
[4447.10s -> 4448.86s]  to predict the tail entity T.
[4448.86s -> 4453.86s]  So this is further corroborated
[4454.98s -> 4457.58s]  by the causal tracing results here.
[4458.42s -> 4461.74s]  So remember, through causal tracing,
[4461.74s -> 4466.58s]  we can quantify the causal strength of each state
[4466.58s -> 4469.42s]  to the final prediction.
[4469.42s -> 4473.34s]  And this is the causal strength of each state
[4473.34s -> 4477.42s]  at the beginning of grocking, and this is after grocking.
[4477.42s -> 4480.66s]  Then we can do, we can calculate the difference
[4480.66s -> 4483.14s]  between the two and find their diff.
[4484.18s -> 4489.18s]  You can see the diff mainly happens here at S5 R1.
[4492.50s -> 4495.70s]  So that means in the beginning of grocking,
[4495.70s -> 4498.90s]  even though the model has the bridge entity B here,
[4498.90s -> 4500.90s]  it does not use it, right?
[4500.90s -> 4503.30s]  You just do rote learning
[4503.30s -> 4505.82s]  so it directly predicts the tail entity T.
[4505.82s -> 4508.26s]  But the grocking process is the process
[4508.26s -> 4512.18s]  of the model learning to use this bridge entity properly.
[4512.18s -> 4515.02s]  So giving it a stronger causal strength
[4515.02s -> 4516.70s]  to the final prediction.
[4516.70s -> 4520.78s]  And this combined with the fact that the,
[4520.78s -> 4523.50s]  we just shown that through roted dense,
[4523.50s -> 4526.18s]  that R2, the grocking process
[4526.18s -> 4529.62s]  is where these R2s start to emerge, right?
[4529.62s -> 4532.30s]  These combines give us,
[4533.06s -> 4537.26s]  validates the hypothesis that now this grocking process
[4537.26s -> 4540.42s]  is really the process of forming these generalizing
[4540.42s -> 4543.62s]  circuits that is staged circuits
[4543.62s -> 4545.62s]  that we just shown for composition.
[4546.66s -> 4549.94s]  And we can actually explain even though not,
[4549.94s -> 4552.42s]  we didn't do any proof of any sort,
[4552.42s -> 4554.02s]  but you can roughly understand
[4554.02s -> 4556.30s]  why this kind of grocking behavior happens
[4556.30s -> 4559.62s]  through the circuit efficiency and the regularization.
[4560.62s -> 4564.42s]  The generalizing circuits is much more efficient
[4564.42s -> 4567.22s]  than the memorizing circuit,
[4567.22s -> 4569.02s]  the circuit for rote learning.
[4570.30s -> 4574.34s]  So, and because you also have regularization, right?
[4574.34s -> 4577.38s]  The L2 regularization term, for example,
[4577.38s -> 4579.94s]  then if you just keep training,
[4579.94s -> 4581.78s]  even after overfitting,
[4581.78s -> 4584.94s]  the regularization term will decrease
[4584.94s -> 4588.70s]  and then will gradually favor the circuit
[4588.78s -> 4590.74s]  with higher efficiency,
[4590.74s -> 4593.02s]  which is the generalizing circuit.
[4593.02s -> 4595.70s]  So that's why you find this is still beneficial
[4595.70s -> 4598.50s]  to claim way over beyond overfitting
[4598.50s -> 4601.02s]  because that's when the regularization starts to kick in
[4601.02s -> 4604.22s]  and gets you the more efficient circuit.
[4605.30s -> 4608.22s]  But I think, at least let me give the definition
[4608.22s -> 4610.18s]  of planning because that's in itself
[4610.18s -> 4612.70s]  is a very confusing thing for various reasons.
[4613.58s -> 4615.90s]  So in the context of the language agents,
[4615.94s -> 4619.14s]  we'll work with this simplified definition of planning.
[4619.14s -> 4622.54s]  Even a GoG decides on the sequence of actions,
[4622.54s -> 4626.22s]  A0 to An, that will lead to a state
[4626.22s -> 4630.42s]  that will pass the Go test G, right?
[4630.42s -> 4632.62s]  So of course, this is over simplified
[4632.62s -> 4634.46s]  because we don't talk about the state space,
[4634.46s -> 4639.18s]  the observation space, and then the actions have,
[4639.18s -> 4640.86s]  they are not just atomic actions,
[4640.86s -> 4644.02s]  they have preconditions that have to be met
[4644.02s -> 4646.14s]  before they can be taken and so on and so forth.
[4646.14s -> 4648.10s]  But for this purpose,
[4648.10s -> 4651.30s]  I think that it's enough to have this definition.
[4651.30s -> 4655.18s]  Then with this definition,
[4655.18s -> 4658.14s]  we can analyze these general trends
[4658.14s -> 4660.66s]  in planning settings for language agents
[4660.66s -> 4663.14s]  compared with the classic planning setting,
[4663.14s -> 4665.78s]  which has been studied for decades.
[4667.06s -> 4670.58s]  Generally, I think the expressiveness
[4670.58s -> 4674.38s]  in Go specification is drastically increasing.
[4674.38s -> 4678.22s]  So now we can express our Go in natural language
[4678.22s -> 4680.46s]  as opposed to some formal language,
[4680.46s -> 4684.66s]  which is usually much more limited in expressiveness.
[4684.66s -> 4687.50s]  And also we have a substantially expanded
[4687.50s -> 4689.90s]  or open-ended action space.
[4689.90s -> 4693.22s]  So instead of like some very constrained action space,
[4693.22s -> 4695.66s]  like, hey, you have a rubber, you can move forward,
[4695.66s -> 4697.74s]  you can turn left and so on and so forth.
[4697.74s -> 4701.22s]  Now you can have an even open-ended action space,
[4701.22s -> 4703.26s]  and we'll see some examples soon later.
[4704.70s -> 4707.30s]  Then because of those,
[4707.30s -> 4711.26s]  it becomes increasingly hard to do automated Go test.
[4712.50s -> 4714.10s]  Imagine you have a web agent
[4714.10s -> 4716.34s]  that is just doing things on the web,
[4716.34s -> 4718.18s]  then a lot of the time,
[4718.18s -> 4722.50s]  you just simply cannot just write a Go test beforehand,
[4722.50s -> 4724.74s]  like what the Go state will be like.
[4725.74s -> 4728.74s]  But that's fine because fuzziness
[4728.74s -> 4732.46s]  is really an inherent part of this world.
[4732.46s -> 4734.30s]  I'll skip this one.
[4734.30s -> 4738.38s]  So I'll just give some examples for web agents,
[4738.38s -> 4740.18s]  like from our Mind2Web,
[4740.18s -> 4745.18s]  you'll see in terms of the Go specification,
[4745.70s -> 4750.70s]  the user can ask for anything on an upstream websites.
[4752.38s -> 4754.10s]  So it's very broad.
[4754.50s -> 4756.06s]  And in natural language.
[4756.06s -> 4758.78s]  And then in terms of the action space, right?
[4758.78s -> 4762.74s]  Yeah, you have some broad actions, like types,
[4762.74s -> 4765.42s]  like you can type, you can click, you can drag,
[4765.42s -> 4768.34s]  you can hover on some elements,
[4768.34s -> 4773.34s]  but the actual actions, like what elements you click on,
[4773.46s -> 4778.46s]  these actual actions are dynamically populated
[4779.78s -> 4781.54s]  on each webpage.
[4781.54s -> 4783.06s]  So if you go to a different webpage,
[4783.10s -> 4785.54s]  your action space will be different.
[4785.54s -> 4788.66s]  And it's a very big action space
[4788.66s -> 4791.30s]  you have to discover on the fly.
[4791.30s -> 4794.10s]  And of course the Go test is also very fun.
[4794.10s -> 4798.50s]  But, and then there's another example of travel planning
[4798.50s -> 4800.38s]  that has some similar characteristics,
[4800.38s -> 4802.26s]  but we'll skip here.
[4802.26s -> 4806.74s]  So I think these are some of the general trends
[4806.74s -> 4808.14s]  for language agent planning.
[4808.14s -> 4811.22s]  And these are, I think these are good trends.
[4811.22s -> 4812.94s]  Yeah, it makes things more challenging,
[4813.78s -> 4814.62s]  but for the better.
[4814.62s -> 4818.82s]  Because now we can support much more realistic
[4818.82s -> 4823.66s]  and useful application scenarios with these language agents.
[4823.66s -> 4828.66s]  So let's see whether we can want to talk about this.
[4828.86s -> 4830.90s]  Maybe very quickly,
[4830.90s -> 4834.82s]  but for people who are interested in web agents
[4834.82s -> 4836.34s]  or computer user agents,
[4836.34s -> 4839.22s]  I encourage you to look at these series of work,
[4839.22s -> 4842.22s]  like Mindsweb, the first LM-based web agents,
[4842.22s -> 4845.38s]  and then CF where we'll first introduce a visual perception
[4845.38s -> 4846.70s]  into web agents.
[4846.70s -> 4849.38s]  And then the new ground where for the first time
[4849.38s -> 4851.82s]  we'll make a human-like embodiment
[4851.82s -> 4853.38s]  for computer user agents.
[4853.38s -> 4855.86s]  So you only perceive the environment visually,
[4855.86s -> 4858.82s]  there's no HTML or anything like that.
[4858.82s -> 4861.46s]  And then you're directed to pixel level operations
[4861.46s -> 4862.34s]  on the screen.
[4863.26s -> 4867.94s]  And this minimal design actually works the best
[4867.94s -> 4869.22s]  across the board.
[4869.22s -> 4872.00s]  And then we will talk about Web Dreaming,
[4872.64s -> 4875.60s]  which is model-based planning for web agents.
[4878.32s -> 4881.48s]  So let's consider the different planning paradigms
[4881.48s -> 4883.60s]  for language agents.
[4883.60s -> 4887.48s]  The most common one is reactive planning or React.
[4888.36s -> 4891.00s]  Imagine each node here is a web page.
[4891.00s -> 4894.76s]  Then at each state you have several candidate actions
[4894.76s -> 4897.20s]  you can take that will result in a different state.
[4897.20s -> 4900.28s]  For reactive planning, you just at each state
[4900.28s -> 4902.84s]  observe the environment, reason about it,
[4902.84s -> 4905.92s]  then make a decision and commit to the decision.
[4905.92s -> 4907.92s]  And then that gets you to another state,
[4907.92s -> 4910.08s]  then you just keep doing this.
[4910.08s -> 4912.96s]  So it's fast, it's easy to implement,
[4912.96s -> 4915.00s]  but the downside is that it's greedy
[4915.00s -> 4916.52s]  and it's sort of siding.
[4916.52s -> 4919.72s]  So you often find yourself stuck in some bad state
[4919.72s -> 4921.08s]  and there's no way out.
[4922.54s -> 4924.60s]  Then naturally when we talk about planning,
[4924.60s -> 4927.20s]  we just think about search or tree search.
[4928.20s -> 4931.84s]  So for tree search, compared with reactive planning,
[4931.84s -> 4934.44s]  you can do backtracking.
[4934.44s -> 4938.08s]  You maintain a value assignment
[4938.08s -> 4941.96s]  for the states on your search frontier,
[4941.96s -> 4945.20s]  then you explore the most promising branch.
[4945.20s -> 4948.52s]  At some points, if you find that not promising,
[4948.52s -> 4951.16s]  you can backtrack and then to find
[4951.16s -> 4954.10s]  to explore another branch.
[4954.14s -> 4957.94s]  So that gives you more systematic exploration.
[4957.94s -> 4961.10s]  But the downside here is that
[4961.10s -> 4964.58s]  it's in real world environments like the internet,
[4964.58s -> 4967.18s]  there are a lot of irreversible actions
[4967.18s -> 4971.30s]  that makes backtracking impossible as it's very hard.
[4971.30s -> 4973.70s]  And then a lot of this exploration
[4973.70s -> 4975.42s]  could also be unsafe or slow.
[4976.38s -> 4980.82s]  To just show you how pervasive these state changing
[4980.86s -> 4984.82s]  and irreversible actions are in real world environments,
[4984.82s -> 4987.18s]  let's consider just amazon.com.
[4987.18s -> 4990.54s]  On these single websites, you can have many,
[4990.54s -> 4993.66s]  dozens if not hundreds of these state changing actions.
[4993.66s -> 4997.06s]  You can place an order, you can make a return,
[4997.06s -> 4999.66s]  you can create an account that you will agree
[4999.66s -> 5001.16s]  to the terms and use,
[5001.16s -> 5003.46s]  and then that has legal implications.
[5003.46s -> 5006.38s]  You can change your privacy settings and so on and so forth.
[5006.82s -> 5011.82s]  If there is no like this universal magical undo button
[5012.46s -> 5014.78s]  that you can just try a bunch of things
[5014.78s -> 5016.56s]  and then magically undo
[5016.56s -> 5020.30s]  and then go back to the initial states.
[5020.30s -> 5023.86s]  So that makes tree search in real world environments hard.
[5023.86s -> 5027.04s]  And then there's also the safety and costly issues.
[5028.06s -> 5030.98s]  So ideally, we want to model-based planning.
[5032.00s -> 5034.38s]  Imagine you have the word model
[5034.38s -> 5036.46s]  that at the initial states,
[5036.46s -> 5041.46s]  it can trigger the word model to simulate the outcome
[5042.10s -> 5044.82s]  of each candidate action.
[5044.82s -> 5048.82s]  Then that gives you a chance to evaluate
[5052.26s -> 5055.90s]  the long-term value and the safety
[5055.90s -> 5059.90s]  of each candidate action before you have to commit to it.
[5061.06s -> 5063.82s]  Then you find that the most promising candidate actions
[5064.26s -> 5066.50s]  is through simulation, then you commit to it,
[5066.50s -> 5068.78s]  take it, assume it's safe,
[5068.78s -> 5070.20s]  and then you get to another state,
[5070.20s -> 5072.10s]  then you can do this all over again.
[5073.34s -> 5076.48s]  So it's faster and safer compared with tree search,
[5076.48s -> 5080.28s]  and you can also still do systematic exploration.
[5080.28s -> 5082.82s]  But the downside is really how to get
[5082.82s -> 5084.92s]  this magical word model.
[5086.02s -> 5088.44s]  Let's start with what is a word model,
[5088.44s -> 5091.12s]  because this is another overloaded term.
[5091.12s -> 5093.62s]  For our purpose here, we'll take this definition.
[5094.26s -> 5095.26s]  It's a computational model
[5095.26s -> 5098.46s]  of the environment's transition dynamics.
[5098.46s -> 5102.02s]  So basically, if I do this right now,
[5102.02s -> 5104.60s]  what would happen next?
[5104.60s -> 5105.44s]  Very simple.
[5106.34s -> 5108.50s]  Then if it's that simple and so good,
[5108.50s -> 5111.08s]  what hasn't been done for language agents yet?
[5112.22s -> 5114.34s]  Well, the issue here is that
[5115.82s -> 5117.62s]  if you think about word models
[5117.62s -> 5120.18s]  in the classic deep learning literature,
[5120.18s -> 5123.54s]  that's usually started in reinforcement learning, right?
[5124.54s -> 5127.78s]  Where you have these simple simulated environments
[5127.78s -> 5131.88s]  that you can do millions of times of trials,
[5131.88s -> 5134.98s]  and then you can use that to learn a word model
[5134.98s -> 5136.38s]  for that simple environment.
[5137.26s -> 5141.72s]  What we're dealing with here is much more complicated.
[5141.72s -> 5143.78s]  Even just for a single website, right?
[5143.78s -> 5145.94s]  There could be hundreds of different web pages.
[5145.94s -> 5147.18s]  On a single web page,
[5147.18s -> 5149.74s]  there could be hundreds of different actions,
[5149.74s -> 5152.66s]  and they can be constantly changing
[5152.70s -> 5155.14s]  because the backend databases changes.
[5155.14s -> 5157.70s]  And then this complexity quickly compounds
[5157.70s -> 5159.30s]  if we consider that there are billions
[5159.30s -> 5161.84s]  of other websites out there, right?
[5161.84s -> 5165.74s]  In that sense, we need a kind of generalist
[5165.74s -> 5167.46s]  word model for the internet.
[5168.50s -> 5169.88s]  That seems very hard.
[5171.74s -> 5174.10s]  Fortunately, we find that LLMs
[5174.10s -> 5177.86s]  can actually reasonably predict these state transitions.
[5177.86s -> 5180.40s]  So in this example, if you ask an LLM,
[5180.40s -> 5183.56s]  if I click this icon, what would happen?
[5183.56s -> 5184.92s]  Then you can recognize that, hey,
[5184.92s -> 5188.22s]  this is a shirt and this is probably a product,
[5188.22s -> 5190.40s]  so the next state will probably be
[5190.40s -> 5193.12s]  about some product details, and because it's a shirt,
[5193.12s -> 5197.18s]  so there will be sizing options and other things, right?
[5197.18s -> 5200.12s]  Because LLMs have this word knowledge,
[5200.12s -> 5202.98s]  common sense knowledge, it's trained on the internet,
[5202.98s -> 5206.32s]  so it can do a reasonable job.
[5206.32s -> 5208.12s]  Far from perfect, but reasonable.
[5208.96s -> 5213.08s]  So Web Dreamweaver leverages exactly that.
[5213.08s -> 5217.24s]  We stimulate a word model for the internet using an LLM,
[5217.24s -> 5219.76s]  in this case GPT4O.
[5219.76s -> 5222.20s]  Then when you're at a state,
[5222.20s -> 5224.40s]  you have a few candidate actions.
[5224.40s -> 5227.38s]  You can use the word model to simulate the outcome
[5227.38s -> 5228.76s]  of each candidate action.
[5228.76s -> 5231.08s]  You can even do multi-step simulation.
[5231.08s -> 5232.88s]  You have another value function,
[5232.88s -> 5235.12s]  which is also simulated by LLM,
[5235.12s -> 5239.32s]  to tell you how much progress you would have made
[5239.32s -> 5241.00s]  going down that path,
[5241.00s -> 5244.58s]  and then you can choose the highest valued action.
[5244.58s -> 5247.04s]  You take that action, get to the next state,
[5247.04s -> 5249.72s]  and then do this all over again, right?
[5249.72s -> 5252.16s]  So that's model-based planning for web agents.
[5252.16s -> 5253.36s]  It's far from perfect,
[5253.36s -> 5256.60s]  but it's a reasonable starting point.
[5257.84s -> 5261.88s]  In terms of results, we evaluate Vue Web Arena,
[5261.88s -> 5265.88s]  so that model-based planning is more accurate
[5265.88s -> 5270.08s]  than reactive planning, and slightly trails tree search.
[5270.08s -> 5272.28s]  But remember, tree search is only possible
[5272.28s -> 5275.28s]  because we are working in a sandbox environment,
[5275.28s -> 5277.16s]  which is Vue Web Arena.
[5277.16s -> 5278.64s]  If it's on the real websites,
[5278.64s -> 5282.16s]  then the backtracking in tree search will become part.
[5282.16s -> 5285.32s]  And also the model-based planning is much cheaper
[5285.32s -> 5287.84s]  and much faster, okay?
[5287.84s -> 5290.12s]  So the takeaways for planning,
[5290.12s -> 5291.92s]  the new language agents are expanding
[5291.92s -> 5294.16s]  into these new planning scenarios,
[5294.16s -> 5297.24s]  categorized by the expressive but fuzzy
[5297.24s -> 5300.84s]  goal specifications, the open-ended action spaces,
[5300.84s -> 5303.18s]  and the more difficult goal tests.
[5304.52s -> 5307.64s]  But the language for reasoning enables new capabilities,
[5307.64s -> 5310.60s]  planning abilities, like the word models,
[5310.60s -> 5311.96s]  model-based planning.
[5311.96s -> 5314.48s]  And what we didn't talk about today,
[5314.48s -> 5317.36s]  but the hierarchical planning and dynamic replanning
[5317.36s -> 5318.60s]  is also very important.
[5319.60s -> 5323.12s]  And the best of planning strategy is dependent on the LLM.
[5323.12s -> 5325.24s]  If you have a stronger base LLM,
[5325.24s -> 5328.16s]  that it may require less scaffolding,
[5328.16s -> 5331.36s]  so your planning strategy could be more reactive.
[5332.80s -> 5335.60s]  But generally, how to improve planning in LLM
[5335.60s -> 5337.98s]  is still largely an open question.
[5339.22s -> 5341.56s]  I think many people are trying the recipe
[5341.56s -> 5343.84s]  for O-1 style reasoning,
[5345.18s -> 5347.16s]  try to see whether that works for planning,
[5347.16s -> 5349.16s]  but that's still a big open question.
[5350.84s -> 5354.44s]  Okay, so I think we're really just standing at the dawn,
[5354.44s -> 5356.08s]  after a long journey.
[5356.08s -> 5357.80s]  We talked about planning, reasoning,
[5357.80s -> 5359.84s]  word models, and memory,
[5359.84s -> 5361.32s]  but there are a lot of other things
[5361.32s -> 5364.80s]  we didn't talk about, and there's a lot to be done.
[5364.80s -> 5369.80s]  Just a few immediate future directions I find interesting.
[5370.72s -> 5374.56s]  Then memory, personalization, and continual learning,
[5374.56s -> 5376.84s]  we're really just scratching the surface.
[5377.38s -> 5379.16s]  There is a whole lot to be done.
[5379.16s -> 5381.00s]  How to enable agents to continue
[5381.00s -> 5383.20s]  the learning from use and exploration.
[5384.42s -> 5389.42s]  The reasoning, how to make O-1 or O-1 style reasoning work
[5390.72s -> 5393.48s]  for language agents, where you need to deal
[5393.48s -> 5396.72s]  with this fuzzy world without reliable rewards,
[5396.72s -> 5399.02s]  and how to integrate these external actions
[5399.02s -> 5400.56s]  and the environmental states.
[5400.56s -> 5402.52s]  And that's a big open question,
[5402.52s -> 5405.08s]  and I expect there will be a lot of study on this
[5405.08s -> 5405.92s]  in 2025.
[5406.92s -> 5409.28s]  Then planning, how to build better word models
[5409.28s -> 5411.36s]  instead of just like simulated one,
[5411.36s -> 5413.74s]  and how to balance the reactive
[5413.74s -> 5415.00s]  and the model-based planning,
[5415.00s -> 5417.32s]  because you don't really want to do simulation
[5417.32s -> 5418.82s]  at every single step.
[5418.82s -> 5420.28s]  That's very costly.
[5420.28s -> 5422.12s]  Even though humans can do simulation,
[5422.12s -> 5426.16s]  we don't do that for every single decision,
[5426.16s -> 5427.92s]  only for those difficult ones.
[5427.92s -> 5430.00s]  But how to balance this reactive
[5430.00s -> 5433.60s]  and the model-based planning is still an open question.
[5434.50s -> 5436.40s]  How to sustain a long horizon.
[5436.84s -> 5440.40s]  For safety, I think that's a very pressing issue
[5440.40s -> 5443.96s]  that really keeps me up at night.
[5443.96s -> 5447.18s]  I think the attack surface of language agents
[5447.18s -> 5449.16s]  is scarily broad.
[5449.16s -> 5451.02s]  For web agents, the attack surface
[5451.02s -> 5453.84s]  is essentially the entire internet.
[5453.84s -> 5457.88s]  Someone can embed something on a seemingly benign website,
[5457.88s -> 5459.48s]  and you're like, for example,
[5459.48s -> 5461.20s]  opening a head deep research agent
[5461.20s -> 5462.86s]  that can go there, visit there,
[5462.86s -> 5464.28s]  and get tricked by it,
[5464.28s -> 5467.96s]  and then maybe reveal your private information
[5467.96s -> 5469.96s]  and things like that.
[5469.96s -> 5473.40s]  So there are two general types of safety issues.
[5473.40s -> 5475.72s]  There's the endogenous risks.
[5475.72s -> 5477.68s]  These are the safety risks
[5477.68s -> 5481.46s]  originate from within, from the agent itself,
[5481.46s -> 5484.40s]  usually because of the incompetency of the agents.
[5484.40s -> 5487.92s]  So you can mistakenly take some irreversible actions
[5487.92s -> 5489.18s]  that do harm.
[5489.18s -> 5492.20s]  And then there are exogenous risks.
[5492.20s -> 5495.08s]  So these risks are from the external environments.
[5496.28s -> 5497.96s]  Okay, so, but there are also
[5497.96s -> 5500.04s]  a lot of exciting applications.
[5501.36s -> 5506.36s]  Probably the one with the most clear business case
[5507.00s -> 5509.74s]  is agentic search or deep research.
[5509.74s -> 5512.52s]  If you have not used like complexity pro
[5512.52s -> 5514.92s]  or Google OpenAI deep research,
[5514.92s -> 5517.06s]  I highly encourage you to try that.
[5517.06s -> 5522.06s]  And I think something big is being baked here.
[5522.52s -> 5525.64s]  I think this will become a huge thing in 2025.
[5525.64s -> 5528.44s]  Then also like workflow automation.
[5528.44s -> 5529.96s]  And personally, I'm very excited
[5529.96s -> 5533.68s]  about developing agents for sciences.
[5533.68s -> 5535.12s]  These are all very exciting.
[5536.08s -> 5540.48s]  So for a more comprehensive coverage on language agents,
[5540.48s -> 5542.84s]  I encourage you to check out our tutorial
[5542.84s -> 5544.72s]  with DE, Xun, Yu, and Tao.
[5544.72s -> 5547.20s]  We just did that EMLP like two months ago.
[5547.20s -> 5550.76s]  And all the materials like slides and videos
[5550.76s -> 5553.02s]  are available on our website.
[5554.52s -> 5556.84s]  I thank all of my sponsors
[5556.84s -> 5559.22s]  and be happy to take any questions.
