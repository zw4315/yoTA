# Detected language: en (p=1.00)

[0.00s -> 6.00s]  Good afternoon. Quick sound check. Can people hear me?
[6.00s -> 8.00s]  Yes.
[8.00s -> 18.00s]  Thanks. Okay, so I guess today, so far, it's the last lecture for SOA1 this semester.
[18.00s -> 24.00s]  And we don't really have a formal topic.
[24.00s -> 30.00s]  There's a Q&A lecture, so feel free to ask questions about anything.
[30.00s -> 36.00s]  This is, I guess, your last chance at least this semester for this class.
[36.00s -> 43.00s]  By default, my plan was to go over a couple of topics based on the questions over email.
[43.00s -> 49.00s]  One, I was going to talk a little bit about the Network Lab, the solutions.
[49.00s -> 54.00s]  If possible, also a little bit of time about the MLab.
[54.00s -> 60.00s]  Again, talking about the solutions. And then there were a lot of questions about the follow-on classes.
[60.00s -> 66.00s]  And in fact, I think I'll cover that first.
[66.00s -> 77.00s]  But before diving in, in any of these three topics, let me remind you, there's a subject evaluation
[77.00s -> 87.00s]  ongoing, and we would, as a staff of SOA1, we would appreciate if you give your feedback on SOA1.
[87.00s -> 93.00s]  Hopefully it's positive and we get a chance to teach it again next year.
[93.00s -> 99.00s]  And maybe this is also a good time to actually thank you for attending today's lecture,
[99.00s -> 101.00s]  even though there's not a sort of very formal program.
[101.00s -> 108.00s]  But I also appreciate the fact that the staff generally appreciate the fact that you have been very engaged.
[108.00s -> 112.00s]  And particularly with the papers that are not directly to the labs,
[112.00s -> 119.00s]  it's clear that many of you dove into the papers and tried to really understand them and ask great questions over email.
[119.00s -> 125.00s]  So we very much appreciate the engagement.
[125.00s -> 136.00s]  Any questions before I go over these set of topics?
[136.00s -> 140.00s]  OK, let me start in with the question that probably came up most in email.
[140.00s -> 150.00s]  What next? There's quite a number of classes that are systems oriented focused.
[151.00s -> 157.00s]  And I'm sure I missed a number of classes that I should have listed here.
[157.00s -> 162.00s]  Immediately, sort of strictly related to operating systems. You know, clearly, you know, if you haven't taken 033 yet,
[162.00s -> 168.00s]  it's a great class to take, particularly to read or learn how to read papers.
[168.00s -> 176.00s]  6828 was the original and the only OS class that we offered this year.
[176.00s -> 181.00s]  This is the second year. This year is the first year that we're offering both.
[181.00s -> 189.00s]  So S081 is the undergrad OS and S828 is the grad level OS class.
[189.00s -> 203.00s]  And basically, 828 assumes you have taken 081 or undergrad OS and goes off in doing projects and building interesting OS artifacts and reading more papers from the literature.
[203.00s -> 207.00s]  More of the modern research literature.
[207.00s -> 212.00s]  Then maybe you're more, you know, the OS sort of does a lot of interfacing with the hardware.
[212.00s -> 215.00s]  It would have been more of the hardware side of systems.
[215.00s -> 220.00s]  You know, the computer architecture class 823, which I believe is operating in the coming spring.
[220.00s -> 232.00s]  And of course, the 6111, you know, sort of the building something hardware devices on the performance sites and the compiler sites, 6172 and 6035 are good places.
[232.00s -> 236.00s]  The number of you are taking 6172 in parallel with S081.
[236.00s -> 246.00s]  So but if you haven't taken, for example, the compiler class 035 or the dynamic compiler class, both of them are also great classes.
[246.00s -> 248.00s]  Systems is a broad topic.
[248.00s -> 254.00s]  So like there's networking and there's a whole branch of classes, a whole stream of classes around networking.
[254.00s -> 262.00s]  There's a whole stream of class around databases, which are all good and important.
[262.00s -> 271.00s]  And aspects that you've seen in S081 will show up in much more prominent ways in those classes.
[271.00s -> 275.00s]  So, for example, networking clearly is a great topic of 829.
[275.00s -> 280.00s]  And so you will see a lot more about networking there than you've seen in S081.
[280.00s -> 290.00s]  683, you know, we talked about file systems, you know, clearly another important class of storage systems and probably in some ways more important for many applications is databases.
[290.00s -> 295.00s]  So 683 is a great topic class to learn a lot more about databases.
[295.00s -> 302.00s]  In terms of like classes that are sort of, you know, sometimes called whatever the PDOS classes.
[302.00s -> 311.00s]  There's a number of them. 824, the Distributed Systems class will be offering that coming semester, spring semester.
[311.00s -> 320.00s]  And a section of the number of the staff involved this semester in S081 will be involved in 824.
[320.00s -> 324.00s]  There's 858, which is the computer security class.
[324.00s -> 331.00s]  We won't be offering that coming this spring, but we hope to operate the next academic year.
[331.00s -> 336.00s]  And there's 826, which is the principles of computer systems.
[336.00s -> 339.00s]  A number of people ask about verification and system software.
[339.00s -> 343.00s]  This is the class to take if you're interested in that topic.
[343.00s -> 349.00s]  More broadly speaking, a lot of people wondering like what is actually going on in research.
[349.00s -> 356.00s]  If you're interested, you know, almost all the papers in the systems literature are publicly available, even the ones at the recent conferences.
[356.00s -> 360.00s]  So, for example, the OSDI conference happened a couple of weeks ago.
[360.00s -> 366.00s]  And you can just look at them and see what kind of topics are covered and read the papers that you find it interesting.
[366.00s -> 368.00s]  All the talks are published.
[368.00s -> 373.00s]  One of the advantages of the COVID period is that all these conferences are virtual.
[373.00s -> 379.00s]  And so all the conferences, talks are recorded and you can just watch them and see what's going on.
[379.00s -> 385.00s]  If you're interested in the sort of two parts, the research part, then, of course, you know, there's what's going on in practice.
[385.00s -> 392.00s]  And, you know, if you're interested in keeping track of what's happening with Linux and how it's evolving,
[392.00s -> 404.00s]  lwn.net actually publishes a very regular page and very interesting articles about like sort of big changes or topics that are around the Linux kernel.
[404.00s -> 411.00s]  Finally, you know, if you're just excited about like doing the labs, you know, just keep hacking.
[411.00s -> 418.00s]  You can easily to cook up projects or either by doing extensions of labs or just trying things out on your own.
[418.00s -> 424.00s]  And it's a great way to really learn and appreciate how things work.
[424.00s -> 428.00s]  You probably have gotten a good sense of that during the labs.
[428.00s -> 437.00s]  We're sort of big fans of learning by doing and a lot of that you can do on your own.
[437.00s -> 456.00s]  Any questions about what next after isolate one?
[456.00s -> 457.00s]  OK.
[457.00s -> 463.00s]  Well, let's talk about the Netlab.
[463.00s -> 470.00s]  I guess yesterday.
[470.00s -> 474.00s]  And so there's a couple of things that maybe I'll start at.
[474.00s -> 478.00s]  We'll dive in sort of slowly and start a little bit at the top.
[478.00s -> 488.00s]  And let me talk a little bit in generic terms about the lab.
[488.00s -> 494.00s]  There's a couple of things. First, I want to talk a little bit about your structural things.
[494.00s -> 498.00s]  That sort of influenced the design of the lab or the coding of the lab.
[498.00s -> 504.00s]  And those are four different things and aspects. One is I just want to talk a little bit about hardware.
[504.00s -> 509.00s]  This is one of the labs where we there's a lot of interaction between software and hardware.
[509.00s -> 515.00s]  In fact, hardware determines for a great part, like how the structure of the software.
[515.00s -> 527.00s]  A little bit about software structure and just come sort of back to this general topic that we covered in sort of somewhere in the middle of the term about the organization of drivers relative to the rest of the operating system kernel.
[527.00s -> 537.00s]  Then talk a little bit more specific about hardware structures, the rings and the descriptors.
[537.00s -> 543.00s]  The two primary sort of hardware structures that the driver deals with or must deal with.
[543.00s -> 554.00s]  And then I'll talk a little about the code, particularly the solutions and sort of focusing on a little bit about the role of MBuffs.
[554.00s -> 562.00s]  You know, walking a lot of questions over e-mail about, you know, why not a walk in receive lock, receive in the receive function?
[562.00s -> 575.00s]  Why a loop in the receive handler? And, you know, what exactly, you know, do the flags mean in the command commands field of the structure of the descriptor?
[575.00s -> 581.00s]  So we'll get to those issues that we as we look at the top as we look at the code there.
[581.00s -> 588.00s]  Just before we dive in, like it may be worthwhile to reminding ourselves here what the challenges are, although they're probably fresh in your head.
[588.00s -> 595.00s]  But like more thermal high level, one of the sort of core talent challenges that you have to deal with in this lab.
[595.00s -> 601.00s]  If you'll jump in to add any ones that I might have missed and you struggled with.
[601.00s -> 605.00s]  So first of all, you know, there's the hardware specification.
[605.00s -> 611.00s]  This is a PDF is a pretty serious document.
[611.00s -> 624.00s]  And this network card, the 1000, even though it's a reasonable simple network card is still a very sophisticated piece of hardware that offers a lot of different features and ways you can program.
[624.00s -> 631.00s]  And as you're just getting on top of actually what the card does and how do you program it?
[631.00s -> 641.00s]  You know, just any terminal lines and all that is difficult and poses a challenge.
[641.00s -> 647.00s]  Second, I think, you know, major challenges, the concurrency side of things.
[647.00s -> 652.00s]  And we're sort of two parts of the concurrency that makes this lab challenging.
[652.00s -> 655.00s]  One is just the concurrency between the hardware and the software.
[655.00s -> 663.00s]  Like the network card is just doing things like sending packets, receiving packets at the same time, you know, the OS kernel is running and the driver is running.
[663.00s -> 671.00s]  And so there has to be some coordination plan, you know, between the hardware and the software, you know, to actually make sure that everything works out.
[671.00s -> 679.00s]  And a large part of the driver is, you know, dealing with that coordination.
[679.00s -> 683.00s]  There's also the software-shocker coordination.
[683.00s -> 690.00s]  You know, the driver, multiple threats or multiple kernel threats might be in the interrupt handler run inside of the driver.
[690.00s -> 696.00s]  And you've got to make sure that they don't step on each other's tokes when everything else happens.
[696.00s -> 701.00s]  And this comes a lot to like, you know, the locking topic.
[701.00s -> 709.00s]  Even in this driver, you know, things are reasonable, straightforward in terms of the software concurrency.
[709.00s -> 713.00s]  But nevertheless, you know, it's something that requires attention and a little bit of thinking.
[713.00s -> 719.00s]  And then finally, I guess that's probably on the top of your list or many of your list is debugging.
[719.00s -> 729.00s]  This is a little bit hard, a little bit sometimes more challenging than pure software debugging because you can't set, you know, breakpoints inside of the network card.
[729.00s -> 735.00s]  You know, you can program the registers and then basically give the card a kick and say like, you know, please do your work.
[735.00s -> 744.00s]  And then if the card doesn't do anything or you don't see your packet coming out on the other side, you know, in the log, you know, of the packet trace,
[744.00s -> 754.00s]  then, you know, you basically have to scratch your head and sort of start thinking about like what might have gone wrong or what might you have missed in the hardware specification.
[755.00s -> 765.00s]  And so you have to go circle back and forth and there's no sort of easy way or you can just can't not single step through the network card.
[767.00s -> 777.00s]  Does that make sense in terms of challenges? You know, does that line up with your own experience or am I missing some of the missing one of the core challenges that you dealt with?
[784.00s -> 799.00s]  Okay, let's start with the hardware side of things.
[799.00s -> 808.00s]  And the reason I want to start there because, you know, it's easy to keep or lose track of the fact that you're actually dealing with hardware here.
[808.00s -> 817.00s]  You know, even though, you know, it's sort of virtualized where it is emulated by QEMU and you're just running it on your, on a FINA or on your laptop,
[817.00s -> 825.00s]  the actual thing that's implemented by QEMU is hardware. So just to remind you again, you've seen this picture before.
[825.00s -> 834.00s]  The way to think about it is that, you know, QEMU emulates a complete board of devices and a processor.
[834.00s -> 848.00s]  And, you know, and so, you know, as we all know, I've talked a little bit before about it, like the processor actually is like almost like a small piece of the whole board.
[848.00s -> 850.00s]  You know, it's the thing that sits under the fan here.
[850.00s -> 857.00s]  But then, you know, there's a range of devices, you know, that are, you know, connected or can be connected to this board.
[857.00s -> 861.00s]  And then interact, you know, with the code running on the processor.
[861.00s -> 869.00s]  You know, like here, you know, for this particular lab, you know, what is really interesting sort of is the actually even at jack, you know, where you can plug in your even at cable.
[869.00s -> 875.00s]  And then that is actually the thing that the network card sends its packets over.
[875.00s -> 880.00s]  Now, QEMU doesn't emulate exactly this board, so things are slightly differently.
[880.00s -> 885.00s]  But again, you know, at least conceptually, this is like the picture you should have in your head.
[885.00s -> 900.00s]  Right. And so when the process where you're writing your registers off the device driver, that cost some stuff to happen that actually in the end, some packet comes out of this cable that's connected to the connector.
[900.00s -> 907.00s]  In a slightly more, you know.
[907.00s -> 914.00s]  In a slightly more in a schematic diagram, you know, this is the same picture basically in here.
[914.00s -> 919.00s]  You know, we're seeing here's our processor board that has four cores on it.
[919.00s -> 924.00s]  It has one of the L1 and L2 caches. And then as like a link, you know, going to memory.
[924.00s -> 927.00s]  So here's the memory and the RAM, random access memory.
[927.00s -> 931.00s]  And as we'll see in a second, you know, that plays actually a big role.
[931.00s -> 939.00s]  And then here, like, you know, it's actually the link to the, or here actually, it's the link to the gigabit Ethernet controller.
[939.00s -> 946.00s]  Now, it's a little bit messy, you know, so to think about this level of detail, what exactly is going on.
[946.00s -> 952.00s]  And so for the rest of the lecture, a little bit more simpler picture.
[953.00s -> 957.00s]  And the picture really you're in your head.
[957.00s -> 963.00s]  And you probably developed while doing the lab is as follows.
[963.00s -> 970.00s]  Basically, we have our RISC-IV processors for cores.
[970.00s -> 974.00s]  Here's our RISC-V.
[974.00s -> 981.00s]  And there is RISC-V cores that execute the instructions that you're writing.
[981.00s -> 984.00s]  And you can just think about them. They're sort of all connected to a bus.
[984.00s -> 989.00s]  And there's a little bit of a simplification, but like it makes it easier to explain things.
[989.00s -> 994.00s]  And so like on the bus, you know, the random access memory.
[994.00s -> 998.00s]  Where all the data, you know, the kernel actually uses is stored.
[998.00s -> 1003.00s]  You know, the kernel text itself is stored, you know, with a great amount of detail in the beginning of the semester.
[1003.00s -> 1011.00s]  And then in this, you know, for this particular lab, the thing that's interesting is that on the bus is actually the network card.
[1011.00s -> 1016.00s]  You know, the E1000.
[1016.00s -> 1023.00s]  And, you know, the RAM is generally more connected to the RISC-V processor directly versus a private bus.
[1023.00s -> 1028.00s]  And the devices that tend to be on the slower type of bus.
[1028.00s -> 1042.00s]  And in particular, the bus that in the lab, you know, there's a PCIe bus that actually connects the processor part with the network card.
[1042.00s -> 1048.00s]  To get the network card to do something, the network card has a controller inside of it.
[1048.00s -> 1052.00s]  And, you know, these controllers have registers.
[1052.00s -> 1061.00s]  And you can, one of the cool aspects of these registers, they're called MME mapped IO registers.
[1061.00s -> 1072.00s]  And so you can just write them with, you know, whatever star, you know, OX and whatever the value is, you know, for in the physical memory space, what the value is for that particular register.
[1072.00s -> 1076.00s]  We might have our head, our tail registers.
[1076.00s -> 1090.00s]  And, you know, we can, the driver can write those by executing, you know, a load or store instruction, you know, to the address, you know, that corresponds, you know, in the physical address space to the location of that register.
[1090.00s -> 1097.00s]  And the hardware will ensure when we do load a store, you know, that that store will either go, you know, we'll go through the
[1097.00s -> 1100.00s]  if it is to one of the control register, we'll go through the control registers.
[1100.00s -> 1105.00s]  If it goes to, you know, one of the addresses in RAM, it will go to the RAM.
[1105.00s -> 1113.00s]  And so you can just manipulate or program the network card by loading, you know, reading and writing these controller registers.
[1113.00s -> 1119.00s]  And basically the bits in these controller registers have special meaning, as you have seen.
[1119.00s -> 1127.00s]  Now, for this particular card, you know, the network card needs to send, of course, packets across the network and, you know, and it needs to get the packet somewhere.
[1127.00s -> 1137.00s]  And so the packets actually live, you know, are allocated, as we'll see in a second, you know, they just live somewhere in memory.
[1137.00s -> 1149.00s]  And what we do is, in fact, in addition to these packets, you know, there are sort of two ring structures that also live in memory.
[1149.00s -> 1160.00s]  And we can, you know, program and we can tell the card, you know, the card also knows where these ring structures are.
[1160.00s -> 1167.00s]  You know, it has an address for where the TX ring is, you know, it has an address for where the RX ring is.
[1167.00s -> 1174.00s]  And, you know, the hardware card, you know, looks at those addresses to actually see which packets need to be sent.
[1174.00s -> 1183.00s]  So here we might have a queue of packets that need to be transmitted or a queue of packets that needs to be received.
[1183.00s -> 1195.00s]  And, you know, the E1000, you know, DMAs, you know, for example, like, you know, it's going to work on like the first packet to send.
[1195.00s -> 1207.00s]  It will DMA basically, you know, those packets, you know, the data associated with the packets, you know, straight from RAM to the network without actually having the processor involved at all.
[1207.00s -> 1214.00s]  So these are what are called DMA, direct memory access.
[1214.00s -> 1216.00s]  Okay.
[1216.00s -> 1227.00s]  Does this make sense in terms of a more abstract picture for the organization of the RISC-V, RAM and E1000?
[1227.00s -> 1236.00s]  Any questions about this?
[1236.00s -> 1251.00s]  So one thing in talking about sort of this hardware, hardware and hardware software concurrency, clearly, you know, the E1000 and, you know, the processor are both manipulating these transmission and reception queues or rings.
[1251.00s -> 1263.00s]  And so there has to be some protocol that sort of give and take protocol where, you know, at some point, particular parts of the ring are operated through, are owned or operated by the network card
[1263.00s -> 1268.00s]  and then there are parts that are maybe under control of the software or the OS or the kernel.
[1268.00s -> 1271.00s]  And, you know, we'll see in a second how that is arranged.
[1271.00s -> 1283.00s]  But, you know, we got to be careful, again, that the card doesn't triple over or triple over the operating system or the other way around.
[1283.00s -> 1295.00s]  A quick question. So is there a separate like transmit ring and receive in the RAM and then also in the E1000 driver?
[1295.00s -> 1300.00s]  Yeah, well, the one way to think, okay, so there's two things going on and we'll look at the code in a second.
[1300.00s -> 1315.00s]  The transmission ring and the receive ring basically live in RAM and both the E1000 and the RISC-V manipulate the ring that actually stored in RAM.
[1315.00s -> 1320.00s]  And those are the rings that you saw in the hardware documentation.
[1320.00s -> 1331.00s]  It happens to be the case that xv6 also has a, you know, the driver maintains a sort of a ring of emboss separately of the transmit ring and the RX ring.
[1331.00s -> 1336.00s]  So like there's something like TX emboss.
[1336.00s -> 1340.00s]  And that's, you know, that is a data structure only the operating system really knows about.
[1340.00s -> 1343.00s]  You know, the network card doesn't really know about it.
[1343.00s -> 1354.00s]  The network card only knows about the TX ring and RX ring because those are the values that were programmed into the registers saying like, here's where you can find.
[1354.00s -> 1358.00s]  Here's the address where you can find the transmit ring. Here's the address where you can find the receive ring.
[1358.00s -> 1360.00s]  OK, yeah, that makes sense.
[1360.00s -> 1370.00s]  That makes sense? OK, so that's sort of the hardware picture in a more schematic version.
[1370.00s -> 1374.00s]  Actually, let's talk a little bit about the software structure.
[1374.00s -> 1378.00s]  It is a bit confusing, actually, in this lab.
[1378.00s -> 1386.00s]  So software structure.
[1386.00s -> 1392.00s]  So, you know, let's just draw the thing that we most care about, of course, is the driver.
[1392.00s -> 1400.00s]  So here's our driver, the E-1000 driver.
[1400.00s -> 1409.00s]  And it basically has only two functions. It has the transmit function.
[1409.00s -> 1415.00s]  And it has the receive function.
[1415.00s -> 1421.00s]  And the receive function runs in response to interrupts.
[1421.00s -> 1428.00s]  If an interrupt happens, then xv6 will use the usual interrupt mechanism.
[1428.00s -> 1433.00s]  You know, a trap will happen, a trap will look to see if there's an interrupt from the network card.
[1433.00s -> 1437.00s]  If there's an interrupt in the network card, it will call the receive function.
[1437.00s -> 1446.00s]  And as you remember, we used to in one of the previous lectures about device drivers, you know, it's often helpful to think of the device drivers were split in two parts.
[1446.00s -> 1452.00s]  The bottom half, which runs in the context of an interrupt handler.
[1452.00s -> 1457.00s]  That's the typical name for used in the top half.
[1457.00s -> 1464.00s]  Which generally runs in the context of a kernel process or like a user level process.
[1464.00s -> 1470.00s]  So, for example, if we're thinking about things that sit on top of here, there's the network stack,
[1470.00s -> 1476.00s]  you know, the code that actually implements IP, UDP, and so there sits sort of above this.
[1476.00s -> 1485.00s]  And then, you know, there's our usual sort of line of kernel space blow, user space blow up.
[1485.00s -> 1492.00s]  And this is a very OS view of the world where use space is not particularly that big or important.
[1492.00s -> 1501.00s]  And, you know, for example, our whatever, net test, you can use a lot of program that runs user space, and it makes system calls.
[1501.00s -> 1506.00s]  System calls go to the network stack.
[1506.00s -> 1516.00s]  For example, like if you call the writes system call with a file descriptor, the kernel knows, you know, for that file descriptor.
[1516.00s -> 1520.00s]  If you do a write, then, you know, the writes should go to the network stack.
[1520.00s -> 1525.00s]  The network stack builds up the packets and then it calls transmit.
[1525.00s -> 1531.00s]  And then transmits, you know, whatever manipulates the transmission queue, the TX queue will ring.
[1531.00s -> 1534.00s]  And, you know, that actually gets packets off the network.
[1534.00s -> 1537.00s]  You know, at some point, their response might come back.
[1537.00s -> 1544.00s]  You know, that will generate an interrupt and then the receive function will run and inspect the receive ring.
[1544.00s -> 1552.00s]  Right. So there's a couple of things that, you know, we want to I want to point out, you know, in terms of concurrency.
[1552.00s -> 1557.00s]  And so first of all, like the interrupt handler basically can run in any instant in time.
[1557.00s -> 1567.00s]  Right. You know, we might be in user space and if interrupt comes in, you know, the interrupt mechanism will cost, you know, the current user program to switch to the kernel mode.
[1567.00s -> 1572.00s]  You know, the trap functionality code that we've seen before runs and it will call receive.
[1572.00s -> 1578.00s]  So we might be in the middle of some usual process and boom, you know, we get an interrupt and suddenly we're in the receive function.
[1578.00s -> 1582.00s]  And even in the kernel, like if we're not holding locks, you know, locks turn interrupts off.
[1582.00s -> 1594.00s]  But if we were not holding locks, you know, we could get an interrupt and the kernel may change the program counter basically to the interrupt handler and run the receive function.
[1594.00s -> 1600.00s]  So this code sort of runs, you know, the things that in the bottom half, you know, can basically run instantly almost any time.
[1601.00s -> 1612.00s]  And the code that in the top half, it really operates on behalf of user level processes or kernel threads that actually make calls into the top half.
[1612.00s -> 1615.00s]  And, you know, easily it could have been the case, correct.
[1615.00s -> 1616.00s]  In fact, this is the case.
[1616.00s -> 1622.00s]  And even in net tests, you know, we might actually have Netex actually for, you know, a whole bunch of processes.
[1622.00s -> 1625.00s]  But we have actually many instances in one of the tests.
[1625.00s -> 1629.00s]  We have many instances of net tests running.
[1629.00s -> 1635.00s]  And they all jump into, you know, the network stack and they all, you know, then call transmit.
[1635.00s -> 1636.00s]  So that is the case.
[1636.00s -> 1639.00s]  So we're looking at this from the perspective of concurrency.
[1639.00s -> 1644.00s]  Multiple senders can be actually in the context or can be in the top half.
[1644.00s -> 1654.00s]  And so it's pretty clear, you know, we need some locking scheme or discipline to at least, you know, make sure that the different senders don't trip over each other.
[1654.00s -> 1655.00s]  OK.
[1655.00s -> 1659.00s]  Then on the bottom half, there's only one interrupt handler directive.
[1659.00s -> 1663.00s]  There's never in the case that receive is going to be called by multiple.
[1663.00s -> 1667.00s]  There are multiple receives running at the same time in parallel and different course.
[1667.00s -> 1674.00s]  You know, the interrupt handler, when an interrupt happens, the processes are alerted.
[1674.00s -> 1681.00s]  And if future interrupts happen, they're actually blocked until the current interrupt actually is dealt with.
[1681.00s -> 1693.00s]  And so, in fact, when the receive function runs, there's actually only one interrupt handler running at the time that calls receive.
[1693.00s -> 1699.00s]  So there's no real immediately concurrency in receive itself.
[1699.00s -> 1706.00s]  Although, of course, the transmit, you know, threads can run concurrently with the interrupt handler.
[1706.00s -> 1714.00s]  For example, we might have one core running in the interrupt handler and we may have another core that actually is about to transmit.
[1714.00s -> 1716.00s]  Is this?
[1716.00s -> 1720.00s]  Any questions about this?
[1720.00s -> 1724.00s]  Sort of important to have.
[1724.00s -> 1726.00s]  To understand.
[1726.00s -> 1735.00s]  It's a bit of review, but it's important to.
[1735.00s -> 1738.00s]  OK, so I guess I have a general question.
[1738.00s -> 1745.00s]  How do we know what is supposed to be classified as a bottom half and a top half?
[1745.00s -> 1750.00s]  So, yeah, I think the way to think about it, there's not anything that runs in the context of an interrupt handler.
[1750.00s -> 1755.00s]  That's the bottom half. And in this case is basically just receive.
[1755.00s -> 1766.00s]  And anything that runs in the context of a regular process or thread or kernel thread, you know, that's the top half.
[1766.00s -> 1776.00s]  OK, so if you look at this picture a little bit more, since we're actually let me make a high level statement here.
[1776.00s -> 1780.00s]  A lot of people ask, you know, why no lock in receive?
[1781.00s -> 1790.00s]  And where is a lock needed? And so at first glance, it might not be necessary that the lock is necessary, correct?
[1790.00s -> 1797.00s]  Because there's only one instance of receive running. So there's not multiple instances of receive running at the same time.
[1797.00s -> 1804.00s]  And so these receives, since there's only one receive, you know, it doesn't share data structures with any other receive because there's only one of them.
[1804.00s -> 1808.00s]  Now, of course, it could be the case that the receive and the transmitter share data structures.
[1808.00s -> 1811.00s]  But as we see in a second, that actually is not the case.
[1811.00s -> 1819.00s]  Basically, the emission part of the driver is completely separate from the reception part of the driver or the receiving packets.
[1819.00s -> 1824.00s]  And so as a result, the receive doesn't really need actual locks.
[1824.00s -> 1829.00s]  It's really not sharing data structures with any other concurrent activity.
[1829.00s -> 1836.00s]  The weird part is that, you know, some of you found out if you do use a lock, you actually get a panic.
[1836.00s -> 1853.00s]  And so let me say a little bit about that, because the reason the panic happens is a little bit because there's a software structure actually in the E1, in the XVC external, slightly more complicated than you might think.
[1853.00s -> 1861.00s]  And the reason is that basically the receive interrupt handle actually does quite a bit of work or can do quite a bit of work.
[1861.00s -> 1864.00s]  So let me talk a little bit about it.
[1864.00s -> 1874.00s]  In the particular case that actually where it does quite a bit of work is, for example, ARP requests.
[1874.00s -> 1878.00s]  ARP is one of those packet types that Robert talked about a little while ago.
[1878.00s -> 1882.00s]  But basically what happens, like an interrupt comes in.
[1882.00s -> 1898.00s]  So an ARP request, you know, to discover what the Ethernet address is where an IP address, you know, comes in, you know, that will call, you know, E1000 receive.
[1898.00s -> 1901.00s]  And, you know, we'll call whatever netRx.
[1901.00s -> 1906.00s]  So that will go call into the network stack on the previous picture.
[1906.00s -> 1912.00s]  So net receive calls calls into this code over here.
[1912.00s -> 1918.00s]  Net receive calls.
[1918.00s -> 1921.00s]  Net receive calls.
[1921.00s -> 1924.00s]  NetRx.
[1924.00s -> 1927.00s]  ARP.
[1927.00s -> 1938.00s]  And it looks, you know, there's an ARP packet and basically sends back, you know, the Ethernet address for this particular XVC.
[1938.00s -> 1951.00s]  And the way it does that, of course, you know, to send the packet, it will call, you know, E1000 transmit.
[1951.00s -> 1956.00s]  And, you know, we know that will actually need to acquire a lock.
[1956.00s -> 1965.00s]  All of you did that, you know, whatever. There's the E1000 lock.
[1965.00s -> 1975.00s]  And the reason that actually has to acquire a lock is because there might be multiple senders and, you know, the sender should not trip over each other.
[1976.00s -> 1992.00s]  So that's sort of the picture. And so what we're basically in terms of this previous picture, what we're seeing here is that the bottom half may end up, you know, calling code in the top half and then come back into the driver through the top half.
[1992.00s -> 2004.00s]  And a number of, you know, you run into trouble with that because, you know, the, you know, it's not 100% clear that you didn't need locks in the receiving part.
[2004.00s -> 2020.00s]  And so quite a number of you ran into this particular bug where, you know, you actually acquired the, you know, E1000 lock in receive.
[2020.00s -> 2024.00s]  And that worked all fine, correct? Except that once in a while, you got to panic.
[2024.00s -> 2034.00s]  And why do you get the panic? Hopefully with this picture, that's pretty clear, but.
[2034.00s -> 2040.00s]  I mean, isn't it going to be the same lock that E1000 transmit is going to try to acquire down the stack?
[2040.00s -> 2050.00s]  Yeah, exactly, correct? So the E1000 here at this point now owns the lock, holds it, calls net rx, calls net rxr, then E1000 transmit, then calls acquire again.
[2050.00s -> 2062.00s]  And as you might remember from previous labs, if acquire is called and is already held, that's a deadlock, right? And so the XVC external panics.
[2063.00s -> 2070.00s]  Got a question. Let's assume you had a slightly different implementation of locks such that those panics are not occur.
[2070.00s -> 2076.00s]  Like, let's assume like if I wanted to acquire a lock that already held, nothing happened. Yeah.
[2076.00s -> 2082.00s]  So these are these are called recursive locks.
[2082.00s -> 2092.00s]  Or reentering locks is another name for them. And so if you have recursive locks or reentering locks, then, you know, you could just freely do this and there would be no problem.
[2092.00s -> 2101.00s]  And I think one of you over email mentioned to me that actually they implemented recursive locks to exactly deal with this particular problem.
[2101.00s -> 2106.00s]  And so they acquired the locks in the E1000 receive function.
[2106.00s -> 2115.00s]  And, you know, after the panic, basically fixed, you know, acquiring a lease in the XVC to actually support recursive locks.
[2115.00s -> 2118.00s]  And that would be the solution.
[2118.00s -> 2122.00s]  The simple solution is a fine solution.
[2122.00s -> 2134.00s]  The solution that we intended and the staff solution has is that basically it just doesn't acquire locks in the receive at all because it actually turns out not to be necessary.
[2134.00s -> 2141.00s]  Sorry, can you say again why there couldn't be two threads in the receive?
[2141.00s -> 2144.00s]  There's only one thread in the receive. The interrupt handler runs.
[2144.00s -> 2148.00s]  There's only one receive function ever running at the time on any core.
[2148.00s -> 2155.00s]  That actually calls into the top half of the top half of the operating system.
[2155.00s -> 2160.00s]  And that goes back out into the bottom half. Sorry, not in the bottom half.
[2160.00s -> 2165.00s]  That calls back into E transmit and transmit definitely needs to acquire a lock.
[2165.00s -> 2176.00s]  So you want to receive E1000 receive while holding the lock calls E1000 transmit, which tries to acquire the same lock that the interrupt handler already holds.
[2176.00s -> 2181.00s]  If you have, if you're locked in the receive handler.
[2181.00s -> 2187.00s]  I see. So, so it is the case and like one interrupt can go off.
[2187.00s -> 2193.00s]  And then while it's still running, another interrupt could go off as well.
[2193.00s -> 2199.00s]  No, no, no, no, no, no. Maybe they'll go back to this picture. So interrupt happens.
[2199.00s -> 2205.00s]  That calls the receive function. There's only one receive function running that receive function calls.
[2205.00s -> 2213.00s]  You know, RX net receive that calls RX net receive arc and then that arc function calls transmit.
[2213.00s -> 2222.00s]  And if you were using the same lock in transmit and receive, you know, you would get a deadlock.
[2222.00s -> 2228.00s]  OK, I see. Thank you.
[2228.00s -> 2235.00s]  And that's OK. And so basically the two solutions to this one is to use recursive locks.
[2235.00s -> 2245.00s]  Other solution was no locks in receive or, you know, have two locks.
[2245.00s -> 2251.00s]  All three of them are totally reasonable solutions.
[2251.00s -> 2258.00s]  Any questions about this? Would there be a reason to have a lock around receive at all?
[2258.00s -> 2264.00s]  In this particular case, there's no reason to have a lock at all.
[2264.00s -> 2269.00s]  What would be, could we think of a situation where you would want a receive lock?
[2269.00s -> 2273.00s]  Would it be like if you had two network cards?
[2273.00s -> 2280.00s]  Yeah, or if you remember in the UART code.
[2280.00s -> 2291.00s]  OK, so the main common reason why you might want to have a locking shared is because the bottom half and the top half share data structures.
[2291.00s -> 2296.00s]  So, for example, the receive and the transmit might manipulate the same shared data structures.
[2296.00s -> 2303.00s]  At that point, you need a lock. And so, for example, if you remember from the console driver, they shared a queue.
[2303.00s -> 2314.00s]  And so the receive handler needed to get the lock to actually get access to that queue.
[2314.00s -> 2319.00s]  Does that answer your question? Yeah, thanks.
[2319.00s -> 2329.00s]  So this brings me up with a second sort of point they wanted to make that the let me actually draw a new picture because it gets a little bit crowded.
[2329.00s -> 2335.00s]  So think about the bottom half again in the top half.
[2335.00s -> 2341.00s]  Here's the bottom, a little bit abstract, but it may be helpful.
[2341.00s -> 2352.00s]  In much, typically in much shopper and driver shopper, the bottom half basically doesn't really call into the top half at all.
[2352.00s -> 2359.00s]  So, for example, if you go back and you look at the console code, this sequence cannot happen there.
[2359.00s -> 2363.00s]  And the reason it doesn't happen is because the bottom half typically does very little work.
[2363.00s -> 2368.00s]  The only thing it does maybe is grab the packet and stick the packet on the queue.
[2368.00s -> 2381.00s]  And then at some point later, the top half, there's a separate thread in the top half that basically looks at that queue and then grabs packets out of the queue and then keeps on doing whatever needs to be done.
[2381.00s -> 2389.00s]  Now, for simplicity in this, for simplicity to reduce the amount of code, that is not the structure that this particular driver follows.
[2389.00s -> 2394.00s]  This driver, once in a while, may actually call up and call up.
[2394.00s -> 2409.00s]  Actually, sometimes this structure is followed by the driver, but once in a while it can actually go into the top half and come back out into the bottom half.
[2409.00s -> 2417.00s]  Any questions about this?
[2417.00s -> 2426.00s]  Okay, so that's sort of, I guess, software concurrency, and we'll come back to it a little more later if we look at the code in a little bit more detail.
[2426.00s -> 2432.00s]  Let's talk a little bit about the rings.
[2432.00s -> 2442.00s]  And so basically, the way to think about it, there's a ring, there's two rings, one for receiving and one for sending.
[2442.00s -> 2456.00s]  The TX and RX both live in RAM, and they're manipulated by code running on the RISC-V cores and by the network card itself.
[2456.00s -> 2464.00s]  And so there has to be some protocol between the network card and the RISC-V core about who gets to look at what.
[2464.00s -> 2473.00s]  And so the way, and this is a very common organization in hardware devices, the way that typically is done, there is sort of like, let's look at the transmit queue.
[2473.00s -> 2478.00s]  There's basically a queue of some fixed size structures, descriptors, as we'll look at in a second.
[2478.00s -> 2483.00s]  These are the descriptors.
[2483.00s -> 2501.00s]  And the organization or the coordination that is actually, that's happening between the driver and the network card is in consumer producer coordination.
[2502.00s -> 2511.00s]  Basically, one way to think about it, if this is the transmit queue, there's maybe there's a head pointer.
[2511.00s -> 2515.00s]  There's a tail pointer.
[2515.00s -> 2520.00s]  And the tail pointer is controlled by the software.
[2520.00s -> 2525.00s]  And so the software looks at the tail pointer and if it wants to send another packet,
[2525.00s -> 2533.00s]  it sticks it in the ring at the location of tail plus one and then moves up the tail pointer in that direction.
[2533.00s -> 2537.00s]  And the head pointer is basically controlled by the hardware.
[2537.00s -> 2541.00s]  So tail by software, head by hardware.
[2541.00s -> 2546.00s]  And then basically the hardware basically looks at the head and this is the first packet that is going to be sent.
[2546.00s -> 2553.00s]  And there's a little bit of information in the descriptor or enough information in the descriptor for the network card to look at it and say like,
[2553.00s -> 2559.00s]  this is the bytes that I need to move on to the cable.
[2559.00s -> 2563.00s]  And once it is done, it moves the head pointer in that direction.
[2563.00s -> 2568.00s]  Once it's consumed, one of the packets out of the transmission queue.
[2568.00s -> 2574.00s]  And one way to think about it is that all the descriptors between here,
[2574.00s -> 2581.00s]  between the tail and all the things that are actually being filled in.
[2581.00s -> 2587.00s]  So like the tail moved up maybe to here.
[2587.00s -> 2591.00s]  All the packets, all the descriptor entries that are actually filled in.
[2591.00s -> 2596.00s]  The way you think about them is they're sort of owned by the hardware, owned by the network card.
[2596.00s -> 2600.00s]  The network card is allowed to read them, do things with them.
[2600.00s -> 2604.00s]  But the software is not allowed to do anything with that.
[2604.00s -> 2613.00s]  If the software would be scribbling over these descriptors while they really sort of owned by the network card,
[2613.00s -> 2616.00s]  that would change the data that the network card sees.
[2616.00s -> 2619.00s]  And so that would be pretty undesirable.
[2619.00s -> 2625.00s]  And so the protocol basically is that like once the software moves the tail pointer one up,
[2625.00s -> 2632.00s]  then at that point what it just moved into the network queue and the transmission queue is now owned by the hardware.
[2632.00s -> 2637.00s]  And it will stick in, it will be owned by the hardware until it has to be sent,
[2637.00s -> 2644.00s]  and basically until the head pointer moves past to that particular structure.
[2644.00s -> 2653.00s]  Does that make sense?
[2654.00s -> 2656.00s]  For the transmission basically it's the same story.
[2656.00s -> 2665.00s]  There is a head pointer that's controlled by the hardware,
[2665.00s -> 2671.00s]  and there's a tail pointer that's controlled by the software pointer,
[2671.00s -> 2682.00s]  and basically the packets in between are the packets that actually have been received by the hardware.
[2682.00s -> 2692.00s]  And by inspecting the tail pointer, the software can see if there's actually a new packet that is ready to be consumed.
[2692.00s -> 2698.00s]  And if it's ready to be consumed, the hardware will indicate that by sending that DD bit in the status field.
[2698.00s -> 2704.00s]  And so when the DD bit is set, the hardware sees, oh, good, this packet, the hardware is done with it.
[2704.00s -> 2709.00s]  And so it can take it out and move the tail pointer one up.
[2709.00s -> 2721.00s]  And so there's sort of this give and take between the driver and our hardware to actually coordinate that they don't trip over each other.
[2721.00s -> 2730.00s]  Any questions about the rings?
[2730.00s -> 2740.00s]  Is this like a universal way to kind of implement cues or any kind of communication between two things that have shared memory?
[2740.00s -> 2747.00s]  Yeah, certainly also in the software you see this, but many hardware devices play this sort of trick,
[2747.00s -> 2760.00s]  where they produce a consumer style coordination between the hardware and the software.
[2760.00s -> 2763.00s]  So, yeah, it's a very common structure.
[2763.00s -> 2766.00s]  Maybe one or two questions that we can ask about it.
[2766.00s -> 2770.00s]  Like, why is there actually even a cue? Why is there a ring?
[2770.00s -> 2775.00s]  The reason there's a ring, correct, because the cue wraps around is to make a fixed size.
[2775.00s -> 2782.00s]  But why not cue one entry, one entry?
[2782.00s -> 2793.00s]  Like almost like the UART, if you remember the UART driver and controller is basically only one register to send a byte and there's another register to receive a byte.
[2793.00s -> 2797.00s]  And so why have, you know, why go for this complicated scheme?
[2797.00s -> 2804.00s]  If you could have yourself a single register and then you can basically say, you know, hardware, this register is now ready, send it.
[2804.00s -> 2809.00s]  And then you wait until it's done.
[2809.00s -> 2815.00s]  To allow for when there are bursts of packets?
[2815.00s -> 2822.00s]  Yeah, yeah, exactly. You know, the network interface or the network card or cable is pretty high performance.
[2822.00s -> 2825.00s]  In fact, it's very high performance, right, gigabits per second.
[2825.00s -> 2830.00s]  And so it can be hard for the core, the processor to actually keep up with it.
[2830.00s -> 2837.00s]  So you would like to give us a whole bunch of packets, you know, the network card can chunk along and all send them out at high speed.
[2837.00s -> 2844.00s]  And similarly, you know, in reception, you might get a burst of packets coming in and you want to place them into cue.
[2844.00s -> 2848.00s]  And then, you know, the operating system can start them processing the cue.
[2848.00s -> 2854.00s]  Right. And so this is a common.
[2854.00s -> 2863.00s]  This is the reason that these cues are common. To handle bursts.
[2863.00s -> 2873.00s]  What happens if the cue is full, like on reception?
[2873.00s -> 2881.00s]  I think the document mentioned that it does some form of a drop tail scheme.
[2881.00s -> 2885.00s]  Yeah, yeah, basically the packets get dropped.
[2885.00s -> 2891.00s]  So any future incoming packets, if the cue is full and the packets, there's no room anymore and they're wrong.
[2891.00s -> 2893.00s]  The network card can't do anything with it, correct?
[2893.00s -> 2902.00s]  And so the only option it actually has is to basically delete the packet or not add it to the ring and therefore disappears.
[2902.00s -> 2908.00s]  And so one reason of packet loss sometimes is that an operating system is overloaded.
[2908.00s -> 2915.00s]  You know, we can't keep up and the ring fills and then, you know, the packets get dropped.
[2915.00s -> 2927.00s]  Now, higher level software, of course, may like a TCP connection might retransmit those packets, but this is one reason why packets can get dropped.
[2927.00s -> 2936.00s]  So even if the hardware sort of works all perfectly fine, you know, because of these burstiness, you know, it can happen, the packet section might get dropped.
[2936.00s -> 2941.00s]  You had a really, really big burst.
[2941.00s -> 2948.00s]  The head and tail pointers, they're all software abstractions, right, of the cue?
[2948.00s -> 2952.00s]  OK, so these are actually turns out that those are these control registers.
[2952.00s -> 2960.00s]  Right. So there's a control register for the hardware head pointer is a control register for the basically the tail pointer.
[2960.00s -> 2962.00s]  There's no real distinction between hardware and software.
[2962.00s -> 2970.00s]  Basically, the driver knows about the tail pointer and it knows about the hardware knows about the tail pointer and the head pointer.
[2970.00s -> 2972.00s]  And these are basically control registers.
[2972.00s -> 2973.00s]  Right. OK, yeah.
[2973.00s -> 2977.00s]  We'll see in the code in a second how that shows up.
[2977.00s -> 2983.00s]  If you go back to our, maybe, let's see if I have this picture still here.
[2983.00s -> 2986.00s]  Here's the earlier picture that we looked at.
[2986.00s -> 2991.00s]  And, you know, here's the control register that holds the head and here the control register that holds the tail.
[2991.00s -> 2998.00s]  And here, of course, there's one for both reception and one for sending.
[2998.00s -> 3003.00s]  OK.
[3003.00s -> 3005.00s]  Let's go back here.
[3005.00s -> 3009.00s]  OK, so let's talk a little bit about the descriptors.
[3009.00s -> 3013.00s]  And so the descriptors are defined by the hardware.
[3013.00s -> 3016.00s]  So the hardware says, like, here's how a descriptor should look like.
[3016.00s -> 3020.00s]  And you driver, you know, these are the bits you can fill in.
[3020.00s -> 3023.00s]  And if you fill in that bit, that tells me the following.
[3023.00s -> 3029.00s]  And so here are the two descriptors that are important.
[3029.00s -> 3038.00s]  You know, here's the received descriptor, RX, and here's the TX, one of the TX descriptors.
[3038.00s -> 3041.00s]  So let's look a little bit.
[3041.00s -> 3044.00s]  Let's look a little bit at it.
[3044.00s -> 3048.00s]  Probably the most important part is this address.
[3048.00s -> 3055.00s]  And that is the address that the software filled in to say, like, where they should, where the car should dump the packets.
[3055.00s -> 3064.00s]  You know, where in RAM should the driver, where should the network card put the data that it receives into RAM.
[3064.00s -> 3071.00s]  So that's the address for the reception buffer, if you will.
[3071.00s -> 3079.00s]  And then, you know, probably the most important part is the status field, as you've seen.
[3079.00s -> 3086.00s]  So when the driver basically still continues to look sort of at the tail and sees if there's a new packet has been received,
[3086.00s -> 3095.00s]  and the way it tells whether there's a new packet received is that the DD bit, correct, is set by the hardware.
[3095.00s -> 3105.00s]  And, you know, the shaded areas are the fields of the packet descriptor or the received descriptor that actually the hardware fills in.
[3105.00s -> 3111.00s]  And the white, you know, the non-shaded ones are the fields that are actually shaded in, are filled in by the software.
[3111.00s -> 3123.00s]  Again, here we see this clear distinction about some things are owned by software, some things are owned by the hardware.
[3123.00s -> 3132.00s]  OK, and there's a similar descriptor, a very similar descriptor that looks, that's the hardware descriptor for the transmission.
[3132.00s -> 3140.00s]  It has an address, and that address, of course, is the address where the data is in memory that needs to be sent.
[3140.00s -> 3143.00s]  And then it has a bunch of, you know, it has a command field.
[3143.00s -> 3149.00s]  And here's how the software tells the driver, tells the network card, this is what you should do with this particular packet,
[3149.00s -> 3152.00s]  or this is what you should know about this particular packet.
[3152.00s -> 3157.00s]  And so one thing, I think we fill in two things.
[3157.00s -> 3161.00s]  We fill the EOP that says, you know, the end of packet.
[3161.00s -> 3166.00s]  And that basically tells the driver, this is the last descriptor of a particular packet.
[3166.00s -> 3172.00s]  And now it can, you know, send off whatever data that's in these descriptors.
[3172.00s -> 3184.00s]  And I think we set the response requested command bit, or RS, I think it is.
[3184.00s -> 3196.00s]  That basically tells the network card when you're done transmitting this thing, you know, set the bit that it actually has been transmitted.
[3196.00s -> 3206.00s]  We'll see in a second how that shows up. Any questions?
[3206.00s -> 3211.00s]  So one thing to keep in mind, you know, with these structures, these are defined by the hardware.
[3211.00s -> 3214.00s]  The software has no control over their structure.
[3214.00s -> 3219.00s]  You know, they're just literally defined by the hardware, by the network card.
[3219.00s -> 3225.00s]  OK, let's look at a little bit of code and see what the solution looks like.
[3225.00s -> 3228.00s]  And we'll look at a couple of issues that we haven't looked at yet.
[3228.00s -> 3235.00s]  You know, the MBUS issues and doing transmission and the issues during receiving.
[3235.00s -> 3243.00s]  And this should be presumably all well familiar with to you, given the fact that you just finished the lab.
[3243.00s -> 3250.00s]  OK, so here, just to go to the top, here's the transmit ring.
[3250.00s -> 3257.00s]  And that's basically the ring of descriptors that we grew off in the last couple of slides.
[3257.00s -> 3263.00s]  There's a separate, you know, we talked briefly about this, there's a separate basically ring of MBUS.
[3263.00s -> 3270.00s]  But that ring is completely, you know, a software or driver only abstraction.
[3270.00s -> 3276.00s]  The hardware, this is defined by the structure or is defined by the hardware.
[3276.00s -> 3297.00s]  In fact, if you look at, oops.
[3297.00s -> 3301.00s]  You look at the definition, it's correct, here's the struct, you know, TS desk.
[3301.00s -> 3306.00s]  And that corresponds to the C version of exactly the structure that actually was defined by the hardware.
[3306.00s -> 3313.00s]  You know, the 64-bit address, you know, the length field, the CSO byte, the command byte, the status byte, the CCS byte.
[3313.00s -> 3320.00s]  And then, you know, to a word for special. And there's a similar ring that is literally defined by the hardware.
[3320.00s -> 3323.00s]  Or similar descriptor, receive descriptor that's defined by the hardware.
[3323.00s -> 3329.00s]  And we just have C decorations corresponding to these hardware structures.
[3329.00s -> 3337.00s]  All right, let's look at transmit. So here's the sort of staff solution for transmit.
[3337.00s -> 3343.00s]  It acquires a lock. And the reason, as we said before, that it needed a lock is because multiple functions or multiple threads,
[3343.00s -> 3348.00s]  kernel threads might call transmit at the same time.
[3348.00s -> 3355.00s]  And then it looks at the tail, you know, to see if there's any, if there's room to actually send a new packet,
[3355.00s -> 3362.00s]  because, you know, the network hardware got behind and all the descriptors actually are being in use.
[3362.00s -> 3375.00s]  And so the way it checks whether actually the descriptor is in use is by checking whether the DD bit is not set.
[3375.00s -> 3386.00s]  And if it is set, then, if it's not set, then it returns immediately, not sending the packet at all.
[3386.00s -> 3394.00s]  And in any other case, it keeps on going and actually will send the m-buff.
[3394.00s -> 3400.00s]  So if there's still an m-buff in this position, in the TxM-buff, so that means this is an m-buff from a while back,
[3400.00s -> 3405.00s]  you know, where you free that m-buff, because we're going to actually stick in a new m-buff into this descriptor.
[3405.00s -> 3409.00s]  And we just need to remember which m-buff we stuck in that descriptor.
[3409.00s -> 3414.00s]  And so the way we do that is by keeping that in the TxM-buff.
[3414.00s -> 3419.00s]  And then we fill in the descriptor. And so the first thing we fill in is the head.
[3419.00s -> 3428.00s]  And we fill in, we put in the address, the start of the first header in the m-buff.
[3428.00s -> 3436.00s]  And then put the length, set the status to zero, put the command field in place by sending both the RS and the EOP bit,
[3436.00s -> 3445.00s]  as we discussed the second. And then basically we synchronize to make actually sure that there's no reordering of,
[3445.00s -> 3450.00s]  you know, that all these instructions are performed before the next instruction.
[3450.00s -> 3453.00s]  And what the next instruction does, it actually updates the tail pointer.
[3453.00s -> 3460.00s]  And so it updates the tail pointer, and this tail pointer is a control register in the network card.
[3460.00s -> 3467.00s]  And so basically when we update the tail pointer, the network card knows something changed and is going to read,
[3467.00s -> 3474.00s]  you know, the data at that descriptor and is going to basically read these fields we just filled in.
[3474.00s -> 3481.00s]  So we got to make absolutely sure that all these, you know, writes, all these stores are materialized in memory
[3481.00s -> 3488.00s]  before the network card actually reads them. And so this is why the sync synchronizes there.
[3488.00s -> 3495.00s]  And then we release a lot, and that's basically all we have to do to just make a packet.
[3495.00s -> 3499.00s]  Can you explain the sync synchronize again? Like what?
[3499.00s -> 3507.00s]  Yeah, it's mostly a memory ordering technicality, correct? Like compilers are free to reorder instructions.
[3507.00s -> 3513.00s]  The writes might actually sit in the L1 cache or the L12 cache.
[3513.00s -> 3520.00s]  And basically the synchronizes is sort of a memory fence that tells the hardware and the compiler,
[3520.00s -> 3527.00s]  please don't move any instructions across this barrier and make sure that all the data that, you know,
[3527.00s -> 3533.00s]  all the stores that before this barrier are actually materialized in RAM.
[3533.00s -> 3541.00s]  And the reason that is important is because the, you know, go back to our picture from a little while ago,
[3541.00s -> 3549.00s]  all the way back here, make sure both, you know, the RISC-V cores that run the driver code,
[3549.00s -> 3555.00s]  you know, basically put values here in RAM, correct? But, you know, those values are first in the L1 cache and the L2 cache
[3555.00s -> 3558.00s]  and the memory flushing ensures that they actually show up in the RAM.
[3558.00s -> 3561.00s]  This is important because as soon as we update the tail pointer,
[3561.00s -> 3567.00s]  the E1000 is going to look in RAM for these two descriptors and we've got to make sure that all the,
[3567.00s -> 3577.00s]  you know, fields are indeed set. Otherwise it would reach stale values. Does that make sense?
[3577.00s -> 3581.00s]  Yep. Thank you.
[3581.00s -> 3595.00s]  So why do AMBOs exist?
[3595.00s -> 3604.00s]  Why have these sort of separate structure of, you know, why have these two sort of corresponding structures?
[3604.00s -> 3613.00s]  So I guess part of it is it's all nice and well if the driver needs to talk to the hardware about where things are,
[3613.00s -> 3618.00s]  but at the end of the day, we do need to store the packet to hand off to the network stack somehow.
[3618.00s -> 3624.00s]  Yeah, exactly. You know, so one way to think about it is that, you know, the AMBOs are completely an OS abstraction,
[3624.00s -> 3627.00s]  nothing to do with really the network card at all.
[3627.00s -> 3634.00s]  And it is there because, like, you know, if at some point, you know, maybe you receive a packet, you know,
[3634.00s -> 3640.00s]  we hand it off to the network stack and the network stack sticks in some queue so that when later a user process calls read,
[3640.00s -> 3643.00s]  you know, it actually can read it. In the meantime,
[3643.00s -> 3647.00s]  we need to have some structure that sort of holds that data that we actually received.
[3647.00s -> 3655.00s]  And that's exactly what those AMBOs structures are.
[3655.00s -> 3664.00s]  Sorry, can you talk about the like, how do you use the linked list structure in your code?
[3664.00s -> 3668.00s]  Because I was trying to to understand it and to track it down.
[3668.00s -> 3678.00s]  And it seems like there is some push tail or push head, but I don't ever call it in my code.
[3678.00s -> 3684.00s]  Oh, it might be in the net.c file.
[3684.00s -> 3688.00s]  Well, the net.c file is the file that we gave you.
[3688.00s -> 3694.00s]  Yeah.
[3694.00s -> 3697.00s]  And where are you thinking?
[3697.00s -> 3708.00s]  If you look for something like pull pop, I think pop.
[3708.00s -> 3718.00s]  Yeah, so this is this is the only place where it looks like it does things with the linked list structure of AMBOF.
[3719.00s -> 3723.00s]  And if you don't ever call this function or the push tail,
[3723.00s -> 3732.00s]  you basically just treat the AMBOF as an array of however many characters.
[3732.00s -> 3736.00s]  So, so why is AMBOF there?
[3736.00s -> 3742.00s]  Because here, when you receive a UDP packet,
[3742.00s -> 3747.00s]  so the UDP packet comes in from the network card, it goes into the network stack.
[3747.00s -> 3757.00s]  Then the UDP packet, the AMBOF for that UDP that holds that UDP packet is actually stuck in the queue right here.
[3757.00s -> 3762.00s]  On the socket for the receiver.
[3762.00s -> 3769.00s]  And so this is the reason that you can have queues of AMBOFs.
[3769.00s -> 3772.00s]  And the reason it's stuck in the queue there, correct, is because, you know, who knows?
[3772.00s -> 3776.00s]  The usual process that's going to read from this particular socket
[3776.00s -> 3780.00s]  might be doing something else, and at some point later, you know, it's going to call the read system call,
[3780.00s -> 3786.00s]  and then the read system call is going to remove, you know, that AMBOF from that socket.
[3786.00s -> 3796.00s]  And so the point of each socket can have a list of AMBOFs for reception.
[3796.00s -> 3800.00s]  I see. Thank you.
[3800.00s -> 3803.00s]  Okay, let's go back to the driver.
[3803.00s -> 3807.00s]  So that was transmit.
[3807.00s -> 3813.00s]  So then receive is, you know, almost similar, not that much different.
[3813.00s -> 3818.00s]  You know, basically you read the tail pointer from the network card.
[3818.00s -> 3821.00s]  You see if there actually is a packet.
[3821.00s -> 3826.00s]  If there's a packet, then the DD field would be set.
[3826.00s -> 3831.00s]  If the DD field is not set, then we know there's no packet, and so we're done.
[3831.00s -> 3836.00s]  If it is set, then we're going to read it.
[3836.00s -> 3841.00s]  And read the length, you know, and when we...
[3841.00s -> 3846.00s]  And then we call nNetRx, you know, do the up call into the higher level stack.
[3846.00s -> 3853.00s]  And when we're done, you know, we're basically freed up, you know, that entry
[3853.00s -> 3857.00s]  so that the driver can use it to actually receive more packets in it.
[3857.00s -> 3860.00s]  And the real crucial operation there is correct.
[3860.00s -> 3863.00s]  And we bump up basically the tail, you know, to tell the driver,
[3863.00s -> 3866.00s]  okay, you know, we're done with it, you can use it again.
[3866.00s -> 3870.00s]  Now it's yours again, that slot.
[3870.00s -> 3874.00s]  So maybe the most interesting question is, and the number of questions you have,
[3874.00s -> 3880.00s]  you asked us, you know, why is the while there?
[3880.00s -> 3882.00s]  Why is this in a loop?
[3882.00s -> 3885.00s]  Don't you get an interrupt and then, you know, you grab the one packet and you'll be done?
[3885.00s -> 3889.00s]  Or like a number of people that actually forgot the while loop first, you know,
[3889.00s -> 3897.00s]  noticed that basically at some point you don't receive packets anymore during net tests.
[3897.00s -> 3902.00s]  Is it because you would like to transmit as many packets that are ready as possible
[3902.00s -> 3908.00s]  with only one interrupt so that you can kind of amortize the cost of the interrupt?
[3908.00s -> 3910.00s]  Yeah, that's an absolute part of it.
[3910.00s -> 3915.00s]  You know, you get to the, let's first talk about, you know, the root cause is right.
[3915.00s -> 3922.00s]  If a number, you know, the burst of packets comes in and the network card will generate,
[3922.00s -> 3924.00s]  you know, the interrupt, but more packets are coming in.
[3924.00s -> 3930.00s]  And so it will just, you know, put them in the three descriptors that it actually can use.
[3930.00s -> 3933.00s]  It can't generate more interrupts because it already generated interrupt.
[3933.00s -> 3937.00s]  And at some point, you know, the, you know, whatever the processor,
[3937.00s -> 3939.00s]  you know, maybe you're like, we were the core version,
[3939.00s -> 3943.00s]  just in a section of code that turned off interrupts because it was doing something atomic.
[3943.00s -> 3945.00s]  At the end of it, you know, there's a releasing from lock
[3945.00s -> 3947.00s]  that actually turns interrupts or not.
[3947.00s -> 3951.00s]  And that point in time, then basically the receive handler is,
[3951.00s -> 3953.00s]  the interrupt handler is going to interrupt, right?
[3953.00s -> 3955.00s]  But between the period when like that first packet comes in
[3955.00s -> 3957.00s]  and before the receive handler runs,
[3957.00s -> 3962.00s]  there's a whole bunch of other packets might have come, could have come in.
[3962.00s -> 3965.00s]  And so we don't run in the while loop.
[3965.00s -> 3969.00s]  We'll leave those packets, we'll grab the first one,
[3969.00s -> 3973.00s]  then we'll grab the other and the later ones stick there in the queue, right?
[3973.00s -> 3975.00s]  They're just sitting there in that ring.
[3975.00s -> 3979.00s]  Now, if a subsequent interrupt would come in, we'll grab the next one.
[3979.00s -> 3984.00s]  But like the tests, you know, what they do, they run their UDP packets, you know,
[3984.00s -> 3989.00s]  so they send off a whole bunch, like one of the tests sends like 10 ping requests in parallel.
[3989.00s -> 3993.00s]  You know, the 10 responses come in parallel back, you know, correct?
[3993.00s -> 3996.00s]  The first one receives, generates an interrupt.
[3996.00s -> 3999.00s]  The other nine go into the queue.
[3999.00s -> 4003.00s]  The higher level software, you know, reached the first one.
[4003.00s -> 4006.00s]  And then, you know, returns just like it's done.
[4006.00s -> 4008.00s]  And at that point, nothing happens anymore
[4008.00s -> 4011.00s]  because the receiver is waiting for more receive packets.
[4011.00s -> 4013.00s]  They happen to be there, they're sitting in the ring,
[4013.00s -> 4016.00s]  except, you know, the receive didn't do them in the while loop
[4016.00s -> 4018.00s]  and so it didn't pick them up.
[4020.00s -> 4022.00s]  Does that make sense?
[4024.00s -> 4028.00s]  So for example, if the test programs would have sent one request,
[4028.00s -> 4032.00s]  waited for response, one request, waited for response,
[4032.00s -> 4036.00s]  then you would never notice that you needed to do the while loop.
[4036.00s -> 4039.00s]  It is because like there's a burst of responses might come back
[4039.00s -> 4043.00s]  and that only generate together one interrupt.
[4046.00s -> 4048.00s]  Any questions about this?
[4049.00s -> 4051.00s]  Any questions about this?
[4051.00s -> 4053.00s]  I have a question.
[4053.00s -> 4057.00s]  So for example, if instead of while one,
[4057.00s -> 4060.00s]  we would have a for loop that just goes through the whole queue,
[4060.00s -> 4064.00s]  that would also be wrong for the same reason?
[4064.00s -> 4068.00s]  No, as long as we skip all the ones that have the DD not set.
[4069.00s -> 4073.00s]  We should only look at packets that actually,
[4073.00s -> 4075.00s]  we should only look at the tail pointer.
[4076.00s -> 4080.00s]  But in principle, you could just scan the whole queue
[4080.00s -> 4082.00s]  and I guess look at the DD packets
[4082.00s -> 4084.00s]  and let it see if the DD bit is set.
[4084.00s -> 4085.00s]  And that might work.
[4085.00s -> 4087.00s]  I don't know, I haven't tried that.
[4088.00s -> 4094.00s]  Okay, but isn't the device able to put more M-Buffs
[4094.00s -> 4098.00s]  in the receive after we read tail?
[4099.00s -> 4101.00s]  Yes, yeah, yeah, yeah.
[4103.00s -> 4104.00s]  Okay.
[4106.00s -> 4107.00s]  And we have to,
[4116.00s -> 4118.00s]  it's very dangerous like this proposal
[4118.00s -> 4119.00s]  of like sort of looping around
[4119.00s -> 4121.00s]  and seeing the DB be the set
[4121.00s -> 4123.00s]  because really what we should do,
[4123.00s -> 4125.00s]  we should not look at buffers
[4125.00s -> 4127.00s]  that really are controlled by the driver
[4127.00s -> 4128.00s]  or by the hardware
[4128.00s -> 4131.00s]  and only look at buffers that are actually available to us
[4131.00s -> 4132.00s]  through the software
[4133.00s -> 4135.00s]  which is indicated by the DD bit
[4135.00s -> 4137.00s]  and basically by looking at the tail pointer
[4138.00s -> 4140.00s]  because basically anything owned between the tail
[4140.00s -> 4143.00s]  and the head is owned by the hardware.
[4147.00s -> 4149.00s]  Going back to the M-Buff,
[4149.00s -> 4151.00s]  like the reason we need the M-Buff,
[4151.00s -> 4155.00s]  why do we need the other information in the buffer
[4155.00s -> 4157.00s]  like the care array?
[4158.00s -> 4160.00s]  I think it's called the backing store.
[4163.00s -> 4165.00s]  I think it's in net.h.
[4175.00s -> 4177.00s]  So we need the buff field, correct?
[4177.00s -> 4179.00s]  Because it actually contains the data.
[4181.00s -> 4184.00s]  I thought the head contained a pointer to the data.
[4184.00s -> 4185.00s]  Yeah, yeah, yeah.
[4185.00s -> 4186.00s]  Okay, but the buff actually is the actual,
[4186.00s -> 4189.00s]  we allocate space to whole packets, correct?
[4189.00s -> 4191.00s]  And then head points into buff
[4192.00s -> 4194.00s]  to basically distort the packet.
[4196.00s -> 4197.00s]  Okay, okay.
[4199.00s -> 4200.00s]  And this is M-Buff,
[4200.00s -> 4203.00s]  this is, okay, so a little bit more context here.
[4203.00s -> 4204.00s]  M-Buff is a structure
[4204.00s -> 4206.00s]  that you see in a lot of network stacks.
[4206.00s -> 4207.00s]  So you look in the Linux kernel
[4207.00s -> 4209.00s]  and you will see a structure similar like this
[4209.00s -> 4211.00s]  and it's typically called M-Buff.
[4211.00s -> 4212.00s]  And so we have to,
[4214.00s -> 4216.00s]  we could have simplified this structure quite a bit
[4216.00s -> 4217.00s]  if we wanted to
[4217.00s -> 4219.00s]  and maybe in subsequent years we should
[4220.00s -> 4221.00s]  but this is sort of the standard way
[4221.00s -> 4223.00s]  in which packets are represented
[4223.00s -> 4225.00s]  in operating system kernels.
[4234.00s -> 4236.00s]  Okay, any further questions about this?
[4245.00s -> 4247.00s]  So I have a choice now.
[4247.00s -> 4248.00s]  There's a couple of things
[4248.00s -> 4250.00s]  that I could say about the network driver.
[4251.00s -> 4254.00s]  But then I won't have any time to talk about MMAP.
[4254.00s -> 4256.00s]  I could also talk a little bit about MMAP
[4257.00s -> 4258.00s]  and stop here
[4259.00s -> 4261.00s]  in terms of talking to the network driver.
[4264.00s -> 4265.00s]  Any preferences?
[4279.00s -> 4280.00s]  No preferences?
[4281.00s -> 4282.00s]  No.
[4286.00s -> 4288.00s]  Let me talk a little bit about MMAP
[4288.00s -> 4291.00s]  because there was a question that came up a couple of times
[4291.00s -> 4294.00s]  and that might be worth addressing.
[4296.00s -> 4300.00s]  It's almost more a question about why MMAP exists
[4300.00s -> 4303.00s]  than actually how to implement it.
[4304.00s -> 4306.00s]  So if there's no real objections,
[4306.00s -> 4307.00s]  which is something recent,
[4307.00s -> 4310.00s]  let me talk a little bit more about MMAP.
[4311.00s -> 4313.00s]  So this is your last chance to ask any questions
[4313.00s -> 4316.00s]  about the network driver until the end of class.
[4321.00s -> 4322.00s]  Okay.
[4324.00s -> 4327.00s]  Let's talk about MMAP.
[4338.00s -> 4339.00s]  Okay.
[4342.00s -> 4345.00s]  Okay, so the question really relates
[4346.00s -> 4348.00s]  to sort of the file system API.
[4348.00s -> 4352.00s]  And so the file system has a reasonable broad API,
[4352.00s -> 4355.00s]  supporting directories, symbolic links,
[4355.00s -> 4357.00s]  hard links, et cetera, et cetera.
[4357.00s -> 4361.00s]  But presumably the main part of it is the file API.
[4361.00s -> 4365.00s]  The file API is hopefully well familiar to you now.
[4365.00s -> 4367.00s]  You can open a file
[4368.00s -> 4370.00s]  with some permissions.
[4370.00s -> 4372.00s]  And once you have opened the file,
[4372.00s -> 4375.00s]  you can read from the file
[4378.00s -> 4381.00s]  into a buffer, read some data,
[4383.00s -> 4386.00s]  and you can write through the file,
[4388.00s -> 4391.00s]  whatever file descriptor, buff, length.
[4391.00s -> 4395.00s]  So let's say we, and then we can close.
[4396.00s -> 4400.00s]  And then we can close the file descriptor.
[4400.00s -> 4402.00s]  Now let's say we want to write an application
[4402.00s -> 4405.00s]  and you can think about the,
[4405.00s -> 4407.00s]  maybe that's the file as a,
[4407.00s -> 4410.00s]  the file is normally an array of bytes,
[4410.00s -> 4412.00s]  but like maybe this application has,
[4412.00s -> 4415.00s]  sort of it's an array of structs that are stored in it.
[4415.00s -> 4417.00s]  And so let's say the application
[4417.00s -> 4418.00s]  wants to update the struct.
[4418.00s -> 4421.00s]  So we have the file, right?
[4421.00s -> 4422.00s]  Here's our file.
[4422.00s -> 4426.00s]  So divided in, you know, structs, whatever, zero, two,
[4426.00s -> 4428.00s]  you know, here's the file length.
[4428.00s -> 4431.00s]  And, you know, we maybe like in the middle,
[4431.00s -> 4434.00s]  let's have the struct maybe 16 bytes or something.
[4434.00s -> 4436.00s]  And, you know, we want to like update one of these,
[4436.00s -> 4440.00s]  or maybe you want to update the first one.
[4440.00s -> 4442.00s]  And so when the way we could do that correctly,
[4442.00s -> 4444.00s]  if we read and we opened the file,
[4444.00s -> 4446.00s]  we read, you know, the first, you know,
[4446.00s -> 4450.00s]  whatever 16 bytes that correspond to this, you know,
[4450.00s -> 4453.00s]  maybe, you know, let's say update, you know,
[4453.00s -> 4455.00s]  the first, you know, field or struct,
[4455.00s -> 4458.00s]  and the first byte of that struct, you know, to be one.
[4458.00s -> 4462.00s]  And then we call write to basically write it back.
[4462.00s -> 4464.00s]  And then we close the file.
[4464.00s -> 4466.00s]  So when does the write happens?
[4466.00s -> 4468.00s]  Where does it actually write?
[4475.00s -> 4479.00s]  So we have set the memory above to one,
[4479.00s -> 4482.00s]  you know, will this first byte change to one?
[4487.00s -> 4488.00s]  If we call write.
[4503.00s -> 4504.00s]  So remember the file descriptor
[4504.00s -> 4506.00s]  has an offset associated with it, correct?
[4507.00s -> 4511.00s]  And, you know, read pushes that offset, you know, forward.
[4511.00s -> 4513.00s]  In this case, if we read 16 bytes,
[4513.00s -> 4516.00s]  the offset goes from zero to 16.
[4516.00s -> 4518.00s]  And so where does write write?
[4521.00s -> 4524.00s]  As you notice, there's no offset argument
[4524.00s -> 4526.00s]  in the write system call.
[4526.00s -> 4528.00s]  So where does it write?
[4531.00s -> 4532.00s]  Len?
[4533.00s -> 4535.00s]  You know, it will write Len bytes
[4535.00s -> 4537.00s]  and which offset in the file?
[4538.00s -> 4541.00s]  Presumably at Len exactly at 16, correct?
[4541.00s -> 4542.00s]  Yes.
[4542.00s -> 4543.00s]  It will write here.
[4543.00s -> 4546.00s]  So which, so this byte will change to one, correct?
[4548.00s -> 4551.00s]  Not the actual thing
[4551.00s -> 4553.00s]  that we were actually trying to modify.
[4554.00s -> 4555.00s]  Does that make sense?
[4556.00s -> 4558.00s]  So the way you would have written this application,
[4558.00s -> 4560.00s]  it turns out there's one system call
[4560.00s -> 4562.00s]  that we didn't really talk much about at all,
[4562.00s -> 4564.00s]  but every operating system, Unix operating system,
[4564.00s -> 4567.00s]  is a system called Lseq.
[4568.00s -> 4571.00s]  And Lseq allows you basically to change the offset
[4571.00s -> 4573.00s]  through some position.
[4573.00s -> 4575.00s]  And so in this particular case,
[4575.00s -> 4577.00s]  we want to do half the effect
[4577.00s -> 4580.00s]  that we're gonna write this struct and not that one.
[4580.00s -> 4583.00s]  Then we would have set like, you know, Lseq zero,
[4583.00s -> 4588.00s]  basically resetting the offset to actually the zero.
[4588.00s -> 4590.00s]  And so then this write would actually,
[4590.00s -> 4592.00s]  so here now offset is zero.
[4592.00s -> 4594.00s]  And then this write actually, you know,
[4594.00s -> 4597.00s]  would write actually at the first 16 bytes.
[4598.00s -> 4599.00s]  Does that make sense?
[4606.00s -> 4607.00s]  So this is slightly inconvenient, correct?
[4607.00s -> 4609.00s]  If you think about this, you know,
[4609.00s -> 4610.00s]  like you look at this interface, you know,
[4610.00s -> 4612.00s]  there's actually quite a bit of stuff going on.
[4612.00s -> 4614.00s]  You know, you gotta read it, you know,
[4614.00s -> 4616.00s]  and you're gonna seek it and then you're gonna write it.
[4616.00s -> 4619.00s]  And so one reason that mmap is popular
[4621.00s -> 4625.00s]  is you can sort of avoid, you know, these complications
[4626.00s -> 4628.00s]  because if you would write this using mmap,
[4628.00s -> 4630.00s]  you know, we would do fd is open
[4630.00s -> 4632.00s]  and we open the file as before.
[4635.00s -> 4638.00s]  And then, you know, we just call mmap.
[4640.00s -> 4642.00s]  It's gonna be like whatever our struct,
[4642.00s -> 4645.00s]  let's say our struct is H, you know,
[4646.00s -> 4648.00s]  blah, blah, blah, you know, whatever.
[4648.00s -> 4653.00s]  Some length for the file, you know, read, write,
[4656.00s -> 4659.00s]  permissions, you know, map shared
[4662.00s -> 4665.00s]  and then file descriptor comma zero.
[4666.00s -> 4670.00s]  And then, so now, so basically with this dot script,
[4670.00s -> 4672.00s]  we're in mapping, you know, the file F
[4673.00s -> 4676.00s]  at the location H, you know, in memory.
[4676.00s -> 4678.00s]  Now we can just like say H, you know,
[4678.00s -> 4683.00s]  whatever first byte is zero or one as we did before.
[4683.00s -> 4685.00s]  And then, you know, at some point, you know,
[4685.00s -> 4688.00s]  we can mmap, you know, mmap H, right?
[4696.00s -> 4697.00s]  And so if we think about this,
[4697.00s -> 4699.00s]  when we look in memory,
[4699.00s -> 4702.00s]  we're looking at this from the perspective of the file,
[4702.00s -> 4703.00s]  you know, what this does, correct?
[4703.00s -> 4705.00s]  This code literally did is basically you got a pointer,
[4705.00s -> 4708.00s]  you know, to this H and it can just update here,
[4708.00s -> 4711.00s]  you know, that thing to one and be done with it,
[4711.00s -> 4712.00s]  right?
[4712.00s -> 4715.00s]  And so if we sort of need to manipulate,
[4715.00s -> 4717.00s]  you know, file structure, you know,
[4717.00s -> 4719.00s]  data structure that sit inside of a file,
[4719.00s -> 4722.00s]  this mmap interface is much more convenient than,
[4722.00s -> 4725.00s]  you know, this previous interface where, you know,
[4725.00s -> 4728.00s]  we have to read, you know, we manipulated in memory,
[4728.00s -> 4730.00s]  you know, we, you know, seek back,
[4730.00s -> 4732.00s]  and then we actually write the information.
[4732.00s -> 4734.00s]  And in this particular case, you know,
[4734.00s -> 4737.00s]  we really can think about the file as being an array of
[4737.00s -> 4740.00s]  bytes, you know, that we can write willy nilly,
[4740.00s -> 4743.00s]  without actually having to navigate or jump around.
[4746.00s -> 4751.00s]  Do people see what the advantage of the mmap interface is?
[4758.00s -> 4760.00s]  Any questions about this?
[4763.00s -> 4766.00s]  Do any big storage systems use this?
[4766.00s -> 4768.00s]  Like, I don't know, like databases,
[4768.00s -> 4771.00s]  do they use this a lot or not really?
[4771.00s -> 4773.00s]  They use this a lot, yeah.
[4773.00s -> 4775.00s]  I think a lot of applications will use mmap
[4775.00s -> 4777.00s]  because this is for these kinds of things,
[4777.00s -> 4779.00s]  like a database is much more convenient
[4782.00s -> 4783.00s]  than this interface.
[4783.00s -> 4784.00s]  This interface is great, correct?
[4784.00s -> 4786.00s]  If you need to read from standard input
[4786.00s -> 4788.00s]  and there's a stream of bytes coming in,
[4788.00s -> 4789.00s]  you just read, you read, you read,
[4789.00s -> 4792.00s]  and then you produce some output on some other stream.
[4793.00s -> 4794.00s]  That interface, you know,
[4794.00s -> 4796.00s]  that interface is very suitable for here.
[4796.00s -> 4798.00s]  You don't have to update offset, you know, you read,
[4798.00s -> 4800.00s]  you know, offset immediately update, you read again,
[4800.00s -> 4802.00s]  you read a little bit further, et cetera, et cetera.
[4802.00s -> 4806.00s]  So it was really good for sort of the stream oriented inputs.
[4806.00s -> 4808.00s]  While this is really good, you know,
[4808.00s -> 4811.00s]  if the file has sort of a data structure
[4811.00s -> 4814.00s]  and you just want to sort of update
[4815.00s -> 4817.00s]  bits and pieces of the file and, you know,
[4817.00s -> 4818.00s]  arbitrary locations.
[4826.00s -> 4827.00s]  Okay.
[4831.00s -> 4832.00s]  Let's see.
[4832.00s -> 4836.00s]  So I'm going to start, I'm about to run out of time.
[4837.00s -> 4840.00s]  So let me stop here.
[4840.00s -> 4843.00s]  Hopefully this is one sort of tidbit about mmap,
[4843.00s -> 4846.00s]  you know, it's useful to know
[4846.00s -> 4849.00s]  and tells you why, you know,
[4849.00s -> 4851.00s]  that sort of is a popular scheme.
[4855.00s -> 4856.00s]  Let's see.
[4856.00s -> 4858.00s]  So I think for that actually, you know,
[4858.00s -> 4861.00s]  we were basically, we come to the end of this lecture
[4861.00s -> 4863.00s]  and I guess the end of,
[4867.00s -> 4870.00s]  the end of 6S081.
[4870.00s -> 4872.00s]  And so again, you know,
[4872.00s -> 4876.00s]  if you have not filled out the feedback survey,
[4876.00s -> 4879.00s]  you know, please do so.
[4879.00s -> 4881.00s]  You know, we really appreciate it here,
[4881.00s -> 4883.00s]  fill out the subject evaluation.
[4883.00s -> 4888.00s]  And again, I want to thank you for your attention.
[4888.00s -> 4890.00s]  Again, it is a strange semester.
[4890.00s -> 4893.00s]  I wish I could have met you all in person
[4893.00s -> 4895.00s]  and talk to you in person,
[4895.00s -> 4898.00s]  either in class or before class or after class.
[4899.00s -> 4903.00s]  And unfortunately that hasn't happened,
[4904.00s -> 4907.00s]  but I hope you got quite a bit out of S081
[4907.00s -> 4909.00s]  and certainly with the staff
[4909.00s -> 4911.00s]  thank you for the engagement.
[4911.00s -> 4914.00s]  You guys have been wonderful this semester.
[4914.00s -> 4916.00s]  So with that, thank you.
[4917.00s -> 4920.00s]  And if you have any questions, please hang around.
[4921.00s -> 4923.00s]  But I'm sure some of you have deadlines.
[4924.00s -> 4927.00s]  I want to say on behalf of the TAs as well,
[4927.00s -> 4930.00s]  we've really enjoyed all the time we spent with you guys
[4930.00s -> 4932.00s]  in office hours and doing checkoffs.
[4932.00s -> 4934.00s]  And this is the first semester where we've done checkoffs.
[4934.00s -> 4938.00s]  And I think David and I both found that a really enjoyable process.
[4938.00s -> 4941.00s]  And, you know, hopefully we were able to help you guys out
[4941.00s -> 4943.00s]  and have some good conversations as well.
[4943.00s -> 4945.00s]  So thanks for making it a great semester
[4945.00s -> 4947.00s]  and adjusting on the fly with us
[4947.00s -> 4950.00s]  with all of the remote learning problems
[4950.00s -> 4952.00s]  we've run into and had to deal with.
[4952.00s -> 4954.00s]  It's been an absolute pleasure achieving this course.
[4958.00s -> 4961.00s]  Any further questions?
[4962.00s -> 4965.00s]  Again, feel free to hang around.
[4975.00s -> 4977.00s]  Oh, I had a question.
[4977.00s -> 4978.00s]  Yeah.
[4978.00s -> 4981.00s]  So it's it's Oh, yeah.
[4981.00s -> 4982.00s]  Thank you so much for the class.
[4982.00s -> 4983.00s]  I really like the class.
[4983.00s -> 4985.00s]  It's the best class this semester.
[4986.00s -> 4990.00s]  I wanted to ask about the interrupts, the receive interrupts.
[4990.00s -> 4994.00s]  How does the how does the hardware knows
[4994.00s -> 4997.00s]  when when the software has finished
[4998.00s -> 5002.00s]  handling the previous interrupt to not issue another one?
[5003.00s -> 5006.00s]  Because there are multiple steps.
[5008.00s -> 5010.00s]  One is looking
[5015.00s -> 5017.00s]  you want thousands at the bottom.
[5018.00s -> 5021.00s]  We've been able to interrupt or we tell the card, you know,
[5021.00s -> 5023.00s]  we're ready to receive further interrupts.
[5024.00s -> 5025.00s]  Oh, yeah.
[5025.00s -> 5027.00s]  Don't tell the car to generate interrupts.
[5027.00s -> 5030.00s]  Of course, at this point, interrupts actually not really turned on.
[5030.00s -> 5032.00s]  And we just tells the car you can generate interrupts again.
[5033.00s -> 5035.00s]  The interrupts get turned on, correct?
[5035.00s -> 5038.00s]  Like, you know, by, you know, these functions, you know, interrupt, enable.
[5039.00s -> 5041.00s]  And at that point, if you turn on the interrupt,
[5041.00s -> 5044.00s]  you're able if this interrupt, if this this field was set plus,
[5045.00s -> 5047.00s]  you know, at the point where you turn on interrupt enable
[5047.00s -> 5049.00s]  and actually interrupt might happen again.
[5050.00s -> 5052.00s]  Right. I forgot about this register.
[5052.00s -> 5053.00s]  Thank you so much.
[5053.00s -> 5055.00s]  Yeah, it's like one instruction.
[5056.00s -> 5057.00s]  Easy to miss.
[5060.00s -> 5061.00s]  Any other questions?
[5065.00s -> 5066.00s]  Thank you all.
[5067.00s -> 5068.00s]  Thank you.
[5071.00s -> 5072.00s]  Thank you.
[5073.00s -> 5076.00s]  Thank you so much.
