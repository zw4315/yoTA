# Detected language: en (p=1.00)

[0.00s -> 19.52s]  I'd like to spend today's lecture talking about threads and how xv6 does thread switching.
[19.52s -> 24.08s]  This is sort of one of our under the hood lectures about xv6.
[24.08s -> 30.68s]  We had lectures before about how system calls, interrupts, page tables, and locks work.
[30.68s -> 38.28s]  Today we're going to talk about how it is that xv6 switches among different processes.
[38.28s -> 41.82s]  The reason for this, the highest level reason for this, is that people like their computer
[41.82s -> 45.94s]  to be able to do more than one task at the same time.
[45.94s -> 50.52s]  So the reason might be that you're supporting time sharing, like Athena allows many users
[50.52s -> 54.08s]  to log in at the same time, and they can all run processes.
[54.08s -> 59.46s]  Or even on a single user machine, or even your iPhone, you may run many different processes
[59.46s -> 66.74s]  and expect the computer to do all the things you ask of it, not just one thing.
[66.74s -> 72.20s]  Another reason that people like to support multiple tasks is because it can ease program
[72.20s -> 73.20s]  structure.
[73.20s -> 78.28s]  Threads in particular, today's topic, are sometimes used as a way to help people, to
[78.28s -> 84.12s]  help programmers put together a program in a sort of simple, elegant way to reduce complexity.
[84.12s -> 88.28s]  And you actually saw an example of this in the first lab with the prime number sieve,
[88.28s -> 94.04s]  which didn't use threads exactly, but used multiple processes in order to help structure
[94.04s -> 96.80s]  this your prime number sieve software.
[96.80s -> 103.40s]  And arguably it's sort of a more convenient or elegant or simpler way to write that software.
[104.40s -> 110.80s]  And a final reason why people use threads is to get parallel speed up from multi-core machines.
[110.80s -> 118.40s]  So it's common to break up your program in a way that using threads to allow different
[118.40s -> 121.36s]  parts of the same program to run on different cores.
[121.36s -> 125.80s]  And if you can, maybe if you're lucky, if you can split your program up to run
[125.80s -> 130.24s]  on four threads on four cores, you might be able to get a factor of four speed up
[130.24s -> 133.08s]  in how fast it runs.
[133.08s -> 139.08s]  And indeed, you can view the xe6 kernel as a multi-core parallel program.
[139.08s -> 146.92s]  So what threads are, is an abstraction to simplify programming when you have many tasks,
[146.92s -> 148.96s]  when you want to juggle many tasks.
[148.96s -> 154.40s]  So what a thread is, is you can think of a thread as just being a single serial
[154.40s -> 155.40s]  execution.
[155.40s -> 160.60s]  If you just write a program that does one thing after another and you run that program,
[160.60s -> 164.60s]  you can view the program as a sort of single thread of control.
[164.60s -> 173.16s]  So this is a loose definition because there's many different sort of flavors of what people
[173.16s -> 173.96s]  mean by threads.
[173.96s -> 179.48s]  But we'll say it's one serial execution.
[179.48s -> 187.16s]  So it's what you get if you fire up one CPU and have it just execute one instruction
[187.16s -> 188.72s]  after another in the ordinary way.
[191.60s -> 195.16s]  We often talk about a thread having state, because it's going to turn out we're going
[195.16s -> 197.72s]  to want to save away a thread state and restore it later.
[197.72s -> 204.48s]  And so the right way to think about a thread state, for the most part, the most important
[204.48s -> 207.60s]  part perhaps of the thread state is its program counter.
[207.60s -> 212.08s]  Because it's an execution, we care a lot about where is it in its execution, and what
[212.08s -> 215.04s]  address is it executing instructions.
[215.04s -> 220.00s]  But also, we care about the rest of the microprocessor state that's required to support
[220.04s -> 221.04s]  this execution.
[221.04s -> 227.60s]  And so that means it's the state of a thread includes the registers that the compiler
[227.60s -> 233.72s]  uses to hold variables, and also because the way the compiler generates code, a thread
[233.72s -> 235.20s]  state includes a stack.
[235.20s -> 240.88s]  So typically, each thread has its own stack dedicated to executing that thread, and the
[240.88s -> 249.28s]  stack records the record of function calls that reflect the current point in the execution
[249.84s -> 250.84s]  of that thread.
[251.84s -> 256.72s]  And so what a threading system, xv6 includes a threading system inside it, what a
[256.72s -> 262.92s]  threading system does is manage, it's interleave, automate the interleaving of
[262.92s -> 264.20s]  multiple threads.
[264.76s -> 269.08s]  We'd like to be able to fire up two or four, a hundred or a thousand threads and
[269.08s -> 275.00s]  have the threading system figure out how to juggle all those threads and cause them
[275.00s -> 276.88s]  all to make progress and all to execute.
[279.88s -> 283.04s]  And there's really two main strategies.
[283.04s -> 284.44s]  So we want to interleave.
[286.00s -> 292.76s]  This is going to be a big topic here is how to interleave threads, many threads.
[294.08s -> 297.92s]  One way to interleave the execution of many threads is to have multiple CPUs.
[301.32s -> 306.92s]  Maybe as on a multi-core processor, and then each CPU can run its own thread.
[306.92s -> 312.08s]  So if you have four CPUs, there's an obvious way to run, sorry, four threads is to run
[312.08s -> 316.44s]  one thread per CPU, and then each thread automatically gets its own program counter
[316.44s -> 320.96s]  and registers, that is the program counter and registers associated with the CPU it's
[320.96s -> 321.96s]  running on.
[321.96s -> 329.24s]  But if you have four CPUs and you have a thousand threads, then using one core
[329.24s -> 332.44s]  per thread is not going to be enough of an answer.
[332.68s -> 340.08s]  And so the other main strategy that we'll see, indeed, the topic of most of this lecture
[340.08s -> 347.56s]  is how each CPU is going to switch among different threads.
[347.56s -> 353.36s]  So if I have one CPU and a thousand threads, we're going to see how xv6 builds a switching
[353.36s -> 358.48s]  system that allows xv6 to run one thread for a while and then switch and set aside
[358.48s -> 362.24s]  and save the state of that one thread and switch to executing a second thread
[362.24s -> 365.64s]  for a while and then the third thread and so forth until it's executed a little bit
[365.64s -> 370.48s]  of each thread and then go back and execute more of the first thread and so on.
[372.28s -> 378.64s]  And indeed, xv6, like most operating system, combines these xv6 or run threads on all
[378.64s -> 383.60s]  the cores that are available and each core will switch among threads because there's
[383.60s -> 387.40s]  typical meant typically, although not always, there's typically many more threads
[387.40s -> 389.04s]  than there are CPUs.
[389.56s -> 398.56s]  One of the many ways in which different threading systems or instances of threading
[398.56s -> 401.60s]  systems differ is in whether or not they share memory.
[403.00s -> 409.44s]  So this is a pertinent point.
[411.32s -> 415.60s]  One possibility is that you could have a single address space with many threads
[415.60s -> 418.88s]  executing in that address space, and then they see each other's changes.
[419.20s -> 424.08s]  If one of the threads sharing some memory modifies a variable, then the other thread
[424.08s -> 425.88s]  sharing that memory will see the modification.
[427.56s -> 431.00s]  And so it's in the context of threads running and sharing memory that we need
[431.00s -> 434.52s]  things like the locks that you saw in the last lecture.
[436.48s -> 439.52s]  xv6 kernel is shared memory.
[440.24s -> 445.96s]  So xv6 supports the notion of kernel threads.
[445.96s -> 450.76s]  There's one kernel thread per process that executes system calls for that process.
[451.04s -> 453.56s]  All those kernel threads share kernel memory.
[454.36s -> 456.84s]  So xv6 kernel threads do share memory.
[460.52s -> 465.60s]  And on the other end, xv6 has another kind of threads.
[465.60s -> 469.84s]  Each user process essentially has a single thread of control that executes
[469.84s -> 471.92s]  the user instructions for that process.
[472.80s -> 478.08s]  And indeed, a lot of the xv6 kernel threading machinery is ultimately in
[478.08s -> 483.04s]  support of being able to support and switch among many user processes.
[483.12s -> 487.12s]  Each user process has a memory and a single thread that runs in that memory.
[487.52s -> 499.20s]  So xv6 user processes, each process has one thread.
[499.20s -> 506.64s]  And so there's no sharing of memory among threads within a single xv6 user process.
[506.64s -> 509.80s]  Of course, you can have multiple processes, but each of those processes
[509.84s -> 511.84s]  is in a draft space with a single thread.
[513.12s -> 515.36s]  The processes in xv6 don't share memory.
[516.00s -> 519.56s]  In other more sophisticated operating systems, for example, Linux.
[521.40s -> 528.40s]  Linux at user level does allow multiple threads in a process
[528.44s -> 532.84s]  and the processes and those threads share the memory of that single process.
[533.56s -> 537.52s]  And that's super cool if you want to write user level programs that use
[537.52s -> 542.00s]  level parallel programs that get speed up from multiple cores, but it requires
[542.00s -> 545.64s]  sort of another, it uses a lot of the same basic techniques we're going
[545.64s -> 549.88s]  to talk about today, but there's a certain amount more sophistication in Linux
[549.88s -> 554.64s]  to get it to keep track of multiple threads per process instead of just one.
[558.40s -> 563.24s]  Okay, at a sort of high level, I just want to mention that there's other
[563.24s -> 568.28s]  ways to support the interviewing of multiple tasks on a single computer.
[569.32s -> 572.76s]  And we're not going to talk about them, but if you're curious, you can
[572.76s -> 578.40s]  look up things like event-driven programming or state machines, and these
[578.40s -> 583.48s]  are non-thread techniques to share one computer among many different tasks.
[583.72s -> 588.84s]  It turns out, you know, and sort of on the spectrum of different schemes
[588.84s -> 592.72s]  for supporting multiple tasks on a computer, threads are not very efficient.
[592.72s -> 596.56s]  There's other more efficient schemes, but threads are usually the most
[596.56s -> 603.24s]  convenient way, the most programmer friendly way to support lots of different tasks.
[605.96s -> 611.96s]  Okay, there's a couple of challenges that we're going to have to bite off
[612.00s -> 614.52s]  if we want to implement a threading system.
[618.40s -> 621.84s]  The, so this is just high level challenges.
[629.16s -> 633.24s]  One is, as I mentioned before, how to actually implement the switching
[634.12s -> 639.16s]  for interleave, the switching that allows us to interleave
[641.32s -> 643.12s]  the execution of multiple threads.
[644.36s -> 649.80s]  And this sort of broad name for this process of switching, deciding,
[649.80s -> 653.56s]  oh, I'm going to leave off one thread and start executing another thread.
[653.56s -> 654.72s]  It's often called scheduling.
[657.76s -> 661.92s]  And we'll see that xv6 indeed has an actual piece of code.
[661.92s -> 664.80s]  There's the scheduler, indeed it has multiple schedulers, one per core.
[665.80s -> 670.80s]  But the general idea of how do you drive the decision to switch from one to another,
[670.80s -> 673.88s]  how to pick the next thread to run, it's called scheduling.
[677.44s -> 680.88s]  Another question is if you want to actually implement the switch from one thread to another,
[680.88s -> 683.16s]  you need to save and restore.
[683.16s -> 688.04s]  So we need to decide what needs to be saved and where to save it.
[689.04s -> 693.16s]  When, what needs to be saved when we leave off executing one thread and restored
[693.16s -> 696.88s]  when we want to resume executing that thread at some later time.
[697.88s -> 700.84s]  And final question is what to do about compute bound threads.
[704.12s -> 710.56s]  The, many of the options, many of the most straightforward options for thread switching
[710.92s -> 714.44s]  involve the threads voluntarily saying, well, I'm going to save away my state
[714.44s -> 717.48s]  and sort of run another, you know, let another thread be run.
[717.88s -> 722.20s]  But what if we have a user program that's, you know, doing some long running calculation
[722.20s -> 726.04s]  that might take hours, it's not going to be particularly thinking about,
[726.04s -> 728.64s]  oh, now would be time to, good time to let something else run.
[728.96s -> 734.76s]  So it's most convenient to have some way of just sort of automatically revoking control
[734.76s -> 737.32s]  from some long running compute bound process,
[738.20s -> 740.36s]  setting it aside and maybe running it later.
[743.28s -> 745.00s]  All right, so I'm going to talk about these.
[745.40s -> 750.96s]  I'm actually going to talk about the machinery for dealing with compute bound threads first.
[753.28s -> 760.12s]  And the scheme for that is something you've come up before, that's timer interrupts.
[764.44s -> 771.40s]  And the idea here is that there's a little piece of hardware on each CPU,
[771.40s -> 775.16s]  on each core, that generates periodic interrupts.
[776.04s -> 779.36s]  And the XP6 or any operating system really arranges to, you know,
[779.36s -> 781.00s]  have those interrupts delivered to the kernel.
[781.00s -> 784.36s]  So even if we're running at user level in some loop,
[784.36s -> 787.68s]  that's, you know, computing the first billion digits of pi,
[788.16s -> 792.08s]  nevertheless, the timer interrupts are going to go off at some point,
[792.08s -> 796.92s]  maybe every 10 milliseconds, and transfer control from that user level code
[797.12s -> 798.92s]  into the interrupt handler in the kernel.
[799.00s -> 804.44s]  And so that's the sort of first step in the kernel being able to gain control
[804.44s -> 808.48s]  to switch among different user level processes, user level threads,
[808.84s -> 811.92s]  even if those user level threads aren't cooperative.
[815.32s -> 818.88s]  And the basic scheme is that in the interrupt handler,
[818.88s -> 823.00s]  so we're going to have a kernel handler for these interrupts.
[824.00s -> 830.36s]  And we'll see that the kernel handler yields,
[830.36s -> 833.08s]  this is the sort of usual name for this, it yields,
[835.52s -> 839.88s]  the kernel handler sort of voluntarily yields the CPU back to the scheduler
[839.88s -> 842.56s]  and tells the scheduler, look, you can let something else run now.
[844.40s -> 849.16s]  And this yielding is really a form of thread switch
[849.16s -> 852.28s]  that saves away the state of the current thread.
[852.84s -> 854.04s]  So it can be restored later.
[856.48s -> 858.52s]  As we'll see, the full story here,
[858.52s -> 860.12s]  actually, you've seen a lot of the full story here
[860.12s -> 863.12s]  because it involves an interrupt, which you already know about,
[863.12s -> 864.48s]  the full story is somewhat complex,
[864.48s -> 868.64s]  but the basic idea is that a timer interrupt gives control to the kernel
[868.64s -> 871.72s]  and the kernel voluntarily yields the CPU.
[873.00s -> 877.00s]  This is called as a piece of terminology, preemptive scheduling.
[882.88s -> 889.88s]  And what that means is that, what the preemptive means is that
[889.88s -> 894.20s]  even if the code that's running doesn't, you know, doesn't want to,
[894.20s -> 897.32s]  you know, doesn't explicitly yield the CPU,
[897.32s -> 900.28s]  the timer interrupt is going to take control away
[900.28s -> 901.60s]  and we're going to yield for it.
[901.60s -> 904.40s]  And the opposite of preemptive scheduling
[904.40s -> 908.24s]  might be called maybe voluntary scheduling.
[908.24s -> 914.88s]  And the interesting thing is that the implementation in XV6
[914.88s -> 917.36s]  and other operating systems of preemptive scheduling
[917.36s -> 920.68s]  is this timer interrupt, which forcibly takes away the CPU.
[920.68s -> 924.88s]  And then the kernel basically does a voluntary yield
[924.88s -> 927.36s]  or thread switch on behalf of that process.
[930.44s -> 936.84s]  Now, another just piece of terminology that comes up here
[936.84s -> 943.48s]  is that while a thread's running, there's a, we need to distinguish,
[943.48s -> 945.32s]  the system needs to distinguish between threads
[945.32s -> 948.96s]  that are currently actually running on some CPU
[948.96s -> 952.16s]  versus threads that would like to run,
[952.16s -> 954.44s]  but aren't currently running on any CPU,
[954.44s -> 957.64s]  but, you know, could run if a CPU became free
[957.64s -> 960.16s]  versus threads that actually don't want to run
[960.16s -> 964.76s]  because they're waiting for IO or waiting for some event.
[964.84s -> 969.24s]  And unfortunately, this distinction is often called state,
[969.24s -> 971.16s]  even though the full state of a thread
[971.16s -> 973.16s]  is actually much more complicated than that.
[976.20s -> 978.68s]  Since this is going to come up,
[978.68s -> 980.56s]  I just want to list out a couple of states
[980.56s -> 982.60s]  that we'll be seeing.
[982.60s -> 985.48s]  And these are states that XV6 actually maintains.
[985.48s -> 988.04s]  There's a state called running, which means, ah,
[988.04s -> 991.88s]  it's actually executing on some CPU right now.
[991.88s -> 997.80s]  There's runnable, which means not currently executing anywhere,
[997.80s -> 1000.40s]  but just a saved state,
[1000.40s -> 1002.72s]  but would like to run as soon as possible.
[1002.72s -> 1004.64s]  And then it turns out there's a state
[1004.64s -> 1006.32s]  which won't come out much today,
[1006.32s -> 1008.56s]  but won't come up next week called sleeping,
[1008.56s -> 1011.68s]  which means the thread's waiting for some IO event
[1011.68s -> 1014.52s]  and only wants to run after the IO event occurs.
[1014.52s -> 1017.64s]  So today we're mostly concerned with running and runnable threads.
[1017.64s -> 1019.76s]  And what this preemptive switch does,
[1019.76s -> 1021.68s]  what this timer interrupt does and yield
[1021.68s -> 1023.88s]  is basically convert a running thread,
[1023.88s -> 1026.04s]  whatever thread was interrupted by the timer,
[1026.04s -> 1028.36s]  into a runnable thread.
[1028.36s -> 1030.36s]  That is the thread that's, by yielding,
[1030.36s -> 1032.32s]  you're converting that thread into a thread
[1032.32s -> 1034.68s]  that's not running right now, but would actually like to,
[1034.68s -> 1036.48s]  clearly, because it was running at the time
[1036.48s -> 1040.48s]  of the timer interrupt.
[1040.48s -> 1043.04s]  Okay, so a running thread,
[1043.04s -> 1047.52s]  its program counter and registers are actually in the CPU,
[1047.52s -> 1049.52s]  you know, in the hardware registers of the CPU
[1049.56s -> 1051.68s]  that's executing it.
[1051.68s -> 1055.20s]  A runnable thread, though, has no, you know,
[1055.20s -> 1057.80s]  doesn't have a CPU associated with it.
[1057.80s -> 1061.20s]  And therefore we need to save, for every runnable state,
[1061.20s -> 1065.68s]  we need to save whatever CPU state,
[1065.68s -> 1068.80s]  whatever state the CPU was keeping
[1068.80s -> 1071.92s]  when that thread was running.
[1071.92s -> 1074.08s]  So we need to copy the CPU contents, you know,
[1074.08s -> 1076.64s]  which is not RAM, but just registers really,
[1076.64s -> 1080.40s]  from the CPU into memory somewhere to save them
[1080.40s -> 1082.84s]  when we turn a thread from running to runnable.
[1082.84s -> 1085.32s]  And again, this is the, basically, the state
[1085.32s -> 1089.20s]  we have to explicitly save here is just the state,
[1089.20s -> 1090.80s]  the executing state in the CPU,
[1090.80s -> 1097.00s]  which is the program counter and the registers in the CPU.
[1097.00s -> 1099.20s]  So these need to be saved
[1099.20s -> 1101.88s]  when we convert a thread to runnable.
[1101.88s -> 1106.04s]  When some scheduler finally decides to run a runnable thread,
[1106.04s -> 1107.60s]  then as part of the many steps
[1107.60s -> 1111.40s]  in getting that thread going again and resuming it,
[1111.40s -> 1113.16s]  we're going to see that the program count,
[1113.16s -> 1115.28s]  the saved program counter registers
[1115.28s -> 1120.44s]  are copied back into the CPU's actual register on the CPU
[1120.44s -> 1125.16s]  that the scheduler decides to run it on.
[1125.16s -> 1126.72s]  All right.
[1126.72s -> 1129.44s]  Any questions about these terminology?
[1136.08s -> 1138.12s]  All right, I'm going to now sort of talk about
[1138.12s -> 1144.00s]  a sort of more XV6-oriented view of things.
[1144.00s -> 1148.60s]  I'm going to draw two pictures, really, of threads in XV6
[1148.60s -> 1149.80s]  that are kind of simplified picture
[1149.80s -> 1151.76s]  and a more detailed picture.
[1151.76s -> 1155.96s]  So as usual, we have the user stuff up here
[1155.96s -> 1160.24s]  and the kernel down here.
[1160.24s -> 1164.04s]  We might be running, you know, multiple processes at user level,
[1164.04s -> 1170.64s]  maybe, you know, the C compiler and the LS and the shell.
[1170.64s -> 1172.44s]  You know, they may or may not be all wanting
[1172.44s -> 1175.52s]  to run at the same time.
[1175.52s -> 1184.60s]  At user level, each of these processes has,
[1184.60s -> 1189.12s]  you know, it has memory and of particular interest to us.
[1189.12s -> 1193.44s]  Each of these processes has a user stack
[1193.48s -> 1198.76s]  and while it's running, it has registers in the RISC-V hardware,
[1198.76s -> 1202.04s]  so PC plus registers, right?
[1202.04s -> 1204.32s]  So while the program is running, you know,
[1204.32s -> 1206.32s]  there's essentially a thread of control
[1206.32s -> 1208.96s]  that's running up at user level.
[1208.96s -> 1210.64s]  And the way I'm going to talk about it
[1210.64s -> 1215.20s]  is as if there's a user thread that
[1215.20s -> 1218.20s]  consists of the user stack, user memory, user program
[1218.20s -> 1219.88s]  counter, user registers.
[1219.88s -> 1222.24s]  If the program makes the system callers interrupted
[1222.28s -> 1226.60s]  and goes into the kernel, then this stuff
[1226.60s -> 1229.76s]  saved away in this program's trap frame
[1229.76s -> 1235.84s]  and a kernel, the kernel thread for this program
[1235.84s -> 1236.88s]  is activated.
[1236.88s -> 1241.92s]  And so this is the trap frame holds saved user stuff
[1241.92s -> 1245.16s]  after we saved away the user program counter registers.
[1245.16s -> 1251.60s]  Then we switch the CPU to using the kernel stack
[1251.60s -> 1253.76s]  and we don't need to restore registers
[1253.76s -> 1258.76s]  because the kernel thread for process
[1258.76s -> 1262.88s]  isn't really running and has no real safe state when
[1262.88s -> 1265.36s]  the user thread is running.
[1265.36s -> 1267.04s]  Instead, it's sort of the kernel thread
[1267.04s -> 1269.12s]  is kind of activated on its stack
[1269.12s -> 1274.88s]  the first time in the trampoline and user trap code.
[1278.40s -> 1280.60s]  And then the kernel runs for a while maybe
[1280.64s -> 1282.48s]  running a system call or an interrupt handler
[1282.48s -> 1284.48s]  or whatever it may be.
[1284.48s -> 1287.68s]  And sometimes, if it's a system call in particular,
[1287.68s -> 1289.56s]  we'll just simply return from this point
[1289.56s -> 1293.12s]  back to the same process and the return to user space
[1293.12s -> 1297.24s]  will restore this program counter registers.
[1297.24s -> 1301.32s]  But it could also be that instead of simply returning
[1301.32s -> 1303.36s]  for one reason or another, maybe because it was a timer
[1303.36s -> 1306.12s]  interrupt, we're actually going to switch to another process.
[1306.12s -> 1308.08s]  And the very high level view of that
[1308.08s -> 1311.44s]  is that if the XPSS scheduler decides
[1311.44s -> 1316.08s]  to switch from this process to a different process,
[1316.08s -> 1317.52s]  the first thing that really happens
[1317.52s -> 1320.68s]  is that we're going to switch kernel threads
[1320.68s -> 1322.64s]  from this process's kernel thread
[1322.64s -> 1324.48s]  to the other process's kernel thread.
[1324.48s -> 1326.24s]  And then the other process's kernel thread
[1326.24s -> 1327.76s]  will turn back to user space.
[1327.76s -> 1329.84s]  So supposing that the C compiler, say,
[1329.84s -> 1332.24s]  needs to read the disk, and so it's
[1332.24s -> 1334.36s]  going to yield the CPU while it's
[1334.36s -> 1336.60s]  sleeping to wait for the disk read to complete,
[1336.64s -> 1340.84s]  maybe LS wants to execute and is in runnable state.
[1340.84s -> 1343.04s]  What the XPSS scheduler maybe may do
[1343.04s -> 1346.40s]  is that, well, if LS is in runnable state,
[1346.40s -> 1348.24s]  that means it left off somewhere
[1348.24s -> 1352.08s]  and its state was saved away, possibly by a timer interrupt.
[1352.08s -> 1354.76s]  And so LS will actually have a saved trap
[1354.76s -> 1359.80s]  frame with user registers and its own kernel stack.
[1359.80s -> 1365.44s]  And as it turns out, a saved set of kernel registers
[1365.44s -> 1368.04s]  associated with the kernel thread, which is going
[1368.04s -> 1369.80s]  to be called the context.
[1369.80s -> 1373.56s]  So if xv6 switches from the compiler kernel thread
[1373.56s -> 1378.08s]  to LS's kernel thread, xv6 will save away
[1378.08s -> 1380.84s]  the kernel registers in a context
[1380.84s -> 1386.04s]  for the compiler's kernel thread,
[1386.04s -> 1390.84s]  switch to the LS thread, complex scheme, which
[1390.84s -> 1393.04s]  I'll describe a little bit later,
[1393.04s -> 1397.72s]  will restore LS's kernel thread registers
[1397.72s -> 1399.88s]  from the previously saved context
[1399.88s -> 1402.24s]  from when LS last left off.
[1402.24s -> 1404.24s]  Maybe LS will finish whatever system call
[1404.24s -> 1410.80s]  it was executing on the LS's kernel thread stack
[1410.80s -> 1412.72s]  and then return back to LS from the system call
[1412.72s -> 1414.64s]  and on the way to returning to user space
[1414.64s -> 1418.12s]  that will restore these previously saved user registers
[1418.12s -> 1421.80s]  for LS and then resume executing LS.
[1421.80s -> 1426.04s]  So there's a bunch of details here, which we'll talk about.
[1426.04s -> 1428.76s]  But maybe the main point here is
[1428.76s -> 1433.48s]  that we never in xv6 see direct user to user context
[1433.48s -> 1436.44s]  switches when we're switching from one process to another.
[1436.44s -> 1441.72s]  Always the sort of strategy by which xv6
[1441.72s -> 1444.84s]  switches from executing one process to another process
[1444.84s -> 1447.72s]  is you jump into the kernel, save the process state,
[1447.72s -> 1450.00s]  run this process's kernel thread,
[1450.00s -> 1452.80s]  switch to the kernel thread of another process that
[1452.80s -> 1456.12s]  suspended itself and then return and restore user registers.
[1456.12s -> 1458.52s]  So it's always the sort of indirect strategy.
[1458.52s -> 1460.64s]  It's actually even more indirect than this
[1460.64s -> 1462.92s]  to thread switch where the net effect is
[1462.92s -> 1465.84s]  to switch from one user process to another user
[1465.84s -> 1466.36s]  process.
[1470.04s -> 1472.48s]  Questions about this diagram or anything?
[1475.48s -> 1477.08s]  Switch to the scheduler.
[1477.08s -> 1479.40s]  That happens in between those two, right?
[1479.44s -> 1481.72s]  Yep, all right.
[1481.72s -> 1483.24s]  Let me talk about the scheduler.
[1483.24s -> 1487.08s]  So the real picture is actually significantly more complex
[1487.08s -> 1489.16s]  than that.
[1489.16s -> 1494.04s]  This is going to be a more full diagram.
[1494.04s -> 1498.32s]  Let's say we have process one, which is executing,
[1498.32s -> 1502.92s]  and process two, which is runnable but not currently
[1502.92s -> 1504.12s]  running.
[1504.12s -> 1505.72s]  Now, the additional layer of details,
[1505.72s -> 1508.04s]  we actually have multiple cores.
[1508.08s -> 1510.20s]  In XP6, let's say we have two cores.
[1510.20s -> 1514.92s]  So that means that sort of at the hardware level,
[1514.92s -> 1518.80s]  we have CPU 0, which is one of the cores,
[1518.80s -> 1520.92s]  and let's say CPU 1.
[1526.16s -> 1528.16s]  And the more full story about how
[1528.16s -> 1534.88s]  we get from executing user space to in one process
[1534.88s -> 1537.52s]  executing user spaces in another runnable
[1537.52s -> 1540.08s]  but not yet running process.
[1540.08s -> 1543.20s]  Now, the first part is about the same as I talked about.
[1543.20s -> 1547.76s]  It may say a timer interrupt forces transfer control
[1547.76s -> 1549.88s]  from the user process into the kernel.
[1549.88s -> 1553.68s]  The trampoline code saves the user registers
[1553.68s -> 1557.60s]  in the trap frame for process one
[1557.60s -> 1562.00s]  and then executes user trap, which figures out
[1562.00s -> 1564.92s]  what to do with this trap or interrupt system call.
[1564.92s -> 1567.48s]  Let's say for a little while we're
[1567.48s -> 1572.32s]  executing ordinary kernel C code on the kernel
[1572.32s -> 1573.88s]  stack of process one.
[1576.52s -> 1578.96s]  Let's say process one, the kernel code for process one
[1578.96s -> 1582.16s]  decides it wants to yield the CPU.
[1582.16s -> 1583.64s]  It does a bunch of things, which
[1583.64s -> 1586.32s]  we'll see details of, that end up
[1586.32s -> 1589.00s]  in a call to this routine switch,
[1589.00s -> 1592.40s]  just sort of the central routines in this story.
[1592.44s -> 1597.52s]  Switch saves away this context, the registers
[1597.52s -> 1599.92s]  for the kernel thread that's running in context one.
[1599.92s -> 1601.60s]  So there's two sets of registers,
[1601.60s -> 1604.20s]  the user registers in the trap frame,
[1604.20s -> 1608.64s]  the kernel thread registers in the context.
[1608.64s -> 1613.48s]  Switch switches from one thread to another.
[1613.48s -> 1616.96s]  But in fact, the way xe6 is designed,
[1616.96s -> 1621.40s]  the only place that a user thread, sorry,
[1621.40s -> 1623.88s]  the kernel thread running on a CPU can switch to
[1623.88s -> 1626.48s]  is what's called the scheduler thread for that CPU.
[1629.16s -> 1632.00s]  So we can't even switch directly to another process.
[1632.00s -> 1633.84s]  We can only switch to the scheduler thread.
[1633.84s -> 1640.60s]  So there's the complete thread apparatus dedicated
[1640.60s -> 1642.68s]  to the scheduler for CPU zero.
[1642.68s -> 1644.56s]  Since we're running on CPUs, this switch
[1644.56s -> 1649.72s]  is going to switch to the previously saved registers
[1649.72s -> 1651.56s]  for the scheduler thread.
[1651.56s -> 1655.72s]  So let's say it's scheduler zero.
[1655.72s -> 1659.44s]  And in the scheduler for CPU zero,
[1659.44s -> 1662.24s]  switch will, by restoring these registers,
[1662.24s -> 1664.68s]  since the registers include the stack pointer,
[1664.68s -> 1666.84s]  the return from switch, as we'll see,
[1666.84s -> 1678.60s]  will now actually return up to the scheduler function
[1678.64s -> 1679.92s]  on CPU zero.
[1679.92s -> 1682.48s]  And the scheduler function will do some cleanup
[1682.48s -> 1684.92s]  to finish putting process one to sleep.
[1684.92s -> 1687.48s]  Then it'll look in the process table
[1687.48s -> 1690.76s]  for another process to run, a runnable process.
[1690.76s -> 1695.24s]  And if it finds one, and so we've sort of gone down here
[1695.24s -> 1697.88s]  and up into the scheduler, if the scheduler finds
[1697.88s -> 1699.96s]  another process to run, or even finds,
[1699.96s -> 1702.44s]  if process one is runnable and still wants to run,
[1702.44s -> 1705.96s]  it may find process one if nothing else wants to run.
[1706.00s -> 1709.08s]  But in any case, the scheduler will call switch again
[1709.08s -> 1714.08s]  to switch contexts to, say, process two,
[1714.08s -> 1716.72s]  in the process of which it'll save its own registers
[1716.72s -> 1719.40s]  again in its own context.
[1719.40s -> 1722.84s]  There'll be a previously saved context two
[1722.84s -> 1725.12s]  from whenever process two left off,
[1725.12s -> 1728.92s]  that this set of registers will be restored.
[1728.92s -> 1734.68s]  Process two will have made a previous call to switch
[1734.68s -> 1736.52s]  to the scheduler thread, just like process one
[1736.52s -> 1737.96s]  did when it left off.
[1737.96s -> 1741.48s]  That call to switch will return to whatever system
[1741.48s -> 1743.96s]  call or interrupt process two was in.
[1743.96s -> 1747.40s]  When that's finished, there will be a previously saved trap
[1747.40s -> 1750.36s]  frame for process two that will contain user registers.
[1750.36s -> 1753.44s]  Those will be restored and will return back up
[1753.44s -> 1756.52s]  into user space.
[1756.52s -> 1762.08s]  And there's a complete separate scheduler thread for each CPU.
[1762.08s -> 1766.52s]  So there will also be saved context
[1766.52s -> 1773.92s]  for the scheduler thread for CPU one and a scheduler loop
[1773.92s -> 1775.12s]  running on scheduler one.
[1775.12s -> 1777.92s]  And whatever process, process three or something,
[1777.92s -> 1780.92s]  is running on CPU one, when it decides to give up the CPU,
[1780.92s -> 1786.00s]  it'll switch into the scheduler thread for it,
[1786.00s -> 1787.24s]  for its CPU.
[1787.24s -> 1794.28s]  All right, there's a question, where are the contexts stored?
[1794.28s -> 1797.60s]  It turns out that for the operations I've
[1797.60s -> 1803.40s]  been talking about, the saved, in fact, always,
[1803.40s -> 1808.56s]  for a thread switch, these contexts,
[1808.56s -> 1810.60s]  these saved register sets for kernel threads,
[1810.60s -> 1812.48s]  are in the process structure.
[1812.48s -> 1814.88s]  So any given kernel thread can only
[1814.92s -> 1818.52s]  have one set of saved kernel registers,
[1818.52s -> 1820.80s]  because each thread is only executing
[1820.80s -> 1821.92s]  at sort of a single place.
[1821.92s -> 1824.88s]  And its context kind of reflects that place
[1824.88s -> 1826.80s]  that it was executing when it left off,
[1826.80s -> 1829.64s]  because a thread is a single thread of control.
[1829.64s -> 1832.48s]  So a thread really only needs one context
[1832.48s -> 1833.56s]  full of registers.
[1833.56s -> 1838.60s]  So it's in the process structure, it's P arrow, P arrow
[1838.60s -> 1841.52s]  context.
[1841.52s -> 1844.84s]  And each scheduler thread has its own context,
[1844.84s -> 1846.64s]  which is actually not in the, there's
[1846.64s -> 1849.84s]  no process associated with this scheduler thread.
[1849.84s -> 1852.32s]  So this is actually, scheduler's context
[1852.32s -> 1857.56s]  is stored in the struct CPU for that core.
[1857.56s -> 1860.20s]  There's an array of these CPU structs, one per core,
[1860.20s -> 1861.60s]  each one has a context.
[1864.32s -> 1866.72s]  And a question, why can't we include the registers
[1866.72s -> 1868.84s]  in the trap frame for the process?
[1868.84s -> 1873.76s]  That is, actually, those registers
[1873.76s -> 1875.92s]  could be stored in the trap frame,
[1875.92s -> 1879.16s]  because there's only one saved set of kernel thread
[1879.16s -> 1881.20s]  registers per process.
[1881.20s -> 1884.08s]  We could save them in any data structure for which there's
[1884.08s -> 1888.80s]  one element of instance of that data structure per process.
[1888.80s -> 1890.56s]  There's one struct proc per process,
[1890.56s -> 1892.92s]  there's one struct trap frame per process.
[1892.92s -> 1894.96s]  We could store the registers in the trap frame.
[1897.40s -> 1901.40s]  But I mean, just sort of for maybe simplicity or clarity
[1901.40s -> 1903.68s]  of code, the trap frame, I think,
[1903.68s -> 1907.08s]  entirely consists of data that's needed when
[1907.08s -> 1909.16s]  entering and leaving the kernel.
[1909.16s -> 1912.60s]  And the struct context consists of the stuff
[1912.60s -> 1914.10s]  that needs to be saved and restored
[1914.10s -> 1917.32s]  when switching to and from between the kernel thread
[1917.32s -> 1920.56s]  and the scheduler thread.
[1920.56s -> 1922.04s]  OK, question, does yield something
[1922.04s -> 1923.80s]  that's called by the user or the kernel?
[1923.80s -> 1925.28s]  It's called by the kernel.
[1925.28s -> 1928.04s]  So the user threads, there's not really a direct way
[1928.08s -> 1933.48s]  in xv6 for user threads to talk about yielding the CPU
[1933.48s -> 1935.40s]  or switching.
[1935.40s -> 1940.20s]  It's done by the kernel kind of transparently at points
[1940.20s -> 1942.96s]  in time when the kernel feels that it needs to happen.
[1942.96s -> 1945.48s]  If there are threads, there's some times
[1945.48s -> 1949.88s]  when you can sort of guess that probably
[1949.88s -> 1952.60s]  a certain system call will result in a yield,
[1952.60s -> 1956.28s]  like if a process does a read on a pipe
[1956.28s -> 1958.24s]  where it knows that really nothing is waiting
[1958.24s -> 1961.96s]  to be read in the pipe, then the read will block.
[1961.96s -> 1963.52s]  You can predict the read will block
[1963.52s -> 1966.80s]  and that the kernel will run some other process
[1966.80s -> 1969.04s]  while we're waiting for data to appear in the pipe.
[1971.60s -> 1973.96s]  And so the times when yield is called in the kernel,
[1973.96s -> 1975.24s]  there's really two main times.
[1975.24s -> 1978.36s]  One is if a timer interrupt goes off,
[1978.36s -> 1980.88s]  the kernel always yields.
[1980.88s -> 1984.52s]  It's just on the theory that we
[1984.56s -> 1988.68s]  should interleave the execution of all the processes
[1988.68s -> 1992.96s]  that want to run on timer interrupt periods.
[1992.96s -> 1995.56s]  So timer interrupt also always calls yield.
[1995.56s -> 1997.36s]  And whenever a process, a system
[1997.36s -> 2000.16s]  calls waiting for IO, like waiting for you
[2000.16s -> 2003.04s]  to type the next keystroke, does a read of the console
[2003.04s -> 2004.72s]  and you haven't typed a key yet,
[2004.72s -> 2010.12s]  then the machinery to wait for IO calls yield.
[2010.12s -> 2011.88s]  It's called from sleep, something
[2011.88s -> 2013.08s]  we'll talk about next week.
[2015.44s -> 2016.28s]  All right.
[2018.68s -> 2019.52s]  Okay.
[2019.52s -> 2020.36s]  So-
[2020.36s -> 2021.20s]  Can I ask another question?
[2021.20s -> 2022.32s]  Yes.
[2022.32s -> 2026.16s]  If it is asleep, is it gonna do the same thing roughly?
[2026.16s -> 2028.96s]  So it's gonna be some system call
[2028.96s -> 2030.96s]  and then it's gonna save the trap frame
[2030.96s -> 2034.00s]  and then basically the same picture,
[2034.00s -> 2039.00s]  but it's just that the thing that made the process
[2039.96s -> 2041.92s]  go into the kernel was not a timer interrupt,
[2041.92s -> 2046.68s]  but the processes own decision.
[2046.68s -> 2047.52s]  Yeah.
[2047.52s -> 2049.64s]  So if the process does a read system call
[2049.64s -> 2051.48s]  and that's why it's in the kernel
[2051.48s -> 2056.16s]  and the read requires the process to wait for the disk
[2056.16s -> 2058.88s]  to finish reading or to wait for data to appear
[2058.88s -> 2060.80s]  on a pipe, then actually the diagram's
[2060.80s -> 2062.64s]  exactly the same as this.
[2064.64s -> 2066.20s]  Enter the kernel with a system call,
[2066.20s -> 2068.24s]  trap frame, all the saved user registers
[2068.24s -> 2069.12s]  will execute the system call,
[2069.12s -> 2070.12s]  the system call will be like,
[2070.16s -> 2073.72s]  ah, I need to wait for the disk to finish reading something.
[2073.72s -> 2075.68s]  The system call code will call sleep,
[2075.68s -> 2078.36s]  which ends up calling switch,
[2078.36s -> 2083.00s]  which saves away the kernel thread registers
[2083.00s -> 2084.76s]  and the process context,
[2084.76s -> 2087.28s]  switches to this current CPU schedule
[2087.28s -> 2089.24s]  to let some other thread run
[2089.24s -> 2092.84s]  while this thread is waiting for the disk read to finish.
[2092.84s -> 2095.00s]  So everything we're gonna talk about now,
[2095.00s -> 2099.32s]  except for the timer interrupt is pretty much the same
[2099.36s -> 2101.84s]  if what's going on is we're in a system call
[2101.84s -> 2104.92s]  and the system call needs to wait for IO
[2104.92s -> 2106.04s]  and give up the CPU.
[2108.68s -> 2110.44s]  For the purposes of today's discussion,
[2110.44s -> 2112.44s]  the two situations are almost identical.
[2115.32s -> 2116.20s]  Okay, so the question,
[2116.20s -> 2118.28s]  does each per CPU scheduler have its own stack?
[2118.28s -> 2123.28s]  Yes, there's a stack for this scheduler
[2125.84s -> 2128.64s]  and a stack for this, separate stack.
[2130.32s -> 2132.60s]  For the scheduler for CPU one.
[2137.96s -> 2140.12s]  Yeah, and indeed the stacks for the scheduler
[2140.12s -> 2141.12s]  are just set up.
[2142.36s -> 2144.28s]  In fact, all this stuff, you know,
[2144.28s -> 2147.92s]  the contacts and the stacks for the scheduler threads
[2147.92s -> 2151.12s]  are set up in a different way than for user processes.
[2152.56s -> 2154.24s]  They're set up at boot time.
[2154.24s -> 2159.24s]  So if you poke around in start.s or start.c,
[2160.16s -> 2162.20s]  start.s probably, you'll see some of the setup
[2162.20s -> 2165.24s]  for each core's scheduler thread.
[2165.24s -> 2167.20s]  There's a place where the stack,
[2167.20s -> 2169.60s]  very early in the assembly code during boot,
[2169.60s -> 2172.44s]  where the stack is set up for each CPU
[2172.44s -> 2176.00s]  and it's on that stack that the CPU boots on
[2176.00s -> 2177.80s]  and then runs its scheduler thread.
[2181.92s -> 2186.92s]  Okay, one piece of jargon,
[2187.44s -> 2189.92s]  when people talk about context switch,
[2191.40s -> 2193.64s]  they're talking about usually,
[2196.92s -> 2200.24s]  this act of switching from one thread to another
[2200.24s -> 2203.48s]  by saving one set of register sets for the old thread
[2203.48s -> 2205.68s]  and restoring previously saved registers
[2205.68s -> 2207.72s]  for the thread we're switching to.
[2207.72s -> 2209.40s]  So that's what's usually meant by context switch.
[2209.40s -> 2213.88s]  Also sometimes it's applied to the complete dance
[2213.88s -> 2216.04s]  that goes on when switching from one user process
[2216.04s -> 2218.08s]  to another and occasionally you'll see context switch
[2218.08s -> 2221.12s]  apply to switching between user and kernel.
[2221.12s -> 2225.24s]  But for us, we mostly mean switching
[2225.24s -> 2229.92s]  from one kernel thread typically to a scheduler thread.
[2232.28s -> 2234.32s]  Just some pieces of information
[2237.12s -> 2239.72s]  that are handy to keep in mind.
[2240.64s -> 2242.80s]  Every core just does one thing at a time.
[2242.80s -> 2246.76s]  Each core is either is just running one thread
[2246.76s -> 2247.60s]  at any given time.
[2247.60s -> 2250.60s]  It's either running some processes, user thread,
[2250.60s -> 2254.40s]  some process kernel thread or that core scheduler thread.
[2254.40s -> 2255.28s]  So at any given time,
[2255.28s -> 2257.00s]  the core is not doing multiple things,
[2257.00s -> 2258.36s]  it's just doing one thing.
[2258.36s -> 2260.88s]  And it's this switching that sort of creates the illusion
[2260.88s -> 2265.88s]  of multiple threads running at different times on that core.
[2266.12s -> 2271.12s]  Similarly, each thread is running
[2273.08s -> 2276.96s]  is either running on exactly one core
[2276.96s -> 2280.28s]  or its state has been saved
[2280.28s -> 2282.52s]  and we've switched away from it.
[2282.52s -> 2284.64s]  So a thread, just to be clear,
[2284.64s -> 2286.44s]  a thread never runs on more than one core.
[2286.44s -> 2288.44s]  A thread is either running on just one core
[2288.44s -> 2289.60s]  or it's not running at all.
[2289.60s -> 2291.56s]  It has saved state somewhere.
[2293.92s -> 2296.36s]  Another interesting thing about the XV6 setup
[2296.36s -> 2301.36s]  is that these contexts that hold saved kernel thread
[2302.28s -> 2306.48s]  registers, they're always produced by a call to switch.
[2307.64s -> 2310.72s]  And so these contexts basically always refer
[2310.72s -> 2314.80s]  to the state of the thread as it was executing
[2314.80s -> 2316.64s]  inside a call to switch.
[2319.40s -> 2321.52s]  And the way we'll see that come up
[2321.52s -> 2324.76s]  is that when we switch from one thread to another
[2324.76s -> 2327.32s]  and restore the target threads context,
[2327.32s -> 2329.04s]  the first thing that we'll do is return
[2329.08s -> 2331.56s]  from a previous call to switch.
[2331.56s -> 2334.44s]  So these contexts are always saved state
[2334.44s -> 2336.44s]  in as it is in switch.
[2338.76s -> 2343.04s]  Okay, any more questions
[2343.04s -> 2346.64s]  about the sort of diagram level situation?
[2351.72s -> 2352.92s]  I have a question.
[2352.92s -> 2355.04s]  So we're using the term thread all the time,
[2355.04s -> 2357.36s]  but it seems to me like our implementation
[2357.36s -> 2362.08s]  for XV6 process is only one thread.
[2362.08s -> 2364.40s]  So like, could it be possible that one process
[2364.40s -> 2367.96s]  could have multiple threads or am I wrong here?
[2367.96s -> 2372.96s]  In XV6, there's definitely some confusing things
[2374.60s -> 2376.08s]  about the way we use the words here.
[2376.08s -> 2381.08s]  In XV6, a process is either executing
[2387.80s -> 2390.28s]  instructions, the user level,
[2390.28s -> 2395.28s]  or it's executing instructions in the kernel,
[2395.96s -> 2399.08s]  or it's not executing at all.
[2399.08s -> 2400.84s]  And its state has been saved away
[2400.84s -> 2405.56s]  into this combination of a context and a trap frame.
[2408.68s -> 2410.28s]  So that's the actual situation.
[2410.28s -> 2411.84s]  Now what you wanna call that,
[2415.28s -> 2417.00s]  you can call it what you like.
[2417.60s -> 2420.52s]  I don't know of a simple explanation for this structure.
[2420.52s -> 2422.96s]  We've been calling it, I've been calling it,
[2422.96s -> 2427.28s]  I've been saying that each process has two threads,
[2427.28s -> 2431.24s]  a user level thread and a kernel level thread
[2431.24s -> 2433.24s]  and that's a process, there's this restriction
[2433.24s -> 2436.60s]  that a process is either executing in the kernel
[2436.60s -> 2439.68s]  in the user space or executing in the kernel
[2439.68s -> 2442.48s]  and interrupt your system call, but never both.
[2444.84s -> 2446.68s]  Okay, that makes sense, thank you.
[2447.36s -> 2450.00s]  I apologize for the kind of complexity of this.
[2453.72s -> 2458.40s]  Okay, so let me switch to code, looking at the XV6 code.
[2468.84s -> 2470.48s]  So first of all,
[2477.16s -> 2480.72s]  I just wanna just to show some of the stuff
[2480.72s -> 2481.68s]  we've been talking about.
[2481.68s -> 2486.68s]  I'm gonna look at the process structure
[2487.24s -> 2488.60s]  and we can see in the process structure
[2488.60s -> 2491.40s]  a lot of the things we've been talking about
[2491.40s -> 2492.40s]  just for review.
[2492.40s -> 2497.40s]  There's the trap frame that saves
[2498.72s -> 2503.72s]  the user level registers, there's a context here,
[2506.72s -> 2509.96s]  that saves the kernel thread registers
[2509.96s -> 2512.76s]  when we switch to the scheduler thread,
[2512.76s -> 2517.48s]  there's a pointer to this processes kernel stack,
[2517.48s -> 2520.16s]  which is where function calls are saved
[2520.16s -> 2521.96s]  while we're executing in the kernel.
[2523.44s -> 2525.00s]  There's this state variable,
[2525.00s -> 2529.04s]  which records whether this process is running
[2529.04s -> 2532.08s]  or runnable or sleeping or not allocated at all.
[2533.08s -> 2536.32s]  And then finally, there's a lock
[2536.32s -> 2538.88s]  that protects various things as we'll see.
[2541.84s -> 2545.96s]  For now, we can observe that it at least protects
[2546.80s -> 2549.64s]  changes to the state variable.
[2549.64s -> 2552.52s]  So that for example, two scheduler threads
[2552.52s -> 2554.60s]  don't try to grab a runnable process
[2554.60s -> 2556.48s]  and run it at the same time.
[2556.48s -> 2558.00s]  One of the many things this lock does
[2558.00s -> 2559.60s]  is prevent that from happening.
[2560.60s -> 2565.52s]  I'm gonna run a simple demo program for you,
[2565.52s -> 2566.92s]  the Spin program.
[2568.44s -> 2571.20s]  I'm using it mostly just to drive,
[2571.20s -> 2573.40s]  to sort of create the predictable situation
[2573.40s -> 2576.32s]  in which we switch from one thread to another.
[2576.32s -> 2579.80s]  But this is, this program, Spin program,
[2579.80s -> 2581.76s]  creates two processes and the processes
[2581.76s -> 2583.52s]  both compute forever.
[2583.52s -> 2587.56s]  You know, call fork here and make a child
[2588.56s -> 2591.40s]  and then forever, both children,
[2591.40s -> 2593.32s]  both children just sit in this loop
[2593.32s -> 2595.00s]  and everyone wants to know they'll print a character
[2595.00s -> 2597.80s]  just so we can see they're making progress.
[2597.80s -> 2599.36s]  But they don't print characters very often
[2599.36s -> 2603.60s]  and they never sort of intentionally give up the CPU.
[2603.60s -> 2605.68s]  So what we have here is two,
[2605.68s -> 2607.96s]  essentially two compute bound processes.
[2607.96s -> 2609.40s]  And in order for both of them to run,
[2609.40s -> 2612.04s]  I'm gonna run them on a single CPU,
[2613.60s -> 2615.84s]  XV6 that has only one core.
[2616.00s -> 2617.92s]  And so in order for both of them to execute,
[2617.92s -> 2622.52s]  you know, it's gonna be necessary to do switching
[2623.64s -> 2625.04s]  between the two processes.
[2626.72s -> 2631.72s]  Let me fire up the Spin program under GDB.
[2637.28s -> 2638.12s]  Run the Spin program.
[2638.12s -> 2639.48s]  And you can see it's printing
[2639.48s -> 2642.88s]  one of the two processes prints forward slash
[2642.88s -> 2644.56s]  and the other prints backward slash.
[2644.60s -> 2647.60s]  And you can see that every once in a while,
[2647.60s -> 2649.36s]  XV6 is switching between them
[2649.36s -> 2651.80s]  and only has one core the way I've configured it.
[2651.80s -> 2655.12s]  So we see a bunch of forward slashes printing
[2655.12s -> 2659.04s]  and then apparently a timer interrupt most go off,
[2659.04s -> 2661.56s]  switch the one CPU to the other process
[2661.56s -> 2664.20s]  and then prints the other kind of slash for a while.
[2664.20s -> 2667.00s]  So what I wanna observe is the timer interrupt going off.
[2667.00s -> 2670.76s]  So I'm gonna put a break point and trap
[2670.76s -> 2675.76s]  and in particular at line 207 and trap,
[2681.96s -> 2686.96s]  which is the code in trap in dev enter
[2691.16s -> 2695.44s]  that recognizes that we are in an interrupt
[2695.44s -> 2698.36s]  and the interrupt was caused by a timer interrupt.
[2698.36s -> 2703.36s]  So I'm gonna put a break point here at trap.c, line 207
[2706.04s -> 2710.28s]  and continue, boom, the trap right triggers right away
[2710.28s -> 2712.44s]  cause timer interrupts are pretty frequent.
[2712.44s -> 2714.88s]  And we can tell from where that indeed we're in user trap
[2714.88s -> 2717.56s]  and user trap has called dev enter
[2717.56s -> 2719.72s]  to handle this interrupt.
[2719.72s -> 2723.24s]  I wanna type finish to get out of dev enter
[2723.24s -> 2724.64s]  and back into user trap.
[2725.64s -> 2727.52s]  Because in fact we don't,
[2728.88s -> 2730.16s]  the code in dev enter for timer
[2730.16s -> 2731.56s]  and it's almost nothing.
[2733.68s -> 2738.68s]  However, once we're back at in user trap,
[2742.64s -> 2745.60s]  we can see that from this line here
[2745.60s -> 2747.92s]  that we just returned from dev enter.
[2747.92s -> 2752.92s]  And the interesting thing about this is that
[2760.88s -> 2762.76s]  what we're about to do,
[2762.76s -> 2763.60s]  I mean, looking forward,
[2763.60s -> 2765.48s]  we're currently at this line here
[2765.48s -> 2769.88s]  and we're looking forward to this call to yield.
[2769.88s -> 2772.20s]  When dev enter return two,
[2772.20s -> 2775.44s]  you can see from this return is two,
[2775.44s -> 2777.56s]  that two is basically the device number
[2778.08s -> 2780.48s]  and we're gonna see that by and by,
[2780.48s -> 2782.12s]  because which device is two,
[2783.48s -> 2785.00s]  user traps gonna call yield,
[2785.00s -> 2786.84s]  which will give up the CPU
[2786.84s -> 2788.84s]  and allow us to switch to another process.
[2788.84s -> 2790.12s]  So we'll see that in a moment.
[2790.12s -> 2793.08s]  Meantime, let's look at what was currently executing
[2793.08s -> 2794.64s]  when the interrupt happened.
[2794.64s -> 2796.04s]  So I'm gonna print P,
[2797.48s -> 2799.76s]  the variable P holds a pointer
[2799.76s -> 2802.16s]  to the current processes struct proc.
[2805.12s -> 2805.96s]  Okay, we have a question,
[2805.96s -> 2809.56s]  what makes each processes kernel thread different?
[2809.56s -> 2813.40s]  Every process has a separate kernel thread.
[2814.32s -> 2815.92s]  So there's really two things
[2815.92s -> 2818.72s]  that differentiate different processes kernel thread,
[2818.72s -> 2820.08s]  because more than one could be executing
[2820.08s -> 2821.96s]  on different cores.
[2823.92s -> 2826.04s]  One is indeed that every process
[2826.04s -> 2828.12s]  has a separate kernel stack
[2828.12s -> 2831.24s]  and that's what's pointed to by that case stack element
[2831.24s -> 2832.72s]  of struct proc.
[2832.72s -> 2834.32s]  And the other is that
[2835.96s -> 2840.96s]  early in, we're in user trap,
[2843.68s -> 2847.52s]  which is the C code that's called by trampoline
[2847.52s -> 2849.00s]  when an interrupt occurs.
[2851.76s -> 2853.60s]  We can tell by this call,
[2853.60s -> 2858.04s]  any kernel code can tell by calling my proc,
[2858.04s -> 2862.80s]  what the process is that's running on the current CPU.
[2862.80s -> 2865.16s]  And that's another thing that differentiates
[2866.64s -> 2871.40s]  that allows kernel code to tell what process it's part of,
[2871.40s -> 2874.28s]  that is which processes kernel thread is executing.
[2874.28s -> 2876.80s]  And what my proc does is basically use the TP register,
[2876.80s -> 2879.76s]  which you may recall is set up
[2879.76s -> 2884.60s]  to contain the current cores heart ID or core number.
[2884.60s -> 2887.16s]  It uses that to index into an array of structures
[2887.16s -> 2890.68s]  that say for each core that the scheduler sets
[2890.68s -> 2893.64s]  whenever it switches processes to indicate for each core,
[2893.68s -> 2895.76s]  which process is running on that core.
[2897.64s -> 2900.44s]  So that's how different kernel threads are differentiated.
[2901.96s -> 2903.96s]  Okay, so I was gonna use that P value,
[2905.12s -> 2906.44s]  the name and that P value
[2906.44s -> 2908.08s]  to figure out what process is running.
[2908.08s -> 2910.84s]  So xv6 remembers the name, it's that spin process
[2910.84s -> 2912.96s]  just exactly as expected.
[2913.88s -> 2915.28s]  There were two of them,
[2915.28s -> 2919.04s]  I think with process IDs three and four, oops.
[2920.04s -> 2923.64s]  We're currently executing in process ID three.
[2923.64s -> 2927.60s]  So after the switch, we'd expect to be in process ID four,
[2927.60s -> 2929.20s]  the other spin process.
[2930.60s -> 2933.60s]  How can we can look at the saved user registers
[2933.60s -> 2934.64s]  in the trap frame.
[2940.88s -> 2943.12s]  And these are just the 32 registers
[2943.12s -> 2946.84s]  that the trampoline codes saves away
[2946.84s -> 2948.44s]  to save the user state.
[2948.76s -> 2952.12s]  There's the user RA, return address register,
[2952.12s -> 2957.12s]  user stack pointer, user program counter at x 62.
[2958.56s -> 2960.80s]  These are all familiar things from when we looked
[2960.80s -> 2963.52s]  at traps.
[2963.52s -> 2965.88s]  And maybe of the most interest is that
[2969.56s -> 2971.96s]  the trap frame saves the user program counter
[2971.96s -> 2973.36s]  and it's at value 62.
[2974.24s -> 2979.24s]  If we cared, we can look in the assembly code
[2980.48s -> 2985.48s]  for spin.c, spin.asm and look for 62.
[2987.40s -> 2991.24s]  Now we can see that interrupt timer interrupt occurred
[2991.24s -> 2996.24s]  during this add instruction in that infinite loop in spin.
[2996.88s -> 2998.28s]  So it's not too surprising.
[2998.28s -> 3003.28s]  Okay, so back to the trap code,
[3006.48s -> 3008.08s]  they have inter just returned.
[3008.08s -> 3012.96s]  I'm gonna type step a few times to get us to the,
[3015.52s -> 3018.40s]  just being about to execute this yield.
[3018.40s -> 3020.64s]  And yield is sort of the first step in the process
[3020.64s -> 3023.24s]  of giving up the CPU, switching to the scheduler,
[3023.24s -> 3025.88s]  letting the scheduler choose another kernel thread
[3025.88s -> 3027.08s]  and process to run.
[3028.84s -> 3033.68s]  All right, so let's actually step into yield.
[3033.68s -> 3035.08s]  Now we're in yield yields.
[3036.28s -> 3037.80s]  Have you have a question?
[3037.80s -> 3038.64s]  No.
[3049.80s -> 3050.92s]  Okay, we're in yield.
[3053.76s -> 3055.32s]  Yield does just a couple of things.
[3055.32s -> 3060.32s]  It acquires the lock for this process,
[3060.64s -> 3062.32s]  because it's about to make a bunch of changes
[3062.32s -> 3065.32s]  to this process and it doesn't want any other.
[3065.32s -> 3067.68s]  And in fact, until it gives up the lock,
[3067.68s -> 3071.12s]  the state of this process will be sort of inconsistent.
[3071.12s -> 3074.08s]  Like for example, yield is about to change
[3074.08s -> 3075.80s]  the state of the process to runnable,
[3075.80s -> 3079.84s]  which would indicates that the process is not running
[3079.84s -> 3081.32s]  but would like to.
[3081.32s -> 3083.88s]  But this process is running, right?
[3083.92s -> 3085.44s]  I mean, we're running the process right now.
[3085.44s -> 3087.08s]  That's what's executing is the kernel thread
[3087.08s -> 3088.28s]  for this process.
[3088.28s -> 3090.92s]  And so one of the many things that acquiring this lock
[3090.92s -> 3094.40s]  does is makes it so that even though we just changed
[3094.40s -> 3098.08s]  the state to runnable, no other core scheduling thread
[3098.08s -> 3101.96s]  will look at this process and because of the lock
[3101.96s -> 3104.48s]  and see that it's runnable and try to run it
[3104.48s -> 3106.20s]  while we're still running it on this core,
[3106.20s -> 3107.32s]  which would be a disaster, right?
[3107.32s -> 3111.12s]  Running the same process on two different cores
[3111.12s -> 3113.32s]  and that process has only one stack.
[3113.36s -> 3115.16s]  So that means like two different cores
[3115.16s -> 3117.28s]  are calling subroutines on the same stack,
[3117.28s -> 3120.28s]  which is just a recipe for disaster.
[3121.88s -> 3123.12s]  So we take the lock out.
[3125.64s -> 3128.64s]  We yield changes the state to runnable.
[3130.52s -> 3134.60s]  And what this means is that when we finally given up the,
[3138.20s -> 3140.28s]  when we finally yielded the CPU and given it up
[3140.28s -> 3141.72s]  and switched to the scheduler process,
[3141.72s -> 3143.64s]  this state will be left in this runnable state
[3143.64s -> 3145.28s]  so that it will run again.
[3145.28s -> 3146.96s]  Because after all, this was a timer interrupt
[3146.96s -> 3149.20s]  that interrupted a running user level process
[3149.20s -> 3150.96s]  that would like to continue computing.
[3150.96s -> 3153.36s]  So we're gonna leave it in state runnable
[3153.36s -> 3154.32s]  so that it will run again
[3154.32s -> 3156.80s]  as soon as the scheduler decides to.
[3160.84s -> 3163.64s]  And then the only other thing,
[3163.64s -> 3168.64s]  that yield does is called a scheduler function.
[3174.84s -> 3177.56s]  So I'm gonna step into the scheduler function.
[3177.56s -> 3179.48s]  I will show the whole thing here.
[3189.24s -> 3190.80s]  This scheduler function does almost nothing.
[3190.80s -> 3192.44s]  It does a bunch of checks.
[3192.48s -> 3195.52s]  It does a whole bunch of sanity checks and panics.
[3195.52s -> 3197.52s]  And the reason for that is actually that
[3199.76s -> 3203.60s]  this code in XV6 over its many year lifetime
[3203.60s -> 3207.16s]  has been among the most bug prone
[3207.16s -> 3211.04s]  and had the most surprises, unhappy surprises.
[3211.04s -> 3214.84s]  So there's a lot of sanity checks and panics here
[3214.84s -> 3219.52s]  because there's often been bugs associated with this code.
[3219.52s -> 3224.52s]  All right, I'm gonna skip over these sanity checks
[3226.72s -> 3230.72s]  and proceed to the call to switch.
[3233.88s -> 3235.84s]  This call to switch is where the real action happened.
[3235.84s -> 3236.68s]  This is called the switch.
[3236.68s -> 3241.68s]  It's gonna save away the current kernel threads registers
[3241.88s -> 3245.36s]  in PR context, which is the current processes
[3245.36s -> 3248.24s]  saved kernel thread context, save set of registers.
[3249.08s -> 3252.04s]  C arrow context, C is the pointer
[3252.04s -> 3255.04s]  to this core's struct CPU.
[3255.92s -> 3259.80s]  And struct CPU has the context to save registers
[3259.80s -> 3262.56s]  of this core's scheduler thread.
[3262.56s -> 3264.32s]  So we're gonna be switching from this thread
[3264.32s -> 3266.00s]  and saving this thread state,
[3266.00s -> 3270.32s]  restoring the threat state of this core's scheduler
[3270.32s -> 3272.28s]  and sort of continuing the execution
[3272.28s -> 3276.12s]  of this core's scheduler thread.
[3279.04s -> 3284.04s]  Okay, so let's see what, let's do a quick preview
[3285.20s -> 3289.24s]  at the context that we're gonna be switching to.
[3289.24s -> 3291.48s]  And I can get that, turns out that
[3291.48s -> 3294.00s]  I can't actually print C arrow context,
[3294.00s -> 3298.64s]  but I happen to know that C prints to CPU's zero,
[3298.64s -> 3300.72s]  just because we're on the zero with core,
[3300.72s -> 3304.24s]  there's only one core and I can print its context.
[3304.56s -> 3309.56s]  And so this is the saved registers
[3311.44s -> 3314.72s]  from this core's scheduler thread.
[3316.72s -> 3318.64s]  And of particular interest is the RA
[3318.64s -> 3322.68s]  because the RA register is where
[3322.68s -> 3325.28s]  the current function call is gonna return to.
[3325.28s -> 3326.80s]  So we're gonna switch to the scheduler thread
[3326.80s -> 3331.12s]  and it's gonna do a return and return to that RA.
[3331.12s -> 3336.12s]  And we can find out where that return address is
[3336.28s -> 3338.72s]  by looking in Chrome.asm.
[3341.32s -> 3342.16s]  Actually that's,
[3350.24s -> 3353.92s]  and as you can see this X slash I prints the instructions
[3353.92s -> 3354.88s]  that are at a certain address,
[3354.88s -> 3356.36s]  but it also prints the label
[3358.32s -> 3359.60s]  of the name of the function
[3359.60s -> 3360.76s]  that those instructions are in it.
[3361.36s -> 3365.04s]  So we're gonna be returning to scheduler by and by.
[3365.04s -> 3367.20s]  That's just, as you might expect.
[3371.20s -> 3372.04s]  Okay.
[3377.28s -> 3379.68s]  I wanna look at what switch actually does,
[3379.68s -> 3381.08s]  we're about to call switch.
[3385.00s -> 3386.28s]  So I'm gonna put a break point on switch,
[3386.28s -> 3387.12s]  I'm putting a break point
[3387.12s -> 3388.76s]  because there's a bunch of setup code
[3388.84s -> 3391.80s]  like that pulls the values of context
[3391.80s -> 3392.92s]  out of those structures.
[3392.92s -> 3394.12s]  I'm gonna skip over it.
[3395.16s -> 3399.60s]  Okay, so now we're at a break point in switch.
[3399.60s -> 3402.48s]  The GDB won't show us the instructions,
[3402.48s -> 3405.08s]  but we can look in switch.s
[3405.08s -> 3407.68s]  to look at the instructions we're about to execute.
[3407.68s -> 3408.52s]  So as you can see,
[3408.52s -> 3409.44s]  we're on the very first instruction,
[3409.44s -> 3414.44s]  the store of RA to the address pointed to by A zero.
[3415.04s -> 3417.08s]  You may remember in the call to switch
[3417.08s -> 3420.60s]  that the first argument was the current threads context.
[3420.60s -> 3423.20s]  And the second argument was the context
[3423.20s -> 3424.60s]  of the thread we're switching to.
[3424.60s -> 3427.32s]  The two arguments go in A zero and A one.
[3427.32s -> 3429.56s]  And so the reason why we see all these stores
[3429.56s -> 3432.68s]  through register A zero is because we're storing away
[3432.68s -> 3435.84s]  a bunch of registers in the memory that A zero points to
[3435.84s -> 3439.48s]  that is in the context of the thread we're switching from
[3439.48s -> 3442.04s]  and the loads load from address A one
[3442.04s -> 3444.60s]  because that's a pointer to the context
[3444.60s -> 3446.24s]  of the thread we're switching to.
[3447.08s -> 3452.08s]  Okay, and so thread, you know, switch saves registers,
[3459.92s -> 3462.28s]  loads registers from the target threads context
[3462.28s -> 3463.24s]  and then return.
[3464.44s -> 3465.76s]  That's why the RA was interesting
[3465.76s -> 3467.16s]  because it's going to return to the place
[3467.16s -> 3469.40s]  that RA pointed to, namely into scheduler.
[3470.48s -> 3472.52s]  All right, so one question is,
[3472.52s -> 3475.88s]  you may notice here that while switch saves RA SP
[3475.88s -> 3477.48s]  and a bunch of S registers,
[3477.48s -> 3480.72s]  one thing it does not save is the program counter.
[3480.72s -> 3483.00s]  There's no mention of the program counter here.
[3483.92s -> 3484.92s]  So why is that?
[3491.12s -> 3494.64s]  Is it because the program counter is updated
[3494.64s -> 3497.36s]  with like the function calls anyway?
[3497.36s -> 3499.44s]  Yeah, it's the program counter.
[3499.44s -> 3501.96s]  There's no actual information value in the program counter.
[3502.00s -> 3504.40s]  We know that where we're executing right now
[3504.40s -> 3506.56s]  is in switch, right?
[3506.56s -> 3508.96s]  So there'd be no point in saving the program counter
[3508.96s -> 3511.16s]  because it has an extremely predictable value,
[3511.16s -> 3513.24s]  namely this instruction,
[3513.24s -> 3515.48s]  the address of this instruction and switch.
[3516.36s -> 3519.60s]  What we really care about is where we were called from
[3519.60s -> 3522.20s]  because when we switch back to this thread,
[3522.20s -> 3524.68s]  we wanna continue executing at whatever point
[3524.68s -> 3525.96s]  switch was called from.
[3525.96s -> 3529.68s]  And it's RA that holds the address of the instruction
[3529.68s -> 3531.80s]  that switch was called from.
[3532.48s -> 3535.64s]  So it's RA that's being saved away here.
[3535.64s -> 3540.64s]  And RA is the point at which we'll be executing at again,
[3541.96s -> 3542.80s]  when switch returns.
[3542.80s -> 3545.96s]  So we can even print that, we can print RA.
[3549.96s -> 3551.84s]  We can print RA and we haven't actually
[3551.84s -> 3553.00s]  switched threads yet.
[3553.00s -> 3556.16s]  You remember, we came here from the sked function.
[3556.16s -> 3558.32s]  So RA is, as you might expect,
[3558.32s -> 3560.24s]  a pointer back into the sked function.
[3561.16s -> 3562.00s]  Another question is,
[3562.00s -> 3564.96s]  how come switch only saves 14 registers?
[3564.96s -> 3565.80s]  I counted them.
[3565.80s -> 3568.88s]  It only saves and restores 14 registers.
[3568.88s -> 3571.68s]  Even though the RISC-V has 32 registers
[3571.68s -> 3576.04s]  available for code to use,
[3576.04s -> 3580.00s]  why only half the registers are saved?
[3580.00s -> 3581.44s]  Well, when switch was called,
[3581.44s -> 3583.12s]  it was called as a normal function.
[3583.12s -> 3585.20s]  So whoever called switch already assumed,
[3585.20s -> 3586.68s]  well, switch might modify those
[3586.68s -> 3589.60s]  so that that function already saved that
[3589.60s -> 3592.92s]  on its stack, meaning that like when we jump
[3592.92s -> 3595.24s]  from one to the other,
[3595.24s -> 3598.96s]  that one's gonna self restore its call
[3598.96s -> 3600.52s]  or save registers.
[3600.52s -> 3601.52s]  That's exactly right.
[3601.52s -> 3604.44s]  The switch is called from C code.
[3604.44s -> 3609.44s]  We know that the C compiler saves on the current stack,
[3610.72s -> 3614.40s]  any caller saved registers that have values
[3614.40s -> 3617.44s]  in them that the compiler is gonna need later.
[3617.44s -> 3619.32s]  And those caller saved registers
[3620.16s -> 3625.16s]  actually include, I think there's 18,
[3625.16s -> 3626.40s]  or depending on how you count them,
[3626.40s -> 3628.24s]  there's somewhere between 15 and 18
[3628.24s -> 3630.28s]  caller saved registers.
[3631.16s -> 3633.32s]  And so the registers we see here
[3633.32s -> 3635.88s]  are all the registers that aren't caller saved
[3635.88s -> 3638.32s]  and that the compiler doesn't promise to save,
[3638.32s -> 3642.04s]  but nevertheless may hold values
[3642.04s -> 3643.84s]  that are needed by the calling function.
[3643.84s -> 3647.40s]  So we only have to save the callee saved registers
[3647.40s -> 3648.88s]  when we're switching threads.
[3649.32s -> 3654.32s]  Okay, final thing I wanna print is the,
[3657.08s -> 3659.36s]  we do save and restore the stack pointer.
[3659.36s -> 3660.28s]  The current stack pointer,
[3660.28s -> 3662.40s]  it's like hard to tell from this value what that means,
[3662.40s -> 3665.52s]  but it's the kernel stack of the current process,
[3665.52s -> 3667.80s]  which I don't know if you recall,
[3667.80s -> 3670.80s]  but is allocated, is mapped by the virtual memory system
[3670.80s -> 3671.68s]  at high memory.
[3673.52s -> 3674.36s]  Okay, so,
[3674.36s -> 3679.36s]  okay, so we're gonna save away the current registers
[3681.40s -> 3686.40s]  and restore registers from the scheduler threads context.
[3686.40s -> 3688.52s]  I don't wanna like execute every single one
[3688.52s -> 3689.84s]  of these loads in store,
[3689.84s -> 3694.00s]  so I'm gonna step over all the 14 loads,
[3694.00s -> 3697.16s]  the 14 stores and the 14 loads gonna proceed directly
[3697.16s -> 3699.08s]  to the return instructions.
[3699.08s -> 3702.04s]  Okay, so we executed everything in switch except the return
[3703.04s -> 3704.84s]  before we do the return,
[3704.84s -> 3707.16s]  we'll just print the interesting registers again
[3707.16s -> 3708.00s]  to see where we are.
[3708.00s -> 3713.00s]  So stack pointer now has a different value.
[3713.08s -> 3714.12s]  The stack pointer now points
[3714.12s -> 3717.32s]  into this stack zero area in memory.
[3717.32s -> 3720.52s]  And this is actually the place very, very early
[3720.52s -> 3724.76s]  in the boot sequence where start.s puts the stack
[3724.76s -> 3727.84s]  so it can make call the very first C function.
[3727.84s -> 3730.68s]  So actually back on the original boot stack for this CPU,
[3730.76s -> 3734.12s]  which just happens to be where the scheduler runs.
[3736.76s -> 3740.68s]  Okay, the program counter, not very interesting,
[3740.68s -> 3742.76s]  we're in switch because we haven't returned yet.
[3742.76s -> 3747.48s]  And the RA register now points to scheduler
[3747.48s -> 3751.84s]  because we've loaded, we've restored the register set
[3751.84s -> 3753.88s]  previously saved by the scheduler thread.
[3755.28s -> 3757.76s]  And indeed, we're really now in the scheduler thread.
[3757.76s -> 3760.96s]  If I run where, the where now looks totally different
[3760.96s -> 3762.04s]  from the last time we ran it.
[3762.04s -> 3764.48s]  We're now indeed in a call to switch,
[3764.48s -> 3767.08s]  but now we're in a call from switch to switch
[3767.08s -> 3770.56s]  that the scheduler made at some point in the past.
[3770.56s -> 3773.56s]  And the schedule was run long ago during boot
[3773.56s -> 3775.88s]  was called as the last thing that main did
[3775.88s -> 3777.20s]  during the boot process.
[3779.60s -> 3781.76s]  So I'm gonna execute one instruction to return
[3781.76s -> 3784.80s]  from switch now into scheduler.
[3785.80s -> 3788.24s]  So now we're in this course scheduler.
[3788.24s -> 3789.68s]  Let's look at the full code.
[3794.68s -> 3796.20s]  So this is the scheduler code,
[3797.72s -> 3798.88s]  this function called scheduler.
[3798.88s -> 3801.24s]  And now we're executing in the scheduler thread
[3801.24s -> 3802.68s]  for the CPU.
[3802.68s -> 3806.12s]  And we're just at the point we just returned
[3806.12s -> 3809.28s]  from a previous call to switch
[3809.28s -> 3811.48s]  that the scheduler made a while ago
[3811.48s -> 3814.76s]  when it decided it was gonna start running that process,
[3815.64s -> 3818.96s]  which was the spin process that was interrupted.
[3818.96s -> 3823.28s]  So it's this switch, process ID three
[3823.28s -> 3824.72s]  that spin called switch,
[3824.72s -> 3826.76s]  but it's not that switch that's returning,
[3826.76s -> 3828.40s]  that switch hasn't returned yet.
[3828.40s -> 3833.40s]  It's still saved away in process IDs three stack
[3833.60s -> 3836.28s]  and contexts just returned from this earlier call
[3836.28s -> 3837.12s]  to switch.
[3839.48s -> 3840.84s]  All right, so the stuff that happens here
[3840.84s -> 3845.84s]  in the scheduler, we've stopped running this process.
[3846.04s -> 3849.80s]  And so you wanna forget about the various things
[3849.80s -> 3852.84s]  we did in the process of running this process.
[3852.84s -> 3856.00s]  We wanna forget the CRProc equals zero,
[3856.00s -> 3857.28s]  basically means that we're forgetting
[3857.28s -> 3860.48s]  that we're no longer running this process in this core.
[3860.48s -> 3864.00s]  So we don't wanna have anybody be confused about that.
[3864.00s -> 3867.84s]  We set this per core proc pointer to zero
[3867.84s -> 3870.24s]  instead of this process.
[3870.28s -> 3874.28s]  The next thing that happens is that you remember yield
[3874.28s -> 3876.56s]  acquired the lock for this process
[3876.56s -> 3878.60s]  because it didn't want any other core scheduler
[3878.60s -> 3881.00s]  to look at this process and maybe run it
[3881.00s -> 3883.84s]  until the process was completely put to sleep.
[3885.16s -> 3888.36s]  We've now completed the switch away from this process
[3888.36s -> 3890.16s]  and so we can release the lock
[3890.16s -> 3891.96s]  on the process that just yielded.
[3893.20s -> 3894.60s]  That's the release.
[3894.60s -> 3899.12s]  Now at this point, we're still in the scheduler.
[3899.12s -> 3901.24s]  If there was another core, at this point,
[3901.24s -> 3904.92s]  some other core scheduler could find that process
[3904.92s -> 3906.88s]  because it's runnable and run it.
[3906.88s -> 3909.40s]  But that's okay because we've completely saved
[3909.40s -> 3911.72s]  its registers, we're no longer executing
[3911.72s -> 3914.88s]  on that process's stack because we're now executing
[3914.88s -> 3918.56s]  on this core scheduler stack.
[3918.56s -> 3920.56s]  So it's actually fine if some other core
[3920.56s -> 3922.12s]  decides to run that process.
[3923.12s -> 3924.56s]  Okay, but there is no other core
[3924.56s -> 3927.36s]  so that doesn't actually happen in this demonstration.
[3930.04s -> 3935.04s]  I actually wanna spend a moment talking
[3935.04s -> 3937.56s]  about the PR lock a little bit more.
[3939.36s -> 3942.52s]  PR lock actually does a couple of things.
[3945.36s -> 3947.44s]  It does really two things
[3947.44s -> 3949.00s]  from the point of view of scheduling.
[3949.00s -> 3953.64s]  One is that yielding the CPU involves multiple steps.
[3953.64s -> 3955.56s]  We have to set the state to run up,
[3955.56s -> 3957.48s]  change the state from running to runnable.
[3957.52s -> 3959.56s]  We need to save the registers
[3959.56s -> 3961.92s]  in the yielding processes context,
[3961.92s -> 3964.96s]  and we have to stop using the yielding processes stack.
[3964.96s -> 3968.04s]  There's at least three steps, which take time
[3969.64s -> 3974.16s]  in order to do all the steps required to yield the CPU.
[3974.16s -> 3976.92s]  And so one of the things that lock does, as I mentioned,
[3976.92s -> 3978.92s]  is prevent any other core scheduler
[3978.92s -> 3980.08s]  from looking at our process
[3980.08s -> 3982.20s]  until all three steps have completed
[3982.20s -> 3986.00s]  so the lock is basically making those steps atomic.
[3986.04s -> 3987.60s]  They either all happen from the point of view
[3987.60s -> 3991.08s]  of other cores or none of them have happened.
[3992.92s -> 3996.60s]  It's gonna turn out also when we start running a process
[3996.60s -> 3999.00s]  that the PR lock is gonna have
[3999.00s -> 4001.76s]  a similar protective function.
[4003.08s -> 4005.40s]  We're gonna set the state of a process to running
[4005.40s -> 4006.96s]  when we start executing a process
[4006.96s -> 4009.08s]  and we're gonna move its registers
[4009.08s -> 4012.56s]  from its process context into the RISC-V registers.
[4013.48s -> 4016.16s]  But if an interrupt should happen
[4016.16s -> 4017.72s]  in the middle of that process,
[4017.72s -> 4020.00s]  the interrupt's gonna see the process in a weird state,
[4020.00s -> 4022.36s]  like maybe in the state of marked running,
[4022.36s -> 4025.48s]  but hasn't yet finished moving its registers
[4025.48s -> 4028.08s]  from the context into the RISC-V registers.
[4028.08s -> 4029.12s]  And so that would be a disaster
[4029.12s -> 4030.84s]  if a timer interrupt happened then,
[4030.84s -> 4033.24s]  because we might switch away from that process
[4033.24s -> 4035.76s]  before it had restored its registers.
[4037.04s -> 4038.52s]  And switching away from that process
[4038.52s -> 4042.04s]  would save now uninitialized RISC-V registers
[4042.40s -> 4045.04s]  into the context, process's context,
[4045.04s -> 4047.24s]  overwriting its real registers.
[4047.24s -> 4049.40s]  So indeed, we want starting a process
[4049.40s -> 4051.88s]  to also be effectively atomic.
[4053.20s -> 4056.60s]  And in this case, holding the PR lock across,
[4056.60s -> 4058.96s]  switching to a process,
[4058.96s -> 4060.40s]  as well as preventing other cores
[4060.40s -> 4061.36s]  from looking at that process,
[4061.36s -> 4065.92s]  also turns off interrupts for the duration of firing up,
[4065.92s -> 4067.56s]  of switching to that thread,
[4067.56s -> 4070.80s]  which prevents a timer interrupt from ever seeing a process
[4070.84s -> 4074.32s]  that's only midway through being switched to.
[4077.52s -> 4078.36s]  Okay.
[4080.56s -> 4082.16s]  So we're in the scheduler.
[4082.16s -> 4083.68s]  We're executing this loop in the scheduler.
[4083.68s -> 4084.80s]  There's this loop in the scheduler
[4084.80s -> 4086.56s]  that looks at all the processes in turn
[4086.56s -> 4087.76s]  to find one to run.
[4088.96s -> 4090.72s]  And in this case, we know there's another process
[4090.72s -> 4095.12s]  because there's that other spin process that we forked.
[4097.08s -> 4100.16s]  But there's a lot of process slots to examine.
[4100.20s -> 4102.04s]  So I wanna skip over the actual process,
[4102.04s -> 4104.64s]  the scanning of the process table
[4104.64s -> 4107.04s]  and go direct to the point at which
[4107.04s -> 4108.56s]  the scheduler finds the next process.
[4108.56s -> 4110.16s]  So I'm gonna put a break point
[4112.60s -> 4114.44s]  at line 474,
[4114.44s -> 4116.84s]  where it's actually found a new process to run.
[4117.96s -> 4119.36s]  But here we are,
[4119.36s -> 4122.32s]  the scheduler has scanned the process table
[4122.32s -> 4124.00s]  and found another process to run.
[4125.12s -> 4128.12s]  And it's gonna cause that process to run.
[4128.16s -> 4132.00s]  You can see at line 468, it acquired that process's lock.
[4132.00s -> 4134.68s]  So now it's entitled to do the various steps
[4134.68s -> 4136.80s]  that are required to switch that process.
[4137.92s -> 4141.52s]  And line 473, it set the process's state to running.
[4141.52s -> 4145.44s]  It's now at 474, gonna record in the CPU structure,
[4145.44s -> 4147.88s]  which process the CPU is executing
[4149.56s -> 4153.16s]  and then call switch to save the scheduler's registers
[4153.16s -> 4156.72s]  and restore the target processor's registers.
[4156.76s -> 4158.20s]  So we can see what process it found
[4158.20s -> 4161.32s]  by looking at the new process's name.
[4161.32s -> 4162.84s]  Surprisingly, it's spin.
[4164.00s -> 4165.72s]  It's process ID is now four,
[4165.72s -> 4167.92s]  used to be running three, now running four.
[4170.16s -> 4171.72s]  And we've already set the state to running.
[4171.72s -> 4176.72s]  So the state's running.
[4178.84s -> 4181.60s]  We can see where this thread is gonna switch to
[4181.60s -> 4183.96s]  in the call to switch at line 475.
[4184.96s -> 4189.16s]  Print this context, you saved registers.
[4189.16s -> 4192.44s]  So where is the RA, the person we're gonna call switch,
[4192.44s -> 4195.84s]  but switch, as we know, returns.
[4195.84s -> 4198.80s]  When it returns, it returns to the restored RA.
[4198.80s -> 4200.60s]  So what we really care about is
[4200.60s -> 4203.44s]  where is it that RA points to?
[4203.44s -> 4206.44s]  We can find that out by using X slash I.
[4207.16s -> 4210.48s]  Oops, using X slash I.
[4213.84s -> 4216.72s]  It's gonna return RA points to some point in sched.
[4216.72s -> 4217.80s]  That's not too surprising,
[4217.80s -> 4220.92s]  since presumably that other spin process
[4222.56s -> 4224.40s]  was suspended due to a timer interrupt,
[4224.40s -> 4228.24s]  which as we know, calls sched, which calls switch.
[4232.68s -> 4235.92s]  All right, so we're about to call switch.
[4236.44s -> 4238.48s]  Let me just bring up the switch code again.
[4244.92s -> 4247.88s]  Actually, enter switch, we're still,
[4247.88s -> 4250.76s]  where shows that we're still in the scheduler's context.
[4253.20s -> 4256.40s]  I wanna again execute all of the instructions to switch,
[4256.40s -> 4260.52s]  this time switching from the scheduler to the new process.
[4260.52s -> 4263.48s]  So we skip over the 28 stores and loads.
[4266.88s -> 4268.48s]  Just convince ourselves
[4268.48s -> 4270.44s]  that we are actually about to return to sched.
[4270.44s -> 4272.48s]  So now, since we're about to return to sched
[4272.48s -> 4273.68s]  and not scheduler,
[4273.68s -> 4276.36s]  we must now be in a processes kernel thread
[4276.36s -> 4280.00s]  and no longer in the scheduler thread.
[4280.00s -> 4282.48s]  And indeed, if we look at the backtrace,
[4282.48s -> 4284.32s]  we had a user trap call
[4284.32s -> 4286.48s]  that must have been a timer interrupt from long,
[4286.48s -> 4288.80s]  you know, sometime in the past
[4288.80s -> 4291.04s]  that as we've seen called yield and sched,
[4291.04s -> 4294.24s]  but it was a timer interrupt in the other process now,
[4294.24s -> 4296.64s]  not in the process that we originally looked at.
[4301.80s -> 4305.60s]  Okay, any questions about,
[4305.60s -> 4307.44s]  I think I'm gonna leave off
[4307.44s -> 4308.92s]  stepping through the code at this point
[4308.92s -> 4313.60s]  and any questions about any of the material we've seen?
[4316.04s -> 4320.24s]  Oh, sorry, if it was, for example, a disk IO,
[4320.28s -> 4324.96s]  then we would see that our array would point somewhere
[4324.96s -> 4329.04s]  to like sleep or something like that, right?
[4331.44s -> 4336.44s]  Yes, well, we see that the where at this point
[4336.64s -> 4340.12s]  would include some system call implementation functions
[4340.12s -> 4341.20s]  and a call to sleep.
[4341.20s -> 4344.52s]  As it happens, I think, I mean, this is,
[4344.52s -> 4347.52s]  you're basically, the answer to your question is yes.
[4347.52s -> 4351.04s]  If we had just left off executing this process
[4351.04s -> 4353.80s]  for some reason other than timer interrupt,
[4353.80s -> 4355.44s]  switch would be basically returning
[4355.44s -> 4358.84s]  to some system call code instead of to sched.
[4358.84s -> 4360.88s]  As it happens, I think sleep may call sched,
[4360.88s -> 4365.12s]  so the backtrace would look different,
[4365.12s -> 4367.44s]  it would just happen to include sched, but yes.
[4367.44s -> 4370.12s]  So I've chosen just one way of, you know,
[4370.12s -> 4372.88s]  just one way of switching between processes
[4372.88s -> 4374.28s]  due to timer interrupts.
[4375.04s -> 4378.48s]  But you also get switches to wait for user IO
[4378.48s -> 4380.20s]  or to wait for other processes to do things
[4380.20s -> 4381.28s]  like write to a pipe.
[4384.68s -> 4386.88s]  Okay, one thing you probably noticed
[4386.88s -> 4391.08s]  is that scheduler called switch
[4391.08s -> 4394.08s]  and we're about to return from switch here,
[4394.08s -> 4397.52s]  but we're returning really from a different call to switch
[4397.52s -> 4399.12s]  than the one the scheduler made.
[4399.12s -> 4400.56s]  We're returning from a call to switch
[4400.56s -> 4402.60s]  that this process made a long time ago.
[4404.32s -> 4407.48s]  So, you know, this is potentially a little bit confusing,
[4407.48s -> 4409.64s]  but you know, this is how the guts
[4409.64s -> 4410.96s]  of a thread switch work.
[4412.08s -> 4414.60s]  Another thing to notice is that the code
[4414.60s -> 4415.84s]  we're looking at, the switch code,
[4415.84s -> 4419.76s]  this is really the heart of thread switching.
[4419.76s -> 4423.00s]  And really all you have to do to switch threads
[4423.00s -> 4426.76s]  is save registers and restore registers.
[4426.76s -> 4428.28s]  Now, now threads have a lot more state
[4428.28s -> 4430.32s]  than just registers, they have variables
[4430.32s -> 4432.56s]  and stuff in the heap and who knows what,
[4432.56s -> 4437.04s]  but all that other state is in memory
[4437.04s -> 4438.56s]  and isn't gonna be disturbed.
[4438.56s -> 4440.80s]  We haven't done nothing to disturb
[4440.80s -> 4445.48s]  any of these threads stacks, for example, or heap values.
[4446.56s -> 4448.88s]  So the registers in the microprocessor
[4448.88s -> 4450.92s]  are really the only kind of volatile state
[4450.92s -> 4452.48s]  that actually needs to be saved
[4452.48s -> 4454.20s]  and restored to do a thread switch.
[4454.20s -> 4455.56s]  All this stuff's in memory.
[4455.56s -> 4456.52s]  The stack, for example,
[4456.52s -> 4459.12s]  is still be in memory undisturbed.
[4459.12s -> 4459.96s]  And so it doesn't have to be
[4459.96s -> 4461.80s]  explicitly saved and restored.
[4462.60s -> 4463.72s]  Now we're only saving and restoring
[4463.72s -> 4466.24s]  this microprocessor, the CPU registers,
[4466.24s -> 4468.88s]  because we wanna reuse those very registers
[4468.88s -> 4470.48s]  in the CPU for the new thread
[4470.48s -> 4473.52s]  and overwrite whatever values they have.
[4473.52s -> 4478.08s]  So that's why we have to save the old threads registers.
[4479.40s -> 4482.04s]  What about other processor state?
[4482.04s -> 4483.76s]  So I don't know if the RISC-V processor
[4483.76s -> 4486.04s]  that we're using has other flags,
[4486.04s -> 4488.32s]  but I know like some x86 Intel chips
[4488.32s -> 4491.12s]  have like the floating point unit state
[4491.16s -> 4492.72s]  and like things like that.
[4492.72s -> 4495.36s]  Do we just not have that in RISC-V?
[4495.36s -> 4498.12s]  Absolutely, your point's very well taken
[4498.12s -> 4500.80s]  on other microprocessors like x86.
[4500.80s -> 4505.20s]  The details of switching are a bit different
[4505.20s -> 4508.04s]  because they had different registers in different state.
[4509.24s -> 4510.08s]  And so the code,
[4510.08s -> 4512.56s]  this is very, very RISC-V dependent code
[4512.56s -> 4517.44s]  and the switch routine for some other processor
[4517.44s -> 4518.28s]  might look quite different.
[4518.32s -> 4521.76s]  RISC-V might have to save floating point registers.
[4521.76s -> 4524.96s]  RISC-V actually uses the general purpose register.
[4526.44s -> 4529.92s]  Actually, I'm not sure what it does for floating point,
[4529.92s -> 4531.20s]  but the kernel doesn't use floating point,
[4531.20s -> 4533.68s]  so it doesn't have to worry about it.
[4533.68s -> 4536.32s]  But yeah, this is totally microprocessor dependent.
[4538.00s -> 4540.88s]  A question about the timer interrupts.
[4540.88s -> 4542.80s]  So it sounds like the core
[4542.80s -> 4545.80s]  of all of the scheduling working
[4545.80s -> 4548.24s]  is that there will be a timer interrupt.
[4549.16s -> 4551.88s]  So what happens in cases where that malfunctions?
[4551.88s -> 4553.68s]  There is gonna be a timer interrupt.
[4561.44s -> 4565.20s]  Okay, so the reasoning for how come preemptive scheduling
[4565.20s -> 4567.12s]  of user processes works
[4568.04s -> 4570.52s]  is that user processes execute
[4570.52s -> 4573.80s]  with interrupts turned on always.
[4573.80s -> 4577.68s]  x86 just ensures that interrupts are enabled
[4578.12s -> 4579.16s]  before returning to user space.
[4579.16s -> 4581.44s]  And that means that a timer interrupt can happen
[4581.44s -> 4583.44s]  if you're executing in user space.
[4583.44s -> 4586.32s]  So there's nothing a user process,
[4586.32s -> 4587.16s]  if we're in user space,
[4587.16s -> 4590.44s]  the timer interrupt just will happen when the time comes.
[4590.44s -> 4591.84s]  It's a little trickier in the kernel.
[4591.84s -> 4593.48s]  The kernel sometimes turns off interrupts
[4593.48s -> 4595.16s]  like when you acquire a lock,
[4595.16s -> 4596.32s]  the interrupts are gonna be turned off
[4596.32s -> 4597.60s]  until you release it.
[4597.60s -> 4602.60s]  So if there were some bug in the kernel,
[4605.20s -> 4606.72s]  if the kernel turned off interrupts
[4606.72s -> 4608.96s]  and never turned them back on
[4608.96s -> 4612.36s]  and the code in the kernel never gave up the CPU,
[4612.36s -> 4615.92s]  never called sleep or gave up the CPU for any other reason
[4615.92s -> 4619.20s]  then indeed, a timer interrupt wouldn't occur
[4619.20s -> 4622.40s]  and that would mean that this kernel code
[4622.40s -> 4626.44s]  may never give the CPU.
[4626.44s -> 4629.28s]  But in fact, as far as we know,
[4630.56s -> 4634.44s]  we wrote xv6 so that it always turns interrupts back on.
[4634.44s -> 4638.88s]  Or if there's code in xv6 that turns off interrupts,
[4638.88s -> 4640.44s]  it either turns them back on
[4640.44s -> 4644.24s]  and so a timer interrupt can then occur in the kernel
[4644.24s -> 4646.52s]  and we can switch away from this kernel thread
[4646.52s -> 4649.52s]  or the code returns back to user space,
[4649.52s -> 4651.40s]  kernel code returns back to user space.
[4651.40s -> 4653.40s]  We believe there's never a situation
[4653.40s -> 4656.40s]  in which kernel code will simply like loop
[4656.40s -> 4658.20s]  with interrupts turned off forever.
[4660.20s -> 4661.04s]  I got it.
[4661.04s -> 4662.28s]  My question was more about like,
[4662.28s -> 4664.04s]  so I assume the interrupts are actually coming
[4664.04s -> 4665.52s]  from some piece of hardware.
[4665.52s -> 4667.72s]  Like what if that piece of hardware malfunctions?
[4667.72s -> 4668.56s]  No.
[4671.04s -> 4672.36s]  No, it's fine.
[4672.36s -> 4674.68s]  Then your computer's broken and you should buy a new one.
[4676.76s -> 4677.60s]  Okay.
[4677.60s -> 4679.52s]  I mean, that's a valid question for,
[4679.52s -> 4682.92s]  I mean, there's 10 billion transistors in your computer
[4682.92s -> 4686.20s]  and indeed, sometimes the hardware
[4686.20s -> 4687.24s]  just like has bugs in it,
[4687.24s -> 4690.56s]  but that's beyond our reach for,
[4691.56s -> 4693.24s]  I mean, if you add one-on-one
[4693.48s -> 4694.56s]  and the computer says three,
[4694.56s -> 4698.36s]  then you just have deep problems
[4698.36s -> 4700.44s]  that xv6 can't help you with.
[4703.00s -> 4705.20s]  So we're assuming that the computer works.
[4706.80s -> 4710.72s]  The only time when software,
[4710.72s -> 4713.16s]  I mean, there are times when software tries to compensate
[4713.16s -> 4714.28s]  for hardware level errors.
[4714.28s -> 4717.36s]  Like if you're sending packets across a network,
[4717.36s -> 4719.48s]  you always send a checksum
[4719.48s -> 4723.12s]  so that if the network hardware flips a bit,
[4723.84s -> 4725.92s]  malfunctions, flips a bit, then you can correct that.
[4725.92s -> 4727.48s]  But for stuff inside the computer,
[4727.48s -> 4729.56s]  then people tend not to,
[4729.56s -> 4734.32s]  it's just people basically don't try
[4734.32s -> 4736.88s]  to make the software compensate for hardware errors.
[4740.40s -> 4741.56s]  Oh, I have a question.
[4741.56s -> 4746.12s]  Why, so like in trampling.s and in switch,
[4746.12s -> 4748.76s]  we write the code in assembly.
[4748.76s -> 4749.60s]  Is that why?
[4749.60s -> 4751.60s]  Is that because we want to make sure
[4751.60s -> 4754.80s]  that exactly this thing's happening?
[4754.80s -> 4757.28s]  So we cannot write it in C
[4757.28s -> 4761.60s]  because we just need like those exact things
[4761.60s -> 4762.72s]  to happen basically.
[4765.72s -> 4766.92s]  Yeah, yeah.
[4768.96s -> 4771.56s]  Yes, certainly we want this exact sequence to happen.
[4772.68s -> 4776.36s]  And in C, it's very hard to talk about things
[4776.36s -> 4779.52s]  like RA and C or SP.
[4780.52s -> 4782.88s]  Certainly there's no way within the C language
[4782.88s -> 4785.84s]  to talk about changing the stack pointer
[4787.04s -> 4788.84s]  with the RA register.
[4788.84s -> 4792.68s]  So these are things that just can't be,
[4792.68s -> 4795.00s]  you can't say it in ordinary C.
[4795.92s -> 4797.84s]  The only way you can say it in C is,
[4797.84s -> 4799.72s]  there is, it's possible in C
[4799.72s -> 4803.52s]  to sort of embed assembly instructions in C code.
[4803.52s -> 4804.48s]  And so we could have just embedded
[4804.48s -> 4806.96s]  these assembly instructions in the C function,
[4806.96s -> 4809.08s]  but it would amount to the same thing.
[4810.08s -> 4814.04s]  Basically we're operating at a level below C,
[4814.04s -> 4817.28s]  so we can't really use C here.
[4820.16s -> 4823.44s]  I have a question about when a thread finishes executing.
[4823.44s -> 4825.44s]  I'm assuming that happens in the user space
[4825.44s -> 4828.30s]  when we call the exact, I'm sorry,
[4828.30s -> 4830.40s]  exact system call.
[4830.40s -> 4834.12s]  And that also ends the process,
[4834.12s -> 4836.36s]  the thread I'm assuming in the kernel space.
[4836.40s -> 4841.40s]  But if the thread ends before a new time interrupt happens,
[4845.76s -> 4848.86s]  is this like the CPU still acquired by that thread
[4848.86s -> 4851.22s]  or do we end that thread and start a new one
[4851.22s -> 4852.80s]  before the new time interrupt?
[4852.80s -> 4857.80s]  Oh yeah, the thread yields the CPU, the exit,
[4862.12s -> 4863.36s]  exit yields the CPU.
[4863.36s -> 4865.76s]  So there's actually many points.
[4865.76s -> 4867.20s]  Even though I've been driving this discussion
[4867.20s -> 4869.08s]  with the timer interrupt, in fact,
[4870.48s -> 4874.36s]  in almost all cases where XV6 switches between threads,
[4874.36s -> 4876.12s]  it's not due to timer interrupts.
[4876.12s -> 4880.14s]  It's because some system call is waiting for something
[4880.14s -> 4883.96s]  or decides that it needs to give up the CPU.
[4883.96s -> 4887.10s]  And so for example, exit does various things
[4887.10s -> 4889.72s]  and then calls yield to give up the CPU.
[4889.72s -> 4891.68s]  And it does that.
[4891.68s -> 4893.68s]  There's really nothing, it does that independently
[4893.68s -> 4895.52s]  of whether there's a timer interrupt.
[4895.76s -> 4898.60s]  Yes.
[4904.08s -> 4907.08s]  All right, the time is up for this lecture.
[4907.08s -> 4910.40s]  I think I'll continue some of this discussion next week,
[4910.40s -> 4913.80s]  but I'm happy to take more questions right now
[4913.80s -> 4914.84s]  if people have them.
[4918.48s -> 4923.48s]  So let's say the operating system actually takes on
[4923.60s -> 4925.12s]  the thread implementation.
[4925.64s -> 4929.00s]  So for example, you wanna run multiple threads
[4929.00s -> 4931.72s]  of a process on multiple CPUs,
[4931.72s -> 4933.68s]  like that has to be handled by the OS
[4933.68s -> 4936.60s]  that cannot just be handled in user space, right?
[4936.60s -> 4938.14s]  How does that kind of switching work?
[4938.14s -> 4941.12s]  Is each thread now becomes the same as a process?
[4941.12s -> 4942.60s]  It's like, is it always gonna loop
[4942.60s -> 4944.88s]  through all existing threads?
[4944.88s -> 4948.44s]  Or, you know, cause like each CPU will still switch
[4948.44s -> 4950.76s]  between even if one process has give me eight cores,
[4950.76s -> 4953.28s]  like it's still gonna switch each of the CPUs
[4953.28s -> 4956.76s]  between those and then couple of other processes.
[4956.76s -> 4959.12s]  And then also we don't wanna really switch
[4959.12s -> 4960.72s]  like between the one and the other thread
[4960.72s -> 4963.10s]  on the same CPU, or do we?
[4963.10s -> 4963.94s]  I don't know.
[4965.00s -> 4970.00s]  Wait, I'm not sure what the question is.
[4970.10s -> 4972.44s]  Yeah, I guess can you just explain more
[4972.44s -> 4974.68s]  like how does that happen?
[4974.68s -> 4976.12s]  Sorry, how does what happen?
[4977.68s -> 4979.92s]  Let's say we have multiple threads per process
[4979.92s -> 4982.94s]  so that they can run on different GPUs.
[4983.58s -> 4985.50s]  What do we go, how do we go about there?
[4985.50s -> 4986.70s]  Yeah, so Linux, for example,
[4986.70s -> 4989.82s]  supports multiple threads per process.
[4989.82s -> 4992.40s]  And in Linux, the implementation,
[4993.50s -> 4995.18s]  it is a complex implementation,
[4995.18s -> 5000.18s]  but maybe the simplest way to explain it is that each,
[5000.34s -> 5002.78s]  it's almost as if each thread in Linux
[5002.78s -> 5004.48s]  is a complete process.
[5005.38s -> 5008.62s]  And the threads of a given,
[5008.62s -> 5011.38s]  what we would call the threads of a particular process
[5011.38s -> 5013.62s]  are essentially separate processes
[5013.62s -> 5016.24s]  that share the same memory.
[5016.24s -> 5018.46s]  So Linux has sort of separated out the notion
[5018.46s -> 5021.72s]  of thread of execution from address space.
[5021.72s -> 5025.30s]  And you can have them separately.
[5025.30s -> 5027.32s]  And if you make two threads in one process,
[5027.32s -> 5029.00s]  it basically makes two processes
[5029.00s -> 5030.86s]  that share one address space.
[5030.86s -> 5034.74s]  And then from then on, the scheduling is not unlike
[5034.74s -> 5037.94s]  what XV6 does for individual processes.
[5037.94s -> 5038.78s]  I see.
[5038.78s -> 5041.14s]  And then is there anything,
[5041.14s -> 5043.48s]  does the user have to specify like,
[5043.48s -> 5045.76s]  okay, pin each thread to a CPU?
[5046.74s -> 5049.18s]  Or how does the OS make sure
[5049.18s -> 5050.70s]  that different threads of the same process
[5050.70s -> 5051.82s]  don't run on the same core?
[5051.82s -> 5053.62s]  Because that's kind of defeating the purpose.
[5053.62s -> 5055.22s]  Or not, I guess, I don't know.
[5056.90s -> 5059.36s]  It's actually just like, it's much like XV6,
[5059.36s -> 5063.38s]  namely the, you know, there's four cores
[5063.38s -> 5066.34s]  and Linux will just find four things
[5066.34s -> 5068.20s]  to run on those four cores.
[5068.40s -> 5072.00s]  They may be, you know, if there's not much going on,
[5072.00s -> 5075.56s]  then maybe there'll be four threads of the same process.
[5075.56s -> 5078.60s]  Or if there's 100 users logged in on an Athena machine,
[5078.60s -> 5081.06s]  maybe it's one thread each
[5081.06s -> 5083.24s]  from multiple different processes.
[5085.28s -> 5087.08s]  There's not any one answer.
[5087.08s -> 5089.24s]  Or the kernel basically finds something
[5089.24s -> 5092.72s]  for each core to do and then that core does that thing.
[5092.72s -> 5094.76s]  Okay, that makes sense.
[5094.76s -> 5097.00s]  You can, you know, if you're,
[5097.04s -> 5098.32s]  if you want to do careful measurements,
[5098.32s -> 5100.16s]  there is a way to pin threads to cores,
[5100.16s -> 5103.64s]  but people only do it when they're up to something strange
[5106.80s -> 5109.22s]  So they share the same virtual table?
[5110.16s -> 5110.98s]  Can you say that again?
[5110.98s -> 5111.82s]  Virtual memory.
[5111.82s -> 5116.52s]  So they say they have the same page table, those threads?
[5116.52s -> 5118.84s]  Yeah, yeah, yeah.
[5118.84s -> 5120.94s]  If you're on Linux, if you create two threads
[5120.94s -> 5123.44s]  in one process, then you have these two threads.
[5123.44s -> 5127.76s]  I don't know if they like literally share
[5127.76s -> 5129.96s]  the exact same page table
[5129.96s -> 5132.92s]  or whether their page tables are identical,
[5132.92s -> 5133.82s]  one or the other.
[5134.88s -> 5138.08s]  Is there a reason why they would have to be separate ever
[5139.08s -> 5141.02s]  if you manually map memory or?
[5142.86s -> 5147.86s]  I don't know enough to know whether, which Linux does.
[5150.52s -> 5153.36s]  Okay, I have another question about like a small detail.
[5154.36s -> 5156.92s]  So basically like from my understanding,
[5156.92s -> 5161.16s]  when you call switch, you switch from one call
[5161.16s -> 5162.28s]  to switch to another.
[5162.28s -> 5163.88s]  So the first time you call switch,
[5163.88s -> 5166.68s]  you have to like artificially create
[5166.68s -> 5169.72s]  that other end point to come back to, right?
[5169.72s -> 5170.76s]  Yes.
[5170.76s -> 5173.72s]  Cause you can't just randomly jump into any code.
[5173.72s -> 5174.56s]  Yes.
[5176.12s -> 5179.60s]  You want to know where that fake,
[5181.16s -> 5183.10s]  where that context is cooked up?
[5183.96s -> 5187.04s]  Probably somewhere where the process is created now,
[5187.04s -> 5187.88s]  I guess, I don't know.
[5187.88s -> 5188.84s]  Yeah, yeah, yeah, yeah.
[5188.84s -> 5192.72s]  Maybe user in it or not user in it.
[5192.72s -> 5194.04s]  Alloc proc.
[5197.28s -> 5199.12s]  No, alloc proc.
[5199.12s -> 5202.04s]  Is there something called fork trap or something?
[5202.04s -> 5203.20s]  Yeah, look at this.
[5203.20s -> 5205.46s]  Yeah, well, yeah, fork red.
[5205.46s -> 5208.44s]  Okay, so in alloc proc, which is called both
[5208.44s -> 5212.00s]  for the very first process at boot time and by fork,
[5213.00s -> 5217.00s]  alloc proc sets up the critical elements of the context
[5217.00s -> 5218.96s]  for the new processes.
[5220.80s -> 5222.60s]  It sets up the new processes context.
[5222.60s -> 5225.44s]  It actually doesn't matter what most of the registers are,
[5225.44s -> 5227.00s]  but it doesn't matter what RA is
[5227.00s -> 5229.72s]  because that's where the switch, the very first switch
[5229.72s -> 5231.96s]  in that process is going to return to RA.
[5233.68s -> 5236.32s]  And that process is going to need to use its own stack.
[5236.32s -> 5240.80s]  So RA and SP are set up or faked essentially.
[5240.84s -> 5244.44s]  So that the very first switch to do a process works.
[5244.44s -> 5246.48s]  So if I understand this correctly,
[5246.48s -> 5247.88s]  like when the switch will happen,
[5247.88s -> 5250.92s]  then it'll basically just start executing
[5250.92s -> 5253.78s]  the first instruction inside of the fork red
[5253.78s -> 5257.14s]  as if fork red just called switch and return from it.
[5257.14s -> 5258.46s]  Yeah, yeah, yeah.
[5258.46s -> 5261.32s]  Yeah, the return from switch is gonna be a jump
[5261.32s -> 5263.60s]  to the beginning of fork red.
[5263.60s -> 5266.62s]  Right, interesting.
[5266.62s -> 5270.20s]  Do we ever call fork red or is it always have,
[5270.64s -> 5272.04s]  I don't think it always happens like this.
[5272.04s -> 5274.84s]  Yeah, I don't think anything ever calls fork red for real.
[5274.84s -> 5279.04s]  I think it's just, yeah, it's only executed
[5279.04s -> 5283.58s]  in this weird way from the first time a process is run.
[5284.84s -> 5288.12s]  And it's really its job is to release the lock
[5289.22s -> 5291.72s]  that the scheduler took and then return.
[5291.72s -> 5295.72s]  And then this user trap red, of course, is also fake
[5295.72s -> 5299.02s]  in that it's, you know, yeah, it's like,
[5299.02s -> 5300.82s]  it's as if returning from a trap,
[5300.82s -> 5304.00s]  except the trap frame is faked also
[5304.00s -> 5306.86s]  to have like jump to the first instruction
[5306.86s -> 5309.76s]  and the user code.
[5311.02s -> 5313.42s]  Oh, but the trap frame, it's again the same,
[5313.42s -> 5315.34s]  like you don't need to initialize any registers
[5315.34s -> 5318.14s]  because it's like, well, we're going to the beginning.
[5318.14s -> 5320.66s]  So you don't need to assume anything.
[5320.66s -> 5322.62s]  Yeah, the program counter I think is,
[5323.70s -> 5326.58s]  yeah, it needs to be initialized to zero.
[5326.58s -> 5328.58s]  I don't know what else.
[5328.62s -> 5329.94s]  Maybe it...
[5332.62s -> 5335.18s]  I mean, probably if we call them, it doesn't, right?
[5335.18s -> 5336.90s]  Because if we already do the call,
[5336.90s -> 5338.98s]  then that's going to set the program counter.
[5338.98s -> 5341.02s]  Yeah, yeah, so here's the, this only happens.
[5341.02s -> 5345.06s]  Oh, because fork copies, fork copies the program counter,
[5345.06s -> 5346.78s]  the user program counter.
[5346.78s -> 5348.66s]  And so the only time when we're not doing a fork
[5348.66s -> 5349.88s]  is for the very first process
[5349.88s -> 5353.50s]  where it's like explicitly to set to zero.
[5353.50s -> 5355.78s]  And stack pointer also needs to be set up.
[5356.70s -> 5360.30s]  Oh yeah, because that's EPC, that's not PC.
[5360.30s -> 5361.90s]  That's the one that's going to get swapped
[5361.90s -> 5363.98s]  by the trap trampoline.
[5363.98s -> 5364.82s]  Yes.
[5365.90s -> 5366.82s]  Oh, I see.
[5368.02s -> 5370.50s]  Because the real PC is actually going to be in trap,
[5370.50s -> 5372.20s]  like inside trampoline,
[5372.20s -> 5375.54s]  but then we're going to switch it to jump to there.
[5375.54s -> 5376.54s]  Yeah, interesting.
[5379.38s -> 5382.82s]  Can I just ask, like, can you go back to the alloc proc?
[5385.78s -> 5390.78s]  I think there's a, oh no, sorry, fork read.
[5394.50s -> 5396.38s]  There's something there that happens,
[5396.38s -> 5399.16s]  I think for the first process only.
[5400.54s -> 5403.50s]  What's this for a first call?
[5403.50s -> 5405.30s]  I wasn't really sure what happened here.
[5405.30s -> 5407.22s]  Let's see, the file system,
[5407.22s -> 5409.78s]  the file system needs to be initialized.
[5409.78s -> 5413.38s]  And in particular, some stuff needs to be read off the disk
[5413.38s -> 5415.38s]  in order to get the file system going.
[5416.70s -> 5419.10s]  You know, like there's this thing called the superblock,
[5419.10s -> 5421.70s]  which describes like how big the file system is
[5421.70s -> 5423.78s]  and where the various things are in the file system.
[5423.78s -> 5426.06s]  And there's also a crash recovery log
[5426.06s -> 5430.70s]  that needs to be replayed in order to recover
[5430.70s -> 5432.90s]  from a previous crash, if there was one.
[5434.26s -> 5437.18s]  But in order to do anything in the file system,
[5437.18s -> 5438.50s]  you need to be able to wait
[5438.50s -> 5441.18s]  for disk operations to complete.
[5441.18s -> 5443.34s]  But the way xv6 works,
[5443.34s -> 5445.66s]  you really can only execute the file system code
[5446.50s -> 5451.50s]  in the context of a process in order to like wait for IO.
[5451.58s -> 5454.46s]  And so therefore the initialization of the file system
[5454.46s -> 5456.62s]  has to be deferred until the first time
[5456.62s -> 5457.98s]  we have a process running.
[5459.34s -> 5462.98s]  And that occurs in the very first process in fork read.
[5466.66s -> 5467.82s]  I see.
[5467.82s -> 5471.50s]  And I'm guessing we'll learn more about this later.
[5471.50s -> 5473.58s]  Yeah, not about this horrible hack,
[5473.62s -> 5475.66s]  but about how file systems work.
[5477.26s -> 5478.66s]  All right, okay.
[5478.66s -> 5479.50s]  Well, thank you.
[5479.50s -> 5481.30s]  I'm sorry for holding on for so long.
[5483.18s -> 5485.42s]  Sorry, is that okay? Thanks for all the answers.
[5485.42s -> 5486.26s]  Oh.
