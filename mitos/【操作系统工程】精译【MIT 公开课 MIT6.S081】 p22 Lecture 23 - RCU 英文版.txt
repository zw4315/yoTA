# Detected language: en (p=1.00)

[0.00s -> 14.84s]  All right, the sort of underlying topic for today is really getting good multi-core performance,
[14.84s -> 19.12s]  getting good performance on multi-core hardware.
[19.12s -> 28.16s]  And that's actually like a very interesting and deep, fascinating topic with many, many
[28.16s -> 31.76s]  different interesting aspects.
[31.76s -> 39.98s]  Today we're just going to bite off a fairly small piece, and that's how to get good performance
[39.98s -> 45.88s]  for shared data in the kernel that's read much more often than it's written.
[45.88s -> 52.20s]  And, you know, it turns out there's many kind of specific cases in which different
[52.20s -> 59.84s]  ideas for getting good multi-core performance are useful, and what we're going to look at
[59.84s -> 64.60s]  today is Linux's RCU, which has been very successful for sort of read heavy data, read
[64.60s -> 67.90s]  heavy kernel data.
[67.90s -> 76.48s]  The general sort of background here is that if you have modern machines with 4 or 8 or
[76.48s -> 85.72s]  16 or 64 or however many cores running in parallel and sharing memory, the kernel is really
[85.72s -> 87.96s]  a parallel process.
[87.96s -> 92.08s]  It's a parallel program, and if you're going to get good performance, you need to make
[92.08s -> 95.84s]  sure that the kernel can run a lot of its work as much as possible in parallel
[95.84s -> 99.08s]  on different cores in order to get that much more.
[99.08s -> 104.52s]  You know, if you can run the kernel parallel on 8 cores, all of them do useful work.
[104.52s -> 111.28s]  You can get 8 times the performance than if the kernel could only run on a single core.
[111.28s -> 117.52s]  And at a high level, this should clearly be possible.
[117.52s -> 121.92s]  If you have lots and lots of processes running on your computer, first of all, if
[121.92s -> 126.76s]  the processes aren't running and executing in the kernel, then we have very little to
[126.76s -> 127.76s]  worry about.
[127.76s -> 132.12s]  They're likely to run in parallel without any kernel having to do anything.
[132.12s -> 136.96s]  If the processes, if you have many applications running and they're all making system calls,
[136.96s -> 142.24s]  a lot of the time, different system calls made by different processes just seem like
[142.24s -> 145.56s]  they ought to be independent and should be able to proceed.
[145.56s -> 149.04s]  In many cases, though, certainly not all, but should be able to proceed completely
[149.04s -> 150.04s]  without interference.
[150.04s -> 157.76s]  Like if two processes are forking or two processes are reading different pipes or reading
[157.76s -> 165.36s]  or writing different files, there's no obvious reason why these should interfere with each other,
[165.36s -> 169.80s]  why they shouldn't be able to execute in parallel at n times the total throughput.
[169.80s -> 177.40s]  The problem is the kernel has a lot of shared resources in order to, you know, for
[177.40s -> 182.60s]  other good reasons, the kernel shares a lot of resources like memory and CPUs and a disk
[182.60s -> 186.92s]  cache and an inode cache and all this other stuff that's actually under the hood
[186.92s -> 189.96s]  shared between different processes.
[189.96s -> 194.92s]  And that means that even if two processors are doing system calls, two processors that
[194.92s -> 197.72s]  have totally never heard of each other and aren't trying to interact, make system
[197.72s -> 203.72s]  calls, if those system calls happen to allocate memory or use the disk cache or involve
[203.72s -> 210.80s]  scheduling decisions, they may well end up both using data structures in the kernel
[210.80s -> 214.60s]  and therefore we need some story for how they're both supposed to use the same data
[214.60s -> 218.92s]  without getting underfoot, without interfering with each other.
[218.92s -> 223.28s]  And there's been enormous effort over the years in making kernels, in making all these
[223.28s -> 227.64s]  cases in kernels run fast.
[227.64s -> 233.20s]  We've seen one, of course, that's oriented towards correctness, namely spin locks.
[233.20s -> 239.04s]  Spin locks are straightforward as such things go and easy to reason about, but, you know,
[239.04s -> 241.88s]  what a spin lock does is prevent execution.
[241.88s -> 247.68s]  It prevents its job, is to prevent parallelism in cases where there might be a problem between
[247.68s -> 248.68s]  two processes.
[248.68s -> 252.40s]  And so spin locks are just directly a way to decrease performance.
[252.40s -> 253.40s]  That's all they do.
[253.40s -> 258.08s]  Of course, they make it easy to reason about correctness, but they absolutely prevent
[258.08s -> 263.40s]  parallel execution and, you know, that's not always that desirable.
[263.92s -> 271.68s]  Okay, so again, we're going to focus on read heavy data on the case in which you have
[271.68s -> 275.12s]  data that's mostly read and relatively rarely written.
[275.12s -> 279.84s]  And the main example I'm going to use is a linked list, a singly linked list.
[279.84s -> 284.04s]  And so you can think of just the standard linked list diagram.
[284.04s -> 291.32s]  There's some sort of maybe global variable that's a pointer, head pointer, just a
[291.32s -> 298.92s]  pointer, and there's a bunch of list elements, and each list element has some data in it.
[298.92s -> 305.96s]  We'll just say it's a string, like, you know, hello is the sort of data in this element.
[305.96s -> 315.76s]  And each element also has a next pointer that points to the next list element.
[315.76s -> 321.08s]  And then finally, there's a pointer that points to zero to mark the answer.
[321.84s -> 323.84s]  Very straightforward.
[324.84s -> 328.20s]  And again, we're going to assume that most uses of this list that we're interested in
[328.20s -> 333.88s]  are just reads, you know, the kernel thread or whatever it is that's using this list,
[333.88s -> 337.64s]  it's just scanning a list looking for something, it's not trying to modify the list.
[337.64s -> 344.44s]  And occasional writers, though, you know, if there were zero writers ever, we wouldn't
[344.44s -> 348.28s]  need to have to worry about this at all, because it'd be a completely static list,
[348.28s -> 350.56s]  never changes, we can read it freely.
[350.56s -> 353.36s]  But we're going to imagine that every once in a while somebody comes along and wants
[353.36s -> 358.88s]  to write the list, so that may mean that some other thread wants to change the data
[358.88s -> 363.92s]  stored in the list element, or maybe delete an element, or maybe insert a new element
[363.92s -> 365.28s]  somewhere.
[365.28s -> 370.96s]  So even though it's aiming at mostly reads, we do have to worry about writes, we need
[370.96s -> 373.84s]  to make the reads safe in the face of writes.
[375.52s -> 380.16s]  And of course, in xv6, we just have a lock protecting this list, and a reader, you
[380.32s -> 383.84s]  know, not only would writers, in xv6, not only would writers have to acquire the lock,
[383.84s -> 388.32s]  but readers would have to acquire the lock too, because we've got to rule out the situation
[388.32s -> 392.16s]  in which, while we're reading, somebody's actually modifying the list, because that
[392.16s -> 399.36s]  could cause, sort of, the reader to see a half-updated value or follow an invalid
[399.36s -> 400.32s]  pointer or something.
[400.32s -> 408.24s]  So in xv6 we'd have locks, but that has the defect that if the common case is there's
[408.24s -> 415.28s]  no writers, it means that every time somebody comes along and reads, in xv6 they grab an
[415.28s -> 421.20s]  exclusive, like xv6 spin locks are exclusive, even if you have just two readers, only
[421.20s -> 422.72s]  one of them can proceed at a time.
[423.52s -> 429.52s]  So what we'd like, or sort of one way to improve this situation, would be to have
[429.52s -> 434.64s]  a new kind of lock that allows multiple readers, but only one writer.
[435.44s -> 440.40s]  And so I'm going to explore those next, actually, both because they're interesting
[441.44s -> 447.20s]  and because they help motivate the need for RCU, which we'll talk about in a little
[447.20s -> 447.70s]  while.
[449.04s -> 455.92s]  So there's this notion called read-write locks, and the interface is a little more
[455.92s -> 458.32s]  complicated than the spin locks we're used to.
[458.32s -> 462.08s]  We're going to imagine that there's one set of call that you call if you just want
[462.16s -> 463.20s]  to read something.
[463.20s -> 465.36s]  So we're going to imagine an rlock call.
[466.56s -> 471.76s]  You pass it a lock and then also an r unlock call.
[475.52s -> 480.16s]  And readers call these, and then there's a write lock call and a write unlock call.
[481.12s -> 490.80s]  And the semantics are that you can either have multiple readers acquire the lock for
[490.80s -> 494.56s]  reading, so we then would get parallelism.
[494.56s -> 500.96s]  Or you can have exactly one writer have acquired the lock, but you can never have a
[500.96s -> 501.36s]  mix.
[501.36s -> 508.24s]  You can never be in the locks, read-write locks rule out the possibility of somebody
[508.24s -> 511.44s]  having locked the lock for writing and also reading at the same time.
[511.44s -> 515.12s]  You either get one writer or lots of readers, but nothing else.
[517.36s -> 519.76s]  So that's the question.
[519.76s -> 520.00s]  Yes.
[521.28s -> 526.96s]  This may be an implementation detail, but what kind of mechanisms does this locking
[526.96s -> 533.44s]  scheme put in place to prevent someone writing while they hold a read lock?
[533.44s -> 534.08s]  Nothing, nothing.
[534.08s -> 535.76s]  It's just like xv6 locks.
[535.76s -> 536.56s]  They're completely...
[537.60s -> 541.92s]  We're talking about kernel code written by trusted, responsible developers.
[541.92s -> 546.56s]  And so just like spin locks in xv6, if the code that's using locks is incorrect,
[547.52s -> 548.24s]  it's incorrect.
[548.80s -> 549.36s]  There's no...
[549.44s -> 549.94s]  Okay.
[551.68s -> 554.32s]  And this is the way typical kernels are written.
[555.68s -> 560.56s]  You just have to assume that people developing the kernel are following their own rules.
[562.48s -> 563.04s]  Okay.
[563.04s -> 563.28s]  Okay.
[563.28s -> 569.04s]  And again, the reason why we care is that if we have a read-mostly data structure,
[569.04s -> 575.20s]  we'd love to have multiple readers be able to use it at the same time to get genuine
[575.20s -> 578.16s]  speed up from having multiple cores.
[579.60s -> 581.04s]  All right.
[581.04s -> 586.08s]  So if there were no problem here, this would just be the answer and we wouldn't have needed
[586.08s -> 587.28s]  to read today's paper.
[588.32s -> 593.36s]  But it turns out that if you dig into the details of what actually happens when you
[593.36s -> 599.36s]  use read-write locks, especially for data that's actually read a lot, there's some problems.
[599.92s -> 603.76s]  And in order to see what's going on, we actually have to look at the implementation.
[604.48s -> 617.52s]  Linux indeed has read-write lock implementation in it, and this is a kind of simplified version
[617.52s -> 619.36s]  of the Linux code.
[620.16s -> 625.76s]  The idea is that we have a struct rwlock, which is like struct lock in xv6, and it
[625.76s -> 626.64s]  has a count in it.
[628.56s -> 633.20s]  If the count is zero, that means that the lock is not held by anybody in any form.
[633.20s -> 636.24s]  If the count is negative one, that means that a writer has it locked.
[637.36s -> 642.00s]  And if the count is greater than zero, that means that n readers have it locked.
[642.00s -> 645.44s]  And we need to keep track of them because we can only let a writer in if the number
[645.44s -> 647.04s]  of readers descends to zero.
[655.12s -> 656.64s]  Okay, so somebody asked about adding...
[656.64s -> 662.24s]  No, I'm not sure if there's a question in the chat.
[662.88s -> 663.84s]  Interrupt me if there is.
[668.32s -> 677.12s]  The read lock function, it has to sit in a loop because if there's a writer, we have
[677.12s -> 677.92s]  to wait for the writer.
[677.92s -> 682.48s]  It looks...
[687.84s -> 689.84s]  It grabs a copy of the current n value.
[691.36s -> 694.40s]  If it's less than zero, that means there's a writer and we just need to continue our
[694.40s -> 696.96s]  loop and we're going to spin, waiting for the writer to go away.
[697.92s -> 701.92s]  Otherwise, we want to increment that value.
[703.12s -> 707.76s]  But we only want to increment it if it's still greater than or equal to zero.
[707.76s -> 708.40s]  So we can't...
[709.12s -> 710.24s]  There's many things we can't do.
[710.24s -> 716.32s]  We can't, for example, just add one with standard n equals n plus one because if a
[716.32s -> 719.92s]  writer sneaks in between when we check the value of n and when we actually try
[719.92s -> 724.24s]  to increment it, then we may actually go ahead and increment it at the same time
[724.24s -> 727.20s]  that some writer is setting it to minus one, which is wrong.
[727.20s -> 732.00s]  So we need to increment it only if it hasn't changed value since we checked it
[732.00s -> 734.40s]  and verified that it's greater than or equal to zero.
[735.20s -> 741.28s]  And the way people do that is they take advantage of special atomic or interlocked
[741.28s -> 747.60s]  instructions, which you saw before for our implementation of spin locks in xv6.
[748.64s -> 752.24s]  And the interlocked instruction, one that's particularly convenient to use,
[752.24s -> 753.92s]  is something called compare and swap.
[754.48s -> 756.64s]  The idea is that compare and swap takes three arguments.
[758.00s -> 761.44s]  The address of some location of memory that we want to act on.
[761.76s -> 767.28s]  The value that we think it holds and the value that we'd like it to hold.
[767.28s -> 771.60s]  And the semantics of compare and swap are that the hardware checks.
[771.60s -> 776.56s]  The hardware first sort of, you know, basically sets an internal lock that
[776.56s -> 781.60s]  makes only one compare and swap executed at a time on a given memory location.
[781.60s -> 786.80s]  Then the hardware checks that the current value of that location is indeed still x.
[786.80s -> 792.40s]  And if it's still x, sets it to this third argument, which is going to be x plus one.
[793.28s -> 794.80s]  And then the instruction yields one.
[797.76s -> 800.48s]  If compare and swap observes that the current value isn't x,
[802.32s -> 805.68s]  then it doesn't change the memory location and it returns zero.
[806.48s -> 812.56s]  So this is basically an atomic, if the location is x, set it to x plus one.
[813.60s -> 816.00s]  And it has to be atomic because there's really two things going on.
[816.00s -> 819.84s]  The hardware is checking the current value and setting it to a new value.
[821.92s -> 823.44s]  Any questions about compare and swap?
[825.92s -> 827.28s]  I have a question.
[827.28s -> 833.60s]  If there would be a reader and our lock needs to continue,
[834.32s -> 838.96s]  would w unlock reset the value back to x?
[839.44s -> 848.24s]  Oh, well, w unlock, if there's a writer, w unlock, which I'm afraid I didn't show,
[848.24s -> 852.32s]  sets n to zero because there can only be one writer.
[854.08s -> 861.76s]  If there's, what read unlock does is use another compare and swap to decrement n.
[861.76s -> 864.40s]  Okay.
[864.40s -> 876.96s]  Because what happens if a writer locks the lock between when x is being computed and...
[878.16s -> 878.96s]  Say right here?
[881.12s -> 886.80s]  No, between the if and x somehow.
[887.36s -> 887.86s]  Okay.
[888.82s -> 892.74s]  I'm not sure I understand exactly what time you're asking,
[892.74s -> 894.18s]  but it's absolutely a good question.
[894.18s -> 899.14s]  What happens if w lock is called somewhere during this sequence?
[900.18s -> 903.70s]  And for me, the most dangerous time for w lock to be called
[903.70s -> 907.78s]  is after this check, but before the compare and swap.
[908.82s -> 916.34s]  So let's imagine that read lock has gotten as far as seeing that x or l r o n is zero.
[917.30s -> 917.54s]  Okay.
[917.54s -> 922.82s]  So maybe we're right here and x is equal to zero.
[928.18s -> 929.22s]  We've already checked it.
[929.22s -> 930.26s]  So the check is finished.
[930.26s -> 934.42s]  And then right at this time on another core, some other thread calls w lock
[935.70s -> 938.34s]  and it actually gets its compare and swap in first.
[939.94s -> 942.42s]  So on the other core that's trying to grab the right lock,
[943.62s -> 946.02s]  compare and swap is going to see if l r o n is zero.
[946.10s -> 950.10s]  Let's assume that n is zero so that this test is true
[950.10s -> 954.42s]  and the compare and swap on that other core is going to set n to negative one.
[954.42s -> 960.02s]  So now the lock's locked, but we still think that n is zero in this code,
[960.98s -> 962.02s]  even though the lock's locked.
[962.58s -> 963.78s]  So now we're going to execute...
[963.78s -> 967.86s]  Now back on the reading core, we're going to execute compare and swap,
[967.86s -> 970.10s]  but we're going to pass zero here, right?
[970.10s -> 971.30s]  This is the value we actually...
[971.30s -> 973.86s]  We're going to pass in the value we actually looked at,
[973.86s -> 975.22s]  not the current value of n.
[976.02s -> 977.38s]  When we looked at it, it was zero.
[977.38s -> 980.42s]  So we're going to pass zero here and we're telling compare and swap,
[980.42s -> 982.50s]  look, only add one to...
[982.50s -> 985.94s]  Only set it to one if its current value is zero.
[986.74s -> 989.06s]  But it's not zero at this point, it's minus one.
[989.06s -> 994.82s]  And so this compare and swap fails, does not modify n, returns zero.
[994.82s -> 997.78s]  And so that means we'll go back to the top of this loop and try again.
[997.78s -> 999.70s]  Of course, now n is minus one.
[1002.10s -> 1004.34s]  This might be related to the previous question a bit,
[1004.34s -> 1008.18s]  but is it possible for an interrupt to occur
[1009.46s -> 1015.38s]  when that x plus one is being computed in the CAS parameter, or C-A-S parameter?
[1015.38s -> 1019.62s]  You mean before we actually execute C-A-S, but while we're computing its arguments?
[1019.62s -> 1019.94s]  Right.
[1019.94s -> 1024.50s]  So like you compute or you pass in the x argument and that's okay,
[1024.50s -> 1028.82s]  but before you compute the x plus one or while you're computing x plus one,
[1028.82s -> 1032.42s]  an interrupt occurs and so x plus one is wrong.
[1032.50s -> 1035.78s]  So if an interrupt occurs while we're computing x plus one,
[1035.78s -> 1036.82s]  that means we haven't...
[1036.82s -> 1039.86s]  C-A-S is actually an instruction, it's a single machine instruction.
[1040.50s -> 1043.54s]  So if we're computing x plus one, that means we haven't called C-A-S yet.
[1044.34s -> 1047.86s]  If the interrupt happens and all kinds of things may happen,
[1049.14s -> 1050.42s]  we're going to get the same...
[1050.42s -> 1056.26s]  If we originally read zero here, right, then interrupt or no interrupt,
[1057.06s -> 1060.10s]  we're going to pass one as this third argument,
[1060.10s -> 1062.10s]  because the interrupt is not going to reach out and change.
[1062.74s -> 1066.02s]  This x is a local variable for this code.
[1066.02s -> 1069.54s]  So interrupt or context, which or anything, is not going to change x.
[1070.42s -> 1073.70s]  And so that means we're going to pass zero and one here,
[1073.70s -> 1078.18s]  and if n is still zero, then we'll set it to one,
[1078.18s -> 1079.22s]  and that's what we want.
[1079.22s -> 1082.18s]  If it's not still zero, then compare and swap won't change it.
[1082.82s -> 1083.06s]  Right.
[1083.06s -> 1085.86s]  I guess you would have problems if you didn't set that local variable then.
[1088.18s -> 1090.98s]  If you used l arrow n here,
[1091.22s -> 1094.26s]  an l arrow n plus one, you would almost certainly be in big trouble,
[1094.26s -> 1097.70s]  because then n could change underfoot at any time.
[1097.70s -> 1099.94s]  That's why we actually grab a copy,
[1099.94s -> 1104.50s]  and grab a copy here in order to sort of fix a specific value.
[1105.14s -> 1105.46s]  Yeah.
[1108.42s -> 1108.74s]  Okay.
[1113.06s -> 1114.10s]  If two readers...
[1114.10s -> 1117.94s]  Okay, so I covered the case of whatever writer calls at the same...
[1118.50s -> 1120.50s]  w lock is called at the same time as r lock.
[1122.10s -> 1124.90s]  It's also interesting to wonder what if r lock is called at the same time.
[1126.10s -> 1128.42s]  So supposing n starts at a zero,
[1128.42s -> 1131.94s]  we know if two r locks are called at the same time,
[1131.94s -> 1134.82s]  what we want is for n to end up with value two,
[1135.46s -> 1137.78s]  and for both r locks to return.
[1137.78s -> 1139.06s]  That's what we want,
[1139.06s -> 1141.94s]  because we want two readers to be able to execute in parallel,
[1141.94s -> 1143.38s]  to use the data in parallel.
[1143.86s -> 1150.98s]  Okay, so they're both going to see zero at this point.
[1150.98s -> 1154.10s]  So at this point, both of them are going to have x equal to zero.
[1154.10s -> 1158.02s]  They're both going to call compare and swap with zero and one.
[1161.94s -> 1164.02s]  Only one of those two compare and swaps,
[1164.02s -> 1168.02s]  hopefully exactly one of those two compare and swaps will succeed.
[1168.02s -> 1171.70s]  Whichever one, you know, compare and swap is an atomic instruction.
[1172.66s -> 1176.98s]  They're only one of them happens at a time on a given memory location.
[1176.98s -> 1180.18s]  So whichever compare and swap is first,
[1180.18s -> 1183.94s]  we'll see that n is equal to zero and we'll set it to one.
[1184.58s -> 1187.70s]  The other cores simultaneous call to r lock,
[1187.70s -> 1189.62s]  its compare and swap will then execute,
[1190.74s -> 1192.66s]  and it'll still pass zero and one here.
[1194.50s -> 1196.66s]  But n will now be equal to one,
[1196.66s -> 1200.10s]  and so the compare and swap will fail for the second core,
[1201.06s -> 1202.02s]  and return zero.
[1203.70s -> 1205.78s]  The second core will go back to the top of this loop.
[1205.78s -> 1207.06s]  At this point, it will read one.
[1208.82s -> 1212.42s]  That's not less than zero, so we'll go on to the compare and swap,
[1212.42s -> 1214.18s]  and now it'll pass one and two,
[1214.18s -> 1217.30s]  and now the second read lock will succeed.
[1217.30s -> 1218.50s]  Both of them will have the lock.
[1219.70s -> 1221.38s]  So the first one succeeded in the first stride,
[1221.38s -> 1224.26s]  the second one actually go back to the loop and try again.
[1227.78s -> 1228.42s]  Any questions?
[1230.10s -> 1233.38s]  Oh, sorry.
[1233.38s -> 1235.30s]  So it is somehow possible then.
[1236.02s -> 1241.62s]  So a bunch of reads come and they're reading their stuff,
[1241.62s -> 1246.26s]  and then a write also comes and it also wants to write,
[1246.26s -> 1248.98s]  but then some other reads also come after the write,
[1249.86s -> 1254.74s]  but then somehow the reads like outrun the write
[1255.38s -> 1259.46s]  and the write still has to wait somehow?
[1260.42s -> 1264.90s]  So if the sequence is that a reader managed to acquire the lock,
[1264.90s -> 1266.10s]  one or more readers have the lock,
[1266.10s -> 1268.82s]  so now n, each of them,
[1268.82s -> 1270.66s]  this is called a compare and swap,
[1270.66s -> 1272.34s]  adds one to n for each reader.
[1272.34s -> 1276.42s]  So now n is greater than zero because there's multiple readers.
[1276.42s -> 1278.58s]  If a writer tries to acquire the lock at this point,
[1280.42s -> 1282.74s]  the writers compare and swap,
[1282.74s -> 1284.42s]  the compare value is zero.
[1284.42s -> 1287.62s]  So compare and swap will only change n to minus one
[1288.42s -> 1289.86s]  if its current value is zero.
[1290.50s -> 1292.98s]  But we know the current, because there's multiple readers,
[1293.70s -> 1295.38s]  the current value of n is not zero,
[1295.38s -> 1296.98s]  and so the compare and swap will fail
[1297.70s -> 1301.78s]  and return zero when the writer will sit here in this loop,
[1302.74s -> 1304.90s]  basically waiting until n is equal to zero
[1306.18s -> 1309.06s]  before its compare and swap will succeed and return
[1309.06s -> 1310.66s]  and give the lock to the writer.
[1311.70s -> 1313.54s]  So this certainly means the writer can be starved
[1314.66s -> 1315.94s]  if there's a lot of readers.
[1316.02s -> 1318.58s]  N may never be zero, and so the write may never succeed.
[1318.58s -> 1320.26s]  So that's a defect in this locking scheme.
[1322.74s -> 1323.24s]  Thank you.
[1324.58s -> 1328.18s]  I also have a question about the two reader scenario
[1328.18s -> 1329.46s]  that I just mentioned.
[1330.90s -> 1334.10s]  It appears that in the worst case,
[1334.10s -> 1336.18s]  the reader that arrives second
[1336.18s -> 1338.82s]  has to go through another iteration of the loop.
[1339.86s -> 1341.78s]  It sounds somewhat wasteful.
[1342.34s -> 1345.46s]  I wonder if this generalizes to n writers.
[1345.54s -> 1346.50s]  It certainly does.
[1346.50s -> 1348.98s]  They all have to get lost and start again?
[1348.98s -> 1353.46s]  You put your finger on why people don't like this scheme
[1355.30s -> 1357.54s]  if there's a lot of simultaneous reader.
[1358.50s -> 1361.38s]  And so for the reason you just mentioned,
[1363.78s -> 1366.50s]  R-Lock, even if there's no writers at all,
[1366.50s -> 1369.38s]  if there's lots of readers or there's readers on many cores,
[1369.94s -> 1372.74s]  R-Lock can be very, very expensive.
[1373.54s -> 1378.26s]  And one thing you need to know about the R-Lock scheme,
[1378.26s -> 1381.06s]  which I think we've already mentioned in class,
[1381.06s -> 1385.62s]  is that on a multi-core system,
[1385.62s -> 1389.14s]  every core has an associated cache.
[1390.50s -> 1391.78s]  We'll say it's the L1 cache.
[1391.78s -> 1393.78s]  So each core has a bit of cache memory.
[1395.06s -> 1396.66s]  And whenever a reader writes something,
[1399.14s -> 1400.10s]  it sits in the cache.
[1400.10s -> 1402.02s]  And so there may be lots and lots of cores.
[1402.02s -> 1405.22s]  And there's some kind of interconnect network
[1405.22s -> 1406.90s]  that allows the cores to talk to each other.
[1407.86s -> 1412.58s]  Because, of course, if lots of cores have some data cached
[1412.58s -> 1414.74s]  and one of the cores writes that data,
[1414.74s -> 1416.98s]  the writing core has to tell the other cores
[1416.98s -> 1419.22s]  that they're not allowed to cache the data anymore.
[1419.22s -> 1420.66s]  It's called invalidation.
[1423.14s -> 1428.18s]  So what actually happens if you have n readers,
[1428.82s -> 1431.54s]  n people calling R-Lock at about the same time
[1431.62s -> 1436.98s]  on n cores, they're all going to read n,
[1437.78s -> 1439.54s]  sorry, this L-arrow n value,
[1440.18s -> 1443.38s]  and load this memory location into their caches.
[1446.34s -> 1448.10s]  They're all going to call a compare and swap.
[1450.50s -> 1453.14s]  The first one to actually call compare and swap
[1453.14s -> 1454.74s]  is going to modify the data.
[1454.74s -> 1456.34s]  But in order for it to modify the data,
[1456.34s -> 1458.90s]  it has to invalidate all these other copies.
[1458.90s -> 1461.06s]  And so the compare and swap instruction
[1461.62s -> 1463.62s]  has to send out an invalidate message
[1463.62s -> 1466.66s]  over this little network to each of the other n cores.
[1468.18s -> 1470.02s]  And then it returns all the other cores,
[1470.02s -> 1471.22s]  the n minus one cores.
[1471.22s -> 1473.46s]  They have their compare and swaps now
[1473.46s -> 1474.90s]  actually have to reread,
[1475.70s -> 1477.78s]  again requiring traffic over the network,
[1477.78s -> 1482.34s]  reread this data, this memory location,
[1483.30s -> 1484.82s]  compare it with x and they'll have failed
[1484.82s -> 1486.26s]  because they all call x with zero.
[1487.14s -> 1489.06s]  Then the remaining n minus one readers
[1489.06s -> 1490.18s]  go back to the top of the loop
[1490.18s -> 1492.50s]  and all n minus one of them again read the data.
[1493.06s -> 1494.58s]  And again, one of them writes it.
[1497.62s -> 1499.78s]  So there's going to be n times through the loop
[1499.78s -> 1502.74s]  once for each core trying to acquire the lock.
[1503.54s -> 1505.06s]  Each of those trips through the loop
[1506.34s -> 1509.46s]  involves order n messages on the network
[1509.46s -> 1512.98s]  because at least every copy of the cached L-arrow n
[1512.98s -> 1514.18s]  has to be invalidated.
[1516.66s -> 1519.38s]  And that means that the total cost
[1519.38s -> 1522.98s]  for n cores to acquire a particular lock
[1524.10s -> 1526.34s]  even for reading is order n.
[1528.66s -> 1530.58s]  And that means as you increase the number of cores
[1530.58s -> 1532.66s]  for a popular piece of data,
[1532.66s -> 1536.82s]  the cost for everybody to lock it just once
[1538.98s -> 1540.74s]  goes up, sorry, it's order n squared.
[1542.82s -> 1544.82s]  The total cost and time
[1546.18s -> 1548.90s]  or messages sent over this interconnect is n squared.
[1550.18s -> 1551.78s]  And this is a very bad deal.
[1552.98s -> 1556.18s]  You would hope that if you needed to do something 10 times,
[1556.18s -> 1558.18s]  10 different cores needed to do something,
[1558.18s -> 1561.30s]  especially given that they're just reading the list,
[1561.30s -> 1562.50s]  they're not modifying it,
[1562.50s -> 1564.50s]  you'd hope that they could really run in parallel.
[1564.50s -> 1566.26s]  That is the total wall clock time
[1566.26s -> 1569.22s]  for 16 cores to read something
[1570.02s -> 1571.86s]  should be the same as the total wall clock time
[1571.86s -> 1573.06s]  for one core to read something
[1573.06s -> 1575.78s]  because that's what getting parallel as a means
[1575.78s -> 1577.94s]  is that you can do things at the same time.
[1579.46s -> 1581.62s]  But here, the more cores that try to read this,
[1581.62s -> 1584.50s]  the more expensive the lock acquisition is.
[1584.50s -> 1589.06s]  And so what's going on is that this style of locks
[1589.62s -> 1592.58s]  has converted read-only access to data.
[1592.58s -> 1595.14s]  The list is probably sitting in the cache already
[1595.14s -> 1596.66s]  because nobody's modifying the list.
[1597.62s -> 1600.10s]  So the actual access to the list
[1600.10s -> 1601.94s]  might only take a few dozen cycles.
[1602.74s -> 1604.10s]  But if the data is popular,
[1604.10s -> 1607.54s]  getting the lock can take hundreds or thousands of cycles
[1607.54s -> 1609.14s]  because of this n squared effect.
[1609.70s -> 1612.58s]  And the fact that instead of it being cached accesses,
[1612.58s -> 1614.90s]  it's these accesses that have to go over the bus,
[1616.02s -> 1618.66s]  this interconnect in order to invalidate
[1618.66s -> 1621.46s]  and do these cache coherence operations.
[1622.34s -> 1624.42s]  So these locks have turned
[1626.18s -> 1628.26s]  a very cheap read-only access to data
[1628.26s -> 1632.98s]  into an extremely expensive read-write access to this data
[1634.90s -> 1636.66s]  and will probably completely destroy
[1636.66s -> 1638.66s]  any possible parallel performance.
[1639.70s -> 1640.66s]  If what you were doing,
[1641.86s -> 1643.86s]  if the actual data was fairly simple to read,
[1644.66s -> 1647.30s]  the lock will dominate and destroy parallel performance.
[1649.70s -> 1652.98s]  So any questions about this performance story?
[1659.86s -> 1663.54s]  In a sense, the bad performance of read-write locks
[1664.26s -> 1667.22s]  is the reason for the existence of RCU.
[1668.18s -> 1670.18s]  Because if this was efficient,
[1671.78s -> 1676.82s]  then there'll be no need to do better than that, right?
[1676.82s -> 1678.02s]  But it's terribly inefficient.
[1678.90s -> 1681.70s]  And there's two things going on.
[1681.70s -> 1683.06s]  One is the details of this,
[1683.06s -> 1685.06s]  oh, there needs to be a total of n squared trips
[1685.06s -> 1687.06s]  through this loop if we have n cores
[1687.06s -> 1688.82s]  is sort of one way of looking at it.
[1688.82s -> 1691.14s]  The other way of looking at it is that we're writing,
[1692.42s -> 1695.22s]  regardless of the details of what's going on here,
[1695.30s -> 1697.86s]  these locks have turned a read-only access,
[1697.86s -> 1699.78s]  which could be cached and extremely fast,
[1700.66s -> 1704.10s]  into an access that one way or another involves a write,
[1704.10s -> 1705.46s]  one or more writes.
[1705.46s -> 1708.58s]  And writes are just much more expensive than reads
[1708.58s -> 1710.42s]  if we're writing data that might be shared
[1711.86s -> 1712.90s]  with other cores.
[1713.46s -> 1716.66s]  Because a read for data that's not modified
[1716.66s -> 1719.30s]  can be satisfied in a couple cycles out of your own cache.
[1720.42s -> 1723.70s]  Any write to data that may be cached by other cores
[1723.70s -> 1726.50s]  has to involve communication between cores
[1726.50s -> 1728.10s]  to invalidate other copies.
[1728.10s -> 1729.70s]  So no matter how you slice it,
[1730.58s -> 1732.42s]  anything that involves a write to share data
[1732.42s -> 1733.94s]  is a disaster for performance
[1735.30s -> 1737.14s]  if you otherwise could have been read-only.
[1737.78s -> 1741.78s]  So the details of this loop are sort of less important
[1741.78s -> 1745.46s]  than the fact that it did a write to share data.
[1746.42s -> 1747.70s]  So what we're looking for is a way
[1748.26s -> 1753.46s]  to be able to read data without writes, right?
[1753.46s -> 1755.06s]  We want to be able to scan that list
[1756.90s -> 1758.34s]  without doing any writes whatsoever,
[1758.34s -> 1760.74s]  including any writes that might be required
[1760.74s -> 1762.42s]  to do some kind of locking thing.
[1762.42s -> 1765.54s]  So we're looking for really, really read-only access to data.
[1768.58s -> 1774.02s]  Okay, so one possibility.
[1775.78s -> 1776.66s]  It's not a possibility,
[1776.66s -> 1778.10s]  but it's sort of a thought experiment
[1778.10s -> 1781.62s]  is we just have the readers not bother locking, right?
[1781.86s -> 1783.06s]  Occasionally you get lucky,
[1783.06s -> 1784.82s]  and it turns out that readers can read stuff
[1786.02s -> 1787.46s]  and that only writers need to lock.
[1787.46s -> 1788.82s]  So we'll just do a quick experiment
[1788.82s -> 1794.66s]  to see whether we could have readers
[1794.66s -> 1797.54s]  just read the list without locking it.
[1798.58s -> 1801.86s]  So suppose we have this list and it has some strings.
[1801.86s -> 1813.86s]  And we're going to read it.
[1813.86s -> 1816.66s]  Okay, so nothing goes wrong if there's no writer, right?
[1816.66s -> 1817.62s]  You just read the list.
[1817.62s -> 1818.50s]  It's not a problem.
[1818.50s -> 1820.02s]  So we got to imagine there's a writer.
[1820.90s -> 1822.74s]  And there's probably three cases
[1823.30s -> 1828.18s]  if you read a list while some other core is modifying it.
[1828.50s -> 1833.30s]  So one case is that the writer is just changing the content.
[1834.82s -> 1838.10s]  That is, not adding or deleting elements necessarily,
[1838.10s -> 1840.82s]  a writer is just changing the string to be some other string.
[1841.62s -> 1844.42s]  So one is the writer is changing the content.
[1845.06s -> 1848.02s]  Two is the writer is inserting a new list element.
[1849.78s -> 1853.86s]  And the third case is if the writer is deleting a list element.
[1854.82s -> 1860.18s]  And I want to examine these because we need a story for each
[1860.18s -> 1862.42s]  and RCU actually kind of has a story for each.
[1862.42s -> 1865.06s]  So the danger,
[1865.06s -> 1866.42s]  so I'm just talking about what goes wrong
[1866.42s -> 1867.62s]  if somebody's reading a list
[1867.62s -> 1869.22s]  while another core is writing it.
[1869.22s -> 1872.34s]  If the writer wants to just change this string,
[1872.90s -> 1874.58s]  then the danger is that the reader
[1874.58s -> 1877.14s]  will be actually reading the bytes of this string
[1877.14s -> 1879.54s]  or whatever else is in the list element
[1879.54s -> 1882.98s]  while the writer is modifying the same bytes.
[1882.98s -> 1884.66s]  And so if we don't do anything special,
[1884.66s -> 1887.70s]  the reader will see some mixture of the old bytes
[1887.70s -> 1888.50s]  and the new bytes.
[1889.94s -> 1891.46s]  And that's probably a disaster.
[1892.58s -> 1893.94s]  That's one case we have to worry about.
[1894.58s -> 1896.18s]  Another possibility is that the writer
[1896.18s -> 1898.34s]  is inserting a new element.
[1898.34s -> 1900.74s]  And of course, what that means is that,
[1900.74s -> 1903.38s]  you know, supposing the writer wants to insert
[1903.38s -> 1904.50s]  the new element at the head,
[1904.50s -> 1906.66s]  the writer is going to cook up some new element,
[1906.66s -> 1908.82s]  going to change the head pointer to point to it
[1908.82s -> 1911.62s]  and going to change the new element
[1911.70s -> 1914.58s]  to point at the old first element, right?
[1914.58s -> 1918.66s]  And so the danger here,
[1919.62s -> 1921.62s]  if a reader is reading a list
[1922.18s -> 1923.62s]  while the writer is inserting,
[1923.62s -> 1927.62s]  is that maybe, you know, if we really blow it,
[1931.62s -> 1934.18s]  the writer may set the head pointer
[1934.18s -> 1935.62s]  to point to the new element
[1935.62s -> 1938.18s]  before the new element's initialized.
[1938.18s -> 1940.82s]  That is, while it maybe contains garbage for the string
[1940.82s -> 1944.66s]  or some illegal pointer as the next element.
[1946.18s -> 1947.78s]  So that's the thing that could go wrong
[1947.78s -> 1948.90s]  for writers inserting.
[1950.18s -> 1950.66s]  So let's,
[1954.10s -> 1955.38s]  and if the writer's deleting,
[1957.14s -> 1960.18s]  then, you know, what it means to delete an element
[1960.18s -> 1961.70s]  is first to change,
[1961.70s -> 1963.06s]  let's say we're deleting the first element,
[1963.06s -> 1965.86s]  we change the head pointer to point to the second element
[1965.86s -> 1967.86s]  and then call free on the first element
[1967.86s -> 1969.30s]  to return this to the free list.
[1970.90s -> 1972.74s]  And the danger here,
[1972.74s -> 1975.38s]  you know, if the reader sees the new head pointer,
[1975.38s -> 1976.90s]  that's fine, they're just gonna go on
[1976.90s -> 1978.82s]  to the second element to the first.
[1979.62s -> 1982.42s]  If the reader actually was looking at the first element
[1982.42s -> 1984.34s]  and then the writer freed it,
[1984.34s -> 1986.02s]  then the problem we have is now the reader
[1986.02s -> 1988.74s]  is looking at an element that's on the free list
[1988.74s -> 1990.74s]  and could be allocated for some other use
[1990.74s -> 1993.30s]  and overwritten for some completely other use
[1993.30s -> 1995.54s]  while the reader's still looking at this element.
[1995.54s -> 1996.90s]  And so from the reader point of view now,
[1996.90s -> 1998.58s]  all of a sudden the element's filled with garbage
[1999.22s -> 2000.34s]  and said it was expecting.
[2001.06s -> 2002.58s]  So that's the third case we have to,
[2003.94s -> 2005.22s]  if we wanna have a lock,
[2005.22s -> 2007.54s]  we wanna have absolutely no locks for readers,
[2008.26s -> 2010.34s]  we have to worry about these three situations.
[2012.18s -> 2015.62s]  And I'm not talking about writer versus writer problems here
[2015.62s -> 2018.74s]  because I'm just assuming for this entire lecture
[2018.74s -> 2020.58s]  that writers still use locks,
[2020.58s -> 2022.10s]  that there's still some ordinary,
[2022.10s -> 2024.34s]  like xv6 style spin lock here
[2024.34s -> 2027.78s]  and writers acquire this lock before doing anything,
[2027.78s -> 2030.34s]  but readers don't acquire any lock whatsoever.
[2031.46s -> 2033.46s]  So questions about these dangers.
[2038.26s -> 2043.46s]  Okay, the point is we can't just simply have readers
[2043.46s -> 2044.34s]  read with no locks,
[2045.54s -> 2049.94s]  but it turns out we can't fix these specific problems
[2049.94s -> 2053.22s]  and that takes us to RCU.
[2055.14s -> 2056.90s]  And RCU has a couple of ideas in it that,
[2057.86s -> 2059.54s]  RCU is by the way,
[2060.18s -> 2065.06s]  it's as much a kind of approach to concurrency control
[2065.06s -> 2066.90s]  as it is a particular algorithm,
[2067.46s -> 2069.30s]  or it's a way of structuring,
[2069.30s -> 2072.34s]  it's an approach to structuring readers and writers
[2072.34s -> 2074.42s]  so that they can get along
[2075.22s -> 2077.14s]  with the readers not having to take locks.
[2080.18s -> 2081.86s]  A general game with read copy update
[2081.86s -> 2083.94s]  is we're gonna fix those three situations
[2083.94s -> 2085.78s]  in which readers might get into trouble
[2085.78s -> 2087.78s]  if there's concurrent writers,
[2087.78s -> 2090.10s]  and we're gonna do it by making the writers
[2090.10s -> 2091.30s]  a little bit more complicated.
[2092.74s -> 2094.50s]  So the writer is gonna end up somewhat slower,
[2095.70s -> 2097.94s]  they still need to lock plus follow some extra rules,
[2098.58s -> 2100.02s]  but the reward will be the readers
[2100.02s -> 2101.30s]  will be dramatically faster
[2101.94s -> 2103.54s]  because they can operate without locks
[2103.54s -> 2105.14s]  and without ever writing memory.
[2108.10s -> 2112.98s]  Okay, so the first big idea in RCU,
[2116.66s -> 2121.86s]  is that in that first trouble situation
[2121.86s -> 2122.74s]  we talked about before,
[2122.74s -> 2125.30s]  where the writer is updating a list element,
[2125.30s -> 2126.90s]  the content of a list element,
[2128.66s -> 2130.34s]  we're gonna actually outlaw that,
[2130.34s -> 2132.50s]  we're gonna say writers are not allowed
[2132.50s -> 2136.58s]  to modify the contents of list elements.
[2136.58s -> 2141.46s]  Instead, if we have a linked list like this
[2141.46s -> 2142.58s]  with a couple of elements,
[2145.78s -> 2151.70s]  if a writer wanted to update the content of element two,
[2153.70s -> 2155.14s]  instead of changing it in place,
[2155.14s -> 2156.26s]  which it wouldn't do,
[2156.26s -> 2159.22s]  it would actually call the allocator
[2159.22s -> 2161.78s]  to allocate a new element.
[2164.58s -> 2167.06s]  It would initialize the element completely,
[2167.06s -> 2172.66s]  so whatever new content we wanted to put here
[2172.66s -> 2174.26s]  instead of the old content,
[2174.26s -> 2179.70s]  the writer would set the next pointer on this new element,
[2179.70s -> 2182.82s]  so that this new element is now completely correct looking,
[2183.46s -> 2187.78s]  and then in a single write to E1's next pointer,
[2187.78s -> 2194.58s]  the writer would switch E1 from pointing to the old version of E2
[2194.58s -> 2196.34s]  to pointing to the new version of E2.
[2197.06s -> 2199.14s]  So the game is instead of updating things in place,
[2199.14s -> 2203.86s]  we're going to replace them with new versions of the same data,
[2203.86s -> 2207.62s]  and so now a reader,
[2208.26s -> 2210.02s]  if a reader's gotten as far as E1
[2210.02s -> 2212.34s]  and is just looking at E1's next pointer,
[2212.34s -> 2214.50s]  the reader's gonna either see the old next pointer,
[2214.50s -> 2215.54s]  which points to E2,
[2215.54s -> 2218.34s]  and that's fine because nobody was changing E2,
[2218.34s -> 2222.02s]  or the reader's gonna see the new next pointer
[2222.02s -> 2226.66s]  and look at the new list element,
[2227.78s -> 2228.50s]  and either way,
[2228.50s -> 2233.38s]  since the writer fully initialized this list element,
[2233.94s -> 2235.46s]  before setting E1's next pointer,
[2235.46s -> 2238.66s]  either way, the reader's gonna see a correct next pointer
[2238.66s -> 2239.62s]  that points to E3.
[2244.42s -> 2246.90s]  So the point is the reader will never see a string
[2246.90s -> 2248.18s]  that's in the process of being,
[2248.82s -> 2250.66s]  content that's in the process of being modified.
[2252.26s -> 2257.14s]  Is there any questions about this particular idea?
[2257.14s -> 2266.90s]  Okay, I can go ahead.
[2267.86s -> 2272.58s]  Will the link between E2 and E3 be deleted,
[2272.58s -> 2274.02s]  or will it be left there
[2274.02s -> 2277.78s]  in case that a reader somehow reached E2?
[2277.78s -> 2279.62s]  Now we're just gonna leave it.
[2280.58s -> 2282.10s]  So I'll come to this,
[2282.10s -> 2283.46s]  this is an excellent question,
[2284.18s -> 2288.58s]  and it's actually the main piece of complexity in RCU,
[2289.30s -> 2290.74s]  but for now we're just gonna imagine
[2290.74s -> 2292.50s]  that E2's left alone for the moment.
[2295.62s -> 2297.54s]  The link from E2 to E3,
[2297.54s -> 2299.22s]  we don't need to worry about it anyway, right?
[2299.22s -> 2300.42s]  Because that's a part of E2,
[2300.42s -> 2302.90s]  and like in normal implementations,
[2302.90s -> 2304.42s]  we just free that anyway.
[2304.42s -> 2306.42s]  Like with no RCU involved,
[2306.42s -> 2308.50s]  we don't ever need to worry about that link, right?
[2309.46s -> 2312.90s]  The danger is that just before we changed
[2312.90s -> 2314.18s]  this next pointer,
[2314.18s -> 2316.74s]  that some reader had followed the next pointer to E2.
[2318.10s -> 2319.54s]  So what we're worried about here
[2319.54s -> 2321.14s]  is that some reader on some course
[2321.14s -> 2322.66s]  is actually right now reading E2,
[2323.30s -> 2326.58s]  so we'd better not free it right away.
[2326.58s -> 2327.38s]  Right, right.
[2327.94s -> 2329.22s]  That's what I think that's all we're saying,
[2329.22s -> 2330.74s]  is you better not free E2 right away.
[2331.38s -> 2332.10s]  Just leave it alone.
[2336.74s -> 2337.86s]  As a piece of jargon,
[2338.02s -> 2344.42s]  the write, the swap of E1's next pointer
[2344.42s -> 2346.50s]  from the old E2 to the new E2,
[2347.62s -> 2350.18s]  I, in my head, I call this a committing write.
[2353.38s -> 2354.50s]  Part of the reason why this works
[2354.50s -> 2356.26s]  is that with a single committing write,
[2356.26s -> 2357.30s]  which is atomic,
[2357.30s -> 2359.78s]  like writes to pointers on the machines we use
[2359.78s -> 2362.10s]  are atomic in the sense that
[2362.10s -> 2363.70s]  either the write to the pointer happened
[2363.70s -> 2366.18s]  or didn't happen from the perspective of readers.
[2366.18s -> 2367.30s]  Because they're atomic,
[2367.30s -> 2368.82s]  basically with that one instruction,
[2369.78s -> 2371.46s]  with that one atomic store,
[2371.46s -> 2373.54s]  we can, it's an ordinary store,
[2373.54s -> 2376.34s]  but it's indivisible.
[2377.46s -> 2379.30s]  We switch E1 from pointing to the old,
[2380.26s -> 2382.02s]  the next pointer from pointing to the old one,
[2382.02s -> 2382.58s]  to the new one,
[2382.58s -> 2384.66s]  and that write is what sort of commits us to
[2385.78s -> 2388.26s]  now using the second version.
[2392.10s -> 2394.02s]  This is a very basic technique,
[2394.02s -> 2396.74s]  a very important technique for RCU.
[2397.38s -> 2399.94s]  And what it means is that
[2399.94s -> 2404.10s]  RCU is really mostly applicable to data structures
[2404.10s -> 2407.14s]  for which you can have single committing writes.
[2408.02s -> 2409.62s]  So that means there's some data structures
[2409.62s -> 2411.14s]  which are quite awkward in the scheme,
[2411.14s -> 2412.82s]  like a doubly linked list
[2414.42s -> 2416.82s]  where every element is pointed to
[2416.82s -> 2418.18s]  from two different pointers.
[2418.90s -> 2421.78s]  Now we can't get rid of list elements
[2421.78s -> 2423.22s]  with a single committing write
[2423.22s -> 2424.82s]  because there's two pointers to it.
[2424.82s -> 2426.42s]  We can't, on most machines,
[2426.42s -> 2428.10s]  you can't atomically change
[2428.10s -> 2430.34s]  two different memory locations at the same time.
[2431.62s -> 2434.02s]  So doubly linked lists are not so good for RCU.
[2434.90s -> 2437.46s]  A data structure that is good is a tree, right?
[2437.46s -> 2440.18s]  If you have a tree of,
[2445.78s -> 2447.30s]  a tree of nodes like this,
[2447.30s -> 2448.50s]  then we can do,
[2448.50s -> 2449.86s]  supposing we want to change,
[2450.50s -> 2452.50s]  we want to modify this value down here.
[2453.22s -> 2453.94s]  What we can do,
[2457.30s -> 2458.74s]  there's some head to the tree.
[2459.30s -> 2460.66s]  What we can do is cook up
[2463.62s -> 2465.86s]  a new version of this part of the tree here
[2467.30s -> 2470.02s]  and with a single committing write to the head pointer,
[2470.58s -> 2472.26s]  switch to the new version of the tree.
[2472.26s -> 2473.86s]  And so the new version of the tree,
[2474.42s -> 2477.14s]  which will, you know, the writer will allocate,
[2478.66s -> 2479.38s]  sort of create,
[2483.78s -> 2486.10s]  can actually share for convenience,
[2486.10s -> 2487.14s]  can share structure,
[2487.14s -> 2489.14s]  the unmodified part with the old tree.
[2489.14s -> 2490.82s]  And then with a single committing write,
[2490.82s -> 2492.98s]  we're going to change the head pointer to,
[2492.98s -> 2494.90s]  the tree head pointer to point to the new version.
[2498.66s -> 2499.78s]  But for other data structures
[2499.78s -> 2501.14s]  that don't look like Lister trees,
[2501.14s -> 2503.06s]  it's not so easy to use RCU.
[2508.02s -> 2511.14s]  Okay, so that's the first idea.
[2512.02s -> 2513.06s]  Any last questions?
[2516.10s -> 2521.30s]  The second idea,
[2530.82s -> 2532.18s]  one of the problems with,
[2532.98s -> 2535.14s]  one of the potential problems with
[2540.42s -> 2542.50s]  the scheme I just described,
[2542.50s -> 2548.02s]  and we're going to cook up a new E2 prime.
[2548.02s -> 2549.22s]  And what I said was,
[2549.22s -> 2552.34s]  oh, well, we'll initialize the content for E2 prime
[2552.34s -> 2555.62s]  and we'll, you know, set its next pointer correctly.
[2555.62s -> 2560.10s]  And after that, we'll set E1's next pointer
[2560.10s -> 2561.06s]  to point to E2.
[2562.74s -> 2566.66s]  As you may recall from discussions of XV6,
[2566.66s -> 2569.30s]  by default, there's no after that on these machines.
[2570.02s -> 2571.94s]  The compiler and the hardware,
[2572.82s -> 2576.10s]  basically all compilers and many microprocessors,
[2576.82s -> 2578.82s]  reorder memory operations.
[2579.62s -> 2582.10s]  So if you simply, you know,
[2582.10s -> 2586.18s]  say we allocate a new element
[2587.70s -> 2589.78s]  and we just wrote this C code,
[2589.78s -> 2595.06s]  you know, E arrow next equals, you know, E3,
[2596.02s -> 2602.10s]  and then E1 arrow next equals E.
[2603.06s -> 2604.58s]  This is not going to work well.
[2604.58s -> 2605.86s]  This is not going to work reliably.
[2605.86s -> 2607.30s]  It's going to work fine when you test it,
[2608.58s -> 2610.74s]  but it won't work in real life all the time.
[2610.74s -> 2611.94s]  Occasionally it'll go wrong.
[2611.94s -> 2614.58s]  And the reason is that the compiler
[2615.70s -> 2617.62s]  may end up reordering these writes
[2618.18s -> 2620.34s]  or the machine may end up reordering these writes
[2620.34s -> 2623.70s]  or the reading code, which reads these various things,
[2623.78s -> 2625.46s]  the compiler or the machine,
[2625.46s -> 2629.62s]  the microprocessor may end up reordering the reader's reads.
[2629.62s -> 2634.50s]  And of course, if we set E1 arrow next to point to E2
[2634.50s -> 2638.10s]  before we initialize the content of E2
[2638.10s -> 2640.82s]  so that its string it holds or its next pointer
[2642.02s -> 2642.98s]  point off into space,
[2643.54s -> 2645.06s]  then some reader is going to see this pointer
[2645.06s -> 2646.82s]  follow with read garbage and crash.
[2648.34s -> 2651.54s]  So the second idea is that both readers and writers
[2651.54s -> 2652.90s]  have to use memory barriers.
[2653.86s -> 2654.82s]  Even though we're not locking,
[2655.46s -> 2656.98s]  or really because we're not locking,
[2659.22s -> 2662.26s]  the writers and the readers have to use a barrier.
[2662.26s -> 2664.74s]  And for writers, the place the barrier has to go
[2664.74s -> 2666.34s]  is before the committing write.
[2667.06s -> 2668.50s]  So we need a barrier here
[2672.42s -> 2675.38s]  that tells the hardware and the compiler,
[2675.38s -> 2678.74s]  look, all the writes before this barrier,
[2678.74s -> 2682.90s]  please finish them before doing any writes after the barrier
[2682.90s -> 2687.38s]  so that E2 is fully initialized before we set E1 to point to it.
[2687.38s -> 2688.58s]  And on the read side,
[2691.30s -> 2695.46s]  the reader needs to load E1 arrow next
[2695.46s -> 2698.66s]  into some temporary location or register.
[2698.66s -> 2704.42s]  So we'll just say register one equals E1 arrow next.
[2708.18s -> 2709.70s]  Then the reader needs a barrier.
[2709.78s -> 2717.22s]  And then the reader is going to look at R1 arrow,
[2717.86s -> 2720.18s]  its content, and R1 arrow next.
[2720.98s -> 2723.06s]  And what this barrier on the reader says is,
[2724.58s -> 2726.66s]  don't issue any of these loads
[2727.94s -> 2730.66s]  until after we've completed this load.
[2730.66s -> 2734.58s]  So the reader is going to look at E1 arrow next
[2734.58s -> 2737.38s]  and either get the old E2 or the new E2,
[2738.02s -> 2741.62s]  and then the barrier says that only then
[2741.62s -> 2744.82s]  are we going to start looking at only after we've grabbed this,
[2747.62s -> 2750.58s]  all these reads have to execute after this read.
[2752.50s -> 2754.90s]  And since the writer guaranteed to initialize the content
[2755.78s -> 2759.46s]  before committing the pointer to the new E2,
[2760.10s -> 2761.46s]  that means these reads,
[2761.46s -> 2763.94s]  if this pointer points to the new E2,
[2763.94s -> 2766.66s]  that means these reads are guaranteed to see the initialized content.
[2768.34s -> 2775.06s]  Okay, so we saw a little bit of this.
[2775.06s -> 2775.94s]  Sorry, but how can you,
[2777.62s -> 2780.58s]  oh yeah, I was just, I was confused about the reader.
[2780.58s -> 2789.14s]  So how can you read R1, like anything before you read R1?
[2790.50s -> 2796.18s]  I guess like, how would they, so yeah,
[2796.18s -> 2799.54s]  I guess like if even if it reordered it,
[2799.54s -> 2805.86s]  how would it be able to read R1x before it read E1 next?
[2811.54s -> 2812.66s]  I think you've stumped me.
[2815.94s -> 2817.30s]  Yeah, I mean, what you're pointing out
[2817.30s -> 2819.14s]  is that before you even know what the pointer is,
[2820.18s -> 2822.26s]  you can't possibly actually issue the reads.
[2822.26s -> 2829.62s]  The possibility is that whatever this pointer points to,
[2829.62s -> 2831.78s]  maybe it's already cached on this core
[2832.34s -> 2833.38s]  due to something, maybe, you know,
[2833.38s -> 2836.42s]  this memory had been, you know, a minute ago
[2836.42s -> 2838.66s]  used for something else, something totally else.
[2838.66s -> 2841.78s]  And we have an old version of this cached on our core
[2843.86s -> 2845.70s]  at the address, at this address,
[2845.70s -> 2847.54s]  but for some previous use of the memory.
[2847.94s -> 2852.90s]  If this read was to use the old cached value,
[2853.86s -> 2854.90s]  I'm not sure this can happen,
[2855.46s -> 2856.82s]  I'm just making this up for you,
[2856.82s -> 2858.66s]  but if this read could use the old cached value,
[2858.66s -> 2859.70s]  then we'd be in big trouble.
[2862.10s -> 2864.74s]  And I don't know if the machine would actually do that
[2864.74s -> 2865.22s]  or whether,
[2870.18s -> 2873.86s]  another possibility is that the compiler, you know,
[2877.54s -> 2880.02s]  the real answer is I don't know.
[2880.74s -> 2882.26s]  I should go off and think about
[2882.26s -> 2883.70s]  what a specific example would be.
[2884.58s -> 2888.10s]  Okay, okay, I see the cached version makes sense, yes.
[2888.10s -> 2889.86s]  Yeah, I'm not actually completely sure
[2889.86s -> 2891.94s]  it could happen in real life.
[2895.22s -> 2896.02s]  That's a good question.
[2899.22s -> 2902.90s]  Okay, so that's the second idea.
[2902.90s -> 2904.10s]  The third problem we have,
[2904.10s -> 2906.10s]  which is something somebody raised before,
[2906.10s -> 2911.62s]  is that the writer is going to swap the E1 pointer
[2911.62s -> 2912.66s]  to point to the new E2,
[2912.66s -> 2915.54s]  but there could be readers, you know,
[2915.54s -> 2917.30s]  who started looking at, followed this pointer
[2917.30s -> 2919.14s]  just before the writer changed it,
[2919.14s -> 2920.50s]  who are still looking at E2.
[2922.18s -> 2924.18s]  We need to free this list element someday,
[2925.78s -> 2926.82s]  but we better not free it
[2926.82s -> 2928.02s]  while some reader's still using it.
[2928.02s -> 2929.14s]  So we need to somehow wait
[2929.14s -> 2931.46s]  until the last reader has finished using E2
[2931.46s -> 2932.42s]  before we can free it.
[2932.98s -> 2937.14s]  And that's the sort of third and final main problem
[2937.70s -> 2939.14s]  that RCU solves,
[2939.14s -> 2941.06s]  is how long should the writer wait
[2941.06s -> 2942.90s]  before it frees E2?
[2945.62s -> 2947.38s]  You could imagine a number of ways of doing this.
[2948.42s -> 2950.42s]  For example, we could put a little reference count
[2950.42s -> 2951.62s]  in every list element
[2951.62s -> 2953.06s]  and have readers increment it
[2953.06s -> 2954.66s]  and have readers wait.
[2954.66s -> 2957.30s]  Readers increment it when they start using a list element,
[2957.30s -> 2959.70s]  decrement it when they're done using the list element,
[2959.70s -> 2962.42s]  and have the writer wait for the reference count
[2962.42s -> 2963.70s]  on this element to go to zero.
[2964.42s -> 2965.78s]  We would forget that instantly
[2965.78s -> 2968.90s]  because the whole point of RCU
[2968.90s -> 2971.14s]  is to allow reading without writing,
[2972.10s -> 2974.34s]  because we know that if lots of readers
[2974.34s -> 2976.98s]  are changing this reference count,
[2976.98s -> 2978.26s]  it's gonna be terribly expensive
[2978.82s -> 2981.46s]  to do the writes involved in maintaining a reference count.
[2981.46s -> 2983.14s]  We absolutely don't want reference counts.
[2983.78s -> 2984.98s]  Another possibility would be
[2984.98s -> 2986.58s]  use a garbage-collected language.
[2986.58s -> 2990.58s]  And in the garbage-collected language,
[2990.58s -> 2992.34s]  you don't ever free anything explicitly.
[2992.34s -> 2995.38s]  Instead, the garbage collector does the bookkeeping required
[2995.94s -> 2998.66s]  to decide if any thread, for example,
[2998.66s -> 3002.90s]  or any data structure still has a reference to this element.
[3002.90s -> 3005.86s]  And the garbage collector, once it proves this element
[3005.86s -> 3008.02s]  can't possibly be ever used again,
[3008.02s -> 3010.34s]  only then will the garbage collector free this.
[3010.34s -> 3015.70s]  So that's another quite possibly reasonable scheme
[3015.70s -> 3017.54s]  for deciding when to free this list element.
[3018.90s -> 3020.82s]  Linux, which uses RCU,
[3020.82s -> 3022.58s]  is not written in a garbage-collected language.
[3023.94s -> 3025.14s]  And we're not even sure
[3025.14s -> 3027.94s]  the garbage collection would improve performance.
[3029.62s -> 3031.94s]  So we can't use a standard garbage collector here.
[3031.94s -> 3038.82s]  And instead, RCU uses another sort of a trick
[3039.38s -> 3042.74s]  that works well in the kernel for delaying frees.
[3046.10s -> 3055.22s]  And so that idea is that readers and writers
[3055.22s -> 3058.10s]  have to each follow a rule
[3058.10s -> 3060.90s]  that will allow writers to delay the frees.
[3062.34s -> 3065.86s]  Readers are not allowed to hold a pointer
[3065.86s -> 3068.90s]  to RCU-protected data across a context switch.
[3068.90s -> 3071.46s]  So a reader is not allowed to hold a pointer
[3072.42s -> 3077.22s]  to one of those list elements across a context switch.
[3077.78s -> 3084.18s]  So the readers, they cannot yield the CPU
[3087.38s -> 3089.06s]  in a RCU-critical section.
[3093.94s -> 3098.10s]  And then what the writers do is they delay the free
[3102.34s -> 3109.38s]  CPU until every core has context switches at least once.
[3116.90s -> 3118.18s]  So this is easy enough.
[3118.18s -> 3120.18s]  This is actually also a rule for spin locks.
[3120.18s -> 3121.46s]  In a spin lock-critical section,
[3121.46s -> 3122.74s]  you can't yield the CPU.
[3124.10s -> 3126.18s]  But nevertheless, you have to be a bit careful.
[3127.62s -> 3128.90s]  This is a little more involved,
[3129.54s -> 3133.86s]  but it's relatively clear when each core knows
[3133.86s -> 3134.74s]  it's context switching.
[3135.38s -> 3137.62s]  And so this is a pretty well-defined point
[3137.62s -> 3139.30s]  for the writer to have to wait for
[3141.38s -> 3143.22s]  and just require some implementation.
[3143.22s -> 3145.46s]  This may be a significant delay.
[3145.46s -> 3147.94s]  It may be a millisecond or a significant fraction
[3147.94s -> 3149.70s]  of a millisecond that the writer has to wait
[3150.26s -> 3152.58s]  before it's allowed to free that list element
[3152.58s -> 3155.06s]  to be sure that no reader could possibly still be using it.
[3155.06s -> 3162.02s]  People have come up with a bunch of techniques
[3162.02s -> 3164.18s]  for actually implementing this wait.
[3165.94s -> 3168.10s]  The straightforward one the paper talks about
[3168.10s -> 3171.06s]  is that the writing thread simply arranges
[3171.06s -> 3174.18s]  with the scheduler to have the writing thread
[3174.18s -> 3178.58s]  be executed briefly on every one of the cores in the system.
[3178.58s -> 3182.58s]  And what that means is that every one of the cores
[3182.58s -> 3185.54s]  must have done a context switch during this process.
[3186.58s -> 3189.06s]  And since readers can't hold stuff across context switches,
[3189.06s -> 3191.06s]  that means that the writer has now waited long enough.
[3196.98s -> 3199.94s]  And so the way the actual writer code looks like
[3199.94s -> 3202.90s]  is the writing code does whatever modifications
[3202.90s -> 3204.26s]  it's going to do to the data.
[3204.26s -> 3208.34s]  And then it calls this synchronized RCU call,
[3209.22s -> 3213.46s]  which actually implements two.
[3218.26s -> 3222.82s]  And then the writer frees whatever the old element was.
[3223.78s -> 3225.46s]  And so that means that the writer
[3225.46s -> 3227.70s]  is doing whatever it's doing at this point.
[3227.70s -> 3232.42s]  Let's say it's doing the E1 arrow next
[3232.42s -> 3238.34s]  is equal to the new list element.
[3240.90s -> 3250.50s]  And so this synchronized RCU forces a context switch
[3250.50s -> 3252.58s]  on every core.
[3253.38s -> 3259.78s]  So any core that could have read the old value
[3259.78s -> 3261.30s]  must have read it at this point.
[3262.42s -> 3266.26s]  Must have read it at this point in time.
[3266.26s -> 3267.54s]  If after that point in time,
[3267.54s -> 3269.30s]  we've done a context switch on every core,
[3269.30s -> 3271.94s]  that means that no core that read the old value
[3272.50s -> 3274.74s]  could still have a pointer to that value
[3275.30s -> 3278.10s]  at this point in time due to rule one.
[3278.10s -> 3280.02s]  And that means that we're allowed to free the old value.
[3283.70s -> 3284.42s]  Any questions?
[3284.42s -> 3294.50s]  You may object that this synchronized RCU
[3294.50s -> 3298.50s]  will take a significant, perhaps fraction of a millisecond.
[3298.50s -> 3299.30s]  That's quite true.
[3302.74s -> 3303.54s]  So that's too bad.
[3305.38s -> 3307.70s]  One of the justifications is that writing,
[3307.70s -> 3308.82s]  for RCU protected data,
[3308.82s -> 3310.66s]  writing is going to be relatively rare.
[3310.66s -> 3312.34s]  So the fact that the writes take longer
[3313.06s -> 3316.74s]  will probably not affect overall performance very much.
[3317.54s -> 3319.30s]  For the situations in which the writer
[3319.30s -> 3320.42s]  really doesn't want to wait,
[3320.42s -> 3325.62s]  there's another call that defers even the wait
[3326.74s -> 3328.02s]  called callRCU.
[3331.30s -> 3336.34s]  And the idea is you pass it in the usual use case.
[3336.98s -> 3339.78s]  You pass it a pointer to the object you want to free,
[3339.86s -> 3343.22s]  and then a callback function that just calls free
[3344.74s -> 3345.86s]  on this pointer.
[3345.86s -> 3348.66s]  And the RCU system basically stashes away,
[3348.66s -> 3353.54s]  the callRCU stashes away these two values on a list
[3353.54s -> 3354.98s]  and then immediately returns.
[3355.86s -> 3357.22s]  And then does some bookkeeping,
[3358.66s -> 3362.90s]  typically involving basically looking at the counts
[3362.90s -> 3365.30s]  of how many context switches have occurred on each core.
[3365.62s -> 3370.82s]  The system sort of in the background after callRCU returns
[3370.82s -> 3373.62s]  does some bookkeeping to wait until all cores
[3373.62s -> 3376.66s]  of context switch and then calls this callback function
[3376.66s -> 3377.78s]  with this argument.
[3377.78s -> 3380.10s]  And so this is a way of avoiding the wait
[3380.10s -> 3383.22s]  because this call returns instantly.
[3386.98s -> 3388.74s]  On the other hand, you're discouraged from using it
[3388.74s -> 3394.50s]  because now this list that if people,
[3394.50s -> 3396.42s]  if the kernel calls callRCU a lot,
[3397.06s -> 3401.54s]  then the list that holds these values can get very long.
[3402.10s -> 3404.98s]  And it means that there may be a lot of memory
[3404.98s -> 3406.10s]  that's not being freed.
[3409.06s -> 3410.42s]  This list goes very long.
[3410.42s -> 3416.10s]  Each list element has a pointer in it that should be freed,
[3416.10s -> 3417.78s]  a pointer to an object that should be freed.
[3418.42s -> 3420.02s]  And so under extreme circumstances,
[3420.02s -> 3422.18s]  you can run a system and if you're not careful,
[3422.18s -> 3425.54s]  a lot of calls to callRCU can run a system out of memory
[3425.54s -> 3428.50s]  because all the memory ends up on this list of deferred freeze.
[3430.82s -> 3433.22s]  So people don't like to use this if they don't have to.
[3437.86s -> 3438.36s]  Okay.
[3444.58s -> 3450.26s]  Please ask questions if you have questions.
[3450.26s -> 3456.26s]  So this prevents us from freeing something
[3456.26s -> 3457.70s]  that somebody's still using.
[3458.26s -> 3462.90s]  It doesn't prevent us from having the reader
[3462.90s -> 3465.54s]  see a half-baked version of something
[3465.54s -> 3467.70s]  because it's being modified, right?
[3467.70s -> 3469.30s]  Idea one prevented that, yeah.
[3471.22s -> 3471.78s]  Okay.
[3471.78s -> 3474.90s]  So the idea behind idea one is that
[3474.90s -> 3477.14s]  instead of updating a list element in place,
[3477.14s -> 3480.10s]  which would absolutely cause the problem you mentioned,
[3480.58s -> 3484.10s]  writers are not allowed to update RCU-protected data in place.
[3484.74s -> 3488.10s]  Instead, they cook up a new data element
[3488.66s -> 3490.66s]  and sort of swap it into the data structure
[3490.66s -> 3492.26s]  with a single committing write.
[3493.30s -> 3495.46s]  Oh, and the swapping will be atomic,
[3495.46s -> 3496.90s]  so there's no problem of like...
[3496.90s -> 3499.30s]  Right, because that's a single pointer write,
[3499.30s -> 3500.10s]  which is atomic,
[3500.10s -> 3503.14s]  whereas overwriting a string is completely not atomic.
[3504.50s -> 3505.14s]  That makes sense.
[3507.30s -> 3508.02s]  Other questions?
[3510.26s -> 3515.86s]  Does condition one in idea three mean we need to be careful
[3515.86s -> 3519.46s]  about how much work we put inside those protected sections
[3519.46s -> 3523.22s]  since it kind of hogs the core for that entire section?
[3524.42s -> 3525.38s]  Yes, yes.
[3525.38s -> 3526.10s]  So this is...
[3527.30s -> 3527.78s]  That's right.
[3527.78s -> 3531.30s]  So readers in the sort of RCU-critical section,
[3531.30s -> 3533.14s]  while they're looking at the protected data,
[3533.14s -> 3534.58s]  they can't context switch.
[3534.58s -> 3540.34s]  And so you want to keep those critical sections short.
[3541.14s -> 3544.34s]  Now, and that's a consideration.
[3546.02s -> 3547.46s]  The way it plays out, though,
[3548.02s -> 3550.42s]  is that the way RCU has been deployed
[3550.42s -> 3553.14s]  is typically that there'll be some piece of code in Linux
[3553.14s -> 3556.10s]  that was protected with ordinary locks or read-write locks.
[3556.66s -> 3560.42s]  And somebody for some workloads will see,
[3560.42s -> 3563.94s]  oh, that lock is a terrible performance problem,
[3563.94s -> 3567.70s]  and they're going to replace the locking critical section
[3567.70s -> 3569.38s]  with an RCU-critical section.
[3570.50s -> 3572.18s]  Although sometimes it's more involved than that.
[3573.14s -> 3575.78s]  And since locking critical sections were already,
[3575.78s -> 3578.10s]  it was extremely important to make them short.
[3578.10s -> 3579.54s]  Because while you hold a lock,
[3579.54s -> 3581.46s]  there may be lots of other cores waiting for that lock.
[3581.46s -> 3582.50s]  So there was a lot of pressure
[3582.50s -> 3585.06s]  to keep ordinary lock critical sections short.
[3586.02s -> 3588.74s]  Because RCU-critical sections are often sort of
[3588.74s -> 3591.22s]  revised lock critical things
[3591.22s -> 3592.90s]  that used to be lock critical sections.
[3592.90s -> 3594.26s]  They tend to be short also.
[3595.86s -> 3600.34s]  And that means that, not always,
[3600.34s -> 3606.26s]  but usually there's not a direct worry
[3606.26s -> 3608.18s]  about keeping the RCU-critical section short.
[3609.70s -> 3610.90s]  Although it is a constraint.
[3611.86s -> 3613.30s]  The real constraint actually is you're not allowed
[3613.30s -> 3617.86s]  to hold pointers to RCU data over context switches.
[3619.86s -> 3620.82s]  And that's actually a,
[3620.82s -> 3622.02s]  I mean, you can't, for example,
[3622.02s -> 3624.74s]  read the disk and wait for the disk read to complete
[3624.74s -> 3627.62s]  while holding on a pointer onto a pointer
[3627.62s -> 3629.14s]  to RCU-protected data.
[3630.90s -> 3632.42s]  So it's not quite so much the,
[3632.98s -> 3634.18s]  or the thing that usually comes up
[3634.18s -> 3635.78s]  is not the length of the critical section
[3635.78s -> 3639.62s]  so much as the prohibition against yielding CPU.
[3645.14s -> 3645.64s]  Okay.
[3649.70s -> 3650.10s]  Let's see.
[3650.10s -> 3652.82s]  So just to kind of firm up what I,
[3653.38s -> 3654.66s]  all the stuff I just talked about,
[3655.62s -> 3658.82s]  here's a kind of what you would see
[3658.82s -> 3662.26s]  in a simple use of RCU.
[3662.26s -> 3665.30s]  So this is code you might see for reading a list,
[3667.94s -> 3669.06s]  an RCU-protected list.
[3669.06s -> 3671.38s]  And this is the code you might see it on the right side
[3671.38s -> 3672.66s]  for code that just wants to,
[3673.86s -> 3676.98s]  the particular case of replacing the first list element.
[3676.98s -> 3677.94s]  So on the read side,
[3678.50s -> 3682.98s]  there is actually these read lock and read unlock calls.
[3683.94s -> 3686.66s]  Those do almost nothing, almost nothing.
[3689.30s -> 3692.98s]  The only little thing they do is set a flag that says,
[3692.98s -> 3695.14s]  or RCU read lock sets a flag that says,
[3695.86s -> 3697.54s]  if a timer interrupt happens,
[3697.54s -> 3699.06s]  please don't context switch
[3699.06s -> 3702.26s]  because I'm in the middle of a RCU-critical section.
[3702.26s -> 3703.94s]  So that's all it really does is set a flag
[3703.94s -> 3707.06s]  that prohibits timer interrupt context switches.
[3707.06s -> 3709.70s]  The interrupt may still happen, but it won't context switch.
[3709.70s -> 3710.66s]  And then read unlock,
[3711.22s -> 3712.34s]  unsets that flag.
[3712.34s -> 3716.74s]  Really, it's a counter of nested RCU-critical sections.
[3716.74s -> 3718.34s]  So these two functions are extremely fast
[3718.34s -> 3719.22s]  and do almost nothing.
[3720.98s -> 3727.14s]  And then this loop would sort of scan down our list.
[3727.70s -> 3732.26s]  This is the call that inserts the memory barrier.
[3732.26s -> 3735.06s]  So what RCU, this really boils down
[3735.06s -> 3737.38s]  to just a couple of instructions.
[3737.38s -> 3738.50s]  It just reads.
[3740.98s -> 3743.78s]  It grabs a copy of this pointer from memory,
[3743.78s -> 3747.22s]  issues a memory barrier, and then returns that pointer.
[3753.46s -> 3754.90s]  And then we can look at the content
[3755.54s -> 3756.98s]  and go on to the next list element.
[3758.10s -> 3761.70s]  So the reader's quite simple.
[3761.70s -> 3763.46s]  The writer's a little more involved.
[3763.54s -> 3767.94s]  Writers still, the RCU doesn't help writers
[3767.94s -> 3769.14s]  avoid interfering with each other.
[3769.14s -> 3770.58s]  So writers still have to have some way
[3770.58s -> 3772.26s]  of making sure only one writer
[3772.26s -> 3773.94s]  modifies the list at a time.
[3773.94s -> 3775.62s]  In this case, I'm just imagining
[3775.62s -> 3778.58s]  we're going to use ordinary spin locks
[3778.58s -> 3779.86s]  so the writer requires the lock.
[3780.98s -> 3782.98s]  If we're replacing the first list element,
[3783.94s -> 3786.66s]  we need to save a copy at the beginning
[3786.66s -> 3788.58s]  because we're going to need to eventually free it.
[3788.58s -> 3790.74s]  So we save this copy of the oldest element.
[3790.74s -> 3792.58s]  And now this code plays that trick
[3792.58s -> 3794.42s]  I talked again about allocating
[3794.42s -> 3796.02s]  a complete new list element to hold
[3796.02s -> 3798.82s]  this updated content.
[3799.54s -> 3801.30s]  So we're going to allocate a new list element.
[3801.30s -> 3802.90s]  We're going to set its content.
[3802.90s -> 3804.58s]  We're going to set the next pointer
[3806.26s -> 3809.22s]  to the next pointer in the old first list element
[3809.22s -> 3810.66s]  as we're replacing it.
[3810.66s -> 3815.78s]  And then this RCU assign pointer issues a memory barrier
[3816.42s -> 3818.66s]  so that all these writes happened
[3818.66s -> 3823.54s]  and then sets the pointer pointed to
[3823.54s -> 3825.22s]  by this first argument to be equal to that.
[3825.22s -> 3827.94s]  So basically, this just issues a memory barrier
[3827.94s -> 3829.62s]  and then sets head equal to E.
[3830.82s -> 3832.02s]  And now we can release the lock.
[3832.58s -> 3835.38s]  We still have a pointer to the old first list element
[3836.10s -> 3840.90s]  called synchronizeRCU to make sure every CPU
[3840.90s -> 3845.06s]  that could have grabbed a pointer to the old list element
[3845.06s -> 3847.30s]  before we did the committing write
[3847.38s -> 3850.02s]  has yielded the CPU and therefore given up
[3850.02s -> 3852.66s]  its pointer to RCU protected data.
[3852.66s -> 3855.78s]  And now we can free the old list element.
[3859.70s -> 3860.42s]  Any questions?
[3872.02s -> 3872.42s]  All right.
[3877.70s -> 3885.94s]  RCU, one thing to note about this is that while in the reader,
[3885.94s -> 3888.26s]  while we're allowed to look at this list element
[3888.26s -> 3891.70s]  inside the loop here, one thing we're not allowed to do
[3891.70s -> 3893.70s]  is return the list element.
[3893.70s -> 3898.02s]  So for example, using RCU, we couldn't write a lookup,
[3898.02s -> 3903.06s]  a list lookup function that returned either the list element
[3903.70s -> 3908.66s]  or a pointer into data held in the list element,
[3908.66s -> 3910.74s]  like a string that was embedded in the list element.
[3914.74s -> 3917.70s]  Because then we would no longer be in control.
[3917.70s -> 3919.30s]  You know, it has to be the case
[3919.30s -> 3921.46s]  that we don't look at RCU protected data
[3921.46s -> 3925.14s]  outside this RCU critical section,
[3925.14s -> 3926.74s]  or we don't do a context switch.
[3926.74s -> 3928.18s]  If we just write a generic function
[3928.18s -> 3930.42s]  that returns a list element, then for all we know,
[3930.42s -> 3933.38s]  the caller, maybe we can persuade the caller
[3933.38s -> 3934.34s]  to follow some rules, too.
[3934.34s -> 3940.26s]  But for all we know, the caller may context switch,
[3940.26s -> 3943.78s]  or we'd run into trouble.
[3943.78s -> 3946.74s]  Either we call RCU read unlock before returning the list
[3946.74s -> 3949.62s]  element, which is illegal because now a timer interrupt
[3949.62s -> 3952.90s]  could force a switch, or we don't call RCU read unlock.
[3952.90s -> 3957.38s]  So the use of RCU sort of does
[3957.38s -> 3959.14s]  put some additional constraints on readers
[3959.14s -> 3961.30s]  that wouldn't have existed before.
[3961.30s -> 3964.18s]  A question about that?
[3964.18s -> 3964.58s]  Yes.
[3964.58s -> 3968.90s]  So are you saying in particular that if we had some form
[3968.90s -> 3973.94s]  of like read element at index i method,
[3974.74s -> 3977.14s]  that there's no way to structure this
[3977.14s -> 3980.50s]  so that it could return the value held by the node
[3980.50s -> 3981.54s]  at element i?
[3981.54s -> 3982.58s]  It could return a copy.
[3984.18s -> 3986.74s]  So what would work, you know, if e arrow x is a string,
[3986.74s -> 3988.34s]  we could return a copy of this string,
[3988.34s -> 3989.06s]  and that's fine.
[3990.42s -> 3992.82s]  What would be a violation of the RCU rules
[3992.82s -> 3996.34s]  is if we returned a pointer to this very string
[3996.34s -> 3997.06s]  sitting inside.
[4001.14s -> 4004.02s]  It would be a mistake to return a pointer into e,
[4004.02s -> 4004.90s]  somewhere into e.
[4007.06s -> 4009.38s]  Like if the string is stored inside the list element,
[4009.38s -> 4011.30s]  we better not return this pointer to that string,
[4012.18s -> 4020.34s]  because then we have to not context
[4020.34s -> 4023.38s]  switch while we're holding a pointer into RCU protected
[4023.38s -> 4024.18s]  data.
[4024.18s -> 4027.38s]  And the convention is, you know,
[4027.38s -> 4029.54s]  you just use that data within this critical section.
[4030.58s -> 4032.58s]  And so it would almost certainly be breaking the convention,
[4032.58s -> 4035.06s]  or this setup would have to be much more complicated
[4035.06s -> 4036.90s]  if we ended up returning pointers
[4036.90s -> 4038.50s]  into the protected data.
[4038.74s -> 4040.74s]  Thank you.
[4046.34s -> 4048.66s]  So I just want to sort of return briefly
[4048.66s -> 4049.78s]  to the performance story.
[4052.58s -> 4057.86s]  It's hard to characterize sort of what the performance is.
[4057.86s -> 4062.50s]  I mean, in a sense, let's see,
[4062.50s -> 4065.70s]  the overall performance story is that if you use RCU,
[4065.70s -> 4066.82s]  reads are extremely fast.
[4066.82s -> 4069.14s]  They just proceed at, you know, whatever.
[4069.14s -> 4073.06s]  They have sort of no overhead above the ordinary overhead
[4073.06s -> 4074.18s]  of looking at that data.
[4074.18s -> 4077.06s]  So if your list is a billion elements long,
[4077.06s -> 4079.06s]  yeah, reading the list will take a long time.
[4079.06s -> 4081.30s]  But it's not because of synchronization.
[4081.30s -> 4084.74s]  It's just because you're doing a lot of work for readers.
[4085.78s -> 4091.14s]  So you can almost view RCU as having zero overhead for readers.
[4091.14s -> 4095.54s]  And the exceptions are minor.
[4095.54s -> 4098.82s]  RCU read lock, you know, does a tiny amount of work
[4098.82s -> 4101.62s]  to set this flag saying no context switches.
[4101.62s -> 4104.34s]  And RCU dereference issues a memory barrier,
[4104.34s -> 4109.14s]  which actually might slow you down by dozens,
[4109.14s -> 4116.02s]  a few dozen cycles, but it much cheaper than a lot.
[4117.54s -> 4120.18s]  The performance story for writers is much sadder.
[4120.18s -> 4122.90s]  You had to do all the stuff you always had to do using locks.
[4122.90s -> 4125.38s]  In fact, you have to acquire and release locks.
[4125.94s -> 4130.10s]  And you have this potentially extremely expensive call,
[4130.10s -> 4132.90s]  or time consuming call to synchronize RCU.
[4132.90s -> 4134.18s]  In fact, you can give up, you know,
[4134.18s -> 4137.22s]  internally synchronize RCU gives up the CPU.
[4137.22s -> 4139.62s]  So it doesn't spin necessarily.
[4141.70s -> 4144.50s]  But it may require a lot of elapsed time waiting
[4144.50s -> 4146.66s]  for every other core to context switch.
[4148.82s -> 4151.06s]  So depending on the mix of reads and writes
[4151.86s -> 4154.34s]  and how much work was being done
[4155.14s -> 4156.66s]  inside the read critical section,
[4157.30s -> 4160.74s]  the performance increase varies tremendously
[4160.74s -> 4166.10s]  from much, much faster if these critical sections were short
[4166.10s -> 4169.06s]  and there's few writes to perhaps even slower
[4170.50s -> 4171.70s]  if writes are very common.
[4173.78s -> 4176.90s]  And so when people apply RCU to kernel stuff,
[4177.06s -> 4181.30s]  actually, you absolutely have to do performance tests
[4181.30s -> 4183.22s]  against a bunch of workloads in order to figure out
[4183.22s -> 4185.94s]  whether using RCU is a win for you
[4185.94s -> 4188.26s]  because it's so dependent on the workload.
[4191.78s -> 4194.74s]  I have maybe a tangential question.
[4194.74s -> 4198.42s]  But we've seen that, I guess,
[4198.42s -> 4200.42s]  when there's multiple cores being used,
[4200.42s -> 4205.54s]  there's some added complexity to our usual implementations.
[4205.54s -> 4208.58s]  And it's often the like these atomic instructions
[4209.30s -> 4210.82s]  kind of come to the rescue.
[4211.62s -> 4214.58s]  And that's assuming there's one shared memory system.
[4214.58s -> 4218.42s]  But I wonder, like, what happens if a machine
[4218.42s -> 4222.50s]  is trying to maintain like multiple RAM systems?
[4222.50s -> 4224.34s]  How does it unify those?
[4228.74s -> 4229.46s]  The ordinary,
[4229.78s -> 4233.14s]  well,
[4238.50s -> 4240.02s]  at the level we're talking about,
[4240.02s -> 4241.54s]  the machine has one RAM system.
[4243.54s -> 4251.06s]  Okay, for all those sort of ordinary computers
[4251.06s -> 4252.98s]  you would buy that have multiple cores,
[4253.62s -> 4255.70s]  you can pretty much program them
[4255.70s -> 4257.46s]  as if there were just one RAM system
[4257.46s -> 4258.74s]  shared among all the cores.
[4258.82s -> 4261.54s]  That's the logical model the hardware provides you.
[4262.26s -> 4264.82s]  At a physical level, it's not like that often.
[4266.10s -> 4267.46s]  There's plenty of machines out there
[4267.46s -> 4270.18s]  that have this physical arrangement.
[4270.18s -> 4272.02s]  We have a CPU chip.
[4272.74s -> 4277.14s]  So here's one CPU chip, maybe with lots of cores on it.
[4278.66s -> 4280.98s]  And you can get CPU chips with,
[4280.98s -> 4283.06s]  I don't know how many cores these days, 32 cores.
[4283.06s -> 4284.98s]  Let's say you want to build a 64-core machine,
[4284.98s -> 4287.46s]  you can only buy 32-core chips.
[4287.46s -> 4289.06s]  Well, you can make a board.
[4289.70s -> 4291.54s]  I think it has two sockets for chips on it.
[4291.54s -> 4292.66s]  So now we have two chips.
[4294.82s -> 4296.66s]  The fastest way to get at memory
[4296.66s -> 4298.10s]  is to have the memory more or less
[4298.10s -> 4301.38s]  as directly attached to the CPU chip as possible.
[4301.38s -> 4302.66s]  So what you would do is you'd have, like,
[4303.30s -> 4305.94s]  a very fat set of wires here
[4306.66s -> 4309.86s]  to right next to the chip, a bunch of RAM.
[4311.54s -> 4312.82s]  So it has direct access.
[4312.82s -> 4315.06s]  And of course, this chip's going to want its own RAM also.
[4316.02s -> 4318.98s]  So I'm just drawing a picture of what you would see
[4318.98s -> 4323.38s]  if you opened up a PC with two processor chips in it, RAM.
[4326.82s -> 4328.02s]  But now we're faced with the problem
[4328.02s -> 4330.74s]  of what happens if a software over on this chip
[4330.74s -> 4333.30s]  uses a memory location that's actually stored in this RAM.
[4334.18s -> 4336.90s]  So in fact, there's also an interconnect
[4337.78s -> 4338.98s]  between these two chips,
[4340.02s -> 4342.18s]  generally an extremely fast interconnect,
[4342.18s -> 4344.18s]  like gigabytes per second.
[4345.46s -> 4346.82s]  And the chips are smart enough to know
[4346.82s -> 4348.42s]  that certain physical memory locations
[4348.42s -> 4349.86s]  are in this bank of RAM,
[4349.86s -> 4351.38s]  and other physical locations,
[4351.38s -> 4353.30s]  physical memory addresses are in this bank of RAM.
[4353.30s -> 4357.30s]  And if software here uses physical addresses
[4357.30s -> 4359.06s]  over in this one, the chip is clever enough
[4359.06s -> 4360.10s]  to send a message.
[4360.10s -> 4361.62s]  It's basically a little network.
[4361.62s -> 4363.06s]  Send a message over this chip,
[4363.06s -> 4364.66s]  telling it, look, I need to read some RAM.
[4364.66s -> 4366.58s]  Please do it and go read its RAM
[4366.58s -> 4367.54s]  and send the result back.
[4368.26s -> 4370.10s]  You know, you can buy four chip arrangements
[4370.10s -> 4370.90s]  with the same thing,
[4370.90s -> 4373.70s]  with a complex interconnect like this.
[4373.70s -> 4376.66s]  So there's a huge amount of engineering going on
[4376.66s -> 4380.98s]  in order to map the straightforward shared RAM model
[4380.98s -> 4382.42s]  onto what sort of feasible to build
[4382.42s -> 4385.22s]  with high performance in real life
[4385.22s -> 4387.62s]  and fit in two or three dimensions.
[4391.06s -> 4391.94s]  Does that answer your question?
[4391.94s -> 4394.82s]  Yeah, that provides a lot of context.
[4394.82s -> 4395.14s]  Thank you.
[4398.18s -> 4398.42s]  Okay.
[4403.70s -> 4408.90s]  All right.
[4408.90s -> 4410.66s]  And any questions on the actual technique?
[4416.26s -> 4416.58s]  All right.
[4416.58s -> 4420.50s]  So I'm sure you've gotten the sense
[4421.06s -> 4423.78s]  of RCU is not universally applicable.
[4423.78s -> 4426.18s]  There's not, you can't just take every situation
[4426.18s -> 4427.38s]  in which you're using spin locks
[4427.38s -> 4429.38s]  and getting bad parallel performance
[4429.38s -> 4432.10s]  and convert it to RCU and get better performance.
[4432.10s -> 4435.62s]  Because the main reason is it completely doesn't help writes,
[4435.62s -> 4436.42s]  makes them slower.
[4436.98s -> 4439.14s]  Now it really only helps performance
[4439.14s -> 4442.18s]  if the reads outnumber the writes considerably.
[4444.18s -> 4447.38s]  It has this restriction that you can't hold pointers
[4447.38s -> 4449.30s]  to protect the data across sleep,
[4449.30s -> 4452.10s]  which just makes some kind of code quite awkward.
[4452.10s -> 4453.70s]  If you actually need to sleep,
[4453.70s -> 4456.18s]  you may then need to re-look up whatever it is,
[4457.38s -> 4459.38s]  you know, to do another RCU critical section
[4459.38s -> 4460.98s]  after the sleep completes
[4460.98s -> 4464.90s]  in order to look again for the data
[4464.90s -> 4466.42s]  that you originally were looking at,
[4466.42s -> 4468.18s]  assuming it still even exists.
[4468.18s -> 4470.02s]  So it just makes code a bit more complicated.
[4472.42s -> 4473.46s]  The data structures,
[4474.26s -> 4475.78s]  the most straightforward way to apply it
[4475.78s -> 4478.98s]  is the data structures that have a structure
[4478.98s -> 4482.74s]  that's amenable to single committing writes for updates.
[4482.74s -> 4484.02s]  You can't modify things in place,
[4484.02s -> 4485.86s]  so you have to replace stuff.
[4486.66s -> 4489.94s]  And so, you know, lists and trees,
[4489.94s -> 4492.58s]  but not more complex data structures.
[4492.58s -> 4496.34s]  The paper mentioned some more complicated ways
[4496.34s -> 4501.06s]  like sequence locks to be able to update stuff in place,
[4501.06s -> 4504.02s]  you know, despite readers that aren't using locks,
[4504.02s -> 4505.62s]  but they're a good deal more complicated
[4505.62s -> 4507.54s]  and the situations under which
[4507.54s -> 4510.82s]  they actually improve performance are more restricted.
[4511.70s -> 4517.78s]  Another subtle problem is that readers can see stale data
[4520.34s -> 4521.94s]  without any obvious bound
[4521.94s -> 4524.26s]  on how long they can see the stale data for.
[4524.26s -> 4527.22s]  Because if some reader gets a pointer
[4527.22s -> 4528.98s]  to an RCU protected object
[4529.94s -> 4532.66s]  just before a writer replaces it,
[4534.02s -> 4536.42s]  the reader may still hold onto that data
[4536.98s -> 4537.86s]  for quite a long time,
[4537.94s -> 4541.54s]  at least on the scale of modern computer instructions.
[4543.30s -> 4546.18s]  And a lot of the time this turns out not to matter much,
[4547.46s -> 4549.30s]  but the paper mentioned some situations
[4549.30s -> 4550.82s]  which I actually don't really understand
[4551.54s -> 4557.14s]  in which people expect writes to actually take effect
[4557.78s -> 4558.98s]  after the write completes
[4559.62s -> 4562.34s]  and therefore in which readers seeing stale data
[4562.34s -> 4563.46s]  is a bit of a surprise.
[4568.58s -> 4574.42s]  You may also, as a separate topic,
[4574.42s -> 4577.54s]  wonder what happens if you have write heavy data.
[4577.54s -> 4579.06s]  Like RCU is all about read heavy data,
[4579.06s -> 4581.70s]  but that's just one of many situations
[4581.70s -> 4584.58s]  you might care about for getting parallel performance.
[4585.94s -> 4587.46s]  You also care about write heavy data.
[4588.18s -> 4590.50s]  Actually in the extremes,
[4590.50s -> 4592.66s]  in some extreme cases of write heavy data,
[4592.66s -> 4593.70s]  you can do quite well.
[4593.70s -> 4596.98s]  There's no technique I know of for write heavy data
[4596.98s -> 4600.26s]  that's quite as universally applicable as RCU,
[4601.70s -> 4603.94s]  but there are still ideas for coping
[4603.94s -> 4606.02s]  with data that's mostly written.
[4606.02s -> 4608.82s]  So the most powerful idea is to restructure your data,
[4609.54s -> 4612.50s]  restructure the data structure so it's not shared.
[4612.50s -> 4613.54s]  And sometimes you can do that.
[4613.54s -> 4615.94s]  Sometimes the sharing is just completely gratuitous
[4616.66s -> 4619.22s]  and you can get rid of it once you realize it's a problem.
[4620.26s -> 4621.86s]  But it's also often the case
[4621.86s -> 4625.94s]  that while you do sometimes need to have shared data,
[4625.94s -> 4629.78s]  that the common case doesn't require different cores
[4629.78s -> 4631.22s]  to write the same data,
[4631.22s -> 4634.34s]  even though they need to write some of the data a lot.
[4634.34s -> 4636.66s]  And so you've actually seen that in the labs.
[4636.66s -> 4642.42s]  In the locking lab, in the Kalloc part of the lab,
[4642.42s -> 4644.02s]  you restructured the free list
[4644.02s -> 4646.42s]  so that each core has a dedicated free list,
[4646.42s -> 4648.74s]  thus converting a write heavy data structure,
[4648.74s -> 4652.66s]  the free list, into one that was sort of
[4652.66s -> 4654.26s]  semi-private per core.
[4654.26s -> 4656.34s]  So most of the times cores just have to,
[4657.38s -> 4658.82s]  don't conflict with other cores
[4658.82s -> 4660.82s]  because they have their own private free list
[4660.82s -> 4662.82s]  and the only time you have to look at other free lists
[4662.82s -> 4664.90s]  is if your free list runs out.
[4664.90s -> 4667.38s]  So there's actually many examples of this way
[4667.38s -> 4671.22s]  of dealing with write heavy data in the kernel.
[4672.10s -> 4674.58s]  I'm thinking of the allocator and Linux is like this.
[4674.58s -> 4676.34s]  Linux is scheduling lists.
[4678.58s -> 4681.70s]  There's a sort of separate set of threads for each core
[4681.70s -> 4683.86s]  that the scheduler looks at most of the time
[4684.02s -> 4688.74s]  and cores only have to look at each other's scheduling lists
[4688.74s -> 4689.94s]  if they run out of work to do.
[4690.82s -> 4692.82s]  Another example is statistics counters.
[4692.82s -> 4696.82s]  If you're counting something and the counts go change a lot
[4696.82s -> 4698.74s]  but they're rarely read,
[4698.74s -> 4702.18s]  that is the counter truly dominated by writes and not reads,
[4703.38s -> 4704.74s]  you can restructure your counter
[4704.74s -> 4709.14s]  so that each core has a separate counter.
[4710.90s -> 4712.82s]  And so each core just modifies its own counter
[4712.82s -> 4715.22s]  when it needs to change the count.
[4715.22s -> 4716.90s]  And if you want to read something,
[4716.90s -> 4719.62s]  then you have to go out and lock and read
[4719.62s -> 4721.14s]  all the per core counters.
[4721.78s -> 4724.02s]  So that's a technique to make writes very fast
[4724.58s -> 4727.30s]  because the writers just modify the local per core counter
[4728.18s -> 4729.54s]  but the reads are now very slow.
[4730.90s -> 4733.22s]  But if your counters are write heavy
[4733.22s -> 4735.14s]  as statistics counters often are,
[4736.34s -> 4738.82s]  that could be a big win shifting the work now to the reads.
[4741.06s -> 4742.42s]  So the point is there are techniques
[4742.42s -> 4743.62s]  even though we didn't talk about them much,
[4743.62s -> 4746.26s]  there are also sometimes techniques that help
[4747.30s -> 4748.66s]  for write-intensive workflows.
[4752.26s -> 4756.50s]  To wrap up, the RCU, the stuff we read about in the paper
[4756.50s -> 4758.82s]  is actually a giant success story for Linux.
[4759.62s -> 4761.54s]  It's used all over Linux
[4761.54s -> 4763.46s]  to get at all kinds of different data
[4763.46s -> 4767.86s]  because it just turns out that read and read mostly data,
[4767.86s -> 4769.78s]  read-intensive data is extremely common
[4769.78s -> 4773.46s]  like cached file blocks, for example, are mostly read.
[4774.82s -> 4777.06s]  So a technique that speeds up only reads
[4777.06s -> 4780.18s]  is really very widely applicable.
[4782.02s -> 4783.86s]  And RCU is particularly magic.
[4784.90s -> 4788.02s]  Lots of other interesting concurrency techniques,
[4788.90s -> 4790.02s]  synchronization techniques.
[4790.02s -> 4793.70s]  RCU is magic because it completely eliminates
[4793.70s -> 4795.86s]  locking and writing for the readers.
[4795.86s -> 4798.58s]  And so that's just like a big breakthrough
[4798.58s -> 4800.34s]  compared to things like read-write locks,
[4800.34s -> 4804.02s]  which were the previous state of the art.
[4805.30s -> 4807.54s]  And the key idea that really makes it work
[4807.54s -> 4813.22s]  is the sort of garbage collection-like deferring of frees
[4813.22s -> 4815.54s]  for what they call the grace period
[4815.54s -> 4817.86s]  until all the readers are guaranteed to be finished
[4817.86s -> 4818.90s]  using the data.
[4819.46s -> 4822.26s]  So you can, as well as a synchronization technique,
[4822.26s -> 4825.46s]  it's actually fair to view it as very much so
[4825.46s -> 4828.50s]  as a kind of specialized garbage collection technique.
[4831.62s -> 4833.22s]  And that is all I have to say.
[4833.22s -> 4835.46s]  So I'm happy to take questions.
[4839.78s -> 4840.34s]  Oh, sorry.
[4840.34s -> 4845.14s]  Can you explain the stale data for readers?
[4845.70s -> 4849.38s]  So I don't understand how that can happen
[4849.38s -> 4853.70s]  because you're reading your critical section
[4854.58s -> 4859.30s]  and you just get whatever data is there at that point
[4860.34s -> 4863.86s]  and then you just leave.
[4865.22s -> 4866.74s]  It actually usually is not a problem.
[4867.78s -> 4871.14s]  But the reason why it ever might come up,
[4872.02s -> 4878.26s]  well, ordinarily, if you have code that says x equals 1
[4879.22s -> 4887.30s]  and then you print done, gosh, it's pretty surprising
[4887.30s -> 4890.98s]  if after this point, someone reading the data
[4891.62s -> 4895.86s]  sees that value before you set it to 1, right?
[4895.86s -> 4898.50s]  That's maybe a bit of a surprise, right?
[4899.46s -> 4902.74s]  Well, there's a sense in which RCU allows that to happen.
[4902.74s -> 4902.98s]  Right?
[4903.06s -> 4912.02s]  If what we're really talking about is list, replace,
[4913.70s -> 4916.02s]  whatever, find the element that has 1 in it
[4916.02s -> 4920.26s]  and change it to 2 using RCU, right?
[4921.14s -> 4923.14s]  After that finishes and we print, oh, yeah, we're done,
[4924.18s -> 4928.42s]  if there's some reader that was looking at the list,
[4928.42s -> 4932.10s]  right, they may have just gotten to the list element
[4932.10s -> 4934.26s]  that held 1 that we replaced with 2
[4934.26s -> 4935.54s]  and then a good deal longer.
[4935.54s -> 4937.70s]  You know, and then they do the actual read
[4938.74s -> 4940.74s]  of the list element.
[4940.74s -> 4944.50s]  You know, they look at whatever the content is
[4944.50s -> 4946.18s]  in the list element after we've done this.
[4946.74s -> 4948.98s]  You know, they're reading the list element
[4948.98s -> 4950.66s]  only at this point later in time
[4950.66s -> 4952.18s]  and they see the old value.
[4955.94s -> 4957.38s]  So if you're not prepared for this,
[4957.38s -> 4959.06s]  so this is like a little bit odd now.
[4962.26s -> 4965.38s]  I mean, they may even do a memory barrier, right?
[4965.38s -> 4966.98s]  I mean, it's not a memory barrier issue.
[4966.98s -> 4971.78s]  It's just like, and indeed, most of the time
[4971.78s -> 4972.50s]  it doesn't matter.
[4974.50s -> 4974.90s]  I see.
[4974.90s -> 4980.02s]  So this is when this replace is very close.
[4980.82s -> 4985.14s]  So like the read somehow like starts before the replace,
[4985.14s -> 4988.42s]  but it just takes a while.
[4988.74s -> 4991.78s]  Yes, yeah, if the reader is slower
[4991.78s -> 4993.38s]  than the writer or something.
[4993.38s -> 4998.34s]  Now, you know, I think this mostly doesn't matter
[4999.06s -> 5001.94s]  because after all, the reader and the writer
[5001.94s -> 5003.22s]  were acting concurrently
[5003.22s -> 5005.70s]  and, you know, if two things happen concurrently,
[5006.26s -> 5010.02s]  usually you would never have imagined
[5010.02s -> 5011.62s]  that you could have been guaranteed much
[5011.62s -> 5013.78s]  about the exact order
[5014.34s -> 5016.50s]  if the two operations were invoked concurrently.
[5017.38s -> 5018.34s]  I see.
[5018.34s -> 5022.26s]  The paper claims, I mean, the paper has an example
[5022.26s -> 5023.86s]  in which they said it matters.
[5023.86s -> 5025.94s]  It turned out to cause a real problem,
[5025.94s -> 5030.58s]  although I don't really understand why that was.
[5031.86s -> 5032.34s]  I see.
[5032.34s -> 5033.38s]  This makes sense.
[5033.38s -> 5036.42s]  And my other question was it's called RCU
[5036.42s -> 5038.10s]  because of idea one.
[5038.10s -> 5039.22s]  Is that right?
[5039.22s -> 5040.50s]  Read, copy, update.
[5041.14s -> 5044.02s]  Yes, I believe it's because of idea one.
[5044.02s -> 5047.46s]  That is that instead of modifying things in place,
[5048.10s -> 5053.78s]  you make a copy and you sort of copy,
[5053.78s -> 5055.94s]  not the real thing.
[5056.90s -> 5058.02s]  Right, this makes sense.
[5058.02s -> 5058.74s]  Thank you so much.
[5060.82s -> 5061.32s]  Yes.
[5062.58s -> 5064.82s]  So at the beginning of lecture or towards the beginning,
[5064.82s -> 5067.38s]  we talked about the O of N squared runtime
[5067.38s -> 5069.62s]  for the cache coherence protocols
[5071.30s -> 5072.66s]  for updating the read-write locks.
[5072.98s -> 5076.58s]  Isn't this also a problem with spin locks where...
[5078.82s -> 5079.70s]  Yeah, OK.
[5079.70s -> 5084.66s]  So what is the reason why we didn't discuss that aspect?
[5085.70s -> 5086.42s]  Why we didn't?
[5087.06s -> 5090.66s]  Yeah, or is there a reason that that still exists
[5090.66s -> 5093.22s]  or what do spin locks do to address that?
[5093.22s -> 5093.72s]  Nothing.
[5094.26s -> 5095.06s]  Oh, OK.
[5095.06s -> 5096.58s]  Spin locks are hideously expensive.
[5098.18s -> 5101.86s]  Or standard spin locks like xv6 has are extremely fast
[5102.42s -> 5104.82s]  if the lock is not particularly contended
[5105.78s -> 5108.82s]  and terribly slow if lots of cores
[5108.82s -> 5110.18s]  try to get the same lock at the same time.
[5110.90s -> 5111.78s]  Gotcha, OK.
[5111.78s -> 5112.82s]  Yeah, this is one of the things
[5112.82s -> 5113.70s]  that makes life interesting.
[5113.70s -> 5122.42s]  And there's locks that have better scaling
[5122.98s -> 5127.38s]  but worse, that have better high load performance
[5127.38s -> 5128.90s]  but worse load performance.
[5129.70s -> 5130.20s]  OK.
[5131.46s -> 5133.06s]  But I'm not aware of a lock that is...
[5133.94s -> 5136.34s]  Anyway, it's hard to get this stuff right.
[5136.34s -> 5139.62s]  It's hard to get good performance in these machines.
[5147.30s -> 5148.10s]  Other questions?
[5151.62s -> 5155.70s]  This might be unrelated, but can there ever be lockings
[5155.70s -> 5157.78s]  between multiple different systems?
[5158.50s -> 5163.62s]  Like not just contained to one system,
[5163.62s -> 5165.86s]  maybe like multiple servers perhaps?
[5167.38s -> 5169.38s]  There are absolutely distributed systems
[5169.38s -> 5171.94s]  in which there's a sort of locking,
[5175.22s -> 5178.58s]  in which the universe of locks spans multiple machines.
[5179.78s -> 5182.02s]  One place this comes up is in distributed databases
[5182.02s -> 5186.58s]  where you split your data over multiple servers.
[5186.58s -> 5188.42s]  But if you want to have a transaction
[5188.42s -> 5190.98s]  that uses data that's in different pieces
[5190.98s -> 5192.34s]  of the data on different servers,
[5192.34s -> 5193.94s]  you're going to need to collect locks.
[5196.82s -> 5199.54s]  You need to basically collect locks from multiple servers.
[5200.74s -> 5206.50s]  Another place it comes up, although there's
[5206.50s -> 5208.34s]  been a number of systems that essentially
[5208.34s -> 5212.82s]  try to mimic shared memory across independent machines.
[5213.46s -> 5217.30s]  With the machines, if I use the memory that's in your machine,
[5217.30s -> 5219.38s]  then there's some infrastructure stuff
[5219.38s -> 5221.22s]  that causes my machine to talk to your machine
[5221.22s -> 5222.10s]  and ask for the memory.
[5226.82s -> 5230.10s]  And the game is usually to run existing parallel programs
[5230.10s -> 5232.10s]  on a cluster of workstations instead of
[5232.10s -> 5235.54s]  on a big multi-core machine, hoping
[5235.54s -> 5236.82s]  it's going to be cheaper.
[5236.82s -> 5239.14s]  And something needs to be done about spin locks there,
[5239.14s -> 5240.58s]  or whatever locking you're going to use.
[5240.58s -> 5242.18s]  And so people have invented various ways
[5242.18s -> 5245.06s]  to make the locking work out well in that case, too.
[5247.22s -> 5250.90s]  Using things that are often not quite the same as this.
[5252.18s -> 5257.22s]  Although the pressure to avoid costs
[5257.22s -> 5258.42s]  is even higher in that case.
[5271.14s -> 5271.78s]  Anything else?
[5272.18s -> 5277.78s]  Thank you.
[5277.78s -> 5279.78s]  You're welcome.
