# Detected language: en (p=1.00)

[0.00s -> 10.96s]  All right. Hello, everyone. Welcome back from Thanksgiving. Can anyone hear me?
[10.96s -> 15.32s]  Yep, you're good. Good. All right. Today I want to talk about
[15.32s -> 27.12s]  networking and how it relates to operating systems. And a lot of this is geared towards
[27.12s -> 34.84s]  the last lab, in which you'll actually build some network interface driver. Some of it's
[34.84s -> 41.32s]  just geared for general understanding of how the network software typically is set up in
[41.32s -> 46.60s]  operating systems. And then we're going to talk about today's paper on LiveLook, which
[46.60s -> 53.24s]  illustrates an interesting danger in networking stack design.
[53.24s -> 62.48s]  So first, let me set the general scene by drawing a few network pictures. The network, of course,
[62.48s -> 71.40s]  connects up different hosts. There's kind of two ways in which you can view the connections
[71.40s -> 78.72s]  as occurring. One is that for nearby hosts, they're often connected to what's essentially
[79.20s -> 85.84s]  the same network. So there may be a single ethernet, and maybe this is a switch or a cable,
[85.84s -> 94.00s]  and you might have a bunch of hosts connected to this ethernet, where hosts are maybe laptops
[94.00s -> 105.04s]  or servers, or as it will turn out, routers. And the way network software is designed
[105.04s -> 110.64s]  is to kind of try to ignore as much as possible the details of exactly what this network
[110.64s -> 117.68s]  is that directly attaches hosts. It might be a single cable, which is probably the case
[117.68s -> 122.24s]  at the time today's paper was written. This might be an ethernet switch. This might be some
[122.24s -> 129.20s]  sort of Wi-Fi wireless LAN, and these things aren't wires at all, but rather radio links.
[129.20s -> 135.84s]  But for the most part, these differences in sort of exactly what the local connectivity is
[135.84s -> 140.80s]  are kind of totally papered over at a pretty low level in the networking stack.
[143.44s -> 149.28s]  So on each of these hosts, there may be different applications. Maybe there's a web browser here
[149.28s -> 156.80s]  and an HTTP server over here, and they need to talk to each other across this network.
[157.76s -> 165.20s]  Now, there's a limit to how big you can build a single local area network, for which the
[165.20s -> 172.48s]  abbreviation is usually LAN, local area network. And the way to think about it maybe is that a
[172.48s -> 177.20s]  local area network can be as large as the network in which it makes sense for all the
[177.20s -> 183.68s]  hosts to be able to see all of each other's packets. That is, sometimes hosts need to want
[183.68s -> 190.40s]  to broadcast to all of the local hosts. That works fine with a dozen or 20 or 50 or maybe
[190.40s -> 196.32s]  even 100 hosts, but you can't really easily build single networks where all the hosts can
[196.32s -> 201.44s]  more or less directly talk to each other with more than, say, a few hundred hosts.
[203.28s -> 209.04s]  And so to deal with that, the way the larger internet is constructed is that there's a number
[209.04s -> 222.24s]  of these individual LANs, maybe one at MIT, maybe one at Harvard, maybe one far away at Stanford,
[223.36s -> 229.44s]  and there's some sort of connectivity between them, which you can think of as routers. So there
[229.44s -> 235.92s]  might be a router that's plugged into the MIT local area network and also has perhaps a longer
[235.92s -> 244.40s]  link to the Harvard network. And in fact, there's a network of routers, which is essentially the
[244.40s -> 250.64s]  backbone of the internet, including long distance router to router links. So there might be a longer
[250.64s -> 256.72s]  link across the country and maybe this router is plugged into some local area network at
[256.72s -> 263.36s]  Stanford. And then we have hosts, we have the sort of more elaborate task in which we want to
[263.36s -> 269.36s]  host at MIT to be able to talk through a sequence of routers to a host at Stanford. And this is
[269.36s -> 275.04s]  called routing, so we need to have a way for hosts at MIT to address, to name
[275.60s -> 283.52s]  individual hosts at Stanford, and we need some way so that routers near MIT can look at
[283.52s -> 289.04s]  a packet sent by MIT and say, oh, that's a packet for Harvard or a packet for Stanford or
[289.04s -> 296.48s]  a packet that needs to go somewhere in Japan or who knows what. So from the point of view of
[296.48s -> 306.08s]  network protocols, this local communication is taken care of by ethernet protocols,
[306.08s -> 310.96s]  and this long distance communication is sort of layered on top of that and taken care of by
[311.52s -> 317.76s]  IP or internet protocols that know how to route over long distances to distant hosts.
[320.00s -> 327.60s]  Okay, so this is what a network looks like in a nutshell. I now want to talk about what's
[327.60s -> 334.72s]  inside the packets that move across an ethernet or move across the larger internet with an eye
[334.72s -> 341.44s]  to eventually talking about the software that in hosts that has to process hosts and routers
[342.00s -> 346.08s]  that has to process those packets. So let me start with the lowest level
[346.56s -> 353.20s]  and talk about what's inside an ethernet packet. So when two hosts that are quite nearby attached
[353.20s -> 357.44s]  to the same cable or same wi-fi network or same ethernet want to talk to each other,
[359.20s -> 365.52s]  the sort of lowest level protocol which allows two hosts in the same land to talk to each other
[365.60s -> 371.36s]  is the ethernet protocol. And you can think of one host, host one,
[374.16s -> 380.48s]  sending a frame over the ethernet to host two, what's called an ethernet frame, which is the
[380.48s -> 388.48s]  ethernet word for a packet, and it's a series of bytes that are sent over the ethernet from
[388.48s -> 393.28s]  one host to another. And what the ethernet protocol does is have just enough information
[393.28s -> 400.56s]  in it to allow the two hosts to realize who's talking to each other and cause the hosts to
[400.56s -> 406.56s]  be able to recognize packets that are addressed to them. And so what an ethernet header looks,
[406.56s -> 411.52s]  so the way that ethernet deals with this is that every ethernet packet has at the beginning
[413.76s -> 420.16s]  a header that has three fields followed by some ethernet payload.
[423.52s -> 430.40s]  And what's in the header is two ethernet addresses, we'll call them the destination address
[430.40s -> 436.32s]  and the source address, and also the type of the packet. Each of these addresses is just a
[436.32s -> 442.24s]  48-bit number that uniquely identifies a particular network interface card really.
[443.28s -> 448.08s]  And this type field is going to indicate to the recipient host what it's supposed to do with
[448.08s -> 454.80s]  that packet, and what that really means is what higher level protocol should examine and process
[454.80s -> 462.80s]  the payload of that ethernet packet. So these are bits that go, are typically said to go over
[462.80s -> 470.96s]  the wire, these 48 plus 48 plus 16 bits of header, and then however much payload. And not
[470.96s -> 475.52s]  really visible to the software, but there's going to be something at the beginning of the packet
[476.00s -> 480.40s]  that's recognized at a very low level by the hardware that signifies the start of a packet,
[481.60s -> 484.80s]  and the receiving host needs to know when the packet ends, and so there's going to be another
[486.48s -> 489.68s]  sort of special bit pattern at the end that signifies the end of the packet.
[491.12s -> 494.96s]  These two begin and end flags are never seen by the software, but the rest
[496.64s -> 501.36s]  the rest of this ethernet frame is delivered by the network interface card, the NIC, at H2
[502.08s -> 510.24s]  to the software. If you've looked at the final lab for the course, you'll see that the software
[510.24s -> 515.36s]  we give you includes a bunch of new files including kernel slash net dot H, which contains
[515.36s -> 523.44s]  a whole bunch of definitions of packet headers for different network protocols, and so this is
[523.44s -> 530.80s]  just text taken directly from this net dot H file we give you, and it includes a description
[530.88s -> 536.24s]  of the layout of the ethernet header. And this software we give you actually uses,
[536.24s -> 541.84s]  literally uses this struct definition in order to parse incoming ethernet packets, that is to
[541.84s -> 548.32s]  pick apart the head to get the destination and type, and also it uses this structure to format
[548.32s -> 553.76s]  packets. So the host is really in charge of sort of setting up and parsing this header
[554.32s -> 562.32s]  that's used by ethernet. Any questions about ethernet packets?
[568.72s -> 574.48s]  Yeah, I had a question. Is the bit pattern you mentioned that the hardware
[575.20s -> 581.28s]  uses to determine the start and end of a packet similar to the EOP in the lab?
[581.28s -> 595.28s]  Which is the end of packet. No. No, the EOP is a separate mechanism between the driver and the NIC
[595.28s -> 600.96s]  to help them communicate. This is, you know, there's some electrical scheme, there's some
[600.96s -> 607.36s]  low-level electrical or optical signaling scheme to transmit bits over ethernet cables,
[607.92s -> 616.32s]  and these flags have to do with, they're typically electrical patterns that would not be legal
[616.88s -> 623.60s]  inside a packet. And so, you know, one scheme is to, instead of just sending zero one bits
[623.60s -> 631.52s]  over the wire, you can send, you could send sequences of two signals. So there's four different
[631.52s -> 640.48s]  symbols possible with sequences of two different electrical voltage levels or something, and have
[640.48s -> 645.04s]  two of the four possible symbols indicate zero or one bits in the body of the packet,
[645.04s -> 650.08s]  and have the remaining two indicate beginning and end. And that was in fact a scheme that was
[650.08s -> 654.24s]  used years ago, scheme much like that was used years ago on ethernet. I don't actually know how
[654.24s -> 666.08s]  it works now. Okay, something to know about these addresses is that what, these are 48-bit addresses,
[666.08s -> 671.04s]  the reason for the 48 bits is that they wanted to make sure that there was enough bits
[671.04s -> 677.76s]  to be able to give a unique address to every different NIC ever manufactured. So there's a
[677.76s -> 682.80s]  vast number of possible addresses. The internal structure of these 48-bit addresses is that the
[682.80s -> 690.08s]  first half, the first 24 bits is a manufacturer number, and there's every manufacturer of
[690.08s -> 695.36s]  network interface cards of NICs has its own manufacturer number, so that's the first 24 bits.
[695.36s -> 700.40s]  And the second 24 bits is just any number, could be any unique number assigned by the
[700.40s -> 704.64s]  manufacturer. So manufacturers typically assign them in just ascending order. So if you buy,
[705.28s -> 710.80s]  you know, half a dozen network interface cards, the network interface card, each network interface
[710.80s -> 715.44s]  card has programmed into it its own address, and if you look at the address you'll see that
[715.44s -> 719.60s]  the high bits are the same for these six cards you bought from the same manufacturer,
[719.60s -> 728.56s]  but the low 24 bits are probably six sequential numbers. So these addresses are unique,
[729.28s -> 735.20s]  but what they're not helpful in is locating the destination host. So if you know the host
[735.20s -> 741.12s]  you're talking about is on the same local area network as you, you can use an Ethernet address,
[741.12s -> 745.52s]  and it's on the same local area network, so it will be listening for packets with its own address,
[746.24s -> 749.36s]  but if the host you're trying to talk to is on the other side of the country, you have to
[749.36s -> 753.04s]  use a different scheme, and that's what IP is all about, which I'll talk about in a bit.
[755.44s -> 761.92s]  Okay, so this is what these packets look like. You can actually look at Ethernet packets
[762.32s -> 767.76s]  in action using the TCP dump program, and you're encouraged to do this, you'll probably need to do
[767.76s -> 775.76s]  this as part of the lab, and this is actually the output of TCP dump from the lab,
[777.20s -> 781.92s]  and what TCP dump is telling us here, what's telling us a whole bunch of things. This first
[781.92s -> 788.16s]  part is the time at which the packet arrived. If you like, you can try this on your laptops
[788.32s -> 795.12s]  on your laptops if you install TCP dump, and the rest of the first line is a sort of
[795.12s -> 802.24s]  human readable interpretation of what kind of packet that is, and then these next three lines,
[802.24s -> 811.60s]  or the this part here, is a hex dump of the received packet, and you can see we can
[811.60s -> 819.36s]  actually follow along with the Ethernet header. These first 48 bits, or six bytes,
[819.92s -> 826.40s]  is a broadcast address. All Fs and all Fs Ethernet addresses broadcast all the hosts on the local
[826.40s -> 837.60s]  network. The next 48 bits is the sending hosts Ethernet address, which we can't necessarily tell
[837.60s -> 844.24s]  anything about, although the high bits, I mean in fact this was generated by xe6 running under
[844.24s -> 849.52s]  Qemu, so no real NIC was involved, so it's not actually a manufacturer number up here, it's just
[849.52s -> 856.72s]  something that Qemu makes up, and then the next 16 bits, the next two bytes, is the type,
[857.36s -> 863.36s]  it's the Ethernet type of the packet. In this case it's 0806, which is a protocol called ARP,
[864.08s -> 871.44s]  which I'll talk about in a moment, and the rest of this stuff is the payload
[871.44s -> 878.64s]  of an ARP packet, which I'll also talk about. Any questions about what we're looking at here?
[881.04s -> 884.24s]  This is well worth trying out on your own computer if you care about networks.
[886.72s -> 893.28s]  Okay, so the next protocol that's of relevance to the lab and communication over Ethernet,
[893.60s -> 899.20s]  is called ARP. So at the Ethernet level every host has a 48-bit Ethernet address,
[899.92s -> 905.92s]  but for communicating over the Internet it turns out you need to use a 32-bit Internet address,
[905.92s -> 910.56s]  and the reason why Internet addresses are different is that Internet addresses have
[910.56s -> 918.08s]  internal structure. In a 32-bit, in a host's 32-bit Internet address, the high bits are full of
[918.08s -> 923.68s]  all kinds of hints about where in the entire Internet this packet needs to go.
[923.68s -> 927.84s]  And so you can think of an Internet address as having in the high bits a network number,
[927.84s -> 930.48s]  it's actually a little more complex than that, but it's essentially a network number,
[930.48s -> 935.68s]  every network in the Internet has a distinct number, and routers look at these the high bits
[935.68s -> 940.56s]  in the Internet address to decide which router on the Internet this packet needs to be forwarded
[940.56s -> 948.16s]  to. And then the low bits in a 32-bit Ethernet, Internet address, IP address, are the number of
[948.16s -> 955.92s]  that of the host we want to talk to on its local network. But when a packet finally arrives
[957.12s -> 964.40s]  at a, when the Internet packet arrives at a Ethernet, we need some way to, given a 32-bit
[964.40s -> 973.36s]  IP address, figure out the 48-bit Ethernet address of that host. And the way the Internet
[973.36s -> 978.40s]  chooses to do that is to have a dynamic resolution protocol, a kind of request-response
[978.40s -> 983.52s]  protocol called ARP for address resolution protocol. And the way to think about it is that
[984.80s -> 991.28s]  when a IP packet arrives at a router, or it needs to be sent by a host to a host that's known
[991.28s -> 998.16s]  to be on the same LAN, local area network, the sender first broadcasts on that LAN
[998.16s -> 1003.92s]  an ARP packet that's a request that says whoever has, whoever owns this IP address,
[1003.92s -> 1008.80s]  please respond with your 48-bit Ethernet address. And assuming that host exists and
[1008.80s -> 1016.88s]  is turned on, it'll respond with an ARP response packet. And this is the format of the packet,
[1016.88s -> 1022.80s]  of an ARP packet. The way it actually shows up is inside an Ethernet packet. And so you,
[1022.80s -> 1029.36s]  what you would actually see in the network is first the Ethernet header, which has the 48-bit
[1029.36s -> 1035.04s]  source field and a 48-bit destination field. Maybe it's destination source type. So this is
[1035.04s -> 1043.36s]  the Ethernet header. And then from the Ethernet point of view, the rest is payload. But actually
[1044.00s -> 1050.88s]  in the Ethernet payload is an ARP packet, which has these fields, boom, boom, boom,
[1050.88s -> 1056.24s]  right after the Ethernet header. And the way the receiving host knows it's an ARP packet
[1056.24s -> 1063.36s]  is by looking at this type field. And if it's 0806, that's the agreed-on Ethernet protocol
[1063.36s -> 1067.44s]  number for ARP. And then the receiving host software would know to hand this packet to its
[1068.32s -> 1075.60s]  ARP protocol processing code. What's in these packets? There's a bunch of junk here that
[1075.60s -> 1082.08s]  basically amounts to saying, I have an Internet address, I want to turn it into an Ethernet
[1082.08s -> 1090.96s]  address, please respond if you own this Internet address. And then these fields
[1091.04s -> 1098.56s]  hold the Internet and Ethernet addresses of whatever host is sending this ARP packet.
[1098.56s -> 1102.48s]  And that's enough to figure out the host to build dynamically tables
[1103.68s -> 1107.12s]  that tell them the correspondence between Ethernet and IP addresses.
[1110.16s -> 1117.04s]  Again, we can use TCP dump in order to see these packets go by. And you're
[1117.76s -> 1125.04s]  highly likely to see them if you run TCP dump. Here's, again, a TCP dump output taken from
[1126.00s -> 1135.36s]  the lab. And it turns out that in the lab, your XV6 will end up talking simulated,
[1135.36s -> 1140.56s]  true, but talking Ethernet protocol and sending IP packets through Ethernet,
[1140.56s -> 1145.04s]  through a simulated Ethernet protocol with whatever host you're running QM you want.
[1145.76s -> 1152.64s]  And so you'll actually be able to see these ARP exchanges between XV6 and your host.
[1153.36s -> 1161.92s]  And so what we're seeing here is my host knows the IP address of my XV6
[1161.92s -> 1168.96s]  and wants to figure out its Ethernet address on the LAN that QMU simulates. And this second
[1168.96s -> 1176.00s]  packet is my XV6. And you can see the code that generates this. My XV6 has seen this request,
[1176.88s -> 1181.52s]  realized that it's the owner of the IP address in the request and is sending back the response.
[1182.32s -> 1186.56s]  TCP dump has nicely parsed out the fields in the ARP packet and printed them here.
[1188.40s -> 1195.28s]  And I think this is the sender's IP address. And this is the record, sorry,
[1195.28s -> 1202.16s]  this is the sender's IP address. This is the IP address that the sender is interested in.
[1202.80s -> 1208.80s]  And those would presumably go in here and here. And this is the response
[1212.16s -> 1218.40s]  with the Ethernet address of the owner of this IP address. And this
[1219.28s -> 1225.76s]  Ethernet address would probably end up being in this field. And if we're clever enough,
[1225.76s -> 1230.72s]  we can pick apart these packets and see some of these fields. As we know, this part
[1233.20s -> 1238.56s]  is the Ethernet header, destination, Ethernet address, source Ethernet address, and
[1239.52s -> 1251.20s]  packet type 0806. Working backwards, this is the TIP field, which is the IP address that the sender
[1251.20s -> 1256.56s]  wants to find the Ethernet address for. And if you pick this apart, there's one byte
[1257.60s -> 1265.04s]  for each of the four fields of the IP address. Sorry, it's really looking for 10.0.2.15.
[1265.12s -> 1274.48s]  This is 10 hex, 0 hex, 2 hex, 15 hex. And then there's the target's Ethernet address,
[1274.48s -> 1281.76s]  which is not known. And then the sender's IP address, 10.0.2.2, and the sender's Ethernet
[1281.76s -> 1287.52s]  address, and a bunch of other junk here saying that we're interested in Ethernet and IP address
[1287.52s -> 1294.24s]  formats. And here's the request, this is the response. Any questions about ARP?
[1295.84s -> 1306.88s]  Yeah, question. Why is it necessary for the sender to include its IP address if its
[1306.88s -> 1311.76s]  Ethernet address is already included in the packet? Like, to respond to it,
[1312.72s -> 1315.92s]  wouldn't the receiver only need the Ethernet address?
[1316.80s -> 1320.40s]  Yeah, I don't know why all this stuff's in there. I think if you wanted to,
[1320.40s -> 1330.32s]  you could strip this down quite a bit. Maybe the answer is that this protocol was
[1330.32s -> 1336.24s]  designed to be usable on networks other than Ethernet. And so it was designed to be fairly
[1336.24s -> 1345.76s]  self-contained, so that it didn't depend on anything else. And therefore, the ARP header
[1345.76s -> 1350.56s]  has a copy of the Ethernet addresses. Now in fact, if you know you're sending ARP over Ethernet,
[1350.56s -> 1354.24s]  the Ethernet packet also has all the Ethernet addresses, as you can see here.
[1355.84s -> 1361.04s]  So it's redundant of you running ARP over Ethernet, but maybe if you were running ARP
[1361.04s -> 1366.24s]  over something else, you'd need these fields, because maybe something else packet format
[1366.24s -> 1371.28s]  doesn't have the, doesn't already include these addresses. I see, okay, thank you.
[1371.60s -> 1376.80s]  Yeah. Oh, sorry, what is that part on the right versus are you?
[1377.84s -> 1383.92s]  Okay, this is not interesting yet, but this is ASCII interpretation of these bytes.
[1385.12s -> 1395.28s]  So, well, the dot here corresponds to, you know, a byte that has no ASCII equivalent.
[1395.28s -> 1402.00s]  And this, I guess somewhere in here, 52 or 55 probably, you know, 52 is probably R and
[1402.00s -> 1407.36s]  55 is probably U in ASCII. So this would be more interesting when we start sending
[1408.08s -> 1414.08s]  packets that have actual ASCII text in them, rather than binary fields. Okay, I see, thank you.
[1415.12s -> 1420.08s]  Yes. Okay, and I'm showing you this because you'll see these packets in the lab.
[1420.08s -> 1433.68s]  Okay, actually, there's something I wanted to, well,
[1436.40s -> 1441.84s]  there's something I want to make sure that you all caught in this discussion, and that's the
[1442.40s -> 1453.52s]  habit in formatting packets of nesting protocols and nesting headers. So what we just saw was a
[1453.52s -> 1458.80s]  packet that had an Ethernet header and Ethernet payload. The first part of the Ethernet payload
[1458.80s -> 1465.52s]  was, you know, an ARP header. As it happens, ARP has no remaining payload. But there are other,
[1465.52s -> 1468.56s]  what we'll see in a moment is much more complicated structures in which we have an
[1468.56s -> 1475.20s]  Ethernet packet that contains an IP packet, and inside the IP packet is a UDP packet. And so,
[1475.76s -> 1481.28s]  you know, UDP is another protocol that you can run over IP. So there's a UDP header. It also,
[1484.56s -> 1487.92s]  you know, you don't necessarily have to understand these acronyms yet, but
[1487.92s -> 1494.40s]  there's UDP header. A UDP packet also has a header and a payload, and there's times when
[1494.40s -> 1498.96s]  you carry another protocol inside UDP. So for example, the domain name system
[1499.68s -> 1505.04s]  has yet another format, a packet defined that fits inside UDP. So what you see is that
[1506.32s -> 1510.48s]  hosts that are sending packets will build up a packet. The DNS software will say,
[1510.48s -> 1515.76s]  I want to send a packet over UDP. The UDP software will prepend a UDP header.
[1515.76s -> 1520.88s]  We need to send that over IP. The IP software will prepend an IP header. The Ethernet
[1520.88s -> 1526.56s]  software will prepend an Ethernet header and gradually build up packets in that software
[1526.56s -> 1531.60s]  when it's sending. And similarly, when a system receives packets, it first gets the whole packet
[1531.60s -> 1536.40s]  and inspects the first header. It knows it's Ethernet because it received it from an Ethernet
[1536.40s -> 1541.12s]  NIC, you know, checks on validity, strips off this header to look at the next header.
[1541.12s -> 1546.24s]  And there'll be a type, this, you know, always either a type field or in this case,
[1546.24s -> 1552.48s]  a protocol field that tells the software what to expect after the Ethernet header.
[1553.44s -> 1559.60s]  So there's a type field that indicates IP versus ARP. So the software will look at each header,
[1560.24s -> 1566.80s]  validate it, strip it off revealing the next header, you know, check that header, interpret
[1566.80s -> 1570.08s]  it, figure out what it means, strip it off revealing the next and sort of hand it on to
[1570.16s -> 1574.24s]  the next layer of software. I'll talk a bit more about this, but
[1575.12s -> 1582.40s]  this is a sort of universal way of looking at these nested packet headers.
[1587.76s -> 1590.16s]  All right, so the Ethernet packet, the Ethernet header
[1592.08s -> 1594.80s]  is enough to get a packet to a host on the
[1595.68s -> 1602.32s]  local area network, especially when, and if you want to send an IP packet locally,
[1603.36s -> 1609.60s]  you can use ARP, but IP is used much more generally. IP is the sort of layer of the
[1609.60s -> 1614.40s]  protocol that helps you deliver a packet anywhere in the Internet based on IP addresses.
[1616.40s -> 1622.56s]  And so this is the format of an IP packet, again taken, you can find it in net.h in the
[1622.64s -> 1627.76s]  source we give you, and over Ethernet at least, the way you'd see this is
[1629.04s -> 1634.56s]  in an Ethernet packet with destination, source, and type of,
[1637.12s -> 1644.96s]  Ethernet type equals 0800, and then the IP header, and then the IP payload.
[1644.96s -> 1654.40s]  When you send a packet to a distant network, you know, on the other side of the world,
[1654.40s -> 1659.92s]  the IP header gets passed along, you know, the, this Ethernet header gets stripped off after
[1659.92s -> 1666.72s]  you leave the local Ethernet, maybe a new one gets put on it for each hop that your packet
[1666.72s -> 1672.16s]  is routed, but the IP header stays basically the same the whole way from the ultimate,
[1672.16s -> 1678.16s]  the original source host, your computer, all the way to the destination. So this header has
[1678.16s -> 1682.56s]  some global significance, whereas the Ethernet header is really only used for each,
[1684.72s -> 1688.64s]  for a single local area network. So there has to be enough information here to carry a packet
[1688.64s -> 1695.36s]  all the way to the far side of the Internet, and the critical fields for our purposes is
[1695.36s -> 1703.92s]  really three very interesting fields in this packet format. The destination field,
[1703.92s -> 1708.56s]  which is the 32-bit IP address of the host that we want to send the packet to,
[1708.56s -> 1713.76s]  and in particular in its high bits it's going to have network numbers in it that'll help routers,
[1713.76s -> 1718.72s]  and then when the packet's delivered, this P protocol field will tell the destination host
[1719.52s -> 1724.08s]  what to do with the packet, not what to do with it next after it strips off the IP header.
[1725.68s -> 1731.92s]  And if you've ever seen a MIT IP address,
[1734.24s -> 1738.48s]  you'll see, well there's a couple of different ones, but for example if you see an
[1741.60s -> 1747.76s]  Internet address starting with 18, this, the things have actually changed in the last couple
[1747.76s -> 1755.84s]  years, but this for a long time was the network number of MIT, and so most hosts at MIT would
[1755.84s -> 1761.28s]  have IP addresses whose high byte was 18, and routers all over the world would have some table
[1761.28s -> 1765.92s]  they'd look up 18 and say, aha, I know how to route this packet one step closer to MIT.
[1765.92s -> 1774.64s]  So let me, let me show you again TCP dump output,
[1776.72s -> 1779.28s]  again actually taken from the lab, that includes
[1781.84s -> 1792.64s]  an IP header. Okay, so we can parse this packet because it was sent over the
[1792.64s -> 1794.96s]  Ethernet, it starts with an Ethernet header.
[1800.88s -> 1808.32s]  Um, actually one thing that's kind of wrong with these TCP dump, with these packets that
[1808.32s -> 1815.76s]  are generated, and now that I'm seeing it I'm not sure what the problem is, um, they should not
[1815.76s -> 1820.48s]  start with all, these Ethernet headers shouldn't start with all Fs because that's the broadcast
[1820.48s -> 1825.44s]  address that causes the packet to go to every host, and you would not see that for a packet
[1825.44s -> 1829.92s]  sent between two individual hosts as this one is on a real network. So there's something funny
[1829.92s -> 1835.76s]  going on with my solution to the network lab or with QEMU. Anyway, we have the Ethernet
[1835.76s -> 1844.32s]  destination address, Ethernet source address, and the Ethernet type, and 0800 is, and means that
[1844.32s -> 1853.20s]  the remaining bytes are an IP packet. The IP packet header length I think is 20 bytes.
[1854.80s -> 1858.72s]  Um, let's see if we can find the end, 250.
[1862.16s -> 1864.56s]  So this must be the end of the IP header.
[1867.36s -> 1871.60s]  Um, and working backwards because these are the fields we really care about,
[1872.40s -> 1883.68s]  the destination IP field is A is 10 10 0 2 2, um, which is I think the, in QEMU's funny
[1883.68s -> 1888.56s]  simulated network is the address of the real computer I'm running QEMU on,
[1889.28s -> 1895.28s]  and then before that is the IP source address which is 10 0 2 15,
[1896.08s -> 1904.32s]  um, which is the sender which is QEMU's address for the, for xv6 basically running inside QEMU,
[1904.32s -> 1909.92s]  and then this stuff before does, um, all this other stuff, there's a 16-bit checksum,
[1910.72s -> 1914.96s]  uh, which your software is supposed to check to realize that a packet's been corrupted and
[1914.96s -> 1921.60s]  should be discarded, that's this checksum. There's a one byte, all right I messed something up here,
[1921.92s -> 1932.72s]  oh this is, this is 16, sorry, this is 16-bit checksum, this 11 is the protocol number which
[1932.72s -> 1939.76s]  is particularly important, 11 hex is 16 plus 1 or 17, so that means that this is a UDP packet
[1940.80s -> 1946.24s]  based on the protocol field, and then all this other stuff we don't really care about
[1946.24s -> 1953.84s]  has things like the length of the packet. Any questions about IP headers?
[1960.00s -> 1966.80s]  All right, um, again the critical stuff is the IP header has the IP address, the source of
[1966.80s -> 1973.68s]  destination, and this protocol field is going to tell the destination host networking stack that
[1973.68s -> 1980.24s]  this packet should be processed by its UDP software, which I'll talk about right now.
[1981.44s -> 1989.52s]  Okay, the, um, this, this IP header is enough to get a packet to any host on the internet,
[1989.52s -> 1993.20s]  um, but we want to do better than that. Every host, you know, is running lots and
[1993.20s -> 1997.60s]  lots of different programs that need to use the network, they need to send and receive packets
[1997.60s -> 2002.88s]  in the network, and so we need a way that's not in the, it's not included in the IP field
[2002.88s -> 2009.76s]  in order to decide which application, um, needs to, which application on the target host
[2009.76s -> 2016.48s]  this packet ought to be handed off to. And there's a couple of protocols that
[2017.04s -> 2025.36s]  do that job, one of them is TCP, which is quite complex, another is UDP. TCP is actually
[2025.36s -> 2031.44s]  what's used mostly for things like the web, TCP is a very complex protocol that not only
[2031.52s -> 2035.92s]  helps your packet be delivered to the right application but also has a lot of things like
[2035.92s -> 2042.56s]  sequence numbers in order to detect lost packets and retransmit them, make sure packets or data is
[2042.56s -> 2048.48s]  delivered in order and without gaps if, in case anything goes wrong. UDP is a much simpler
[2048.48s -> 2054.80s]  protocol that just delivers a sort of best effort delivery of a packet to a particular
[2054.80s -> 2062.72s]  application but without any error correction or, well, basically without anything else.
[2063.84s -> 2069.60s]  For us the critical fields are these two port numbers, and the game here is that when
[2069.60s -> 2076.24s]  your application wants to send or receive packets it uses the, what's called the Sockets API
[2077.12s -> 2086.24s]  on Unix at any rate, and this is a set of system calls whereby a process can say look I'm interested
[2086.24s -> 2092.56s]  in packets addressed to a particular port and it'll say what port number it's interested, sorry,
[2093.20s -> 2097.36s]  packets with a particular, I want to receive packets with a particular destination port
[2098.24s -> 2103.12s]  and the operating, so you make a system call that sets this up and the operating system will
[2103.12s -> 2109.04s]  return a file descriptor and every time a packet arrives with the port that the application asks
[2109.04s -> 2114.24s]  for that packet will appear on the file descriptor and the application can read it.
[2116.24s -> 2124.08s]  And these ports, there's really two kinds of ports, some are well-known port numbers like
[2124.72s -> 2132.56s]  I think port 53 is the official well-known universally agreed port number for
[2132.56s -> 2136.72s]  a DNS name server, so if you want to send a request to a DNS name server
[2136.72s -> 2141.52s]  you're going to send it in a UDP packet addressed to dport 53, and there's a bunch of
[2141.52s -> 2148.48s]  other sort of well-known ports for commonly available services with universally agreed on
[2148.48s -> 2156.80s]  numbers, and then the remaining, remainder of the 16-bit port number space is used for
[2156.80s -> 2163.68s]  the sort of anonymous client ends of ports, so if I want to send a packet to a DNS server
[2164.24s -> 2169.52s]  its dport will be 53 all right, but its sport will be a more or less randomly chosen number
[2169.52s -> 2176.40s]  for my end so that when the, that that'll be associated with my application's socket so that
[2176.40s -> 2182.08s]  when the DNS server sends a reply it'll be addressed that the DNS server will copy the
[2182.08s -> 2188.32s]  request source port into the destination port field of the reply, send it back to my machine
[2188.32s -> 2195.20s]  and my machine will use this port number to figure out which application should get the reply.
[2197.68s -> 2202.24s]  Okay so the main function here is to have these two port numbers in order to
[2202.32s -> 2209.92s]  hand off, be able to hand off packets to individual applications on this machine.
[2212.24s -> 2217.44s]  So feel free to ask questions. I have TCP dump output for
[2219.52s -> 2229.68s]  UDP also, again taken from the lab. So again we have an ethernet header and
[2229.84s -> 2240.08s]  a 20-byte IP header which probably ends here. The 11 is IP protocol 17 which is UDP,
[2240.08s -> 2250.32s]  so the receiving host will know to process it with its UDP software. The next eight bytes
[2252.64s -> 2259.36s]  are the UDP header which is shown right here, and so who knows what these port numbers are.
[2260.24s -> 2268.24s]  I mean none of this, this is unfortunately a packet just generated by the lab software without any
[2271.28s -> 2274.40s]  any special numbers in it. See these are just the port numbers that happen to choose.
[2275.36s -> 2281.60s]  This must be the length of the packet, so 1B is 20 something, and this my, you know, this
[2281.60s -> 2288.24s]  our software for UDP and xv6 is so lame that it doesn't fill in the checksum field.
[2288.24s -> 2294.00s]  But this is the header, and then after the UDP header is the payload of the UDP packet,
[2294.00s -> 2299.20s]  and in this case the application is sending ASCII text, and that ASCII text is right here.
[2303.20s -> 2307.92s]  So this is a masky text placed inside a UDP packet, placed inside an IP packet,
[2307.92s -> 2313.60s]  placed inside an ethernet packet, and sent over a simulated ethernet.
[2313.60s -> 2324.16s]  Oh well, sorry, I just had a question. So when you send a package to someone,
[2324.16s -> 2331.04s]  you don't know their ethernet address, so do you just send it to your router
[2331.04s -> 2333.84s]  then, and then the router figures it out from there?
[2334.40s -> 2340.96s]  If, for packets sent somewhere else on the internet, let's see,
[2343.52s -> 2348.40s]  your host, if you send a packet to a particular IP address, your host software will look at
[2348.40s -> 2354.16s]  the destination address to figure out if the target host is on the same local area network
[2354.16s -> 2360.00s]  as you are. And if it is, it'll use ARP to translate the IP address into an ethernet
[2360.00s -> 2363.44s]  address and then send the packet over the ethernet to the target host.
[2364.40s -> 2368.96s]  So that's what happens in the special case in which the target host is on the same network.
[2368.96s -> 2372.32s]  In the more general case where you're sending the packet to somewhere else on the internet,
[2372.32s -> 2379.36s]  you know, across the country, you'll send the packet to a router on the same local area
[2379.36s -> 2386.40s]  network. That router will look at the destination IP address to pick the next router to decide
[2386.64s -> 2390.08s]  which router it's attached to, which it forwards the packet to, and the packet will
[2390.08s -> 2394.88s]  go hop by hop through routers, getting closer and closer to the target. Does that answer
[2394.88s -> 2399.36s]  your question? Okay, I see yes. Thank you so much. Someone asked if there's a limit
[2399.36s -> 2403.04s]  to the length of the packet, and the answer is yes. There's a couple of different limits.
[2404.88s -> 2409.36s]  Every underlying network technology, like ethernet, but there are other things that are like
[2409.36s -> 2416.56s]  ethernet, has its own maximum packet length. So when today's paper was written, the maximum
[2416.56s -> 2423.04s]  packet length on ethernet was 1,500 bytes. I think modern ethernets allow packets up to
[2423.04s -> 2429.92s]  around 9,000 or 10,000 bytes, but that's about the highest maximum packet size I've heard of,
[2429.92s -> 2436.88s]  and there's a couple of reasons why you wouldn't want sort of infinitely long single packets.
[2436.96s -> 2442.08s]  One of them is that the packets are, you know, you're sending these packets over wires that
[2442.08s -> 2448.08s]  could be quite long and subject to noise and interference, and so you do get corruption of
[2448.08s -> 2453.84s]  bits when you're sending packets. Basically every networking technology has some kind of
[2453.84s -> 2458.32s]  checksum or error correcting code that goes along with every packet, but checksums and
[2458.32s -> 2463.84s]  error correcting codes are only capable of reliably detecting errors over a certain number
[2463.84s -> 2469.84s]  of bits, and so as you increase the number of bits, the probability of an uncaught error goes up
[2469.84s -> 2475.84s]  and up, and so that limits the, for a reasonable size checksum like 16 or 32 bits, that limits the
[2475.84s -> 2483.92s]  maximum size of a packet. And the other limitation is that if you send huge packets, that means
[2483.92s -> 2490.08s]  that all the routers and hosts involved have to have huge packet buffers to be prepared to
[2490.08s -> 2499.36s]  receive huge packets, and that starts to get unwieldy and expensive, because it's difficult
[2499.36s -> 2503.12s]  to have variable length buffers. It's most convenient to have just a single length of buffer,
[2504.72s -> 2508.16s]  and that works best if the maximum packet length isn't too enormous.
[2509.28s -> 2519.12s]  Anyway, so for, so Ethernet has a 1500 or 9,000 byte limit. In addition, you know, for all
[2519.12s -> 2524.80s]  these IP protocols have length fields, which are 16 bits, so even if you're willing to have Ethernet
[2524.80s -> 2531.68s]  have a larger packet size, IP itself has a kind of baked-in maximum packet size of 64 kilobytes.
[2535.76s -> 2547.20s]  Okay, good, so much for UDP, and hopefully when you finish the lab you'll see
[2547.84s -> 2556.16s]  output very much like this. In particular, the message from xv6, and a message or reply back
[2556.16s -> 2563.68s]  from the host that you're running QEMU on. In fact, actually at the end of the lab you'll use,
[2565.44s -> 2570.64s]  you'll run software which we provide, which will actually send a DNS query to Google's DNS servers
[2571.60s -> 2574.88s]  and get the response back, and our software will print the response, but
[2575.68s -> 2580.88s]  your software will have done the sort of Ethernet level device driver interactions.
[2584.16s -> 2588.88s]  All right, so that's the story for packet headers and protocols on the wire.
[2589.84s -> 2596.56s]  Through corresponding to these packet formats is this, what's called the stack of
[2597.52s -> 2603.20s]  network software that runs on the host. So if you think about what's sitting inside the host,
[2603.60s -> 2609.28s]  and then from now on I'm talking, I mostly talk about sort of typical software arrangements. There's
[2609.28s -> 2616.16s]  all kinds of different ways people structured network software, and it's somewhat quite different
[2616.16s -> 2621.44s]  from what I'm going to talk about. I'm going to talk about kind of the, what I think of as
[2621.44s -> 2625.60s]  at least as the sort of standard approach. So, you know, let's assume we're running Linux or maybe
[2626.48s -> 2631.36s]  xv6, and we have a bunch of applications, maybe a web browser,
[2634.08s -> 2643.44s]  maybe a DNS server, who knows what, a bunch of applications. They'll have used the sockets API
[2643.44s -> 2649.76s]  to open up file descriptors in the sockets layer, so this is going to be this inside the kernel.
[2650.48s -> 2659.20s]  A layer of software called the sockets layer that has tables that remembers the correspondence
[2659.20s -> 2665.92s]  between file descriptors, which the applications read or write, and UDP port numbers or TCP port
[2665.92s -> 2672.72s]  numbers, which is for the sort of endpoints of conversations that these file descriptors
[2672.72s -> 2677.44s]  refer to. So the socket layer has these tables of file descriptors and port numbers,
[2677.44s -> 2683.92s]  and it also typically has a queue of packets that have arrived and are waiting
[2683.92s -> 2693.52s]  to be read by each socket or file descriptor. And the software we provide you has a very
[2693.52s -> 2706.00s]  primitive sockets layer. Underneath that are going to be the UDP and TCP protocol layers.
[2708.40s -> 2714.64s]  UDP has almost nothing going on. It basically looks at incoming packets, extracts the destination
[2714.64s -> 2719.12s]  port number, and hands the packet off to the socket layer too, so the payload isn't queued on
[2719.12s -> 2726.48s]  the correct file descriptor's incoming queue. TCP actually is much more complex. It keeps state
[2726.48s -> 2731.20s]  for each TCP connection, and it remembers all kinds of sequence numbers and packets that
[2731.20s -> 2735.12s]  haven't been acknowledged and need to be re-transmitted. So there's a huge amount of state
[2735.20s -> 2742.56s]  in what's called a protocol control block in TCP, and virtually no state in the UDP layer.
[2743.20s -> 2745.92s]  These are often called transport layers, UDP and TCP,
[2747.44s -> 2755.20s]  and we provide you with a simple UDP layer but not a TCP layer. Underneath TCP and IP is a
[2756.16s -> 2767.92s]  IP layer, which is often fairly simple, and kind of in parallel with the IP layer,
[2767.92s -> 2773.12s]  I'm not sure whether I should draw it on the same level or underneath it, is the ARP layer.
[2774.40s -> 2779.44s]  Under them both, we can think of as an ethernet layer, but it's really, there's not typically
[2779.44s -> 2783.44s]  a separate ethernet layer. Typically there's one or more NIC drivers
[2786.80s -> 2794.32s]  at the lowest layer, and these talk to the actual NIC network interface hardware,
[2794.32s -> 2802.08s]  which itself has a connection off to the local area network, or whatever kind of network you
[2802.08s -> 2809.36s]  are attached to. And sort of at this level, what happens is a packet arrives off the network,
[2809.36s -> 2816.24s]  the NIC pulls it off the network, hands it off to the driver, and the driver essentially
[2816.24s -> 2823.68s]  pushes the packet up the networking stack. And at each layer in the stack, that layer's header,
[2824.88s -> 2830.16s]  the IP layer will look at the IP header, verify the header, strip it off, hand it to UDP.
[2830.72s -> 2836.00s]  UDP will figure out what file descriptor to queue the data on and add it to that queue.
[2836.64s -> 2840.16s]  So packets come in and are parsed, and headers are stripped in the way up,
[2840.16s -> 2844.88s]  and when an application sends a packet, the reverse thing happens. As the packet moves down
[2844.88s -> 2849.20s]  through the layers, more and more headers are added on until you get to the bottom layer,
[2849.20s -> 2852.48s]  and then the packet's handed to the NIC for transmission.
[2852.48s -> 2864.32s]  And so of course the software, the way people think about it in design network software,
[2864.32s -> 2870.16s]  in the kernel is typically driven by the nesting of the protocols inside the packets.
[2872.40s -> 2873.84s]  Any questions about this structure?
[2873.84s -> 2883.60s]  There's actually one important thing that I kind of left out here that sits on the side.
[2884.80s -> 2890.72s]  There's buffers, there's packet buffers all through this. So when a packet arrives, it's
[2891.60s -> 2897.20s]  copied into a packet buffer, and the packet buffers are sent up and down the stack,
[2897.20s -> 2901.12s]  and there's often quite a few packet buffers. There's often queues between these layers.
[2901.20s -> 2906.40s]  There's certainly a queue here of packets waiting to be processed by applications,
[2906.40s -> 2912.80s]  and this will be a linked list of buffers. And so there's a buffer allocator that's,
[2912.80s -> 2918.32s]  this buffer scheme and a buffer allocator that's used throughout the stack. And in the software,
[2918.32s -> 2925.68s]  we give you these, the buffer scheme is called MBuffs. So it's kind of a MBuff
[2926.40s -> 2932.48s]  scheme that's not a layer, but it's used all throughout these layers.
[2935.84s -> 2939.68s]  Okay, this is the layering diagram of the sort of typical network stack.
[2942.08s -> 2946.56s]  For this paper, it's actually important to understand how the control flow works, which is
[2947.68s -> 2950.00s]  maybe a little bit different from what's in that diagram.
[2950.56s -> 2958.88s]  One thing to know about network stacks is that there's typically multiple independent actors
[2958.88s -> 2963.20s]  that process packets and take input, think about those packets, and produce output.
[2963.76s -> 2969.84s]  And for various reasons, these different actors are decoupled so they can run concurrently and
[2969.84s -> 2975.28s]  have packet queues connecting them. And so that's extremely important from the point of view of
[2975.28s -> 2982.24s]  this paper. So within the kernel, so again, we have a network interface card, and then we have
[2982.24s -> 2994.64s]  the kernel. The, a classic arrangement here is for the NIC to somehow get packets, well,
[2994.64s -> 3000.00s]  for the NIC when it receives a packet to generate an interrupt. And there's, so there's this
[3000.56s -> 3006.32s]  interrupt routine that gets triggered whenever there's an interrupt, and the job of the interrupt
[3006.32s -> 3014.40s]  routine is to get the packet from the NIC. And because we don't want to dedicate CPU time to
[3014.40s -> 3019.36s]  completing the processing of the packet now, the interrupt routine typically just
[3019.36s -> 3026.64s]  appends the packet to a queue of packets for later processing and then returns. So it sort of
[3026.64s -> 3033.20s]  does the minimum work required to get the packet from the NIC and put it in a queue.
[3033.20s -> 3038.00s]  And the reason why we want to transfer, or in the sort of traditional network stack,
[3038.00s -> 3042.48s]  we want to quickly move the packet out of the NIC and into the software queue is that
[3042.48s -> 3046.00s]  NICs typically have a very limited amount of memory for queuing packets,
[3046.00s -> 3050.16s]  whereas in the main memory, the RAM and the computer, we might have gigabytes of memory,
[3050.16s -> 3055.52s]  so far more space here. So the NIC, so if there's a burst of packets, the NIC may actually
[3055.52s -> 3061.20s]  run out of space to queue them. So we copy them this queue here to avoid the NIC running out of
[3061.20s -> 3071.76s]  space. And then separately, perhaps in a separate thread, there's what I'll call the IP processing
[3071.76s -> 3078.88s]  thread. Sometimes it's not a thread, sometimes it's a sort of different kind of entity,
[3078.88s -> 3086.00s]  but its basic job is to read packets off these incoming queues, and there may be multiple NICs,
[3087.04s -> 3092.72s]  you know, pending packets to these queues. So our IP thread looks at packets that are
[3092.72s -> 3098.24s]  queued here and decides what to do with them. One possibility is to send them up through UDP
[3102.24s -> 3105.52s]  into the sockets layer to be queued, waiting for some application.
[3106.08s -> 3110.80s]  And typically this will just be a function calls here within the context of this thread.
[3113.20s -> 3117.44s]  Another possibility, and this is the possibility the paper cares most about, is that this host
[3117.44s -> 3122.48s]  is actually a router, and its packets are coming in one NIC and routed out one or more other
[3122.48s -> 3129.04s]  NICs, because it's very common to build routers out of ordinary operating systems like Linux,
[3129.04s -> 3132.72s]  like if you buy a wi-fi box now or a cable modem router or something,
[3133.68s -> 3138.56s]  it's extremely likely to be running Linux internally and to use the standard Linux stack,
[3138.56s -> 3144.16s]  which has a complete router implementation. It's highly likely to be using the standard
[3144.16s -> 3150.32s]  Linux stack in order to do its routing. So if the IP thread looks at the destination
[3150.32s -> 3155.36s]  IP address and decides, oh, I should send this out, I should forward this packet out, you know,
[3155.36s -> 3162.24s]  out another network interface, it'll add the packet to a queue of outgoing packets
[3162.80s -> 3167.76s]  for this outgoing interface, and there's almost certainly, so this is a receive interrupt,
[3168.32s -> 3175.52s]  or RX for receive, there's usually some sort of transmit interrupt scheme
[3178.88s -> 3185.36s]  for the outgoing NIC, that and the NIC will interrupt whenever it's finished sending one
[3185.36s -> 3191.60s]  packet and is ready to be handed more packets. So these outgoing interrupts may also be important.
[3193.52s -> 3200.32s]  And the point here is that there's a bunch of concurrent entities that are sort of
[3200.32s -> 3204.96s]  separately scheduled in various different ways. These interrupts are triggered by the NICs
[3204.96s -> 3207.92s]  asking for interrupts when packets arrive or when packets have been sent.
[3208.64s -> 3216.32s]  This thread may be a kernel thread like we have in xv6, and on a uniprocessor,
[3216.32s -> 3221.36s]  as was the case with today's paper, you know, this thread can't run at the same time as interrupt,
[3221.36s -> 3226.72s]  the interrupts have absolute priority. On a multi-core machine there may be more parallelism,
[3226.72s -> 3229.76s]  and then if it's important that applications be able to read the packets,
[3230.40s -> 3234.32s]  the applications are yet another sort of independently scheduled entities
[3236.24s -> 3240.56s]  that we'd like to get their chance at executing on the CPU.
[3242.80s -> 3250.56s]  So these are all the players in the scheduling game, essentially.
[3252.16s -> 3263.52s]  One thing that comes up a lot is buffering. So there's three queues here. By buffering I mean
[3264.24s -> 3269.76s]  these structures in which one independent entity appends input packets and some other entity
[3270.56s -> 3275.84s]  pulls packets off the front of the queue. These queues are pervasive in networking systems.
[3275.84s -> 3283.36s]  One reason for them is to allow temporary, to cope with temporary bursts. You know,
[3283.36s -> 3286.72s]  this IP thread maybe can only process packets at however many per second,
[3287.28s -> 3292.40s]  but the NIC may be able to deliver packets much more quickly, and so there may be a little
[3292.40s -> 3295.60s]  sort of temporary burst of packets. We'd like to have somewhere to put them,
[3297.04s -> 3301.60s]  waiting for the IP thread to get around to processing them, and so that's one use of queues.
[3302.56s -> 3309.04s]  On the output side, another use of queues is we'd like to, especially if packets are bursty,
[3309.04s -> 3313.36s]  we'd like to be able to stack up a bunch of packets here ready for the NIC to send,
[3313.36s -> 3318.80s]  to keep the NIC, to output NIC busy, because depending on the speeds of things,
[3318.80s -> 3324.48s]  it may be quite important to be able to hunt, to utilize 100% of the network here.
[3324.48s -> 3332.40s]  And the other reason for, or maybe the same reason stated differently for having queues,
[3332.40s -> 3337.60s]  is to be able to structure software into independent parts that are scheduled separately.
[3338.24s -> 3342.24s]  We wouldn't necessarily want to have our IP thread or the application
[3342.24s -> 3346.08s]  know about the other things that have to go on, like interrupt processing.
[3346.08s -> 3352.32s]  So the IP thread is sort of the traditional networking system, you know, it doesn't necessarily
[3352.32s -> 3360.80s]  know when interrupts happen or when applications run, although we'll see in this paper that
[3360.80s -> 3363.28s]  there may be advantages to having a little bit of knowledge there.
[3367.68s -> 3371.04s]  Questions about this sort of scheduling control diagram?
[3371.04s -> 3383.12s]  I have a quick question. So can the same NIC not be used for both transmitting and receiving?
[3383.12s -> 3383.62s]  It can.
[3385.20s -> 3391.44s]  So my laptop really only has one interface and it connected to Wi-Fi. When it receives a packet,
[3391.44s -> 3398.16s]  so on my laptop, this NIC is actually a Wi-Fi radio interface. Packets arrive and go out
[3398.16s -> 3406.16s]  on the same NIC. The two NIC situation is certainly used for routers. So your home,
[3406.96s -> 3411.36s]  Wi-Fi, I don't know, maybe you have, I have Wi-Fi and cable and there's a router box
[3412.80s -> 3418.64s]  that has two NICs. One is the, it's connection to my cable modem, which leads to the rest of
[3418.64s -> 3426.16s]  the internet, and the other one is my, the Wi-Fi interface. So the little box that the cable
[3426.16s -> 3433.04s]  modem cable company sent me is a router with two network interfaces. And there's actually a lot of
[3433.04s -> 3439.12s]  servers have multiple interfaces also, especially ones that are web servers that, you know, you
[3439.12s -> 3446.32s]  want to talk to the outside world. One interface sent to your private sensitive database machine
[3446.32s -> 3452.64s]  or something on a totally separate network on a, with another network interface. This arrangement
[3452.64s -> 3458.24s]  is pretty common. The criteria for having multiple NICs is just wanting to talk to different networks
[3458.24s -> 3463.12s]  then? Yeah. Okay. Yeah, if you want to talk to different networks then you would have multiple
[3463.12s -> 3471.44s]  NICs, yes. Got it. All right, I want to, as an aside, talk a little bit more about
[3472.96s -> 3479.60s]  NICs, what NICs do with packets when they arrive. And this is a special relevance to the lab.
[3480.16s -> 3489.60s]  You, you know, what a NIC looks like internally, you know, it's got a cable leading or a radio
[3489.60s -> 3495.44s]  leading to the, from the outside world. You know, and it looks at electrons as they come in
[3495.44s -> 3502.40s]  and sort of turns them into packets. And then there's the host, and there's the host has some
[3502.40s -> 3508.40s]  sort of driver software in it. And one way or another, you know, we need to get a packet
[3508.40s -> 3516.24s]  that's decoded in the NIC into memory, where the IP software in the host can parse that packet.
[3516.24s -> 3521.68s]  And so there's a lot of different schemes been designed over the years. The paper scheme is
[3521.68s -> 3528.32s]  that the NIC has a lot of internal memory and as packets arrive, the only, the only immediate
[3528.32s -> 3532.32s]  thing that happens is the NIC puts, you know, lays down the packets in its own buffer memory,
[3532.32s -> 3541.36s]  and that's it, and interrupts the host, the host. And so the NIC has an internal queue of packets
[3541.36s -> 3546.80s]  and a bunch of memory. And then in the interrupt, well, in the host driver, the host driver has a
[3546.80s -> 3550.88s]  loop in it. The host driver will talk to the NIC and say, oh, you don't have any packets buffered?
[3550.88s -> 3556.48s]  And if it does, the host has a loop that'll just copy, you know, byte by byte or word by word,
[3556.80s -> 3566.00s]  copy this packet into the memory of the host and append it to a queue inside the host.
[3566.00s -> 3573.04s]  So that's how the papers NIC works. The driver sort of is responsible for doing the copy from
[3573.04s -> 3580.48s]  NIC memory to host memory. That made a lot of sense 30 years ago. Today, though, it turns out
[3580.48s -> 3587.52s]  that loops in the CPU that copy, that, you know, talk to external hardware or hardware sitting on
[3587.52s -> 3594.16s]  buses are very, very slow. This is sort of, you know, in the grand scheme of microprocessor
[3594.16s -> 3601.52s]  design, this distance here between the CPU and the external device, even if it's on the same
[3602.40s -> 3608.08s]  computer, this is a very long distance. And this, each conversation today takes a long time for
[3608.48s -> 3611.84s]  back and forth chitchat. And so you don't want to have a lot of byte by byte
[3613.04s -> 3618.16s]  interaction. So people don't design high-speed interfaces like this anymore.
[3620.80s -> 3627.52s]  So a much more modern arrangement looks like this. And so now I'm going to talk about
[3627.52s -> 3638.88s]  an arrangement which shows up in the E1000 NIC, which you'll use in the lab, or simulation of
[3638.88s -> 3645.60s]  which. So the way the E1000 NIC works, it still has its wire and it's looking at the electrons.
[3645.60s -> 3654.16s]  But as the packets arrive, the NIC writes them. The NIC doesn't really have significant
[3654.16s -> 3658.72s]  internal buffering, although it has a little bit. It actually copies the packets directly into
[3658.72s -> 3663.12s]  host memory where they'll be sitting there in host memory waiting for the driver to pick them
[3663.12s -> 3668.56s]  up, sort of already copied. But that means that NIC has to know where in memory it should put
[3668.56s -> 3679.12s]  each packet. So the way that E1000 NIC works is that the host software formats up what's
[3679.12s -> 3688.72s]  called rings, DMA rings of packet pointers. So a DMA ring is just an array of pointers
[3692.80s -> 3700.88s]  to packet buffers. So the host driver, when it's initialized in the card, will allocate however
[3700.88s -> 3708.48s]  many, say 16, 1500 byte packet buffers, will create an array of 15 pointers or 16 pointers,
[3708.48s -> 3714.32s]  and make these pointers point to there. And then it'll tell the NIC in configuration time,
[3714.32s -> 3724.16s]  look, here's the ring. So this is called a DMA ring, because after you've gone off the end,
[3724.16s -> 3728.88s]  you start back at the beginning. The driver software will tell the NIC, look, here's a
[3728.88s -> 3734.88s]  pointer, the address in my RAM of the ring, the DMA ring that you're supposed to use to deposit
[3734.88s -> 3741.12s]  incoming packets. So when a packet arrives, the NIC will actually remember which ring entry
[3741.12s -> 3747.60s]  is the next one. Well, there's a little pointer here that allows it to remember
[3748.32s -> 3751.68s]  the next entry that it should DMA a packet into. When a packet arrives,
[3751.68s -> 3756.96s]  the NIC will fetch the pointer out of this, fetch this buffer pointer out of the host RAM,
[3757.60s -> 3763.76s]  copy the packet bytes into this buffer, and then advance its internal
[3764.64s -> 3768.72s]  index here to point to the next ring slot, which we'll use for the next packet.
[3769.84s -> 3776.32s]  And there's a similar, so this will be the RX ring for receive, there's a similar ring that
[3776.32s -> 3785.44s]  the driver sets up, in which the driver puts packets that it wants the NIC to send,
[3785.52s -> 3792.56s]  so the NIC also has a pointer to the TX ring. And so you'll learn your job in the lab is
[3792.56s -> 3799.84s]  basically to write the driver software that handle these rings. Any questions about this
[3799.84s -> 3813.04s]  arrangement? Yeah, how does the E1000 compare with production level of NICs that may be used
[3813.04s -> 3819.44s]  in high performance environments? Well, when the E1000 came out, it was the absolute best
[3819.44s -> 3826.24s]  NIC available, and it was the NIC that was used in serious production environments,
[3826.24s -> 3834.48s]  but that was however many years ago. Modern NICs are quite a bit cleverer. What hasn't
[3834.48s -> 3840.96s]  changed that much is this DMA ring structure. You still find that NICs, they use DMA to
[3840.96s -> 3844.96s]  deliver packets, and the way they find the place to deliver the packet is by these
[3845.68s -> 3853.60s]  rings of buffer pointers. There's a couple of things that modern NICs are more clever about.
[3854.16s -> 3859.44s]  One is that modern NICs, you can set them up with many, many queues. The E1000, I think,
[3859.44s -> 3865.44s]  just has a single receive queue, but you can tell a modern NIC, look, I want you to split my
[3865.44s -> 3871.20s]  packets up into 32 different incoming queues, and here's how to decide for each packet. Look
[3871.20s -> 3878.24s]  at this field and use that to choose which ring to DMA the packet into, and there's a whole
[3878.24s -> 3883.12s]  bunch of clever ways that people use that capability, like if you have multiple virtual
[3883.12s -> 3888.24s]  machines, you're Amazon and you're running many guest virtual machines, you may use that
[3888.24s -> 3894.00s]  capability to direct each packet to the queue corresponding to the virtual machine
[3894.56s -> 3899.76s]  that that packet should be read by. And another way in which modern NICs are more clever is that
[3899.76s -> 3906.48s]  they'll do some of the TCP processing on the NIC, like maybe typically check some calculations
[3906.48s -> 3914.56s]  the most. Anyway, yeah, so modern NICs are like the E1000 but more.
[3914.56s -> 3917.20s]  Okay, thanks.
[3920.24s -> 3924.48s]  Sorry, go ahead. Oh yeah, I just wanted to ask, so in our scheme in the lab,
[3925.92s -> 3931.36s]  there is no queue between the IP layer and the driver, right?
[3932.16s -> 3934.16s]  Yeah, yeah, the lab network stack is
[3935.84s -> 3941.20s]  stripped down to the absolute minimum. This is far simpler in structure than a real network stack.
[3942.08s -> 3949.36s]  But this is worse in terms of performance? Oh, I don't know. I never run it in real life,
[3949.36s -> 3955.20s]  I'm sure. I mean, certainly we paid zero attention to performance when writing the
[3955.20s -> 3959.84s]  lab network stack, so it would be surprising to detect good performance. Mostly it's not
[3959.84s -> 3964.48s]  a question of performance, there's limitations. It doesn't do 95% of what you need a network
[3964.48s -> 3974.00s]  stack to do, like handle multiple NICs or have TCP. Right, right. Okay, I see. Thank you.
[3977.92s -> 3988.16s]  Sorry, so were there any hardware changes to the overall system that were needed to enable
[3988.16s -> 3994.96s]  NICs to have direct memory access? Like in the previous picture, was everything mediated
[3994.96s -> 4000.96s]  through the CPU or could NICs also reach the memory directly? In the picture I showed before,
[4000.96s -> 4009.20s]  no, the NIC doesn't reach the memory at all. Okay. I actually don't know. I mean,
[4011.04s -> 4015.36s]  maybe the most important question is how virtual memory, whether and how virtual memory
[4015.92s -> 4020.88s]  translation works when the NIC wants to use an address that refers to host memory.
[4020.88s -> 4027.60s]  And I don't actually know how that works. I don't know how that works. I suspect there's
[4028.32s -> 4036.48s]  translation, the NIC is really sitting on a bus that's connected through some fairly intelligent
[4036.48s -> 4041.92s]  silicon to the DRAM system. And I believe in modern machines, you can set up translation
[4041.92s -> 4047.68s]  tables so that the NIC could use virtual addresses or addresses that are translated
[4049.20s -> 4054.16s]  by this hardware that sits between it and RAM. And that can be very valuable for some situations.
[4055.92s -> 4060.88s]  The other thing, I mean, another thing that I'm aware of is that
[4062.80s -> 4068.48s]  if the NIC is going to write some memory or read some memory and the memory is cached on the CPU,
[4068.48s -> 4073.76s]  you know, you want the NIC to read. If the software just, you know, wrote a packet buffer,
[4073.76s -> 4080.56s]  but the CPU hasn't, the CPU has merely cached the write, because after all,
[4081.36s -> 4087.60s]  most memory is write back. That means that the real latest version of that memory is sitting
[4087.60s -> 4092.96s]  on the CPU cache, not in RAM. And in that case, we'd like the NIC to be reading the CPU cache,
[4092.96s -> 4098.88s]  not the RAM, if it does DMA. And certainly on Intel machines and probably on others,
[4098.88s -> 4103.20s]  there's some fairly elaborate machinery so that if the NIC reads some memory,
[4103.20s -> 4108.32s]  but the latest copy of that memory is in the CPU cache, it's the CPU cache that will produce
[4108.32s -> 4114.96s]  the data and not RAM. And that's just, yeah, that's actually a facility you can,
[4114.96s -> 4120.48s]  that some clever software uses to get high performance. That is to have the effect of
[4121.12s -> 4125.60s]  having the, this happens for writes also, that the NIC will essentially directly write cache lines
[4125.60s -> 4129.68s]  in the cache, where it's, the CPU can get at the data very quickly.
[4132.56s -> 4139.92s]  Yes, they're simple, but they're real life. It's pretty involved. Other questions about,
[4142.40s -> 4143.28s]  about anything?
[4144.16s -> 4144.80s]  About anything.
[4148.64s -> 4153.20s]  Okay, I'd like to switch gears now to today's paper.
[4155.84s -> 4160.16s]  And I'm just going to like, because we've already talked about the sort of,
[4160.16s -> 4165.12s]  a lot of the background of this paper, I'm just going to go directly to the first graph in the
[4165.12s -> 4172.64s]  paper and essentially drive the discussion off of the paper's graphs.
[4174.08s -> 4176.96s]  And so what we're looking at here is the performance graph for a router.
[4178.16s -> 4181.52s]  On the x-axis, there is a router with two NICs. Its job is to,
[4181.52s -> 4184.64s]  packets come in one NIC and it's supposed to just send them out the other NIC.
[4186.32s -> 4190.48s]  The x-axis is the arrival rate, which packets arrive at the input NIC.
[4191.44s -> 4196.48s]  And the y-axis is the output rate at which packets are observed to leave the output NIC.
[4198.24s -> 4207.20s]  And the line we care about is the filled circles, which goes up and then down.
[4208.64s -> 4213.68s]  So even without knowing anything about what's going on here, we can look at this
[4213.68s -> 4218.08s]  graph and we can ask ourselves, gosh, why does it go up? Why does it go down?
[4218.64s -> 4222.32s]  You know, what's special about this point that it's an inflection?
[4222.96s -> 4226.72s]  You know, what is it that governs how fast it goes up or how fast it goes down?
[4227.44s -> 4233.44s]  Right? So even with zero knowledge, we kind of have a good clue about what questions to ask.
[4233.44s -> 4238.88s]  So why does it go up? Why do these thoughts go up?
[4244.80s -> 4245.92s]  It's not a very deep question.
[4248.08s -> 4252.24s]  They go up? Sorry, go ahead.
[4253.36s -> 4257.12s]  Because when they're like, until it gets saturated,
[4257.12s -> 4262.80s]  you can process more input packages and produce more output packages.
[4262.80s -> 4267.68s]  Absolutely. Like until something starts to go wrong, for every packet that comes in,
[4267.68s -> 4271.36s]  the battery just forwards it out. So, you know, until things go wrong,
[4271.36s -> 4275.44s]  if packets arrive at 2000 packets per second, well, it just copies every input packet to
[4275.44s -> 4279.52s]  the output. And that means that the output rate is just equal to the input rate.
[4279.52s -> 4284.08s]  So this is just y equals x, you know, because every input packet gets sent out.
[4284.08s -> 4288.32s]  So it's y equals x for a while. And so why does it stop going up?
[4296.00s -> 4299.36s]  Isn't this the one thing they mentioned in the paper about
[4299.68s -> 4308.80s]  the there being interrupts that can't be processed at the necessary rate?
[4309.60s -> 4312.56s]  That's the answer to the question, why does it go down?
[4314.96s -> 4319.68s]  My question is, why does it stop going up? Like, what is it that could this line in
[4319.68s -> 4323.92s]  a well-designed system, supposing that they hadn't messed up the design, right?
[4323.92s -> 4327.60s]  You and I would design a system that didn't have problems, right?
[4327.60s -> 4330.40s]  Would our system with a line just keep going up?
[4335.44s -> 4342.80s]  I guess not, because at some point, the rate at which packets come will,
[4345.92s -> 4350.96s]  if you can process packets fast enough, then at some point,
[4353.20s -> 4355.60s]  the bottleneck will be the rate at which packets arrive.
[4356.56s -> 4362.16s]  That's right. The system we're talking about has some sort of limits. The CPU is not
[4362.16s -> 4367.12s]  infinitely fast. The CPU executes however many instructions per second and no more.
[4367.12s -> 4372.00s]  So each of these packets has to be processed. The IP software has to look at the header and
[4372.00s -> 4377.04s]  check the checksum and look up the destination addresses and whatever. It takes hundreds of
[4377.04s -> 4384.24s]  thousands of CPU cycles per packet. And so we can never, never expect these lines to go up
[4384.32s -> 4390.72s]  indefinitely. They must stop somewhere, right? And we can sort of tell what, we can make some
[4390.72s -> 4397.28s]  guesses on this system. It goes up to 5,000 and no more. And what that basically suggests
[4397.28s -> 4401.84s]  to us is that it takes about 200 microseconds on this computer to process each packet,
[4402.72s -> 4406.56s]  right? That's what this point, the fact that the inflection point is here,
[4406.56s -> 4412.64s]  means suggests that the total cost of processing a packet is around 200 microseconds of CPU time.
[4412.64s -> 4418.40s]  Yeah, that's a guess, but it's likely to be close to correct. And so there's just no way we could
[4418.40s -> 4421.60s]  get, you know, maybe we could make the software a little more efficient. Maybe we could reduce
[4421.60s -> 4426.24s]  that to 150 microseconds per packet or something. And so maybe we could move the
[4426.24s -> 4431.84s]  inflection point up a bit. But we're certainly faced with some point at which, well, that's
[4431.84s -> 4442.16s]  just how many packets the system can process. Now, that's not necessarily what's going on here.
[4442.16s -> 4448.80s]  It happens to be what's going on here, but it's not written in stone. In fact, there's
[4448.80s -> 4454.32s]  other bottlenecks that could be the limit other than CPU time, which are worth considering.
[4454.32s -> 4458.24s]  The most obvious is the speed of the network. The network they were using ran at only 10
[4458.24s -> 4466.00s]  megabits per second. That's just how fast the low-level networking hardware sent bits.
[4466.00s -> 4473.36s]  And so it can never transmit more than 10 million bits per second. And so that may also
[4473.36s -> 4477.36s]  constitute a limit. And so we're thinking about whether maybe that's actually what's determining
[4477.36s -> 4486.72s]  this 5,000. The paper doesn't quite say enough to know whether it's CPU or
[4486.72s -> 4492.80s]  the NIC is the limiting factor here. But the fact is with their 10 megabit network,
[4493.52s -> 4500.72s]  if you send small packets, the 10 megabits translates into something like 10 or 15,000
[4500.72s -> 4507.36s]  packets per second. So that's the limit that the networking cable puts on the input
[4508.00s -> 4514.32s]  rate. And so this is well under the 10 or 15,000 packets per second that the network is
[4514.32s -> 4519.04s]  capable of. So almost certainly the limit has to do with CPU or memory or something and not the
[4520.00s -> 4529.76s]  network itself. Okay. What we'd love to see in a well-designed router is that
[4531.12s -> 4536.08s]  it can actually, if it takes 200 microseconds to forward a packet, to process a packet,
[4536.08s -> 4540.00s]  what we'd like to see is that the thing, the router can actually forward 5,000 packets per
[4540.00s -> 4545.76s]  second no matter what, even if the load is high. So what we'd like to have seen is this
[4545.84s -> 4553.20s]  line here where the output rate matches the input rate until you get up to the capacity of the
[4553.20s -> 4558.96s]  system, 5,000 packets per second, and then it just continues to forward 5,000 packets per
[4558.96s -> 4567.12s]  second and presumably drops, discards the rest. So this is what we'd like to design,
[4567.12s -> 4571.84s]  but what actually happened is much worse than that. As you increase the rate beyond 5,000,
[4572.48s -> 4575.44s]  the number of packets that it manages to forward goes down towards zero.
[4576.16s -> 4579.92s]  So why does this line go down? I think somebody mentioned this before.
[4591.60s -> 4599.68s]  Well, the reason that the authors figured out is that as you increase the input rate,
[4599.68s -> 4605.76s]  each of these input packets generates an interrupt, and the interrupts take time. Actually,
[4605.76s -> 4608.32s]  on their system, the interrupts are quite expensive because they involved
[4609.44s -> 4613.76s]  copying a packet off of the network interface card and into main memory, which took a long
[4613.76s -> 4619.92s]  time because the CPU was doing it. So we know with packets arriving at 10,000 per second,
[4619.92s -> 4625.52s]  we certainly can't forward 10,000. That means the best we can hope for is to forward 5,000
[4625.52s -> 4632.00s]  and simply discard the other 5,000. But in fact, the extra 5,000 packets each generate a
[4632.00s -> 4638.72s]  very expensive interrupt. So for each additional packet over 5,000 per second, we're generating
[4638.72s -> 4643.12s]  more and more expensive interrupts, which have priority, like interrupts. Boy, whatever you're
[4643.12s -> 4650.16s]  doing, it stops and the machine takes the interrupt. Because the machine is essentially
[4650.16s -> 4654.24s]  giving priority to these interrupts, that means every additional packet per second
[4654.24s -> 4662.72s]  is taking CPU time away from the forwarding code until finally 100% of the CPU time is used
[4662.72s -> 4670.24s]  up in the input interrupt routine and no CPU time is used in the thread that forwards packets.
[4672.96s -> 4674.88s]  Is everyone happy with this explanation?
[4675.84s -> 4682.72s]  And this going down is called interrupt lifelock.
[4685.12s -> 4689.92s]  The fact that this line goes down instead of saying steering flags is what people mean by
[4689.92s -> 4697.92s]  interrupt lifelock. And it's actually a phenomenon that occurs in many systems.
[4699.04s -> 4702.64s]  I mean, the sort of thing that's driving it is that there's two separate interfaces
[4702.64s -> 4706.00s]  The sort of thing that's driving it is that there's two separate tasks,
[4706.00s -> 4710.48s]  like the input interrupt task and the forwarding task. And because of a scheduling
[4711.28s -> 4719.76s]  problem, essentially priority is given to the input task, which can starve the packet processing
[4719.76s -> 4726.64s]  task. And in pretty much any system that has multiple independent tasks or sort of sequence
[4726.64s -> 4731.12s]  of independent tasks that need to be done to each input, and in which the input rate
[4731.12s -> 4736.72s]  can't necessarily be controlled, many systems like that will exhibit lifelock if you push them
[4736.72s -> 4745.04s]  too hard. And you can get lifelock due to many resources, not just CPU, but it could be that
[4745.04s -> 4752.08s]  the NIC, you know, a NIC that DMAs uses up RAM cycles to do the DMAs. And if the NIC
[4752.08s -> 4756.48s]  is using the RAM, the CPU can't use the RAM. So another way to get lifelock,
[4756.48s -> 4761.12s]  even if you have lots of CPU time, you know, in some other design you might get lifelock
[4761.12s -> 4770.32s]  because the NIC is using up RAM resources, RAM performance resources, so the CPU is less able to
[4770.32s -> 4777.60s]  use the RAM. Anyway, this line going down is what they mean by lifelock. You may ask what
[4777.60s -> 4785.44s]  happens to the excess packets. And if you recall, the structure of their software was that they
[4785.44s -> 4792.08s]  have a NIC that basically feeds the receive interrupt, the received interrupt software,
[4792.08s -> 4798.16s]  copies each packet into a queue, and then there's some sort of network thread
[4800.48s -> 4805.36s]  that pulls packets off the queue. The exact place where packets are lost
[4805.36s -> 4809.68s]  are right here. What's going to happen is that the interrupt routine,
[4810.48s -> 4816.40s]  you know, once we get down here with serious lifelock, the interrupt routine is going to fill
[4816.40s -> 4820.40s]  this queue. There's going to be some maximum queue length here, you know, at least all of RAM,
[4820.40s -> 4825.76s]  but probably much less. And the interrupt routine is going to pull a packet off the NIC
[4825.76s -> 4830.88s]  and see that this queue is already as long as it's allowed to be, and the interrupt routine
[4830.88s -> 4835.12s]  will discard the packet. But then, of course, immediately after that, there'll be another interrupt.
[4835.68s -> 4840.96s]  And the interrupt routine will again discard the next packet because
[4841.76s -> 4845.76s]  the network thread is always interrupted and never allowed to run.
[4848.64s -> 4849.76s]  Questions about this diagram?
[4854.32s -> 4857.12s]  This is the most important diagram in the paper.
[4857.36s -> 4862.72s]  All right, well, we've basically run out of time, so I'll try to compress the
[4863.28s -> 4870.96s]  answer to this problem into a minute. The authors proposed a solution. The most immediate
[4870.96s -> 4874.80s]  good news about the solution is that this is the performance of their solution. That is,
[4874.80s -> 4880.24s]  the input rate goes up, goes up to 5,000, and then it's flat at 5,000 regardless of input
[4880.24s -> 4889.36s]  rate. So this is sort of a perfect non-livelock performance line. Of course, it's going to be
[4889.36s -> 4895.12s]  flat because you can only process 5,000 packets per second due to the speed of the CPU.
[4895.92s -> 4899.36s]  And the way they get this, they still have this network thread.
[4902.00s -> 4904.08s]  And they still have an interrupt routine.
[4904.08s -> 4909.04s]  And so the very first time the NIC interrupts, it'll run the interrupt routine, but the interrupt
[4909.04s -> 4914.00s]  routine does not copy packets off the NIC. It wakes up the network thread and then
[4914.00s -> 4920.32s]  leaves interrupts on the NIC disabled, so we'll get no more interrupts, wake up the
[4920.32s -> 4923.92s]  networking thread, and then the interrupt routine will return. So now, interrupts in
[4923.92s -> 4931.12s]  the SNIC are turned off. And so the very first time the NIC interrupts, it'll run
[4931.12s -> 4936.08s]  will return. So now, interrupts in the SNIC are turned off. The network thread basically has a loop
[4940.40s -> 4947.52s]  that it'll check the NIC, you know, pull a few packets from the NIC.
[4950.64s -> 4954.16s]  Five, I think, is what they ended up using, and then process those packets.
[4954.16s -> 4963.20s]  And then, if there were none, if this, it'll check the, so it's this
[4963.20s -> 4966.48s]  networking thread now that reads packets off the NIC, not the interrupt routine.
[4967.12s -> 4970.96s]  If there were no packets waiting, it enables interrupts,
[4976.24s -> 4977.52s]  and then goes to sleep.
[4977.52s -> 4985.60s]  Because it's enabled interrupts, the next time a packet arrives, the interrupt routine will wake up
[4985.60s -> 4990.16s]  this thread, and it'll come out of sleep and go back to the top of the loop. So this is
[4990.16s -> 4996.40s]  the structure of their solution, and one way to view this is that they turn an interrupt
[4996.40s -> 5003.36s]  scheme into a polling scheme that is under high load. They just sit in this loop, and they read
[5003.36s -> 5007.28s]  a packet, process it, read a packet, process it, with interrupts turned off. So they,
[5009.92s -> 5013.68s]  since the interrupts are turned off, they never get this effect where the interrupts steal time
[5013.68s -> 5019.60s]  from the main thread. Whereas under low load, they enable interrupts, and maybe a while until
[5019.60s -> 5024.08s]  the packets arrive, but they'll be woken up by the interrupt routine immediately if a packet does arrive.
[5026.80s -> 5030.40s]  And that's all I have to say. Any questions?
[5030.40s -> 5041.12s]  I have a question. Is that loop looking at all of the devices, or only the one that generated
[5041.12s -> 5045.20s]  the interrupt? If there's multiple NICs...
[5046.72s -> 5055.36s]  So that's a good question. If there's multiple NICs, the loop... I don't actually know how the
[5055.36s -> 5061.44s]  loop worked. A very reasonable design is for this network thread to keep track of,
[5062.80s -> 5067.04s]  for every NIC, whether it's an interrupting mode or polling mode.
[5070.64s -> 5073.52s]  And then it will only... Actually,
[5075.92s -> 5080.40s]  because the interrupt routines no longer read packets from the NIC, that means the loop probably checks
[5080.40s -> 5087.36s]  every interface at this point. It probably checks every NIC at this point and pulls up for every
[5087.36s -> 5095.20s]  NIC. If that NIC has packets waiting, the loop will pull a few out of the NIC and process them.
[5095.84s -> 5102.32s]  And then if none of the NICs had anything, if it checked all the NICs and none of them had
[5102.32s -> 5108.56s]  any packets waiting, the loop will enable interrupts on all the NICs and sleep, and any NIC
[5108.72s -> 5111.92s]  that interrupts will wake it up. That's my guess.
[5118.16s -> 5124.96s]  I had a quick question then. So while the loop is running, how do packets actually
[5124.96s -> 5130.56s]  get into the queues to be pulled? I felt like there would only be one at a time.
[5131.92s -> 5138.16s]  Initially, the packets are queued inside the NIC in its own private memory.
[5139.52s -> 5146.96s]  Then this loop, when it goes back to the top of the loop, it'll look at each NIC and actually
[5146.96s -> 5151.76s]  talk to the NIC hardware and ask it, do you have any input packets waiting in your memory?
[5151.76s -> 5159.44s]  And if it does, then this loop will allocate a packet buffer in RAM and copy the bytes of
[5159.44s -> 5164.80s]  the packet out of the NIC, the packet buffer, and then process that packet buffer.
[5165.20s -> 5170.48s]  So it can copy more than one packet? Yeah, I think they do it in groups of five
[5170.48s -> 5174.88s]  in order to, even if there's a hundred packets waiting here, it would just process the next
[5174.88s -> 5180.32s]  five in order to be fair among the input NICs and to avoid starving the output.
[5180.32s -> 5184.48s]  But this requires increasing the memory capacity of the NIC, right? Possibly?
[5185.36s -> 5194.32s]  Well, I don't know. I don't know how much the NIC might have had a reasonable amount of.
[5196.56s -> 5203.04s]  The thing is, this interrupt, this live lock phenomenon, below this point,
[5203.04s -> 5210.64s]  we're probably interrupting. And if a packet arrives, the network thread will almost
[5210.88s -> 5215.52s]  immediately be woken up and pull the packet out. If we're over here in this regime where
[5215.52s -> 5220.88s]  too many packets are arriving and this loop is polling instead of interrupting,
[5222.64s -> 5225.52s]  packets are going to be lost. We just know that because the difference,
[5227.76s -> 5234.88s]  this difference between the input rate and the output rate, this is all dropped packets.
[5240.64s -> 5245.04s]  Adding, I don't think adding, because these packets are going to be dropped anyway,
[5245.60s -> 5251.28s]  adding buffering to the NIC probably doesn't help very much. I don't, I think it's a NIC,
[5254.24s -> 5258.00s]  it's not clear the NIC needs more than a small amount of buffering.
[5259.20s -> 5262.88s]  Yeah, that makes sense. Yeah, you only need as much as it would need to take,
[5263.44s -> 5268.16s]  to get saturated. So, you know, I think in their design they would pull five packets,
[5268.32s -> 5272.16s]  their quota was five packets, and so the NIC certainly needs five packets of buffering,
[5272.72s -> 5277.20s]  for that to make sense, but it probably, you know, it's not clear that more than that would
[5277.20s -> 5284.40s]  be very beneficial. Really, the purpose of buffering is to absorb transient bursts.
[5287.04s -> 5290.00s]  So, hold the packets long enough that the software can get along to reading them,
[5290.00s -> 5294.00s]  but we're not talking about transient anything in this situation, we're talking about persistent
[5294.08s -> 5299.84s]  overload. So, that means there's not really much function for a lot of buffering.
[5300.40s -> 5302.24s]  Yeah, makes sense. Thank you.
[5305.20s -> 5310.80s]  I think my question is related to that, it's, so if the difference between interrupts on and
[5310.80s -> 5316.80s]  interrupts off here is that it's going to be the same, but like it's still going to be
[5316.80s -> 5322.48s]  putting things on the queue if it's able to, but it will just not issue an interrupt,
[5322.48s -> 5326.96s]  but if there's no place on the queue then it will just drop?
[5326.96s -> 5331.68s]  The interrupt routine in this new scheme never looks at packets.
[5332.64s -> 5339.12s]  Oh yes, I mean like you said that you turn off the interrupts for the NIC, right?
[5340.00s -> 5343.52s]  Yeah, so this is what happens in the interrupt handler is it disables
[5347.12s -> 5351.12s]  interrupts on this NIC and then wakes up the network thread.
[5352.48s -> 5358.00s]  That's all it does, and it returns. Right, I guess my question is when the
[5358.00s -> 5363.36s]  interrupts are disabled, can the NIC still put packets on its own buffer?
[5363.36s -> 5369.28s]  Yes, the NIC is self-contained, so it has internal buffering, whether or not,
[5369.28s -> 5374.48s]  regardless of whether interrupts are enabled or disabled, all that happens when a packet arrives
[5374.48s -> 5382.00s]  is that the NIC adds, appends the packet to its queue of packets in its own internal memory.
[5382.08s -> 5388.64s]  That's all that ever happens when a packet arrives, so in this paper's NIC, I mean different NIC
[5388.64s -> 5394.32s]  designs are very different, but for this paper their NIC never did DMA, it never reached out
[5394.32s -> 5400.56s]  and touched host memory ever. It's got an internal queue in its own memory and the host
[5400.56s -> 5406.48s]  could read packets out if it wanted to. Okay, I see, and if there's no memory then?
[5407.04s -> 5414.48s]  Drop, so in this design, if packets are these excess packets, the place they're dropped is
[5415.84s -> 5421.60s]  inside the NIC. What will happen is a packet will, you know, if we're in an overload situation,
[5421.60s -> 5426.32s]  then the NIC's queue will be full always or almost always, and so when a packet arrives,
[5426.32s -> 5431.92s]  the NIC's queue will typically be full and the NIC will drop the packet without wasting any CPU
[5431.92s -> 5439.60s]  time on the main machine. The fact that it can drop without burning up CPU time here
[5439.60s -> 5446.56s]  is one way of explaining how they avoid lifelock. Right, right, okay, thank you so much.
[5451.92s -> 5459.68s]  I had a quick question. Will there ever be a scenario in which the CPU loop will pull a few
[5459.68s -> 5468.40s]  packets but the internal software queues are all full? Oh sure, yeah, there's some other bottleneck.
[5469.12s -> 5475.68s]  So for example, suppose these incoming packets, some of them need to be delivered to some
[5475.68s -> 5484.32s]  application on a socket. If that application isn't reading packets fast enough, then the socket
[5484.40s -> 5491.04s]  buffer leading to, you know, that that application should be reading will get full and then packets
[5491.04s -> 5500.16s]  may be dropped in the networking thread, and that can also lead to lifelock, because now,
[5500.16s -> 5503.92s]  because we have the same, you know, the reason lifelock comes up is that we expended
[5503.92s -> 5510.72s]  resources processing a packet that was later dropped, so it was wasted effort. If the
[5510.80s -> 5514.88s]  application's not reading, it's, well, one way we can get it to lifelock is
[5516.72s -> 5520.88s]  as the load goes up, maybe we end up spending a hundred percent of our time in the networking
[5520.88s -> 5526.08s]  thread, leaving zero percent of the time for whatever application is supposed to be reading the
[5526.08s -> 5532.48s]  packets, and then we'll again get lifelock, but it won't be interrupt lifelock, it'll be
[5533.20s -> 5538.48s]  network processing lifelock or something. The paper actually had a story for that.
[5540.96s -> 5545.76s]  Somewhere in section six, they talk about the network. If packets are being delivered to a
[5545.76s -> 5551.20s]  local application, the network thread would look at the socket queue for that application,
[5551.20s -> 5557.28s]  and if it was getting long, it would turn off interrupts and stop pulling packets off
[5557.28s -> 5563.84s]  the network interface until the queue got shorter, and so that means the network thread would stop
[5563.84s -> 5568.16s]  running and get the application a chance to run and process the packets, so you can get
[5568.16s -> 5573.76s]  lifelock-like situations at any stage if you're not careful in a kind of multi-stage
[5574.48s -> 5578.00s]  processing scheme. Okay, if that makes sense. Thank you.
[5581.20s -> 5590.40s]  Thank you. Thank you. You're welcome.
