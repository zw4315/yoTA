# Detected language: en (p=1.00)

[0.00s -> 15.16s]  Okay, good afternoon, wherever you are. Can everybody hear me? Quick sound check to make
[15.16s -> 16.16s]  sure.
[16.16s -> 18.16s]  Yep, you're fine.
[18.16s -> 24.84s]  Okay, good. Thank you. So today, we're going to talk, I don't know if you have a specific
[24.84s -> 31.56s]  agenda, but basically, the plan is to try to answer questions that you might have about
[31.56s -> 39.08s]  the last lab or the previous labs. And so the approach I'm going to take today is I'm
[39.08s -> 47.56s]  going to walk through the staff solutions, particularly my own solutions, and discuss
[47.56s -> 54.04s]  them. And hopefully, as we go through the staff solutions, you can jump in if I don't
[54.04s -> 60.68s]  answer a particular question, or wait a little bit because I put all the questions that you've
[60.68s -> 67.24s]  asked, at least before 11am this morning, in the lecture notes at the bottom, and we'll
[67.24s -> 69.24s]  try to go through that.
[69.24s -> 77.40s]  I'm going to start with the page table lab, because most questions worry about the page
[77.40s -> 85.64s]  table lab, and partially because of the room with the hardest labs. And so maybe a couple
[85.64s -> 93.00s]  comments before diving into sort of the technical part of this. Let's talk a little bit about
[93.00s -> 101.24s]  the page table lab in general. And, you know, as you probably observed, you know,
[101.24s -> 113.40s]  there's actually a few lines of code in terms of the solution, but unfortunately, it has,
[113.40s -> 128.68s]  you know, hard to debug problems. And, you know, there's one reason is that when actually
[128.68s -> 134.68s]  the bug shows up, you know, the outcomes are pretty extreme. Like in the worst case,
[134.68s -> 141.32s]  probably some of you observed that in the worst case, QMU just stops, or xv6 just stops,
[141.32s -> 146.76s]  there's no output being printed anymore, and that's it. And, you know, that was your job,
[146.76s -> 151.96s]  to figure out actually what went wrong. In the best case, you know, you get sort of a kernel
[151.96s -> 157.64s]  panic, but the panic, you know, may be just a starting point for tracking down what the
[157.64s -> 162.04s]  actual source of the problem is. You know, the panic itself is probably related to something
[162.04s -> 166.44s]  else, so the invariant in the kernel got broken, but, you know, somewhere out earlier,
[166.44s -> 169.72s]  probably when you set up the page tables, you know, you did something wrong that in the end
[169.72s -> 176.76s]  caused this panic to go off, so you have to track down what's going on. So fewer lines of
[176.76s -> 183.56s]  code, hard to debug problems, and, you know, it's just a harsh environment to debug in.
[184.52s -> 188.76s]  You can do in the kernel debugging, or kernel programming, you know, just the programming
[188.76s -> 194.76s]  environment or debugging environment is more, is unforgiven, as Robert mentioned in the first
[194.76s -> 198.60s]  lecture. This is one of the hard parts about basically doing kernel programming.
[201.40s -> 204.76s]  And that's, you know, just to make you maybe feel better, you know,
[204.76s -> 207.48s]  this is not just hard for you, you know, it's actually always hard for the staff.
[207.48s -> 216.36s]  Both in terms of helping you, and actually, you know, when we do these labs, you know,
[216.36s -> 220.04s]  we make similar problems, and, you know, we probably have more experience in tracking bugs
[220.04s -> 225.32s]  down, but, you know, they just, they do take time. In terms of helping you, you know, the
[228.12s -> 232.28s]  tends to be hard too, because there's some small fraction of code, there's one detail,
[232.28s -> 236.20s]  something probably wrong, and they're figuring out actually what that little detail is.
[236.84s -> 241.16s]  It's not that easy. Of course, you know, some of the problems we've seen before and we recognize,
[241.16s -> 245.56s]  you know, for example, we didn't make the same mistake ourselves, but others ones, you know,
[245.56s -> 250.44s]  you discovered all kinds of different ways of sort of breaking the kernel that we hadn't seen
[250.44s -> 256.68s]  before. So this is just hard, and I think this is, you know, in our experience,
[259.32s -> 262.20s]  this is historically always hard with virtual memory.
[267.00s -> 272.92s]  So every time the first virtual memory lab, you know, comes around either in SO81 or
[272.92s -> 278.44s]  its predecessor in 6.8.8, it has to be the case that, you know, that tends to be the hardest lab
[280.84s -> 284.92s]  of all the labs. And, you know, for a number of reasons, one, you know, the harsh environment,
[284.92s -> 291.72s]  you know, the bugs are spectacular, and partly because, you know, you haven't had that much
[291.72s -> 295.40s]  kernel program experience yet, and so this is sort of the first lab where all those things
[295.40s -> 300.20s]  come together. This year, we tried to make the lab simpler. In fact, this is a new lab.
[301.32s -> 306.68s]  We did not have that lab in the hopes to actually make this introduction or the transition
[306.68s -> 311.64s]  into virtual memory easier. In some ways, I think it was successful. In other ways,
[311.64s -> 315.72s]  you know, it's still hard, and so we don't really know maybe about the easiest way of
[315.72s -> 324.36s]  actually introducing virtual memory programming is. Lab number, the lab next week, the lazy lab,
[324.36s -> 329.64s]  is actually the lab that used to be the first virtual memory lab, and my suspicion is that
[329.64s -> 334.76s]  you'll find it this year, that lab, easier than the students found it last year because
[334.76s -> 343.96s]  you now have much more background in virtual memory. Okay, so that's maybe a good point to
[343.96s -> 348.92s]  stop. This is like a couple high-level points I wanted to make before jumping into the more
[348.92s -> 353.64s]  technical or the more details, so if you have any questions, this is a great time to ask questions.
[354.36s -> 367.64s]  Okay, then let's quickly, you know, some of you asked for this, a very quick review exactly
[367.64s -> 374.52s]  what, you know, where, you know, what the sort of the setting is, and so basically the setting
[374.52s -> 385.56s]  is page tables, and we have physical memory. I will draw it here on the right side. You know
[385.56s -> 395.24s]  that physical memory consists of partly about devices, and they live above this OX,
[396.60s -> 402.36s]  you know, zero zero, and many many more zeros, and then here is basically DRAM chips.
[404.84s -> 410.44s]  And we know that QEMU actually puts the kernel, the kernel text and data, you know, right,
[411.08s -> 417.72s]  you know, near at the above OX zero zero zero zero. So this is where the kernel lives,
[417.72s -> 421.80s]  and, you know, one way you may be able to erase this a little bit, you use the kernel,
[422.44s -> 425.56s]  and, you know, basically literally what it means, you know, what is it, what the kernel is,
[425.56s -> 431.96s]  well here are the instructions, you know, the binary versions of the Istanbul instructions,
[432.20s -> 438.68s]  that you have seen, and there's also some data structures located at these addresses.
[440.60s -> 443.88s]  Okay, so that's the memory part of it, and then on the other side,
[444.84s -> 450.68s]  we got our CPU that executes instructions. And so the CPU has, you know, what's the internal
[450.68s -> 458.60s]  state, and has some registers, you know, whatever, X zero or R zero, blah blah blah blah,
[458.60s -> 464.28s]  and, you know, maybe the program counter, and, you know, when xv6 starts, you know,
[464.28s -> 474.44s]  the program counter contains this value OX zero zero zero zero, and that's the way,
[474.44s -> 478.12s]  you know, the CPU knows that basically, you know, look at that address to find, you know,
[478.12s -> 481.80s]  the first instruction, it looks at the first instruction, decodes the instruction,
[481.80s -> 487.72s]  and updates whatever CPU state that needs to be updated. You know, page tables have
[487.72s -> 491.56s]  sort of a component to it, you know, the addresses, you know, that the CPU
[494.52s -> 497.88s]  others, or the instructions are part of the instructions, like, you know,
[497.88s -> 503.24s]  jump to some particular address, you know, those addresses are typically, or most commonly,
[503.24s -> 509.32s]  or there will be virtual addresses, and those virtual addresses go through, you know,
[509.32s -> 514.28s]  something that's typically called the MMU, and the MMU translates into physical addresses,
[514.28s -> 523.48s]  and that is, allows us to index into either the IO part of the memory, or the VRAM part of the
[523.48s -> 532.12s]  memory. To control how this translation happens, you know, there's this SATP register,
[534.44s -> 541.32s]  which contains the root of the page table, of the current running page table,
[541.32s -> 546.12s]  and if there's zero, then there's basically no translation going on,
[546.12s -> 550.84s]  and the virtual address is literally directly a physical address, so when the processor actually
[550.84s -> 556.84s]  starts up, there's no value in SATP, and so when the program counter is at 0x0x0x,
[556.84s -> 560.92s]  you know, basically the physical address is also 0x08, and basically the CPU
[561.48s -> 566.36s]  fetches the instruction, you know, from that particular location. As soon as, you know, the
[567.32s -> 574.52s]  as soon as the SATP actually contains a non-zero value, then, you know, for example, maybe,
[575.64s -> 578.84s]  so somewhere in here, it's like we'll draw and make something, extend the picture a little bit
[578.84s -> 586.84s]  further, we know that the kernel actually has a maximum of 128 megabytes, and so basically
[586.84s -> 593.64s]  this is all free memory, from here to there is free memory, and the free memory is
[594.36s -> 597.96s]  put on the list in calloc.c, as you've seen before,
[598.84s -> 604.84s]  and so somewhere in here is also the root page table for, you know, a kernel page table,
[604.84s -> 612.92s]  so maybe here's some page, and that is the root page table directory,
[614.68s -> 619.32s]  and as soon as we load that value, you know, the address of the physical address of that value
[619.32s -> 629.08s]  into the SAP address register, so let's say this, maybe this address is 0x7FF blah blah blah,
[629.08s -> 635.72s]  something just below 120 megabytes, so that's free space of memory below that address in SATP,
[636.52s -> 644.92s]  then at that point, you know, the processor or the MMU will use the root page table to
[644.92s -> 647.88s]  actually do the translation, you know, from virtual to physical address.
[649.96s -> 653.00s]  And you can think of this, you know, this particular whole thing
[653.00s -> 657.64s]  sort of being a single sort of box, you know, integrated, you know, together.
[659.64s -> 661.88s]  Any sort of questions about this high-level picture before
[664.52s -> 668.04s]  keep them going? So one of the key points that are sort of through memory here is that the
[668.04s -> 670.44s]  page tables themselves also live in memory.
[670.44s -> 686.04s]  Okay, so let's jump then to the first part of the page table app, namely
[686.36s -> 697.16s]  part one, where you have to print the page table for the init program and,
[699.08s -> 705.40s]  and, you know, explain it basically in terms of this figure 3.4, and the figure 3.4 is right here,
[707.32s -> 712.60s]  it is figure 3.4, and this figure 3.4 shows the user address space,
[712.84s -> 716.28s]  just let's look at the user address space for a second before we go into detail.
[716.92s -> 722.28s]  So we have the text at the bottom, so this is the instructions of the program starting at address
[722.28s -> 728.84s]  zero, above the instructions are, you know, data, global variables live there, then there is
[728.84s -> 733.80s]  something called the guard page we'll talk about, and then there's the stack of the user
[733.80s -> 738.76s]  program. So the kernel has its own stack, but the user or multiple stacks, then the user
[738.76s -> 742.92s]  program has its own stack, and above again is basically what's called heap memory, you know,
[742.92s -> 749.56s]  free memory, you know, that we can, you know, get more memory using sbreak. So sbreak basically
[749.56s -> 755.16s]  points to the top of the user address space, and if we want to grow it, we call the sbreak
[755.16s -> 763.16s]  system call to grow the top of the, to grow the bottom part, you know, up into the heap.
[763.88s -> 768.44s]  We also know that at the top there from the last lecture that there are two pages,
[768.44s -> 774.36s]  two special pages, the trampoline and the tramp frame page, and the trampoline page, you know,
[774.36s -> 778.12s]  contains these instructions to basically transition in and out of the kernel, and the
[778.12s -> 785.56s]  trampoline is a convenient place, you know, to store some state, and when we jump into the
[786.76s -> 790.28s]  kernel, because we need the kernel once we've used the registers, we can't use the registers
[790.28s -> 795.00s]  because the user space program still has them in use. So I'm not going to talk much about the
[795.00s -> 800.52s]  tramp frame and the trampoline at all, but you know, there they are. Okay, so here in this
[800.52s -> 807.72s]  picture a little bit below, you know, is actually the, you know, printout from when I read it,
[808.52s -> 815.72s]  from init, that was the first successful call of exec, right, so like we know that init code
[815.72s -> 821.40s]  makes some system called exec for the program init, and just at the end of the exit we're
[821.40s -> 825.56s]  printing out that page table. And so there's a couple things that we, you know, many things we
[825.56s -> 833.08s]  can observe about this particular page table. So first of all, you know, we, in addition to
[833.08s -> 836.92s]  printing out the physical, the PGEs and the physical addresses, I'm also printing out the flags.
[838.36s -> 842.28s]  And you know, you can see here that the flag is one, and that basically says that this
[842.28s -> 849.88s]  translation or this intermediate page is valid. Similar here, that is an invalid intermediate
[849.88s -> 855.16s]  page, and these basically correspond, you know, to the level two page, and then the second one
[855.16s -> 863.56s]  corresponds to the level. Sorry, I just screwed the arrows around, so that's why I paused for a
[863.56s -> 870.92s]  second. Okay, so the top, this is the top level page table. This, you know, points to this
[870.92s -> 876.12s]  particular page table, and then this, you know, points to that particular page table.
[877.00s -> 880.60s]  Or that's the address, you know, pointing to, you know, sort of a short overhead for saying
[881.16s -> 887.00s]  that is the base address for that particular page. And as you know, each of these pages is
[887.00s -> 894.68s]  496 bytes, that's the size of the page size, and so you divide it by 64 bits, you know, you get
[894.68s -> 903.72s]  512 entries. Okay, so what you see is that the bottom part of this address space for init
[903.72s -> 911.40s]  basically has, actually all this, has three pages, only three pages. And we get a couple
[911.40s -> 918.12s]  things that we can discern from the three pages. We know that the bottom page, which
[918.12s -> 924.20s]  basically corresponds to virtual address zero, lives at physical address, you know, 87640.
[925.40s -> 929.56s]  And, you know, we know from our previous picture that is, you know, somewhere in that
[930.44s -> 934.76s]  free memory that the kernel has available to allocate pages from.
[936.28s -> 941.64s]  And the final thing we see, we see the flags. We see the flags have one F, so it means that
[941.64s -> 946.92s]  the read bit is, the val bit is set, the read bit is set, the value bit is set, the execute bit
[946.92s -> 952.92s]  is set, and view bit is set. So this page can contain both, you know, data and text,
[953.48s -> 960.12s]  and these permissions allow the user program to execute instructions from it, to read the
[960.12s -> 966.44s]  right memory, and do it from user space. Okay, so then maybe the most interesting question is,
[966.44s -> 971.16s]  like, what's up with page one? And, you know, as a hint, you know, we see that page one
[971.88s -> 980.20s]  only has F and not the view bit set. So, you know, what is page one? Anybody?
[983.24s -> 989.64s]  Is it the guard page? Yeah, it's the guard page, right? So the guard page is mapped,
[989.64s -> 995.16s]  because it has a view bit, but it has not the view bit set. So any user instruction that tries
[995.16s -> 1001.72s]  to, so if a user program runs off its stack, so the stack grows from the top down, so if it
[1001.72s -> 1006.44s]  actually has more, you know, the stack is bigger than 496 or completely full, and if then
[1007.48s -> 1011.32s]  the user program, you know, pushes something on the stack, stack going underground, it will
[1011.32s -> 1016.92s]  grow into the guard page, and because the view bit is not set, we're going to get a page fault,
[1016.92s -> 1023.16s]  or a trap into the kernel, because the MMU cannot translate to address,
[1024.28s -> 1029.80s]  to cannot address, cannot translate any addresses on the guard page to a physical address,
[1030.52s -> 1034.36s]  because the not having the view bit basically forbids the translation.
[1036.36s -> 1037.80s]  Okay, and what is then page two?
[1041.32s -> 1051.88s]  Any? Anybody? The stack? Yep, that's the stack page, and again the stack page is
[1051.88s -> 1057.40s]  444, 4596 bytes, and you know, see that's completely elaborate in terms of permissions,
[1058.76s -> 1063.00s]  it has, you know, everything. Could we set it up tighter if we wanted to?
[1063.08s -> 1069.80s]  Yeah, I think you could disable the executable bit.
[1070.36s -> 1077.64s]  Yeah, you probably could execute the X bit, that would forbid having any program code on the stack,
[1078.60s -> 1081.72s]  so if you like generate code on the fly, and put it on the stack, you know,
[1081.72s -> 1084.84s]  you wouldn't be able to execute it, you know, that's maybe probably a good thing,
[1086.44s -> 1092.36s]  and so we could have been a little bit tighter. Okay, so that's the,
[1092.36s -> 1098.20s]  basically the bottom part of this thing, the page table, so now let's look at the
[1098.76s -> 1101.88s]  remaining entries, so basically only two remaining entries,
[1103.24s -> 1110.68s]  and maybe the flat bits are the most telling part of this, so seven, you know, means, you
[1110.68s -> 1122.68s]  know, read, write, and valid, right, and so what do we think that one is? Maybe I'll do the other
[1122.68s -> 1131.64s]  one too, so B is I think 1001, so presumably X and valid, so what do we think 511 is?
[1136.76s -> 1137.24s]  Anybody?
[1141.08s -> 1144.20s]  The trampoline and tramp page?
[1144.76s -> 1148.84s]  Yeah, so the tramp, that's clear, that's the trampoline, and probably 511 is definitely the
[1148.84s -> 1154.20s]  trampoline, correct, because it has the X bit set, so we must be executing, or we're allowing
[1154.20s -> 1159.56s]  instruction to be executed from that page, so that must be the trampoline, and we're allowing
[1159.56s -> 1164.60s]  read and write, you know, to the, to that page, so that's probably the trampoline page,
[1164.68s -> 1170.36s]  because that's the one we use to restore and save, register, save, okay?
[1171.16s -> 1175.48s]  One of the good, probably the thing, the most important thing to note is there's no U-bit,
[1177.00s -> 1180.44s]  right, and so now what does that mean? That means that the user program can actually
[1180.44s -> 1187.40s]  not execute the instructions that are actually at 510 and at the trampoline page, and can't
[1187.40s -> 1191.88s]  read and write to that page, so only the kernel can execute instructions from there,
[1191.88s -> 1193.24s]  and it can only read and write.
[1195.32s -> 1198.44s]  So basically, like, the kernel is going to be doing this before
[1199.00s -> 1203.00s]  the, while still using the user page table, right, that's kind of the point?
[1203.00s -> 1206.60s]  Yeah, exactly, as we, you know, sort of, Robert explained in the last lecture, this is,
[1206.60s -> 1210.20s]  like, just for the transition from user to kernel, and before we jump to the,
[1210.20s -> 1215.16s]  before we load to the kernel page table in the SATP register, we need a little bit of,
[1215.16s -> 1218.04s]  you know, the kernel needs a little bit of memory to actually do its job.
[1218.92s -> 1223.56s]  Okay, so a couple other things that may be interesting of this picture,
[1224.84s -> 1230.20s]  yeah, so we all, you know, all these addresses, right, 87630, 862471,
[1230.20s -> 1236.84s]  these are all pages of memory in that range of the kernel memory that is not,
[1236.84s -> 1242.44s]  you know, that's basically three, right, and aren't these addresses contiguous?
[1248.84s -> 1250.68s]  In physical memory?
[1255.00s -> 1255.50s]  Anybody?
[1257.40s -> 1258.44s]  No, they don't have to be.
[1259.40s -> 1261.24s]  No, they don't have to be, and they aren't, correct?
[1261.24s -> 1265.00s]  You know, look at this, you know, 76400, it reconfigures,
[1265.00s -> 1269.88s]  then the next address would have been 876, you know, 650, right, and it isn't.
[1271.00s -> 1274.68s]  So there's no, one of the cool things about page tables is
[1274.68s -> 1278.44s]  that even though maybe the virtual address space is contiguous,
[1278.44s -> 1282.28s]  the physical address space or the physical pages that go along with the
[1282.28s -> 1285.64s]  contiguous virtual addresses are not, don't have to be contiguous,
[1285.64s -> 1288.92s]  and so this gives the kernel a lot of flexibility in terms of, you know,
[1288.92s -> 1291.72s]  allocation and freeing pages.
[1292.92s -> 1296.76s]  Okay, any questions about this part one?
[1297.80s -> 1298.84s]  I had a question.
[1298.84s -> 1299.34s]  Yeah.
[1300.28s -> 1302.60s]  Could you explain the SVRK a little?
[1303.56s -> 1308.36s]  Also, if we are going to cover it in a future lecture, then maybe we can, like, just a small...
[1308.92s -> 1312.12s]  Yeah, actually we, let me, I'm going to talk about it on Wednesday,
[1312.12s -> 1314.44s]  and it actually will be the topic of the lazy lab.
[1316.20s -> 1318.36s]  So let me maybe postpone that question a little bit to Wednesday,
[1319.00s -> 1321.64s]  and then if that's not clear enough, please ask it again.
[1322.36s -> 1323.24s]  Sounds good, thank you.
[1325.72s -> 1327.48s]  I also had a question.
[1327.88s -> 1328.38s]  Yeah.
[1329.24s -> 1335.56s]  So I remember that the book said the trampoline and the trap frame were
[1335.56s -> 1337.48s]  at the top of the address space.
[1337.48s -> 1337.98s]  Yeah.
[1339.56s -> 1347.88s]  But here it stops at, like, the first, the root page table, it indexes at 255 not 511.
[1347.88s -> 1348.92s]  Yeah, brilliant question.
[1348.92s -> 1349.40s]  Very good.
[1349.40s -> 1350.52s]  I'm glad you asked it.
[1350.52s -> 1355.40s]  I saw it in the Q&A questions, and I was planning to talk about it, but of course I've got.
[1355.40s -> 1357.16s]  So yeah, what's going on here?
[1357.48s -> 1359.40s]  Why is 255 not 511?
[1361.00s -> 1365.48s]  Yeah, we always say the trampoline lives at the top of the address space.
[1365.48s -> 1369.00s]  Well, the top of the address space, correct, is, you know,
[1369.00s -> 1374.92s]  pointed is actually entry 511, the top level directory, and it is only 255.
[1377.32s -> 1380.52s]  Anybody, any ideas why this is the case?
[1380.52s -> 1383.88s]  We said that one bit that we said we were going to use,
[1383.88s -> 1387.00s]  we actually aren't using because of sign extension problems.
[1388.04s -> 1389.64s]  And we also don't need that memory.
[1390.36s -> 1392.60s]  Yeah, so that's exactly the right answer.
[1392.60s -> 1395.64s]  So this is basically stupid technicality.
[1396.84s -> 1401.72s]  So the virtual addresses are in principle, I think, 39 bits, correct?
[1403.56s -> 1407.00s]  And when we actually in xv6 only use 38 of them.
[1408.52s -> 1414.92s]  And as a result, you know, the top of the max VA for us is basically the 255 entry.
[1415.88s -> 1421.00s]  And the reason we don't use the 39 bit is for no particular good reason,
[1421.00s -> 1424.28s]  other than that, basically, if you have the 39 bit set,
[1425.08s -> 1429.72s]  then all the remaining bits in the 64 bit address have to be once.
[1430.68s -> 1434.28s]  And so we just didn't want to deal with this problem that if we ever set the 39 bit,
[1434.28s -> 1438.52s]  we also have to set the 40, 41st, 42nd, the 43rd, et cetera, until the 64.
[1439.24s -> 1444.20s]  And so that's the explanation.
[1444.20s -> 1444.92s]  Does that make sense?
[1446.20s -> 1449.96s]  Yeah, that's a very good observation.
[1452.44s -> 1456.92s]  Sorry, I also had a question about why is the text and the data on the same page?
[1457.56s -> 1458.68s]  Ah, very good question.
[1458.68s -> 1460.52s]  Also, that seems stupid, right?
[1460.52s -> 1462.76s]  I mean, you know, why not put them on separate pages so that
[1462.76s -> 1465.16s]  you can actually set the permission that's more carefully.
[1466.04s -> 1470.60s]  Ah, the main reason we're not doing that is for simplicity.
[1471.24s -> 1475.16s]  Just makes exec more complicated and we wanted the simplest exec possible.
[1477.00s -> 1483.00s]  So real operating system would not have data and text in the same page.
[1483.00s -> 1488.04s]  In fact, you know, we have to specify if you look at the loader flag in the make file,
[1488.04s -> 1492.84s]  you'll see that it has the desk fan option and that forces is actually data and text to be
[1492.84s -> 1495.96s]  in the contiguous and not in separate pages.
[1501.32s -> 1502.52s]  Any more questions about this?
[1503.40s -> 1507.32s]  I had a follow-up question regarding the number of bits we're using.
[1508.28s -> 1510.52s]  So you said we're using just 38 bits.
[1511.32s -> 1516.36s]  Is it like the hardware still provides for us to use 39 bits,
[1516.36s -> 1520.92s]  but we are designing our operating system such that we're using 38?
[1520.92s -> 1521.42s]  Yep.
[1521.96s -> 1527.32s]  So we basically, if the machine had more RAM than 2 to the 38,
[1528.12s -> 1529.88s]  we would not be able to use that RAM.
[1530.60s -> 1533.56s]  Now we're already running with, we're assuming basically
[1533.56s -> 1537.40s]  much less memory than 2 to the 38, so it's not a big deal for us.
[1538.36s -> 1540.76s]  But the real operating system would have done better.
[1544.12s -> 1545.32s]  So just pure for simplicity.
[1546.68s -> 1549.48s]  We want to make it as easy as you, for you as possible by
[1549.48s -> 1551.08s]  reading the few lines of code possible.
[1554.76s -> 1555.26s]  Okay.
[1556.60s -> 1557.40s]  Yeah, makes sense.
[1558.60s -> 1562.36s]  Okay, so now let's switch to part two.
[1564.28s -> 1567.72s]  And so let's bring up, you know, a picture you probably have looked at a lot.
[1568.84s -> 1571.72s]  The kernel address space on the left is the
[1572.28s -> 1574.68s]  virtual address space and the right is physical memory.
[1574.92s -> 1581.80s]  You know, here are your IO devices and then from here on is, you know, DRAM.
[1584.04s -> 1589.32s]  And, you know, and basically running until what actually 128 megabytes.
[1592.28s -> 1598.04s]  For us, because we just assume that there's no more than 250 to 128 megabytes of memory.
[1598.04s -> 1600.36s]  And so this part of the physical memory is the free memory.
[1600.36s -> 1603.56s]  And from that is where the kernel,
[1604.84s -> 1606.44s]  whoops, I drew it a little bit wrong.
[1607.32s -> 1608.76s]  Let me be a little bit more careful.
[1610.12s -> 1615.16s]  So here we basically have, you know, kernel text and data.
[1616.04s -> 1619.80s]  And then, you know, this memory above, you know, is basically memory that the
[1619.80s -> 1623.80s]  kernel allocator has and from there we allocate memory for user programs,
[1623.80s -> 1626.04s]  we allocate memory for page tables, etc.
[1626.04s -> 1627.56s]  The kernel allocates everything from there.
[1628.52s -> 1629.72s]  Until it runs out of memory.
[1629.72s -> 1635.16s]  When it runs out, it gets to 128 megabytes and then it starts returning errors or system calls.
[1636.60s -> 1638.60s]  Okay, good.
[1639.16s -> 1645.64s]  So let me pull up my first part in some sense of this assignment.
[1645.64s -> 1654.52s]  And it was the part two of the assignment was just to run with or copy the kernel page
[1654.52s -> 1659.88s]  table so that every process has its own kernel page tables.
[1661.88s -> 1665.00s]  And that was the, basically, that's the assignment here.
[1665.00s -> 1667.96s]  So let me, before jumping into the code, let me actually say a couple things,
[1668.92s -> 1670.04s]  more general things about it.
[1672.04s -> 1672.84s]  So part two.
[1672.84s -> 1684.28s]  And maybe the first question really to sort of get your head around is like,
[1684.28s -> 1686.44s]  you know, in some ways, you know, this sounds trivial, right?
[1686.44s -> 1687.96s]  We already have a kernel page table.
[1687.96s -> 1691.88s]  We just have to make n copies of it, you know, for one copy for each particular process.
[1693.48s -> 1696.12s]  And you might say, well, you know, how hard can that be?
[1696.12s -> 1698.60s]  And it turns out, I think it was a little bit harder for a couple of reasons.
[1699.32s -> 1701.40s]  Some good ones, some less good ones.
[1702.84s -> 1703.72s]  Harder than it seems.
[1708.44s -> 1712.44s]  And, you know, one reason is that, you know, the xv6 code
[1714.84s -> 1717.88s]  is sort of specialized for one kernel page table.
[1726.68s -> 1729.48s]  And, you know, you saw that in KVM init.
[1730.12s -> 1736.04s]  You know, and so that makes it a little bit, you know, generalizing,
[1736.04s -> 1739.48s]  you know, a little bit of work because you actually have to modify the xv6 code.
[1742.52s -> 1745.32s]  KVM init, as you also saw, is not the full story,
[1746.20s -> 1750.04s]  you know, for building the page table for the kernel.
[1750.04s -> 1752.60s]  You know, there's also stuff in crack init
[1752.60s -> 1755.88s]  that actually adds mappings to the kernel page table.
[1755.88s -> 1758.52s]  And then there's even something in verge.io disk
[1759.88s -> 1762.36s]  that interacts, you know, with the kernel page table.
[1764.04s -> 1766.76s]  So basically, there's no one single place in the kernel
[1766.76s -> 1769.16s]  where actually the kernel page table actually is built.
[1770.76s -> 1772.60s]  Then the third reason why, you know,
[1772.60s -> 1775.40s]  this is slightly complicated is because you also have to deal with cleanup.
[1777.40s -> 1781.00s]  So there's the aspect of actually creating these copies.
[1781.00s -> 1783.00s]  But whenever time, a user process exits,
[1783.80s -> 1787.56s]  we also have to clean up those page tables that were in use
[1788.12s -> 1791.16s]  because we want to return them to the pool of free memory
[1791.16s -> 1792.36s]  so that we can use them later
[1792.92s -> 1794.60s]  so that we can keep on running processes.
[1796.28s -> 1797.00s]  So that is the...
[1799.00s -> 1801.08s]  And that makes things a little bit complicated because, you know,
[1801.08s -> 1802.44s]  we've got to be a little bit careful
[1802.44s -> 1805.16s]  in actually freeing the kernel page table
[1805.16s -> 1807.08s]  or copying the kernel page table,
[1807.08s -> 1810.04s]  which we certainly don't want to, you know, free memory
[1810.04s -> 1811.24s]  that's actually still in use
[1811.24s -> 1813.96s]  where page table entries are still in use by other page tables.
[1814.76s -> 1815.88s]  So we've got to be careful there.
[1816.60s -> 1818.60s]  And then, you know, basically,
[1819.16s -> 1823.00s]  it's easy to make a small error in your page table
[1823.00s -> 1824.52s]  when you copy those page tables.
[1825.48s -> 1826.76s]  You know, if you get a little thing,
[1827.56s -> 1829.64s]  you know, basically, you get a hard bug.
[1832.20s -> 1834.28s]  And one of the problems here is,
[1834.28s -> 1835.32s]  as I said a little bit earlier,
[1835.32s -> 1838.92s]  is the hard bug shows up much, much, much later.
[1838.92s -> 1840.20s]  You built the kernel page tables
[1840.28s -> 1842.28s]  or you built a copy of the kernel page table.
[1842.92s -> 1844.04s]  All looks fine.
[1844.04s -> 1845.64s]  You loaded in SATP.
[1846.28s -> 1848.60s]  Then even maybe the kernel runs for a little while
[1848.60s -> 1849.56s]  and then it panics.
[1850.36s -> 1852.44s]  And it turns out, you know, the reason it panics
[1852.44s -> 1854.44s]  is because, you know, you made some small mistake
[1854.44s -> 1857.88s]  in the page table a long, long, long time ago.
[1857.88s -> 1860.52s]  And so this is one reason why it makes, you know,
[1860.52s -> 1863.24s]  makes life difficult for kernel programming.
[1864.92s -> 1866.44s]  And, you know, and basically, you know,
[1866.44s -> 1868.52s]  these hard bugs basically are just time consuming
[1869.16s -> 1869.80s]  to track down.
[1870.20s -> 1881.16s]  Because at the point that the bug no happens,
[1881.80s -> 1884.60s]  that's actually not the real cost of the bug,
[1884.60s -> 1886.28s]  but the bug, you know, the real cost is,
[1886.28s -> 1887.80s]  you know, somewhere way earlier
[1887.80s -> 1889.40s]  when you set up the page tables.
[1891.72s -> 1892.22s]  Okay.
[1894.20s -> 1898.52s]  So it turns out there's two approaches to go about it.
[1898.52s -> 1899.80s]  Now I'll take this lap,
[1899.80s -> 1903.16s]  which is sort of two solution approaches.
[1906.76s -> 1908.92s]  In fact, some of you probably use the mixture of them.
[1909.96s -> 1911.08s]  The one, you know, approach,
[1911.08s -> 1913.40s]  well, I call the copy approach.
[1915.48s -> 1917.96s]  And the copy approach basically literally makes a copy
[1917.96s -> 1919.16s]  to the kernel page table.
[1919.72s -> 1924.52s]  So every time you need a new kernel page table,
[1924.52s -> 1926.60s]  you allocate pages for the page table
[1926.60s -> 1927.56s]  and you fill them in.
[1929.16s -> 1929.96s]  Et cetera, et cetera.
[1932.92s -> 1933.80s]  That's one approach.
[1933.80s -> 1939.08s]  The second approach is basically to share
[1942.76s -> 1943.80s]  the kernel page table.
[1943.80s -> 1946.28s]  And in this case, what you do is like,
[1946.28s -> 1948.52s]  instead of trying to make a literally nice,
[1948.52s -> 1950.68s]  complete copy of the kernel page tables,
[1950.68s -> 1952.52s]  you share all the entries that are basically
[1953.08s -> 1954.44s]  are going to be unmodified.
[1955.48s -> 1956.84s]  You know from the assignments
[1956.84s -> 1960.52s]  that basically anything above claimed with a flick address
[1960.52s -> 1963.16s]  is actually going to be unchanged or unmodified.
[1963.16s -> 1964.76s]  You know, there's nothing you have to load there
[1965.72s -> 1966.92s]  in part three.
[1966.92s -> 1970.28s]  So you know that basically all the entries from above zero
[1970.28s -> 1971.88s]  are normally identical.
[1971.88s -> 1973.88s]  And so you can share those entries if you will.
[1975.72s -> 1978.52s]  And so both approaches are I think perfectly fine.
[1979.64s -> 1982.68s]  It's not abundantly clear which one is the better one.
[1983.48s -> 1985.56s]  My solution to take this approach
[1985.88s -> 1990.60s]  and I don't really have a great justification for it
[1990.60s -> 1995.00s]  other than maybe, you know, partial relation is.
[1995.00s -> 1996.52s]  I didn't want to think too hard
[1996.52s -> 1998.04s]  what's in the kernel page table.
[1998.04s -> 1999.40s]  And so I figure if all the things
[1999.40s -> 2001.00s]  that are going to stay the same,
[2001.00s -> 2002.52s]  why do you do them copying them over
[2002.52s -> 2005.16s]  or copy the PTEs over
[2005.72s -> 2006.92s]  and then I don't have to think too hard
[2006.92s -> 2009.16s]  what actually is in that part of the kernel address space.
[2011.40s -> 2013.32s]  And you know, it leads to short code,
[2013.32s -> 2015.16s]  but you know, I'm not sure it actually is shorter
[2015.32s -> 2016.76s]  than for example the copy solution.
[2017.48s -> 2018.52s]  But it's important to realize
[2018.52s -> 2020.44s]  that basically there are two different ways
[2020.44s -> 2023.16s]  of going about this particular problem.
[2023.88s -> 2028.36s]  In either case, you know, whatever approach you use,
[2028.36s -> 2030.20s]  there's sort of an implementation strategy to it.
[2030.76s -> 2032.52s]  And you know, the implementation strategy
[2032.52s -> 2035.16s]  that I use for almost any kernel program
[2035.16s -> 2037.80s]  is to do everything in baby steps.
[2041.00s -> 2043.72s]  So I might have sort of a degenerative plan in my head
[2043.72s -> 2045.08s]  about how I go over the whole,
[2046.04s -> 2047.40s]  you know, all the changes that I want to make.
[2047.96s -> 2049.88s]  But once I start making the changes,
[2049.88s -> 2051.80s]  I do like one or two
[2051.80s -> 2054.20s]  and then make sure that those work first
[2054.20s -> 2056.52s]  and then, you know, keep going.
[2057.40s -> 2059.48s]  And the other thing I do is mostly,
[2059.48s -> 2061.80s]  you know, as a strategy is keep the existing code.
[2066.52s -> 2067.88s]  Don't really modify it.
[2067.88s -> 2069.40s]  Certainly not initially.
[2069.40s -> 2073.00s]  I just add code and switch to this new code
[2073.32s -> 2074.60s]  and then do little baby steps.
[2074.60s -> 2075.48s]  And the reason why I'm doing that
[2075.48s -> 2078.04s]  is so that I can easily compare all the new code
[2078.76s -> 2081.08s]  and I always have a working old solution
[2081.08s -> 2082.52s]  that I can just roll back to.
[2083.08s -> 2084.84s]  So in case some strange bug happens,
[2084.84s -> 2086.36s]  then I can go back maybe one step
[2087.80s -> 2089.64s]  and then try again and sort of figure out
[2089.64s -> 2091.32s]  like actually where my reasoning was wrong.
[2093.32s -> 2095.08s]  But basically, you know, baby steps.
[2095.64s -> 2097.80s]  Partly because these bugs are so hard to track down.
[2098.20s -> 2105.72s]  Okay, let me switch to my code.
[2107.40s -> 2110.04s]  So maybe start in vm.cd.
[2110.84s -> 2115.64s]  So here's the existing KVM in it.
[2116.68s -> 2119.16s]  And actually, can everybody see the code?
[2122.52s -> 2123.02s]  Okay.
[2123.98s -> 2128.94s]  And the assignment that we were asked for directly
[2128.94s -> 2130.94s]  is to make a copy of it.
[2130.94s -> 2135.74s]  And the way I do that is, let me see where that is.
[2147.66s -> 2149.74s]  So here's my UVM create.
[2149.74s -> 2151.18s]  That was just a boring part.
[2151.26s -> 2153.50s]  It's basically creating, allocating
[2153.50s -> 2155.02s]  to the top level page directories
[2155.02s -> 2157.02s]  or the L2 page directory.
[2157.66s -> 2161.02s]  And then here's my KVM,
[2161.02s -> 2162.86s]  the sort of equivalent of KVM in it.
[2163.98s -> 2168.14s]  So I get the top level page directory here in this site.
[2168.70s -> 2174.06s]  Then basically, I copy the top 511 entries
[2174.06s -> 2175.50s]  from the kernel page table
[2175.50s -> 2178.38s]  that was already set up by KVM in it.
[2179.02s -> 2183.50s]  And so that gives me most of the kernel page table.
[2183.50s -> 2187.50s]  And then I just have to map in all the devices
[2187.50s -> 2193.42s]  that live in the zero entry
[2193.42s -> 2194.70s]  because the zero entry is the entry
[2194.70s -> 2195.74s]  that we're going to modify later,
[2195.74s -> 2198.06s]  or we're going to map the user pages into the bottom.
[2199.34s -> 2201.26s]  And there's a couple of devices that live in that
[2201.26s -> 2202.14s]  in the zero entry.
[2202.14s -> 2204.38s]  And those devices need to be added
[2204.38s -> 2210.62s]  to this process page table, kernel page table.
[2211.98s -> 2215.26s]  And so maybe as I go back to this picture earlier,
[2219.18s -> 2222.46s]  so let me put up the kernel page table again.
[2226.70s -> 2228.54s]  Maybe this picture is as good as any.
[2229.26s -> 2231.10s]  So if you think about my solution,
[2232.06s -> 2234.22s]  and basically these entries
[2234.86s -> 2239.02s]  I just share with the existing kernel page table.
[2239.02s -> 2243.02s]  So I don't have to allocate any L1 or L2 or L0 levels.
[2243.98s -> 2245.02s]  They already exist.
[2245.02s -> 2246.30s]  And the only thing I do
[2246.30s -> 2249.02s]  is basically literally copy that PTE entry.
[2249.90s -> 2254.62s]  And so only the bottom part or the bottom entry,
[2254.62s -> 2257.50s]  that is the piece I need to actually rebuild
[2257.50s -> 2260.14s]  or copy explicitly instead of copying PTEs.
[2260.70s -> 2264.22s]  And that covers the bottom one gigabyte,
[2264.94s -> 2265.90s]  the address space.
[2267.18s -> 2269.90s]  One page covers 496.
[2274.46s -> 2278.30s]  And so this entry covers 512.
[2279.74s -> 2281.10s]  This is the two megabytes,
[2282.14s -> 2283.50s]  and this is one gigabyte.
[2284.14s -> 2286.06s]  And basically only one of these entries
[2286.06s -> 2286.78s]  I need to fill in.
[2286.78s -> 2290.30s]  Does that make sense?
[2293.26s -> 2295.10s]  So returning to my code here.
[2295.10s -> 2299.02s]  Basically, that's all I do in KVM3.8.
[2301.10s -> 2306.70s]  And in KVM3, it's maybe not maybe clean the solution,
[2306.70s -> 2308.70s]  but I take full advantage of the knowledge
[2308.70s -> 2311.66s]  that I don't have to do anything from the entries
[2311.66s -> 2314.62s]  one to 511 in the top page level directory.
[2314.62s -> 2316.54s]  The only entry I can do anything about
[2316.54s -> 2319.74s]  is the bottom entry of the kernel
[2319.74s -> 2321.18s]  of the top level directory.
[2321.82s -> 2324.54s]  That points to one L1 entry
[2324.54s -> 2327.34s]  and the .1ln1 tree I just need to go through
[2327.90s -> 2332.22s]  and free all the L2 or L0 entries.
[2332.86s -> 2334.70s]  And then the int, free the L1
[2334.70s -> 2335.82s]  and then the kernel page table.
[2337.26s -> 2341.02s]  And so that's my KVM3 and my KVM create.
[2341.90s -> 2344.22s]  So that allows me to create a kernel page table
[2344.22s -> 2348.86s]  for the process and then free it when we're done.
[2350.22s -> 2351.18s]  Sorry, quick question.
[2351.74s -> 2353.66s]  Could you explain again the reasoning
[2353.66s -> 2357.26s]  for only using one to 512 and not zero?
[2358.30s -> 2358.80s]  Yes.
[2359.98s -> 2363.50s]  Okay, so maybe the easy thing to do actually,
[2363.50s -> 2365.18s]  let me go back to the picture here.
[2368.38s -> 2370.14s]  So we have our kernel address space, correct?
[2375.10s -> 2380.14s]  And this is 0x8000, blah, blah, blah.
[2380.14s -> 2384.86s]  And what entry does 0x00 fall into
[2384.86s -> 2386.38s]  for the top level page directory?
[2394.06s -> 2397.98s]  It's like the zero of entry?
[2398.62s -> 2400.22s]  No, not the zero of entry, correct?
[2401.34s -> 2402.70s]  So what do we know?
[2402.70s -> 2404.54s]  So the zero entry, like this entry,
[2404.54s -> 2406.38s]  how much does it cover, this entry?
[2410.30s -> 2412.94s]  How much address space does the bottom entry
[2412.94s -> 2415.26s]  of the zero entry of the top level page directory cover?
[2418.14s -> 2424.54s]  I think Clint, it's free between zero and 0x2000.
[2425.66s -> 2428.22s]  Yeah, okay, so maybe I think you're absolutely
[2428.22s -> 2428.78s]  in the right direction.
[2428.78s -> 2431.58s]  So the bottom entry covers one gigabyte, correct?
[2432.78s -> 2435.42s]  And we know, so like maybe I can,
[2438.86s -> 2440.30s]  so the bottom entry is one gigabyte,
[2441.58s -> 2447.10s]  which in that one gigabyte is the Clint and the Plick,
[2447.10s -> 2451.74s]  correct, UART and the VirtIO disk, I believe.
[2453.58s -> 2456.06s]  And then current base actually already sets up
[2456.06s -> 2459.02s]  in a higher entry, right?
[2459.02s -> 2460.54s]  And we could compute it if we wanted to.
[2460.54s -> 2464.70s]  We can take 0x8880, correct, shift at 12,
[2464.70s -> 2466.06s]  shift at nine, shift at nine.
[2466.62s -> 2469.90s]  And I think, I don't remember at the top of my head
[2469.90s -> 2473.02s]  what it is, but we could try to figure it out
[2473.02s -> 2474.70s]  if we want to, actually, maybe we'll try.
[2476.06s -> 2478.30s]  This is always very risky to do in lecture,
[2478.30s -> 2484.14s]  but it's a DDB, I need to,
[2484.14s -> 2487.90s]  so let's go to layout,
[2488.86s -> 2489.82s]  here's current base.
[2497.74s -> 2501.98s]  So we could print, so shift is 12.
[2504.30s -> 2507.66s]  That gives us the offsets, oops.
[2514.14s -> 2515.34s]  Okay, that is that.
[2515.34s -> 2520.86s]  So then we'll shift that guy, I think you could say this,
[2522.70s -> 2527.50s]  nine, I guess is the next entry, one more.
[2529.26s -> 2530.14s]  So this is entry two.
[2531.58s -> 2532.38s]  Does that make sense?
[2534.14s -> 2536.38s]  So we go back to our picture here.
[2539.18s -> 2542.14s]  Basically current base is entry two
[2542.94s -> 2546.06s]  in the top-level page directory, all right?
[2546.06s -> 2549.82s]  So, and we know from the third part of the assignment
[2549.82s -> 2551.50s]  that basically we don't really have to worry
[2551.50s -> 2554.14s]  about anything above PLIC.
[2555.74s -> 2559.34s]  And all that stuff falls actually in entry zero.
[2561.66s -> 2565.10s]  Okay, so that answered the earlier question.
[2566.86s -> 2567.74s]  Yes, thank you.
[2567.90s -> 2573.58s]  Okay, so now the only thing that we have left to do
[2573.58s -> 2580.54s]  is double check where we are gonna call this function KVM create.
[2581.26s -> 2587.58s]  And that's gonna be in alloc proc.
[2598.14s -> 2602.38s]  So you're aware of the new process initialized,
[2602.38s -> 2605.34s]  and I'm sure all of you do this exactly the same way.
[2605.34s -> 2607.42s]  You know, you entry, you allocate,
[2607.42s -> 2609.98s]  we declare a field in the proc structure
[2609.98s -> 2611.66s]  and basically the result of it is actually
[2611.66s -> 2613.02s]  what we stick in the current page table.
[2615.10s -> 2616.78s]  And then the only other thing that we have to worry about
[2616.78s -> 2618.54s]  is of course we need to use the page table.
[2619.66s -> 2621.90s]  And so we need to look at the scheduler
[2622.46s -> 2624.94s]  and basically the assignments more or less told you what to do,
[2625.82s -> 2629.42s]  which is, you know, before you switch to that user process,
[2629.42s -> 2631.58s]  you need to switch the kernel page tables,
[2631.58s -> 2633.58s]  which basically means loading, you know,
[2633.58s -> 2636.22s]  this process kernel page table in SAPP.
[2636.86s -> 2639.34s]  And then when you're done switching
[2640.62s -> 2641.90s]  and running the other processing,
[2641.90s -> 2643.58s]  you come back from running the other process
[2643.58s -> 2645.58s]  and you're gonna go back and run the scheduler,
[2645.58s -> 2648.22s]  you gotta switch back to the main kernel page table,
[2648.86s -> 2650.78s]  the D kernel page table,
[2650.78s -> 2652.78s]  because that's the one that actually is used by scheduler.
[2655.02s -> 2656.86s]  And why do we need to do this switch?
[2656.86s -> 2657.82s]  Why is this important?
[2664.70s -> 2665.18s]  Anybody?
[2667.74s -> 2669.74s]  So it picks the right kernel page table,
[2670.78s -> 2671.74s]  because the SAT,
[2671.74s -> 2673.90s]  when you go to the page table entries,
[2673.90s -> 2677.82s]  it picks the right page table to pick from.
[2678.62s -> 2678.94s]  Yeah.
[2678.94s -> 2683.58s]  Hold on a second.
[2688.38s -> 2688.78s]  Yeah.
[2688.78s -> 2689.82s]  So, okay.
[2689.82s -> 2691.34s]  So when we're stuck,
[2692.06s -> 2693.90s]  again, let me another way to ask this question.
[2694.70s -> 2696.30s]  When is this kernel page table freed?
[2699.90s -> 2701.58s]  When the user process is done.
[2701.58s -> 2702.94s]  So if we didn't switch,
[2702.94s -> 2706.38s]  that would mean that we could be using a page table
[2706.38s -> 2708.38s]  of a process that was being freed.
[2708.38s -> 2710.38s]  So the page table was freed.
[2710.38s -> 2712.14s]  So we don't want to be dependent on a process
[2712.14s -> 2713.66s]  or not running right now.
[2713.66s -> 2714.30s]  Yeah, exactly.
[2714.30s -> 2714.62s]  Correct.
[2714.62s -> 2716.06s]  We can never free the page table
[2716.06s -> 2717.42s]  of a process that's currently running.
[2718.86s -> 2720.30s]  So we have to have some plan,
[2720.30s -> 2722.46s]  you know, to actually free processes
[2722.46s -> 2724.06s]  at the point that they're not running anymore.
[2724.78s -> 2727.42s]  And the wait system call is our way out.
[2727.42s -> 2728.14s]  Basically, you know,
[2728.14s -> 2730.14s]  wait calls looks if there's any children
[2730.14s -> 2731.90s]  that are not, you know,
[2731.90s -> 2733.26s]  that can be cleaned up
[2733.26s -> 2734.30s]  and then it cleans them up.
[2736.22s -> 2737.42s]  But that means that basically,
[2738.62s -> 2741.18s]  when the parent process things over process,
[2741.18s -> 2742.30s]  we're going to make absolutely sure
[2742.30s -> 2745.58s]  that, you know, that the page table
[2745.58s -> 2747.58s]  that is loaded in the SAP register
[2748.22s -> 2749.74s]  is not, you know, one of the,
[2750.38s -> 2751.90s]  it's not that process,
[2751.90s -> 2752.70s]  you know, page table
[2752.70s -> 2753.90s]  we're actually currently free.
[2754.94s -> 2755.74s]  Now, it could be the case
[2755.74s -> 2757.18s]  that there's no process running at all.
[2757.18s -> 2757.98s]  Correct.
[2757.98s -> 2758.86s]  And so the scheduler
[2758.86s -> 2760.22s]  basically has his own page table
[2760.86s -> 2761.50s]  so that, you know,
[2761.50s -> 2763.10s]  all processes can actually be cleaned up.
[2767.10s -> 2767.82s]  Does that make sense?
[2768.38s -> 2775.10s]  Sorry, I had a question.
[2776.14s -> 2780.06s]  Is it that you're mapping the CLINT
[2780.94s -> 2783.98s]  into the new process kernel page tables?
[2783.98s -> 2784.46s]  Yeah, I do.
[2785.98s -> 2786.30s]  Why?
[2787.98s -> 2789.42s]  Because I think the assignment said,
[2789.42s -> 2792.62s]  like, I only have to use a process
[2792.62s -> 2795.98s]  who will not be bigger than the client.
[2795.98s -> 2796.38s]  What's that?
[2797.26s -> 2799.66s]  Well, I'm at both the CLINT and the CLINT.
[2800.54s -> 2800.94s]  Correct.
[2800.94s -> 2803.26s]  And so, but I think the assignment told us that,
[2803.82s -> 2805.26s]  so what is the lowest one?
[2805.26s -> 2809.42s]  I think the CLINT is the lowest one.
[2809.42s -> 2810.70s]  And basically the assignment told us
[2810.70s -> 2812.38s]  that the user process will not be bigger
[2812.38s -> 2815.42s]  than the CLINT address.
[2818.86s -> 2819.74s]  Just to make it easy.
[2819.74s -> 2820.54s]  You know, we could have done,
[2821.26s -> 2822.70s]  we wanted to make it as easy as possible
[2822.70s -> 2823.34s]  for you, CLINT,
[2823.34s -> 2824.86s]  and you could have done better
[2824.86s -> 2825.50s]  if you wanted to.
[2827.26s -> 2830.30s]  But, you know, that was the simplest thing to do.
[2831.34s -> 2832.94s]  So you have to make a few modifications
[2832.94s -> 2833.58s]  to get it to work.
[2839.50s -> 2840.38s]  Any further questions?
[2841.82s -> 2842.46s]  I have a question.
[2843.66s -> 2848.54s]  Is it possible that you can copy 0 to 512
[2848.54s -> 2850.14s]  and then every time you switch,
[2850.14s -> 2851.42s]  so you're still using, like,
[2851.42s -> 2853.42s]  the global root page table,
[2853.42s -> 2856.62s]  except you only copy the first root page table,
[2857.42s -> 2859.66s]  and every time you switch a process,
[2859.66s -> 2863.66s]  you copy over the user's addresses over to...
[2864.22s -> 2864.94s]  Yeah.
[2864.94s -> 2865.98s]  Is that possible to do?
[2866.86s -> 2868.54s]  In principle, I think you could do something like that.
[2868.54s -> 2870.30s]  So instead of basically doing
[2870.30s -> 2871.90s]  when you allocate a process in three,
[2871.90s -> 2873.74s]  you could do it through dynamically
[2873.74s -> 2874.78s]  during the scheduler switch.
[2877.66s -> 2878.94s]  It seems complicated.
[2881.82s -> 2882.94s]  That might be more costly
[2883.02s -> 2884.38s]  because that means that every time
[2885.10s -> 2886.78s]  you switch between two processes,
[2886.78s -> 2888.38s]  you may have to make a copy of parts
[2888.38s -> 2889.98s]  of the kernel page table.
[2891.58s -> 2892.46s]  And so that might be,
[2893.26s -> 2894.14s]  in terms of performance,
[2894.14s -> 2895.02s]  not an ideal thing.
[2895.82s -> 2897.82s]  The assignment didn't really say anything about it.
[2899.02s -> 2901.82s]  You might have timed out a user test if you did that.
[2906.22s -> 2907.10s]  Okay, I was just wondering
[2907.10s -> 2908.22s]  because I tried that approach
[2908.22s -> 2910.30s]  and it was a bad experience.
[2910.30s -> 2911.82s]  I was wondering if that was possible.
[2911.82s -> 2912.46s]  I can imagine.
[2912.94s -> 2915.42s]  But I think it's, in principle, possible.
[2915.42s -> 2916.94s]  You could usually allocate a new page table
[2916.94s -> 2918.70s]  correctly and switch every time.
[2919.82s -> 2921.18s]  And it frees when you switch out.
[2926.78s -> 2927.74s]  I don't think it's the simplest,
[2927.74s -> 2929.66s]  but principle is possible, I think.
[2931.26s -> 2933.26s]  Perhaps there's some other modifications to xv6.
[2936.62s -> 2937.42s]  Any other questions?
[2942.94s -> 2946.06s]  The only other change that you, of course,
[2946.06s -> 2947.82s]  have to make is a user trap ret.
[2947.82s -> 2950.14s]  You've got to make sure that actually you run
[2950.14s -> 2953.18s]  with the processes kernel page table.
[2955.26s -> 2960.22s]  Okay, so let's switch then to, I guess, part three.
[2964.06s -> 2968.06s]  Switch back to actually here.
[2968.46s -> 2976.14s]  Okay, so that's maybe not a helpful picture.
[2976.14s -> 2978.46s]  Let me see here.
[2978.46s -> 2979.34s]  Okay, so part three.
[2985.66s -> 2987.26s]  So basically what our plan is,
[2987.26s -> 2990.86s]  is we have our kernel page table as before.
[2991.74s -> 2995.18s]  And we have the click entry sitting somewhere.
[2995.42s -> 2996.78s]  And what we're going to do is,
[2996.78s -> 3000.54s]  we're going to use everything below click
[3000.54s -> 3005.34s]  to actually store the user page table.
[3005.34s -> 3007.18s]  Where we're going to map the user page table
[3007.18s -> 3009.66s]  into all the user program actually
[3009.66s -> 3012.78s]  in the kernel page table in the bottom.
[3016.38s -> 3017.50s]  And that's the goal.
[3017.50s -> 3019.50s]  And of course, the first question to ask,
[3019.50s -> 3021.34s]  you know, why the heck do that?
[3021.34s -> 3022.22s]  Why the heck do that?
[3023.98s -> 3026.22s]  Is there any, you know, any advantage of it?
[3026.22s -> 3030.38s]  And, you know, I think maybe the most easy way
[3030.38s -> 3033.98s]  to see why it might be interesting
[3033.98s -> 3035.82s]  is to compare the copy ins,
[3037.18s -> 3038.86s]  the new copy in and the old copy in.
[3040.70s -> 3045.50s]  So what happens in copy in, correct?
[3045.50s -> 3049.02s]  When copy in copies data from the user space
[3049.02s -> 3050.46s]  into the kernel address space.
[3051.34s -> 3055.74s]  And, but, you know,
[3055.74s -> 3058.94s]  if the kernel does not have the user address space mapped,
[3060.46s -> 3063.98s]  basically the kernel has to do this page at a time.
[3065.34s -> 3067.34s]  Because the pages in virtual address space
[3067.34s -> 3068.14s]  might be continuous,
[3068.14s -> 3070.22s]  when the physical address space, they're not continuous.
[3070.78s -> 3072.70s]  And basically what the kernel does in copy in,
[3072.70s -> 3075.66s]  it basically translates the virtual user address
[3076.38s -> 3078.30s]  into a physical address.
[3079.26s -> 3082.54s]  Since the kernel has all physical memory mapped
[3082.54s -> 3084.14s]  with an identity mapping,
[3084.14s -> 3089.10s]  the physical address is also a valid kernel virtual address.
[3089.10s -> 3092.06s]  And then it basically moves whatever part of that page,
[3092.06s -> 3093.58s]  physical page that needs to be copied,
[3093.58s -> 3094.54s]  that actually copies it.
[3095.82s -> 3097.90s]  And so if a data structure, for example,
[3097.90s -> 3100.94s]  spans to a page boundary,
[3100.94s -> 3102.86s]  like for example, this is info struct
[3102.86s -> 3104.30s]  that we used in lab two,
[3104.30s -> 3107.10s]  maybe it spans a page table boundary,
[3107.18s -> 3108.94s]  it will copy like maybe, you know,
[3108.94s -> 3111.58s]  some bytes from the first physical page,
[3111.58s -> 3114.06s]  and then some bytes from second physical page.
[3115.90s -> 3116.62s]  Does it make sense?
[3119.98s -> 3121.90s]  So that's what the current copy in does.
[3121.90s -> 3123.34s]  And the goal was to basically to get them
[3123.34s -> 3124.38s]  to a new copy in,
[3125.74s -> 3128.14s]  where the kernel program didn't really have to worry about
[3131.26s -> 3132.54s]  the physical layout,
[3132.54s -> 3134.70s]  basically off the user address space.
[3135.58s -> 3136.94s]  And in this new copy,
[3136.94s -> 3138.70s]  and you see, we basically do nothing,
[3138.70s -> 3140.62s]  literally nothing else than just copy
[3143.02s -> 3145.42s]  from the user addresses straight into the kernel.
[3146.38s -> 3148.38s]  And we don't really have to call walk anymore
[3149.02s -> 3150.54s]  because we can rely on the,
[3150.54s -> 3152.06s]  we set up the page tables correctly,
[3152.06s -> 3154.38s]  now the page table hardware will do the walk for us.
[3156.62s -> 3158.06s]  And so that was the goal.
[3160.46s -> 3162.06s]  And so that makes life for kernel programmers
[3162.06s -> 3163.10s]  a little bit easier.
[3163.10s -> 3164.54s]  Are there any other advantages
[3164.70s -> 3165.42s]  to this approach?
[3172.94s -> 3174.62s]  Is it also more performant
[3174.62s -> 3177.18s]  because the hardware is gonna do the walking
[3177.18s -> 3178.06s]  and not software?
[3179.02s -> 3179.98s]  Yeah, well, one way to,
[3180.54s -> 3182.30s]  I think there are performance implications for sure.
[3183.10s -> 3184.54s]  One way to think about this is that
[3185.10s -> 3186.46s]  in copy in, correct,
[3186.46s -> 3188.22s]  if the kernel data structure
[3188.22s -> 3190.86s]  or the data that we copied from user space
[3190.86s -> 3193.18s]  to kernel space is big,
[3193.42s -> 3196.22s]  we have to do that page at a time, right?
[3196.22s -> 3199.02s]  In every page, we have to call this function walk adder
[3199.02s -> 3200.70s]  and then does the internal walking.
[3201.66s -> 3205.66s]  And so this might have to be reasonable expenses, expensive.
[3206.70s -> 3207.66s]  What are examples?
[3207.66s -> 3210.14s]  Are there examples of where the kernel copies
[3210.14s -> 3212.70s]  a lot of data from user space?
[3220.38s -> 3222.14s]  Which system call might copy
[3223.10s -> 3224.46s]  a ton of data from user space?
[3226.46s -> 3226.70s]  Right?
[3227.50s -> 3227.98s]  Yeah, right.
[3228.70s -> 3233.42s]  Right, you can give an arbitrary buffer
[3233.98s -> 3235.10s]  of an arbitrary size
[3235.66s -> 3238.38s]  and the kernel may have to copy this
[3238.38s -> 3240.22s]  into, for example, the file system
[3240.94s -> 3241.82s]  or into a pipe.
[3243.02s -> 3244.86s]  And so that could be reasonable expensive.
[3247.98s -> 3248.70s]  The other thing,
[3248.70s -> 3249.82s]  what is another advantage?
[3250.38s -> 3251.02s]  If the user,
[3252.38s -> 3254.14s]  we didn't explore that in this assignment,
[3254.14s -> 3255.50s]  but we assumed we could have,
[3256.30s -> 3257.42s]  what another advantage?
[3260.94s -> 3264.30s]  So if you think about some of this code here, correct,
[3264.30s -> 3266.94s]  when it takes your structure out of user space,
[3266.94s -> 3269.74s]  it copies the whole structure into kernel space.
[3270.78s -> 3273.42s]  If user space is mapped into the kernel page table,
[3273.42s -> 3274.30s]  do we have to do that?
[3274.30s -> 3280.78s]  For example, if we have to update
[3280.78s -> 3282.78s]  one field of the structure.
[3294.70s -> 3297.90s]  So if the data structure is just mapped
[3297.90s -> 3299.34s]  into the kernel address space,
[3299.34s -> 3301.74s]  then we can just read and write
[3301.74s -> 3302.78s]  with stored instructions
[3302.78s -> 3303.98s]  through that particular data structure
[3303.98s -> 3305.42s]  and we can just update one field.
[3306.30s -> 3307.50s]  Unlike what the kernel now does,
[3307.50s -> 3309.82s]  basically it copies the structure
[3309.82s -> 3311.98s]  from kernel space to user space
[3311.98s -> 3314.14s]  and then maybe back out using copy out.
[3315.10s -> 3318.30s]  So if we map the user space into the user program
[3318.30s -> 3319.50s]  into the kernel address space,
[3319.50s -> 3322.22s]  we can just manipulate it much more freely
[3322.22s -> 3323.98s]  than the way we do it now.
[3326.46s -> 3328.22s]  Does that make sense in terms of motivation?
[3328.94s -> 3331.82s]  Why many kernels actually have this particular structure
[3331.82s -> 3336.22s]  where they map the bottom part of the user,
[3337.18s -> 3338.86s]  where they map the user programming
[3338.86s -> 3340.38s]  to the bottom part of the kernel address space?
[3347.18s -> 3347.68s]  Okay.
[3352.70s -> 3354.38s]  Okay, so let's see.
[3354.94s -> 3355.98s]  Let me look at my code
[3357.26s -> 3359.02s]  and see if you understand what actually
[3361.90s -> 3362.22s]  I did.
[3362.22s -> 3363.98s]  So there's basically one function
[3363.98s -> 3365.10s]  that is the key, correct?
[3366.30s -> 3368.38s]  Assuming we're building somewhere valid,
[3368.38s -> 3369.42s]  a user page table
[3369.42s -> 3370.54s]  and then we just have to map
[3371.98s -> 3373.98s]  the entries from the user page table
[3373.98s -> 3375.18s]  into the kernel page table
[3376.54s -> 3378.46s]  or the processes kernel page table.
[3378.46s -> 3380.06s]  So that function can map user
[3380.06s -> 3381.10s]  that exactly does this.
[3382.70s -> 3383.82s]  And it's pretty boring,
[3385.02s -> 3386.30s]  but there are a couple of interesting points
[3386.30s -> 3388.14s]  to point out in work.
[3388.94s -> 3392.86s]  Yeah, that's the same sort of interface
[3392.86s -> 3396.46s]  as a UV analog or whatever,
[3396.46s -> 3399.18s]  where basically if you go from the old size
[3399.18s -> 3401.02s]  up to the new size page at the time,
[3403.10s -> 3407.10s]  you find the UPT,
[3407.10s -> 3412.14s]  you know, the pointer to the PTE
[3412.14s -> 3413.66s]  for that particular virtual address
[3413.66s -> 3416.06s]  in the user page table, right?
[3416.06s -> 3417.18s]  So if we look at the,
[3419.74s -> 3421.02s]  if you look at the picture
[3421.02s -> 3422.78s]  that we had a little while back,
[3422.78s -> 3424.54s]  look at this picture,
[3425.58s -> 3427.26s]  basically what this is going to return, correct,
[3427.26s -> 3429.50s]  is we're going to walk the user page table
[3429.50s -> 3430.94s]  and we'll find, for example,
[3430.94s -> 3433.50s]  the PTE that maps, you know,
[3433.50s -> 3434.86s]  that particular virtual address.
[3434.86s -> 3436.30s]  When we get a basically pointer
[3436.30s -> 3438.06s]  to that entry into the page table.
[3442.54s -> 3443.74s]  So that's what walked us.
[3444.46s -> 3447.18s]  And so if the thing is mapped,
[3447.18s -> 3448.86s]  if it's not zero,
[3448.86s -> 3449.90s]  that must mean, you know,
[3449.90s -> 3450.30s]  it is there,
[3450.30s -> 3452.30s]  so this is just a check to double check
[3452.30s -> 3453.58s]  that actually the mapping is there.
[3455.10s -> 3456.22s]  We check this is valid,
[3456.78s -> 3459.10s]  just like there's debugging help in some sense,
[3459.66s -> 3461.26s]  it should be the case that it is valid.
[3462.30s -> 3463.34s]  And then I do the same thing
[3463.34s -> 3464.30s]  in the kernel page table.
[3465.02s -> 3466.38s]  I look up that virtual address
[3466.38s -> 3468.94s]  also in the processes kernel page table,
[3468.94s -> 3471.90s]  but I call this time walk with one.
[3471.90s -> 3474.06s]  So this actually allocates intermediate pages
[3474.06s -> 3474.78s]  if necessary.
[3477.10s -> 3478.78s]  And then once I got the pointer
[3478.78s -> 3480.46s]  to the kernel PTE,
[3480.46s -> 3482.22s]  I just copied the user PTE
[3482.22s -> 3483.26s]  into the kernel PTE.
[3485.26s -> 3486.46s]  So probably a lot of you
[3486.46s -> 3487.98s]  called map pages,
[3489.18s -> 3491.58s]  but I just copied directly the PTE
[3491.58s -> 3492.94s]  into the kernel PTE.
[3492.94s -> 3493.58s]  And of course, you know,
[3493.58s -> 3495.18s]  I got to disable some bits,
[3495.18s -> 3496.38s]  which we'll talk about in a second,
[3497.42s -> 3500.54s]  but the essence of it is
[3500.54s -> 3502.14s]  I just copied the kernel page,
[3502.14s -> 3503.18s]  the PTE entries.
[3504.54s -> 3505.42s]  And so that means that,
[3505.42s -> 3505.98s]  for example,
[3505.98s -> 3507.66s]  all the physical memory
[3507.66s -> 3508.94s]  for the user program
[3508.94s -> 3509.98s]  is just shared
[3509.98s -> 3511.18s]  between the kernel space
[3511.18s -> 3512.86s]  and the user space.
[3516.06s -> 3517.74s]  Okay, does that make sense?
[3521.90s -> 3523.82s]  Okay, so in terms of switching up some bits,
[3524.62s -> 3527.58s]  I'm a little bit more than necessary,
[3527.58s -> 3529.18s]  but I switch off the execute bit,
[3530.14s -> 3531.58s]  I switch off the write bit,
[3531.58s -> 3533.42s]  this copying only needs to read,
[3534.14s -> 3535.26s]  it never writes to it
[3535.26s -> 3536.46s]  or copy out does that.
[3537.02s -> 3539.98s]  And then you have to switch up the U-bit
[3541.26s -> 3542.22s]  and that is
[3543.58s -> 3545.42s]  sort of risk-wise specific thing
[3545.42s -> 3547.18s]  that if you're running in kernel mode
[3548.30s -> 3551.50s]  and the U-bit is set in the PTE entry,
[3551.50s -> 3553.26s]  the kernel can actually not
[3553.26s -> 3554.46s]  access that particular page.
[3554.46s -> 3557.82s]  And in fact,
[3557.82s -> 3558.14s]  you can,
[3559.42s -> 3560.86s]  this is almost a choice,
[3560.86s -> 3561.98s]  you can also actually program
[3561.98s -> 3563.42s]  the RISC-V hardware
[3563.42s -> 3565.10s]  and basically in kernel mode
[3565.10s -> 3566.62s]  it ignores the U-bit.
[3568.78s -> 3571.98s]  And but xv6 doesn't do that
[3572.94s -> 3574.46s]  and so you have to switch up the U-bit.
[3578.70s -> 3579.82s]  Any questions about the U-bit
[3579.82s -> 3581.18s]  because the required number of U
[3581.18s -> 3581.90s]  are asked about it?
[3584.46s -> 3591.02s]  So is this done just to make sure
[3591.02s -> 3593.82s]  that the kernel doesn't do anything bad
[3593.82s -> 3595.42s]  to user memory?
[3596.06s -> 3597.26s]  Yeah, so why is this the case?
[3597.82s -> 3598.70s]  Is this, you know,
[3598.70s -> 3599.58s]  so the question is like,
[3599.58s -> 3600.78s]  is this for debugging reasons
[3600.78s -> 3601.58s]  or is there sort of an
[3601.58s -> 3602.94s]  isolation reason to do so?
[3603.74s -> 3605.82s]  And I think this is mostly
[3605.82s -> 3606.78s]  for debugging reasons
[3607.74s -> 3608.54s]  because the kernel is
[3608.54s -> 3610.30s]  in full control anyway, right?
[3610.30s -> 3611.58s]  The kernel can change
[3611.58s -> 3613.26s]  to the SATP register at will,
[3613.26s -> 3614.38s]  it can disable paging
[3614.38s -> 3615.10s]  as it wants to.
[3615.98s -> 3616.94s]  So it's not like,
[3616.94s -> 3617.18s]  you know,
[3617.18s -> 3618.86s]  the user space is protected
[3618.86s -> 3619.82s]  from the kernel.
[3619.82s -> 3621.34s]  I think it's mostly to basically
[3621.98s -> 3623.10s]  help kernel developers.
[3623.10s -> 3623.58s]  For example,
[3623.58s -> 3624.86s]  in xv6 case, correct,
[3624.86s -> 3626.46s]  unmodified xv6
[3626.46s -> 3627.98s]  should never dereference
[3627.98s -> 3629.02s]  your user page, period.
[3630.70s -> 3633.18s]  And so that, you know,
[3634.54s -> 3636.06s]  basically this just helps,
[3636.06s -> 3636.38s]  you know,
[3636.38s -> 3637.90s]  if you happen to do that
[3637.90s -> 3639.10s]  by accident anyway,
[3639.10s -> 3639.34s]  you know,
[3639.34s -> 3640.70s]  you would get immediately a page fault
[3640.70s -> 3641.58s]  and, you know,
[3641.58s -> 3642.38s]  or kernel panic
[3642.38s -> 3644.38s]  and it will help the kernel debugger
[3644.38s -> 3646.30s]  or developer to debug the kernel.
[3649.10s -> 3649.90s]  Does that make sense?
[3652.86s -> 3653.66s]  Yeah, thank you.
[3655.82s -> 3657.58s]  I have a follow-up question to that.
[3659.18s -> 3660.30s]  I think the part about
[3660.30s -> 3661.98s]  the user bit makes sense.
[3661.98s -> 3663.74s]  But what about the write
[3663.74s -> 3664.70s]  and execute bits?
[3665.42s -> 3665.66s]  Yeah.
[3667.74s -> 3668.78s]  So does the execute bit
[3668.78s -> 3669.42s]  have to be on?
[3672.78s -> 3675.74s]  What does the kernel do
[3675.74s -> 3676.38s]  with this page?
[3679.82s -> 3680.86s]  It only reads from it,
[3680.86s -> 3681.10s]  right?
[3681.10s -> 3681.74s]  The only thing,
[3681.74s -> 3682.54s]  the only instruction
[3682.54s -> 3684.22s]  that basically grabs data
[3684.22s -> 3684.78s]  from this page
[3684.78s -> 3685.42s]  is this mem move
[3685.42s -> 3686.54s]  construction in copy in.
[3688.46s -> 3689.10s]  So it only,
[3689.10s -> 3689.42s]  you know,
[3689.42s -> 3690.78s]  does execute load instruction
[3690.78s -> 3693.66s]  from it or loads values,
[3693.66s -> 3693.98s]  you know,
[3693.98s -> 3695.10s]  from that particular page
[3695.90s -> 3696.54s]  and the page should
[3696.54s -> 3697.42s]  only contain data.
[3698.14s -> 3699.18s]  So there's no reason
[3699.18s -> 3700.14s]  that the kernel should
[3700.14s -> 3701.34s]  be writing to that page.
[3701.34s -> 3703.74s]  And so just to be conservative,
[3703.74s -> 3704.94s]  I disabled the write bit
[3705.58s -> 3706.38s]  and there should be
[3706.38s -> 3707.02s]  kernel should never
[3707.02s -> 3707.82s]  execute the structure
[3707.82s -> 3708.70s]  from that page.
[3708.70s -> 3709.66s]  So I disabled the
[3709.66s -> 3710.46s]  execute bit too.
[3711.26s -> 3711.74s]  And again,
[3711.74s -> 3712.70s]  this is mostly,
[3712.70s -> 3714.06s]  I think for debugging reasons,
[3714.94s -> 3715.82s]  not, you know,
[3715.82s -> 3716.78s]  isolation reasons.
[3719.98s -> 3720.94s]  Okay, makes sense.
[3720.94s -> 3721.26s]  Thanks.
[3728.62s -> 3728.94s]  Okay.
[3729.98s -> 3731.02s]  So now the only thing
[3731.02s -> 3731.90s]  we need to do is basically,
[3731.90s -> 3732.54s]  there's a bunch of places
[3732.54s -> 3734.14s]  where this code is being,
[3734.14s -> 3734.86s]  where this function
[3734.86s -> 3735.50s]  is being called.
[3736.22s -> 3737.02s]  And when you sort of
[3737.02s -> 3738.62s]  look at these places
[3738.62s -> 3739.66s]  where it's called
[3739.66s -> 3741.02s]  to understand what's going on
[3741.90s -> 3742.94s]  or how it should be used.
[3742.94s -> 3743.90s]  And I think maybe
[3743.90s -> 3744.94s]  one of the interesting ones
[3744.94s -> 3745.42s]  that, you know,
[3745.42s -> 3746.54s]  the number of you asked about
[3747.74s -> 3750.78s]  is fork and exec.
[3750.78s -> 3751.98s]  And we'll talk about both of them.
[3755.34s -> 3756.14s]  The first fork.
[3761.98s -> 3763.66s]  Uh, so here's,
[3763.66s -> 3765.50s]  uh, the call to fork
[3766.14s -> 3768.06s]  or to call to KVM
[3768.06s -> 3769.34s]  map user in fork.
[3769.98s -> 3772.78s]  Uh, and it takes the end.
[3772.78s -> 3773.42s]  So the main,
[3773.42s -> 3774.22s]  main interesting question,
[3774.22s -> 3775.90s]  I think a lot of you asked
[3775.90s -> 3778.22s]  is why does it have to be
[3778.22s -> 3779.34s]  the new process
[3779.34s -> 3780.30s]  kernel page table?
[3780.86s -> 3781.98s]  Why does it have to copy
[3781.98s -> 3783.42s]  from the new process
[3784.22s -> 3785.74s]  page table instead of
[3785.74s -> 3786.70s]  from maybe the
[3787.42s -> 3788.78s]  current process page table?
[3788.78s -> 3789.82s]  Because it's anyway
[3789.90s -> 3790.86s]  an identical, right?
[3790.86s -> 3791.34s]  Going forward
[3791.34s -> 3791.74s]  with the page,
[3791.74s -> 3792.94s]  we know since it replicates
[3792.94s -> 3796.62s]  the user part of the page table.
[3796.62s -> 3797.66s]  And so why can't we use
[3797.66s -> 3800.46s]  the current page table
[3800.46s -> 3801.02s]  from doing that?
[3804.30s -> 3805.10s]  And why?
[3805.10s -> 3806.22s]  And actually turns out the case
[3806.22s -> 3807.42s]  you actually have to use
[3807.42s -> 3809.10s]  the new, you know,
[3809.10s -> 3810.78s]  the child's user part
[3810.78s -> 3811.18s]  of the page,
[3811.18s -> 3813.82s]  the child's user page table.
[3813.82s -> 3814.78s]  And why is that the case?
[3819.82s -> 3825.42s]  Who have you noticed?
[3825.42s -> 3826.46s]  What, what bug do you get?
[3832.06s -> 3833.34s]  Wouldn't you get a remap error?
[3834.46s -> 3835.74s]  You could, um,
[3835.74s -> 3837.02s]  maybe not a remap error.
[3837.02s -> 3837.98s]  Uh, well the thing,
[3837.98s -> 3838.54s]  maybe not too,
[3838.54s -> 3838.94s]  but again,
[3838.94s -> 3839.82s]  the bug I'm thinking of
[3839.82s -> 3840.94s]  is not the remap one.
[3844.06s -> 3845.10s]  Number of you tried it out.
[3845.66s -> 3847.02s]  What was your personal experience?
[3850.78s -> 3855.58s]  Anybody on the call
[3855.58s -> 3857.26s]  who tried this out?
[3858.46s -> 3859.42s]  I remember what the
[3860.46s -> 3861.66s]  resulting experience is.
[3864.22s -> 3865.10s]  I think the resulting,
[3866.30s -> 3867.18s]  I did not have the bug,
[3867.18s -> 3867.82s]  but like I think
[3867.82s -> 3868.86s]  the resulting experience
[3868.86s -> 3870.22s]  is that somewhere in user tests,
[3871.02s -> 3871.98s]  particularly user tests
[3871.98s -> 3873.02s]  that do a lot of forking
[3873.02s -> 3874.06s]  and exiting,
[3874.06s -> 3875.02s]  you're going to get a problem.
[3876.30s -> 3876.94s]  And you get a,
[3878.94s -> 3879.42s]  an error.
[3880.38s -> 3881.10s]  And the error,
[3881.98s -> 3882.94s]  the thing that goes wrong
[3883.50s -> 3886.46s]  is that if you copy
[3886.46s -> 3887.50s]  from the parent process,
[3888.30s -> 3890.06s]  if the parent process exits
[3890.06s -> 3891.58s]  before the child processes us,
[3892.38s -> 3895.26s]  then the parent process page table
[3895.26s -> 3898.14s]  will be completely cleaned up,
[3898.14s -> 3898.38s]  right?
[3898.38s -> 3899.26s]  As we saw before.
[3899.82s -> 3902.70s]  And then the child still has pointers
[3903.50s -> 3908.78s]  to the parent process page table.
[3908.86s -> 3910.22s]  And so basically,
[3910.22s -> 3913.34s]  you're going to be basically pages
[3913.90s -> 3916.78s]  in this child's process page table
[3917.42s -> 3918.38s]  have been freed
[3918.38s -> 3919.58s]  and they're still in use.
[3919.58s -> 3920.30s]  And so that results
[3920.30s -> 3921.50s]  in all kinds of bad behavior
[3921.50s -> 3923.42s]  because the kernel
[3923.42s -> 3924.46s]  when it creates a page
[3924.46s -> 3926.30s]  actually writes ones in it everywhere
[3927.10s -> 3928.46s]  from debugging reasons.
[3928.46s -> 3929.50s]  And so now basically
[3929.50s -> 3931.82s]  you have invalid PTEs
[3931.82s -> 3934.14s]  sitting in your kernel page table.
[3936.86s -> 3937.66s]  Does that make sense?
[3938.78s -> 3949.82s]  Okay, one more thing to look at
[3949.82s -> 3953.18s]  is exec.c in my solutions
[3955.74s -> 3956.70s]  are pretty straightforward.
[3956.70s -> 3958.86s]  Always make no modifications to exec.
[3959.58s -> 3961.02s]  The only medication
[3961.02s -> 3962.22s]  we made is of course
[3962.22s -> 3964.22s]  the first thing for part one.
[3964.78s -> 3966.22s]  And then here for part three,
[3966.22s -> 3967.02s]  it's just basically
[3967.50s -> 3971.02s]  mapping the new user page table
[3971.02s -> 3972.54s]  into the kernel page table.
[3972.54s -> 3973.34s]  And exec basically
[3973.34s -> 3974.06s]  what it does is
[3974.06s -> 3975.02s]  it basically builds
[3975.02s -> 3977.66s]  a new user address space
[3977.66s -> 3979.26s]  and there's one line copies
[3979.26s -> 3980.38s]  of new user address space
[3980.38s -> 3981.50s]  into the kernel page table.
[3983.34s -> 3983.90s]  And that's it.
[3989.18s -> 3989.82s]  Any questions?
[3994.94s -> 3996.22s]  So hope in the meantime
[3997.18s -> 3998.38s]  I've answered many of the questions
[3998.38s -> 4001.42s]  that you submitted.
[4002.38s -> 4004.06s]  But we can look and see
[4004.06s -> 4005.34s]  which ones there were
[4005.34s -> 4006.38s]  a couple more questions
[4006.38s -> 4007.58s]  that I haven't talked about yet.
[4008.54s -> 4010.06s]  So maybe we can look at those
[4010.06s -> 4011.42s]  or unless you have
[4011.42s -> 4012.30s]  questions right away.
[4015.26s -> 4017.98s]  Don't we modify
[4017.98s -> 4019.66s]  the grow proc as well?
[4019.66s -> 4020.30s]  Yes.
[4020.30s -> 4021.10s]  Yeah, there are a couple more
[4021.10s -> 4022.14s]  changes that necessary.
[4022.70s -> 4026.78s]  In S break or group proc
[4030.62s -> 4033.26s]  I added a line right here
[4033.26s -> 4034.38s]  of their similar style
[4035.10s -> 4036.38s]  and there's presumably
[4036.38s -> 4037.74s]  a similar change in
[4039.58s -> 4040.70s]  I think that might be it actually
[4040.70s -> 4041.82s]  or usually in it of course
[4042.46s -> 4043.58s]  probably has a change
[4045.50s -> 4046.78s]  because you have to map
[4046.78s -> 4047.58s]  that one page
[4047.58s -> 4048.70s]  from the init code
[4048.70s -> 4051.82s]  into the kernel page table
[4052.14s -> 4052.78s]  in that process.
[4054.54s -> 4055.90s]  Because the first process is special.
[4063.42s -> 4065.34s]  Okay, so I'm going to walk
[4065.34s -> 4066.30s]  through some of the questions here
[4066.30s -> 4068.06s]  that I put them on the screen
[4068.06s -> 4069.58s]  so you can help them see them.
[4069.58s -> 4070.78s]  I said I don't have to feel
[4071.82s -> 4072.86s]  read them out completely.
[4073.50s -> 4074.38s]  I think the first question
[4074.38s -> 4075.26s]  is a question that actually
[4075.26s -> 4076.54s]  came up quite a bit
[4076.54s -> 4079.02s]  about the PTU bit flag.
[4079.58s -> 4080.78s]  I think we've covered this
[4082.46s -> 4083.34s]  we also covered the fact
[4083.34s -> 4084.38s]  that it's not malicious
[4084.38s -> 4085.90s]  but it's mostly for debugging reasons.
[4088.78s -> 4090.14s]  Similar question about like
[4090.78s -> 4092.22s]  well if you're in supervisor mode
[4092.22s -> 4093.10s]  can't you do anything
[4093.66s -> 4095.82s]  because you can switch the SATP
[4095.82s -> 4098.94s]  you can and the answer is yes
[4098.94s -> 4099.74s]  it's all possible.
[4100.46s -> 4101.50s]  So we can really you know
[4101.50s -> 4102.70s]  this UBIT is not about
[4103.50s -> 4104.70s]  the user program
[4104.70s -> 4106.06s]  being protected from the kernel.
[4106.06s -> 4108.54s]  It really has a flag
[4108.54s -> 4109.90s]  to help the kernel developer
[4109.90s -> 4110.70s]  build the kernel.
[4112.62s -> 4114.14s]  Moving on to one other specific
[4114.14s -> 4115.42s]  RISC-V question
[4115.42s -> 4116.62s]  which is why are the
[4116.62s -> 4118.06s]  interfering practical aids
[4118.06s -> 4119.02s]  why the register
[4119.02s -> 4121.66s]  should have been in a strange order
[4122.94s -> 4124.54s]  and the reason that is
[4125.98s -> 4126.86s]  I think we mentioned this
[4126.86s -> 4127.50s]  a little while ago
[4127.50s -> 4128.30s]  in the previous lecture
[4128.30s -> 4130.78s]  that there's basically
[4130.78s -> 4131.58s]  there's something that's called
[4131.58s -> 4134.06s]  the RISC-V compressed instruction set
[4134.06s -> 4135.82s]  and that has a set of pure registers
[4136.38s -> 4138.06s]  and so that you have to
[4139.02s -> 4140.46s]  have a more compact encoding
[4140.46s -> 4141.26s]  of the instructions.
[4142.38s -> 4144.86s]  And this strange ordering
[4144.86s -> 4145.58s]  reflects the fact
[4145.58s -> 4147.02s]  that they're basically two
[4147.02s -> 4148.94s]  there's the compressed version
[4148.94s -> 4150.22s]  which is some set of registers
[4150.22s -> 4150.86s]  and then sort of
[4150.86s -> 4151.82s]  the uncompressed version
[4151.82s -> 4153.02s]  which is what we're using
[4153.02s -> 4154.54s]  that has the complete set of registers.
[4155.26s -> 4157.18s]  And the compressed version
[4157.18s -> 4158.14s]  for example has
[4158.78s -> 4160.06s]  S1 and S0
[4161.10s -> 4161.98s]  S0 and S1
[4161.98s -> 4163.50s]  but not S2 for S11.
[4165.26s -> 4166.38s]  So that's the main reason.
[4171.82s -> 4175.90s]  So I think I went through
[4175.90s -> 4176.94s]  most of these questions
[4176.94s -> 4178.06s]  but if you see a question
[4178.06s -> 4179.18s]  that I should have covered
[4179.18s -> 4180.06s]  and I didn't
[4180.06s -> 4181.42s]  please interrupt me
[4181.42s -> 4182.46s]  or if you ask the question
[4182.46s -> 4183.50s]  that I haven't covered yet
[4186.78s -> 4187.90s]  please ask.
[4188.70s -> 4189.66s]  These questions are basically
[4189.66s -> 4190.86s]  in order that you submitted them.
[4196.14s -> 4196.94s]  Okay here may be
[4198.70s -> 4199.50s]  so here's our question
[4199.50s -> 4200.46s]  do operating systems use
[4200.70s -> 4201.58s]  article page tables
[4201.58s -> 4202.94s]  ever to set up the page table pages
[4202.94s -> 4203.90s]  so that the lower parts
[4203.90s -> 4204.62s]  of the hierarchy
[4204.62s -> 4205.82s]  are at least partly shared
[4206.46s -> 4208.22s]  and you're clearly starting
[4208.22s -> 4209.10s]  my solution correct
[4209.10s -> 4212.46s]  I share the entries 1 to 511
[4212.46s -> 4214.22s]  and so this is a standard trick
[4214.22s -> 4215.90s]  and lots of operating systems do that.
[4225.66s -> 4226.70s]  I think we talked a little bit
[4226.70s -> 4227.98s]  about this particular issue
[4227.98s -> 4228.62s]  that is important
[4228.62s -> 4229.58s]  that you switch
[4229.66s -> 4231.34s]  to this main kernel page table
[4231.34s -> 4232.78s]  because the scheduler
[4234.30s -> 4235.66s]  there might be no user processes
[4235.66s -> 4236.70s]  to run at all anymore
[4237.58s -> 4238.46s]  and the schedule of course
[4238.46s -> 4239.58s]  it still needs a page table
[4239.58s -> 4240.46s]  and so it runs
[4240.46s -> 4241.82s]  with the main kernel page table.
[4244.30s -> 4245.74s]  We have a question in the chat.
[4245.74s -> 4247.18s]  Yeah go ahead
[4247.18s -> 4247.82s]  what is the
[4247.82s -> 4252.78s]  let me pull up the chat
[4263.02s -> 4263.98s]  I can read it.
[4263.98s -> 4264.86s]  Yeah yeah I got it
[4264.86s -> 4265.58s]  so I think you know
[4266.14s -> 4266.94s]  because the question is
[4266.94s -> 4267.82s]  doesn't the risk
[4267.82s -> 4268.86s]  privilege specs say
[4268.86s -> 4269.90s]  that part of the separation
[4269.90s -> 4270.62s]  is to prevent bugs
[4270.62s -> 4271.74s]  that allow the user program
[4271.74s -> 4272.78s]  to make the kernel jump
[4272.78s -> 4274.38s]  to arbitrary coding user space.
[4275.50s -> 4277.02s]  Yes that's a good point
[4277.74s -> 4280.62s]  and so you know
[4280.62s -> 4281.34s]  you can debate here
[4281.34s -> 4282.94s]  whether this is an isolation property
[4282.94s -> 4284.94s]  or a kernel debugging property.
[4284.94s -> 4286.62s]  Clearly the kernel should never jump
[4286.62s -> 4288.46s]  into any part of the address
[4288.46s -> 4290.86s]  in the user address space directly
[4291.90s -> 4294.78s]  and so you know
[4294.78s -> 4296.22s]  I view this as a tool
[4296.22s -> 4297.98s]  to help the kernel debugger
[4297.98s -> 4299.10s]  catch those kinds of bugs.
[4300.86s -> 4301.58s]  And in some reason
[4301.58s -> 4302.62s]  that's why in XP6
[4302.62s -> 4305.34s]  we also disable
[4305.90s -> 4306.78s]  accepted ubit
[4307.34s -> 4308.78s]  and don't allow the kernel
[4308.78s -> 4310.14s]  to refer to any pages
[4312.14s -> 4313.26s]  that protects you know us
[4313.26s -> 4315.02s]  you know if you did reference
[4315.02s -> 4316.38s]  ever the user address
[4317.34s -> 4318.86s]  that the kernel would actually fault.
[4330.38s -> 4331.34s]  I think I accomplished
[4331.34s -> 4333.82s]  the scheduler point two.
[4341.34s -> 4342.06s]  So here's a question
[4342.06s -> 4343.66s]  how are pipes implemented in XP6
[4343.66s -> 4344.38s]  and should the change
[4344.38s -> 4345.34s]  to page table implemented
[4345.34s -> 4346.14s]  in the page table app
[4346.14s -> 4347.18s]  affect this implementation?
[4349.18s -> 4350.22s]  So pipes are basically
[4350.22s -> 4351.34s]  a buffering memory
[4351.34s -> 4351.98s]  in the kernel
[4352.54s -> 4355.02s]  and when you write to the pipe
[4355.02s -> 4356.70s]  it basically calls copy in
[4356.70s -> 4358.78s]  and copy in copies the bytes
[4358.78s -> 4360.22s]  from user space into the pipe
[4360.86s -> 4361.74s]  and in some ways
[4361.74s -> 4362.70s]  you know the whole part
[4362.70s -> 4363.66s]  of this page table app
[4363.66s -> 4365.74s]  which basically streamline that code
[4367.66s -> 4368.46s]  so that you don't have
[4368.46s -> 4369.58s]  to do many walks
[4370.30s -> 4370.86s]  when you actually
[4370.86s -> 4373.26s]  write from user space into the pipe.
[4377.10s -> 4377.98s]  This is an interesting question
[4377.98s -> 4379.10s]  I think a lot of people ask you
[4379.10s -> 4380.78s]  like why does uvm3 and freewalk
[4380.78s -> 4381.98s]  originally panic at the leaf?
[4383.98s -> 4385.58s]  And the reason is that
[4385.58s -> 4387.50s]  you know we put that panic in there
[4387.50s -> 4388.22s]  because it broke.
[4389.10s -> 4389.98s]  That was an indication
[4389.98s -> 4391.34s]  that the invariant in XP6
[4391.34s -> 4392.54s]  would be broken
[4392.54s -> 4393.34s]  and that's the invariant
[4393.34s -> 4395.10s]  for unmodified in XP6.
[4395.74s -> 4396.70s]  In this particular case
[4398.22s -> 4399.18s]  that is not true
[4399.18s -> 4401.02s]  and so you have to basically
[4401.02s -> 4402.14s]  get rid of the panic
[4402.14s -> 4403.74s]  and realize that it was not
[4403.74s -> 4404.78s]  important to panic there
[4404.78s -> 4405.98s]  or you should not panic there.
[4409.82s -> 4411.02s]  I think we talked quite a bit
[4411.02s -> 4413.34s]  about like why VM copy new is
[4413.34s -> 4414.54s]  you know why the new VM
[4414.54s -> 4415.42s]  copy is a good one.
[4419.98s -> 4423.26s]  Okay here's another question
[4424.46s -> 4425.82s]  let's say and this is maybe
[4425.82s -> 4428.46s]  an interesting one question
[4428.46s -> 4428.94s]  that was asked
[4428.94s -> 4430.30s]  it's sort of more design question
[4431.34s -> 4432.62s]  and maybe it helps to have a picture.
[4434.46s -> 4436.30s]  We look at the kernel address space
[4436.30s -> 4437.34s]  but we limit it
[4438.30s -> 4439.50s]  let's see if I can find my pen
[4440.14s -> 4442.06s]  we limited user address
[4442.06s -> 4443.66s]  user programs can be bigger
[4443.66s -> 4444.86s]  we said basically user programs
[4444.86s -> 4445.98s]  can grow through the claim address
[4445.98s -> 4446.54s]  and no further.
[4446.54s -> 4450.38s]  Let's say we actually wanted to grow
[4450.38s -> 4451.50s]  like all the way through here
[4452.54s -> 4453.50s]  how could we do that
[4453.50s -> 4455.50s]  and what what how should we change
[4455.50s -> 4457.34s]  the design that would allow that.
[4466.22s -> 4466.70s]  Anybody?
[4471.58s -> 4473.82s]  Can we remap the stuff
[4474.62s -> 4477.26s]  like clint and blic and you are
[4479.18s -> 4480.46s]  yeah where would you map out
[4480.46s -> 4481.74s]  so where if you wanted to
[4482.46s -> 4483.50s]  free up basically that part
[4483.50s -> 4484.30s]  of the address space
[4484.86s -> 4485.34s]  what could you do?
[4487.18s -> 4489.90s]  Map it to like a custom thing
[4489.90s -> 4491.82s]  before turn base and after
[4491.82s -> 4493.74s]  like before physical stop
[4493.74s -> 4495.10s]  and after current base.
[4495.98s -> 4497.66s]  Yeah or maybe actually better
[4497.66s -> 4499.10s]  if you after fish stop correct
[4499.10s -> 4500.62s]  here's a huge amount of free space
[4501.18s -> 4502.54s]  like address space is unused
[4502.54s -> 4503.66s]  correct this from here to there
[4503.66s -> 4505.42s]  is basically used for physical memory
[4505.42s -> 4506.46s]  and we basically could set up
[4506.46s -> 4507.42s]  mappings here right
[4507.42s -> 4509.42s]  for example we could put the uart here
[4511.34s -> 4512.22s]  like uart zero
[4512.22s -> 4513.82s]  we could put just above fish page
[4513.82s -> 4515.18s]  and basically set up a mapping that
[4516.30s -> 4517.90s]  maps to that particular physical address
[4518.86s -> 4521.58s]  right and then that will free up this mapping
[4521.58s -> 4523.50s]  and we could use it for user space
[4523.50s -> 4525.50s]  and so similarly we could do that for uart zero
[4525.50s -> 4526.70s]  or for virtual iodisc
[4526.70s -> 4527.82s]  you know for blic and clint
[4529.18s -> 4530.30s]  and real kernels do that
[4532.46s -> 4533.18s]  does that make sense?
[4534.30s -> 4534.80s]  Yes
[4545.34s -> 4546.78s]  Isn't that problematic though
[4546.78s -> 4550.22s]  because we want the same mapping
[4550.22s -> 4554.14s]  as the original kernel page table has?
[4557.18s -> 4558.78s]  Well we have to do it in every kernel
[4558.78s -> 4559.98s]  every kernel page table correct
[4559.98s -> 4560.78s]  if we have copies
[4560.78s -> 4562.06s]  we have to map it everywhere
[4562.06s -> 4563.18s]  at that particular location
[4563.66s -> 4569.42s]  so i'm not sure that is a
[4569.42s -> 4570.94s]  i'm not sure i answered your question
[4570.94s -> 4572.86s]  but i don't think that's a problem
[4573.66s -> 4575.90s]  i see so you you do that mapping
[4575.90s -> 4577.66s]  also in the original
[4577.66s -> 4579.02s]  yeah yeah we would have to do it
[4579.02s -> 4579.98s]  also in the original one
[4583.58s -> 4584.62s]  and that's easier
[4584.62s -> 4586.30s]  like if the old kernel uniformly
[4586.30s -> 4589.18s]  thinks about uart zero sitting at the top
[4594.62s -> 4598.14s]  yeah a number of people asked this
[4598.14s -> 4600.46s]  like why do we map the kernel stacks high up
[4605.02s -> 4606.06s]  what is the reason for that
[4606.06s -> 4607.02s]  like during proc init
[4607.02s -> 4611.18s]  where we map these kernel stacks high up
[4611.18s -> 4612.70s]  and if you did the copy approach
[4612.70s -> 4614.06s]  now you actually have to modify
[4614.78s -> 4617.98s]  proc init to actually copy over that mapping
[4619.50s -> 4621.34s]  and why is sitting it up high
[4621.34s -> 4622.38s]  in the virtual address space
[4623.66s -> 4628.38s]  anybody
[4637.18s -> 4638.78s]  is it specifically because
[4638.78s -> 4641.74s]  the stacks in RISC-V grow downwards
[4641.74s -> 4643.34s]  and so you place it high in the address space
[4643.34s -> 4645.58s]  so that it has space to grow downwards
[4645.58s -> 4647.34s]  yeah how much is the right direction
[4647.34s -> 4649.02s]  how much space does it have to grow downwards
[4653.66s -> 4656.06s]  that shouldn't matter because it's just one page right
[4656.06s -> 4656.94s]  yeah it's only one page
[4656.94s -> 4659.42s]  so what happens if you grow beyond one page
[4665.02s -> 4667.58s]  you run into the guard page correct
[4668.46s -> 4669.90s]  and the guard page is not mapped
[4670.46s -> 4672.30s]  and so the kernel will panic
[4672.30s -> 4674.94s]  which is better than actually writing over its data structures
[4676.70s -> 4678.54s]  and so the reason it's high up is
[4678.54s -> 4680.30s]  because we put a guard page below it
[4680.30s -> 4681.82s]  is there actually any physical memory
[4681.82s -> 4683.18s]  associated with that guard page
[4683.66s -> 4690.46s]  no no there's no physical memory associated with the guard page
[4690.46s -> 4691.90s]  right so that's one of the cool things
[4691.90s -> 4692.78s]  about putting it high
[4693.42s -> 4695.18s]  is that we can put a guard page below it
[4695.18s -> 4697.50s]  that actually doesn't consume any physical memory
[4700.62s -> 4702.46s]  okay does that make sense
[4705.10s -> 4708.30s]  oh so basically the kernel stack page will be
[4709.58s -> 4712.30s]  physical somewhere but the guard page won't
[4712.30s -> 4716.86s]  exactly again one of the cool things you can do with virtual memory
[4717.98s -> 4720.70s]  is it possible to overshoot the guard page
[4720.70s -> 4722.38s]  yeah that's a great question
[4722.38s -> 4725.42s]  you know what example is you know you allocated the guard
[4725.42s -> 4727.82s]  a buffer on the stack that actually
[4729.34s -> 4731.90s]  is you know way bigger than the guard page correct
[4731.90s -> 4735.02s]  and sort of goes into the next kernel stack page
[4736.22s -> 4737.74s]  and yeah then you would have a serious bug
[4738.86s -> 4740.94s]  most likely you will run into that anyway right
[4740.94s -> 4743.98s]  because you presumably would use the first entries of that buffer
[4743.98s -> 4746.06s]  and then you would get the page fault
[4748.46s -> 4749.50s]  but you could get unlucky
[4750.62s -> 4754.38s]  so this is this is not a bulletproof kernel debugging technique
[4754.94s -> 4757.98s]  but it has proven to be extremely effective
[4760.94s -> 4761.50s]  great question
[4766.78s -> 4767.66s]  any more questions
[4768.30s -> 4770.14s]  we'll go back to the list
[4771.34s -> 4772.62s]  let's see we have a couple more minutes
[4772.62s -> 4774.14s]  and then actually we have one more minute
[4775.34s -> 4776.22s]  i have a question
[4777.10s -> 4782.06s]  so once we have our kernel page tables for the processes
[4782.06s -> 4790.06s]  does it mean that in the trap code we don't need to switch page tables
[4790.70s -> 4793.98s]  i think that's a great question it's a good design question
[4794.78s -> 4796.70s]  and the answer is yes you don't need it right
[4796.78s -> 4802.46s]  because the main reason that the kernel or in the trampoline code
[4802.46s -> 4805.18s]  we go through all this trouble is because we have to copy the user
[4806.06s -> 4808.94s]  we have to switch from the kernel page table to the user page table
[4808.94s -> 4812.78s]  and the user page table or a page table doesn't have the rest of the kernel mapped
[4814.14s -> 4817.18s]  and so the downside
[4818.54s -> 4822.38s]  i'm not sure if that's true because the kernel needs to have the user block
[4823.10s -> 4824.30s]  user flag on set
[4825.26s -> 4827.10s]  yeah yeah okay so there's a couple points i wanted to make
[4828.62s -> 4833.74s]  so in principle you could simplify the tramp let me put it this way
[4833.74s -> 4837.34s]  you can simplify entry and exit if you have a single page table
[4837.34s -> 4839.98s]  that maps the user and the kernel in a single page table
[4839.98s -> 4841.82s]  because then you don't have to switch
[4841.82s -> 4845.34s]  you will have to make a couple more modifications to actually 6 to make that happen
[4846.30s -> 4847.42s]  but in principle you could
[4847.42s -> 4853.18s]  and in fact linux until very recently used you know this sort of strategy
[4853.26s -> 4859.74s]  of where the kernel and the user code are sitting in a single page table
[4860.70s -> 4863.82s]  and relying on the basically the ubit you know to make sure that the
[4867.74s -> 4870.22s]  user program couldn't actually modify any kernel pages
[4870.78s -> 4873.82s]  and the entry exit code in that case is slightly simpler
[4873.82s -> 4876.86s]  because you don't have to switch page tables when you enter or release the kernel
[4879.10s -> 4882.54s]  one thing that happened with for example the meltdown attack if you might have
[4883.90s -> 4889.66s]  heard of off this side channel attack in response to the actually side channel attack
[4890.62s -> 4893.82s]  Linux switch or has an other mode of running there are two modes of running
[4893.82s -> 4898.14s]  one is called the kppi mode and in that mode basically that reflects
[4898.14s -> 4901.58s]  basically what xv6 does you know having a separate page table
[4901.58s -> 4904.46s]  for the kernel and a separate page table for user space
[4908.70s -> 4912.14s]  wait so i still don't understand why does that work so let's say the
[4912.14s -> 4916.54s]  user process and the kernel use the same page table
[4917.10s -> 4920.70s]  if the user memory has to have the user bit set
[4920.70s -> 4923.90s]  the kernel will not be able to access that user memory right
[4923.90s -> 4931.50s]  ah okay the on so there's a this is okay on intel processors that is not the rule
[4932.14s -> 4935.90s]  so for example in intel processors if you bid a set the kernel could still write and read that
[4935.90s -> 4940.86s]  page oh that's just a risk five thing and even on the risk five you can change it
[4941.66s -> 4946.86s]  there's a bit in the s status manager that you can set the time to some bit and if you set it
[4946.86s -> 4952.70s]  then basically in kernel mode the new bit is ignored okay so you're telling me that there's
[4952.70s -> 4959.98s]  just one bit in this xemu processor that made me stay up four hours later yes i'm i'm very sorry
[4959.98s -> 4965.58s]  about that i should have realized no no it's okay it was it was a fun realization when you
[4965.58s -> 4969.90s]  reported it uh it reminded me of a bug that i actually had during the summer and it was exactly
[4969.90s -> 4978.38s]  that bug but i forgot about it so my apologies for that no no it's okay it was uh it was fun
[4978.38s -> 4983.50s]  figuring it out for sure but i i at some point i was just like okay like either i'm crazy or
[4983.50s -> 4987.10s]  something's completely wrong so i'm just gonna go to sleep and submit this piazza post and
[4987.10s -> 4991.50s]  hopefully when i wake up someone will answer and fortunately that was the case so i was able
[4991.50s -> 4998.54s]  to just continue right off when i when i did okay so i think we uh ran over time a little
[4998.54s -> 5004.86s]  bit uh but hopefully uh this was helpful and reaffirms you know uh things with virtual memory
[5004.86s -> 5008.62s]  i think it was important to do because we're going to get three more labs involved in
[5008.62s -> 5014.14s]  virtual memory and my hope is that after this one you know there's going to be easier than
[5014.70s -> 5022.54s]  this experience can i ask something very quick yeah so i'm i'm staying on so everybody wants
[5022.54s -> 5026.62s]  to stay on and ask more questions please feel free to do so if you need to go to the next class
[5026.62s -> 5033.74s]  please go so basically uh in all the labs like there's a lot of mention of like oh we this might
[5033.74s -> 5039.90s]  be useful in future labs but we never actually use any of the code that we write like what's
[5039.90s -> 5044.22s]  what's the plan on that like should we just import our code from a previous lab or
[5044.22s -> 5048.46s]  like i don't want to because what if it's buggy i don't want to you know impact the next lab
[5048.46s -> 5052.30s]  yeah so the one reason we don't do it you know why the builds labs don't build on each
[5052.30s -> 5056.38s]  other even though they could uh is to avoid basically dependencies like if you had a bug
[5056.38s -> 5061.50s]  in an earlier lab that you know was not exposed by user tests or any of our tests and but would
[5061.50s -> 5067.82s]  be exposed in that new lab then you know really painful um the reason i'm mentioning that
[5067.82s -> 5072.30s]  is that basically the real thing that's going on is we're going to do more stuff with page
[5072.30s -> 5078.70s]  tables uh so for example this lab you know looks at simplifying copy in uh later labs going to
[5078.70s -> 5085.26s]  look at uh change to s break they're basically orthogonal to the changes you made to the kernel
[5085.26s -> 5090.06s]  for this lab but you know the fact that you've been thinking about kernel page table user
[5090.06s -> 5096.06s]  pages table so much will help you plus really what helps you is the experience of debugging
[5096.06s -> 5102.14s]  these kinds of problems due to incorrect page tables okay i just wanted to make sure that
[5102.14s -> 5107.66s]  it's not like i'm missing out because i didn't copy over my xr code from lab one or something
[5107.66s -> 5110.22s]  decided to purpose that the labs are not dependent on each other
[5112.38s -> 5113.98s]  okay thanks i'll see you on wednesday
[5120.54s -> 5127.66s]  i had a follow up on my question about um mapping mapping uh using the modification from
[5127.66s -> 5137.10s]  the page table lab um would it be then needed to have the trampoline mapped in the user page
[5137.10s -> 5144.46s]  tables uh okay so i haven't really fought this through uh so if you kernels let's say the
[5144.46s -> 5148.78s]  kernel exits so let's say we have a joint one single page table as the one that you're showing
[5148.78s -> 5156.46s]  correct and we're jumping out of the kernel uh through usurette uh and our goal is to
[5157.82s -> 5161.82s]  we're still running in kernel mode so this page certainly accessible correct in the by the
[5161.82s -> 5166.94s]  kernel the guard page oh sorry not the guard page but i'm saying the trampoline page is there
[5167.98s -> 5176.38s]  and presumably uh somewhere uh we need to map maybe the
[5179.58s -> 5185.82s]  uh well the user trap train we don't really uh we have access to anyway right because we're
[5185.82s -> 5191.26s]  running kernel mode we know where it sits in the proc structure or we can get it uh so i think
[5191.26s -> 5198.94s]  we can just exit and when we return uh then the user code can just run you know we have
[5198.94s -> 5206.86s]  of course the ubit is a bit of a uh well the ubit is only set of pages that actually sit
[5206.86s -> 5211.82s]  in the user space so that's fine too so when we enter the kernel back in you know we're still
[5211.82s -> 5217.02s]  over at this page table that page table already has all the kernel now mapped and we could just
[5217.02s -> 5221.10s]  you know copy you know the registers that we need to save into the proc structure directly
[5222.14s -> 5227.10s]  instead of having to go through the uh have a separate page with the proc struct in it
[5228.22s -> 5231.18s]  so i believe the change would be pretty straightforward
[5233.02s -> 5241.66s]  okay i see thank you but you know of course i could be wrong oh yeah i'm i'm now gonna try it
[5241.66s -> 5250.06s]  now
