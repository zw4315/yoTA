# Detected language: en (p=1.00)

[0.96s -> 8.72s]  All right, anyone hear me? Good. All right, today we got Meltdown.
[10.72s -> 18.48s]  The reason why we're reading this paper is that security has kind of been a topic all along and
[19.52s -> 23.28s]  comes up a lot in the design of the kernels that we talked about in the class.
[24.24s -> 30.80s]  And as we know, the main strategy we've talked about for what it means for a kernel to provide
[30.80s -> 36.32s]  security is isolation in the sense that user programs can't read data from the kernel and
[36.32s -> 43.36s]  user programs can't read other users' data from other user programs. And the specific techniques
[43.36s -> 49.84s]  that we've seen operating systems use in order to get isolation are things like the user
[50.00s -> 56.40s]  supervisor mode in the hardware and the page tables in the hardware, as well as just sort of
[57.20s -> 61.36s]  well-designed kernel software, like the system calls are all defensive about how they
[62.56s -> 71.92s]  use user-supplied pointers. But it's worth thinking about, looking at examples of how this
[71.92s -> 76.24s]  kind of stuff goes wrong. I mean, in fact, you know, kernels try hard to provide isolation
[76.24s -> 83.92s]  and security, but there are problems that come up. And this paper is one of the most interesting
[83.92s -> 89.44s]  problems that's come up with operating system security in recent times. Meltdown has
[90.32s -> 96.64s]  came out. It was published at the beginning of 2018, so not too long ago. And a lot of people
[96.64s -> 102.56s]  like me found it surprising and actually pretty disturbing, a pretty disturbing attack on user
[102.56s -> 111.76s]  kernel isolation. It really undermined faith or sort of this very basic assumption that the
[111.76s -> 117.44s]  page tables that the hardware supplies just get you isolation, and that's the end of the story.
[117.44s -> 123.84s]  And this attack does not support that view at all. Further, it was an example of,
[125.12s -> 128.72s]  one of a number of recent examples of what's called a micro-architectural attack,
[128.72s -> 137.52s]  an attack that involves exploitation of hidden implementation details inside the CPU,
[137.52s -> 145.84s]  that are often not even known how CPUs work, but people guess, and they're able to make
[145.84s -> 152.48s]  successful attacks based on correct guesses about hidden details of CPU implementation.
[153.04s -> 159.20s]  Meltdown turns out to be fixable, and it seems to be pretty completely fixed,
[160.08s -> 165.60s]  but nevertheless, it sort of set people up to fear that there might be an open-ended supply
[165.60s -> 175.76s]  of similar micro-architectural attacks. So it's a pretty important recent event worth understanding.
[176.56s -> 187.60s]  Let me start by just laying out the basic core of the attack. We'll talk about
[187.60s -> 192.72s]  what's going on here, but this is a somewhat simplified version of the code in the paper
[192.72s -> 198.40s]  for how the attack works. The basic idea is that you're an attacker, and for one reason or
[198.40s -> 204.16s]  another, you're able to run software on some computer that has some secrets that you'd like
[204.16s -> 207.76s]  to steal. You're not allowed to directly get out the secrets, but they're in memory,
[207.76s -> 212.56s]  maybe kernel memory or another process's memory. But you've been able to run a process maybe
[212.56s -> 217.20s]  because you logged into a time-sharing machine like an Athena machine, or maybe because you
[218.00s -> 225.36s]  bought time on some hosting service. And so what the attack allows you to do is run a program
[225.36s -> 233.04s]  in which you declare a buffer in your own memory. So this buff is just ordinary user memory
[233.04s -> 239.04s]  that's accessible. You have the virtual address in the kernel of something that you're interested
[239.04s -> 249.68s]  in stealing. What I'm writing out here is sort of a mix of C and assembler, but
[249.68s -> 254.56s]  what I mean in line three is that you have the address of the kernel virtual address of the
[254.56s -> 260.16s]  data you want to steal in register one and R1. And on line three, we're just imagining that
[260.16s -> 267.12s]  this is instructions to dereference register one and load its results into register two.
[267.76s -> 271.36s]  That's the instruction that we're going to run. And then there's an instruction that
[274.00s -> 280.80s]  just gets the low bit of register two. So this attack, this particular version of this attack,
[280.80s -> 287.68s]  reads just a single bit, just one low bit of one memory location from the kernel and multiply
[287.68s -> 294.56s]  that 4,096. And since it's either zero or one, that means that R2 will end up being zero 4,096.
[294.56s -> 299.92s]  And then we simply read the contents of our buffer, which is a buffer in user memory.
[299.92s -> 307.20s]  We simply read either buffer of zero or buffer of 4,096. And that's the basic attack.
[307.20s -> 317.04s]  So one question is, why doesn't this just directly work? Like line three,
[317.04s -> 321.52s]  it's reading this kernel address. Can we just read addresses from the kernel?
[325.60s -> 330.08s]  No, no. We all have faith that the answer can't possibly be yes. We can't possibly be able
[330.08s -> 335.76s]  to just directly read from the kernel if we're in user space. And the machinery that we know
[335.84s -> 343.28s]  the CPU somehow is invoking to make this not work out is that when we use a kernel virtual address,
[343.28s -> 349.04s]  that implies a lookup in the page table. And there's permission bits in the page table.
[349.04s -> 355.20s]  And we're just assuming that the operating system has not set the flag in the page table
[355.20s -> 361.28s]  entries for kernel virtual addresses, has not set that flag that allows users to use those
[361.28s -> 367.20s]  addresses. That's the PTEU flag on the RISC-V. And that therefore this instruction must fail,
[367.20s -> 371.92s]  must cause a page fault. And indeed, if we ran this code, this instruction would cause a page
[371.92s -> 379.28s]  fault. And if we added a code after this to say print the value in register three,
[380.08s -> 383.76s]  we get a page fault on line three and we'd never get to the print statement.
[383.76s -> 386.88s]  And we'd find we couldn't directly seal data out of the kernel.
[387.84s -> 397.04s]  Nevertheless, the sequence turned out to be useful as the paper shows. One thing that
[400.88s -> 406.80s]  the paper assumes, which is no longer really true for the most part, is that the kernel is
[406.80s -> 415.04s]  mapped into every user processes address space. That is, when a user codes running
[415.92s -> 423.36s]  a full set of kernel PTEs is present in the page table, but they have the PTEU bit clear. So
[424.96s -> 430.64s]  user code will get a fault if it tries to use a kernel, a virtual address. So all those mappings,
[431.28s -> 435.92s]  at the time this paper was written, all those mappings were there when executing in user space.
[436.96s -> 441.76s]  They just couldn't be used by a user code or they cause a fault if they were used by user
[441.76s -> 447.68s]  code. And the reason why people, by operating system designers, mapped both kernel and user
[447.68s -> 454.56s]  addresses when running user code is that made system calls quite a bit faster. Because that meant
[454.56s -> 458.72s]  that when a system call happened, you didn't have to switch page tables. And switching page
[458.72s -> 465.12s]  tables usually takes time itself and also typically causes CPU caches to be flushed, which
[465.12s -> 471.52s]  makes subsequent code slower. So people got a boost by mapping both user and kernel. Mappings
[471.52s -> 478.48s]  always in user space. But this attack actually, and this attack relies on that habit.
[480.16s -> 486.08s]  Okay, so I'm going to explain what's going on here that makes this code useful, but
[486.08s -> 492.48s]  before I do that, any questions about this code fragment?
[495.28s -> 500.32s]  I was actually wondering if you could repeat what you just said about kernel to user mapping.
[500.88s -> 504.24s]  It didn't really register. Okay, let's see.
[508.48s -> 513.60s]  You know how in XV6, when you're executing, when a process is executing in user space,
[513.60s -> 518.24s]  if you look at the page table, that page table has mappings for the user addresses
[518.88s -> 523.12s]  and for like the trampoline page and the trap frame page and nothing else.
[524.08s -> 531.60s]  So that's how XV6 works. The page table that this paper assumes were different from that.
[533.04s -> 536.88s]  In the time this paper was written, most operating systems
[538.72s -> 548.08s]  would have a complete set of kernel mappings in the page table while user code was running.
[548.08s -> 556.16s]  And so all those page table entries would be there, all the kernel page table entries
[556.16s -> 561.60s]  would be there when user code was running, but since the PTEU bit was clear on each of those
[561.60s -> 565.28s]  page table entries, user code wouldn't actually be able to use a kernel virtual address,
[565.92s -> 570.08s]  but the mappings were there. And the reason for that is so that when you do a system call,
[570.08s -> 576.40s]  you didn't have to switch page tables because you do a system call into the kernel and boom,
[576.40s -> 581.28s]  you're using the same page table, but now you can use all those kernel PTEs because you're
[581.28s -> 585.44s]  in supervisor mode and that saved a bunch of time getting into and out of the kernel
[585.44s -> 591.36s]  during system calls. So everybody used that technique. Indeed, that was almost certainly
[591.36s -> 596.96s]  what Intel had in mind for how you should write an operating system. Okay, so for the
[596.96s -> 603.60s]  whole paper, that structure is assumed for the attack. Of course, getting rid of it was the
[604.32s -> 608.64s]  most immediate solution to this problem, but at the time that paper was written,
[608.64s -> 611.44s]  all those kernel mappings were present in user space.
[613.76s -> 614.40s]  Other questions?
[616.88s -> 620.48s]  So you need to know the address that you want to get.
[621.04s -> 630.56s]  Yeah, that's right, it is, and, you know, so that's a good point. You need to know the
[630.56s -> 639.52s]  kernel virtual address and that's actually maybe no joke. You might think that would make
[640.16s -> 646.56s]  the attack harder, but first of all, a point of philosophy,
[648.32s -> 652.64s]  in security, you just have to assume that the attacker has infinite time and patience.
[653.60s -> 658.00s]  If they're after some valuable secret, they are probably willing to spend like a couple of months
[658.64s -> 663.60s]  trying to steal that secret, or longer, right? Because, you know, it's going to be
[663.60s -> 667.68s]  somebody's password that like protects all kinds of valuable stuff, maybe money or,
[667.68s -> 673.76s]  you know, secret email. So that means, for example, the attacker probably has time to try
[673.76s -> 679.60s]  every single kernel address, right? Looking for whatever precious data they're after,
[679.60s -> 684.96s]  maybe a password, or the attacker may have time to like study, to look through the kernel code
[684.96s -> 689.84s]  and look through typical compiled kernels and find addresses and maybe put print statements
[689.84s -> 694.80s]  in their kernels to examine the structure of data in kernel memory until they understand
[694.80s -> 700.24s]  how the kernel works well enough to be able to get an address here.
[700.24s -> 705.76s]  Now, actually, because this game has been going on, other versions of this game,
[705.76s -> 709.84s]  of the security game have been going on for a long time, kernels actually defend themselves
[710.64s -> 717.92s]  against attacks that involve guessing kernel addresses. And one of the things
[717.92s -> 724.32s]  that was actually mentioned in this paper is this thing called kernel address space
[724.88s -> 730.72s]  layout randomization. So modern kernels actually load the kernel at a random address in order to
[730.72s -> 737.36s]  make it harder to guess kernel virtual addresses. And they did this before this,
[737.36s -> 744.32s]  long before this paper came out, because it was helpful in defeating other attacks. So this is
[744.32s -> 753.04s]  the game, but we have to assume that the attacker, in the end, the attacker will probably win this game.
[754.64s -> 760.24s]  Okay, so we'll just assume the attacker either knows a juicy kernel virtual address to look at,
[760.24s -> 765.60s]  or can guess one, or is willing to exhaustively try every address. And the paper suggests that
[765.68s -> 769.28s]  that's a plausible strategy once you have Meltdown.
[771.92s -> 778.48s]  Okay, so what's going to happen? Well, so we're wondering how can this code be
[779.28s -> 785.60s]  possibly be useful to an attacker? And the answer has to do, like if the way CPUs worked
[785.60s -> 791.92s]  was just what you read in the CPU manual, this attack clearly is nonsense. Like it'll fault it
[791.92s -> 795.76s]  at instruction three, and that'll be the end of it. But it turns out CPUs
[797.12s -> 804.96s]  work in far more complex ways than is in the manual. And the reason the attack works is
[804.96s -> 812.48s]  because of some CPU implementation details. And there's actually two main things that
[812.48s -> 818.24s]  the attack relies on. One is an implementation trick of CPUs called speculative execution,
[818.24s -> 823.36s]  which I'll talk about first. And the other implementation trick the attack
[823.36s -> 831.20s]  relies on is the way CPUs do caching. Okay, so first speculative execution.
[833.20s -> 839.84s]  Let me, I have a code example for that also. And for the moment I'm not talking about security
[839.84s -> 845.92s]  at all. The speculative execution, this stuff is just a technique to improve the performance of
[846.56s -> 856.88s]  CPUs. It's an optimization trick that CPUs use. So imagine that we're just running this code.
[857.92s -> 863.20s]  This is a somewhat contrived example, but it sort of illustrates what speculative execution
[863.20s -> 870.80s]  is all about. So we have an address and some say register zero. And just because the logic
[870.80s -> 875.52s]  of my program, the address is either valid or not valid. Maybe it contains zero if under
[875.52s -> 880.72s]  some circumstances, like we haven't initialized my data yet. So there's this,
[880.72s -> 884.08s]  we'll assume there's a valid variable that's sitting in memory somewhere.
[885.20s -> 890.72s]  And so before using the address here, this address and register zero here on line four,
[890.72s -> 894.64s]  we're going to test. We're going to load valid from memory. And we're only going to use the
[894.64s -> 899.68s]  address if valid is set to one and if valid is set to zero, we're just not going to use
[899.68s -> 903.76s]  the address at all. And if valid is set to one, then we're going to dereference the address
[903.76s -> 908.48s]  and load the data at points two into register two and add one to it. It doesn't really matter.
[908.48s -> 912.16s]  We're going to do something with that data we loaded. In this case, add one to it and
[913.12s -> 922.88s]  set register three equal to the data plus one. All right, well, in a simple CPU implementation
[924.40s -> 930.80s]  on line two here, you've got to load, valid is a variable sitting in memory in RAM.
[931.36s -> 936.08s]  And we have to issue some kind of, this is going to be, line two is going to be some sort of load
[936.08s -> 941.44s]  instruction that reads valid out of RAM. All else being equal, if we actually have to load it from
[941.44s -> 949.84s]  RAM, that'll take hundreds of cycles on our, say, two gigahertz machine. Like any load that
[949.84s -> 954.08s]  actually has to go to RAM will take hundreds of cycles. The machine, you know, can execute an
[954.08s -> 959.60s]  instruction up to an instruction every cycle. So if we actually had to wait a couple of hundred
[960.48s -> 964.64s]  cycles here, the machine would be sitting there idling for hundreds,
[964.64s -> 970.72s]  hundreds of cycles, sort of wasting its time. And because that's a significant, significant
[970.72s -> 975.68s]  slowdown, right, if everything went well, we'd be able to execute an instruction every cycle
[975.68s -> 983.44s]  instead of every couple hundred cycles. All serious modern CPUs do use something
[983.44s -> 988.80s]  called branch prediction. So this if statement is a branch. If we'd actually turned it into
[988.80s -> 993.36s]  machine instructions, we'd see there was a branch here, and it's a conditional branch, branch based
[993.36s -> 999.68s]  on this test of whether register one is equal to one. And what CPUs do is they use what's
[999.68s -> 1007.36s]  called branch prediction. That is for every branch, more or less, the CPU essentially remembers
[1007.36s -> 1012.64s]  a cache of information about each of the branches in your program, or at least each recently
[1012.64s -> 1017.28s]  executed branch, and remembers, oh, did that branch, was the branch taken or not taken?
[1018.48s -> 1024.96s]  And if the CPU doesn't have enough information to predict, so that's prediction based on the
[1024.96s -> 1030.72s]  last time you executed the branch, even if the CPU doesn't have a prediction, it may still just
[1030.72s -> 1036.96s]  go ahead and execute the instructions either that the branch takes you to or the
[1036.96s -> 1043.76s]  fall through instructions, assuming the branch wasn't taken. That is, even before the CPU knows
[1043.76s -> 1049.36s]  whether this conditional is true, it'll choose one way or the other and start executing down
[1049.36s -> 1055.60s]  that path, even though it might be the wrong path, it doesn't know yet. And so in this case,
[1055.60s -> 1061.36s]  maybe before this load completes and before the value of valid is known, the CPU may start
[1061.36s -> 1067.92s]  executing instruction four and do the load with whatever value is sitting in R0,
[1067.92s -> 1073.60s]  which may or may not be a valid pointer. And once that load yields something, maybe even add
[1073.60s -> 1079.28s]  one to it and set register three equal to that value. And then maybe a long time later, when
[1079.28s -> 1087.28s]  this load at line two finally completes, and now we know what the value of valid is, the CPU
[1087.28s -> 1094.32s]  will then know it kept track of the fact that it executed lines four and five without
[1094.32s -> 1100.48s]  really knowing whether that was proper. If valid is one, then that's fine and it just keeps going.
[1100.48s -> 1107.20s]  If valid is zero, then the CPU has enough cleverness to cancel the effects of its execution
[1107.20s -> 1113.12s]  of line four and five and restart execution in the proper place after the branch at line seven.
[1114.00s -> 1120.80s]  And this execution of code before you know whether you really should be executing it is called speculation.
[1127.36s -> 1133.20s]  And again, the point is performance. If the CPU guesses right, then it got a big head start
[1133.20s -> 1137.84s]  executing these instructions and didn't have to wait for the memory, for the expensive memory load.
[1137.92s -> 1142.00s]  Any questions about what this means?
[1148.96s -> 1155.52s]  Okay, this machinery, the hardware, the transistors in the CPU for speculation
[1156.88s -> 1162.80s]  are extremely complex. There's a huge amount going on in the CPU to make this work,
[1163.44s -> 1171.12s]  none of which is published, right? It's all Intel internal stuff, not in the instruct, not in the
[1171.12s -> 1178.56s]  machine manual. So surrounding meltdown and attacks like it is a huge amount of speculation
[1178.56s -> 1184.08s]  about what's probably going on inside the CPU in order to make such and such attack work or not work.
[1185.12s -> 1187.52s]  Okay, back to speculation though.
[1187.92s -> 1203.36s]  One thing that's going on is that in order to undo speculative, failed speculative,
[1203.36s -> 1210.24s]  mispredicted speculative execution, the machine keeps shadow versions of registers essentially.
[1210.24s -> 1214.56s]  You know, it'll assign to registers two and register three, but it's assigning kind of a
[1214.56s -> 1221.92s]  temporary registers. If the speculation succeeds, then those registers, those shadow registers,
[1221.92s -> 1226.96s]  become the real registers. If it fails, then those shadow registers are discarded, the CPU
[1226.96s -> 1232.64s]  discards the shadow registers. So these two assignments, star two and r2, would just be as if they never happened.
[1232.64s -> 1245.92s]  So in this code we need to think about what happens if register zero is a valid pointer
[1245.92s -> 1252.40s]  and what happens if it's not a valid pointer, right? If we're speculatively executing line four
[1254.88s -> 1259.76s]  and register two is a valid pointer, then it turns out the CPU will actually do the load
[1260.72s -> 1266.56s]  and load it into at least the transient version of register two. So we'll actually go out and
[1268.32s -> 1275.52s]  try to fetch what r0 points to and that will certainly work if the data r0 is
[1275.52s -> 1282.08s]  pointed to is sitting in the cache. And I don't know if the CPU will do the load
[1282.08s -> 1285.04s]  if it misses in the cache and has to load from RAM, it might.
[1285.12s -> 1291.60s]  But maybe the more interesting question for us, for this attack, is what happens if register zero
[1291.60s -> 1299.84s]  is not a valid pointer? In that case, if we're speculatively executing here,
[1302.16s -> 1307.60s]  the machine can't fault at this point because we're speculatively executing. The machine doesn't
[1307.60s -> 1313.68s]  know. It may know that r0 was an invalid, that this speculatively executed instruction
[1313.68s -> 1318.80s]  tried to use an invalid pointer, an invalid address, but it can't page fault because it's
[1318.80s -> 1327.52s]  not sure whether this execution is a correct speculative execution or a misspeculation.
[1327.52s -> 1336.64s]  And so it's only, so it can't actually raise a fault on line four until after valid, the value
[1336.64s -> 1345.68s]  of valid is known. And after this branch, this speculative predicted branch is after we know,
[1345.68s -> 1351.84s]  after the machine knows what the condition is. If the machine at line four sees
[1351.84s -> 1360.32s]  oh register zero is an invalid address and then valid turns out to be one, then, and only then,
[1360.32s -> 1365.60s]  does the machine actually generate the page fault. If r0 was an invalid address and valid
[1365.60s -> 1372.56s]  turns out to be zero, the machine does not generate a page fault. So the decision about
[1372.56s -> 1379.52s]  whether to fault is deferred possibly for hundreds of cycles until the value of valid is known.
[1379.52s -> 1384.24s]  And the technical term for the point at which we know whether an instruction
[1385.76s -> 1392.88s]  was correctly speculatively executed rather than being thrown away is called retirement.
[1395.60s -> 1403.20s]  So we say an instruction is speculative and at some point it's retired and that's when we know
[1403.20s -> 1407.44s]  it's either going to be thrown away or it's real and should be, its effect should be committed
[1407.44s -> 1415.84s]  to the visible state of the machine. And the rule is that an instruction can only be retired
[1415.84s -> 1421.20s]  if first of all it's finished executing, you know, loading memory or adding one to something
[1421.20s -> 1425.52s]  and every instruction before it that was executed before it has also retired.
[1426.16s -> 1431.52s]  So, you know, this line four can't be retired until the load of valid completes and the
[1431.52s -> 1437.44s]  condition is evaluated and only then can be retired. So if it's going to fault it may,
[1437.44s -> 1443.04s]  it's going to fault possibly hundreds of instructions after it did the memory load
[1445.36s -> 1446.88s]  or attempted to do the memory load.
[1451.68s -> 1453.92s]  And as a critical detail for this attack,
[1456.96s -> 1465.52s]  there's an even more, let's see, if the address in R0 is invalid and has
[1465.52s -> 1469.92s]  no mapping in the page table at all, then I actually don't know what happens.
[1472.88s -> 1479.20s]  If the address in R0 has a page table mapping but there's no permission for it,
[1479.20s -> 1485.20s]  that is the PTU flag is not set, then what Intel machines actually do is
[1485.92s -> 1490.40s]  load that data and assign it into the transient register two.
[1493.28s -> 1497.36s]  And where it can be used by the speculative execution of line five,
[1497.36s -> 1502.48s]  so even if R0 was an address for which we don't have permission because it's a kernel address,
[1503.28s -> 1506.96s]  we'll still see its value loaded into R3 and its value plus one.
[1506.96s -> 1513.36s]  Sorry, we won't see it but it will be loaded into R2 and that plus one into R3 and then
[1514.32s -> 1517.68s]  when this load is retired the machine will realize, aha,
[1518.80s -> 1523.68s]  that was an invalid load because the page table entry didn't allow it and so we're going
[1523.68s -> 1529.60s]  to raise a fault and cancel the execution of the subsequent instructions and cancel the
[1529.60s -> 1535.44s]  effects of this instruction, undo the modification to R2 and R3.
[1535.44s -> 1541.28s]  So in this example there's two speculations going on.
[1541.84s -> 1545.84s]  One is we're speculating about where this branch, the machine's speculating about where
[1545.84s -> 1550.00s]  this branch went and just saying, oh, you know, one where the other, I'm just going to give
[1550.00s -> 1555.28s]  that a shot speculatively. In addition, there's speculative execution after each load.
[1555.28s -> 1559.44s]  We're essentially speculating about whether the machine is speculating
[1560.16s -> 1565.84s]  about whether that load could complete successfully and in the case of a load,
[1565.84s -> 1572.08s]  Intel machines always just go on, you know, if data could be provided because it's in the cache
[1572.08s -> 1576.72s]  and at least the page table entry exists, permissions or not, the machine will speculatively
[1576.72s -> 1584.00s]  continue to execute and only on retirement of the load will it actually generate the fault
[1584.00s -> 1592.08s]  and that'll cause the speculation to be cancelled. All right, any questions about this stuff?
[1599.84s -> 1605.44s]  I'm a little confused about the second speculation which is loading R0 into R2.
[1606.32s -> 1613.52s]  So does that mean that like the value of R0 is loaded into R2 and the flags are checked later?
[1614.32s -> 1622.80s]  Yes, yes, so what happens, yes, that's exactly right. So what actually happens
[1622.80s -> 1630.64s]  is that during the speculative phase, whatever it is that R0 points to, if there's sort of an,
[1631.28s -> 1637.52s]  if it points to anything, if R0 points to anything, then the data at that memory location
[1637.52s -> 1646.56s]  will be loaded into R2. Later, when this load is retired and it turns, then the permissions will
[1646.56s -> 1654.32s]  be checked and if we didn't have permission to do that load, then all subsequent instructions
[1654.32s -> 1660.32s]  effects will be cancelled, like all these modifications registered will be undone and
[1660.32s -> 1665.76s]  a fault will be raised with the state of the machines and registers as they were just before
[1665.76s -> 1670.08s]  instruction forward. Okay, that's interesting. Yes.
[1672.72s -> 1678.64s]  Yes, I also have a question. Is there no possible way to restrict the CPU
[1678.64s -> 1682.24s]  from checking permissions before doing a speculative load?
[1687.04s -> 1692.00s]  You mean is there a way to cause the machine to do the check before the load?
[1692.00s -> 1699.20s]  Yes, I guess more concretely, like the only reason this is a problem, or one of the ways,
[1699.20s -> 1706.24s]  is that we're just loading a page that if we can be aware that it has permissions that
[1706.96s -> 1711.52s]  are gonna like be bad somehow, right, we're actually accessing a page that we don't,
[1711.52s -> 1717.28s]  we shouldn't be able to access. Can the speculative execution be cancelled given that
[1717.28s -> 1726.80s]  we can read these permissions? Yes, well there's two answers. One is
[1726.80s -> 1732.56s]  that's not the way the Intel chips CPUs actually worked and the other answer is yes,
[1732.56s -> 1738.72s]  it would have been, I believe, it would have been easy for them to have done and enforced
[1738.72s -> 1744.16s]  the check even for speculative loads so that even in speculation register two would never have been
[1744.16s -> 1753.20s]  written. So, and indeed it turns out that you may not have noticed the paper mentioned that the
[1753.20s -> 1762.08s]  meltdown seems not to work on AMD CPUs. Even though AMD CPUs, the instruction manual
[1762.08s -> 1766.24s]  is the same as for Intel CPUs essentially, that is they run the same instruction set and the
[1766.24s -> 1772.32s]  instructions mean the same things. The attack doesn't work on AMD CPUs and it's widely believed
[1772.32s -> 1780.56s]  that the reason is that AMD CPUs, even when speculatively executing, if you don't have
[1780.56s -> 1790.00s]  permission to read this address, won't even speculatively load the value into R2 and that's
[1790.00s -> 1797.36s]  why the attack didn't work on AMD CPUs and recent Intel CPUs apparently have adopted that
[1797.36s -> 1805.04s]  approach and they actually won't won't speculatively load if they don't have permissions.
[1806.40s -> 1812.16s]  And as far as I know there's no particular sacrifice and performance. I think the information
[1812.16s -> 1819.76s]  was all there but for just, I don't know, just say maybe a few gates they decided only to apply
[1819.76s -> 1825.36s]  it on on retirement. That's very interesting.
[1828.40s -> 1831.84s]  Yeah, I mean let me just warn you this there's a lot of guesswork here
[1833.92s -> 1835.52s]  and I believe what I'm saying is true but
[1837.52s -> 1840.88s]  in Intel and AMD have not been very revealing about what's going on.
[1840.88s -> 1851.20s]  Okay, there's some terminology here that's important. What you read in the
[1851.84s -> 1856.00s]  manual for the CPU that says oh you know an add instruction takes two registers and adds them
[1856.00s -> 1861.60s]  and puts them in a third, that stuff, that aspect of the design is called architectural
[1861.60s -> 1872.16s]  and sort of the advertised behavior, the advertised behavior of the machine.
[1873.20s -> 1876.00s]  And so the advertised behavior of the machine is that if you load from an address you don't
[1876.00s -> 1878.64s]  have permissions for you get a page fault period and you're not allowed to load
[1879.68s -> 1884.48s]  and that's in distinction to what the machine's actually doing which is called micro
[1885.44s -> 1887.20s]  architectural.
[1889.68s -> 1893.12s]  That is you know actually the machine has speculative executions doing all these crazy
[1893.12s -> 1899.92s]  things without telling you and the intent of the CPU designers when they design all these
[1899.92s -> 1904.72s]  complex micro architectural optimizations is that they be transparent that yeah you know it's doing
[1904.72s -> 1910.00s]  all this stuff internally but it looks the results you get from programs are the same results you
[1910.00s -> 1914.56s]  would get from a simpler machine that just did the straightforward thing that was in the manual.
[1915.36s -> 1920.96s]  Right, they're intended to be transparent and so you know for example at some level what
[1920.96s -> 1925.60s]  Intel is doing here is transparent yeah maybe they don't check for permissions when you
[1925.60s -> 1930.80s]  when you do the memory load but if there was a problem on retirement it's going to undo all
[1930.80s -> 1935.84s]  these effects and so you'll never see that memory you weren't supposed to see so gosh that
[1935.84s -> 1939.84s]  looks just like what the manual said you're not allowed to load stuff you don't have permission
[1939.84s -> 1943.52s]  for. So the distinction is
[1946.24s -> 1951.36s]  a lot of what this attack is playing on that it's knows the attack knows a lot about what's
[1951.36s -> 1960.24s]  going on inside. Okay other questions about speculative execution?
[1960.24s -> 1973.36s]  Okay I'm gonna put that aside for a moment and talk about another piece of micro architecture
[1980.64s -> 1983.20s]  and that's caches.
[1986.40s -> 1989.68s]  And these again I mean everybody knows caches are there but you know they're supposed to be
[1989.68s -> 1992.16s]  more or less more or less transparent so.
[1996.24s -> 1999.76s]  And let me draw a picture of the caches I think are relevant to the cache structure that I think
[1999.76s -> 2006.00s]  is most relevant to Meltdown. So first of all you have the core which is the
[2007.44s -> 2012.00s]  you know the part of the machine that parses instructions and has registers and
[2012.00s -> 2018.56s]  has an addition unit and a division unit and you know whatever the sort of execution part
[2018.56s -> 2023.84s]  of the machine and then whenever it needs to do a load or a store
[2026.56s -> 2033.92s]  it talks to the memory system and the memory system has a bunch of caches so
[2033.92s -> 2040.96s]  in particular in the machines we're talking about there's a data cache called the level one
[2041.84s -> 2051.36s]  data cache that is maybe you know 64 kilobytes in size or something not very big but it's extremely
[2051.36s -> 2057.52s]  fast if you if the data you need is in the L1 cache that you get back to you in a couple
[2057.52s -> 2062.00s]  of cycles and the structure of the L1 cache it has a bunch of lines what are called lines
[2063.04s -> 2069.04s]  each of which holds probably 64 bytes of data the lines are indexed it's a table really the
[2069.04s -> 2076.40s]  cache the lines are indexed by virtual address if a virtual address is in the cache then
[2077.44s -> 2082.96s]  the cache holds the data for that virtual address and in addition as it turns out
[2085.44s -> 2091.68s]  it's believed that a L1 cache entry contains a copy of the permissions taken from the
[2092.24s -> 2094.72s]  page table entry that corresponds to this virtual address
[2095.52s -> 2102.96s]  um so and there's a whole this is a table when the when the core issues a load instruction
[2103.52s -> 2107.92s]  um the first thing that happens is that the hardware looks in the L1 cache and see if there's
[2107.92s -> 2116.56s]  a cache entry whose virtual address matches the requested uh the address we're trying to load
[2116.56s -> 2121.20s]  from it and if so the we can just the machine just returns this data from the cache we're
[2121.20s -> 2128.24s]  done very quickly if it is not in the L1 cache then the next step is that we um the rest of the
[2128.24s -> 2133.04s]  memory system is is in is addressed with physical addresses so we at this point we're going to need
[2133.04s -> 2138.64s]  a physical address if we missed the L1 cache this translation lookaside buffer is a cache of
[2138.64s -> 2144.96s]  page table entries um so we're going to look up the virtual address that the program issued in
[2144.96s -> 2149.52s]  the translation lookaside buffer it may not be there in which case now we got a lot of work
[2149.52s -> 2154.96s]  to do because we've got to load the relevant page table entry from memory um but let's assume
[2154.96s -> 2161.04s]  we hit in the translation lookaside buffer and we can now um get the needed physical address
[2161.04s -> 2167.60s]  typically there's another cache another much bigger cache that's physically indexed indexed
[2167.60s -> 2173.12s]  with the physical address and so we might now that we have the physical address we can look
[2173.12s -> 2180.16s]  in this cache and if we miss there then we have to send the physical address after the ram system
[2180.16s -> 2183.84s]  takes a long time but when we finally get data back then we can populate the level two cache
[2185.04s -> 2188.08s]  and populate the level one cache with the stuff we got back from ram
[2188.64s -> 2191.76s]  and return the uh the data back to the core
[2194.80s -> 2197.68s]  um so this is caching
[2198.16s -> 2201.04s]  um
[2203.52s -> 2210.56s]  just uh by the by the hidden L1 cache probably takes a few cycles hidden the L2 cache
[2210.56s -> 2214.56s]  probably takes a dozen or two cycles and a miss that requires you to go around probably
[2214.56s -> 2222.48s]  takes you a couple hundred cycles these cycles are you know less say half an anasecond on a
[2222.48s -> 2227.84s]  two gigahertz machine so it's extremely advantageous to have caching i mean you
[2227.84s -> 2231.76s]  would if you didn't have caching you would be sacrificing a factor of a couple of hundred
[2231.76s -> 2236.80s]  in performance so these are just absolutely critical to decent performance these caches
[2240.24s -> 2249.44s]  now um this cache is is the L1 cache um well both it turns out both both these
[2249.44s -> 2255.12s]  caches can contain if we're running in user space both these caches and the operating systems
[2255.12s -> 2259.92s]  meltdown was aimed at both of these caches can contain both user data and kernel data
[2260.72s -> 2263.44s]  the L2 cache can contain kernel data because um
[2265.92s -> 2271.92s]  uh it's physically addressed and there's just no problem um the L1 cache is a little bit
[2271.92s -> 2276.48s]  trickier it's virtually addressed when we change page tables the contents of the L1
[2276.48s -> 2280.88s]  cache are no longer valid because we change page tables that means that the meaning of
[2280.88s -> 2285.04s]  virtual address is changed so you'd have to flush the L1 cache if you change page tables
[2285.04s -> 2291.92s]  although there's you know more complex tricks that can allow you to avoid that um and so but
[2291.92s -> 2297.04s]  the fact that these operating systems in the days of this paper didn't change page tables when
[2297.04s -> 2302.72s]  changing between user space and kernel space because both were mapped meant that uh data we
[2302.72s -> 2307.12s]  didn't have to flush the L1 cache and that meant the L1 cache would have both user and
[2307.12s -> 2311.52s]  kernel data in it and that it made some calls even faster right if you call a system call
[2311.52s -> 2317.20s]  on the system call returns there's still going to be uh likely still to be useful user data in
[2317.20s -> 2322.08s]  the cache because we never changed we never changed page tables or changed the meanings
[2322.08s -> 2328.72s]  of these addresses um anyway so there's likely to be kernel data even though you're running
[2328.72s -> 2334.32s]  in user space there's likely to be kernel data in the L1 cache and it's these permissions which
[2334.32s -> 2339.84s]  are copied out of a TLB copied out of page table entries that tells the machine that oh even
[2339.84s -> 2344.96s]  though the data is in the cache you're not allowed to see it and raise raise a page fault
[2344.96s -> 2358.00s]  um so this is a good time to mention that even though the intent of
[2358.00s -> 2365.92s]  micro architectural optimizations is they be completely transparent um that can't possibly
[2365.92s -> 2371.68s]  be true because the whole point of these micro architectural optimizations is almost always
[2371.68s -> 2378.24s]  to improve performance and so they will you're guaranteed to be at least visible in terms of
[2378.24s -> 2382.56s]  performance that is you could tell if your machine has a cache or not because if it doesn't
[2382.56s -> 2389.44s]  have a cache it'll run a couple hundred times slower right um in addition you can tell whether
[2389.44s -> 2393.68s]  the data you're trying to fetch if you're capable of measuring time accurately enough
[2393.68s -> 2398.72s]  and you do a load you can tell if the load returned in a couple cycles the data must have
[2398.72s -> 2405.12s]  been cached if the load returned after a hundred times that the data probably was had to be loaded
[2405.12s -> 2410.56s]  from ram and so the differences are profound and if you can measure time to you know a few
[2410.56s -> 2414.96s]  nanoseconds or even tens of nanoseconds you can tell the difference so in the performance
[2414.96s -> 2420.48s]  level this micro architecture is absolutely not transparent and all the things we talk about
[2420.48s -> 2426.64s]  like branch prediction caches or whatever all that stuff is at least indirectly visible
[2427.60s -> 2432.88s]  through timing um and so of course many people even though the
[2434.48s -> 2441.44s]  micro architectural design is sort of in any detailed level secret to intel it's just their
[2441.44s -> 2446.16s]  private business how they implement this in fact it's all along been of extremely intense
[2446.16s -> 2452.40s]  interest um to a lot of people because it affects performance a lot so uh compiler writers
[2452.40s -> 2458.48s]  for example know a lot about micro architecture because the most many many compiler optimizations
[2458.48s -> 2464.72s]  are implicitly exploiting people's good guesses about what the machine's actually doing inside
[2465.84s -> 2473.36s]  and indeed the CPU does uh manufacturers publish optimization guides that reveal some of the
[2473.36s -> 2479.20s]  micro architectural tricks but they rarely go into much detail um certainly not enough detail
[2479.20s -> 2486.56s]  to really understand sort of exactly why meltdown works um so the micro architecture stuff is sort
[2486.56s -> 2491.60s]  of sit somewhere between supposed to be transparent and visible and hidden and sort of
[2493.76s -> 2496.00s]  partially you know certainly
[2498.32s -> 2501.84s]  a lot of people are interested and a lot of people know all kinds of random things about it
[2501.84s -> 2511.52s]  okay so the reason why this cache stuff is interesting for meltdown first of all any
[2511.52s -> 2522.24s]  any questions about about caching okay um let me talk then
[2525.04s -> 2531.20s]  about the sort of main way that the paper uses caching the paper talks about this technique
[2531.20s -> 2542.80s]  called flush plus flush and reload um and what flush and reload is up to is that it's answering
[2542.80s -> 2552.88s]  the question um did a particular piece of code um use the memory at a particular address
[2553.84s -> 2561.60s]  and it's not directly a security exploit because it only works for memory that you can get at
[2562.16s -> 2565.44s]  so if you're user coding and you have some memory that's your memory and you're allowed
[2565.44s -> 2571.36s]  to use it you can and you call one of your own functions or you know then you you'll be able
[2571.36s -> 2576.24s]  to tell you can use flush and reload to tell whether the function your function that you just
[2576.24s -> 2583.04s]  executed used your memory um you can't directly use this attack or it's not an attack you can't
[2583.04s -> 2588.96s]  use this technique to figure out if some other process use that process as private memory
[2591.12s -> 2598.48s]  although because processes sometimes share memory yeah you may still be able to do well
[2599.36s -> 2602.56s]  the way way to put it is you can only find out about memory you're allowed to access
[2603.52s -> 2610.88s]  okay so it's answering the question uh did a particular function uh use this memory so step one
[2614.32s -> 2618.64s]  is we're gonna flush supposing we were interested in address x
[2621.20s -> 2626.56s]  we want to flush the cache we want to make sure the cache doesn't contain the memory at
[2626.56s -> 2632.24s]  location x and it turns out that for our convenience intel supplies an instruction
[2633.36s -> 2643.04s]  um called cl flush and you give it an address and it will um get rid of it'll ensure that
[2643.04s -> 2649.28s]  um that location is not cached in any of the caches and so that's super convenient um
[2650.56s -> 2654.00s]  even if the machine didn't provide this instruction though it turns out there's ways
[2654.00s -> 2660.00s]  of getting rid of stuff from the cache like for example if you know the cache holds 64 kilobytes
[2660.00s -> 2666.64s]  then it's likely to be the case that if you load 64 kilobytes of random memory
[2666.64s -> 2673.04s]  you know just load instructions that um those will be loaded into the cache and after you've
[2673.04s -> 2677.52s]  loaded 64 kilobytes of new data into the cache everything that used to be in it must
[2677.52s -> 2683.52s]  be gone because the cache can only hold 64 kilobytes or whatever it may be so even without this
[2683.52s -> 2689.52s]  nifty instruction you can still flush everything in the cache then step two is you know you're
[2689.60s -> 2695.44s]  interested in whether some particular piece of code uses um the data at x you just call that
[2695.44s -> 2704.72s]  code whatever it is and it does what it does maybe uses x maybe doesn't um and now you want
[2704.72s -> 2708.96s]  to tell if x is actually in the cache because if it is since you flushed it from the cache
[2708.96s -> 2713.44s]  if it's in the cache now it must be that f causes to be loaded unless something else is
[2713.44s -> 2718.88s]  going on um so you need to you want to do a load but you want to know how long the load takes
[2719.52s -> 2723.12s]  so but you know we're only talking about nanoseconds like five nanoseconds versus
[2723.12s -> 2729.60s]  100 nanoseconds here how can we measure time that accurately that's a tough assignment however
[2729.60s -> 2735.60s]  again um the CPUs come to our aid they in fact provide an instruction which gives you
[2735.60s -> 2742.72s]  cycle granularity time and it's called rdtsc so we're just gonna
[2742.80s -> 2749.44s]  execute the rdtsc instruction which tells us essentially the number of cycles that have
[2749.44s -> 2754.64s]  elapsed since the machine cycles that have elapsed since the machine started and since
[2754.64s -> 2759.20s]  it's probably you know two gigahertz machine that means that the precision we have here is
[2759.20s -> 2764.64s]  half a nanosecond which is pretty small um and now we're going to load
[2764.64s -> 2774.96s]  um we're just going to say junk equals star x you know we're going to load the
[2776.16s -> 2779.36s]  data at location x um i get the time again
[2783.28s -> 2786.48s]  and look at the difference right b minus a
[2786.48s -> 2794.88s]  if b minus a is you know five or six or seven or something uh that means that the
[2794.88s -> 2801.04s]  this load hid in the cache and that means that this function used the data if b minus a is 150
[2802.00s -> 2810.24s]  then uh that means that x wasn't in the cache and you know that may that probably means that
[2810.24s -> 2815.04s]  f that may mean that f didn't ever use x now that's not quite that cut and dry because
[2815.04s -> 2820.88s]  f might have used x and then use something else um that conflicted with x in the cache
[2820.88s -> 2825.20s]  and caused x to be kicked out of the cache again but you know for simple situations
[2825.84s -> 2834.56s]  um a very large value b minus a means f didn't use it and a small value b minus a means that
[2834.56s -> 2842.48s]  f did use that data so this is not an attack yet because again we have to be able to
[2842.80s -> 2846.16s]  uh access this memory so this is our memory
[2848.32s -> 2851.36s]  any questions about flush plus reload
[2860.32s -> 2866.32s]  all right i think that's all the preliminaries let's go back to meltdown
[2866.40s -> 2874.08s]  so this is a more full version i showed you a sort of core meltdown at the beginning
[2874.08s -> 2882.48s]  this is a more uh complete meltdown um and so we actually now have i added the flush and reload
[2882.48s -> 2890.64s]  part again we're gonna declare this buffer and um the idea is that uh depending on we're gonna
[2890.88s -> 2897.68s]  just fetching one bit from the kernel um and we're gonna multiply that one bit by 4096 so we're
[2897.68s -> 2902.56s]  hoping to use flush plus reload to see that either buff of zero is in the cache or buff
[2902.56s -> 2910.96s]  of 4096 is in the cache and the reason for the large separation there um is that apparently
[2910.96s -> 2915.84s]  the pref this hardware has a prefetcher in it so if you load one thing from memory
[2915.84s -> 2919.76s]  it'll like load the next couple things up from memory to the next couple cache lines
[2919.76s -> 2924.48s]  and so we can't have the two different cache lines that we're going to apply flush and reload
[2924.48s -> 2929.92s]  to be to be particularly close need them to be far enough apart that even prefet hardware prefetching
[2929.92s -> 2937.04s]  won't cause confusion so we put them a whole page apart the flush part now that we just
[2937.04s -> 2945.84s]  call the cl flush instruction to make sure that the relevant parts of our buffer are not cached
[2946.80s -> 2947.36s]  um now
[2958.32s -> 2963.84s]  we're exploiting this line seven is not maybe may or may not be necessary but what um
[2964.80s -> 2967.68s]  what's going on here is we're exploiting this sort of the um
[2968.32s -> 2971.92s]  um we're going to be exploiting the gap in time
[2973.68s -> 2981.76s]  uh between you know we we uh we're doing this loaded at line 10 it's a load of a kernel address
[2981.76s -> 2987.12s]  so it's gonna fault um but we're hoping to be able to execute another couple of instructions
[2987.12s -> 2992.88s]  speculatively before the this instruction is retired and before it actually raises the
[2992.88s -> 2999.12s]  fault and cancels these instructions right if the fault if this load would be to retired say
[2999.12s -> 3004.48s]  at this point that would be too early for us because it's going to turn out we actually need
[3004.48s -> 3010.56s]  line 13 to be speculatively executed in order to complete the attack so we want to make sure
[3010.56s -> 3017.52s]  that this load isn't retired for as long as possible in order to delay the fault and to delay
[3017.52s -> 3023.52s]  the speculative cancellation now we know instructions aren't retired until all previous
[3023.52s -> 3030.48s]  instructions have retired that's one of the rules so at line seven i'm imagining that we're going to
[3030.48s -> 3034.24s]  launch some expensive instruction that doesn't complete for a long time you know maybe it
[3034.24s -> 3038.56s]  loads something else that's known to have to come from ram so it'll take a few hundred cycles
[3038.56s -> 3043.20s]  or maybe it does a divide or a square root or something who knows what something that takes
[3043.20s -> 3048.48s]  a long time and won't be retired for a long time and therefore will cause this load not
[3048.48s -> 3055.20s]  to be retired for a long time giving these instructions time to execute speculatively okay
[3057.36s -> 3061.60s]  all right now we're assuming again we have the virtual address in the kernel
[3062.72s -> 3069.92s]  um i'm going to execute line 10 uh line 10 won't raise a fault until that um we know it's
[3069.92s -> 3074.32s]  going to raise a fault it won't raise a fault until it retires but uh we're intending we
[3074.32s -> 3079.12s]  believe we set things up so won't retire for a while since it hasn't retired and because on
[3079.12s -> 3085.60s]  intel CPUs the data is returned even if you weren't allowed to see it the data's returned
[3085.60s -> 3091.28s]  for speculative execution even if you didn't have permission that means that we can speculatively
[3091.28s -> 3097.44s]  execute the machine will speculatively execute line 11 and get the low bit of kernel data now
[3098.08s -> 3103.60s]  multiplied by 4096 line 13 is itself a load it's another load
[3104.64s -> 3112.80s]  um using an address basically the address of buffer plus the contents of r2 um we know it's
[3112.80s -> 3118.96s]  going to get cancelled because we know this will fault right we know the actual
[3119.04s -> 3122.88s]  write to r3 will be cancelled but line 13 will cause
[3126.40s -> 3131.44s]  some data from buffer to be loaded into the cache even if it doesn't end up affecting
[3131.44s -> 3138.08s]  register 3 so our line 13 is going to cause something to be loaded into the cache
[3138.08s -> 3141.04s]  and in this case depending on whether the low bit is 0 or 1
[3142.72s -> 3148.40s]  line 13 will cause the actual cache to contain either buffer 0 or buffer 4096
[3149.76s -> 3157.92s]  then right and that you know that even though r2 and r3 are cancelled the change of the cache
[3157.92s -> 3161.68s]  you know because it's supposed to be hidden in micro architectural state it will actually be
[3161.68s -> 3168.08s]  the cache will be changed finally at some point the fault will happen um and we need to
[3168.80s -> 3172.88s]  sort of recover after the fault but it it's just a page fault and it turns out
[3173.92s -> 3178.24s]  you can or user process can register a page fault handler and get control back after
[3178.56s -> 3183.60s]  a page fault and the paper mentions a couple of other ways of being able to continue after the fault
[3185.44s -> 3190.08s]  and so now all we have to do is figure out whether it was buff of 0 or buff of 4096
[3190.08s -> 3194.08s]  that was loaded into the cache and now we can do the reload part of flush and reload
[3194.08s -> 3201.20s]  we you know read the accurate time load buff of 0 read the time again load buff of 1
[3201.84s -> 3206.16s]  read the time again and compare the two differences in time and whichever one of these
[3206.16s -> 3213.04s]  took the shorter amount of time is likely to indicate whether the low bit of the
[3213.04s -> 3223.28s]  kernel data was 0 or 1 and then we report that repeat that you know a couple of billion times
[3224.32s -> 3232.56s]  we can scan all of kernel memory but in this example if b minus a is smaller than c minus b
[3232.56s -> 3241.12s]  doesn't that mean that buff of 0 was cached um let's see if i got this wrong b minus a
[3242.00s -> 3245.12s]  yeah means the buffer zero was cached oh yeah yeah you're right yeah
[3250.72s -> 3251.36s]  now we're cooking
[3254.48s -> 3254.96s]  good catch
[3254.96s -> 3267.68s]  oh sorry do you need for before we had an if before like line nine do we need it if now or
[3267.68s -> 3276.40s]  is it is it still i don't know oh the if was to help me illustrate uh the legitimate reasons
[3276.40s -> 3283.04s]  for speculative execution keep computing even though we don't know whether the branch took
[3283.04s -> 3289.84s]  or not but here the real core of the speculation is that we don't know if this load will fault
[3291.36s -> 3297.76s]  right and so we're the machine is speculatively executing past the load on the theory that it's
[3297.76s -> 3302.48s]  probably on most loads don't fault right or even though they may take a long time like a load
[3302.48s -> 3308.00s]  could take hundreds of cycles um so we'd love to be able to so the machine will speculatively
[3308.00s -> 3313.92s]  execute past a load even though it doesn't you know not knowing whether it's going to fault or
[3313.92s -> 3318.72s]  not and if the load did fault it will then undo all this speculative execution
[3318.72s -> 3327.04s]  there's a speculative execution comes up anytime you have a long-running instruction that may or
[3327.04s -> 3331.36s]  may not succeed so like divide do we know whether it's going to be divided by zero or not no
[3331.36s -> 3334.00s]  so instructions after a divide are also speculative
[3336.00s -> 3339.92s]  anyway the speculation the real critical speculation starts here now in fact we tried
[3340.72s -> 3344.32s]  you know in order to make the attack likely more successful we
[3344.32s -> 3358.72s]  sort of ensured that speculation starts here but this is the real speculation we care about
[3361.68s -> 3362.48s]  other questions
[3366.24s -> 3370.40s]  for this example we've only read one bit
[3373.04s -> 3377.28s]  yes is there some like really small simple modification that we could make
[3377.28s -> 3382.56s]  to read like a full register size of bits yeah run this 64 times one for each bit
[3385.12s -> 3389.60s]  why is it not possible to just read 64 bits at a time
[3390.48s -> 3397.36s]  well 60 60 well you need the buffer the size of this buffer has to be uh two to the um
[3398.80s -> 3405.12s]  you know two to the number of bits you're reading times 4096 or something so 64 bits
[3405.12s -> 3411.20s]  is too big as we don't have enough memory to uh make a buffer that big right we need this
[3411.60s -> 3412.64s]  the way this is set up
[3414.80s -> 3415.28s]  but anyway
[3419.20s -> 3422.32s]  yeah 64 bits too much you could certainly read eight bits at a time
[3422.32s -> 3429.12s]  and have this buffer size be 256 times 4096
[3431.60s -> 3436.16s]  the paper actually argues in a the paper observes
[3436.16s -> 3443.28s]  that since the most of the time is here and the flush was reload
[3446.24s -> 3453.36s]  if you read a byte at a time then figuring out what the bits of that byte are takes 256
[3455.36s -> 3462.56s]  flush and reloads right one for once for each possible value if you load a bit at a time
[3463.20s -> 3470.72s]  then each bit takes just one flush plus reload or sort of two two probes or two flush plus
[3470.72s -> 3476.08s]  reloads so if you read a bit at a time then you end up only doing 16 flush plus reloads
[3476.08s -> 3480.56s]  if you read a byte at a time you end up doing 256 flush plus reloads so the paper
[3480.56s -> 3487.04s]  says that it's faster to do it a bit at a time than a byte at a time which seems a little
[3487.04s -> 3490.40s]  counterintuitive but it seems to be true
[3498.96s -> 3499.52s]  other questions
[3508.72s -> 3514.80s]  so where would this user or what would this program have to be run from is there like any
[3514.80s -> 3520.88s]  particular location on the machine like does it have to be or like where would you write it i
[3520.88s -> 3526.88s]  guess is there would you like where would this program be run from uh can it be like a user
[3527.92s -> 3533.28s]  well yeah that depends on what kind of access you have to the machine
[3533.28s -> 3536.56s]  and where the data is that you want to steal and then you know
[3536.56s -> 3546.00s]  who knows right but one example is supposing you're logged into an athena dial-up machine
[3546.00s -> 3550.08s]  you know with a couple hundred other users and you want to steal somebody's password
[3551.44s -> 3557.28s]  and you're patient you can use and let's assume that athena's is now it's a couple years ago
[3557.28s -> 3561.04s]  and athena was running a version of linux that mapped the kernel into every user's
[3561.04s -> 3565.84s]  every process's address space then you can use meltdown to get out you know bit by
[3565.84s -> 3570.88s]  bitch or everything in the kernel including say the io buffers and the network buffers and stuff
[3570.88s -> 3576.00s]  and if somebody's typing their password if you're lucky or patient and somebody's typing
[3576.00s -> 3579.84s]  your password and you load all of kernel memory you're going to see that password in kernel memory
[3581.36s -> 3587.84s]  right and because in fact the kernel probably maps like xv6 maps all the physical memory
[3587.84s -> 3593.44s]  that means you can probably read all physical memory that is all of all other processes memory
[3593.44s -> 3595.60s]  using this technique on a time sharing machine
[3597.52s -> 3601.28s]  so i can see what's everybody's in text editor contents or whatever i like
[3602.56s -> 3607.92s]  now you have to that that's a way you could use it if you're using a time sharing machine
[3607.92s -> 3616.56s]  for other situations yeah it'd be different in time sharing is not that pervasive anymore
[3617.68s -> 3622.24s]  but the sort of killer scenario would be some kind of cloud computing thing where
[3622.80s -> 3627.60s]  you know you're using a cloud provider like amazon and you know which runs many customers
[3627.60s -> 3632.32s]  in the same machine and you know depending on the details of how they set up their virtual
[3632.32s -> 3640.40s]  machine monitor or container system or whatever it may be if you buy time from amazon
[3641.36s -> 3647.44s]  then you may be able to peer into the memory of other customers software running on the same
[3647.44s -> 3654.40s]  amazon machine maybe so i think that's the really you know that's how people would actually
[3654.40s -> 3659.44s]  use this probably actually another time that it might be useful is your browser when you're
[3659.44s -> 3663.20s]  browsing the web your browser actually runs a lot of code in it that is not trusted that
[3663.20s -> 3669.44s]  is supplied by the random websites that you visit maybe in the form of plugins maybe in
[3669.44s -> 3676.08s]  the form of javascript that's loaded into your browser and compiled by the browser and executed
[3676.08s -> 3684.08s]  and it is possible that this attack could be carried out by code that you run your browser
[3684.08s -> 3690.32s]  when you browse the web that you may not even know it's running there loaded from websites and
[3690.32s -> 3695.12s]  they would steal whatever stuff is sitting in your laptop i don't know if the details of
[3695.12s -> 3701.52s]  that quite work out but has anyone demonstrated an attack through either javascript or like web
[3701.60s -> 3706.16s]  assembly i don't know i don't know i i feel certainly people were worried about web assembly
[3706.88s -> 3712.00s]  i don't know whether the attack was literally possible for javascript i know that
[3713.84s -> 3720.48s]  maybe the sticking point was the accurate timing that you couldn't quite get this
[3720.48s -> 3725.20s]  nanosecond timing so you couldn't quite execute flush plus reload now you know whether
[3725.20s -> 3729.04s]  somebody with a bit more cleverness could figure out a way to do it i don't know
[3729.04s -> 3735.68s]  web assembly is much closer to just running machine code and you know i don't know exactly
[3735.68s -> 3741.12s]  how the details worked out but you know boy was it something people rapidly thought about
[3745.36s -> 3751.60s]  okay it turns out the attack doesn't always work like and for reasons that i don't think the
[3751.60s -> 3757.92s]  authors never explained or only speculated about and you can see i don't know if you can see this
[3757.92s -> 3762.80s]  well maybe you can't see this but if you turn to the last page of their paper you'll see the
[3762.80s -> 3767.76s]  output it's actually you know they mounted the attack on their own machines and extracted a
[3767.76s -> 3773.84s]  bunch of data from their own the kernel on their own machine and if you look closely you'll
[3773.84s -> 3781.60s]  see there's a huge all these lines are just x x x x x x x all these lines x x's with dots
[3781.60s -> 3786.80s]  these are places where they didn't manage to extract anything where meltdown failed even
[3786.80s -> 3792.64s]  though they repeated it many times and you can tell they must have been you know they were
[3792.64s -> 3798.24s]  the papers version of this attack was retrying many many times because for example section 6.2
[3798.24s -> 3803.28s]  that talks about performance says that in some cases they the rate at which they could extract
[3803.28s -> 3807.20s]  data was only 10 bytes per second which means they were sitting there trying again and again
[3807.20s -> 3812.16s]  and again and after thousands of times they finally managed to get some data that is the
[3812.80s -> 3817.60s]  um flush plus reload indicated that the two cache lines at different load times
[3818.16s -> 3824.16s]  um so there's something unexplained going on about why it's quite frequent for meltdown
[3824.16s -> 3828.72s]  to actually fail i get some data i actually got real data here but there was also a bunch
[3828.72s -> 3835.44s]  of data that they didn't get and um i don't know what people as far as i know people are
[3835.44s -> 3840.72s]  not really sure what all the conditions are about when it succeeds and when it doesn't you know
[3840.72s -> 3845.52s]  the most straightforward possibility is that if the kernel data is in the l1 cache
[3846.08s -> 3851.28s]  the meltdown succeeds and if the kernel data is not in the l1 cache it doesn't succeed that's very
[3851.28s -> 3857.60s]  easy to believe that that could be what's going on um because if it's not in the l1 cache
[3857.60s -> 3862.32s]  then there's a whole bunch more machinery involved in a speculative load and it's easy
[3862.32s -> 3868.88s]  to imagine that the cpu for a speculative load that's maybe not known if it's even needed
[3869.68s -> 3874.72s]  would not bother doing all the work required to load stuff from ram
[3876.56s -> 3882.00s]  but it it's not quite that simple and you can tell it's not quite that simple because
[3882.00s -> 3885.76s]  the paper says that sometimes when they retried many retried many times
[3886.56s -> 3893.60s]  it finally worked so there's some more complex condition maybe a race effectively a race inside
[3893.68s -> 3899.76s]  the cpu under which it occasionally works even for data that's not not in the cache
[3907.12s -> 3911.12s]  the end of the paper is actually also if you didn't get that for worth reading because it
[3911.12s -> 3915.76s]  does explain a sort of more real world like we wanted to find out this particular thing you
[3915.76s -> 3920.40s]  know this like we know there's passwords stored in our firefox's password manager
[3920.40s -> 3924.72s]  we wanted to get them out we want to steal them using meltdown you know what are all the
[3925.36s -> 3930.40s]  how do you find out what the address is for example they sort of lay out a complete attack
[3930.40s -> 3935.28s]  i mean a complete attack done by academics not real attackers but nevertheless
[3935.28s -> 3937.20s]  and fill in many of the pragmatic details
[3940.56s -> 3946.16s]  the final thing i want to talk about is is fixes um which we've already touched on a little
[3946.16s -> 3953.60s]  bit when this paper came out uh it got a lot of attention and there was actually another
[3953.60s -> 3959.20s]  second paper by an overlapping set of people about a different attack that also used different
[3960.00s -> 3964.32s]  different kind of speculation inside cpus called specter so the pair of the papers
[3965.12s -> 3971.84s]  came out at about the same time and was very exciting and so people hustled people realized
[3971.84s -> 3975.36s]  that boy this is extremely damaging because now what we're talking about is that you know
[3975.36s -> 3980.72s]  isolation has been broken right you don't hardly it's so basic you hardly even think about it
[3980.72s -> 3986.48s]  anymore but you know this thing this is a technique for breaking page table protections
[3986.48s -> 3992.00s]  which is you know how we enforce isolation between user and kernel it's like deeply
[3992.00s -> 3999.04s]  fundamental attack or at any rate undermines a extremely important piece of security
[4000.08s -> 4002.40s]  in a very general way right seems like you'd read anything
[4002.40s -> 4009.36s]  and so people really really hustled to deploy fixes for this and the immediate fix that
[4009.36s -> 4017.20s]  a lot of operating systems installed within weeks of this paper coming out and sometimes
[4017.20s -> 4024.32s]  had already installed this thing called kaiser and which is now called kpti in linux and it's
[4024.32s -> 4029.84s]  a pretty straightforward idea the idea is just like not to put the kernel mappings in the user
[4029.84s -> 4039.28s]  page table and instead as in xv6 switch page tables during system call so in user space you
[4039.28s -> 4044.08s]  just have user mappings make a system call there's some kind of trampoline arrangement like in
[4044.08s -> 4049.52s]  xv6 and you switch page tables to a page table that has the kernel mappings in order to
[4049.52s -> 4059.76s]  execute the kernel and that causes this attack to not work because the in that because you
[4059.76s -> 4069.12s]  switch page tables this the virtual address in r1 is not only no longer valid
[4070.24s -> 4077.04s]  it's no longer meaningful because there's no translation for it and so the cpu doesn't know
[4077.04s -> 4084.16s]  what to do with it like this virtual address won't be cached it's not even in the tlb so there's
[4084.16s -> 4088.56s]  just no way for the kernel to decide what memory corresponds to this virtual address
[4090.48s -> 4094.08s]  you know when this attack is executed in user space because this virtual this kernel
[4094.08s -> 4099.36s]  virtual address no longer means anything it's not it's not illegal it's just meaningless
[4100.24s -> 4105.68s]  and so that would cause the attack not to work the downside of this kaiser
[4106.24s -> 4109.84s]  fix is that now system calls are more expensive because switching page tables
[4113.04s -> 4117.76s]  if you don't do anything switching page tables causes the tob to be flushed because
[4117.76s -> 4122.00s]  now all those virtual addresses in the tob are the wrong virtual addresses they don't correspond
[4122.00s -> 4126.24s]  to this page table anymore and it causes the l1 cache to be flushed because it's virtually
[4126.24s -> 4133.68s]  addressed and so on a on some machines the switching page tables made system calls considerably slower
[4136.80s -> 4141.84s]  and but recent machines actually have this trick called pcid which you can look up but basically
[4141.84s -> 4146.16s]  makes it so you can avoid flushing these caches on a page table switch although it still
[4146.16s -> 4151.44s]  takes some time and if you poke around on the web looking for people there was a lot of worry
[4151.44s -> 4158.48s]  at the time that this split that this two-page table idea would be unacceptably slow and in fact
[4159.20s -> 4163.28s]  that didn't really turn out to be a serious problem and if you poke around you'll see that
[4163.28s -> 4168.96s]  people's guesses about typical workloads you know how much it impacts overall performance of
[4168.96s -> 4173.76s]  typical workloads which after all don't spend all their time entering a next-name kernel is
[4173.76s -> 4177.76s]  like five percent um so it wasn't such a bad deal
[4180.40s -> 4185.68s]  um do any questions about this kaiser fix
[4191.52s -> 4195.20s]  um so people adopted this pretty rapidly
[4195.20s -> 4199.04s]  in fact there have been kernels that had already adopted it because it defended against some other
[4199.04s -> 4207.04s]  attacks um there's also a reasonable hardware fix that i believe Intel has actually made in
[4207.04s -> 4213.04s]  recent processors and that AMD had already made and that's basically to um because the uh
[4214.40s -> 4220.72s]  in fact the permission you know this is it's a structure of the cache when
[4221.60s -> 4226.32s]  when an instruction loads something from the l1 cache like this kernel data we're trying to attack
[4227.28s -> 4232.32s]  the permissions where people believe that the permissions are sitting right there in the cache
[4232.32s -> 4236.32s]  entry and so there's no trouble with the CPU checking the permissions at that point
[4236.32s -> 4243.60s]  and indeed AMD CPUs and perhaps modern Intel CPUs will actually do the permission check very
[4243.60s -> 4248.80s]  early and won't return this data they won't even return it to the core if the permission
[4248.80s -> 4253.28s]  checks don't work out so there's none of this speculative instructions are able to see
[4254.24s -> 4259.92s]  uh forbidden data so i i don't know if you know the answer to this question
[4259.92s -> 4266.56s]  it's probably just uh speculative but um uh no pun intended um but why why do you think
[4266.56s -> 4272.16s]  intel would do this like this seems like okay because to me it seems like it was a discussion
[4272.16s -> 4277.44s]  shall we check permissions on transient instructions and they were just like no why why bother
[4277.44s -> 4282.80s]  well it's just a simple check but why bother indeed stuff's transparent right i mean the user's
[4282.88s -> 4287.36s]  not going to be able to see the data either way doing the check early you know that's like
[4287.36s -> 4294.40s]  some gates on a pretty critical path right the the you know the core to l1 data cache
[4295.44s -> 4300.72s]  path is extremely performance critical you know and if you can shave a few transistors off the
[4302.16s -> 4306.88s]  you know off the critical path here between issuing instruction and getting the data back
[4306.88s -> 4311.76s]  you know that may may allow you to have a slightly faster cycle time and run programs faster
[4313.76s -> 4318.64s]  and so it's got to be the case that well i don't know got to be the case but it's easy to imagine
[4318.64s -> 4324.80s]  that it would have cost them a few transistors to actually enforce the permissions early because
[4324.80s -> 4330.16s]  after all they still need all the stuff at retirement it's not like doing it early
[4330.16s -> 4334.40s]  would save them some work later on they still they still have to defer the fault until
[4334.40s -> 4340.96s]  retirement um so all that stuff's still there i'm just guessing that it
[4342.00s -> 4346.08s]  didn't seem like it would have any advantages and would have been like a little bit of extra
[4346.08s -> 4355.44s]  work um and either way completely invisible you know theoretically invisible to the architectural
[4355.44s -> 4363.60s]  level did any kernel decide to like revert this kaiser um fix now that like intel has
[4363.60s -> 4370.88s]  fixed the cpu to improve performance again i know it's optional on a lot of kernels i'm i'm not
[4370.88s -> 4378.00s]  totally sure what's going on with the intel fix i'm fairly sure that they have this fix out there
[4378.00s -> 4384.32s]  but exactly you know i i i don't really know what's going on i think the linux kernel you
[4384.32s -> 4389.04s]  can just ask you know which hardware fixes have been implemented and linux you know changes
[4389.04s -> 4393.20s]  the mitigation that it enables depending on what actually the hardware tells it
[4394.88s -> 4400.32s]  so might also so you can you can actually do that like you can read enough info about the
[4400.32s -> 4405.84s]  processor as the kernel to know whether what to do but you can run on your running your laptop
[4405.84s -> 4411.92s]  there's a linux command that is actually uh on like which is what it tells you exactly you
[4411.92s -> 4416.40s]  know what fixes have been implemented where things are mitigated in hardware because there's
[4416.40s -> 4421.12s]  a wide range of these you know spectrum application attacks are you saying that
[4421.12s -> 4425.84s]  linux will actually use the combined page table if the cpu yes i believe so oh cool
[4429.76s -> 4435.76s]  yeah i think it was uh you know 99 i haven't checked it recently right i believe that's the
[4435.76s -> 4436.72s]  case cool
[4444.00s -> 4449.92s]  all right sorry so what were people doing like like how did they find find this what were they
[4449.92s -> 4456.08s]  trying to do what are they trying to do i'm trying to break into computers no then well
[4457.20s -> 4462.80s]  the the uh who knows what they were really trying to do i mean the papers are you know
[4462.80s -> 4469.92s]  written by a various academics maybe you know their research is finding
[4470.96s -> 4476.40s]  security problems i think one thing that motivated them for a long time is they wanted
[4476.40s -> 4482.24s]  to break uh address space randomization and they had earlier tapers you know with different
[4482.24s -> 4486.08s]  schemes trying to be uh great address standardization so like one group of
[4486.08s -> 4491.68s]  you know one stream of researchers that were in this area had that as a background uh i think
[4491.68s -> 4494.32s]  the project zero people came from a completely different angle
[4498.80s -> 4504.00s]  i see thank you this is people i think robert said before people have been working this area
[4504.00s -> 4511.36s]  for decades you know trying to find books that they can exploit and understand
[4511.36s -> 4523.68s]  it so i guess it how this is a hard question to answer but like how likely is it that there's
[4523.68s -> 4531.92s]  another like meltdown out there because it seems like extremely likely okay exactly micro
[4531.92s -> 4538.72s]  the fundamental like thing with micro architecture like exposing changes that's right i think
[4538.72s -> 4543.52s]  part of what's going on is that the cpu manufacturers have you know for decades and
[4543.52s -> 4550.56s]  decades have been um you know opt adding more and more and more optimism there's many many
[4550.56s -> 4556.72s]  many sort of cool little tricks inside the micro architecture for making things go faster and
[4557.84s -> 4563.76s]  you know now and then people didn't worry that much or it just wasn't on the radar that
[4563.76s -> 4569.92s]  this could be like a serious security problem and so now people are now very aware that this
[4569.92s -> 4574.16s]  stuff could be a serious security problem but we're now in a position where we're living with
[4574.16s -> 4582.08s]  you know 30 years of clever ideas inside the CPUs and so indeed a bunch of since this paper
[4582.08s -> 4587.60s]  came out and indeed before this paper came out a bunch of kind of a bunch of this style of
[4587.60s -> 4594.64s]  attacks have come to light exploiting various different uh micro architectural
[4596.64s -> 4601.92s]  thingies and the CPUs so i i think this is going to be a while before this is all laid to
[4601.92s -> 4605.52s]  rest yeah you look through the security conferences in the last you know two years
[4605.52s -> 4611.04s]  basically every year every conference has basically session on like exploiting special
[4611.04s -> 4619.12s]  execution properties and see if they can make attacks work maybe a larger question is whether
[4620.16s -> 4626.40s]  you know whether the situation is well you know there's you know 15 or 20 or 30 things
[4626.40s -> 4635.36s]  that sort of have to be worked out and then will be done or whether there's some much higher
[4635.36s -> 4644.64s]  level approach gone wrong you know that we all i mean this is probably way too pessimistic but
[4644.64s -> 4648.88s]  you know people had a lot of faith in isolation as an idea that there's like a totally reasonable
[4648.88s -> 4656.00s]  thing to assume that isolation works and will design stuff like cloud computing and you know
[4656.00s -> 4660.16s]  running javascript in the browser and all this stuff under the assumption which is not actually
[4660.16s -> 4664.80s]  true but was close enough believed to be close enough to true that isolation will just you know
[4665.36s -> 4671.20s]  cause there not to be serious security problems and that's actually probably still doable but
[4672.16s -> 4679.04s]  this whole bag of micro architectural attacks has not made that story seem more convincing
[4679.04s -> 4688.00s]  that's for sure um just to add on to that uh i'm not sure uh prof's levels of expertise
[4688.00s -> 4696.64s]  with like cpu design but to what extent can cpu design be made straightforwardly without
[4696.64s -> 4703.44s]  a micro architecture while still preserving its high performance oh performance i mean
[4704.08s -> 4709.92s]  people believe this nicely security too but yeah well some of this clearly can be fixed like this
[4709.92s -> 4714.56s]  meltdown thing i mean there is a fix this you know to actually check the permissions
[4715.44s -> 4720.80s]  um that probably doesn't sacrifice any performance for some of the other attacks that have come up
[4721.76s -> 4727.12s]  um it's not clear that you could fix them without sacrificing performance i mean some of
[4727.12s -> 4733.84s]  this very very deep like the fact that we're sharing you know this there's a lot of sharing
[4733.84s -> 4740.48s]  like in a time sharing or cloud environment there's just a lot of sharing and um so for example
[4740.56s -> 4748.48s]  supposing there's a disk drive or a network on your cloud server right gosh you might be able
[4748.48s -> 4753.44s]  to get information about the other people on that cloud server simply by watching how their
[4753.44s -> 4757.60s]  traffic interferes with your traffic disk traffic or network traffic or memory traffic
[4757.60s -> 4762.08s]  or something so there's some sort of you know i don't know whether that's practical maybe it's
[4762.08s -> 4766.56s]  not although you know for many many things in which people said boy that attack just doesn't
[4766.56s -> 4773.20s]  seem to be practical you know it's turned out to be practical enough um and i think
[4774.64s -> 4778.08s]  and then so a lot of this micro architectural stuff maybe could be cleaned up without
[4778.08s -> 4782.32s]  performance loss or maybe can't be cleaned up without performance loss you know but i i think
[4782.32s -> 4788.32s]  it's i think it's a much more serious problem than just we're gonna apply some fixes
[4788.32s -> 4796.80s]  and they'll all go away the place has been the most acute is cryptography there's been
[4796.80s -> 4805.92s]  many many years of people looking into these kind of clever often cash timing based ways of sensing
[4809.12s -> 4815.04s]  bits out of keys and other people's cryptographic you know people running i'm running a cryptographic
[4815.04s -> 4820.56s]  an encryption on the same machine as you can you guess anything about my key by
[4821.36s -> 4828.32s]  watching by doing cash timing the answer is absolutely and it's not like a micro architectural
[4828.32s -> 4836.48s]  bug it's you know it's just a consequence of sharing often anyway i don't know how all
[4836.48s -> 4841.44s]  this is going to play out but it's not it's not straightforward i mean the
[4841.44s -> 4849.36s]  scariest part is when people made progress on um just measuring the like em radiation from a
[4849.36s -> 4854.32s]  cpu and figuring out what instructions are run and what data is in it with machine learning
[4854.32s -> 4861.12s]  with like some accuracy you know not 100 not nearly but like a scary amount of accuracy
[4861.12s -> 4869.60s]  because anything over zero is scary yeah we all live in you know this all
[4872.16s -> 4878.80s]  well there's a boundary between attacks that are or there's some threshold between
[4878.80s -> 4883.68s]  attacks that are like possible but you know gosh just seems like that would be too expensive
[4883.68s -> 4890.72s]  or awkward or painful or whatever complex to carry out and attacks that really could be
[4890.72s -> 4894.00s]  carried out of course we only should defend against the second class because the first
[4894.00s -> 4899.52s]  class is often too expensive to defend against but as the value of stuff contained in computers
[4899.52s -> 4905.12s]  gets larger and you know attackers get more clever get more closer access to
[4905.12s -> 4912.96s]  shared environments a threshold about which attacks are feasible enough to defend against
[4912.96s -> 4926.08s]  changes all right i'm done with the lecture but i'm happy to take more questions if people happen
[4926.48s -> 4935.52s]  thank you thank you thanks i actually had a question about the cache um so the l1 cache
[4935.52s -> 4945.20s]  it's it's per cpu right yes and the l2 is it shared well the this picture is different
[4945.20s -> 4954.48s]  for each cpu for different models of cpu etc the the habit today is um the the the
[4954.80s -> 4961.68s]  is um looks uh it's a little bit more complex than this typically you have multiple cores
[4963.20s -> 4969.84s]  no two or four or eight or 64 or something each one has an l1 cache that's quite close
[4969.84s -> 4976.08s]  to the cpu but it's small fast and small each core typically also has a bigger l2 cache that's
[4976.08s -> 4990.08s]  you know and it's sort of dedicated to that cpu um and then um and and then there's often a
[4990.08s -> 4996.24s]  shared l3 cache often but not always and another approach is to make the
[4997.04s -> 5003.20s]  summation of the l2 caches sort of convenient for all the cpus to use so that i have super
[5003.20s -> 5008.88s]  high speed access to my l2 cache but i can get at other people's at a slightly bigger penalty so
[5008.88s -> 5014.72s]  the effective cache size is larger so you often 3c either three level caches or or
[5015.60s -> 5017.68s]  these are joint two second level caches
[5020.96s -> 5023.68s]  and typically the l2 and l3 are physically addressed
[5027.92s -> 5030.96s]  sorry so what's the point of having physically addressed
[5031.76s -> 5038.80s]  is that because of the difference oh yeah it's easy the the um stuff in the l1 in a virtually
[5038.80s -> 5046.32s]  addressed cache um if if the same data is used with different virtual addresses you can't you
[5046.32s -> 5050.88s]  know the the virtually addressed cache doesn't really help you find it um if it was cached
[5050.88s -> 5056.16s]  under a different address where these l2 caches the data is independent it's usable no matter
[5056.16s -> 5064.48s]  what virtual address um you addressed it under um yeah um where does the mmu sit
[5065.12s -> 5070.88s]  relative to all these caches and the tlb oh it's not it's distributed really because the uh
[5071.68s -> 5081.28s]  i mean the the most obvious um i mean i think in real life the tlb the most critical thing
[5081.28s -> 5085.12s]  is the tlb and i believe it's indexed in parallel with the l1 cache typically
[5086.16s -> 5090.24s]  right so if you hit in the cache the l1 cache great although there may be a
[5092.72s -> 5096.88s]  anyway and if you miss in the l1 cache then the now you have the physical you were looking
[5096.88s -> 5102.64s]  up in the tlb the same time now you're the physical cat addressed the mmu though is not
[5102.64s -> 5110.80s]  just a single box that sits somewhere it's actually kind of involved oh okay but isn't
[5110.80s -> 5116.80s]  it hardware so oh everything is here's hardware yeah oh that's interesting but but remember these
[5116.80s -> 5122.56s]  you know these chips have billions of transistors on them so yeah maybe it's hardware
[5122.56s -> 5129.76s]  but we're talking about massively complex hardware that's designed using very sophisticated
[5129.76s -> 5135.84s]  software like design techniques so that it can do very very complex and sophisticated things
[5136.48s -> 5141.52s]  so yeah it's hardware but yeah it's not at all at all straightforward
[5143.68s -> 5148.00s]  so do table mapping like page table mappings ever end up in the
[5148.72s -> 5153.92s]  caches at all or are they always just routed through the tlb because if you miss it the tlb
[5153.92s -> 5159.84s]  you have to go to memory retrieve that right so the certainly the l2 cache will hold uh
[5160.48s -> 5167.12s]  the the from the point of view the l2 cache it tlb misses tlb reloads are just memory accesses so
[5167.12s -> 5172.40s]  the tlb needs to load a bunch of page table junk it's just a memory load and it could stuff
[5172.40s -> 5177.04s]  could easily be cached in the l2 but it has to skip the l1 because l1 has virtual addresses
[5177.04s -> 5184.24s]  which i don't think the tlb would consult the l1 for its okay because it's uh virtually addressed
[5184.24s -> 5193.60s]  yes and then one thing about the spectra attack um um how would you i i so so the thing is like
[5193.60s -> 5198.16s]  i've heard about meltdown inspector like at least a dozen times and every time i looked
[5198.16s -> 5202.08s]  it up i would not understand it so this is the first time i actually understand what's going on
[5202.88s -> 5210.16s]  but for specter is there um like how similar is it to melt down it's not it's not okay
[5210.16s -> 5219.68s]  okay or well my understanding of the specter attack is by training the branch predictor
[5219.68s -> 5226.80s]  that you know the the other code that you're trying to attack stuff from let's suppose it's
[5226.80s -> 5232.80s]  um another process and you share some memory with it right you know because you're it's really
[5232.80s -> 5239.36s]  the same program as you but it's some other user running the program right um you can
[5240.80s -> 5249.92s]  the branch the branch the tables that the branch predictor uses are shared uh between different you
[5249.92s -> 5253.68s]  know if i run on a cpu forbidden that you run or maybe you run on different hyper threads of
[5253.68s -> 5258.32s]  the same cpu everybody sees the same branch you know sees the same branch predictor so i
[5258.32s -> 5262.72s]  can train the branch predictor to predict branches in a certain way and then i let you run
[5264.32s -> 5269.76s]  you're running with my branch predictor training right and so that means i can essentially trick
[5269.76s -> 5275.28s]  your program into speculatively executing instructions of my choice right and it's only
[5275.28s -> 5282.00s]  speculative so of course they'll be undone but they they will cause cache loads um that
[5282.00s -> 5286.64s]  to some extent i can control because i control how you speculate execute and then if we share
[5286.64s -> 5292.64s]  memory i can use flush and reload to sense what cache lines your program loaded and this
[5292.72s -> 5300.72s]  speculative execution that oh so in that case you don't need to um you don't need to like
[5300.72s -> 5306.96s]  like directly address address assist a piece of memory you just need to make sure that program
[5306.96s -> 5312.00s]  will speculatively execute it like in meltdown we did it ourselves but inspection you just
[5313.28s -> 5317.68s]  directed there that's kind of cool because you can just say oh you know just go and load that
[5317.68s -> 5322.56s]  secret by training the branch predictor without knowing where the secret is yeah or like
[5322.64s -> 5327.92s]  you gotta know someone yeah you kind of have to know you need to know great oh yeah sorry
[5327.92s -> 5332.00s]  yeah you have to know but you don't have to load it yourself yeah that stuff's not secret
[5332.00s -> 5336.00s]  right you're probably running a program that i know what program you're running you know yeah
[5337.76s -> 5339.20s]  okay that makes sense thank you
[5341.68s -> 5346.72s]  yeah i was just wondering it seems like when you when a research paper like this gets released
[5346.72s -> 5352.32s]  you know it's out there for like people linux and windows and like intel for them just
[5352.32s -> 5356.32s]  try to go and scramble to patch the bug but it's also out there for like hackers
[5356.32s -> 5359.44s]  can start to like learn from the paper and be like oh this is a method we could use
[5360.24s -> 5366.48s]  um and i'm i'm wondering like it as a researcher is there a general practice of like as we're
[5366.48s -> 5370.80s]  working on the paper we'll like sort of tip off the yes the quote-unquote good guys
[5370.80s -> 5375.68s]  first so that they can get a head start the authors informed every you know the CPU
[5375.68s -> 5379.68s]  manufacturers and the os manufacturers before they publish the paper yes there's a whole
[5379.68s -> 5384.08s]  there's a whole protocol in the paper anything these these kind of papers won't even be accepted
[5384.08s -> 5387.76s]  anymore unless you know you follow the protocol yeah that doesn't mean the attackers weren't
[5387.76s -> 5392.56s]  already using it right because probably know the attackers discovered this 20 years ago but
[5395.92s -> 5401.36s]  thank you it's not straightforward you know i think in this particular case i think you know
[5401.36s -> 5406.16s]  there was some uh intel was not too excited about the collaboration between the linux
[5406.16s -> 5408.72s]  community and intel was not completely smooth i think at the
[5411.04s -> 5414.48s]  when this happened i think they worked out some of the kinks but
[5418.64s -> 5424.96s]  i think it was kind of scary because i i read i want to make a video that the ubuntu
[5424.96s -> 5430.96s]  fix was published after the paper was published which i found scary but
[5436.88s -> 5440.08s]  yeah thank you so much thank you
[5442.24s -> 5447.04s]  thank you thank you well see you next week
