# Detected language: en (p=1.00)

[0.00s -> 2.84s]  All right.
[2.84s -> 5.84s]  Good morning or good afternoon, everyone.
[5.84s -> 7.64s]  I'd like to get started.
[7.64s -> 9.32s]  Can people hear me?
[9.32s -> 10.32s]  Yep.
[10.32s -> 11.52s]  Thank you.
[11.52s -> 13.42s]  All right, today what I'd like to do
[13.42s -> 17.52s]  is give you a chance to ask questions about XV6
[17.52s -> 19.96s]  and about the recent labs.
[19.96s -> 23.92s]  And in order to kind of give us something to talk about,
[23.92s -> 28.80s]  I'm going to do as much as I can of the copy
[28.92s -> 32.56s]  and write fork lab here, just to give us sort of something
[32.56s -> 34.00s]  to chew on.
[34.00s -> 37.48s]  And you should feel free to ask any questions you like.
[37.48s -> 39.64s]  You can ask about why my solution is
[39.64s -> 43.88s]  different from your solution or why XV6 works the way it does
[43.88s -> 48.60s]  or why the bugs that we encounter
[48.60s -> 51.52s]  show up in the way they do.
[51.52s -> 55.28s]  So just to remind you, although I'm sure this is sure
[55.28s -> 57.70s]  I don't need to remind you, the point of copy and write
[57.70s -> 62.66s]  fork is to avoid copying costs for pages in a fork that
[62.66s -> 65.22s]  are never modified.
[65.22s -> 67.62s]  I put in a few lines of code to measure
[67.62s -> 70.38s]  the reduction in the number of bytes
[70.38s -> 74.86s]  that mem copy had to copy during user tests
[74.86s -> 77.54s]  for my copy and write solution and found that it actually
[77.54s -> 81.90s]  reduced the amount of copying by 90%.
[81.90s -> 84.10s]  And this is for user tests, which is kind of a weird
[84.10s -> 86.62s]  program, but nevertheless.
[86.62s -> 88.38s]  And I think a lot of it is reductions
[88.38s -> 90.86s]  in copying of the instruction pages
[90.86s -> 94.98s]  because the instruction pages are never modified.
[94.98s -> 98.06s]  So there's never any point in having copies of them.
[98.06s -> 100.62s]  And I think that's where a lot of the wind was from.
[100.62s -> 102.06s]  Copy and write fork also reduces
[102.06s -> 105.14s]  RAM use, the sort of maximum amount of RAM
[105.14s -> 106.34s]  you ever need to use.
[106.34s -> 110.26s]  And it reduces the amount of time you spend in fork.
[110.26s -> 112.26s]  And so fork returns more quickly.
[112.26s -> 115.10s]  On the other hand, of course, the total cost may be larger
[115.14s -> 118.70s]  because if programs end up modifying all the copy and write
[118.70s -> 121.66s]  pages, you end up having to do all the copies
[121.66s -> 124.86s]  plus take all the page faults, which
[124.86s -> 126.74s]  can be a little bit expensive.
[126.74s -> 128.98s]  But nevertheless, people have found that in general,
[128.98s -> 133.26s]  it's a net benefit.
[133.26s -> 135.98s]  And as you know, the main challenges are,
[135.98s -> 139.66s]  one challenge is how to avoid freeing memory.
[139.66s -> 141.94s]  Now we're sharing pages among processes.
[141.94s -> 143.38s]  We have to not deallocate them,
[143.38s -> 146.26s]  have to not free them until the last process is finished
[146.26s -> 149.62s]  using them so we need more bookkeeping.
[149.62s -> 151.06s]  And the other interesting challenge
[151.06s -> 154.66s]  is that there's in copy out, in particular,
[154.66s -> 158.78s]  the XV6 modifies user memory without actually going
[158.78s -> 162.74s]  to the MMU paging hardware.
[162.74s -> 167.10s]  And so we have to mimic page faults there.
[167.10s -> 170.06s]  I'm going to go through the lab starting at the beginning,
[170.06s -> 172.06s]  the copy and write lab.
[172.10s -> 175.66s]  Before I start, I just want to mention the strategy
[175.66s -> 178.90s]  that I think of myself as following when I do labs.
[178.90s -> 181.34s]  And I do a lot of labs from one year to an x.
[181.34s -> 186.10s]  I don't remember the details of how to do the labs.
[186.10s -> 188.46s]  And in addition, the labs change.
[188.46s -> 190.54s]  All right, so when I'm doing the labs,
[190.54s -> 191.86s]  I always take small steps.
[191.86s -> 195.70s]  I may find some sort of next subset of the problem
[195.70s -> 198.70s]  to solve, maybe five or 10 lines of code.
[198.70s -> 200.86s]  I program that up, and I run it and try
[200.86s -> 205.30s]  to get it to working before I go on to the next proceeding
[205.30s -> 206.30s]  to the next step.
[206.30s -> 208.90s]  And this is as opposed to, for example,
[208.90s -> 213.26s]  thinking through and writing a complete solution
[213.26s -> 215.90s]  before starting to test and debug it.
[215.90s -> 219.50s]  I always test and debug a little bit at a time.
[219.50s -> 224.06s]  A lot of the reason for that is that even though you may
[224.06s -> 226.38s]  have sort of thought through a lot of the things,
[226.42s -> 228.86s]  a lot of the challenges, and a lot of the design
[228.86s -> 231.18s]  for what you're going to have to do to solve a lab,
[231.18s -> 234.46s]  I find at least there's often surprises.
[234.46s -> 237.74s]  And so if I implement everything first,
[237.74s -> 240.22s]  I may find then that I wasted a lot of time
[240.22s -> 242.66s]  because I didn't fully understand the problem
[242.66s -> 247.86s]  until I actually got into debugging it.
[247.86s -> 250.70s]  And the way I choose the next step at each point is usually
[250.70s -> 253.94s]  driven by whatever panic or crash or test failure
[253.94s -> 259.38s]  I see next sort of tells me what I need to fix next.
[259.38s -> 260.82s]  And while I'm doing this lab, I'll
[260.82s -> 262.42s]  make a few mistakes that I actually
[262.42s -> 265.66s]  made the last time I did the lab so
[265.66s -> 270.14s]  that we can have a little practice debugging together.
[270.14s -> 272.86s]  And again, feel free to break in at any time
[272.86s -> 277.22s]  and ask questions about anything.
[277.22s -> 280.54s]  I'm starting here with a fresh copy of the copy
[280.54s -> 285.54s]  and write lab source, the same as all of you did.
[285.54s -> 293.94s]  So I'll just fire this up and run the copy and write test.
[293.94s -> 296.74s]  And maybe I'll get lucky and all the tests will pass
[296.74s -> 299.78s]  and won't have to do anything.
[299.78s -> 301.50s]  Too bad, one of the tests failed.
[301.50s -> 302.54s]  So not too surprising.
[305.90s -> 309.02s]  And a good step at this point is
[309.02s -> 311.58s]  to go have a look at the tests and figure out
[311.58s -> 313.38s]  what it's trying to do.
[313.38s -> 315.14s]  And the sad truth is some of the tests
[315.14s -> 318.42s]  are complicated and hard to understand
[318.42s -> 321.38s]  and don't necessarily test anything specific.
[321.38s -> 323.54s]  They just sort of try different things
[323.54s -> 325.14s]  and see if something will break.
[325.14s -> 328.42s]  But luckily, with this simple test,
[328.42s -> 330.54s]  it's pretty clear what it's doing.
[330.54s -> 331.70s]  That's a quick question.
[331.70s -> 332.42s]  Yeah, yeah.
[332.42s -> 333.82s]  Do you mean to share your screen?
[333.82s -> 334.82s]  You cannot see anything.
[334.82s -> 337.54s]  Oh, gosh.
[337.62s -> 339.82s]  I'm sorry about that.
[339.82s -> 341.94s]  Thank you for letting me know.
[344.54s -> 347.10s]  All right, is that better?
[347.10s -> 348.06s]  Yes.
[348.06s -> 348.58s]  Good.
[348.58s -> 349.06s]  Brilliant.
[349.06s -> 350.34s]  All right.
[350.34s -> 352.62s]  OK, well, that's bug number one.
[355.38s -> 361.46s]  OK, so here I am in my copy and write test.
[361.46s -> 365.02s]  I ran copy and write test and it fails.
[368.42s -> 371.42s]  I look at the test, the simple test on the screen
[371.42s -> 374.14s]  at right is what actually failed.
[374.14s -> 376.62s]  And this test, luckily enough, it actually
[376.62s -> 377.90s]  tells us what it's doing.
[377.90s -> 380.26s]  It allocates what it knows to be
[380.26s -> 385.78s]  more than half the available memory in xv6 and then forks.
[385.78s -> 388.06s]  And so of course, the reason this fails
[388.06s -> 390.54s]  is that fork makes a copy.
[390.54s -> 394.30s]  Then ordinary xv6 that actually literally makes a copy.
[394.30s -> 397.94s]  And so if we have to copy more than half of memory,
[397.94s -> 400.18s]  there's just not enough space for that.
[400.18s -> 404.94s]  OK, so of course, this is what copy and write is all about.
[404.94s -> 406.86s]  And we know that the basic game
[406.86s -> 410.74s]  is that instead of copying the process's memory,
[410.74s -> 415.10s]  we want to just copy its page table and not the memory.
[415.10s -> 418.78s]  So the child just has a carbon copy of the parent's page
[418.78s -> 421.06s]  table instead of memory, referring
[421.06s -> 424.90s]  to all the same physical pages.
[424.90s -> 432.50s]  The place where fork does its copying is in vm.c,
[432.50s -> 436.98s]  in uvm copy.
[436.98s -> 442.62s]  And so we can just modify this code
[442.62s -> 447.94s]  to instead of allocating a page memory here
[447.98s -> 450.14s]  and copying onto it, we're going
[450.14s -> 458.62s]  to eliminate those three lines and instead just
[458.62s -> 460.30s]  copy page table entries.
[460.30s -> 475.42s]  So I'm actually just going to hack this copy
[475.42s -> 479.42s]  of map pages.
[479.42s -> 481.34s]  Since we didn't actually allocate memory here,
[481.34s -> 483.38s]  I'm going to get rid of this free.
[483.38s -> 486.78s]  I'm going to fix this map pages to instead of mapping
[486.78s -> 494.66s]  mem at address i, I'm going to map the pa, which
[494.66s -> 497.18s]  is the physical address we pulled out of the parent's
[497.18s -> 498.98s]  page table.
[498.98s -> 501.50s]  So is that clear?
[501.50s -> 504.50s]  This will just have the effect of mapping all of the parent's
[504.50s -> 508.50s]  pages into the child's address space.
[508.50s -> 510.26s]  Isn't pa already a un64?
[514.18s -> 515.18s]  Probably.
[515.18s -> 517.38s]  Or is it bad if you were to cast it
[517.38s -> 518.70s]  if it's already that type?
[518.70s -> 522.70s]  The cast is absolutely nothing other than shut the compiler
[522.70s -> 523.78s]  up.
[523.78s -> 528.18s]  Since addresses are 64 bits and un64 is 64 bit,
[528.18s -> 530.58s]  that cast doesn't change the bits.
[530.62s -> 534.94s]  It just makes the type checker less upset.
[534.94s -> 537.42s]  But let's see, I actually don't know what any of these types
[537.42s -> 539.38s]  are.
[539.38s -> 542.02s]  All right, one thing is we don't need this mem variable.
[544.86s -> 546.46s]  Oh, well, OK.
[546.46s -> 547.54s]  So you're right about pa.
[550.62s -> 552.54s]  Gosh, I wonder if we're done now.
[552.54s -> 554.98s]  Let's run the cow test and see what happens.
[554.98s -> 557.70s]  Oh, we're definitely not done.
[557.70s -> 565.98s]  OK, so what we got here is this user trap with scause of 2.
[565.98s -> 567.94s]  Anybody remember what scause 2 is?
[570.74s -> 571.62s]  I'll just tell you.
[571.62s -> 573.38s]  I think it's instruction failure.
[573.38s -> 574.82s]  It's an illegal instruction.
[574.82s -> 580.58s]  So why would it be getting an illegal instruction now?
[580.58s -> 581.74s]  Is this what we expected?
[581.74s -> 594.70s]  Did we overwrite the portion where we have instructions?
[594.70s -> 598.66s]  Yes, something is damaging the instructions of the user
[598.66s -> 599.18s]  program.
[601.74s -> 603.62s]  And of course, the fault, we're not surprised
[603.62s -> 606.42s]  we get a fault here.
[606.42s -> 609.62s]  But we were hoping for stored faults
[609.66s -> 614.02s]  to drive the copying process and copy on write.
[614.02s -> 621.22s]  We're not getting the store fault.
[621.22s -> 625.22s]  Any other hypotheses for this?
[625.22s -> 627.82s]  This is not really fair because this is my bug,
[627.82s -> 629.22s]  not your bug.
[629.22s -> 632.90s]  But nevertheless.
[632.90s -> 635.74s]  Does it have to do with the flags?
[635.74s -> 637.06s]  Yes, it does.
[639.66s -> 645.30s]  OK, so I'll just leave that at the side.
[645.30s -> 647.26s]  It'll actually come up again.
[647.26s -> 649.70s]  Another question is, we're hoping
[649.70s -> 656.38s]  for store page faults, which will drive the copying process.
[656.38s -> 658.22s]  Why didn't we get a store page fault?
[670.50s -> 676.62s]  All right, why would we get a store page fault?
[676.62s -> 678.54s]  Under what circumstances does the RISC-V
[678.54s -> 681.18s]  generate a store page fault?
[684.10s -> 686.82s]  We would get it if the right flag is not set.
[686.82s -> 690.22s]  But now we can just write to everything still normally.
[690.22s -> 691.78s]  Yeah, I've left the right flag set.
[691.78s -> 694.30s]  All right, so that's bug one, which I did actually
[694.30s -> 696.34s]  make last time I did this lab.
[696.34s -> 698.54s]  So I've left the right flag set in the parent.
[698.58s -> 701.42s]  And I haven't done anything to the right flags.
[701.42s -> 705.70s]  This flags variable here is just the way
[705.70s -> 707.58s]  we pulled it out of the parent's page defaults.
[707.58s -> 709.54s]  All its pages are writable.
[709.54s -> 711.82s]  So that means that it's going to be writable in the child
[711.82s -> 711.98s]  too.
[711.98s -> 713.36s]  We won't get store page faults.
[713.36s -> 716.98s]  So I'll just be sharing a page read-write,
[716.98s -> 718.26s]  which is not what we wanted.
[718.26s -> 722.22s]  So how am I going to write protect these pages?
[729.54s -> 732.50s]  Any proposals?
[732.50s -> 733.34s]  What should I type?
[737.02s -> 738.62s]  How about?
[738.62s -> 750.58s]  You can do flags and equals the wave sign underscore w.
[750.58s -> 753.38s]  Yeah, yeah, so we're going to need the tilde there.
[753.38s -> 755.42s]  I call that wave sign a tilde.
[755.42s -> 759.66s]  We need to clear this bit in the flags.
[759.66s -> 761.74s]  But we want to leave all the other flags set.
[761.74s -> 764.06s]  OK, so that's going to, will this clear the flag
[764.06s -> 765.26s]  in the parent or the child?
[771.10s -> 771.94s]  Child.
[771.94s -> 773.26s]  Just the child.
[773.26s -> 774.46s]  So how do I clear?
[774.46s -> 776.66s]  Do I also need to clear the flag in the parent?
[779.66s -> 783.62s]  Yes, because we want the child to be
[783.62s -> 785.06s]  independent for the parent.
[785.06s -> 787.46s]  So if we write something, we don't want the child
[787.46s -> 790.26s]  to also get the modification from the parent.
[790.26s -> 791.26s]  That's absolutely right.
[791.26s -> 792.84s]  We need to write protect the page and the parent
[792.84s -> 794.84s]  as well, because we don't want the child to see
[794.84s -> 796.58s]  the parent's modifications.
[796.58s -> 799.34s]  We're trying to mimic them having completely separate
[799.34s -> 800.18s]  copies.
[800.18s -> 804.28s]  So what can I do to clear the flag in the parent's page
[804.28s -> 804.78s]  table?
[804.78s -> 818.94s]  You can do something like star PTE and equals not PTE, right?
[818.94s -> 822.02s]  Yeah, so I have a parent's, a pointer to the parent's page
[822.02s -> 824.70s]  table entry right here in PTE.
[824.70s -> 828.06s]  And so I can clear the PTE flag here too.
[828.06s -> 831.46s]  So that's pretty convenient.
[831.46s -> 837.18s]  And my belief is that's enough to make both copies of the page
[837.18s -> 842.24s]  or both mappings of the page read only.
[842.24s -> 848.62s]  So hopefully now we'll start getting write faults.
[848.62s -> 854.02s]  Is that possible to clear the PTEW bit before you
[854.02s -> 857.82s]  say flags equals PTE flags of PTE?
[857.82s -> 861.66s]  And that way, the original flag is just the correct thing?
[861.66s -> 862.66s]  Oh, I see.
[862.66s -> 864.22s]  I'll put it way up here.
[864.22s -> 864.94s]  Yeah.
[864.94s -> 865.44s]  Oh, yeah.
[865.44s -> 865.94s]  I know.
[865.94s -> 866.94s]  I'll give that a shot.
[873.98s -> 876.50s]  Well, it seems to work.
[876.50s -> 877.58s]  Yeah, that makes sense.
[880.18s -> 884.46s]  OK, so now I get my write fault,
[884.46s -> 886.00s]  which is what we're hoping for.
[886.00s -> 887.26s]  Hopefully what this write fault means
[887.26s -> 889.92s]  is that one of the other processes tried to modify the page,
[889.92s -> 893.26s]  and the RISC-V generated a page fault from that.
[893.26s -> 896.22s]  And we're going to want to react to that
[896.22s -> 904.62s]  by making a copy of the page and mapping it read-write.
[904.62s -> 907.22s]  Before we do that, just to make sure
[907.22s -> 913.62s]  that what we're seeing, it's this F here, this S cause,
[913.62s -> 915.66s]  that is sort of making me imagine
[915.66s -> 918.26s]  that we're seeing what we want to see, namely a store fault.
[918.26s -> 922.42s]  Let's actually go and look and see where that happened.
[922.42s -> 924.66s]  We know it's in process ID 3.
[924.66s -> 926.16s]  Process ID 1 is in it.
[926.16s -> 929.46s]  Process ID 2 is the shell.
[929.46s -> 933.18s]  And so maybe process ID 3 is going to be cow test.
[933.18s -> 944.74s]  We can look in cowtest.asm and look
[944.74s -> 952.42s]  for 9DA and see if it makes sense as a instruction that
[952.42s -> 958.14s]  could generate a store page fault.
[958.14s -> 959.46s]  Boy, this doesn't look so good.
[963.90s -> 965.78s]  I can't even remember what AUIPC does,
[965.78s -> 969.42s]  but I don't believe it performs a store.
[969.42s -> 970.98s]  Any guesses what's going on?
[974.74s -> 980.86s]  It would be nice if the page faults we were seeing
[980.86s -> 981.74s]  actually made sense.
[987.06s -> 988.38s]  I'll tell you.
[988.38s -> 992.74s]  It turns out this page fault occurred in the shell
[992.74s -> 997.10s]  after the shell forked, but before it exec'd cowtest.
[997.10s -> 999.90s]  And so we're really looking at a wrong ASM file.
[999.90s -> 1002.46s]  It turns out the right ASM file is the shell ASM file.
[1002.98s -> 1006.18s]  We'll look for 9DA there.
[1006.18s -> 1006.94s]  Oh, yes.
[1006.94s -> 1010.54s]  And in the shell, 9DA is a store instruction.
[1010.54s -> 1013.82s]  And it's at the beginning of this parse command function
[1013.82s -> 1016.82s]  that actually parses the cowtest command that we
[1016.82s -> 1020.70s]  typed and figures out what to do with it, namely call exec.
[1020.70s -> 1023.06s]  And it's doing a store into the stack.
[1023.06s -> 1027.66s]  It's the preamble that stores the callee saved
[1027.66s -> 1030.22s]  registers.
[1030.22s -> 1031.62s]  And not surprisingly, this should
[1031.62s -> 1035.02s]  be the first page fault we get because it's on the stack.
[1035.02s -> 1037.26s]  The very first thing most functions do after fork
[1037.26s -> 1039.58s]  returns is they modify their variables in the stack
[1039.58s -> 1041.50s]  and boom.
[1041.50s -> 1044.38s]  Unsurprisingly, that causes a store page fault.
[1044.38s -> 1047.30s]  So any questions about why we're faulting?
[1050.94s -> 1052.02s]  I have a quick question.
[1052.02s -> 1055.02s]  How did you figure out it was in the shell rather than
[1055.02s -> 1057.10s]  in cowtest?
[1057.10s -> 1060.10s]  Because I looked at 9DA in cowtest,
[1060.10s -> 1063.10s]  and that is not a store instruction.
[1063.10s -> 1068.94s]  I thought, gosh, what could possibly be going on?
[1068.94s -> 1070.42s]  And I looked at process ID.
[1070.42s -> 1072.30s]  So is PID 3 the shell?
[1072.30s -> 1076.82s]  PID 3 is, PID 2 is the shell that printed that prompt.
[1076.82s -> 1080.18s]  PID 3 is the next process that's created.
[1080.18s -> 1085.54s]  And so initially, the shell forks to make PID 3.
[1085.54s -> 1088.70s]  But after the fork, it's still running the shell.
[1088.70s -> 1093.06s]  It's a child of the shell running a copy of the shell.
[1093.06s -> 1096.02s]  It's going to call exec to run cowtest,
[1096.02s -> 1098.22s]  but it hasn't done it yet.
[1098.22s -> 1099.78s]  It hasn't been able to get that far
[1099.78s -> 1108.14s]  without taking a store page fault when it writes a stack.
[1108.14s -> 1109.14s]  So can you clarify?
[1109.14s -> 1113.14s]  Does that mean that there's an error with the shell
[1113.14s -> 1115.02s]  related to process ID 2 because it
[1115.02s -> 1117.58s]  doesn't have the proper data in order
[1117.58s -> 1119.86s]  for the child to have the right data,
[1119.86s -> 1124.46s]  or is it an error with process ID 3?
[1124.46s -> 1129.86s]  It's a page fault that occurs in process ID 3.
[1129.86s -> 1131.82s]  Right, but the bug that we're seeing right here.
[1131.82s -> 1134.74s]  There's no bug.
[1134.74s -> 1138.86s]  This is, I'm implementing copy on write fork.
[1138.90s -> 1146.74s]  And what we just did in VM.c was write protect every single page
[1146.74s -> 1149.98s]  in the parent and the child.
[1149.98s -> 1151.82s]  That's what this line is doing.
[1151.82s -> 1154.14s]  So the next time either the parent or the child
[1154.14s -> 1157.18s]  does a store from user code, it's
[1157.18s -> 1159.42s]  going to be a store into a write protected page
[1159.42s -> 1161.10s]  because every page is write protected.
[1161.10s -> 1163.78s]  And so it will cause a page fault.
[1163.78s -> 1166.58s]  Oh, so this is the proper page fault that we want.
[1166.58s -> 1168.50s]  Right, and I was just while I was
[1168.54s -> 1172.74s]  doing is checking that what's going on
[1172.74s -> 1175.46s]  is exactly what we think is going on as opposed
[1175.46s -> 1176.70s]  to some other bug.
[1176.70s -> 1177.94s]  Gotcha, makes sense.
[1177.94s -> 1180.26s]  OK, all right, so we took a page fault here
[1180.26s -> 1182.42s]  in this store, which is presumably the first store
[1182.42s -> 1183.86s]  the shell does after the fork.
[1186.42s -> 1189.30s]  OK, so now we have to handle these.
[1189.30s -> 1193.30s]  Now you want to do something useful in this page faults.
[1193.34s -> 1201.14s]  So first the page fault handlers and trap.c and user
[1201.14s -> 1207.42s]  fault. I think it pretty much doesn't matter
[1207.42s -> 1211.14s]  where we insert code to mean that there's
[1211.14s -> 1213.94s]  all these different cases here that user
[1213.94s -> 1216.02s]  vec is user vec, right?
[1216.02s -> 1218.86s]  Yeah, or user trap, that user trap is dealing with.
[1218.86s -> 1222.22s]  We're just going to throw in another else.
[1222.22s -> 1224.30s]  We're only interested in write faults.
[1224.30s -> 1226.18s]  We're happy to read these shared pages
[1226.18s -> 1228.54s]  because all the data is there.
[1228.54s -> 1230.30s]  It's just writes we want to catch, so.
[1237.10s -> 1240.14s]  Now it turns out, as I'm sure you know,
[1240.14s -> 1242.86s]  that we're going to need to use the code I'm about to write
[1242.86s -> 1244.54s]  from two different places.
[1244.54s -> 1246.46s]  So I'm going to just wrap it up
[1246.46s -> 1252.70s]  in the code that makes the copy of this page in a function
[1252.70s -> 1255.82s]  I'm going to call cowFault. It needs
[1255.82s -> 1258.42s]  to know the current page table, and it
[1258.42s -> 1261.34s]  needs to know the virtual address that we faulted on.
[1261.34s -> 1263.58s]  That's stval.
[1263.58s -> 1267.82s]  Sometimes it can fail because, say, it runs out of memory.
[1267.82s -> 1269.50s]  Maybe chaotic will fail.
[1269.50s -> 1271.54s]  So we need to prepare for return value.
[1271.54s -> 1274.90s]  And pretty much no matter what the failure is,
[1274.94s -> 1278.06s]  we're going to kill this process.
[1278.06s -> 1281.34s]  And if we didn't kill it, we'll have cowFault return minus 1
[1281.34s -> 1282.98s]  if there's a failure.
[1282.98s -> 1285.10s]  Return 0 if there's no failure.
[1285.10s -> 1286.74s]  If there's no failure, then we're
[1286.74s -> 1289.02s]  going to rely on cowFault having made this virtual
[1289.02s -> 1291.82s]  address writable so that when user trap returns,
[1291.82s -> 1294.58s]  the process can actually continue successfully.
[1300.10s -> 1304.18s]  Is there a reason why you put this line above the else if
[1304.18s -> 1305.86s]  which devinter line?
[1308.74s -> 1310.74s]  I don't think so.
[1310.74s -> 1313.90s]  Whether it's correct depends on what devinter does.
[1313.90s -> 1315.46s]  We'll just take a look.
[1315.46s -> 1319.34s]  The risk is that devinter might
[1319.34s -> 1321.98s]  see a device interrupt for some reason,
[1321.98s -> 1324.02s]  even though we are in a page fault.
[1324.02s -> 1330.70s]  So we just got to check that devinter looks at scause.
[1330.70s -> 1335.10s]  And if scause is F, it's going to return 0.
[1335.10s -> 1343.18s]  So we just want to make sure that if devinter returns 0,
[1343.18s -> 1347.50s]  then it's going to go on to our code.
[1347.50s -> 1349.42s]  So that's fine.
[1349.42s -> 1350.66s]  OK.
[1350.66s -> 1353.82s]  OK, now we only need to write.
[1353.82s -> 1355.98s]  Actually, there's one thing I want to observe here.
[1355.98s -> 1359.98s]  And that's that if there's a failure, we kill the process.
[1360.02s -> 1362.66s]  So this is unfortunate.
[1362.66s -> 1366.54s]  It would be fantastic if instead, most of the time,
[1366.54s -> 1368.34s]  processes allocate memory or whatever
[1368.34s -> 1369.46s]  by calling system calls.
[1369.46s -> 1372.46s]  The system calls can return some error value, negative 1
[1372.46s -> 1373.98s]  or something, if there's a failure.
[1373.98s -> 1377.26s]  And the process can then do something appropriate
[1377.26s -> 1379.86s]  if it knows how in order to deal with that failure.
[1379.86s -> 1381.82s]  Here, though, because there's no system call,
[1381.82s -> 1384.54s]  there's not any obvious way to tell the process
[1384.54s -> 1389.86s]  that something went wrong, which is irritating.
[1389.86s -> 1391.54s]  You can imagine solutions to this.
[1391.54s -> 1395.46s]  For example, in some more sophisticated operating system,
[1395.46s -> 1400.14s]  we could have something like the alarm user fault handler
[1400.14s -> 1402.50s]  that you implemented that we could call up into to say,
[1402.50s -> 1404.74s]  look, we can't continue your process
[1404.74s -> 1407.38s]  because we can't fix this page fault,
[1407.38s -> 1410.90s]  but we want to tell you something went wrong.
[1410.90s -> 1413.86s]  But we're not going to do that now.
[1413.86s -> 1416.10s]  It's much simpler to just kill the process.
[1416.10s -> 1418.54s]  OK, so we have to implement page.
[1418.54s -> 1426.50s]  This COW fault program takes a page table.
[1426.50s -> 1428.42s]  The current process page table is argumented.
[1428.42s -> 1431.18s]  The virtual address that the fault occurred on.
[1435.06s -> 1436.86s]  OK, so the first question we always
[1436.86s -> 1439.66s]  have to worry about when dealing with virtual addresses
[1439.66s -> 1442.42s]  that the user program produced, like this one,
[1442.42s -> 1447.38s]  is what if it's a completely crazy address, like way
[1447.38s -> 1448.82s]  up above the top of the process,
[1448.82s -> 1452.54s]  or what if it's an address like in the trap frame
[1452.54s -> 1455.66s]  or in the trampoline page or in the page,
[1455.66s -> 1457.10s]  the stack guard page?
[1462.58s -> 1464.46s]  So can anybody sort of outline a strategy
[1464.46s -> 1470.70s]  for defending ourselves against the user process intentionally
[1470.70s -> 1472.90s]  defaulting on crazy addresses?
[1479.26s -> 1483.54s]  Isn't there like a max virtual address?
[1483.54s -> 1484.82s]  Yes, yes, there is.
[1484.82s -> 1487.86s]  And that will actually come up.
[1487.86s -> 1491.30s]  But the top of the user address space
[1491.30s -> 1497.26s]  is below max VA.
[1497.26s -> 1504.10s]  And the stack guard page, which we shouldn't be allowing,
[1504.10s -> 1507.06s]  certainly shouldn't be treating as a copy on right thing.
[1507.06s -> 1513.98s]  All right, I'll show a solution here.
[1513.98s -> 1515.86s]  We're just going to call walk for now.
[1516.70s -> 1521.74s]  And one thing that can go wrong is walk
[1521.74s -> 1523.78s]  doesn't find a page table entry.
[1523.78s -> 1529.22s]  So for most illegal pages that a process might refer to,
[1529.22s -> 1531.26s]  there's no PTE.
[1531.26s -> 1533.22s]  And there's only a few exceptions to that.
[1533.22s -> 1538.42s]  So if we detect the illegal, if we detect the PTE not
[1538.42s -> 1542.70s]  found return value from walk and return minus 1,
[1542.70s -> 1544.74s]  we've handled almost every case.
[1544.78s -> 1548.46s]  The only cases I'm aware of where walk would return a page
[1548.46s -> 1551.38s]  table entry, but we don't want to allow rights to it,
[1551.38s -> 1557.38s]  are for the trampoline, trap frame, and stack guard pages.
[1557.38s -> 1561.02s]  And those all have PTEU clear.
[1561.02s -> 1563.94s]  So the way I'm going to detect crazy addresses,
[1563.94s -> 1571.94s]  illegal addresses, is by checking that PTEU is set.
[1571.98s -> 1575.82s]  And also, just for chuckles, look at PTEV.
[1578.70s -> 1584.38s]  And if either of those is clear, then we know this is like not
[1584.38s -> 1586.78s]  an OK address.
[1586.78s -> 1590.34s]  And we'll just return minus 1, which will kill the process.
[1590.34s -> 1600.86s]  So I believe we've taken care of all the illegal addresses,
[1600.86s -> 1605.30s]  the user process could try to use.
[1605.30s -> 1607.74s]  How can we find out, we want to do a copy now.
[1607.74s -> 1611.70s]  We want to make this process a copy of the page.
[1611.70s -> 1614.50s]  How do we find the page to copy from?
[1614.50s -> 1615.98s]  I have a question here.
[1615.98s -> 1617.90s]  Please.
[1617.90s -> 1623.58s]  Wouldn't the strategy of using the PTEU to judge if the page
[1623.58s -> 1629.18s]  is, if it's a legit page for copy and write,
[1629.22s -> 1631.22s]  wouldn't the strategy not be very
[1631.22s -> 1633.22s]  good for the long-term process?
[1633.22s -> 1639.54s]  Maybe you add some other part to the address space, which
[1639.54s -> 1643.34s]  is supposed to be a writable, which is only
[1643.34s -> 1644.62s]  supposed to be readable.
[1644.62s -> 1647.10s]  But that doesn't get taken care of here.
[1647.10s -> 1649.26s]  As in, you'll have to come back here again.
[1649.26s -> 1650.70s]  You're absolutely right.
[1650.70s -> 1656.02s]  If we were to add any other interesting paging feature,
[1656.18s -> 1659.10s]  maybe lazy page allocation, for example,
[1659.10s -> 1661.06s]  we'd have to revisit these decisions and come up
[1661.06s -> 1663.14s]  with some other strategy for deciding,
[1663.14s -> 1664.94s]  is this a copy and write page?
[1664.94s -> 1667.26s]  Is this a lazy allocation page?
[1667.26s -> 1669.78s]  Is this a paged out to disk page?
[1669.78s -> 1672.98s]  Is this a memory mapped file?
[1672.98s -> 1676.66s]  And we'd need more sophisticated.
[1676.66s -> 1680.30s]  In the end, I think most serious operating systems
[1680.30s -> 1682.98s]  actually keep their own data structure,
[1682.98s -> 1685.46s]  nothing to do with not a page table,
[1685.46s -> 1688.62s]  but mimicking a page table that describes the process's
[1688.62s -> 1691.94s]  address space and describes what each page means
[1691.94s -> 1692.98s]  and what its state is.
[1692.98s -> 1696.50s]  And we'd have to consult that table.
[1696.50s -> 1699.30s]  But for this lab, since we don't have those other
[1699.30s -> 1702.70s]  features and we're not sure what they would be,
[1702.70s -> 1704.66s]  I'm just going to do something straightforward.
[1704.66s -> 1706.70s]  And indeed, we would have to come back and fix it
[1706.70s -> 1711.18s]  if we made xv6 more sophisticated.
[1711.22s -> 1715.90s]  Wouldn't that panic if VA is more than max VA
[1715.90s -> 1717.10s]  because of the walk?
[1722.54s -> 1723.70s]  Gosh, maybe it would.
[1727.98s -> 1728.82s]  Yeah, we'll look at that.
[1728.82s -> 1730.78s]  Yeah, maybe if the process...
[1732.90s -> 1735.10s]  Yeah, I think you're right.
[1735.10s -> 1736.18s]  Well, that's too bad.
[1736.18s -> 1737.66s]  Okay, so we were certainly...
[1738.62s -> 1739.46s]  You're right.
[1739.46s -> 1742.02s]  I think I was wrong about this.
[1742.02s -> 1744.54s]  Wait, can you just return a negative one
[1744.54s -> 1746.26s]  if you just check the max VA?
[1746.26s -> 1750.02s]  I think the observation is that if the user tries
[1750.02s -> 1751.14s]  to intentionally...
[1751.14s -> 1754.78s]  Program tries to write a very large address,
[1754.78s -> 1757.50s]  of course, I can't expect to survive that.
[1759.94s -> 1762.58s]  But what we would do here is call walk
[1762.58s -> 1764.34s]  with a very large address.
[1764.34s -> 1766.58s]  And I'm looking at here the implementation of walk
[1766.58s -> 1770.18s]  and it's going to panic.
[1771.42s -> 1773.58s]  I think we can just use the same approach
[1773.58s -> 1774.74s]  as walk address does.
[1774.74s -> 1776.38s]  I had the same bug.
[1776.38s -> 1777.90s]  You can just return minus one
[1777.90s -> 1779.30s]  like other people were saying,
[1779.30s -> 1782.70s]  if VA is greater than max VA before walk.
[1784.62s -> 1786.18s]  What I'm upset about is that the tests
[1786.18s -> 1787.62s]  don't seem to test this
[1788.98s -> 1791.14s]  because I didn't put this in my solution.
[1792.34s -> 1793.18s]  Okay.
[1797.58s -> 1798.42s]  Okay.
[1799.34s -> 1801.50s]  Okay, so how do we find out?
[1801.50s -> 1804.06s]  We want to copy the page.
[1804.06s -> 1809.06s]  How do I get a pointer to something I can copy?
[1811.86s -> 1813.46s]  The PTA to PA?
[1813.46s -> 1814.58s]  Yeah.
[1814.58s -> 1816.94s]  So it's just in the PTA.
[1823.82s -> 1825.38s]  And where am I going to copy to?
[1826.58s -> 1831.58s]  What should I copy to?
[1833.50s -> 1835.38s]  A nearly allocated page.
[1835.38s -> 1836.34s]  Yeah.
[1836.34s -> 1838.18s]  So I just call kalloc.
[1839.50s -> 1842.34s]  Of course, kalloc is likely to fail.
[1842.34s -> 1845.46s]  So I got to catch that.
[1846.86s -> 1848.66s]  We're actually going to see this.
[1851.18s -> 1853.66s]  So I'm going to put in a print statement so we know.
[1855.06s -> 1855.90s]  Okay, now we're-
[1855.90s -> 1857.06s]  PA2, not PA1.
[1862.62s -> 1864.94s]  I actually made this error and I wanted to reproduce it
[1864.94s -> 1866.22s]  so that we would have to track it down,
[1866.22s -> 1868.30s]  but you're too clever for me.
[1870.66s -> 1875.66s]  All right, I want to copy to PA2 from PA1.
[1880.66s -> 1882.62s]  Okay, so how can I,
[1882.62s -> 1885.66s]  what do I need to do in order to map this page?
[1885.66s -> 1890.66s]  This new page, PA2, went to the address space.
[1895.62s -> 1896.50s]  What should I type?
[1897.90s -> 1899.58s]  There's like two broadways.
[1899.58s -> 1902.50s]  There's either unmapping and then mapping
[1902.50s -> 1904.46s]  with the built-ins that exist,
[1904.46s -> 1907.90s]  or you could manipulate the bits.
[1907.90s -> 1911.46s]  Yeah, well, I am way too lazy to figure out
[1911.46s -> 1913.58s]  how to use munmap and mmap.
[1913.58s -> 1916.30s]  So I'm going to just cook up
[1916.30s -> 1918.02s]  a new page table entry right here.
[1923.10s -> 1924.18s]  I have a question.
[1924.18s -> 1927.82s]  So actually I ran into this issue in almost every lab
[1927.82s -> 1932.66s]  in which I somehow needed to remap things.
[1932.66s -> 1936.90s]  And because it occurred so often,
[1936.90s -> 1940.34s]  I implemented a map that,
[1941.26s -> 1943.86s]  which could remap basically.
[1943.86s -> 1948.10s]  And I was wondering, is that a bad design choice?
[1948.10s -> 1952.06s]  Is it like dangerous to allow remapping?
[1952.06s -> 1953.54s]  That's totally reasonable.
[1953.54s -> 1958.54s]  The functions in XV6 are specialized to XV6 as it exists.
[1962.78s -> 1967.78s]  And so they make assumptions about how the other code works.
[1967.78s -> 1968.90s]  They have panics in them
[1968.94s -> 1971.06s]  that are intended to catch other code
[1971.06s -> 1972.62s]  using them in unexpected ways.
[1972.62s -> 1974.58s]  But if you change the way XV6 works
[1974.58s -> 1975.86s]  and you need to do different things,
[1975.86s -> 1978.58s]  then it's totally reasonable to modify.
[1979.50s -> 1982.94s]  Oh, you need to be able to remap this without a panic.
[1982.94s -> 1985.30s]  Well, old XV6 never needs to do that.
[1986.98s -> 1988.42s]  But your code does,
[1988.42s -> 1991.62s]  so you should feel free to modify anything you need to.
[1994.70s -> 1998.34s]  Okay, so the deal is here,
[1998.82s -> 1999.70s]  we had a write protected page
[1999.70s -> 2001.90s]  that was probably shared with another process.
[2001.90s -> 2003.74s]  What do we have to do to that other process
[2003.74s -> 2004.74s]  at this point?
[2008.46s -> 2010.74s]  What do we need to do to the other process?
[2015.62s -> 2018.18s]  We can just not do anything.
[2018.18s -> 2020.82s]  And then when a page falls,
[2020.82s -> 2025.78s]  we allocate a new page and remove the original one.
[2025.78s -> 2026.94s]  That's absolutely right.
[2026.98s -> 2028.30s]  So you could imagine doing something,
[2028.30s -> 2030.74s]  but it would be quite complex to do it correctly.
[2031.62s -> 2033.34s]  And so we just do nothing.
[2033.34s -> 2036.06s]  And if the other process never writes this page,
[2036.06s -> 2037.46s]  well, it's not a problem.
[2037.46s -> 2038.38s]  If it does write a page,
[2038.38s -> 2039.90s]  then it'll go through all this,
[2039.90s -> 2041.18s]  it'll make a copy,
[2041.18s -> 2044.90s]  and it'll proceed safely as well.
[2044.90s -> 2046.86s]  So doing nothing is totally reasonable.
[2049.30s -> 2050.54s]  Okay, so now we're gonna return,
[2050.54s -> 2053.78s]  the return value of zero says there was no error.
[2053.78s -> 2056.58s]  And hopefully the process will.
[2058.30s -> 2062.10s]  Well, we should free the physical page, right?
[2062.10s -> 2063.46s]  Or are we not there yet?
[2064.30s -> 2065.70s]  Well, we can do it or not.
[2068.46s -> 2070.90s]  I guess if we're going steps, we shouldn't, yeah.
[2070.90s -> 2074.22s]  So if I did this, what would happen?
[2074.22s -> 2076.46s]  Well, we would definitely crash right now.
[2076.46s -> 2078.18s]  All right, so let's not do this.
[2079.14s -> 2082.30s]  Or I don't know if we would crash,
[2082.34s -> 2086.62s]  but we would then be, if we freed a page,
[2086.62s -> 2088.82s]  it would be presumably PA one,
[2088.82s -> 2091.66s]  because we better not free PA two,
[2091.66s -> 2092.70s]  because we're using PA two.
[2092.70s -> 2093.58s]  If we free PA one,
[2093.58s -> 2097.10s]  that means the other process has got a problem.
[2097.10s -> 2098.82s]  It is now mapped into its address space
[2098.82s -> 2100.42s]  and presumably using, you know,
[2100.42s -> 2102.82s]  executing instructions in, or I don't know what,
[2102.82s -> 2104.22s]  a page that we just freed
[2104.22s -> 2106.14s]  and might be reusing for some other purpose
[2106.14s -> 2108.50s]  and writing something else over.
[2108.50s -> 2110.34s]  So I'm reluctant to free it.
[2112.58s -> 2117.58s]  Although it's, oh yeah, so sadly,
[2118.94s -> 2120.58s]  among many other things,
[2121.74s -> 2126.62s]  xv6-walk is never called from outside of vm.c
[2127.78s -> 2130.66s]  in ordinary xv6, but now we're doing it.
[2140.54s -> 2142.06s]  All right, where were we?
[2142.30s -> 2143.14s]  Oh.
[2148.58s -> 2151.82s]  Okay, remember the very first page fault we got
[2151.82s -> 2152.66s]  was an scos2.
[2153.90s -> 2156.50s]  Why are we getting scos2 page faults?
[2163.66s -> 2165.22s]  That's an illegal instruction.
[2170.22s -> 2172.14s]  Like this is the next problem.
[2172.94s -> 2174.26s]  We have to solve this problem now.
[2175.30s -> 2176.18s]  What's gone wrong?
[2181.62s -> 2184.10s]  Or what are some plausible guesses
[2184.10s -> 2185.70s]  for what might have gone wrong?
[2192.66s -> 2195.54s]  So the sequence we, when I type cow test,
[2195.54s -> 2198.74s]  what we think happens is that the shell forks,
[2198.74s -> 2200.18s]  it's copy and write fork,
[2201.06s -> 2206.06s]  we have the child running the shell's instructions,
[2208.50s -> 2210.74s]  probably taking store faults,
[2210.74s -> 2212.94s]  but we're handling those store faults correctly.
[2212.94s -> 2215.14s]  And then the child execs,
[2215.14s -> 2217.78s]  the child copy of the shell execs cow test.
[2223.26s -> 2226.30s]  What bad thing might that do to the parent shell?
[2231.18s -> 2234.50s]  Well, when you call exec, the implementation of exec
[2235.94s -> 2239.30s]  frees up all the processes current pages
[2240.42s -> 2243.06s]  and then allocates new pages
[2243.06s -> 2246.06s]  to load the file into that you're execing.
[2248.38s -> 2250.22s]  What's gonna happen in the child?
[2250.22s -> 2251.70s]  The child shell that calls exec
[2251.70s -> 2253.82s]  when it frees all of its pages.
[2253.82s -> 2258.82s]  What's that gonna do to the parent shell?
[2261.94s -> 2263.78s]  Is it gonna end up-
[2263.78s -> 2264.62s]  Oh, sorry.
[2265.54s -> 2268.92s]  Is it gonna accidentally free the shells,
[2270.02s -> 2272.26s]  page, pages?
[2272.26s -> 2273.54s]  Yes, we're gonna free every single one
[2273.54s -> 2275.74s]  of the shells pages except the one or two
[2275.74s -> 2278.18s]  that were written.
[2278.18s -> 2281.06s]  So this includes the shells instruction pages,
[2281.06s -> 2283.30s]  and then we're gonna reallocate them.
[2283.74s -> 2284.66s]  And then they're gonna be instantly reallocated
[2284.66s -> 2286.82s]  by exec to hold other stuff loaded
[2286.82s -> 2289.46s]  from the file that we're running, cow test.
[2289.46s -> 2290.78s]  So that's gonna totally change everything
[2290.78s -> 2291.82s]  in the shell underfoot.
[2291.82s -> 2295.60s]  And unsurprisingly, it's gonna instantly crash.
[2295.60s -> 2299.44s]  So we have to not free those pages somehow.
[2302.02s -> 2303.26s]  Does everyone see that?
[2304.22s -> 2305.46s]  I have a good question.
[2305.46s -> 2308.18s]  What does the SCPC here point to?
[2308.18s -> 2310.94s]  Which assembly file would keep on that instruction?
[2311.90s -> 2312.74s]  Well.
[2322.62s -> 2324.06s]  You said it's shell, right?
[2325.10s -> 2328.62s]  Last time was for some other fault somewhere else.
[2328.62s -> 2329.46s]  It was shell.
[2329.46s -> 2330.28s]  What it is here?
[2330.28s -> 2331.12s]  I actually don't know.
[2331.12s -> 2334.38s]  I never, I did not, unfortunately,
[2334.38s -> 2336.18s]  take the time to track this down.
[2339.42s -> 2340.74s]  So one problem with any answer,
[2341.58s -> 2342.50s]  even if I could give you an answer.
[2342.50s -> 2345.16s]  So the answer is either the shell or cow test.
[2346.66s -> 2350.58s]  For process ID three, that again is the,
[2351.70s -> 2353.52s]  it's probably cow test.
[2354.50s -> 2357.66s]  I have to say I don't completely know what's going on here.
[2357.66s -> 2360.98s]  Looking at the ASM files unlikely to be helpful
[2360.98s -> 2363.82s]  because the whole problem that we believe has happened
[2363.82s -> 2368.54s]  is that the page holding the instructions
[2368.54s -> 2370.74s]  was freed and reused.
[2370.74s -> 2375.14s]  And so they were therefore not executing the instructions
[2375.14s -> 2377.66s]  and it's in the ASM file anymore.
[2377.66s -> 2379.94s]  We're executing some garbage.
[2379.94s -> 2381.66s]  That makes sense, yeah.
[2381.66s -> 2383.38s]  So we could look at the address 1004,
[2383.38s -> 2387.16s]  but it wouldn't really tell us what had happened.
[2388.34s -> 2389.86s]  We might, we probably could find this out
[2389.86s -> 2390.86s]  using the debugger.
[2392.64s -> 2396.42s]  We could break in GDB and then look at the instructions.
[2396.42s -> 2401.42s]  It's actually at whatever virtual address 1004 points to.
[2402.62s -> 2406.22s]  Quick follow up question about that fault.
[2407.06s -> 2409.74s]  So my understanding is that we're getting
[2409.74s -> 2411.46s]  like an invalid instruction fault
[2411.46s -> 2415.78s]  because like we're changing the instruction page,
[2415.78s -> 2417.42s]  instruction physical memory underfoot
[2417.42s -> 2419.02s]  and like the memory is just being written
[2419.02s -> 2419.98s]  to something else.
[2421.26s -> 2425.54s]  Is it possible that like in some lucky case
[2425.58s -> 2427.98s]  that they are being rewritten,
[2427.98s -> 2429.00s]  but when we look at it again,
[2429.00s -> 2431.98s]  it's being rewritten to valid instruction pages.
[2431.98s -> 2434.72s]  So we actually just start executing like
[2434.72s -> 2435.56s]  random instructions
[2435.56s -> 2437.58s]  and we don't actually get an invalid instruction fault.
[2437.58s -> 2438.94s]  We get some other fault.
[2438.94s -> 2441.84s]  Yes, absolutely, absolutely.
[2442.78s -> 2443.62s]  Yeah, yeah.
[2443.62s -> 2446.92s]  I mean, really we're now like anything could happen.
[2448.86s -> 2450.56s]  I would not have been able to guess
[2450.56s -> 2453.42s]  if I had not done this lab,
[2455.98s -> 2457.34s]  wouldn't be able to guess what would go wrong
[2457.34s -> 2458.18s]  at this point.
[2461.06s -> 2464.96s]  And I think I have seen other strange things
[2464.96s -> 2466.26s]  happen at this point.
[2467.90s -> 2468.78s]  It's maybe the data,
[2468.78s -> 2470.94s]  maybe the first page that has garbage in it
[2470.94s -> 2472.10s]  actually has data in it
[2472.10s -> 2475.48s]  and the shell trips over some complete garbage data,
[2475.48s -> 2477.86s]  like a stack instead of instructions.
[2480.30s -> 2482.90s]  Okay, so we don't wanna free the page right away.
[2482.90s -> 2484.10s]  We only wanna free the page
[2484.10s -> 2486.18s]  when it's really not being used anymore.
[2489.10s -> 2494.10s]  So can anybody propose a sort of criterion
[2494.46s -> 2499.46s]  for when we should or shouldn't free each page of memory?
[2500.42s -> 2503.22s]  We could keep track of like how many times
[2504.08s -> 2506.90s]  or like basically whenever in Kalloc,
[2506.90s -> 2510.34s]  whenever you should increment and decrement
[2510.34s -> 2511.82s]  some type of variable
[2511.82s -> 2515.42s]  and keep an array of like all of the different addresses.
[2515.42s -> 2516.58s]  Yes, yeah, exactly.
[2516.58s -> 2518.42s]  So one way to think about this
[2518.42s -> 2520.66s]  is that we wanna only free a page
[2520.66s -> 2525.12s]  when there's zero page tables that refer to it.
[2526.26s -> 2528.82s]  But there could be many if a program forks
[2528.82s -> 2530.54s]  and then forks again and then forks again,
[2530.54s -> 2533.66s]  boy, we can now have like three or four different processes
[2533.66s -> 2536.02s]  that all refer due to copy and write fork
[2536.02s -> 2537.30s]  to this one page.
[2538.30s -> 2542.90s]  So this count of how many page tables refer to a page
[2542.90s -> 2546.22s]  can go up due to fork and it can go back down
[2546.22s -> 2548.54s]  when a process exits or calls exec,
[2549.58s -> 2551.26s]  it's clears all these references
[2551.26s -> 2552.78s]  out of its page table entry,
[2552.78s -> 2555.54s]  or if a process actually does a write
[2555.54s -> 2557.16s]  and causes a write fault,
[2557.16s -> 2559.18s]  that's also a situation in which
[2559.18s -> 2562.76s]  we have one fewer reference from one fewer page tables
[2562.76s -> 2564.54s]  referring to a page.
[2564.54s -> 2565.86s]  So we wanna keep a count
[2566.86s -> 2570.02s]  and we wanna be counting the number of page table entries
[2570.02s -> 2571.32s]  that refer to a page.
[2572.26s -> 2573.86s]  So we're gonna need to think about
[2573.86s -> 2574.90s]  how to maintain this count
[2574.90s -> 2578.00s]  and when to increment it exactly
[2578.00s -> 2580.70s]  where on the code to increment it and decrement it.
[2583.10s -> 2587.50s]  Okay, so there's, as you may be aware,
[2587.50s -> 2592.50s]  there's multiple ways to maintain this count.
[2593.50s -> 2598.50s]  The way I do it is I make an array called ref count,
[2599.22s -> 2602.34s]  which is gonna have it for every page account.
[2603.38s -> 2605.86s]  We need to know how many entries are in ref count.
[2605.86s -> 2610.14s]  In Xe6, it's so simple that it just has,
[2610.14s -> 2612.96s]  it just uses a fixed amount of memory, physical memory,
[2612.96s -> 2614.26s]  right, we only need to keep a count
[2614.26s -> 2615.80s]  per page of physical memory.
[2617.06s -> 2621.28s]  We know that from inspecting KNN,
[2621.28s -> 2626.28s]  that Xe6 uses only phys top amount of physical memory.
[2627.60s -> 2630.50s]  We only need to keep a count per page, not per byte.
[2630.50s -> 2634.00s]  So we're gonna divide phys top by 4096.
[2634.00s -> 2636.04s]  That's how many array elements we need.
[2639.00s -> 2640.48s]  In a more serious operating system
[2640.48s -> 2642.80s]  where we don't know how much memory is available
[2642.80s -> 2644.54s]  until we inspect the hardware,
[2644.54s -> 2645.88s]  we might have to,
[2645.88s -> 2650.34s]  we would have to allocate this array dynamically.
[2650.34s -> 2653.34s]  Any questions about where to put the counts?
[2657.78s -> 2659.10s]  All right.
[2659.10s -> 2661.18s]  Actually, yes, I'm wondering,
[2661.18s -> 2664.02s]  is there a reason that you're using 4096 specifically
[2664.02s -> 2665.90s]  and not the page size macro?
[2667.50s -> 2670.42s]  Yeah, the reason is that I can remember 4096
[2670.42s -> 2673.24s]  and I don't remember the name of the page size macro,
[2673.24s -> 2675.10s]  but I'm happy to use it if you like.
[2675.10s -> 2680.10s]  I mean, if Xe6 was intended to be portable
[2681.82s -> 2684.30s]  and to be able to run on all kinds of different machines
[2684.30s -> 2685.62s]  with different page sizes,
[2685.62s -> 2687.24s]  we'd have to be much more careful about this,
[2687.24s -> 2689.86s]  but there's 500 ways in which Xe6
[2689.86s -> 2691.70s]  is totally not portable.
[2693.70s -> 2698.58s]  So I don't worry about it.
[2698.58s -> 2702.18s]  Space size is determined in hardware and I guess where?
[2702.18s -> 2707.18s]  In the hardware, the RISC-V manual says how big a page is.
[2712.02s -> 2717.02s]  Yeah, because it's the MMU that takes a virtual address
[2719.26s -> 2722.14s]  and uses it to index into the page table.
[2722.14s -> 2723.30s]  Oh, right, right,
[2723.30s -> 2725.90s]  because the address translation happens in hardware,
[2725.90s -> 2728.42s]  so the hardware has to know how big the pages are.
[2728.42s -> 2729.54s]  Okay, that makes sense.
[2729.54s -> 2730.58s]  It's configurable,
[2730.62s -> 2732.98s]  but there's a couple of different strategies
[2732.98s -> 2734.14s]  you can tell the hardware to use,
[2734.14s -> 2737.72s]  but we tell it to use 4,096 white pages.
[2739.10s -> 2740.30s]  Okay.
[2740.30s -> 2741.70s]  Sorry for the question.
[2741.70s -> 2743.74s]  So where are, I guess this is a C question,
[2743.74s -> 2748.74s]  so where are global variables like ref count stored?
[2751.86s -> 2755.62s]  Are they associated with a specific process or?
[2755.62s -> 2756.86s]  This is the kernel.
[2757.86s -> 2760.98s]  What's going on is that the file,
[2760.98s -> 2762.46s]  the executable file
[2765.74s -> 2769.58s]  that the compiler and the loader or linker produces
[2772.26s -> 2773.42s]  has a sort of,
[2776.14s -> 2777.96s]  indicates how much date,
[2777.96s -> 2780.28s]  how the total size of all the global variables
[2780.28s -> 2781.16s]  in the program.
[2782.10s -> 2785.10s]  And so we don't see the code for this,
[2785.10s -> 2788.06s]  but when QEMU loads the kernel,
[2792.46s -> 2796.10s]  well, what's really going on is that when you compile,
[2796.10s -> 2800.98s]  the program called the linker figures out based on,
[2800.98s -> 2803.62s]  looks at all the global variables and their sizes
[2803.62s -> 2808.62s]  and assigns an address in memory to each global variable.
[2808.62s -> 2813.62s]  And that's where it lives in memory.
[2817.46s -> 2818.30s]  I mean,
[2823.86s -> 2828.24s]  and we just arranged that,
[2828.24s -> 2831.62s]  so the XV6 kernel uses, I don't know,
[2831.62s -> 2835.38s]  10,000 bytes of various global variables
[2835.38s -> 2838.46s]  and wherever the boot process loads it into memory,
[2839.18s -> 2840.78s]  so he had addressed a million,
[2840.78s -> 2845.78s]  it just uses those addresses for the global variables.
[2848.66s -> 2849.50s]  I see.
[2849.50s -> 2853.94s]  Is this similar to how there's only one copy
[2853.94s -> 2857.10s]  of the instructions for a program on disk
[2857.10s -> 2860.82s]  and there's only one copy of the kind of global variables
[2860.82s -> 2862.18s]  for that program on disk?
[2864.68s -> 2865.52s]  Well,
[2868.62s -> 2869.94s]  a program like this, well,
[2872.78s -> 2874.32s]  I'm not really sure how to answer this.
[2874.32s -> 2875.16s]  I mean,
[2878.78s -> 2881.74s]  when you declare a global variable like INTX,
[2881.74s -> 2885.02s]  when you compile, the compiler and the linker just decide
[2885.02s -> 2887.38s]  based on who knows what or it's configurable,
[2887.38s -> 2889.66s]  but they decide the address for that variable.
[2889.66s -> 2891.02s]  They decide, all right,
[2891.02s -> 2893.90s]  boy, X is gonna go to address 1000.
[2893.90s -> 2896.46s]  And then the code that reads or write X,
[2896.46s -> 2900.18s]  if we have code that says ref count of zero equals one,
[2902.52s -> 2905.38s]  what that compiled into is just setting the memory
[2905.38s -> 2908.28s]  at address 1000 or wherever the linker decided
[2908.28s -> 2911.34s]  to put ref count, which just does a store
[2911.34s -> 2914.32s]  to set the memory at address 1000 to one.
[2920.26s -> 2921.10s]  Yeah.
[2921.10s -> 2926.10s]  It's not, it's actually almost exactly the same thing
[2927.30s -> 2929.94s]  that happens when you run an ordinary user program.
[2932.30s -> 2933.78s]  You know, the linker and the compiler decide
[2933.78s -> 2936.70s]  where in memory global variables are.
[2936.70s -> 2940.90s]  They just read and write there and it just works.
[2942.58s -> 2944.78s]  I'm sorry if that's not much of an explanation.
[2948.02s -> 2948.84s]  All right.
[2948.84s -> 2951.92s]  So we just define this global array of counts,
[2951.92s -> 2953.66s]  one per physical page.
[2955.12s -> 2959.02s]  And we need to modify these counts in various places.
[2959.02s -> 2962.12s]  Certainly when we first allocate a page,
[2962.12s -> 2964.56s]  we're gonna say that it has one reference
[2964.56s -> 2966.00s]  because we return address to the page
[2966.00s -> 2968.60s]  to whoever called kalloc.
[2968.60s -> 2969.92s]  And at the moment,
[2969.92s -> 2972.68s]  only that program has a reference to this page.
[2972.68s -> 2973.68s]  So I'm just gonna,
[2979.24s -> 2982.76s]  set the ref count for this page
[2985.18s -> 2986.02s]  to be one.
[2986.02s -> 2986.84s]  So first of all,
[2986.84s -> 2988.14s]  I'm just gonna calculate the page number,
[2988.14s -> 2993.14s]  which is the address of the page divided by 4,096.
[2993.96s -> 2995.64s]  That's the page number.
[2999.20s -> 3002.28s]  And then I'm going to set the reference count
[3002.28s -> 3004.68s]  for the page we just allocated to one.
[3008.84s -> 3012.60s]  Any questions about this code?
[3017.92s -> 3020.76s]  Just because I know that I'm gonna have bugs
[3020.76s -> 3022.56s]  with reference counting,
[3022.56s -> 3025.36s]  I'm actually gonna do a little bit of a sanity check here
[3025.36s -> 3030.36s]  and just that the reference count really should be zero
[3032.80s -> 3035.40s]  if the page is, we just allocated a page that was free.
[3035.40s -> 3037.76s]  Boy, it better have a reference count of zero.
[3039.84s -> 3044.84s]  All right, any questions about this code?
[3049.84s -> 3050.76s]  All right, so in general,
[3050.76s -> 3052.84s]  we want to increment the reference count
[3052.84s -> 3056.04s]  when we add a,
[3056.04s -> 3059.32s]  when copy and write fork adds a page table entry
[3059.32s -> 3061.48s]  that points to an existing page.
[3061.48s -> 3063.12s]  And we wanna,
[3063.12s -> 3067.04s]  actually we wanna decrement the reference count.
[3067.04s -> 3069.00s]  As it turns out in many places,
[3069.00s -> 3071.04s]  for example, when a process exits,
[3071.04s -> 3072.40s]  we need to decrement the reference count
[3072.40s -> 3073.44s]  of all its pages.
[3073.44s -> 3078.20s]  When you call exec and that frees all the current memory
[3078.20s -> 3079.16s]  because it's gonna replace it,
[3079.16s -> 3080.80s]  we wanna decrement all those reference counts.
[3080.80s -> 3083.08s]  When a copy and write page fault happens
[3083.08s -> 3084.48s]  and we make a copy,
[3085.40s -> 3087.80s]  we wanna decrement a reference count on the old page.
[3087.80s -> 3089.80s]  It turns out these are all the same places
[3089.80s -> 3092.24s]  that currently free a page
[3092.24s -> 3094.20s]  because the current xe6
[3094.20s -> 3097.36s]  thinks there's only ever one reference to each page.
[3097.36s -> 3098.88s]  So in most of the places we care about
[3098.88s -> 3099.80s]  where we wanna decrement,
[3099.80s -> 3101.68s]  there's currently a call to kfree.
[3101.68s -> 3104.76s]  So what I'm gonna do is modify kfree
[3104.76s -> 3106.92s]  to have somewhat different semantics
[3106.92s -> 3111.76s]  and to have kfree be sort of a function
[3111.76s -> 3113.68s]  that decrements the reference count
[3113.68s -> 3115.44s]  and frees the page only
[3115.44s -> 3118.28s]  if the reference count has dropped to zero.
[3118.28s -> 3120.00s]  I mean, that will just automatically fix
[3120.00s -> 3121.60s]  all the places that call kfree.
[3125.20s -> 3129.44s]  All right, so we need to actually do the check
[3131.20s -> 3133.84s]  before we fill the page with junk.
[3135.20s -> 3137.40s]  So this makes the kfree a little bit more complicated
[3137.40s -> 3139.52s]  because even though there's a critical section
[3139.52s -> 3141.40s]  with locks and kfree already,
[3141.40s -> 3142.84s]  we can't use it because it happens
[3142.84s -> 3145.72s]  after the point at which we filled the page with garbage.
[3147.00s -> 3150.44s]  So we need to lock
[3150.44s -> 3154.40s]  because we could be freeing the same page at the same time
[3154.40s -> 3156.60s]  from multiple different cores.
[3156.60s -> 3161.32s]  Again, I'm gonna find the page number
[3161.32s -> 3166.32s]  by dividing the physical address by 4,096.
[3168.72s -> 3169.72s]  I wanna panic.
[3170.92s -> 3173.96s]  Again, just a sanity check.
[3173.96s -> 3175.64s]  Gosh, if we're freeing a page,
[3175.64s -> 3178.08s]  it better have more than zero ref counts.
[3181.04s -> 3186.04s]  We need to return if the page
[3187.20s -> 3189.28s]  has more than one reference count to it.
[3190.60s -> 3194.96s]  Or actually, let's decrement the reference count.
[3196.72s -> 3198.88s]  We wanna return after we release locks
[3198.88s -> 3200.36s]  and so we have to remember a variable.
[3200.36s -> 3203.00s]  We're gonna remember whether the page has more references
[3203.00s -> 3205.00s]  and then release locks and then return.
[3205.00s -> 3207.20s]  So I'm gonna make a temporary variable
[3207.20s -> 3208.40s]  which has the,
[3212.56s -> 3213.60s]  then release,
[3216.12s -> 3219.56s]  then say if there's still references to this page,
[3219.56s -> 3222.08s]  let's just return and not free it.
[3222.08s -> 3224.40s]  And only if the reference count dropped to zero
[3224.40s -> 3225.24s]  do we free it.
[3226.72s -> 3228.00s]  Any questions?
[3228.00s -> 3230.24s]  Can you explain again why you have to acquire
[3230.24s -> 3233.64s]  the kmemlock process?
[3233.64s -> 3236.64s]  Why you have to acquire it at all?
[3237.12s -> 3240.00s]  When you're doing the page number calculations.
[3240.00s -> 3241.84s]  It's this line.
[3244.88s -> 3247.20s]  Really is this line and the next line that are the problem.
[3247.20s -> 3250.44s]  The issue is that on this page,
[3250.44s -> 3253.12s]  we now is likely has more than one reference to it.
[3253.12s -> 3255.88s]  And so if two processes with a reference to the same,
[3255.88s -> 3258.48s]  with a page table entry pointed at the same page,
[3258.48s -> 3261.52s]  if they both exit at the same time on different cores,
[3261.52s -> 3264.04s]  they're both gonna call k free for the same page
[3264.04s -> 3265.16s]  at the same time.
[3266.16s -> 3267.40s]  Oh, I see.
[3267.40s -> 3268.24s]  Yeah.
[3268.24s -> 3270.56s]  Could you just create a new lock
[3270.56s -> 3274.12s]  for the reference count variable and use that as well?
[3274.12s -> 3275.16s]  Yes.
[3275.16s -> 3276.00s]  Cool.
[3277.96s -> 3278.80s]  Yes.
[3280.64s -> 3281.76s]  Yeah, basically everybody,
[3281.76s -> 3283.60s]  all code that manipulates these counts
[3283.60s -> 3284.64s]  needs to use the same block,
[3284.64s -> 3287.80s]  but I don't think it matters what lock it is.
[3290.24s -> 3293.20s]  Okay, so this takes care of most of the decrements
[3293.20s -> 3294.84s]  we care about every time a page is free,
[3294.84s -> 3296.00s]  we're really gonna, you know,
[3296.00s -> 3298.36s]  only free it if the reference count is falling to zero.
[3298.36s -> 3302.52s]  We also need to increment the reference count.
[3302.52s -> 3304.72s]  Where do we need to increment the reference count?
[3311.56s -> 3313.96s]  Well, because I know we're gonna need to do it
[3313.96s -> 3317.28s]  and write the function that anybody can call.
[3324.84s -> 3325.68s]  Okay.
[3330.96s -> 3332.92s]  Again, I wanna panic if something is weird.
[3332.92s -> 3335.36s]  So if the address is wacky,
[3335.36s -> 3340.12s]  I certainly don't wanna go beyond the end of the ray.
[3342.12s -> 3343.80s]  Or if we don't wanna increment
[3343.80s -> 3345.44s]  the reference count of a page,
[3345.44s -> 3347.44s]  whose reference count is currently zero.
[3348.60s -> 3349.72s]  That's also an error.
[3350.64s -> 3354.84s]  And I'm just putting these in there
[3354.84s -> 3357.04s]  because I know from experience with my code
[3357.04s -> 3359.04s]  that this is exactly the kind of place
[3359.04s -> 3360.32s]  I'm gonna have a bug in.
[3375.00s -> 3377.24s]  All right, where should I call inc ref from?
[3380.20s -> 3382.44s]  You have the UVM copy when you-
[3382.44s -> 3383.28s]  Yeah.
[3384.44s -> 3386.36s]  Yeah, I think UVM's copy is the only place
[3386.36s -> 3390.32s]  that makes another reference to a page.
[3396.00s -> 3398.00s]  All right, gosh, where am I gonna put this?
[3398.00s -> 3399.96s]  Well, it almost doesn't really matter.
[3401.72s -> 3404.32s]  All right, we're making another reference to PA.
[3404.32s -> 3407.36s]  So we wanna say inc ref PA.
[3410.72s -> 3415.44s]  And we're likely to run into trouble from the compiler
[3415.44s -> 3420.44s]  if we don't put a definition for it in traps.h.
[3420.68s -> 3422.00s]  Whatever.
[3422.00s -> 3422.84s]  Okay.
[3427.80s -> 3428.84s]  All right, let's see.
[3439.72s -> 3444.72s]  Oh, well, that was pretty quick.
[3452.72s -> 3454.20s]  All right, what's k-free ref?
[3455.32s -> 3456.60s]  K-free is unhappy.
[3459.72s -> 3461.24s]  Okay, well, here's the time.
[3461.24s -> 3462.08s]  We don't know.
[3462.08s -> 3463.36s]  This is very early in the boot process,
[3463.36s -> 3466.08s]  which is maybe a hint as to what's going on.
[3467.08s -> 3472.08s]  If anybody has any guesses, I'd be happy to hear them.
[3472.08s -> 3473.68s]  Meanwhile, I'll just fire up the debugger
[3473.68s -> 3475.16s]  to try to get a backtrace.
[3476.04s -> 3477.52s]  It is during k init
[3477.52s -> 3482.28s]  because you're trying to put things into the linked list
[3482.28s -> 3484.32s]  and you haven't allocated yet.
[3485.64s -> 3490.64s]  So when you're trying to load everything
[3490.92s -> 3492.44s]  into the linked list,
[3492.44s -> 3495.96s]  you haven't called kalloc before that.
[3495.96s -> 3497.84s]  Yeah, that's exactly right.
[3497.84s -> 3500.76s]  So just see that in action here.
[3500.76s -> 3503.68s]  Okay, I got the panic, I type where,
[3503.68s -> 3507.32s]  and it's in, here's k-free being called and panicked
[3507.32s -> 3508.60s]  and it's being called from free range
[3508.60s -> 3509.44s]  right at the beginning
[3509.44s -> 3513.12s]  when we're initializing the free list indeed.
[3513.12s -> 3516.72s]  So there's something to be fixed in free range.
[3517.80s -> 3520.24s]  The problem is I'm calling k-free as you said,
[3520.24s -> 3521.08s]  I'm calling k-free,
[3521.08s -> 3522.60s]  but of course all the reference counts are zero
[3522.60s -> 3525.52s]  and k-free panics if the reference count is zero.
[3525.56s -> 3526.40s]  So I'll just,
[3528.72s -> 3532.60s]  I'll just hack this to be,
[3540.44s -> 3541.28s]  how about that?
[3549.68s -> 3551.40s]  Okay, we managed to boot now.
[3551.96s -> 3555.36s]  Okay, quite wonderful done, let's see.
[3559.48s -> 3561.80s]  Okay, so let's run the cow tests again.
[3564.64s -> 3566.40s]  At least we passed the simple tests, aha.
[3566.40s -> 3569.00s]  So we passed the simple tests, which is great.
[3570.80s -> 3574.84s]  And we passed one time through the three tests,
[3574.84s -> 3578.92s]  but we didn't make it through the second time
[3578.92s -> 3581.04s]  through the three tests.
[3581.52s -> 3584.64s]  And the three tests triggered the print statement
[3584.64s -> 3589.64s]  that I added to trap, which said cow kalloc failed,
[3589.80s -> 3592.96s]  which only would print it if we're out of memory.
[3594.40s -> 3595.64s]  So we're out of memory.
[3598.92s -> 3600.20s]  Why are we out of memory?
[3606.60s -> 3608.60s]  So we've updated reference counts
[3608.60s -> 3610.92s]  every time we've allocated,
[3610.92s -> 3614.40s]  but we haven't decremented counts
[3614.40s -> 3616.92s]  whenever we've tried to free, right?
[3616.92s -> 3618.44s]  So where should I add a?
[3621.12s -> 3625.36s]  So with your current approach, I think PA one,
[3625.36s -> 3627.36s]  that reference count needs to get decremented.
[3627.36s -> 3629.32s]  So we would need to k-free it.
[3629.32s -> 3630.64s]  Yeah, exactly.
[3630.64s -> 3632.12s]  So I think somebody actually,
[3632.12s -> 3637.12s]  possibly you brought this up 45 minutes ago.
[3637.56s -> 3640.00s]  Let's see, let's free PA one.
[3640.96s -> 3642.64s]  PA two.
[3642.64s -> 3645.88s]  Okay, and so now what k-free means now
[3645.88s -> 3648.12s]  is not free this page.
[3648.12s -> 3650.76s]  It's what k-free means is decrement the ref count
[3650.76s -> 3653.16s]  on this page and free it if it's zero.
[3653.16s -> 3654.60s]  And so if we're in a simple situation
[3654.60s -> 3656.48s]  where there was just one fork,
[3656.48s -> 3658.88s]  and so there were two references to the page,
[3660.24s -> 3662.64s]  and we've taken a right fault and made a copy,
[3662.64s -> 3664.40s]  where now all k-free is gonna do
[3664.40s -> 3665.84s]  is decrement the count to be one,
[3665.84s -> 3668.76s]  and now the other process can use the page.
[3668.76s -> 3670.40s]  We're not actually gonna free it.
[3671.76s -> 3676.76s]  Okay, so hopefully now we'll not run out of memory
[3685.88s -> 3687.16s]  in count test.
[3691.60s -> 3692.44s]  I love it.
[3692.44s -> 3694.40s]  Okay, we made it through the three test
[3695.28s -> 3696.88s]  without running out of memory.
[3697.88s -> 3702.88s]  All right, we're not done sadly.
[3705.28s -> 3708.28s]  We have this interesting error in the file tests.
[3712.52s -> 3714.48s]  Let's take a look at the file test.
[3721.16s -> 3723.36s]  All right, so here's the file test.
[3723.36s -> 3725.36s]  And it says right in the comment for the test
[3725.36s -> 3727.36s]  that it's investigating copy out.
[3727.36s -> 3732.36s]  So a good hint of where our problem is gonna be.
[3733.04s -> 3736.04s]  The thing that actually failed is,
[3736.04s -> 3738.40s]  I mean, you have to use your imagination a little bit,
[3738.40s -> 3741.60s]  but the error message seems to be this one
[3741.60s -> 3744.64s]  printed from two different processes at the same time.
[3746.80s -> 3747.96s]  And indeed, this is a loop.
[3747.96s -> 3752.96s]  This test is a loop that for four iterations, just forks.
[3753.12s -> 3756.72s]  And so it's gonna produce four concurrent processes
[3756.72s -> 3758.76s]  whose error messages will be interleaved.
[3759.84s -> 3761.56s]  So we need to know why that read failed.
[3761.56s -> 3763.92s]  If we wanna understand what the bug is,
[3763.92s -> 3765.56s]  we need to know why the read failed.
[3765.56s -> 3766.84s]  So the first step is maybe to figure out
[3766.84s -> 3768.04s]  what the read actually returned.
[3768.04s -> 3772.96s]  All we know is that it returned something
[3772.96s -> 3776.00s]  other than size of I, but we wanna know what it returned.
[3776.00s -> 3779.16s]  So I'm gonna capture the return value.
[3782.96s -> 3787.96s]  And I'm gonna print the return value, at least now.
[3791.24s -> 3793.56s]  Oh, I have a question, sorry.
[3793.56s -> 3796.92s]  It seems like three of the threads failed,
[3796.92s -> 3799.32s]  but one of them didn't.
[3799.32s -> 3802.56s]  And it seems like it's still running.
[3802.56s -> 3803.92s]  Why is it not failing?
[3804.80s -> 3806.76s]  Why did the third thread not fail?
[3806.76s -> 3808.00s]  The fourth.
[3808.00s -> 3808.84s]  Why did the fourth one?
[3808.84s -> 3812.08s]  Like if you do count tests and then echo,
[3812.08s -> 3814.32s]  that other one is still gonna be running
[3814.32s -> 3816.20s]  and you're gonna see more output.
[3818.64s -> 3820.40s]  Yeah, three failed and one didn't.
[3824.08s -> 3827.12s]  Let's just finish investigating why any of them failed.
[3827.12s -> 3829.88s]  And then I can make a guess,
[3829.88s -> 3831.92s]  although I don't really know why one of them
[3831.92s -> 3832.80s]  didn't fail.
[3834.44s -> 3836.20s]  Okay, so what's happening here
[3836.20s -> 3838.00s]  is that read is returning minus one.
[3839.08s -> 3841.00s]  So why is read returning minus one?
[3842.40s -> 3845.56s]  You might hypothesize that we were passing
[3845.56s -> 3847.04s]  this buffer into read.
[3848.12s -> 3851.96s]  You might imagine that, and read needs to do something,
[3851.96s -> 3855.84s]  namely call copy out to copy file data
[3855.84s -> 3859.36s]  into this buff buffer that we passed to read.
[3859.36s -> 3860.64s]  And maybe something's going wrong with that
[3860.64s -> 3862.80s]  and its relationship to copy and write fork.
[3862.80s -> 3866.80s]  So let's take a look at read and see why it's failing.
[3870.04s -> 3871.04s]  Try to track down.
[3873.04s -> 3875.76s]  Why read is, all right, so let's capture the,
[3875.76s -> 3877.28s]  what read, all sys read does.
[3877.28s -> 3879.60s]  This is in calls to calls file read
[3879.60s -> 3881.44s]  after fetching a bunch of arguments.
[3884.36s -> 3888.88s]  So we'll fix this to take us one step closer
[3888.88s -> 3891.12s]  to understanding why things are going wrong.
[3894.64s -> 3899.04s]  And we're expecting cc to be minus one.
[3902.08s -> 3907.08s]  Ah, that's interesting.
[3911.04s -> 3914.44s]  We never saw cc minus equals minus one.
[3914.44s -> 3916.24s]  In fact, the failure occurred,
[3917.40s -> 3919.64s]  if failure starts occurring here
[3919.64s -> 3924.64s]  before we get any corresponding cc print statements,
[3925.32s -> 3927.36s]  and it's not minus one at all, it's four,
[3927.36s -> 3929.16s]  which is a non-failure value.
[3930.16s -> 3933.96s]  So what that means is that something before
[3933.96s -> 3937.40s]  the call to file read must have caused,
[3937.40s -> 3940.24s]  must have triggered this return.
[3940.24s -> 3941.80s]  So one of these functions here
[3942.92s -> 3945.44s]  has caused read to return minus one.
[3948.28s -> 3950.44s]  Please feel free to guess while I'm typing.
[3953.28s -> 3955.00s]  This was a complete surprise to me, by the way,
[3955.00s -> 3957.36s]  when I tracked it down.
[3958.32s -> 3961.32s]  Oh, I think it's the file descriptor.
[3961.32s -> 3963.04s]  Yeah, the file descriptor.
[3963.04s -> 3966.76s]  I saw, what I expected was that file read was failing
[3966.76s -> 3968.96s]  due to something in copy out,
[3968.96s -> 3972.60s]  not doing the right thing with copy on write pages.
[3973.52s -> 3975.60s]  But that is not at all where the failure is.
[3975.60s -> 3978.12s]  Okay, so if we wanna find out,
[3978.12s -> 3980.76s]  like it's either argfd or argent or argatter
[3980.76s -> 3983.60s]  that's failing, let's start with argfd.
[3983.60s -> 3985.36s]  I'm just gonna stick print statements in here
[3985.44s -> 3986.28s]  to help me
[3989.84s -> 3991.72s]  figure out which one of these failure cases
[3991.72s -> 3993.28s]  is actually triggering.
[3993.28s -> 3995.32s]  So only two places it returns minus one.
[3996.76s -> 3999.56s]  And this one, there's something wrong with the FD itself.
[4001.92s -> 4003.28s]  So we're gonna get these print statements
[4003.28s -> 4006.16s]  on cases where argfd returns minus one.
[4006.16s -> 4007.00s]  Okay.
[4015.88s -> 4016.92s]  What does this mean?
[4018.36s -> 4019.52s]  FD.
[4019.52s -> 4024.32s]  All right, so we expect these FD printouts
[4024.32s -> 4026.08s]  to occur just before the error.
[4026.08s -> 4031.08s]  So what we saw is FD9, it was FD9, this causes the error.
[4032.08s -> 4034.44s]  So FD9 failed one of these three tests.
[4034.52s -> 4036.72s]  We know it's not less than zero.
[4036.72s -> 4038.28s]  I happen to know it's not greater than or equal
[4038.28s -> 4041.64s]  to NO file for number of open files.
[4041.64s -> 4045.16s]  And so that means that the O file array
[4045.16s -> 4047.40s]  for nine file descriptor nine must have been zero.
[4047.40s -> 4050.72s]  That is in this process, there is no file descriptor nine.
[4053.04s -> 4056.84s]  So somehow copy on write test,
[4059.04s -> 4061.32s]  this code in the copy on write test
[4061.32s -> 4065.84s]  is passing an invalid file descriptor to read.
[4065.84s -> 4069.96s]  So this file descriptor comes from FDS.
[4069.96s -> 4073.16s]  FDS comes from a call to pipe.
[4076.04s -> 4078.08s]  How could I usually pipe if it doesn't fail?
[4078.08s -> 4079.52s]  It leaves the file descriptor in the array.
[4079.52s -> 4080.96s]  So how could this go wrong?
[4080.96s -> 4085.96s]  Oh, we all know the answer has to be blah, blah, blah,
[4091.36s -> 4092.28s]  copy out.
[4095.20s -> 4096.76s]  So what's the missing piece?
[4096.76s -> 4101.76s]  I mean, I guess one thing that helped me track this down
[4111.32s -> 4113.68s]  is I realized at least first of all,
[4115.12s -> 4117.76s]  FD should not be nine in every iteration.
[4117.76s -> 4120.36s]  Nine is kind of the value that you would expect
[4121.72s -> 4123.48s]  kind of on the last iteration.
[4123.48s -> 4126.28s]  That's kind of like the highest call to pipe.
[4126.32s -> 4128.24s]  Yeah, they all got a nine, you're right.
[4128.24s -> 4129.88s]  That's something they're kind of
[4129.88s -> 4132.36s]  touching each other's memory, strange.
[4132.36s -> 4134.24s]  Yeah, that's exactly right.
[4134.24s -> 4138.08s]  So the FDS pipe, the way pipe returns
[4138.08s -> 4140.68s]  the file descriptors is that it calls copy out
[4141.64s -> 4143.92s]  to copy the file descriptor number
[4143.92s -> 4146.52s]  that it's allocated into the caller's address space.
[4146.52s -> 4148.96s]  Well, we're using copy on write fork here.
[4148.96s -> 4152.08s]  So if we're not careful, so at least initially,
[4152.92s -> 4153.80s]  there's just one page
[4153.80s -> 4155.88s]  and it's shared among all the processes.
[4155.88s -> 4157.96s]  So the first process that calls pipe,
[4159.04s -> 4160.52s]  if something goes wrong,
[4160.52s -> 4163.44s]  the pipe system call might conceivably
[4163.44s -> 4167.32s]  write the shared page and that value,
[4167.32s -> 4168.84s]  that file descriptor will then be seen
[4168.84s -> 4171.52s]  by all the processes instead of just the process
[4171.52s -> 4173.48s]  that actually allocated the descriptor.
[4175.92s -> 4178.04s]  So everybody see this as a kind of
[4178.04s -> 4179.76s]  at least hypothetical risk.
[4183.32s -> 4185.56s]  So let's look at what, try to imagine,
[4186.16s -> 4190.64s]  so first, let's look at sys file to see
[4192.96s -> 4197.96s]  how pipe copies out the file descriptors it allocates.
[4201.88s -> 4206.36s]  And indeed down here are these two calls to copy out,
[4206.36s -> 4208.96s]  to copy the two elements of that pipe array.
[4208.96s -> 4212.88s]  So pipe is using copy out to copy the file descriptor
[4212.88s -> 4215.20s]  numbers into the array and user space.
[4216.08s -> 4217.28s]  So what does copy out do?
[4220.76s -> 4223.36s]  Well, it looks up the virtual address
[4223.36s -> 4224.84s]  to find a physical address
[4226.40s -> 4231.40s]  and then it's writes, it copies
[4232.68s -> 4235.48s]  over that physical address and that's it.
[4235.48s -> 4237.92s]  So one thing it is not doing
[4237.92s -> 4239.80s]  is checking for write permission.
[4239.80s -> 4241.32s]  It doesn't look at P-T-E-W.
[4242.88s -> 4245.04s]  And so if this page, if this virtual address
[4245.48s -> 4248.00s]  refers to a copy on write shared page,
[4248.00s -> 4249.84s]  copy out just has no idea,
[4249.84s -> 4251.92s]  it just goes ahead and writes it.
[4251.92s -> 4255.64s]  And so I mean, indeed, when pipe calls copy out,
[4255.64s -> 4257.76s]  it's gonna be writing a shared page
[4257.76s -> 4261.36s]  that all of the forked processes see this modification,
[4261.36s -> 4263.32s]  not just the process that called pipe.
[4264.64s -> 4267.40s]  Any questions about why this is happening?
[4267.40s -> 4272.40s]  Okay, all right, so as you all know,
[4279.60s -> 4282.36s]  we need to modify copy out to,
[4282.36s -> 4283.80s]  since copy out is, you know,
[4283.80s -> 4285.00s]  looking up a virtual address
[4285.00s -> 4286.64s]  and translating to a physical address,
[4286.64s -> 4288.68s]  but it is not using the MMU.
[4288.68s -> 4291.08s]  So the MMU doesn't catch the write fault
[4291.08s -> 4293.12s]  because the MMU is not involved.
[4294.60s -> 4296.68s]  And walk at or looks at the page table itself
[4296.72s -> 4299.56s]  instead of basically getting the MMU
[4299.56s -> 4301.28s]  to look at the page table for it.
[4303.40s -> 4307.80s]  Okay, so we need to modify copy out.
[4307.80s -> 4308.96s]  We don't wanna call walk at
[4308.96s -> 4311.12s]  or walk at or just produces a physical address.
[4311.12s -> 4313.20s]  We actually want the page table entry
[4313.20s -> 4315.52s]  because we wanna look at the permission bits
[4315.52s -> 4317.88s]  to see if it's a copy on write,
[4317.88s -> 4319.52s]  a write protected page.
[4319.52s -> 4321.16s]  So we're gonna call walk instead.
[4327.52s -> 4332.52s]  We need to check all the usual errors.
[4333.84s -> 4335.64s]  So maybe there's no mapping at all.
[4339.04s -> 4340.60s]  Maybe it's not a valid mapping
[4342.84s -> 4344.52s]  or maybe it's a mapping
[4344.52s -> 4347.72s]  that the user code is not allowed to use.
[4347.72s -> 4350.80s]  And in all those cases, it's an error,
[4350.80s -> 4352.84s]  but this time we can actually return an error.
[4352.84s -> 4354.60s]  First of all, just read to return minus one
[4354.60s -> 4355.96s]  instead of killing the process.
[4356.08s -> 4356.92s]  That's nice.
[4360.04s -> 4361.32s]  Next, we need to know how to check
[4361.32s -> 4363.20s]  whether it's a copy on write page.
[4365.24s -> 4369.08s]  Any ideas for how we can check
[4369.08s -> 4372.68s]  whether we should do a copy at this point?
[4380.60s -> 4382.64s]  What's different between copy on write pages
[4382.64s -> 4384.84s]  and just sort of ordinary unshared pages?
[4386.96s -> 4389.40s]  Would the reference count be large?
[4390.56s -> 4391.48s]  Ah, that's a good point.
[4391.48s -> 4396.48s]  If the reference count was greater than one,
[4397.08s -> 4399.96s]  that's certainly be a very strong hint
[4400.80s -> 4402.56s]  that's a copy on write page.
[4403.56s -> 4405.68s]  But if the reference count is one,
[4405.68s -> 4408.12s]  does that mean it's not a copy on write page?
[4408.12s -> 4413.12s]  It turns out if you fork and the child
[4416.84s -> 4419.64s]  takes a copy on write fault and makes a copy,
[4419.64s -> 4422.96s]  that reduces the reference count to one,
[4422.96s -> 4425.56s]  but the page is still write protected in the parent.
[4429.00s -> 4432.40s]  And actually probably we could get away
[4432.40s -> 4433.88s]  with modifying in that case.
[4433.88s -> 4437.92s]  So I think we actually could check the reference count.
[4439.12s -> 4442.04s]  And only treat it as a copy on write page
[4442.04s -> 4443.96s]  if the reference count is greater than zero.
[4443.96s -> 4445.80s]  But what I'm gonna do is a different,
[4446.72s -> 4449.84s]  an even heavier shortcut and say,
[4449.84s -> 4451.44s]  if the page is write protected,
[4452.40s -> 4454.76s]  the only way a page can be write protected
[4454.76s -> 4459.76s]  and have the PTU bit set in this version of XV6
[4460.04s -> 4461.76s]  is if it's a copy on write page.
[4464.40s -> 4466.08s]  So this saves me a little bit of typing
[4466.08s -> 4468.08s]  compared to checking the reference count.
[4468.24s -> 4472.20s]  We already wrote the code to deal with this in trap.c.
[4475.80s -> 4478.48s]  So I'm just gonna call that code from here.
[4481.20s -> 4484.20s]  We still have to deal with the possible error return
[4484.20s -> 4489.20s]  if it's a crazy address.
[4490.08s -> 4491.24s]  Oh, actually we don't.
[4498.16s -> 4499.16s]  All right.
[4504.36s -> 4507.52s]  All right, so if the page is not writable,
[4507.52s -> 4509.52s]  we're just gonna call copy on write fault.
[4509.52s -> 4511.52s]  And copy on write fault does the copy
[4511.52s -> 4513.32s]  and it makes the page writable.
[4514.44s -> 4519.44s]  So I think we're almost done at this point.
[4520.40s -> 4524.76s]  However, if copy on, if COW fault did make a,
[4528.08s -> 4532.76s]  let me get rid of this.
[4532.76s -> 4535.16s]  If COW fault did make a copy,
[4535.16s -> 4537.48s]  it's gonna have modified the physical address
[4537.48s -> 4538.84s]  in the page table entry.
[4538.84s -> 4540.52s]  So we have to pull that physical address
[4540.52s -> 4542.04s]  out of the page table entry again,
[4542.04s -> 4543.52s]  in case it was changed.
[4543.52s -> 4545.72s]  I'm just gonna do that since we have the
[4548.64s -> 4550.12s]  page table entry right here.
[4551.84s -> 4555.64s]  And I think this is more or less enough.
[4556.60s -> 4561.60s]  So any questions about the modification to copy out?
[4565.28s -> 4568.64s]  All right, I got to make a global declaration
[4568.64s -> 4570.20s]  for COW fault.
[4586.60s -> 4587.44s]  Beautiful.
[4593.20s -> 4594.80s]  All right, that's looking great.
[4595.84s -> 4597.68s]  Anybody happen to know if we're done?
[4607.08s -> 4607.92s]  We may or may not be.
[4607.92s -> 4610.16s]  We also have to pass user tests.
[4612.00s -> 4614.00s]  Oh, all right, let me get rid of this.
[4616.08s -> 4617.36s]  It's print statements.
[4634.96s -> 4637.72s]  I think the execout test intentionally
[4637.72s -> 4639.08s]  allocates a lot of memory.
[4642.64s -> 4644.44s]  All right, any questions while we,
[4646.40s -> 4648.52s]  wait for the verdict from user tests?
[4652.36s -> 4654.64s]  Aha, okay, user test was unhappy.
[4654.64s -> 4657.48s]  I think this is a familiar piece of damage,
[4657.48s -> 4659.52s]  which somebody has brought up already.
[4664.60s -> 4666.44s]  It's the panic and walk.
[4666.44s -> 4668.52s]  If the user process, the user test actually,
[4668.52s -> 4671.00s]  I think does intentionally,
[4671.00s -> 4672.64s]  we'll take a look at user tests.
[4675.64s -> 4680.64s]  And it's the copy out test.
[4683.44s -> 4685.28s]  And it even says in the comment,
[4685.28s -> 4688.08s]  it passes ridiculous pointers to system calls.
[4688.08s -> 4691.04s]  All right, so what we're faced with is copy out.
[4692.88s -> 4694.36s]  We pass in a ridiculous pointer.
[4694.36s -> 4696.92s]  It simply calls walk with that ridiculous pointer
[4697.80s -> 4701.48s]  and walk panics if you give it a ridiculous pointer.
[4701.48s -> 4706.16s]  So, as somebody mentioned,
[4706.16s -> 4710.56s]  we need to not call walk with ridiculous pointers.
[4712.32s -> 4713.96s]  What is it, max VA or something?
[4715.84s -> 4717.36s]  All right, and of course,
[4717.36s -> 4720.92s]  we're totally entitled to return an error
[4720.92s -> 4725.80s]  if a user process passes us out of bounds pointer.
[4726.72s -> 4731.72s]  All right, I'm just hoping it passes now.
[4734.36s -> 4736.08s]  Not aware of any other problems.
[4738.88s -> 4742.44s]  Okay, questions.
[4745.72s -> 4749.04s]  I guess my question is more broad.
[4749.04s -> 4753.36s]  Like we saw when we were doing that with small steps,
[4753.40s -> 4758.40s]  we were getting random bugs at this point at our step.
[4759.20s -> 4762.24s]  And if we're doing the lab from scratch,
[4762.24s -> 4764.56s]  then wouldn't it be confusing
[4764.56s -> 4767.08s]  to get all of those weird bugs?
[4768.00s -> 4773.00s]  And how would you know whether it's a bug
[4773.00s -> 4774.16s]  with something you wrote
[4774.16s -> 4777.88s]  or a bug because you didn't write the rest of it?
[4777.88s -> 4780.04s]  I'm afraid I cannot answer your question.
[4780.88s -> 4783.32s]  The sad truth about bugs,
[4783.32s -> 4785.80s]  particularly bugs caused by doing funny things
[4785.80s -> 4788.40s]  to page tables inside the kernel,
[4788.40s -> 4792.92s]  is that every bug is different
[4793.76s -> 4797.08s]  and what turns out to be the winning strategy
[4797.08s -> 4798.72s]  for tracking a bug down.
[4799.80s -> 4802.48s]  It just depends totally on the nature of the bug.
[4804.72s -> 4809.16s]  My usual, and I have some,
[4809.24s -> 4810.92s]  I have a sort of a collection of strategies
[4810.92s -> 4814.44s]  that are sometimes helpful and sometimes not.
[4814.44s -> 4815.68s]  Like I'm a huge believer
[4815.68s -> 4817.48s]  in putting print statements into the code
[4817.48s -> 4821.64s]  in order to gather information or to verify hypotheses.
[4824.20s -> 4826.76s]  I'll often spend time just looking at the code
[4826.76s -> 4829.56s]  just for the purpose of generating,
[4829.56s -> 4830.96s]  sort of brainstorming with myself
[4830.96s -> 4832.84s]  to just try to generate things
[4832.84s -> 4835.68s]  that could possibly be going wrong at this point
[4835.68s -> 4838.00s]  and then put print statements in or panics or something
[4838.00s -> 4843.00s]  to try to rule in or rule out various guesses
[4843.36s -> 4845.24s]  at what the problem might be.
[4845.24s -> 4849.96s]  Another possibility is that your code worked
[4849.96s -> 4853.44s]  or didn't show this error or something half an hour ago
[4853.44s -> 4855.64s]  and then you made some changes and now you have some bug.
[4855.64s -> 4858.16s]  So another possibility is to, if you can,
[4859.12s -> 4861.52s]  back up to a version of the code that didn't have the bug
[4861.52s -> 4863.36s]  and sort of add your changes bit by bit
[4863.36s -> 4864.60s]  until the bug shows up.
[4865.28s -> 4866.92s]  I mean, none of these are,
[4866.92s -> 4869.92s]  I don't know any strategy that always works
[4869.92s -> 4872.08s]  or even works more than a fraction of the time.
[4874.72s -> 4876.48s]  But the truth is part of what you're learning
[4876.48s -> 4879.96s]  in this class as well as learning about operating systems,
[4879.96s -> 4883.32s]  you're getting experience writing and debugging code.
[4887.20s -> 4888.80s]  Sorry, not to be more helpful.
[4892.28s -> 4893.24s]  Other questions?
[4894.60s -> 4899.60s]  All right, we're at that end of class time.
[4909.96s -> 4911.32s]  I'm happy to talk more,
[4912.88s -> 4915.88s]  but we're done with the formal part of this class.
[4920.28s -> 4923.48s]  All right, and to see you all on Wednesday.
[4924.68s -> 4926.52s]  Oh, sorry, I actually have another question.
[4926.52s -> 4930.52s]  When we were seeing our first right fault,
[4933.92s -> 4937.64s]  we also saw after that for the shell
[4937.64s -> 4939.52s]  illegal instruction fault
[4939.52s -> 4942.80s]  and then something for the init process.
[4942.80s -> 4943.64s]  What was that?
[4950.16s -> 4951.00s]  Well,
[4954.60s -> 4957.92s]  I don't have a theory.
[4957.92s -> 4959.24s]  The init process,
[4960.52s -> 4963.04s]  I don't think would share pages with anything.
[4964.44s -> 4967.32s]  I just don't know, I'm sorry.
[4967.32s -> 4968.16s]  We could,
[4971.16s -> 4972.20s]  I would have to back up.
[4972.20s -> 4974.56s]  This happened very early when I was doing the lab, right?
[4974.56s -> 4976.68s]  We just had a copy of the page table and nothing else.
[4976.68s -> 4981.68s]  Yeah, I think it was like s cos f for the shell
[4982.04s -> 4987.04s]  and I think s cos c for the init process, maybe.
[4991.12s -> 4992.72s]  All right, let me, let's see.
[4992.72s -> 4994.56s]  If you really, let's see, let's see.
[4996.04s -> 4996.88s]  We can,
[5001.60s -> 5002.96s]  let's reconstruct
[5009.72s -> 5011.56s]  the lab from scratch.
[5012.40s -> 5014.08s]  So I'm gonna clone.
[5037.16s -> 5039.04s]  Okay, at this point we had just made,
[5039.04s -> 5043.72s]  just made changes to UVM copy, right?
[5046.00s -> 5047.52s]  So we were,
[5049.44s -> 5050.48s]  what are we not doing here?
[5050.48s -> 5054.04s]  We're not allocating and we're not doing m move.
[5057.72s -> 5058.92s]  But we are,
[5063.72s -> 5068.64s]  we're gonna pass PA to map pages
[5068.64s -> 5073.12s]  and I think the very first time I ran anything,
[5073.12s -> 5075.36s]  I think this was the state of play.
[5075.36s -> 5077.88s]  It was after we did the flags also.
[5077.88s -> 5078.72s]  Okay.
[5084.80s -> 5086.44s]  All right, so let's clear.
[5091.36s -> 5092.20s]  And,
[5098.64s -> 5102.40s]  does that look good?
[5124.76s -> 5127.48s]  All right, we got a C.
[5127.48s -> 5128.32s]  Well, let's see.
[5128.36s -> 5130.80s]  C is instruction page fault.
[5130.80s -> 5134.24s]  So, okay, so s cos two,
[5135.68s -> 5138.40s]  you know, is, you know, we think a shared
[5138.40s -> 5140.84s]  and process ID two, oh man.
[5140.84s -> 5142.76s]  You're right, it's process ID one.
[5150.12s -> 5152.72s]  I just don't have a theory for why process ID one.
[5154.80s -> 5156.76s]  Might have run into trouble.
[5157.48s -> 5159.52s]  Let's look at what init does.
[5163.80s -> 5164.64s]  Is this init?
[5182.88s -> 5184.40s]  Okay, it execs the shell.
[5186.76s -> 5189.40s]  I don't know.
[5192.12s -> 5194.56s]  You know, okay, oh, here's an idea.
[5194.56s -> 5197.60s]  Process ID two has taken a fault and it's exited.
[5197.60s -> 5199.40s]  So you do a fork later, like right?
[5199.40s -> 5201.40s]  Yeah, and so that means that this wait
[5201.40s -> 5205.04s]  in init has returned, right?
[5205.04s -> 5206.36s]  And so it's gonna go back around the loop
[5206.36s -> 5207.28s]  and call fork again.
[5207.28s -> 5208.84s]  And so, you know, after a fork,
[5208.84s -> 5211.64s]  we're now sharing and, you know,
[5211.64s -> 5213.40s]  depending on all this damage code.
[5216.80s -> 5218.56s]  Okay, I see.
[5218.56s -> 5221.04s]  Okay, yeah, that's very confusing.
[5221.04s -> 5224.36s]  Oh yeah, these errors are just like complete bananas
[5224.36s -> 5227.76s]  because we violated all sorts of intuitions
[5227.76s -> 5229.20s]  about how things should work.
[5230.52s -> 5231.36s]  Okay.
