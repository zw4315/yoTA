# Detected language: en (p=1.00)

[0.00s -> 8.48s]  All right, can you guys hear me?
[8.48s -> 9.52s]  Yes.
[9.52s -> 10.08s]  Excellent.
[10.08s -> 11.16s]  Thank you.
[11.16s -> 12.96s]  All right, today I want to talk mostly
[12.96s -> 15.48s]  about microkernels.
[15.48s -> 18.08s]  But first, a little bit of context
[18.08s -> 22.70s]  to sort of help explain why people explored microkernels
[22.70s -> 23.44s]  in the first place.
[27.44s -> 29.04s]  People got the microkernels by trying
[29.08s -> 32.64s]  to think a little more broadly about what
[32.64s -> 34.28s]  kernels should actually do.
[34.28s -> 38.96s]  Like with XV6, it sort of does the things that Unix does,
[38.96s -> 43.32s]  and we kind of take that set of abstractions and system calls
[43.32s -> 46.04s]  and keep sort of facilities inside the kernel
[46.04s -> 49.36s]  as kind of for granted as the target of what
[49.36s -> 50.36s]  we're trying to design.
[50.36s -> 53.92s]  But it's totally worth wondering, gosh,
[53.92s -> 55.68s]  what should a kernel do in the first place?
[55.68s -> 58.18s]  Maybe the particular kind of stuff
[58.18s -> 62.42s]  that XV6 or Linux does is not really the best answer,
[62.42s -> 64.46s]  or maybe it is.
[64.46s -> 67.30s]  And of course, we're on somewhat treacherous ground
[67.30s -> 70.62s]  here because now what kernels are
[70.62s -> 73.58s]  is kind of a development platform for programmers.
[73.58s -> 75.46s]  As we know, programmers, different people
[75.46s -> 80.22s]  have very different sort of subjective preferences
[80.22s -> 82.38s]  about what kind of infrastructure
[82.38s -> 83.42s]  they like to program on.
[83.42s -> 86.78s]  So we can't necessarily expect a single best answer.
[88.66s -> 91.62s]  But we can expect to maybe learn something and maybe
[91.62s -> 93.66s]  make some progress by trying to think
[93.66s -> 96.02s]  about what answers might be.
[96.02s -> 98.94s]  So first of all, let me try to kind of crystallize
[98.94s -> 101.78s]  what the traditional approach is
[101.78s -> 105.06s]  to what kind of kernel interfaces
[105.06s -> 106.46s]  we ought to be using.
[106.46s -> 109.62s]  And Linux and Unix and XV6 are all
[109.62s -> 115.10s]  examples of what I personally call the traditional design
[115.10s -> 115.70s]  approach.
[115.74s -> 120.42s]  But another word for it that kind of summarizes
[120.42s -> 122.94s]  what this approach has ended up like is monolithic.
[128.78s -> 129.62s]  Monolithic.
[129.62s -> 133.50s]  And what that means is that the kernel
[133.50s -> 137.26s]  is a single big program that does all kinds of things
[137.26s -> 140.74s]  all within the same program.
[140.74s -> 143.10s]  And indeed, this really reflects
[143.14s -> 146.58s]  the way people thought about what kernels ought to be doing.
[146.58s -> 150.82s]  A real hallmark of kernels like Linux
[150.82s -> 155.86s]  is that they provide powerful abstractions.
[155.86s -> 159.94s]  They choose things like file systems, which is really
[159.94s -> 162.38s]  a complicated item.
[162.38s -> 165.06s]  And they present file systems and files and directories
[165.06s -> 167.86s]  and file descriptors as their interface
[167.86s -> 171.74s]  rather than, for example, presenting disk hardware
[171.78s -> 175.02s]  as their interface to applications.
[175.02s -> 178.42s]  And presenting powerful abstractions
[178.42s -> 180.10s]  instead of very low-level abstractions
[180.10s -> 182.18s]  has some big advantages.
[182.18s -> 183.62s]  So these monolithic kernels often
[183.62s -> 196.46s]  have sort of big abstractions like file to file system.
[196.46s -> 197.86s]  One advantage of big abstractions
[197.86s -> 201.62s]  is that they're often portable of files and directories.
[201.62s -> 203.58s]  You can implement files and directories
[203.58s -> 206.18s]  on all kinds of storage.
[206.18s -> 209.06s]  And you can use files and directories
[209.06s -> 212.06s]  without having to worry about what brand of disk drive
[212.06s -> 214.38s]  it's running on, or maybe it's an SSD instead
[214.38s -> 216.66s]  of a hard drive, or maybe it's a network file system.
[216.66s -> 219.30s]  It all has the same interface because the file system
[219.30s -> 222.34s]  interface is pretty high level, pretty abstract.
[222.34s -> 225.74s]  So an advantage of this is that it's
[225.74s -> 227.42s]  a way to get portability.
[227.42s -> 228.94s]  You can write an application and have
[228.94s -> 231.30s]  it run on all kinds of different hardware
[231.30s -> 235.70s]  without having to modify the application.
[235.70s -> 239.94s]  Another example of this is that Unix, Linux
[239.94s -> 243.30s]  provides an address space abstraction rather than
[243.30s -> 246.22s]  providing something that's like, rather than providing
[246.22s -> 249.46s]  direct access to the MMU hardware.
[249.46s -> 252.02s]  And that's useful for portability
[252.02s -> 255.90s]  and to sort of hide complexity from applications.
[255.90s -> 260.10s]  So another big advantage here of these powerful abstractions
[260.10s -> 262.90s]  is that they tend to hide complexity from applications.
[266.18s -> 269.98s]  So for example, the file descriptor interface
[269.98s -> 272.66s]  that xv6 provides, it's a very simple interface,
[272.66s -> 275.26s]  but just read and write on file descriptors
[275.26s -> 276.74s]  can get much simpler.
[276.74s -> 278.74s]  But behind it is some very complicated code
[278.74s -> 282.66s]  for actually reading and writing the disk, the file system
[282.66s -> 283.14s]  on disk.
[285.74s -> 287.18s]  And so that's nice for programmers,
[287.18s -> 290.02s]  but it makes for a big complex kernel.
[290.26s -> 293.02s]  These big abstractions also help the kernel
[293.02s -> 294.66s]  manage and share resources.
[294.66s -> 296.42s]  We sort of delegated to the kernel things
[296.42s -> 297.86s]  like memory management.
[297.86s -> 300.82s]  The kernel keeps track of what memory is free.
[300.82s -> 303.06s]  Similarly, the kernel keeps track
[303.06s -> 304.62s]  of what parts of the disk are free
[304.62s -> 306.58s]  and what parts of the disk are in current use.
[306.58s -> 309.38s]  So programs don't get to think about it.
[309.38s -> 311.02s]  And again, it simplifies programs.
[311.02s -> 313.30s]  It also helps with robustness and security
[313.30s -> 316.30s]  even because if programs are allowed
[316.30s -> 318.38s]  to decide what parts of the disk are free or not,
[318.38s -> 322.22s]  then maybe one program could use part of the disk that's
[322.22s -> 325.70s]  already being used by another program.
[325.70s -> 328.02s]  So the fact that the kernel is
[328.02s -> 332.46s]  in charge of resource management
[332.46s -> 335.06s]  helps with sharing and helps with security.
[335.06s -> 339.82s]  But again, it's a force that sort of causes
[339.82s -> 340.78s]  the kernel to be big.
[344.74s -> 346.42s]  So anyway, so having the kernel
[346.42s -> 350.18s]  be in charge of all these sort of juicy abstractions
[350.18s -> 353.42s]  that, even if they have simple interfaces,
[353.42s -> 355.98s]  have a lot of complexity inside, have led kernels
[355.98s -> 361.14s]  to be big and complex items.
[361.14s -> 364.50s]  And another aspect of this monolithic design approach
[364.50s -> 367.22s]  is that because it's all one program,
[367.22s -> 369.74s]  all the different kernel systems, like the file system
[369.74s -> 371.58s]  and the memory allocator and the scheduler
[371.58s -> 373.10s]  and the virtual memory system, they're
[373.10s -> 374.90s]  all part of one big integrated program.
[374.94s -> 377.02s]  It means that they can peer into each other's data
[377.02s -> 377.74s]  structures.
[377.74s -> 381.38s]  And so that's just tended to make it much easier
[381.38s -> 385.62s]  to implement facilities that are sort of parts of more than
[385.62s -> 389.02s]  one or what you might think of as more than one kind
[389.02s -> 390.82s]  of modular subsystem.
[390.82s -> 394.98s]  So for example, a system called like exec.
[394.98s -> 397.26s]  Exec has its fingers deeply into the file system
[397.26s -> 400.50s]  because it's reading binary images off the disk in order
[400.50s -> 401.78s]  to load them into memory.
[401.78s -> 405.02s]  It also has its fingers into the memory allocation
[405.02s -> 407.06s]  and virtual memory paging system
[407.06s -> 409.90s]  because it needs to set up the address space of the process.
[409.90s -> 410.86s]  But it's really easy.
[410.86s -> 413.62s]  There's no problem with doing that in xv6 or Linux
[413.62s -> 417.06s]  because both all the file system is right there
[417.06s -> 419.38s]  in the same kernel program and the virtual memory system
[419.38s -> 422.46s]  is also right there as part of the same program.
[422.46s -> 426.66s]  And if somehow there was a rigid split between the file
[426.66s -> 428.26s]  system and the virtual memory system,
[428.26s -> 430.70s]  it'd be much harder to implement something like exec that
[430.70s -> 433.94s]  has sort of fingers in both of these pies.
[433.94s -> 437.54s]  But in a monolithic system, just one big program,
[437.54s -> 440.22s]  it's much easier.
[440.22s -> 443.50s]  Another thing that makes implementing software
[443.50s -> 446.30s]  inside a monolithic kernel like xv6 or Linux easy
[446.30s -> 451.18s]  is that all the code runs with full hardware privileges.
[451.18s -> 453.54s]  All of xv6 runs in supervisor mode,
[453.54s -> 456.62s]  for example, which means there's no limits.
[456.62s -> 458.30s]  There's no irritating.
[458.38s -> 460.74s]  You can't read or write that memory here
[460.74s -> 462.62s]  because you don't have enough privilege.
[462.62s -> 468.34s]  All the kernel code runs with maximum privilege.
[468.34s -> 473.14s]  The same is true of operating systems like Linux.
[473.14s -> 479.02s]  So this design strategy is very convenient
[479.02s -> 480.94s]  for kernel developers.
[480.94s -> 484.38s]  And it's made it easy to build these big abstractions which
[484.42s -> 488.98s]  are convenient for application developers.
[488.98s -> 491.34s]  However, there's also a certain amount
[491.34s -> 494.06s]  to criticize with the monolithic,
[494.06s -> 495.62s]  the traditional monolithic approach.
[498.58s -> 500.90s]  And this is starting to be part of the motivation
[500.90s -> 506.26s]  for looking at other architectures
[506.26s -> 507.82s]  like microkernels.
[507.82s -> 512.06s]  So you might ask, why not monolithic kernels?
[514.38s -> 523.14s]  So one is just that they're big and complex.
[523.14s -> 528.10s]  So anything that's Linux is, depending on how you count,
[528.10s -> 530.70s]  Linux is somewhere between many hundreds of thousands
[530.70s -> 533.86s]  and a few million lines of code.
[533.86s -> 536.94s]  And people really do take advantage of the fact
[536.94s -> 539.90s]  that one part of Linux can look at the data of another.
[539.90s -> 541.58s]  That makes the programming easier.
[541.62s -> 545.38s]  But it also make there be a lot of sort of interconnections
[545.38s -> 546.98s]  and interrelationships between code.
[546.98s -> 549.82s]  And so it can be a bit challenging sometimes
[549.82s -> 551.58s]  to look at Linux kernel code and figure out
[551.58s -> 552.38s]  what it's up to.
[554.70s -> 557.10s]  And anytime you get big programs, especially
[557.10s -> 559.98s]  ones that are complex structure, you get bugs.
[562.50s -> 565.38s]  And operating system kernels are no exceptions.
[565.38s -> 568.54s]  And over the years, they've had all kinds of bugs,
[568.54s -> 572.54s]  including bugs that are exploitable for security.
[579.62s -> 583.06s]  So this is sort of a troubling set of relationships.
[583.06s -> 584.90s]  If you allow a big monolithic kernel,
[584.90s -> 589.26s]  you most certainly can't avoid bugs and exploitable security
[589.26s -> 590.42s]  problems.
[590.42s -> 593.14s]  And that's a real, I mean, that really is a problem.
[594.14s -> 600.02s]  Another reason why people are maybe not entirely
[600.02s -> 602.42s]  happy with monolithic kernels is that they
[602.42s -> 608.02s]  tend to just grow with all desirable features over time.
[608.02s -> 611.02s]  And so Linux is used for all kinds of different things,
[611.02s -> 614.74s]  from telephone handsets to desktop workstations
[614.74s -> 618.98s]  and laptops to tablets to servers on the internet
[618.98s -> 621.30s]  to routers.
[621.30s -> 622.74s]  And that's caused Linux.
[622.74s -> 625.62s]  It's fantastic that Linux can support all those things,
[625.62s -> 627.42s]  but it has caused it to be very general.
[627.42s -> 630.54s]  It has support in there for many, many different things.
[630.54s -> 633.62s]  And any one application, like me running my web servers,
[633.62s -> 637.74s]  unlikely to need, for example, Linux's very sophisticated
[637.74s -> 639.42s]  sound card support.
[639.42s -> 641.22s]  So there's just a huge amount of stuff
[641.22s -> 644.34s]  that's there to allow Linux to be
[644.34s -> 647.42s]  general purpose, which is good.
[647.42s -> 652.26s]  But there's a worry that general purpose
[652.26s -> 654.82s]  is going to tend to mean slow, that it
[654.82s -> 657.30s]  may be good for all kinds of different things,
[657.30s -> 661.98s]  but maybe not optimum for anything in particular.
[661.98s -> 664.50s]  So it's very hard to, when you're
[664.50s -> 666.18s]  trying to make something run really fast,
[666.18s -> 669.30s]  it's great to have it just only do one or two things.
[669.30s -> 672.74s]  So you can focus on optimizing a single code path.
[672.74s -> 674.30s]  But if your software needs to do
[674.30s -> 675.76s]  any one of 1,000 different things,
[675.80s -> 680.60s]  it's much harder to have focused optimization.
[680.60s -> 684.06s]  So Linux is not necessarily slow, but you
[684.06s -> 686.96s]  might wonder if it's really as fast as it could possibly
[686.96s -> 690.56s]  be for any given situation.
[690.56s -> 694.76s]  And so if you think about almost anything
[694.76s -> 698.56s]  in Linux or XP6, you may wonder whether it really
[698.56s -> 699.88s]  needs to do everything it does.
[699.88s -> 703.04s]  So for example, if you write a single byte over a pipe
[703.04s -> 705.08s]  from one process to another, boy,
[705.12s -> 707.20s]  there's a lot of instructions that could execute it,
[707.20s -> 710.04s]  even in XP6, which is a simple kernel.
[710.04s -> 713.56s]  There's buffering, there's locking,
[713.56s -> 715.64s]  there could be a sleep and a wake up
[715.64s -> 717.02s]  during a pipe read and write.
[717.02s -> 718.72s]  There's maybe a schedulering, they
[718.72s -> 721.08s]  call to the scheduler or context switch.
[721.08s -> 726.20s]  That's a lot of stuff that's maybe not necessarily
[726.20s -> 728.76s]  the absolute minimum that would be required to move a byte
[728.76s -> 730.60s]  from one process to another.
[730.60s -> 735.40s]  Another potential problem with these big kernels
[735.40s -> 740.92s]  is that because they're so big and they sort of intentionally
[740.92s -> 744.92s]  bite off some very sophisticated abstractions,
[744.92s -> 747.34s]  they tend to have a lot of design decisions kind of baked
[747.34s -> 750.04s]  into the kernel.
[750.04s -> 754.24s]  So in ways that you can't, even if you disagree with them,
[754.24s -> 756.04s]  you can't really, tough luck.
[756.04s -> 758.52s]  Like applications just have to live with it.
[758.52s -> 765.28s]  So as opposed to some fantasy world,
[765.28s -> 767.12s]  maybe the applications could make
[767.12s -> 768.28s]  a lot more of the decisions.
[768.28s -> 775.72s]  So some examples of things where you may be bummed out
[775.72s -> 779.44s]  by the way the API is designed.
[779.44s -> 782.36s]  For example, in Unix, you can wait
[782.36s -> 784.40s]  for a process to your own children.
[784.40s -> 785.88s]  If you fork, you can then wait for your children,
[785.88s -> 787.50s]  but you can't wait for some other process.
[788.08s -> 789.70s]  Maybe you want to wait for a grandchild
[789.70s -> 792.38s]  or an unrelated process, but that's just not an option.
[792.38s -> 794.08s]  It's just not the way things work,
[794.08s -> 797.04s]  even if it would be convenient for you.
[797.04s -> 800.82s]  Maybe you want to change the way another process's address
[800.82s -> 802.06s]  space is set up.
[802.06s -> 805.34s]  Maybe call nmap on behalf of another process
[805.34s -> 806.38s]  that you're controlling.
[806.38s -> 808.42s]  But again, that's just not an option.
[808.42s -> 810.86s]  You can nmap, change your own address space,
[810.86s -> 813.78s]  but not change other process's address space.
[813.78s -> 817.64s]  Maybe you're a database, and you have B-tree indexes
[817.64s -> 818.94s]  on the disk.
[818.94s -> 821.22s]  And you may know a lot about it, the fastest way
[821.22s -> 823.34s]  to lay out a B-tree on a disk.
[823.34s -> 825.78s]  But if you're reading and writing files with the file
[825.78s -> 827.74s]  system, the file system has no idea
[827.74s -> 829.38s]  that you're actually writing a B-tree
[829.38s -> 831.38s]  or how a B-tree ought to be laid out
[831.38s -> 834.22s]  on a disk for fastest access.
[834.22s -> 837.18s]  And so if you're a database, you're going to be bummed.
[837.18s -> 838.52s]  Maybe you're happy that you have this file
[838.52s -> 840.64s]  system at your disposal, but it doesn't really
[840.64s -> 843.18s]  do what you want it to do.
[843.22s -> 846.02s]  And that's the sense in which design decisions are often
[846.02s -> 848.70s]  baked into big kernels.
[848.70s -> 857.66s]  And finally, a specific issue that came up in a big way
[857.66s -> 865.62s]  in the 1990s, probably, was the notion of extensibility,
[865.62s -> 867.32s]  that it might be desirable for programs
[867.32s -> 869.18s]  to be able to change the kernel on the fly,
[869.18s -> 871.10s]  like to be able to download new code into the kernel
[871.14s -> 873.98s]  or change the way it operates or something,
[873.98s -> 876.18s]  in order to do things like have databases
[876.18s -> 881.34s]  be able to control the layout of data on the disk.
[881.34s -> 886.72s]  And at least in decades past, monolithic kernels
[886.72s -> 890.44s]  tended not to have any particular features that
[890.44s -> 892.18s]  helped with this kind of extensibility.
[892.18s -> 894.14s]  You're just stuck with whatever the kernel did.
[894.14s -> 901.10s]  OK, so these were sort of problems
[901.10s -> 904.22s]  in the back of people's minds that
[904.22s -> 907.30s]  led them to think about other kinds of other ways
[907.30s -> 909.54s]  of designing other architectures for operating
[909.54s -> 911.02s]  systems.
[911.02s -> 913.94s]  And there were a number of ideas,
[913.94s -> 916.58s]  some quite radically different, that people pursued.
[916.58s -> 919.82s]  We're going to talk about one of them, a particularly
[919.82s -> 921.14s]  popular one today.
[921.22s -> 923.10s]  And that's the idea of microkernels.
[926.94s -> 929.46s]  Microkernels, although many of the ideas
[929.46s -> 935.02s]  go back to the beginning of computer history,
[935.02s -> 938.50s]  they became a sort of hot research topic
[938.50s -> 943.42s]  starting in maybe the mid to late 1980s.
[943.42s -> 949.34s]  And the big idea, so a microkernel,
[949.34s -> 952.34s]  this word, by the way, refers to a sort
[952.34s -> 954.10s]  of general approach or concept.
[954.10s -> 956.14s]  It doesn't refer to any specific artifact.
[956.14s -> 959.86s]  There were many people who designed and built
[959.86s -> 963.58s]  operating systems that followed the general sort of plan
[963.58s -> 964.34s]  for microkernels.
[964.34s -> 966.48s]  But each of these projects ended up
[966.48s -> 967.94s]  designing an operating system that was maybe
[967.94s -> 969.58s]  quite different from the others.
[969.58s -> 977.82s]  So the key idea was the tiny kernel
[977.82s -> 984.74s]  that supported just IPC, or inter-process communication,
[984.74s -> 990.94s]  and some sort of notion of threads or tasks.
[990.94s -> 993.30s]  So you have a kernel that provides you
[993.30s -> 995.92s]  a notion of sort of process-like abstraction
[995.92s -> 997.62s]  and a way for processes to communicate
[997.62s -> 1000.30s]  to each other with this inter-process communication
[1000.30s -> 1001.74s]  and nothing else.
[1001.74s -> 1003.54s]  And everything else you might want to do,
[1003.54s -> 1005.74s]  like have a file system you'd implement
[1005.74s -> 1008.78s]  as a process, as a task, as a user-level code,
[1008.78s -> 1011.42s]  not in the kernel at all.
[1011.42s -> 1017.06s]  So a picture for that might be now
[1017.06s -> 1020.34s]  we're going to use Mu for microkernel.
[1020.34s -> 1021.94s]  We've got the microkernel down here.
[1021.94s -> 1024.14s]  And we've got user space processes up here.
[1024.14s -> 1027.30s]  And we might have all the kind of usual processes run,
[1027.30s -> 1031.18s]  maybe we're going to run VI, my favorite text
[1031.18s -> 1036.18s]  editor, my compiler, my Windows system.
[1040.66s -> 1042.78s]  But also up here, as user-level processes,
[1042.78s -> 1046.50s]  we're going to have the file system just as a server
[1046.50s -> 1047.90s]  process and user space.
[1047.90s -> 1051.90s]  Maybe we're going to have a disk driver that knows how
[1051.90s -> 1053.74s]  to talk to my disk hardware.
[1053.78s -> 1055.58s]  Maybe we'll have a network stack
[1055.58s -> 1063.78s]  that knows how to talk TCP to my network interface card.
[1063.78s -> 1066.46s]  Maybe we'll have user-level processes
[1066.46s -> 1069.22s]  in charge of doing fancy paging
[1069.22s -> 1072.42s]  tricks like memory-mapped files
[1072.42s -> 1074.66s]  or maybe implements copy-on-write fork
[1074.66s -> 1076.82s]  or something.
[1076.82s -> 1081.10s]  And when my text editor needs to read a file,
[1081.10s -> 1082.66s]  it needs to talk to the file system.
[1082.66s -> 1092.86s]  So it's going to send a message via IPC in our process
[1092.86s -> 1096.02s]  communication to the file system
[1096.02s -> 1098.50s]  server, which has all the file system code in it,
[1098.50s -> 1099.90s]  knows about files and directories.
[1099.90s -> 1101.30s]  And the file system server code
[1101.30s -> 1103.34s]  may need to talk to the disk.
[1103.34s -> 1106.34s]  So it might send another disk read or write
[1106.34s -> 1109.98s]  to another IPC to the disk driver, which somehow
[1109.98s -> 1111.58s]  talks to the disk hardware.
[1111.82s -> 1116.54s]  Disk driver may return a disk block to the file server.
[1116.54s -> 1118.74s]  After it does its thing, maybe the file server finally
[1118.74s -> 1120.42s]  returns the data you asked for, again,
[1120.42s -> 1123.46s]  by inter-process communication messages.
[1123.46s -> 1125.62s]  Back to my text editor.
[1125.62s -> 1128.70s]  But the critical thing to notice here
[1128.70s -> 1131.78s]  is that the only stuff going down here in the kernel
[1131.78s -> 1136.42s]  is support for these processes or tasks or threads
[1136.42s -> 1140.42s]  or whatever they might be and support
[1140.42s -> 1142.50s]  for the inter-process communication message
[1142.50s -> 1143.70s]  passing and nothing else.
[1143.70s -> 1145.14s]  There's no file system down here.
[1145.14s -> 1148.02s]  There's no device drivers necessarily down here
[1148.02s -> 1148.70s]  in the kernel.
[1148.70s -> 1150.86s]  There's no network stack.
[1150.86s -> 1155.18s]  All that stuff up here is more or less ordinary user
[1155.18s -> 1158.58s]  level processes.
[1158.58s -> 1161.98s]  And so it leads you to a very small kernel
[1161.98s -> 1166.34s]  with relatively little code to optimize.
[1166.34s -> 1168.74s]  You can optimize IPC, and there's not much else going on.
[1171.42s -> 1173.02s]  And so this is the kind of picture
[1173.02s -> 1175.26s]  we're going to talk about for the rest of the lecture.
[1175.26s -> 1179.10s]  And just to give you a taste of where this ended up,
[1179.10s -> 1182.62s]  there are actually still microkernels in use today.
[1182.62s -> 1184.54s]  And indeed, the L4 microkernel, which
[1184.54s -> 1189.02s]  is the topic of today's paper, turns out to be used.
[1189.02s -> 1192.14s]  There's many instances, many, many instances of L4 running,
[1192.14s -> 1194.82s]  because it's used in a lot of cell phones
[1194.82s -> 1197.24s]  in the little microcontrollers that control the cell phone
[1197.24s -> 1198.66s]  radios.
[1198.66s -> 1202.26s]  And it's also apparently used in recent iPhones
[1202.26s -> 1203.82s]  as the operating system that runs
[1203.82s -> 1208.90s]  on the special dedicated enclave processor in the iPhone
[1208.90s -> 1212.62s]  that hides the secret cryptographic keys.
[1212.62s -> 1216.14s]  So there's a bunch of embedded, where these microkernels have
[1216.14s -> 1220.50s]  won out is in little embedded specialized computer systems.
[1220.50s -> 1223.86s]  Not laptops, but computers that are
[1223.86s -> 1227.82s]  dedicated to single specialized tasks,
[1227.86s -> 1229.86s]  where you may not need the complexity of Linux,
[1229.86s -> 1233.18s]  but you do need some operating system.
[1233.18s -> 1237.38s]  And the other thing that's the other final result
[1237.38s -> 1240.26s]  from microkernels is that the idea of user level
[1240.26s -> 1245.18s]  services with other programs talking to them with IPC,
[1245.18s -> 1248.14s]  that also has made its way into a lot of operating systems
[1248.14s -> 1252.14s]  like Mac OS, which I'm running right now to talk to you.
[1252.14s -> 1256.34s]  It's as well as being a kind of ordinary monolithic kernel,
[1256.34s -> 1259.74s]  it also has good support for user level
[1259.74s -> 1263.58s]  services and IPC between Unix processes
[1263.58s -> 1264.78s]  to talk to these services.
[1264.78s -> 1270.98s]  So that idea also was a successful idea
[1270.98s -> 1272.98s]  and widely adopted.
[1272.98s -> 1276.82s]  OK, so this is the basic architecture.
[1276.82s -> 1278.46s]  I'm going to go on and sort of talk
[1278.46s -> 1282.14s]  about some ways and reasons why this might be attractive.
[1282.14s -> 1284.42s]  But first, are there any just kind of high level questions
[1284.42s -> 1288.90s]  about what it is I mean by a microkernel?
[1294.26s -> 1308.34s]  OK, so what is it that people were hoping for when they
[1308.34s -> 1309.86s]  started building microkernels?
[1309.86s -> 1314.10s]  So one big motivation, although you wouldn't necessarily
[1314.10s -> 1317.18s]  see it written down much, is just a sense of aesthetics.
[1317.18s -> 1319.10s]  I think just a lot of people feel
[1319.10s -> 1325.50s]  that huge, complicated, single programs like a Linux kernel
[1325.50s -> 1326.98s]  are just not very elegant.
[1326.98s -> 1328.58s]  That surely we can build something
[1328.58s -> 1334.14s]  that's much more, much smaller, much more focused design
[1334.14s -> 1339.18s]  isn't such a huge grab bag of random different features.
[1339.18s -> 1342.94s]  So I think there was a strong sort of aesthetic feeling
[1342.98s -> 1345.62s]  that surely we can do better than big kernels.
[1348.30s -> 1350.50s]  But other sort of more specific things that you might be able
[1350.50s -> 1356.58s]  to quantify are something, a kernel that's small
[1356.58s -> 1357.58s]  might be more secure.
[1361.22s -> 1364.62s]  Fewer lines of code you have, probably fewer bugs you have,
[1364.62s -> 1367.42s]  less chance of somebody being able to exploit one
[1367.42s -> 1370.58s]  of those bugs to break security.
[1370.58s -> 1376.86s]  And in the extreme of that, you could imagine an operating
[1376.86s -> 1382.18s]  system that's actually provably correct,
[1382.18s -> 1384.14s]  where somebody can sit down and write a proof
[1384.14s -> 1386.38s]  that the operating system has no bugs
[1386.38s -> 1388.82s]  or does exactly what it's supposed to do and nothing
[1388.82s -> 1389.30s]  else.
[1389.30s -> 1395.18s]  And indeed, there is at least one verified, proved correct,
[1395.18s -> 1397.66s]  proved secure operating system named
[1397.66s -> 1401.90s]  SEL4, which is one of the many descendants of the L4
[1401.90s -> 1406.94s]  microkernel that the day's paper.
[1406.94s -> 1408.74s]  But you really, people know how
[1408.74s -> 1411.46s]  to verify sort of small to medium-sized programs,
[1411.46s -> 1413.54s]  but they don't know how to verify huge programs.
[1413.54s -> 1415.74s]  And that's the fact that microkernels are small.
[1415.74s -> 1417.18s]  It's sort of a critical ingredient
[1417.18s -> 1420.94s]  and being able to prove they're correct.
[1420.94s -> 1423.98s]  Another reason why you might like small is that a small
[1423.98s -> 1425.86s]  amount of code is often a lot easier
[1425.86s -> 1433.10s]  to optimize than a huge program.
[1433.10s -> 1435.30s]  Another reason why small might result in fast
[1435.30s -> 1439.22s]  is that you don't have to pay for a lot of features
[1439.22s -> 1440.10s]  that you don't use.
[1440.10s -> 1441.86s]  If your microkernel does hardly anything,
[1441.86s -> 1443.94s]  then you're not paying for a lot of features
[1443.94s -> 1446.98s]  you're not using.
[1446.98s -> 1451.06s]  Another reason for small is that a small kernel probably
[1451.06s -> 1453.62s]  bakes in far fewer design decisions,
[1453.62s -> 1457.82s]  forces fewer design decisions on application writers.
[1457.82s -> 1461.30s]  And so maybe it leaves them more flexibility
[1461.30s -> 1463.10s]  to make their own design decisions.
[1466.06s -> 1470.90s]  So by the way, these are not necessary consequences
[1470.90s -> 1472.10s]  of the microkernel approach.
[1472.10s -> 1473.74s]  These are things that people hoped for
[1473.74s -> 1477.58s]  and tried to achieve by using microkernels.
[1477.58s -> 1480.10s]  Another set of reasons why microkernels seemed attractive
[1480.10s -> 1482.42s]  has to do with the fact that a lot of the code
[1482.46s -> 1485.82s]  was at user level.
[1485.82s -> 1487.58s]  That is, a lot of features and functions
[1487.58s -> 1490.78s]  that we have grown used to being inside the kernel
[1490.78s -> 1492.78s]  were actually user-level services.
[1492.78s -> 1496.54s]  So they hope that by breaking the kernel apart and running
[1496.54s -> 1500.02s]  the different parts, like user-level services,
[1500.02s -> 1508.42s]  like a file service server, that might cause the code
[1508.42s -> 1510.62s]  to be more modular.
[1510.66s -> 1513.58s]  It might encourage operating system designers
[1513.58s -> 1519.22s]  to split up all these functions into many separate services,
[1519.22s -> 1522.74s]  and that might be a good thing.
[1522.74s -> 1525.94s]  User-level code is also possibly easier
[1525.94s -> 1527.50s]  to modify if stuff's at user level.
[1527.50s -> 1530.90s]  It's usually easier to tweak it or replace it or modify it
[1530.90s -> 1533.38s]  than doing the same stuff in the kernel.
[1533.38s -> 1535.46s]  So maybe it's easier to customize.
[1535.46s -> 1542.90s]  Putting the operating systems at user level
[1542.90s -> 1546.70s]  also might make them more robust.
[1546.70s -> 1550.86s]  If something goes wrong with the kernel,
[1550.86s -> 1553.30s]  usually you have to panic and reboot
[1553.30s -> 1556.98s]  because you can't necessarily trust
[1556.98s -> 1558.54s]  what's in the kernel anymore if it's
[1558.54s -> 1561.10s]  had some bug that maybe causes it to overwrite
[1561.10s -> 1563.06s]  a random part of its data.
[1563.06s -> 1565.44s]  Whereas if you have a bunch of user-level services
[1565.44s -> 1568.20s]  and one of them malfunctions and divides by zero
[1568.20s -> 1572.16s]  or dereferences a wild pointer, maybe only that one server
[1572.16s -> 1574.56s]  will crash and leaving the rest of the operating system
[1574.56s -> 1575.04s]  intact.
[1575.04s -> 1577.60s]  And maybe you can restart it, just that one server.
[1577.60s -> 1582.84s]  So maybe user-level moving OS functionality
[1582.84s -> 1585.76s]  to user-level processes might lead to more robustness.
[1585.76s -> 1589.28s]  And this is probably particularly evident for drivers.
[1589.28s -> 1593.08s]  Most bugs in the kernel are actually
[1593.08s -> 1594.64s]  hardware device drivers.
[1594.68s -> 1596.48s]  If we can manage to move the device drivers out
[1596.48s -> 1599.52s]  of the kernel, then we might have many fewer bugs
[1599.52s -> 1601.80s]  and crashes in the kernel.
[1601.80s -> 1604.24s]  And a final advantage that people were thinking about
[1604.24s -> 1606.96s]  back then is that you could emulate or run
[1606.96s -> 1609.00s]  multiple operating system personalities
[1609.00s -> 1610.56s]  on top of a microkernel.
[1610.56s -> 1613.08s]  So even though a microkernel does hardly anything for you
[1613.08s -> 1615.44s]  directly, you might be able to run a Unix server
[1615.44s -> 1619.64s]  or something on top of it, maybe more than one
[1619.64s -> 1621.00s]  on the same machine.
[1621.00s -> 1625.88s]  And of course, that's what today's paper is about,
[1625.88s -> 1629.24s]  is running a Unix, running Linux
[1629.24s -> 1631.84s]  as a service on a microkernel.
[1631.84s -> 1634.80s]  These are all the set of things
[1634.80s -> 1637.72s]  that people were hoping to be able to get some traction on
[1637.72s -> 1642.76s]  by looking into microkernel signs.
[1642.76s -> 1646.36s]  Of course, there's some sort of puzzles
[1646.36s -> 1649.28s]  you have to think through, some challenges.
[1651.00s -> 1656.84s]  One challenge, if you want to design your own microkernel,
[1656.84s -> 1658.44s]  is actually figuring out.
[1658.44s -> 1661.12s]  You want the API, you want the microkernel system call
[1661.12s -> 1663.56s]  interface to be as simple as possible,
[1663.56s -> 1665.72s]  because the whole point was to keep it small.
[1665.72s -> 1670.00s]  What is the actual smallest set of useful system calls
[1670.00s -> 1671.92s]  you can get away with?
[1671.92s -> 1674.68s]  And what does it look like?
[1674.68s -> 1677.92s]  That's not particularly clear.
[1677.96s -> 1679.68s]  So we're looking at the minimum system call.
[1687.60s -> 1690.16s]  You need this minimum system call API.
[1690.16s -> 1691.96s]  It's great if it's simple, but you actually
[1691.96s -> 1695.36s]  have to be able to build some pretty sophisticated features
[1695.36s -> 1697.52s]  out of your minimum system call API,
[1697.52s -> 1700.48s]  because even if the kernel doesn't do much, in the end,
[1700.48s -> 1702.44s]  you've got to be able to run programs.
[1702.44s -> 1705.68s]  Maybe if you're trying to run Unix on top of a microkernel,
[1705.68s -> 1709.28s]  you've got to be able to do things like fork and mmap.
[1709.28s -> 1713.88s]  So as part of the system call interface, simple,
[1713.88s -> 1715.84s]  low-level system call interface, it
[1715.84s -> 1721.88s]  has to be powerful enough to support all the stuff people
[1721.88s -> 1727.24s]  need to do, like exec and fork, and heck,
[1727.24s -> 1731.92s]  maybe even copy on write fork or memory mapping on disk
[1731.92s -> 1733.80s]  files, but all in a kernel that
[1733.84s -> 1736.68s]  has no idea about files or a file system.
[1736.68s -> 1738.96s]  It needs to support exec, but with a kernel that
[1738.96s -> 1739.96s]  knows nothing about files.
[1742.68s -> 1748.24s]  We need the rest of the operating system somehow.
[1748.24s -> 1750.96s]  Sure, the microkernel may be very simple,
[1750.96s -> 1753.80s]  but now we're sort of requiring
[1753.80s -> 1756.64s]  the development of some set of user-level servers
[1756.64s -> 1759.76s]  that implement the rest of the operating system.
[1759.76s -> 1762.12s]  So that has to get done at least
[1762.12s -> 1766.40s]  and may require some solving design puzzles.
[1766.40s -> 1770.56s]  And finally, this arrangement requires a lot of chitchat
[1770.56s -> 1774.64s]  over interprocessor communication over IPC.
[1774.64s -> 1776.92s]  So there's going to be great pressure
[1776.92s -> 1782.72s]  to make the IPC very fast.
[1782.72s -> 1789.56s]  So we're going to wonder whether IPC can be made fast enough
[1789.56s -> 1793.16s]  to keep microkernels competitive.
[1793.16s -> 1798.12s]  All right, and just in general, actually, not just IPC speed,
[1798.12s -> 1801.08s]  but in general, there's a lot of reason
[1801.08s -> 1804.48s]  to believe that monolithic kernels derive some performance
[1804.48s -> 1806.20s]  out of the fact that they're integrated,
[1806.20s -> 1809.04s]  that the file system code can talk to the virtual memory
[1809.04s -> 1810.64s]  code and memory allocation code.
[1810.64s -> 1815.08s]  And it's all sort of one big, happy, giant program.
[1815.08s -> 1817.20s]  And if you require all those things
[1817.20s -> 1818.96s]  to be split out into separate servers
[1818.96s -> 1821.84s]  or maybe split between a kernel and user level,
[1821.84s -> 1824.20s]  there may be fewer opportunities for optimization
[1824.20s -> 1826.00s]  by way of integration.
[1826.00s -> 1829.40s]  And that may or may not end up hurting performance.
[1832.52s -> 1838.32s]  All right, so these are sort of cross-cutting,
[1838.32s -> 1843.80s]  hoped-for wins, and challenges that all the many microkernel
[1843.80s -> 1844.64s]  projects faced.
[1845.24s -> 1846.80s]  Because of today's paper, I'm going
[1846.80s -> 1850.60s]  to tell you a bunch about L4 specifically,
[1850.60s -> 1856.68s]  which is the microkernel that the authors of today's paper
[1856.68s -> 1857.68s]  developed and used.
[1860.68s -> 1865.36s]  L4 is not, certainly not the earliest microkernel ever made,
[1865.36s -> 1868.88s]  but it's one of the sort of early microkernels that came out
[1868.88s -> 1872.24s]  of all the work in this, starting in the 1980s.
[1872.48s -> 1877.48s]  And it's fairly representative as far as how it works.
[1877.48s -> 1880.88s]  There's been, it's a bit of a moving target.
[1880.88s -> 1883.88s]  It was the subject of intense development and evolution
[1883.88s -> 1886.80s]  for many years, and it's still going strong.
[1886.80s -> 1888.44s]  If you look it up on Wikipedia, you'll
[1888.44s -> 1891.84s]  see that there's maybe 15 or 20 different variants of L4
[1891.84s -> 1893.28s]  that have kind of come and gone,
[1893.28s -> 1895.76s]  and some are still here, starting,
[1895.76s -> 1897.92s]  I think, in the late 1980s.
[1897.92s -> 1899.64s]  And I know what I'm going to go over,
[1899.84s -> 1904.12s]  and what I'm going to try to explain to you
[1904.12s -> 1907.64s]  is my understanding of how L4 worked at about the time
[1907.64s -> 1910.24s]  that today's paper came out.
[1914.40s -> 1921.16s]  All right, so just at a high level,
[1921.16s -> 1924.20s]  the L4 was certainly micro in the sense
[1924.20s -> 1927.92s]  that it was actually is a small kernel.
[1927.96s -> 1930.76s]  It has only seven system calls.
[1930.76s -> 1932.32s]  Some of them are a little bit complex,
[1932.32s -> 1934.32s]  but still it only has seven system calls,
[1934.32s -> 1936.28s]  whereas today's Linux, the last time I counted,
[1936.28s -> 1939.20s]  had in the mid 300s.
[1939.20s -> 1941.68s]  And even xv6, which is an extremely simple kernel,
[1941.68s -> 1944.52s]  even xv6 has 21 system calls.
[1944.52s -> 1949.76s]  So L4 has only seven.
[1949.76s -> 1951.48s]  So by that metric, it's simple.
[1951.48s -> 1953.76s]  It's also not very big.
[1953.76s -> 1956.92s]  I think as of the time this paper was written,
[1956.96s -> 1962.16s]  it had about 13,000 lines of code, which is not too much.
[1962.16s -> 1965.16s]  xv6 is smaller than that.
[1965.16s -> 1967.64s]  I think xv6 is maybe 6,000 or 7,000 lines of code
[1967.64s -> 1968.40s]  in the kernel.
[1968.40s -> 1970.64s]  But still, xv6 is very simple as kernels go,
[1970.64s -> 1973.12s]  and so L4, not much more complex than that.
[1973.12s -> 1978.84s]  And this is a 10th or a 20th or a 30th as big as Linux is.
[1978.84s -> 1981.72s]  This is pretty small.
[1981.72s -> 1986.28s]  It had only a few basic abstractions.
[1986.32s -> 1993.56s]  It had a notion of what they call tasks or address spaces.
[1997.40s -> 1998.76s]  And these more or less correspond
[1998.76s -> 2001.88s]  to what we would call a process in Unix.
[2001.88s -> 2005.52s]  It's a bunch of memories mapped starting at 0,
[2005.52s -> 2011.68s]  and you're able to execute in here, just like in a process.
[2011.68s -> 2015.20s]  One difference from xv6 is that there
[2015.20s -> 2020.32s]  can be multiple threads per task.
[2020.32s -> 2024.52s]  And L4 was in charge of scheduling
[2024.52s -> 2029.68s]  the multiple threads of execution within each task.
[2029.68s -> 2031.60s]  And part of the reason for this
[2031.60s -> 2033.08s]  is just that it's very convenient
[2033.08s -> 2037.16s]  to have threads as a program structuring tool.
[2037.16s -> 2041.64s]  And it was also, I don't know if they actually supported
[2041.64s -> 2044.28s]  multi-core or multi-processor machines at the time
[2044.28s -> 2047.60s]  that paper was written, but they may well have.
[2047.60s -> 2049.84s]  And threads are, of course, just what
[2049.84s -> 2053.12s]  you need to be able to harness multiple cores
[2053.12s -> 2054.72s]  executing in the same program.
[2054.72s -> 2058.60s]  So there were threads supported by the L4 kernel.
[2062.88s -> 2065.28s]  So L4 supported tasks.
[2065.28s -> 2066.88s]  It knew about tasks.
[2066.88s -> 2068.20s]  It knew about threads.
[2068.20s -> 2071.92s]  And it also knew about address spaces in the sense
[2071.92s -> 2074.64s]  that you could ask, tell L4, look,
[2074.64s -> 2079.00s]  here's how I want pages mapped in my address space.
[2079.00s -> 2082.48s]  And the other main thing that L4 knew about
[2082.48s -> 2085.40s]  is inter-process communication.
[2085.40s -> 2088.88s]  So every thread had an identifier,
[2088.88s -> 2091.76s]  and one thread could say, look, I want to send a message,
[2091.76s -> 2094.52s]  just some bytes, to another thread.
[2094.52s -> 2096.12s]  And here is its identifier.
[2096.12s -> 2100.80s]  Please send a message to that other thread.
[2102.92s -> 2110.04s]  So these are really the only tasks, threads, address spaces,
[2110.04s -> 2112.32s]  and IPC were really the only abstractions.
[2112.32s -> 2115.68s]  The system calls, I don't know
[2115.68s -> 2121.96s]  if I can be able to list them all, but the system
[2121.96s -> 2128.80s]  calls were, there was a thread create system call, which
[2128.80s -> 2132.20s]  also, you gave it a address space ID
[2132.20s -> 2134.00s]  and asked it to create a new thread.
[2134.00s -> 2136.88s]  And if the address space or task didn't already exist,
[2136.88s -> 2138.68s]  it would create a new task for you,
[2138.68s -> 2140.76s]  so to combine thread and task creation.
[2143.52s -> 2149.76s]  There's send and receive, various flavors of send
[2149.76s -> 2152.12s]  and receive, IPC system calls.
[2152.12s -> 2160.76s]  There's a way to map pages into your or other address spaces.
[2160.76s -> 2165.76s]  So you could ask L4 to change the way your address
[2165.76s -> 2168.96s]  space was set up, the way your page table maps were set up.
[2168.96s -> 2172.88s]  But you could also ask L4, if you had the right permissions,
[2172.88s -> 2175.76s]  to go and change the way another task's address
[2175.76s -> 2176.72s]  space was set up.
[2182.84s -> 2186.00s]  This was actually done through the IPC interface.
[2186.00s -> 2188.60s]  You would send a kind of special IPC message
[2188.60s -> 2191.04s]  that the kernel knew about to the target thread,
[2191.04s -> 2194.36s]  and the kernel would modify the target thread's address
[2194.36s -> 2196.52s]  space.
[2196.52s -> 2198.44s]  And this is if you were creating a new thread.
[2198.44s -> 2199.86s]  This is actually new threads are
[2199.86s -> 2201.56s]  created with no memory at all.
[2201.56s -> 2203.04s]  So if you want to create a thread,
[2203.04s -> 2204.84s]  you first call the thread create system
[2204.84s -> 2208.36s]  call to create the new thread and task and address space.
[2208.36s -> 2210.12s]  And then you send it.
[2210.16s -> 2212.08s]  You make one of these magic IPCs to send it
[2212.08s -> 2215.40s]  to some of your own memory that maps some of your own memory
[2215.40s -> 2218.12s]  that you've prepared with instructions or data
[2218.12s -> 2223.32s]  to map that memory into the new task address space.
[2223.32s -> 2225.72s]  And then you send a special start IPC
[2225.72s -> 2227.84s]  to this new task with a program counter
[2227.84s -> 2230.44s]  in the stack pointer you want it to start executing with.
[2230.44s -> 2232.10s]  And it'll start executing in that memory
[2232.10s -> 2233.84s]  you've set up at the program counter
[2233.84s -> 2237.44s]  that you asked it to start at.
[2237.44s -> 2239.72s]  There's a way not through system calls.
[2239.76s -> 2243.88s]  In fact, I don't know how it worked, but privileged tasks
[2243.88s -> 2250.60s]  could map device hardware, device control registers
[2250.60s -> 2251.76s]  into their own address spaces.
[2255.36s -> 2258.04s]  So L4 didn't really know much about devices like disks
[2258.04s -> 2262.80s]  or network interface cards, but user level software
[2262.80s -> 2268.00s]  could get directly at user level software that implemented
[2268.00s -> 2270.12s]  device drivers at user level could get directly
[2270.12s -> 2272.88s]  at device hardware.
[2272.88s -> 2276.04s]  There was a way you could tell
[2276.04s -> 2282.68s]  L4 to turn an interrupt, any interrupt from any device.
[2282.68s -> 2284.52s]  L4 didn't really know which device.
[2284.52s -> 2289.40s]  It just turned a given interrupt into an IPC message.
[2289.40s -> 2294.12s]  So a device driver task could not just
[2294.12s -> 2296.48s]  read and write the device hardware, but also tell L4,
[2296.52s -> 2298.12s]  well, anytime that device interrupts,
[2298.12s -> 2304.08s]  please send me an IPC message to notify me of the interrupt.
[2304.08s -> 2307.72s]  And finally, one task could tell the kernel
[2307.72s -> 2313.84s]  to give it notifications of another task page faults.
[2313.84s -> 2317.12s]  So if this task page faults, L4
[2317.12s -> 2321.12s]  would turn that into a IPC message
[2321.12s -> 2325.28s]  and send it to another designated pager task,
[2325.32s -> 2326.76s]  send the notification of the page fault
[2326.76s -> 2328.32s]  to a designated pager task.
[2328.32s -> 2331.20s]  So every task had an associated pager task
[2331.20s -> 2332.80s]  that handled its page faults.
[2332.80s -> 2336.96s]  And that's the way you get hooks into the page faults
[2336.96s -> 2339.76s]  in order to implement things like copy on write fork
[2339.76s -> 2340.92s]  or lazy allocation.
[2344.32s -> 2345.40s]  And that's it for the kernel.
[2345.40s -> 2346.92s]  There's nothing else in L4.
[2346.92s -> 2348.56s]  There's no file system.
[2348.56s -> 2353.24s]  L4 didn't itself have support for things like fork or exec.
[2353.28s -> 2356.68s]  It didn't have any communication beyond this very simple IPC,
[2356.68s -> 2359.36s]  like didn't have pipes, didn't have device drivers,
[2359.36s -> 2361.76s]  no networking support, nothing.
[2361.76s -> 2363.80s]  Everything else, if you wanted it,
[2363.80s -> 2366.80s]  you need to supply as user-level services.
[2372.36s -> 2378.12s]  OK, so one thing that L4 does supply
[2378.12s -> 2381.84s]  is switching among threads.
[2381.92s -> 2386.72s]  L4 would actually do the scheduling and context switches
[2386.72s -> 2390.08s]  in order to multiplex a single CPU among multiple threads.
[2390.08s -> 2391.52s]  And the way it did it, you would
[2391.52s -> 2394.20s]  find completely unsurprising.
[2394.20s -> 2397.24s]  L4 basically had saved registers
[2397.24s -> 2400.20s]  for every task, for every thread.
[2400.20s -> 2402.76s]  When it executed a thread, the executed thread
[2402.76s -> 2405.36s]  would jump into user space and switch page tables
[2405.36s -> 2406.08s]  to that thread.
[2406.08s -> 2408.48s]  And that thread will execute for a while in user space.
[2408.48s -> 2410.52s]  Then maybe the timer interrupt would go off.
[2410.52s -> 2413.04s]  And that was actually a device L4 knew about.
[2413.04s -> 2415.64s]  A timer interrupt might go off after a while,
[2415.64s -> 2416.64s]  interrupt into L4.
[2416.64s -> 2421.88s]  L4 would save this task's user registers in a per task,
[2421.88s -> 2425.00s]  an array of tasks or thread structures,
[2425.00s -> 2428.92s]  would save this thread's registers away,
[2428.92s -> 2430.60s]  pick a new task to run in a loop,
[2430.60s -> 2433.48s]  much like the scheduling loop in XV6,
[2433.48s -> 2438.40s]  restore this task's registers from its previously saved
[2438.40s -> 2442.12s]  registers, switch page tables, and then jump into this task
[2442.12s -> 2445.36s]  and execute it for a while until the timer interrupt went
[2445.36s -> 2450.20s]  off or until this task either yielded,
[2450.20s -> 2452.32s]  I think there's also probably a yield system
[2452.32s -> 2454.96s]  call or something like it.
[2454.96s -> 2457.24s]  A task could yield the CPU or a task
[2457.24s -> 2459.44s]  could wait to receive an IPC.
[2459.44s -> 2463.36s]  And in that case, L4 would jump back into L4
[2463.36s -> 2465.52s]  and L4 would save its registers,
[2465.52s -> 2468.00s]  switch to a new task, and run that task.
[2468.00s -> 2474.28s]  So that thread switching part of L4 would be very familiar.
[2485.96s -> 2490.32s]  I mentioned this before, but because it comes up,
[2490.32s -> 2497.56s]  I want to just write here this notion of a pager.
[2497.56s -> 2504.00s]  The repeat, if a process takes a page fault,
[2504.00s -> 2506.52s]  traps into the kernel, and the kernel turns that page fault
[2506.52s -> 2510.24s]  into an IPC message to a designated pager task
[2510.24s -> 2517.40s]  and tells this pager task which thread faulted
[2517.40s -> 2520.40s]  and the address it faulted on.
[2520.40s -> 2522.20s]  And then the pager task, if it wants
[2522.20s -> 2525.08s]  to, say, implement lazy allocation,
[2525.08s -> 2527.92s]  maybe this thread writes a memory
[2527.92s -> 2530.60s]  that wasn't allocated yet, but it
[2530.60s -> 2532.84s]  asked to be lazily allocated.
[2532.84s -> 2534.68s]  Its pager task would then be in charge
[2534.68s -> 2538.08s]  of allocating some memory from L4,
[2538.08s -> 2543.88s]  sending one of these special IPCs
[2543.88s -> 2547.40s]  that caused the memory to be mapped into this task,
[2547.40s -> 2549.92s]  and then sending an IPC to resume
[2549.92s -> 2553.72s]  execution inside this thread.
[2553.72s -> 2555.36s]  So there was this notion of pager task
[2555.36s -> 2559.20s]  to implement all the stuff that xv6 or Linux implements
[2559.20s -> 2560.36s]  in page fault handlers.
[2560.36s -> 2563.40s]  Like, you could implement copy on right fork with this
[2563.40s -> 2566.28s]  if you liked, or memory mapped files,
[2566.28s -> 2568.84s]  all using one of these pager tasks.
[2568.84s -> 2574.20s]  So they were a powerful user-level way
[2574.20s -> 2579.04s]  to play tricks driven by page faults.
[2579.04s -> 2581.52s]  And so this is an example, one of many examples
[2581.52s -> 2584.48s]  in which a microkernel like L4 might
[2584.48s -> 2587.48s]  have been quite a bit more flexible for user programs
[2587.48s -> 2588.96s]  than a conventional kernel.
[2588.96s -> 2591.80s]  Like, if you think Linux ought to do some extra thing,
[2591.80s -> 2596.60s]  like maybe some, if Linux didn't already
[2596.60s -> 2598.24s]  have copy on right fork and you wanted
[2598.24s -> 2600.48s]  to have copy on right fork, you really
[2600.48s -> 2603.52s]  can't implement that in Linux without modifying the kernel.
[2603.52s -> 2605.52s]  There's no way to write portable code,
[2605.52s -> 2608.00s]  portable user-level code for Linux
[2608.00s -> 2612.24s]  that could implement something like copy on right fork.
[2612.24s -> 2615.32s]  So that's not quite true, but it would be very difficult.
[2615.32s -> 2618.68s]  Whereas in L4, it's relatively straightforward.
[2618.68s -> 2620.56s]  L4 is completely set up for you
[2620.56s -> 2622.48s]  to be able to write user-level code that
[2622.48s -> 2624.60s]  gets the page faults that are required
[2624.60s -> 2628.64s]  to drive copy on right fork, all in user space
[2628.64s -> 2630.44s]  without having to mess with the kernel.
[2630.84s -> 2631.34s]  OK.
[2636.00s -> 2640.80s]  So any questions so far about how L4 works?
[2640.80s -> 2641.32s]  Oh, sorry.
[2641.32s -> 2643.04s]  Can you just clarify the difference
[2643.04s -> 2645.96s]  between a thread and a task?
[2645.96s -> 2648.28s]  Yes.
[2648.28s -> 2653.20s]  A task corresponds to a, it's like a process.
[2653.20s -> 2657.28s]  In XV6, it has a bunch of memory and an address space.
[2657.28s -> 2659.92s]  And you can execute user code in it.
[2659.92s -> 2664.16s]  XV6, if you have a process in XV6,
[2664.16s -> 2666.20s]  there can only be one thread of control
[2666.20s -> 2670.92s]  on a single executing inside a process in XV6.
[2670.92s -> 2675.40s]  But in modern operating systems and in L4,
[2675.40s -> 2678.60s]  in a single process, in a single address space,
[2678.60s -> 2680.92s]  if you have multiple cores, you're
[2680.92s -> 2687.80s]  going to have multiple cores executing in a single task,
[2687.84s -> 2690.80s]  typically always each set up with its own stack
[2690.80s -> 2693.84s]  inside that task's address space.
[2693.84s -> 2695.88s]  And so that means you can, for example, write
[2695.88s -> 2699.72s]  a single program that can get parallel speedup, improved
[2699.72s -> 2702.52s]  performance from multi-core hardware
[2702.52s -> 2706.04s]  by running one thread on, having multiple threads each
[2706.04s -> 2710.76s]  running on a different core, thereby getting more work done.
[2710.76s -> 2711.32s]  OK, I see.
[2711.32s -> 2712.32s]  Thank you.
[2712.32s -> 2712.80s]  Yes.
[2712.80s -> 2722.20s]  OK, so as you can see, this is a design that relies heavily
[2722.20s -> 2725.84s]  on IPC, because you're going to want
[2725.84s -> 2727.04s]  to talk to your file server.
[2727.04s -> 2729.12s]  And the file server is going to want to talk
[2729.12s -> 2730.48s]  to the device driver server.
[2730.48s -> 2733.16s]  You're going to have IPC messages flying back and forth
[2733.16s -> 2735.56s]  for every system call, for every page fault,
[2735.56s -> 2736.88s]  for every device interrupt.
[2736.88s -> 2739.60s]  The IPC system just has to be fast.
[2739.60s -> 2747.04s]  However, now we're starting to talk
[2747.04s -> 2748.88s]  about a serious potential defect
[2748.88s -> 2751.00s]  in the microkernel story.
[2760.04s -> 2764.60s]  So first, let me show you a straightforward but very slow
[2764.60s -> 2770.72s]  design for IPC patterned off of Unix pipes.
[2770.72s -> 2772.80s]  And I'm bringing this up because some early
[2772.80s -> 2777.24s]  microkernels worked in a similar way
[2777.24s -> 2780.72s]  to what I'm about to show you, which turned out to be slow.
[2780.72s -> 2787.96s]  OK, so let's suppose you have two processes.
[2787.96s -> 2789.96s]  We got P1.
[2789.96s -> 2792.16s]  P1 wants to send a message to P2.
[2795.32s -> 2796.64s]  So how should that actually work?
[2796.64s -> 2801.72s]  Well, one possibility is to have a send a system call.
[2801.72s -> 2805.52s]  And you give send system call the ID of the thread
[2805.52s -> 2811.04s]  you want to send the message to and a pointer to the message
[2811.04s -> 2813.48s]  to the bytes maybe that you actually
[2813.48s -> 2814.76s]  want to send to that process.
[2814.76s -> 2815.92s]  So this is a system call.
[2815.92s -> 2818.76s]  So you're going to jump into the kernel.
[2818.76s -> 2822.40s]  Maybe we design this patterned after pipes in xv6.
[2822.88s -> 2827.04s]  So you can imagine there being a buffer of messages waiting.
[2827.04s -> 2829.12s]  Maybe P2 is doing something else right now.
[2829.12s -> 2830.12s]  Maybe it's a server.
[2830.12s -> 2831.72s]  It's serving somebody else's request.
[2831.72s -> 2834.32s]  So it's not ready to handle your request.
[2834.32s -> 2838.56s]  You can imagine maybe a buffer of waiting messages
[2838.56s -> 2840.56s]  in the kernel, like a pipe buffer.
[2840.56s -> 2844.80s]  When you call send, it appends your message
[2844.80s -> 2847.60s]  to this buffer waiting for P2 to receive it.
[2847.60s -> 2852.76s]  Now, in fact, almost always in these systems,
[2852.76s -> 2855.20s]  you rarely just wanted to send a message.
[2855.20s -> 2857.28s]  You almost always wanted to get a response to.
[2857.28s -> 2861.52s]  You wanted an RPC or remote procedure call operation.
[2861.52s -> 2864.16s]  So in fact, P1 would probably follow this immediately
[2864.16s -> 2867.68s]  by a receive to try to get the response back.
[2867.68s -> 2870.08s]  But in general, let's just imagine
[2870.08s -> 2871.88s]  we're doing a one-way IPC for the moment.
[2871.88s -> 2874.84s]  So send would append your message
[2874.84s -> 2877.40s]  to the in kernel buffer.
[2877.40s -> 2880.40s]  We would have to copy the message bytes from user space
[2880.40s -> 2883.12s]  into this buffer and then return and process
[2883.12s -> 2885.80s]  one can do something else, like maybe prepare
[2885.80s -> 2888.04s]  to receive the response.
[2888.04s -> 2893.48s]  After a while, P2 is going to want to receive the next message.
[2893.48s -> 2897.68s]  It's going to make the receive system call.
[2897.68s -> 2902.52s]  And that's going to return the ID of the sender
[2902.52s -> 2906.64s]  and copy the message into P2's memory.
[2906.68s -> 2908.92s]  That's going to take the front message off the queue,
[2908.92s -> 2912.52s]  copy into P2's memory, and then return.
[2916.08s -> 2927.92s]  So this is called a, there's some words for this,
[2927.92s -> 2931.04s]  whose opposites you'll see and saw in today's paper.
[2931.08s -> 2936.68s]  This is called an asynchronous scheme
[2936.68s -> 2938.64s]  because P1 sends a message without having
[2938.64s -> 2939.64s]  to wait for anything.
[2939.64s -> 2942.64s]  It just appends it to this queue and returns.
[2942.64s -> 2947.92s]  And it's called a buffered system
[2947.92s -> 2952.08s]  because the kernel copies its message into the buffer,
[2952.08s -> 2954.16s]  into its internal buffer on a send.
[2954.16s -> 2956.40s]  And then later, when the receive happens,
[2956.40s -> 2958.76s]  it copies the message out of the buffer to the target.
[2958.76s -> 2962.44s]  So this is asynchronous and buffered.
[2962.44s -> 2965.32s]  If you're doing a full request response pair,
[2965.32s -> 2966.56s]  then P1's going to call send.
[2966.56s -> 2967.76s]  Send's going to return.
[2967.76s -> 2970.24s]  P1 is then immediately, let's assume,
[2970.24s -> 2972.28s]  we're going to assume that there's really two sets
[2972.28s -> 2974.32s]  of buffers, one for each direction.
[2974.32s -> 2978.40s]  P1's immediately going to call receive.
[2978.40s -> 2980.24s]  Receive's going to wait, going to need
[2980.24s -> 2984.04s]  to wait for something to appear in the reply buffer.
[2984.04s -> 2985.84s]  So it's going to have to yield the CPU.
[2985.84s -> 2987.20s]  It's going to have to do something
[2987.20s -> 2990.40s]  deep in xv6 to yield the CPU.
[2990.40s -> 2994.12s]  And on a single CPU system, it may be only at this point
[2994.12s -> 2998.04s]  that P1 gives up the CPU and now P2 can run.
[2998.04s -> 3000.28s]  And indeed, the hardware in this era
[3000.28s -> 3003.36s]  was almost always single core.
[3003.36s -> 3005.44s]  Certainly, this paper is running
[3005.44s -> 3006.48s]  on single core hardware.
[3006.48s -> 3010.76s]  So P1's going to be P1 executing and P1 not executing
[3010.76s -> 3012.84s]  until P1 finally gives up the CPU
[3012.84s -> 3015.92s]  and receive, waiting for a message to appear here.
[3015.92s -> 3017.72s]  And only then will P2 be scheduled.
[3017.72s -> 3019.68s]  Maybe it'll call receive.
[3019.68s -> 3021.40s]  Receive will copy the message.
[3021.40s -> 3028.16s]  And then P2 will make its call to send,
[3028.16s -> 3031.44s]  to append its reply.
[3031.44s -> 3033.96s]  And then the send system call will return to P2.
[3033.96s -> 3036.48s]  And at some point, presumably, P2 will give up the CPU.
[3036.48s -> 3038.32s]  Maybe the timer will go off.
[3038.32s -> 3041.60s]  And P1 will resume execution in the kernel,
[3041.60s -> 3043.04s]  see that there's a message there,
[3043.04s -> 3045.76s]  and return it back to user space.
[3046.08s -> 3051.24s]  So that means that in this design, this slow design,
[3051.24s -> 3057.36s]  in order to have a request and a response,
[3057.36s -> 3061.76s]  there's four system calls, two sends, and two receives,
[3061.76s -> 3064.92s]  and eight user kernel crossings, each one of which
[3064.92s -> 3067.72s]  is reasonably expensive.
[3067.72s -> 3069.44s]  There's a need to sleep.
[3069.44s -> 3074.24s]  This receive has to sleep waiting for data to appear.
[3074.24s -> 3076.40s]  And there's a full call to the scheduler loop
[3076.40s -> 3080.52s]  and a context switch from P1 to P2 in order to make this.
[3080.52s -> 3083.56s]  And each of these kernel crossings and context
[3083.56s -> 3085.12s]  switchings is potentially expensive
[3085.12s -> 3088.12s]  because every time you cross the kernel user boundary,
[3088.12s -> 3090.52s]  you switch page tables.
[3090.52s -> 3097.96s]  And that has a near certainty of disturbing the CPU caches.
[3097.96s -> 3100.24s]  Like, changing the page table probably
[3100.24s -> 3104.72s]  flushes the TLB, the virtual memory lookup cache, which
[3104.72s -> 3107.68s]  is going to slow things down.
[3107.68s -> 3111.44s]  So this is a pretty slow way to go
[3111.44s -> 3114.20s]  and involves a lot of kernel crossings,
[3114.20s -> 3116.56s]  copying of messages between user and kernel,
[3116.56s -> 3120.40s]  maybe allocation of buffers, et cetera.
[3120.40s -> 3124.04s]  But it turns out that for this stylized case in which
[3124.04s -> 3128.40s]  you're sending a request and you want to get a response back,
[3128.40s -> 3133.60s]  you can strip this down to a considerably simpler design.
[3133.60s -> 3137.32s]  And in fact, this is the way L4 worked.
[3137.32s -> 3140.88s]  And this was laid out in a famous paper called
[3140.88s -> 3144.44s]  Improving IPC by Kernel Design, published a few years
[3144.44s -> 3145.68s]  before today's paper.
[3150.24s -> 3152.48s]  So it does a couple of things differently.
[3152.48s -> 3157.76s]  For one thing, it's synchronous.
[3157.76s -> 3162.60s]  That is, there's none of this.
[3162.60s -> 3168.16s]  There's no dropping something off and returning and waiting,
[3168.16s -> 3171.00s]  letting the other guy, letting the other process pick up
[3171.00s -> 3172.80s]  the data when it feels like it.
[3172.80s -> 3176.60s]  Instead, send waits for receive
[3176.60s -> 3178.04s]  and receive waits for send.
[3178.04s -> 3186.64s]  So if I'm process one and I call send,
[3186.64s -> 3191.52s]  it doesn't copy my message into a buffer.
[3191.52s -> 3197.04s]  P1 will now immediately, if P1 send in the L4 kernel,
[3197.04s -> 3199.92s]  waits for P2 to call receive.
[3199.92s -> 3202.60s]  And if P2 is already in the kernel waiting
[3202.60s -> 3205.52s]  in a call to receive, well, P2 is either already
[3205.52s -> 3209.64s]  in the kernel waiting in a call to receive,
[3209.64s -> 3215.24s]  or P1 send waits for P2's next call to receive.
[3215.24s -> 3217.88s]  When both have arrived here, when
[3217.88s -> 3221.40s]  P1 is in the kernel and it's called to send,
[3221.40s -> 3223.52s]  and P2's in the kernel and it's called to receive,
[3223.52s -> 3226.36s]  only then does anything happen.
[3226.36s -> 3232.32s]  And one reason this is fast is that if P2 is already
[3232.32s -> 3237.32s]  in receive, then P1, when it's executing send in the kernel,
[3237.32s -> 3240.64s]  can just, without a context switch
[3240.64s -> 3244.44s]  or a general purpose scheduling, can just
[3244.44s -> 3247.44s]  jump back into user space into P2
[3247.44s -> 3250.60s]  as if it was returning from this receive.
[3250.60s -> 3253.36s]  And that's a much faster path through the kernel
[3253.36s -> 3256.62s]  than saving registers, giving up the CPU,
[3256.62s -> 3260.56s]  calling the scheduler, and finding a new process to run.
[3260.56s -> 3265.96s]  Instead, P1 send knows that there's a waiting receive
[3265.96s -> 3271.34s]  and just sort of immediately jumps into P2
[3271.34s -> 3272.90s]  as if it was returning from receive.
[3276.22s -> 3282.94s]  The scheme that they developed is also unbuffered,
[3282.94s -> 3287.62s]  and it can do that partially because it's synchronous.
[3287.62s -> 3291.52s]  When both the send and the receive are in the kernel,
[3291.52s -> 3296.18s]  the message can be sending some message.
[3296.18s -> 3299.38s]  The kernel can directly copy the message from user space
[3300.22s -> 3302.26s]  without having to first copy it into the kernel
[3302.26s -> 3303.82s]  and then back out of the kernel.
[3303.82s -> 3308.30s]  Because since both sides wait for the other system call
[3308.30s -> 3310.98s]  to happen, that means that they've
[3310.98s -> 3313.14s]  waited for both pointers to be known.
[3313.14s -> 3315.10s]  Receive specifies where it wants
[3315.10s -> 3317.46s]  the message to be deposited.
[3317.46s -> 3319.74s]  So at this point, we know both addresses,
[3319.74s -> 3321.62s]  then kernel can just do the copy directly
[3321.62s -> 3323.26s]  instead of through the kernel.
[3323.26s -> 3330.78s]  And if the message is super small,
[3330.78s -> 3332.90s]  like maybe only a few dozen bytes,
[3332.90s -> 3335.78s]  then it can be passed in registers
[3335.78s -> 3342.06s]  without any copy at all, what you might call zero copy.
[3345.30s -> 3346.90s]  Remember, the send only proceeds
[3346.90s -> 3350.70s]  if P2 is already in receive, and the send basically
[3350.70s -> 3352.18s]  jumps directly to P2.
[3352.18s -> 3354.82s]  Well, this code path through the kernel
[3354.82s -> 3359.50s]  takes care to not disturb a bunch of the registers.
[3359.50s -> 3361.70s]  And that means that P1 can put its system call
[3361.70s -> 3363.38s]  if the message is short.
[3363.38s -> 3367.02s]  It can put the message in certain designated registers.
[3367.02s -> 3370.34s]  The kernel guarantees to preserve those registers
[3370.34s -> 3371.54s]  on its way up to P2.
[3371.54s -> 3376.46s]  And that means that when the kernel returns from the receive
[3376.46s -> 3379.10s]  system call, but as a result of send,
[3379.10s -> 3381.90s]  the contents of those designated registers
[3381.94s -> 3385.66s]  hold the message, and therefore never had to be copied at all
[3385.66s -> 3388.02s]  from memory to memory, never had to be moved at all.
[3388.02s -> 3389.98s]  They're just sitting right in the registers
[3389.98s -> 3391.66s]  where they can be accessed very quickly.
[3394.14s -> 3395.50s]  And this, of course, this only
[3395.50s -> 3397.98s]  works for small messages.
[3397.98s -> 3403.26s]  For very large messages, L4 could carry a page mapping
[3403.26s -> 3404.50s]  in an IPC message.
[3404.50s -> 3409.86s]  So for huge messages, like the result
[3409.86s -> 3415.18s]  of reading a block from a file or something,
[3415.18s -> 3416.82s]  you could just send the page, and it
[3416.82s -> 3418.90s]  would be mapped into the target's address space
[3418.90s -> 3421.86s]  again without any copy.
[3421.86s -> 3424.78s]  So this is done through page mapping.
[3424.78s -> 3431.62s]  Give away the page or access to permission
[3431.62s -> 3433.74s]  to share the page.
[3433.74s -> 3436.18s]  And so small messages are fast.
[3436.18s -> 3437.82s]  Huge messages are pretty fast.
[3437.82s -> 3439.74s]  You still have to adjust the page table to target.
[3439.74s -> 3442.46s]  But that's much faster than copying.
[3442.46s -> 3444.70s]  And a final trick that L4 played
[3444.70s -> 3449.50s]  was noticing that if you're doing an RPC with a request
[3449.50s -> 3457.34s]  and a response, there's a very stylized pairs of system
[3457.34s -> 3460.22s]  calls, and you may as well combine system calls,
[3460.22s -> 3461.98s]  send and receive system calls, in order
[3461.98s -> 3463.98s]  to reduce kernel crossings.
[3463.98s -> 3468.50s]  So for the special case of RPC, which is almost always
[3468.50s -> 3471.90s]  what people are doing when they're using IPC,
[3471.90s -> 3476.18s]  there was a call, system call.
[3476.18s -> 3484.06s]  And a call was basically a combined send plus receive,
[3484.06s -> 3485.98s]  but without the return to user space,
[3485.98s -> 3488.58s]  and then re-entry into kernel space
[3488.58s -> 3493.38s]  that a pair of system calls take.
[3493.38s -> 3497.54s]  And on the server side, there was a single call
[3497.54s -> 3502.58s]  that would send the reply from one system call
[3502.58s -> 3507.90s]  and then wait for the request message from anyone
[3507.90s -> 3510.06s]  for the next system call.
[3510.06s -> 3514.02s]  And this was basically a send of one response plus a wait
[3514.02s -> 3516.06s]  to receive the next request.
[3516.06s -> 3520.02s]  And this, again, cut in half the number of kernel crossings.
[3520.02s -> 3525.58s]  And it turned out that the sum of all of these optimizations
[3525.62s -> 3530.38s]  for the kind of short RPCs, which
[3530.38s -> 3538.42s]  are one typical workload, all this led to a 20x speedup.
[3538.42s -> 3539.98s]  This is what their paper reported,
[3539.98s -> 3542.06s]  20x speedup over their previous system, which
[3542.06s -> 3544.34s]  was presumably a little bit more like what
[3544.34s -> 3548.18s]  I showed in the previous design.
[3548.18s -> 3551.58s]  And so this was an impressive.
[3551.58s -> 3553.06s]  This paper came out a few years
[3553.70s -> 3555.78s]  by some of the same authors, but a few years
[3555.78s -> 3557.10s]  before the people were reading.
[3557.10s -> 3561.66s]  And this caused people to view microkernels a little bit more
[3561.66s -> 3567.46s]  favorably, that the IPC could actually be made quite fast.
[3567.46s -> 3574.58s]  Any questions about these IPC tricks that L4 plays?
[3574.58s -> 3576.26s]  Yeah, I think I missed this.
[3576.26s -> 3582.38s]  But when is the process receiving messages?
[3582.46s -> 3585.50s]  When is it using that system call?
[3585.50s -> 3591.94s]  OK, so for RPCs, for request response, in fact,
[3591.94s -> 3596.82s]  the process is use this pair of system calls
[3596.82s -> 3599.62s]  rather than send and receive.
[3599.62s -> 3602.62s]  So yeah, call, you really give it two arguments,
[3602.62s -> 3604.94s]  a message you want to send and a place
[3604.94s -> 3606.58s]  to put the response.
[3606.58s -> 3609.30s]  And inside the kernel, it just combines these two.
[3609.30s -> 3613.86s]  You could view this as a bit of a hack, but because IPC
[3613.86s -> 3617.50s]  is so frequent, it's worth a little bit of hackery
[3617.50s -> 3620.58s]  in order to make it be fast.
[3620.58s -> 3624.14s]  And in the diagram up there in the box where
[3624.14s -> 3627.34s]  you have P2 sending that or running the receive system
[3627.34s -> 3631.22s]  call, what prompted P2 to?
[3631.22s -> 3639.10s]  OK, in my RPC world, we have clients.
[3639.10s -> 3640.94s]  And they're sending requests to servers.
[3644.82s -> 3647.02s]  And the server is going to do something and reply.
[3647.02s -> 3649.58s]  So since P2 is a server, we imagine
[3649.58s -> 3653.10s]  that P2 is sitting in a while loop in which it's
[3653.10s -> 3655.34s]  going to receive the next message from any client,
[3655.34s -> 3657.10s]  do a little bit of work to process it,
[3657.10s -> 3658.66s]  look up some data in a database or something,
[3658.66s -> 3661.14s]  and then send a reply, then go back to the top of the loop
[3661.14s -> 3662.22s]  and wait again.
[3662.22s -> 3664.90s]  So to a first approximation, we expect
[3664.90s -> 3670.22s]  P2 to spend all its time waiting for the next message
[3670.22s -> 3674.58s]  from anyone, next request from anyone.
[3674.58s -> 3682.02s]  And this design really does kind of rely on P2 always
[3682.02s -> 3683.62s]  when it's at rest, basically sitting
[3683.62s -> 3686.22s]  in the kernel in a receive system call,
[3686.22s -> 3691.74s]  waiting for the next request so that the next request can
[3691.74s -> 3696.06s]  directly basically return from that system call.
[3696.06s -> 3700.66s]  That's the fast path that's super efficient in this design.
[3700.66s -> 3703.82s]  Cool, thank you.
[3703.82s -> 3705.86s]  Sorry, just to follow up on that,
[3705.86s -> 3709.10s]  that means that you said that it goes from P1
[3709.10s -> 3711.26s]  and returns to P2.
[3711.26s -> 3716.14s]  So to come back, you would need to send the response.
[3716.14s -> 3720.74s]  That's right, we expect P2 to send a response.
[3720.78s -> 3722.90s]  And that sending of the response actually
[3722.90s -> 3725.54s]  follows basically the same code path in reverse,
[3725.54s -> 3729.86s]  so that when P2 sends a response,
[3729.86s -> 3734.18s]  that effectively causes P1 to return from the database.
[3734.18s -> 3736.58s]  P1 is actually making this call, system call.
[3736.58s -> 3739.74s]  So the delivery of P2's response
[3739.74s -> 3744.82s]  causes the return from this system call back into P1.
[3744.82s -> 3746.26s]  OK, I see.
[3746.26s -> 3747.38s]  Thank you.
[3747.38s -> 3749.50s]  This is a little bit different from the usual setup
[3749.50s -> 3752.50s]  where you jump into the kernel in a system call,
[3752.50s -> 3755.30s]  and you execute that system call, and it returns sort of all
[3755.30s -> 3757.30s]  working on behalf of P1, which is the way
[3757.30s -> 3759.02s]  pipe read and write work.
[3759.02s -> 3761.98s]  Here, P1 is entering the kernel.
[3761.98s -> 3766.86s]  It's P1 entering the kernel, but the return goes to P2.
[3766.86s -> 3771.10s]  So it's kind of odd, but very fast.
[3771.10s -> 3784.62s]  OK, so this was a big sort of contribution
[3784.62s -> 3789.86s]  to people taking microkernels, people's willingness
[3789.86s -> 3793.22s]  to take microkernels seriously as potentially a replacement
[3793.22s -> 3795.98s]  for monolithic kernels.
[3795.98s -> 3799.66s]  However, you still have to leave open the question,
[3799.66s -> 3801.50s]  even if RPC is fast, where do you
[3801.50s -> 3803.94s]  get the rest of the operating system?
[3803.94s -> 3805.86s]  This kernel only has a few percent
[3805.86s -> 3808.18s]  of all the stuff like file systems and network stacks
[3808.18s -> 3810.98s]  that we expect to be in a full operating system.
[3810.98s -> 3813.06s]  What do we do about the rest?
[3813.06s -> 3815.46s]  And this question is usually being asked
[3815.46s -> 3817.94s]  in the context of some university research project
[3817.94s -> 3822.30s]  with relatively limited resources.
[3822.30s -> 3827.62s]  We need to get all those user level services from somewhere.
[3827.62s -> 3830.06s]  Actually, there are specialized applications
[3830.06s -> 3831.90s]  for which that's not too much of a problem.
[3831.90s -> 3836.46s]  If we're running some sort of controller,
[3836.46s -> 3841.22s]  maybe the ignition control system for your car
[3841.22s -> 3844.74s]  that is only running a few thousand lines of code anyway,
[3844.74s -> 3847.58s]  maybe doesn't need a file system,
[3847.58s -> 3850.10s]  then we can get away with very little stuff at user level.
[3850.10s -> 3852.78s]  And microkernels totally make sense
[3852.78s -> 3854.78s]  for that kind of application.
[3854.78s -> 3857.74s]  But the people in these projects, really,
[3857.74s -> 3859.58s]  they had ambitions that, oh, gosh, we
[3859.58s -> 3862.62s]  can totally replace existing operating systems.
[3862.62s -> 3864.62s]  And they hoped that they could build something
[3864.62s -> 3867.62s]  that people would want to run on their workstations
[3867.62s -> 3870.34s]  and run on their servers and everywhere and replace
[3870.34s -> 3872.86s]  big monolithic kernels altogether.
[3872.86s -> 3875.54s]  But for that, you need all the stuff
[3875.54s -> 3878.58s]  that an operating system does.
[3878.58s -> 3884.10s]  One possibility, the most maybe philosophically consistent
[3884.14s -> 3888.14s]  possibility would be to re-implement everything
[3888.14s -> 3891.58s]  you need, but in a sort of microkernel way
[3891.58s -> 3895.54s]  as lots and lots of different user level processes.
[3895.54s -> 3898.78s]  But that's just, actually, there
[3898.78s -> 3900.18s]  were projects that did that.
[3900.18s -> 3903.02s]  But it's a vast amount of work.
[3903.02s -> 3907.50s]  And more specifically, people really want to run.
[3907.50s -> 3908.86s]  In order for me to use a laptop,
[3908.86s -> 3911.42s]  it just has to run Emacs.
[3911.42s -> 3913.86s]  And it has to run my favorite C compiler.
[3913.86s -> 3915.34s]  Otherwise, I'm just definitely not
[3915.34s -> 3917.86s]  going to switch to your operating system.
[3917.86s -> 3919.86s]  And what that meant is that microkernels,
[3919.86s -> 3921.70s]  in order to gain any kind of adoption,
[3921.70s -> 3924.94s]  they had to be able to support existing applications.
[3924.94s -> 3927.02s]  They had to be able to be compatible, provide
[3927.02s -> 3930.34s]  an identical, at least at the system call,
[3930.34s -> 3935.06s]  at the higher level service API level.
[3935.06s -> 3937.58s]  They had to be totally compatible with some existing
[3937.58s -> 3939.86s]  operating system, like Unix, like Linux,
[3939.90s -> 3943.86s]  in order for anybody to be willing to switch.
[3943.86s -> 3946.54s]  So these projects faced a more specific problem
[3946.54s -> 3948.66s]  of how they were going to get, how
[3948.66s -> 3951.06s]  were they going to attain compatibility
[3951.06s -> 3954.02s]  with existing applications written for Linux,
[3954.02s -> 3955.50s]  or maybe Windows, or something.
[3955.50s -> 3959.70s]  But for this project, it was Linux.
[3959.70s -> 3963.10s]  And rather than write their own totally new set
[3963.10s -> 3965.34s]  of user level servers that mimic Linux,
[3965.34s -> 3967.82s]  they decided to take the far easier path,
[3967.86s -> 3971.38s]  and many projects did this, of simply directly running
[3971.38s -> 3979.26s]  an existing monolithic kernel on top of their microkernel
[3979.26s -> 3981.18s]  instead of re-implementing some new thing.
[3981.18s -> 3983.98s]  And so that's exactly what today's paper is about.
[3986.70s -> 3993.62s]  It has, indeed, L4 microkernel down at the bottom.
[3998.06s -> 4002.10s]  But also, as a pretty big server, they
[4002.10s -> 4010.18s]  run a pretty full Linux kernel as a user level process.
[4010.18s -> 4012.50s]  And so that may sound a little surprising.
[4012.50s -> 4014.34s]  The kernel is not a user level process, right?
[4014.34s -> 4016.02s]  The kernel is the kernel.
[4016.02s -> 4017.90s]  You think of it as running on the hardware.
[4017.90s -> 4020.94s]  But in fact, the Linux kernel, as you
[4020.94s -> 4023.94s]  can see from running XV6 and QEMU,
[4023.94s -> 4026.50s]  which is running in user space after all,
[4026.78s -> 4028.26s]  a kernel is just a program.
[4028.26s -> 4030.46s]  And so with some modifications, it
[4030.46s -> 4034.70s]  can be made to run at user level.
[4034.70s -> 4036.30s]  And so they had to modify Linux.
[4036.30s -> 4039.78s]  And they took a lot of the low level stuff in Linux,
[4039.78s -> 4041.26s]  for example, the code in Linux that
[4041.26s -> 4043.86s]  expects to be able to directly modify page tables,
[4043.86s -> 4046.78s]  or read and write processor registers.
[4046.78s -> 4048.98s]  There was some low level stuff they had to modify.
[4048.98s -> 4050.54s]  So some parts of Linux they had
[4050.54s -> 4055.62s]  to change in order to convert them to basically make
[4055.62s -> 4058.86s]  system calls or send IPC messages through L4 instead
[4058.86s -> 4061.26s]  of directly get at hardware.
[4061.26s -> 4062.86s]  But for the most part, they were
[4062.86s -> 4066.18s]  able to directly run, without change, almost all of Linux.
[4066.18s -> 4069.54s]  So that means they got, as part of Linux, a file
[4069.54s -> 4073.46s]  system and a network support and all kinds of device
[4073.46s -> 4078.74s]  drivers and who knows what that comes with Linux
[4078.74s -> 4083.74s]  without having to write their own version of this.
[4083.74s -> 4086.70s]  Now, in fact, the way this was set up
[4086.70s -> 4093.86s]  was that the Linux kernel ran as one L4 task.
[4093.86s -> 4098.26s]  But each Linux process ran as a separate L4 task.
[4098.26s -> 4100.62s]  So when you log into this Linux
[4100.62s -> 4102.58s]  and you ask it to run a shell for you,
[4102.58s -> 4104.42s]  a terminal window or something, it's
[4104.42s -> 4107.86s]  going to fire up an L4 task that's
[4107.86s -> 4112.18s]  going to run that Linux program at user level.
[4112.18s -> 4114.50s]  So there were one task for Linux and one task
[4114.50s -> 4119.98s]  for each Linux process that you fire up under Linux.
[4119.98s -> 4127.30s]  And Linux, instead of directly modifying the page table
[4127.30s -> 4129.46s]  that the VI process uses, Linux
[4129.46s -> 4133.14s]  is going to send the right IPCs to L4
[4133.14s -> 4136.38s]  to cause L4 to change VI's page table.
[4137.30s -> 4144.14s]  Any questions about the basic scheme here?
[4150.42s -> 4154.98s]  Another thing that was changed, many small things were
[4154.98s -> 4158.58s]  changed, but a specific thing of interest
[4158.58s -> 4161.30s]  is that when VI wants to make a system call,
[4161.30s -> 4166.30s]  so if VI doesn't know it's running on L4,
[4167.02s -> 4170.26s]  in this scheme, all these programs
[4170.26s -> 4172.22s]  just think of themselves as running on Linux.
[4172.22s -> 4178.06s]  When VI wants to make a system call, L4 does not support.
[4178.06s -> 4179.94s]  It's not making an L4 system call.
[4179.94s -> 4181.42s]  It's making a Linux system call.
[4181.42s -> 4184.10s]  So VI system calls, like fork, there's
[4184.10s -> 4192.18s]  a little library basically that was linked into these Linux
[4192.18s -> 4195.90s]  processes that would turn calls to things like fork
[4195.90s -> 4200.94s]  or exec or pipe or read or write into IPC messages
[4200.94s -> 4205.22s]  that it would send to the Linux task
[4205.22s -> 4207.06s]  and wait for the response in the Linux task
[4207.06s -> 4211.30s]  and then return as if the system call had returned.
[4211.30s -> 4213.42s]  So these little libraries would turn system calls
[4213.42s -> 4217.42s]  into IPC messages to Linux.
[4217.42s -> 4221.22s]  And what that meant is that if the Linux kernel task isn't
[4221.22s -> 4223.66s]  doing anything else, it's sitting in a call
[4223.66s -> 4226.38s]  to receive waiting for the next system call request
[4226.38s -> 4229.50s]  IPC from any one of these processes.
[4234.26s -> 4239.14s]  And that leads to a significant difference
[4239.14s -> 4244.50s]  between how this Linux works and how ordinary Linux works.
[4244.50s -> 4246.38s]  In ordinary Linux, just like XV6,
[4246.38s -> 4248.66s]  there's basically a kernel thread
[4248.66s -> 4251.94s]  that corresponds to every user level process.
[4251.94s -> 4255.50s]  And when a program makes a system call,
[4255.50s -> 4259.94s]  the kernel runs a thread on behalf of that system call.
[4259.94s -> 4265.10s]  And in ordinary Linux, when Linux switches between kernel
[4265.10s -> 4268.86s]  threads, that basically implies a switch from one process
[4268.86s -> 4270.46s]  to another.
[4270.46s -> 4275.30s]  So there's kind of one-to-one correspondence between what
[4275.30s -> 4277.42s]  kernel thread Linux kernel is running
[4277.42s -> 4280.54s]  and what process is going to run when Linux is done.
[4280.58s -> 4282.02s]  Here, that connection is broken.
[4282.02s -> 4285.58s]  There were indeed, in this Linux server,
[4285.58s -> 4289.62s]  a kernel thread corresponding to each.
[4289.62s -> 4290.38s]  I'm sorry.
[4290.38s -> 4291.18s]  Let me start again.
[4291.18s -> 4297.10s]  The Linux kernel server was running in a single L4 thread.
[4297.10s -> 4299.78s]  So there was only a single sort
[4299.78s -> 4303.78s]  of thread of control executing in Linux at a time.
[4303.78s -> 4309.54s]  However, just as in XV6, this one thread of control
[4309.54s -> 4315.78s]  would switch, using a technique very much like XV6's context
[4315.78s -> 4321.14s]  switch, could switch between a kernel thread corresponding
[4321.14s -> 4322.70s]  to each user process.
[4322.70s -> 4328.46s]  However, these kernel threads were implemented purely
[4328.46s -> 4331.14s]  within Linux and nothing to do with L4 threads.
[4331.14s -> 4334.02s]  There's only one L4 thread here.
[4334.02s -> 4338.34s]  But which user process was running was determined by L4.
[4338.38s -> 4342.58s]  So in this setup, Linux might be serving a request
[4342.58s -> 4348.06s]  from executing the kernel thread for serving a VI system
[4348.06s -> 4351.26s]  call at the same time that L4 is causing the shell
[4351.26s -> 4355.94s]  to run in user space, which is very unlike what happens
[4355.94s -> 4359.66s]  in XV6 or Linux, where there's a direct correspondence
[4359.66s -> 4362.66s]  between the sort of active kernel thread
[4362.66s -> 4365.74s]  and the corresponding user level thread.
[4365.74s -> 4368.74s]  Here, L4 is off running whatever it feels like.
[4368.74s -> 4370.66s]  And these threads in the Linux kernel
[4370.66s -> 4372.50s]  are really much more private and are just
[4372.50s -> 4376.30s]  about Linux being able to concurrently execute
[4376.30s -> 4378.94s]  system calls in different stages of execution,
[4378.94s -> 4381.58s]  where maybe one process is waiting
[4381.58s -> 4383.90s]  for the disk in its thread.
[4383.90s -> 4387.22s]  Linux can run a different process's kernel thread
[4387.22s -> 4389.82s]  to serve that process's call.
[4395.90s -> 4400.66s]  So you might wonder why this design didn't directly
[4400.66s -> 4408.70s]  use L4 threads to implement the various different kernel
[4408.70s -> 4410.90s]  threads inside Linux.
[4410.90s -> 4412.90s]  Why did Linux implement its own sort
[4412.90s -> 4416.22s]  of internal threads package instead of using L4 threads?
[4416.22s -> 4420.10s]  And the answer was that in those days, A, they
[4420.10s -> 4422.78s]  didn't have access to multi-core hardware.
[4422.78s -> 4424.46s]  They were using single-core hardware.
[4424.50s -> 4428.14s]  So there would be no performance advantage
[4428.14s -> 4430.58s]  to be able to execute multiple threads in the kernel
[4430.58s -> 4433.78s]  at the same time because there was only one core.
[4433.78s -> 4435.78s]  So a second thread couldn't be executing.
[4435.78s -> 4439.06s]  Only one thread could execute at a time due to the hardware.
[4439.06s -> 4441.30s]  And the other maybe even more powerful reason
[4441.30s -> 4443.94s]  is that in those days, the version of Linux they were
[4443.94s -> 4447.98s]  using did not have the support that's
[4447.98s -> 4451.42s]  required to have multiple threads, multiple cores
[4451.42s -> 4453.90s]  executing inside the kernel at the same time.
[4453.94s -> 4456.70s]  They were using a uniprocessor Linux,
[4456.70s -> 4459.78s]  so an old enough Linux that it expected only one core
[4459.78s -> 4462.78s]  in the kernel at a time.
[4462.78s -> 4464.78s]  It didn't have things like the spin locks
[4464.78s -> 4470.82s]  that XV6 has that would allow it to correctly execute
[4470.82s -> 4472.30s]  multiple cores inside the kernel.
[4472.30s -> 4475.42s]  So there would have been no performance advantage
[4475.42s -> 4479.74s]  in having multiple L4 threads active inside the kernel.
[4479.74s -> 4482.14s]  But it would have required adding
[4483.10s -> 4486.46s]  for no performance win, adding in all the spin locks
[4486.46s -> 4489.34s]  and other stuff that's required to support concurrency.
[4489.34s -> 4493.46s]  So they didn't do it.
[4493.46s -> 4495.98s]  A drawback of this arrangement is
[4495.98s -> 4499.06s]  that in ordinary Linux, in native Linux,
[4499.06s -> 4501.14s]  like you would run directly on your laptop,
[4501.14s -> 4504.30s]  Linux has a lot of sophisticated scheduling
[4504.30s -> 4507.22s]  machinery that can do things like impose priorities
[4507.22s -> 4510.14s]  on different processes or ensure various kinds of fairness.
[4512.42s -> 4515.02s]  And that works fine on your laptop
[4515.02s -> 4517.86s]  because Linux is in control of what process
[4517.86s -> 4520.02s]  is running on each core.
[4520.02s -> 4522.70s]  But in this setup, Linux is not controlling that at all.
[4522.70s -> 4526.06s]  Linux has no control over what process
[4526.06s -> 4529.86s]  is running because it's L4 that does this scheduling, not
[4529.86s -> 4530.94s]  Linux.
[4530.94s -> 4533.02s]  These processes are scheduled by L4,
[4533.02s -> 4534.46s]  so they kind of lost the ability
[4534.46s -> 4540.10s]  to have Linux be in charge of scheduling.
[4540.10s -> 4542.26s]  That's a bit of a defect of this,
[4542.26s -> 4545.54s]  although I'm sure later versions of L4
[4545.54s -> 4549.14s]  had some way for Linux or something like it
[4549.14s -> 4552.82s]  to be able to tell the L4 scheduler, oh, look, please
[4552.82s -> 4555.90s]  give this process higher priority or whatever.
[4555.90s -> 4557.14s]  Still, it's a bit awkward.
[4562.30s -> 4562.78s]  All right.
[4562.78s -> 4573.02s]  So they went to all this work to get this going.
[4573.02s -> 4579.02s]  And you should ask yourself, what's the takeaway lesson
[4579.02s -> 4585.54s]  from this paper about microkernels?
[4585.54s -> 4587.50s]  Now, one thing that's so for us,
[4587.50s -> 4589.58s]  this paper has a lot of interesting tidbits
[4589.58s -> 4591.94s]  about how microkernels work and about how Linux works
[4591.94s -> 4594.54s]  and how you set up, how you can design
[4594.54s -> 4598.10s]  a system like this, which may be interesting.
[4598.10s -> 4600.62s]  But in the larger world, people
[4600.62s -> 4602.98s]  want to draw some lessons.
[4602.98s -> 4605.50s]  They need to be able to present
[4605.50s -> 4608.66s]  some lessons in this paper.
[4608.66s -> 4611.14s]  The paper is not really answering the question,
[4611.14s -> 4613.30s]  are microkernels a good idea?
[4613.30s -> 4615.94s]  That's not really what's going on here.
[4615.94s -> 4619.18s]  The paper, what the paper is is part of a argument
[4619.18s -> 4623.22s]  about whether microkernels have enough performance
[4623.22s -> 4627.10s]  to be worth using.
[4627.10s -> 4632.58s]  And the reason is that in maybe five years or five or 10
[4632.58s -> 4635.06s]  years before this paper came out, there
[4635.06s -> 4636.46s]  was a famous set of measurements
[4636.46s -> 4638.14s]  on one of the predecessor microkernels,
[4638.14s -> 4640.34s]  an earlier microkernel called Mach,
[4640.34s -> 4644.38s]  basically running in very much this configuration
[4644.38s -> 4647.54s]  but a totally different design internally
[4647.54s -> 4649.54s]  but kind of the same architecture.
[4652.62s -> 4655.98s]  The name of this earlier microkernel project is Mach.
[4655.98s -> 4657.62s]  There was measurements on Mach that showed
[4657.62s -> 4662.62s]  that Mach was dramatically slower than just ordinary Unix
[4663.02s -> 4665.06s]  when it was run in this configuration.
[4665.06s -> 4668.38s]  And there are a lot of reasons for that having to do
[4668.38s -> 4671.38s]  with the IPC system not being as optimized
[4671.38s -> 4672.50s]  as you might hope.
[4672.50s -> 4675.06s]  There being just sort of more context switches
[4675.06s -> 4680.06s]  and we use our kernel crossings and cache misses
[4680.34s -> 4681.18s]  and whatever.
[4681.18s -> 4683.98s]  There's a whole lot of reasons why Mach was slow,
[4683.98s -> 4687.06s]  but many people saw those benchmark results showing
[4687.06s -> 4689.74s]  that Mach was much slower than native operating systems
[4689.74s -> 4693.10s]  and decided that microkernels were just hopeless,
[4693.10s -> 4695.86s]  hopelessly inefficient, were unlikely ever to be fast
[4695.86s -> 4696.98s]  enough to be competitive.
[4696.98s -> 4700.94s]  And we should just all use monolithic kernels.
[4701.82s -> 4706.82s]  Today's paper is like an answer basically to that argument.
[4706.98s -> 4709.90s]  It's sort of the rebuttal to that argument.
[4709.90s -> 4711.54s]  And the point of this paper is to show
[4711.54s -> 4713.78s]  that you can build this architecture.
[4713.78s -> 4717.02s]  And if you pay enough attention to optimizing performance,
[4717.02s -> 4719.98s]  you can get competitive performance
[4719.98s -> 4721.94s]  with native operating systems.
[4721.94s -> 4724.34s]  So it was just directly running Unix.
[4724.34s -> 4728.02s]  And therefore you can't dismiss microkernels simply
[4728.02s -> 4729.86s]  on the basis of performance.
[4730.06s -> 4731.50s]  You may not want them for other reasons,
[4731.50s -> 4735.14s]  but you can't use performance as the reason
[4735.14s -> 4735.98s]  to reject them.
[4738.42s -> 4742.42s]  A huge part of the ingredient in making that argument
[4742.42s -> 4744.06s]  is that they made the IPC much faster
[4744.06s -> 4746.86s]  with the techniques that I outlined a few minutes ago.
[4746.86s -> 4748.22s]  And you can see this, I think,
[4748.22s -> 4752.50s]  in a very simple benchmark in table two.
[4752.50s -> 4754.70s]  If you have a copy of the paper with you.
[4755.90s -> 4759.18s]  Table two has measurements of just native Linux
[4759.18s -> 4761.42s]  running in the ordinary way on hardware.
[4761.42s -> 4764.18s]  And on native Linux, they show that,
[4764.18s -> 4766.30s]  on their hardware and their version of Linux,
[4766.30s -> 4769.86s]  that a single simple system called GetPID
[4769.86s -> 4772.42s]  took 1.7 microseconds.
[4773.86s -> 4778.26s]  And they also show that the sort of equivalent thing
[4778.26s -> 4782.18s]  in their L4 setup, where you have to send an IPC request
[4782.18s -> 4783.66s]  and get an IPC response
[4783.66s -> 4786.50s]  just for this get process ID system call,
[4786.50s -> 4791.50s]  that that took four microseconds under L4 Linux,
[4793.74s -> 4795.14s]  which is to say twice as long,
[4795.14s -> 4798.02s]  but there's sort of twice as much work going on
[4798.02s -> 4801.02s]  because you're doing two sets of user kernel crossings
[4801.02s -> 4804.86s]  instead of just a single simple system call.
[4804.86s -> 4806.34s]  That is, they could claim
[4806.34s -> 4808.62s]  that they had paired the expense
[4808.62s -> 4810.46s]  of these IPC-based system calls
[4810.46s -> 4812.02s]  down to basically the minimum,
[4812.02s -> 4816.34s]  that is twice the cost of a system call on native Linux.
[4816.34s -> 4818.02s]  And therefore, they were doing roughly as good
[4818.02s -> 4819.78s]  as you could possibly expect.
[4821.86s -> 4824.34s]  Now, of course, their system calls
[4824.34s -> 4826.42s]  are still half as fast as native Linux.
[4828.54s -> 4831.78s]  And it's not clear, unless you did some measurements,
[4831.78s -> 4833.94s]  whether system calls taking twice
[4833.94s -> 4835.50s]  or simple system calls taking twice
[4835.50s -> 4838.66s]  as long as a disaster or not a problem.
[4838.66s -> 4841.42s]  And in order to show that it might be a disaster
[4841.42s -> 4842.82s]  if you do a lot of system calls
[4842.82s -> 4844.14s]  or might be not a problem
[4844.14s -> 4847.14s]  if you do relatively few system calls
[4847.14s -> 4849.62s]  or there's a lot of work per system call
[4849.62s -> 4851.22s]  because maybe your system calls
[4851.22s -> 4852.98s]  are more complicated than get PID.
[4854.42s -> 4856.34s]  And the answer to that in the paper
[4856.34s -> 4859.30s]  is the figure eight benchmark
[4859.30s -> 4862.18s]  using this benchmark called AIM,
[4862.18s -> 4863.70s]  which is just a more,
[4863.70s -> 4865.14s]  it's a benchmark that does all kinds
[4865.14s -> 4866.22s]  of different system calls.
[4866.22s -> 4868.18s]  It reads and writes files and creates processes
[4868.18s -> 4869.26s]  or does all the things
[4869.26s -> 4872.18s]  with the kernel that processes do.
[4872.22s -> 4873.98s]  And they basically showed in figure eight
[4873.98s -> 4877.98s]  that they're set up running a much more full application
[4877.98s -> 4881.86s]  that does much more than just get PID,
[4881.86s -> 4885.62s]  runs only a few percent slower than native Linux.
[4885.62s -> 4887.66s]  And that therefore, hopefully you could expect
[4887.66s -> 4890.26s]  that whatever it is you wanted to run on a computer
[4890.26s -> 4893.58s]  would run almost as fast under L4 plus Linux
[4893.58s -> 4896.34s]  as it does under a straight operating system,
[4896.34s -> 4898.22s]  under native operating system.
[4898.22s -> 4901.94s]  Therefore, they were basically to a first approximation
[4902.54s -> 4905.94s]  as fast as just running straight Linux
[4905.94s -> 4909.02s]  and therefore you should take them seriously.
[4910.98s -> 4913.54s]  Okay, so that was an impressive result by the way.
[4913.54s -> 4917.66s]  This is like somewhat unexpected and cool.
[4918.58s -> 4923.14s]  Just fast forwarding 20 years where this ended up.
[4923.14s -> 4926.26s]  As I mentioned before, people actually use L4
[4926.26s -> 4927.78s]  in a bunch of embedded situations,
[4927.78s -> 4929.58s]  particularly it's used a lot.
[4929.58s -> 4931.42s]  There's many instances of L4 running
[4932.42s -> 4937.42s]  in smartphones hidden from view but nevertheless,
[4937.46s -> 4939.34s]  in all running various kinds of custom software
[4939.34s -> 4942.58s]  not running, they don't have to have compatibility
[4942.58s -> 4946.18s]  with Unix in these situations.
[4946.18s -> 4948.98s]  Micro kernels in other more general situations
[4948.98s -> 4952.62s]  like workstations or servers never really caught on.
[4952.62s -> 4955.18s]  And it's not because there's necessarily anything wrong
[4955.18s -> 4957.58s]  with that design, it's just they would have
[4957.58s -> 4961.54s]  in order to display some existing software,
[4961.54s -> 4964.26s]  your new thing has to be better
[4964.26s -> 4966.86s]  so people will be motivated to switch.
[4966.86s -> 4969.34s]  And these micro kernels were perfectly good,
[4969.34s -> 4973.14s]  not certainly elegant but it was hard for people
[4973.14s -> 4975.46s]  to put their finger on why it was so much better
[4975.46s -> 4978.54s]  that they should go to the trouble of switching from Linux
[4978.54s -> 4980.14s]  or whatever they were running.
[4981.18s -> 4983.50s]  And so it never really caught on,
[4983.50s -> 4985.06s]  not necessarily for good reasons
[4985.06s -> 4988.98s]  but because they weren't like dramatically better.
[4988.98s -> 4992.18s]  On the other hand, many ideas from this architecture
[4992.18s -> 4993.74s]  had a lasting impact.
[4996.26s -> 4999.30s]  People had to work out much more interesting
[4999.30s -> 5001.82s]  and flexible ways of using virtual memory
[5001.82s -> 5005.78s]  in order to support operating systems on their micro kernels
[5005.78s -> 5008.50s]  and those more sophisticated interfaces
[5008.50s -> 5011.38s]  made their way through things like MMAP
[5011.38s -> 5013.94s]  into mainstream operating systems like Linux.
[5015.22s -> 5018.90s]  This idea of running an operating system kind of on top
[5018.90s -> 5023.70s]  as a server on top of a lower level operating system
[5023.70s -> 5025.30s]  is extremely popular today
[5025.30s -> 5027.82s]  in the form of virtual machine monitors
[5027.82s -> 5029.18s]  which use all over the place
[5029.18s -> 5031.62s]  and sort of cloud hosting services.
[5032.98s -> 5034.90s]  The desire for extensibility,
[5034.90s -> 5039.42s]  you could modify a user level service.
[5039.42s -> 5041.90s]  The way that played out in things like Linux
[5041.90s -> 5043.58s]  was loadable kernel modules
[5043.58s -> 5046.10s]  which allow you to modify the way
[5046.10s -> 5048.22s]  the Linux kernel works on the fly.
[5049.34s -> 5051.58s]  And of course the sort of client server,
[5051.58s -> 5054.46s]  good support for this client server architecture
[5054.46s -> 5056.74s]  also made its way into kernels like Mac OS
[5056.74s -> 5059.42s]  which has good IPC and good client server.
[5061.02s -> 5063.50s]  And that's all I have to say for this lecture.
[5063.50s -> 5066.66s]  I'm happy to stick around for questions.
[5066.66s -> 5067.50s]  Thank you.
[5067.50s -> 5068.34s]  Thank you.
[5068.34s -> 5069.66s]  You're welcome.
[5070.66s -> 5071.70s]  Oh, I wanted to ask,
[5071.70s -> 5075.98s]  so the paper was talking about virtual,
[5075.98s -> 5080.98s]  about page tables at I think 4.2.
[5081.46s -> 5083.26s]  And it was saying how,
[5085.26s -> 5088.42s]  I think it was kind of what you mentioned before
[5088.42s -> 5092.70s]  where you said that there is a wrong way to do that.
[5092.70s -> 5095.14s]  I think might be kind of similar to that,
[5095.14s -> 5100.14s]  but if you do this thing that you explained
[5100.38s -> 5103.58s]  in your picture now,
[5103.58s -> 5106.58s]  would it be, I guess,
[5106.58s -> 5110.46s]  how would the page tables work in this case?
[5110.46s -> 5114.62s]  Well, you may be referring to section 4.3,
[5114.62s -> 5116.10s]  the dual space mistake.
[5116.10s -> 5118.10s]  Oh yes, sorry, 4.3.
[5118.10s -> 5119.94s]  I'm sorry, I'm sorry.
[5119.94s -> 5121.62s]  4.3, the dual space mistake.
[5121.62s -> 5123.70s]  Oh yes, sorry, 4.3.
[5123.70s -> 5125.90s]  Oh no, yeah, that's a bit of a complicated story,
[5125.90s -> 5129.30s]  but the, let's see,
[5130.42s -> 5132.02s]  the part of the background
[5132.02s -> 5134.18s]  is the way that Linux worked in those days
[5134.18s -> 5136.06s]  and then indeed until recently
[5136.06s -> 5140.58s]  is that when you're running at user level,
[5140.58s -> 5141.90s]  the page table that's active
[5141.90s -> 5143.90s]  has both the processes pages,
[5143.90s -> 5145.26s]  user level pages mapped in
[5145.26s -> 5148.62s]  and all of the kernel mapped into that one page table
[5148.66s -> 5150.22s]  on the xv6 anyway.
[5150.22s -> 5152.66s]  So when you made a system call
[5153.54s -> 5155.50s]  and jumped into the kernel,
[5155.50s -> 5158.10s]  the kernel was already mapped into the page table
[5158.10s -> 5160.74s]  and therefore no page table switch was required.
[5160.74s -> 5161.94s]  So when you make a system call,
[5161.94s -> 5163.18s]  it's that much more expensive
[5163.18s -> 5164.78s]  and much more cheaper
[5164.78s -> 5166.14s]  because there was no page table switch.
[5166.14s -> 5167.54s]  If you're calling xv6,
[5167.54s -> 5169.46s]  the trampoline code switches page tables,
[5169.46s -> 5172.34s]  which is an expensive thing to do
[5172.34s -> 5175.50s]  because it flushes the TLB cache
[5175.50s -> 5178.14s]  of virtual to physical mappings.
[5178.66s -> 5179.50s]  Anyway, so for efficiency,
[5179.50s -> 5181.90s]  Linux used to map kernel and user space
[5181.90s -> 5182.86s]  in the same page table
[5182.86s -> 5185.94s]  and had fast system calls as a result.
[5187.82s -> 5192.50s]  So they, for reasons that aren't very clear,
[5192.50s -> 5195.62s]  decided to do this same thing
[5195.62s -> 5197.82s]  to set up the mappings in the Unix server.
[5199.34s -> 5204.02s]  Well, what they wanted was that when VI,
[5204.02s -> 5206.82s]  when a process sent a system call over here,
[5206.86s -> 5209.06s]  they wanted to have the page table that was active
[5209.06s -> 5212.46s]  while in the Linux server
[5212.46s -> 5213.90s]  while processing that system call
[5213.90s -> 5217.18s]  include all the virtual memory mappings
[5217.18s -> 5220.14s]  for the process that sent the system call.
[5222.26s -> 5225.10s]  And that at least would make it simpler
[5225.10s -> 5227.70s]  to look up virtual addresses
[5227.70s -> 5231.14s]  passed as system call arguments like pass to read.
[5231.14s -> 5233.46s]  The reason why this worked out poorly,
[5233.46s -> 5234.42s]  there were a bunch of reasons.
[5234.42s -> 5238.18s]  One is that L4,
[5238.18s -> 5240.46s]  which doesn't know anything about any of this stuff,
[5240.46s -> 5242.46s]  L4 just knows there's two processes.
[5242.46s -> 5245.54s]  And so when you send an IPC from one process to another,
[5245.54s -> 5247.78s]  L4 just switches page tables.
[5247.78s -> 5249.10s]  It always just switches page tables.
[5249.10s -> 5252.18s]  This guy had a page, VI had a page table.
[5252.18s -> 5254.34s]  L4 associates the page table with the Linux kernel.
[5254.34s -> 5256.06s]  It just always switches page tables.
[5256.06s -> 5258.86s]  So you couldn't even, due to L4,
[5258.86s -> 5261.30s]  due to the different ways system calls were implemented
[5261.30s -> 5263.54s]  and the fact that L4 was involved,
[5263.54s -> 5265.94s]  there was no way to preserve the page table
[5265.94s -> 5267.54s]  during a system call.
[5267.54s -> 5269.22s]  That just wasn't possible
[5269.22s -> 5272.22s]  because L4 always switched page tables
[5272.22s -> 5273.94s]  when it switched from one process to another.
[5273.94s -> 5276.46s]  So they were never gonna get the efficiency win
[5276.46s -> 5278.70s]  of not having to switch page tables
[5278.70s -> 5283.46s]  when one sort of crossing from user to kernel.
[5285.46s -> 5287.66s]  But I think they wanted the convenience
[5287.66s -> 5290.46s]  of being able to directly use
[5290.46s -> 5292.90s]  user-supplied virtual addresses.
[5292.94s -> 5297.18s]  But that meant that the mappings they needed to be active
[5297.18s -> 5299.46s]  depended on which process
[5299.46s -> 5302.50s]  they were executing a system call on behalf of.
[5302.50s -> 5305.98s]  So there couldn't be any one page table for Linux.
[5305.98s -> 5308.30s]  The page table Linux server wanted to use
[5308.30s -> 5311.98s]  depending on what process it sent the system call RPC.
[5311.98s -> 5315.18s]  But L4 did not know how to play that game.
[5315.18s -> 5318.78s]  L4 associated a single page table with each process,
[5318.78s -> 5320.10s]  with each task.
[5320.10s -> 5321.82s]  And so in order,
[5321.82s -> 5323.22s]  and it would just switch to that page table.
[5323.22s -> 5325.10s]  So tough luck, Linux didn't have any way
[5325.10s -> 5328.14s]  to cause the page table to differ
[5328.14s -> 5331.66s]  depending on who had sent the system call.
[5331.66s -> 5333.14s]  In order to deal with that,
[5333.14s -> 5337.66s]  apparently they made a bunch of shared memory copies
[5337.66s -> 5341.66s]  of the kernel, one for each process.
[5341.66s -> 5344.74s]  And so each of these shared memory copies of the kernel
[5344.74s -> 5348.18s]  had all of the kernel memory mapped into it.
[5348.18s -> 5351.10s]  So they were all, this was the same kernel data structures,
[5351.10s -> 5355.22s]  but each process had a dedicated kernel task
[5357.22s -> 5358.06s]  associated with it.
[5358.06s -> 5361.62s]  And therefore that basically allowed them to trick L4
[5361.62s -> 5363.46s]  into switching to the appropriate page table
[5363.46s -> 5366.98s]  that included that process plus the kernel,
[5366.98s -> 5370.02s]  depending on which process sent the system call request.
[5370.02s -> 5373.06s]  And I think that kind of worked,
[5373.06s -> 5374.94s]  but, or I don't know, forget what they said,
[5374.94s -> 5377.78s]  that worked, it was slow or something.
[5378.30s -> 5379.94s]  Because there were a lot of tasks.
[5382.10s -> 5383.54s]  Anyway, it's like a complicated story
[5383.54s -> 5386.18s]  and I think it didn't work out very well for them.
[5386.18s -> 5387.62s]  Okay, okay, I see.
[5387.62s -> 5390.30s]  I think that explains well,
[5390.30s -> 5392.50s]  why this thing is harder to do
[5392.50s -> 5396.18s]  than what we do in xv6, okay.
[5396.18s -> 5398.78s]  Yeah, yeah, because there's not,
[5398.78s -> 5401.02s]  yeah, this picture in xv6 or even standard Linux
[5401.02s -> 5402.38s]  is much simpler than this
[5402.38s -> 5405.74s]  because you're just jumping directly into the kernel
[5405.78s -> 5407.18s]  and the kernel has control over,
[5407.18s -> 5409.50s]  direct control over all the paging hardware,
[5409.50s -> 5412.46s]  which it doesn't have when it runs on L4.
[5412.46s -> 5413.98s]  Right, okay, I see.
[5413.98s -> 5415.18s]  Thank you, thank you.
[5416.58s -> 5419.16s]  We're gonna ask why,
[5420.06s -> 5423.86s]  it seems like some tasks are more appropriate
[5423.86s -> 5426.70s]  to be put outside the kernel than others,
[5426.70s -> 5429.78s]  but this ELF, like the approach with microkernels
[5429.78s -> 5432.26s]  always seems to be either everything or nothing,
[5432.26s -> 5434.66s]  or like either you have a monolithic kernel
[5434.70s -> 5436.62s]  doing everything or nothing.
[5436.62s -> 5440.36s]  Just like, I feel like paging and some other things
[5440.36s -> 5442.34s]  could be very efficient inside the kernel
[5442.34s -> 5444.30s]  and then maybe like file systems
[5444.30s -> 5447.78s]  that things that need to be swappable could be outside.
[5447.78s -> 5450.74s]  And then even like, you could maybe even have a kernel
[5450.74s -> 5452.04s]  that has some functionality,
[5452.04s -> 5455.88s]  but you can opt to not use it and provide your own.
[5455.88s -> 5456.72s]  Is there any-
[5456.72s -> 5460.62s]  Yes, everything you say is absolutely well taken
[5460.62s -> 5463.74s]  and indeed there were a lot of microkernel
[5463.78s -> 5466.58s]  or microkernel related projects.
[5466.58s -> 5469.38s]  And many of them built various kinds of hybrids.
[5470.22s -> 5471.58s]  Like there were actually a couple of different versions
[5471.58s -> 5475.10s]  of Mach and some of them were sort of hybrid kernels
[5475.10s -> 5476.54s]  in which, yeah, there was this microkernel
[5476.54s -> 5477.86s]  that knew about IPC,
[5477.86s -> 5482.86s]  but also in the kernel was a complete Unix.
[5482.96s -> 5487.80s]  So for instance, Mach 2.5 was this hybrid with it,
[5488.86s -> 5492.38s]  but microkernel and Unix all sort of in the same kernel.
[5492.42s -> 5494.02s]  And you could make system calls to either
[5494.02s -> 5497.82s]  and some stuff was built in the sort of microkernel-y way,
[5497.82s -> 5500.22s]  but some things where they would just use the kernel
[5500.22s -> 5503.32s]  that was in Mach that was built into the Mach kernel.
[5503.32s -> 5506.38s]  The Unix kernel was built into the Mach kernel.
[5506.38s -> 5509.86s]  And modern, you know, Mac OS also is built in a way
[5509.86s -> 5511.66s]  that like the way you describe, you know,
[5511.66s -> 5516.22s]  Mac OS has a complete operating system
[5516.22s -> 5517.74s]  with a file system and everything inside it,
[5517.74s -> 5520.54s]  but it also has good support for IPC
[5520.54s -> 5522.34s]  and sort of like threads,
[5522.34s -> 5523.46s]  all the stuff you would want
[5523.46s -> 5527.72s]  to build microkernel-style services.
[5528.98s -> 5531.10s]  I think Google's Fuchsia, I'm aware of,
[5531.10s -> 5533.86s]  also implements some of these ideas now as well.
[5533.86s -> 5534.90s]  Oh, I'll bet, yeah.
[5537.54s -> 5540.24s]  So anyway, you know, there's no one way.
[5540.24s -> 5544.02s]  There were people who were sort of hoping that a pure,
[5544.02s -> 5547.78s]  a very pure scheme could be made to work.
[5550.94s -> 5553.88s]  But it was not the only possible way forward.
[5556.50s -> 5557.78s]  All right, thanks.
[5557.78s -> 5561.78s]  Got around to my lecture, but I'll see you guys later.
[5564.16s -> 5565.34s]  Thank you.
[5565.34s -> 5566.18s]  You're welcome.
[5567.90s -> 5570.62s]  Oh, I didn't have a, I have a sort of a mark.
[5570.62s -> 5575.62s]  I think it's fascinating that it's like 5% slower,
[5576.02s -> 5577.98s]  but it does so much more for it.
[5577.98s -> 5579.58s]  Yeah, I was fascinated by that.
[5581.54s -> 5583.06s]  You mean that even though it's doing much more work,
[5583.06s -> 5584.06s]  it's only slightly slower?
[5584.06s -> 5585.50s]  Yeah.
[5585.50s -> 5590.50s]  Well, they really sweat blood over the IPC performance.
[5591.02s -> 5593.34s]  And another thing to remember, of course,
[5593.34s -> 5595.82s]  is that if you start doing,
[5595.82s -> 5598.54s]  if you're doing significant amount of work
[5598.54s -> 5601.46s]  per system call, like, you know,
[5601.46s -> 5604.04s]  looking at files and directories or something,
[5604.04s -> 5606.70s]  then the cost of the system call or the IPC itself
[5606.70s -> 5609.14s]  starts to be less important.
[5610.14s -> 5612.10s]  So the combination of faster system calls
[5612.10s -> 5613.74s]  plus real programs do things
[5613.74s -> 5615.70s]  other than making system calls.
[5617.18s -> 5620.02s]  But you would also like switch page tables
[5620.02s -> 5622.66s]  and do other stuff.
[5622.66s -> 5624.66s]  Yeah, although the paper, I did not talk about it,
[5624.66s -> 5626.62s]  but the paper had some clever tricks
[5626.62s -> 5630.50s]  for avoiding the cost of switching page tables.
[5630.50s -> 5633.58s]  I don't know if you're a member or some,
[5633.58s -> 5634.98s]  but it's like on page six,
[5634.98s -> 5639.02s]  we're talking about supporting tagged TOBs or small spaces.
[5639.90s -> 5643.90s]  They had some clever ideas for not switch page tables,
[5643.90s -> 5646.54s]  which I had not heard of before I wrote this paper.
[5647.66s -> 5648.58s]  This is pretty cool.
[5648.58s -> 5650.14s]  Thank you so much.
[5650.14s -> 5651.38s]  Bye.
[5651.38s -> 5652.22s]  Bye-bye.
